- en: '*Chapter 6*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploratory Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the concept of reproducibility with Jupyter notebooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform data gathering in a reproducible way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement suitable code practices and standards to keep analysis reproducible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid the duplication of work by using IPython scripts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will learn what problem definition is and how to use the
    KPI analysis techniques to enable coherent and well rounded analysis from the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important stages, and the initial step of a data science project,
    is understanding and defining a business problem. However, this cannot be a mere
    reiteration of the existing problem as a statement or a written report. To investigate
    a business problem in detail and define its purview, we can either use the existing
    business metrics to explain the patterns related to it or quantify and analyze
    the historical data and generate new metrics. Such identified metrics are the
    **Key Performance Indicators** (**KPIs**) that measure the problem at hand and
    convey to business stakeholders the impact of a problem.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is all about understanding and defining a business problem, identifying
    key metrics related to it, and using these identified and generated KPIs through
    pandas and similar libraries for descriptive analytics. The chapter also covers
    how to plan a data science project through a structured approach and methodology,
    concluding with how to represent a problem using graphical and visualization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a Business Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A business problem in data science is a *long-term* or *short-term* challenge
    faced by a business entity that can prevent business goals being achieved and
    act as a constraint for growth and sustainability that can otherwise be prevented
    through an efficient data-driven decision system. Some typical data science business
    problems are predicting the demand for consumer products in the coming week, optimizing
    logistic operations for **third-party logistics** (**3PL**), and identifying fraudulent
    transactions in insurance claims.
  prefs: []
  type: TYPE_NORMAL
- en: Data science and machine learning are not magical technologies that can solve
    these business problems by just ingesting data into pre-built algorithms. They
    are complex in terms of the approach and design needed to create end-to-end analytics
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: When a business needs such solutions, you may end up in a situation that forms
    a requirement gap if a clear understanding of the final objective is not set in
    place. A strong foundation to this starts with defining the business problem quantitatively
    and then carrying out scoping and solutions in line with the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are a few examples of common data science use cases that will
    provide an intuitive idea about common business problems faced by the industry
    today that are solved through data science and analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: Inaccurate demand/revenue/sales forecasts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poor customer conversion, churn, and retention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraud and pricing in the lending industry and insurance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ineffective customer and vendor/distributor scoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ineffective recommendation systems for cross-sell/up-sell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unpredictable machine failures and maintenance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer sentiment/emotion analysis through text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-automation of repetitive tasks that require unstructured data analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we all know, in the last few years, industries have been changing tremendously,
    driven by the technology and innovation landscape. With the pace of evolving technology,
    successful businesses adapt to it, which leads to highly evolving and complex
    business challenges and problems. Understanding new business problems in such
    a dynamic environment is not a straightforward process. Though, case-by-case,
    business problems may change, as well as the approaches to them. However, the
    approach can be generalized to a large extent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following pointers are a broad step-by-step approach for defining and concluding
    a business problem, and in the following section, a detailed description of each
    step is provided:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem identification
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Requirement gathering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data pipeline and workflow
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identifying measurable metrics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Documentation and presentation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The target variable, or the study variable, which is used as the attribute/variable/column
    in the dataset for studying the business problem, is also known as the **dependent
    variable** (**DV**), and all other attributes that are considered for the analysis
    are called **independent variables** (**IVs**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Problem Identification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's start with an example where an **Asset Management Company** (**AMC**)
    that has a strong hold on customer acquisition in their mutual funds domain, that
    is, targeting the right customers and onboarding them, is looking for higher customer
    retention to improve the average customer revenue and wallet share of their premium
    customers through data science-based solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the business problem is how to increase the revenue from the existing
    customers and increase their wallet share.
  prefs: []
  type: TYPE_NORMAL
- en: The problem statement is "*How do we improve the average customer revenue and
    increase the wallet share of premium customers through customer retention analytics?*"
    Summarizing the problem as stated will be the first step to defining a business
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement Gathering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the problem is identified, have a point-by-point discourse with your client,
    which can include a **subject matter expert** (**SME**) or someone who is well-versed
    in the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Endeavor to comprehend the issue from their perspective and make inquiries about
    the issue from various points of view, understand the requirements, and conclude
    how you can define the problem from the existing historical data.
  prefs: []
  type: TYPE_NORMAL
- en: Now and again, you will find that clients themselves can't comprehend the issue
    well. In such cases, you should work with your client to work out a definition
    of the issue that is satisfactory to both of you.
  prefs: []
  type: TYPE_NORMAL
- en: Data Pipeline and Workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After you have comprehended the issue in detail, the subsequent stage is to
    characterize and agree on quantifiable metrics for measuring the problem, that
    is, agreeing with the clients about the metrics to use for further analysis. In
    the long run, this will spare you a ton of issues.
  prefs: []
  type: TYPE_NORMAL
- en: These metrics can be identified with the existing system for tracking the performance
    of the business, or new metrics can be derived from historical data.
  prefs: []
  type: TYPE_NORMAL
- en: When you study the metrics for tracking the problem, the data for identifying
    and quantifying the problem may come from multiple data sources, databases, legacy
    systems, real-time data, and so on. The data scientist involved with this has
    to closely work with the client's data management teams to extract and gather
    the required data and push it into analytical tools for further analysis. For
    this, there needs to be a strong pipeline for acquiring data. The acquired data
    is further analyzed to identify its important attributes and how they change over
    time in order to generate the KPIs. This is a crucial stage in client engagement
    and working in tandem with their teams goes a long way toward making the work
    easier.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Measurable Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the required data is gathered through data pipelines, we can develop descriptive
    models to analyze historical data and generate insights into the business problem.
    **Descriptive models/analytics** is all about knowing *what has happened in the
    past* through time trend analysis and the density distribution of data analysis,
    among other things. For this, several attributes from the historical data need
    to be studied in order to gain insights into which of the data attributes are
    related to the current problem.
  prefs: []
  type: TYPE_NORMAL
- en: An example, as explained in the previous case, is an AMC that is looking for
    solutions to their specific business problem with customer retention. We'll look
    into identifying how we can generate KPIs to understand the problem with retention.
  prefs: []
  type: TYPE_NORMAL
- en: For this, historical data is mined to analyze the customer transaction patterns
    of previous investments and derive KPIs from them. A data scientist has to develop
    these KPIs based on their relevance and efficiency in explaining the variability
    of the problem, or in this case, the retention of customers.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation and Presentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final step is documenting the identified KPIs, their significant trends,
    and how they can impact the business in the long run. In the previous case of
    customer retention, all these metrics—**length of relationship**, **average transaction
    frequency**, **churn rate**—can act as KPIs and be used to explain the problem
    quantitatively.
  prefs: []
  type: TYPE_NORMAL
- en: If we observe the trend in the churn rate, and let's say in this example, it
    has an increasing trend in the last few months, and if we graphically represent
    this, the client can easily understand the importance of having predictive churn
    analytics in place to identify the customer before they churn, and targeting stronger
    retention.
  prefs: []
  type: TYPE_NORMAL
- en: The potential of having a retention system in place needs to be presented to
    the client for which the documentation and graphical representation of the KPIs
    need to be carried out. In the previous case, the identified KPIs, along with
    their change in pattern, need to be documented and presented to the client.
  prefs: []
  type: TYPE_NORMAL
- en: Translating a Business Problem into Measurable Metrics and Exploratory Data
    Analysis (EDA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a specific business problem comes to us, we need to identify the KPIs that
    define that business problem and study the data related to it. Beyond generating
    KPIs related to the problem, looking into the trends and quantifying the problem
    through **Exploratory Data Analysis** (**EDA**) methods will be the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach to explore KPIs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Data gathering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of data generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KPI visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Gathering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The data that is required for analyzing the problem is part of defining the
    business problem. However, the selection of attributes from the data will change
    according to the business problem. Consider the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: If it is a recommendation engine or churn analysis of customers, we need to
    look into historical purchases and **Know Your Customer** (**KYC**) data, among
    other data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is related to forecasting demand, we need to look into daily sales data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It needs to be concluded that the required data can change from problem to problem.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of Data Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the available data sources, the next step is to identify the metrics related
    to the defined problem. Apart from the preprocessing of data (refer to *Chapter
    1*, *The Python Data Science Stack*, for details on data manipulation), at times,
    we need to manipulate the data to generate such metrics, or they can be directly
    available in the given data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's say we are looking at supervised analysis, such as a **predictive
    maintenance problem** (a problem where predictive analytics is used to predict
    the condition of in-service equipment or machinery before it fails), where sensor-
    or computer-generated log data is used. Although log data is unstructured, we
    can identify which of the log files explain the failures of machinery and which
    do not. Unstructured data is without columns or rows. For example, it can be in
    XML or a similar format. Computer-generated log data is an example. Such data
    needs to be converted into columns and rows or make them structured, or label
    them, that is, provide column names for the data by converting the data into rows
    and columns.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is identifying the churn of customers and predicting future
    customers who may churn in the coming periods, where we have transactional data
    on purchases with the features related to each purchase. Here, we need to manipulate
    the data and transform the current data in order to identify which customers have
    churned and which have not from all the purchase-related data.
  prefs: []
  type: TYPE_NORMAL
- en: To explain this better, in the raw data, there may be many rows of purchases
    for each customer, with the date of purchase, the units purchased, the price,
    and so on. All the purchases related to a customer need to be identified as a
    single row whether the customer has churned (churned means customers who discontinue
    using a product or service, also known as customer attrition) or not and with
    all the related information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will be generating a KPI: **Churned** or **Not Churned** attributes
    for a customer, and similarly for all customers. The identified variable that
    defines the business problem is the target variable. A target variable is also
    known as the **response variable** or **dependent variable**. In *Exercise XX*,
    *Generate the Feature Importance for the Target Variable and Carry Out EDA*, in
    this chapter, it''s captured and defined through the **churn** attribute.'
  prefs: []
  type: TYPE_NORMAL
- en: KPI Visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand the trends and patterns in the KPIs, we need to represent them
    through interactive visualization techniques. We can use different methods, such
    as boxplot, time-trend graphs, density plots, scatter plots, pie charts, and heatmaps.
    We will learn more on this in *Exercise XX*, *Generate the Feature Importance
    for the Target Variable and Carry Out EDA*, in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Importance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the target variable is identified, the other attributes in the data and
    their importance to it in explaining the variability of the target variable need
    to be studied. For this, we use association, variance, and correlation methods
    to establish relations with the target variable of other variables (**explanatory**
    or **independent** variables).
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple feature-importance methods and algorithms that can be used
    depending on the type of variables in the study, such as Pearson Correlation,
    Chi-Square tests, and algorithms based on Gini variable importance, decision trees,
    and Boruta, among others.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The **target variable** or the **study variable**, which is used as the attribute/variable/column
    in the dataset for studying the business problem is also known as the **dependent
    variable** (**DV**), and all other attributes that are considered for the analysis
    are called **independent variables** (**IVs**).
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will cover data gathering and analysis-data **(data
    that is generated by merging or combining multiple data sources to get one dataset
    for the analysis)** generation with KPI visualization, and then in the subsequent
    exercise, we will cover what feature importance is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 43: Identify the Target Variable and Related KPIs from the Given Data
    for the Business Problem'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s take the example of a **subscription problem** in the banking sector.
    We will use data that is from direct marketing campaigns by a Portuguese banking
    institution, where a customer either opens a term deposit (ref: [https://www.canstar.com.au/term-deposits/what-is-a-term-deposit/](https://www.canstar.com.au/term-deposits/what-is-a-term-deposit/))
    or not after the campaign. The subscription problem is characterized or defined
    contrastingly by every organization. For the most part, the customers who will
    subscribe to a service (here, it is a term deposit) have higher conversion potential
    (that is, from lead to customer conversion) to a service or product. Thus, in
    this problem, subscription metrics, that is, the outcome of historical data, is
    considered as the target variable or KPI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use descriptive analytics to explore trends in the data. We will start
    by identifying and defining the target variable (here, subscribed or not subscribed)
    and the related KPIs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the bank.csv data from the following online resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Bank+Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/machine-learning-databases/00222/](https://archive.ics.uci.edu/ml/machine-learning-databases/00222/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a folder for the exercise (`packt_exercises`) and save the downloaded
    data there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start Jupyter notebook and import all the required libraries as illustrated.
    Now set the working directory using the `os.chdir()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following code to read the CSV and explore the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After executing the previous command, you will get output similar to the following:![](img/C12913_06_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.1: Bank DataFrame'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: When studying the target variable (subscribed or not subscribed—`y`), it is
    important to look at the distribution of it. The type of target variable in this
    dataset is categorical, or of multiple classes. In this case, it's binary (Yes/No).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When the distribution is skewed to one class, the problem is known as an *imbalance
    in the variable*. We can study the proportion of the target variable using a bar
    plot. This gives us an idea about how many of each class there is (in this case,
    how many each of no and yes). The proportion of no is way higher than yes, which
    explains the imbalance in the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s execute the following commands to plot a bar plot for the given data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.2: Bar plot](img/Image38462.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.2: Bar plot'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, we will take each variable and look at their distribution trends. The
    following histogram, which is provided as an example, is of the ''`age''` column
    (attribute) in the dataset. Histograms/density plots are a great way to explore
    numerical/float variables, similar to bar plots. They can be used for categorical
    data variables. Here, we will show two numerical variables, `age` and `balance`,
    as examples using a histogram, and two categorical variables, `education` and
    `month`, using bar plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The histogram is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.3: Histogram for age](img/C12913_06_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.3: Histogram for age'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To plot the histogram for the `balance` attribute in the dataset, use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The histogram for balance is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.4: Histogram for balance](img/C12913_06_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.4: Histogram for balance'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, using the following code, plot a bar plot for the `education` attribute
    in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The bar plot for the `education` attribute is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.5: Bar plot for education](img/C12913_06_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.5: Bar plot for education'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Use the following command to plot a bar plot for the `month` attribute of the
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plotted graph is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.6: Bar plot for month](img/C12913_06_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.6: Bar plot for month'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The next task is to generate a distribution for each class of the target variable
    to compare the distributions. Plot a histogram of the `age` attribute for the
    target variable (`yes`/`no`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The bar plot of the `month` attribute target variable is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.7: Bar plot of the month attribute for the target variable](img/C12913_06_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.7: Bar plot of the month attribute for the target variable'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, using the following command, plot a bar plot for the target variable grouped
    by month:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plotted graph is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.8: Histogram grouped by month](img/C12913_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Histogram grouped by month'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this exercise, we looked into establishing KPIs and the target variable—**data
    gathering** and **analysis-data (data that is generated by merging or combining
    multiple data sources to get one dataset for analysis) generation**. The KPIs
    and target variable have been determined—**KPI visualization**. Now, in the next
    exercise, we will identify which of the variables are important in terms of explaining
    the variance of the target variable—**feature importance**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 44: Generate the Feature Importance of the Target Variable and Carry
    Out EDA'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous exercise, we looked into the trends of the attributes, identifying
    their distribution, and how we can use various plots and visualization methods
    to carry these out. Prior to tackling a modeling problem, whether it is a predictive
    or a classification problem (for example, from the previous marketing campaign
    data, how to predict future customers that will have the highest probability of
    converting), we have to pre-process the data and select the important features
    that will impact the subscription campaign output models. To do this, we have
    to see the association of the attributes to the outcome (the target variable),
    that is, how much variability of the target variable is explained by each variable.
  prefs: []
  type: TYPE_NORMAL
- en: Associations between variables can be drawn using multiple methods; however,
    we have to consider the data type when choosing a method/algorithm. For example,
    if we are studying numerical variables (integers that are ordered, floats, and
    so on), we can use correlation analysis; if we are studying categorical variables
    with multiple classes, we can use Chi-Square methods. However, there are many
    algorithms that can handle both together and provide measurable outcomes to compare
    the importance of variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will look at how various methods can be used to identify
    the importance of features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the `bank.csv` file and read the data using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Develop a correlation matrix using the following command to identify the correlation
    between the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plotted correlation matrix heatmap is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.9: Correlation matrix](img/C12913_06_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.9: Correlation matrix'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '`-1` to `+1`, where a value close to `0` signifies no relation, -1 signifies
    a relation where one variable reduces as the other increases (inverse), and +1
    signifies that as one variable increases the other also increases (direct).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: High correlation among independent variables (which are all variables except
    the target variable) can lead to multicollinearity among the variables, which
    can affect the predictive model accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Make sure to install boruta if not installed already using the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`pip install boruta --upgrade`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build a feature importance output based on Boruta (a wrapper algorithm around
    a random forest):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.10: Fit Boruta algorithm](img/C12913_06_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.10: Fit Boruta algorithm'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Check the rank of features as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.11: Boruta output](img/C12913_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Boruta output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Structured Approach to the Data Science Project Life Cycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embarking on data science projects needs a robust methodology in planning the
    project, taking into consideration the potential scaling, maintenance, and team
    structure. We have learned how to define a business problem and quantify it with
    measurable parameters, so the next stage is a project plan that includes the development
    of the solution, to the deployment of a consumable business application.
  prefs: []
  type: TYPE_NORMAL
- en: This topic puts together some of the best industry practices structurally with
    examples for data science project life cycle management. This approach is an idealized
    sequence of stages; however, in real applications, the order can change according
    to the type of solution that is required.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a data science project for a single model deployment takes around
    three months, but this can increase to six months, or even up to a year. Defining
    a process from data to deployment is the key to reducing the time to deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Data Science Project Life Cycle Phases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The stages of a data science project life cycle are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and defining the business problem
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data access and discovery
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data engineering and pre-processing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modeling development and evaluation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Phase 1: Understanding and Defining the Business Problem'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every data science project starts with learning the business domain and framing
    the business problem. In most organizations, the application of advanced analytics
    and data science techniques is a fledgling discipline, and most of the data scientists
    involved will have a limited understanding of the business domains.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the business problem and the domain, the key stakeholders and
    **subject matter experts** (**SMEs**) need to be identified. Then, the SMEs and
    the data scientists interact with each other to develop the initial hypothesis
    and identify the data sources required for the solution's development. This is
    the first phase of understanding a data science project.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a well-structured business problem and the required data is identified,
    along with the data sources, the next phase of data discovery can be initiated.
    The first phase is crucial in building a strong base for scoping and the solution
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Phase 2: Data Access and Discovery'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This phase includes identifying the data sources and building data pipelines
    and data workflows in order to acquire the data. The nature of the solution and
    the dependent data can vary from one problem to another in terms of the structure,
    velocity, and volume.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, it's important to identify how to acquire the data from the data
    sources. This can be through direct connectors (libraries available on Python)
    using database access credentials, having APIs that provide access to the data,
    directly crawling data from web sources, or can even be data dumps provided in
    CSVs for initial prototype developments. Once a strong data pipeline and workflow
    for acquiring data is established, the data scientists can start exploring the
    data to prepare the analysis-data **(data that is generated by merging or combining
    multiple data sources to get one dataset for analysis).**
  prefs: []
  type: TYPE_NORMAL
- en: 'Phase 3: Data Engineering and Pre-processing'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pre-processing of data means transforming raw data into a form that's consumable
    by a machine learning algorithm. It is essentially the manipulation of data into
    a structure that's suitable for further analysis or getting the data into a form
    that can be input data for modeling purposes. Often, the data required for analysis
    can reside in several tables, databases, or even external data sources.
  prefs: []
  type: TYPE_NORMAL
- en: A data scientist needs to identify the required attributes from these data sources
    and merge the available data tables to get what is required for the analytics
    model. This is a tedious and time-consuming phase on which data scientists spend
    a considerable part of the development cycle.
  prefs: []
  type: TYPE_NORMAL
- en: The pre-processing of data includes activities such as outlier treatment, missing
    value imputation, scaling features, mapping the data to a Gaussian (or normal)
    distribution, encoding categorical data, and discretization, among others.
  prefs: []
  type: TYPE_NORMAL
- en: To develop robust machine learning models, it's important that data is pre-processed
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Python has a few libraries that are used for data pre-processing. scikit-learn
    has many efficient built-in methods for pre-processing data. The scikit-learn
    documentation can be found at [https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html).
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through the following activity to understand how to carry out data
    engineering and pre-processing using one of the pre-processing techniques, such
    as Gaussian normalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 13: Carry Out Mapping to Gaussian Distribution of Numeric Features
    from the Given Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Various pre-processing techniques need to be carried out to prepare the data
    before pushing it into an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Explore this website to find various methods for pre-processing: [https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will carry out data normalization, which is important
    for parametric models such as linear regression, logistic regression, and many
    others:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `bank.csv` file and import all the required packages and libraries into
    the Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, determine the numeric data from the DataFrame. Categorize the data according
    to its type, such as categorical, numeric (float or integer), date, and so on.
    Let's carry out normalization on numeric data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Carry out a normality test and identify the features that have a non-normal
    distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the probability density of the features to visually analyze the distribution
    of the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the power transformation model and carry out transformations on the
    identified features to convert them to normal distribution based on the `box-cox`
    or `yeo-johnson` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the transformations on new data (evaluation data) with the same generated
    parameters for the features that were identified in the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Multiple variables' density plots are shown in the previous plot after the transformations
    are carried out. The distribution of the features in the plots is closer to a
    Gaussian distribution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 236.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the transformations are carried out, we analyze the normality of the features
    again to see the effect. You can see that after the transformations, some of the
    features can have the null hypothesis not rejected (that is, the distribution
    is still not Gaussian) with almost all features higher `p` values (refer to point
    2 of this activity). Once the transformed data is generated, we bind it back to
    the numeric data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Phase 4: Model Development'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have a cleaned and pre-processed data, it can be ingested into a machine
    learning algorithm for exploratory or predictive analysis, or for other applications.
    Although the approach to a problem may be designed, whether it's a classification,
    association, or a regression problem, for example, the specific algorithm that
    needs to considered for the data has to be identified. For example, if it's a
    classification problem, it could be a decision tree, a support vector machine,
    or a neural network with many layers.
  prefs: []
  type: TYPE_NORMAL
- en: For modeling purposes, the data needs to be separated into testing and training
    data. The model is developed on the training data and its performance (accuracy/error)
    is evaluated on the testing data. With the algorithms selected, the related parameters
    with respect to it need to be tuned by the data scientist to develop a robust
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we learned how to define a business problem from a data science
    perspective through a well-defined, structured approach. We started by understanding
    how to approach a business problem, how to gather the requirements from stakeholders
    and business experts, and how to define the business problem by developing an
    initial hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Once the business problem was defined with data pipelines and workflows, we
    looked into understanding how to start the analysis on the gathered data in order
    to generate the KPIs and carry out descriptive analytics to identify the key trends
    and patterns in the historical data through various visualization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how a data science project life cycle is structured, from defining
    the business problem to various pre-processing techniques and model development.
    In the next chapter, we will be learning how to implement the concept of high
    reproducibility on a Jupyter notebook, and its importance in development.
  prefs: []
  type: TYPE_NORMAL
