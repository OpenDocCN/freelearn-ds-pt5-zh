- en: Distributed Computing, Parallel Computing, and HPCC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since our society has entered a data-intensive era (that is, a big data era),
    we face larger and larger datasets. For this reason, companies and users are considering
    what kinds of tools they could use to speed up the process when dealing with data.
    One obvious solution is to increase their data storage capacity. Unfortunately,
    there is a huge cost associated with this. The other solutions include distributed
    computing and some ways to accelerate our process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to distributed versus parallel computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding MPI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel processing in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda add-ons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to HPCC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to distributed versus parallel computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed computing is a subfield of computer science that studies distributed
    systems and models in which components located on networked computers communicate
    and coordinate their actions by passing messages. The components interact with
    each other in order to achieve a common goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worthwhile to discuss another phrase: parallel computing. Parallel computing
    is more tightly coupled to multi-threading, or how to make full use of a single
    CPU, while distributed computing refers to the notion of divide and conquer, executing
    subtasks on different machines, and then merging the results.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we have entered a so-called big data era, it seems that the distinction
    is melting. In fact, nowadays, many systems use a combination of parallel and
    distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: Task view for parallel processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For R, there is a task view called **High-Performance and Parallel Computing
    with R**. Recall that a task view is a set of R programs around a specific topic.
    To find the task view for **High-Performance and parallel computing using R**,
    we go to [http://r-project.org](http://r-project.org), click on CRAN on the left-hand
    side, choose a nearby mirror location, and click on Packages and Task Views.
  prefs: []
  type: TYPE_NORMAL
- en: 'After double-clicking on Task Views, we can see the following list—to save
    space, only the top task views are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1975c7c-6005-4f4b-b03c-1d708bbc33b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After double-clicking on the related task view (**HighPerformanceComputing**),
    we can see the following screenshot—to save space, only the top few lines are
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/d90b13fd-6d24-43a1-97a9-2d76077101a1.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding task view contains a list of packages, grouped by various topics
    that could be useful for **high-performance computing** (**HPC**) with R. In this
    context, the task view maintainers define high-performance computing quite loosely
    as just about anything related to pushing R a little further: using compiled code,
    parallel computing (in both explicit and implicit modes), working with large objects
    as well as profiling. The aforementioned task view can be downloaded at [https://CRAN.R-project.org/view=HighPerformanceComputing](https://CRAN.R-project.org/view=HighPerformanceComputing).
    As we have discussed in previous chapters, we could use the following command
    to install all related R packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: On May 14, 2018, by running the previous code, 217 related R packages were downloaded
    and installed.
  prefs: []
  type: TYPE_NORMAL
- en: Sample programs in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For Python programs in parallel computing, we can visit the *IPython in-depth
    Tutorial* at [https://github.com/ipython/ipython-in-depth](https://github.com/ipython/ipython-in-depth).
    After downloading the ZIP file and unzipping it, we can find programs related
    to parallel computing (refer to the following two screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73fc867a-93cb-481c-b9d0-39c60d129af7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding screenshot shows a few subdirectories, while the following one
    shows 19 programs including both Jupyter Notebook and Python programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c367b33-a0f3-4d3a-b1e4-570d71e336b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding MPI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Usually, a parallel algorithm needs to move data between different engines.
    One way to do so is by doing a pull and then a push using the direct view. However,
    this method is quite slow since all the data has to go through the controller
    to the client and then back through the controller, to its final destination.
    A much better way of moving data between engines is to use a message passing library,
    such as the **Message Passing Interface** (**MPI**). IPython's parallel computing
    architecture has been designed to integrate with MPI. To download and install
    Windows MPI, readers can refer to [https://msdn.microsoft.com/en-us/library/bb524831%28v=vs.85%29.aspx.](https://msdn.microsoft.com/en-us/library/bb524831%28v=vs.85%29.aspx)
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you could install the `mpi4py` package.
  prefs: []
  type: TYPE_NORMAL
- en: R package Rmpi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To find a demo associated with the `Rmpi` package, we could issue the following
    two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After hitting *Enter*, we will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a82682f-9ddf-4a78-b981-d30b6cd9321a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the first function, called `cslavePI`, we know that we should copy `cslavePI.c`
    in Rmpi library directory to your working directory and compile it as `mpicc -o
    cslavePI cslavePI.c`. To find the path of Rmpi, we issue the `find.package()`
    function (refer to the code and the result in the following code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that different readers will get quite different paths. We can use `demo(cslavePI)`
    to find out its function. Refer to the code and the related output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/682f484b-36c5-45e7-b627-2d1c26fa579f.png)'
  prefs: []
  type: TYPE_IMG
- en: The mpicc software is used to compile and link MPI programs written in C.
  prefs: []
  type: TYPE_NORMAL
- en: R package plyr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The objective of the `plyr` R package is about the split-apply-combine paradigm
    for R. This is quite common in data analysis: we solve a complex problem by breaking
    it down into small pieces, doing something to each piece, and then combining the
    results back together again. The following is an example borrowed from its menu
    with a minor modification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To save space, only the graph is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a600249-cef2-49df-9ae0-b0c3f73ea2d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `arrange()` function orders a data frame by its columns (refer to the following
    code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: R package parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s look at the simple usage of an R function called `lapply()` (refer
    to the following code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The meaning is clear: we have an input size of `1`, `2`, and `3` and we assign
    them to three functions. The following example is a slightly more complex one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The first couple of lines are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2d63458-d0d0-4d95-871a-572d99c2fc8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following example is borrowed from *Gordon* (2015):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the `makeCluster()` function will set up the cluster.
    The `parLapply()` function calls the parallel version of `lapply()` or `parLapply()`
    functions. The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For the following code, we will see an error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The error message is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: To correct it, the base variable will be added (refer to the following code).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To save space, the output will not be shown here. The following is another
    example to see the difference between calling the `lapply()` and `mclapply()`
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the `lappy()` and `mclappy()` functions are used. The
    `mclapply()` function is a parallelized version of the `lapply()` function. It
    returns a list of the same length as `X`, each element of which is the result
    of applying `FUN` to the corresponding element of `X`. The following program is
    borrowed from [http://www.smart-stats.org/wiki/parallel-computing-cluster-using-r](http://www.smart-stats.org/wiki/parallel-computing-cluster-using-r)
    with minor modifications. Note that the program is run on a UNIX instead of a
    PC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The related output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: R package snow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This package is for **Simple Network of Workstations** (**SNOW**). Let''s take
    a look at a program associated with destruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `makeSOCKcluster()` function is used to start and stop a `snow` cluster
    and to set default cluster options. The `clusterApply()` calls the function on
    the first cluster node with arguments `seq[[1]] and ...`, on the second node with
    `seq[[2]] and ...`, and so on. If the length of `seq` is greater than the number
    of nodes in the cluster, then cluster nodes are recycled. A list of the results
    is returned; the length of the result list will be equal to the length of `seq`.
    The `clusterCall()` calls a function with identical arguments on each node in
    the cluster `cl` and returns a list of the results. The `clusterEvalQ()` function
    evaluates a literal expression on each cluster node. It's a cluster version of
    `evalq`, and is a convenience function defined in terms of `clusterCall()`.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following example is about computing π digits and is borrowed from the website [http://ipyparallel.readthedocs.io/en/latest/demos.html#parallel-examples](http://ipyparallel.readthedocs.io/en/latest/demos.html#parallel-examples).
    Since the first part needs a program called `one_digit_freqs()` function, we could
    run a Python program called `pidigits.py` contained at `.../ipython-ipython-in-depth-4d98937\examples\Parallel
    Computing\pi`, and this path depends on where the reader downloaded and saved
    his/her files.
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete our part, we simply include it in the first part of the program,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The related graph is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b303d68c-8b04-4977-85cf-315fd84d7a40.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallel processing for word frequency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s look at a simple Python program to find out the first most frequently
    used word in an input text file. We randomly chose Da Vinci Code at [http://www.gutenberg.org/files/5000/5000-8.txt](http://www.gutenberg.org/files/5000/5000-8.txt).
    Assume that the downloaded novel is saved under `c:/temp/daVinci.txt`. The following
    Python code will list the top 10 most frequent words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The related output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parallel Monte-Carlo options pricing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This example is from the sample programs in Python discussed earlier. The notebook
    shows how to use the `ipyparallel` package to do Monte-Carlo options pricing in
    parallel. The notebook computes the price of a large number of options for different
    strike prices and volatilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To save space, only the first several lines of code are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To run it, click on IPython Clusters. After clicking IPython Clusters on the
    right-hand side, we will see the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6bb005d-ba65-4693-8d5e-3f64735b711b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can click on Start for the default. The number of engines will show a value
    of `4`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc7c234c-79b1-4f87-a690-0d9993eadedf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we go back to our uploaded Jupyter Notebook for the parallel monte-carlo
    options pricing. In total, there are four output images. To save space, only the
    first one is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14427f22-3abb-43e0-802b-841df9ec89c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the payoff function for an Asian put using stock average price as
    the final terminal price is given here, where *Put(Asian)* is the Asian put option,
    *K* is the exercise price, and ![](img/0c79a345-2dd8-47c8-a4a3-7529f93e9281.png) is
    the average price over the path:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1554d15-5e87-4001-945f-93db9761280c.png)'
  prefs: []
  type: TYPE_IMG
- en: Compute nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A compute node provides the ephemeral storage, networking, memory, and processing
    resources that can be consumed by virtual machine instances. The cloud system
    supports two types of compute nodes: **ESX clusters**, where clusters are created
    in VMware vCenter Server, and **KVM compute nodes**, where KVM compute nodes are
    created manually. In the previous chapter, we mentioned the concept of the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Within a cloud environment, which is quite useful for a more complex project,
    compute nodes form the core of resources. Typically, these notes supply the processing,
    memory, network, and storage that virtual machine instances need. When an instance
    is created, it is matched to a compute node with the available resources. A compute
    node can host multiple instances until all of its resources are consumed.
  prefs: []
  type: TYPE_NORMAL
- en: Anaconda add-on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following information is from the Anaconda Addon Development Guide.
  prefs: []
  type: TYPE_NORMAL
- en: An Anaconda add-on is a Python package containing a directory with an `__init__.py`
    file and other source directories (sub packages) inside. Because Python allows
    importing each package name only once, the package top-level directory name must
    be unique. At the same time, the name can be arbitrary, because add-ons are loaded
    regardless of their name; the only requirement is that they must be placed in
    a specific directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The suggested naming convention for add-ons is therefore similar to that of
    Java packages or D-Bus service names: prefix the add-on name with the reversed
    domain name of your organization, using underscores (_) instead of dots so that
    the directory name is a valid identifier for a Python package. An example add-on
    name following these suggestions would therefore be, for example, `org_fedora_hello_world`.
    This convention follows the recommended naming scheme for Python package and module
    names. Interested readers can find lots of information about Anaconda add-ons
    at [https://rhinstaller.github.io/anaconda-addon-development-guide/index.html#sect-anaconda-introduction-addons](https://rhinstaller.github.io/anaconda-addon-development-guide/index.html#sect-anaconda-introduction-addons).'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to HPCC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HPCC stands for **High-Performance Computing Cluster**. It is also known as
    **Data Analytics Supercomputer** (**DAS**), an open source, data-intensive computing
    system platform developed by LexisNexis Risk Solutions. The HPCC platform incorporates
    a software architecture implemented on computing clusters to provide high-performance,
    data-parallel processing design for various applications using big data. The HPCC
    platform includes system configurations to support both parallel batch data processing
    (Thor) and high-performance online query applications using indexed data files
    (Roxie). The HPCC platform also includes a data-centric declarative programming
    language for parallel data processing called ECL.
  prefs: []
  type: TYPE_NORMAL
- en: You can see a simple example of using Wharton's HPCC system at [https://research-it.wharton.upenn.edu/documentation/](https://research-it.wharton.upenn.edu/documentation/).
    Wharton's HPC Cluster (HPCC) provides access to advanced computational research
    hardware and software for Wharton faculty, faculty collaborators and research
    assistants, and Wharton doctoral candidates. It is designed for simple and parallel
    processing across a large set of tightly integrated hardware with dedicated networking
    and storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about the hardware, refer to the Hardware page. HPCC users
    have access to a number of scientific, mathematics, and analytic software, including
    MATLAB, Mathematica, R, Stata, and SAS. MySQL server access can be provided as
    well. The HPCC also has Fortran, C, and C++ compilers in GNU and Intel versions.
    The following is a simple procedure to link to their HPCC platform:'
  prefs: []
  type: TYPE_NORMAL
- en: First, download and install FortClient and MobaXterm software
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Connect to the Wharton VPN via FortClient (as shown in the following screenshot):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/43d91c0d-f130-4be2-8c89-87cdbca7bb73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After connecting, the following screen will appear:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5d7368e8-3d4f-4aa4-b302-7c36789c8295.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Use MobaXterm software to connect to Wharton''s HPCC platform. Here, we assume
    that you have an account with Wharton (refer to the following screenshot):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/076af6c0-16d0-4c35-b086-c4f286b5839f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can connect, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/60abcd45-55a4-4969-8956-1104013ad39c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the users will write their own programs to take advantage of HPCC to speed
    up their research and experiments. For more information, refer to the documentation
    at [https://research-it.wharton.upenn.edu/documentation/](https://research-it.wharton.upenn.edu/documentation/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed several R packages such as `plyr`, `snow`,
    `Rmpi`, and `parallel`, and the Python package `ipyparallel`. In addition, we
    mentioned compute nodes, project add-ons, parallel processing, and HPCC.
  prefs: []
  type: TYPE_NORMAL
- en: Now we've arrived at the end of our journey. We wish you good luck for the amazing
    endeavors you'll be taking up with the knowledge you've got from this book.
  prefs: []
  type: TYPE_NORMAL
- en: Review questions and exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is distributed computing? Why it is useful?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From where could we get a task view for parallel computing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the task view related to parallel computing, we can find many R packages.
    Identify a few of them. Install two and find a few examples of using these two
    packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Conduct a word frequency analysis using: The Count of Monte Cristo by Alexandre
    Dumas (input file is at [http://www.gutenberg.org/files/1184/1184-0.txt](http://www.gutenberg.org/files/1184/1184-0.txt)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From where could we find more information about Anaconda add-ons?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is HPCC and how does it work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we find the path of an installed R package?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the sample Jupyter notebook about parallel Monte-Carlo options pricing,
    the related Asian options are defined here, where `call(Asian)` is the Asian put
    option, `Put(Asian)`, *K* is the exercise price, and ![](img/67daf5c0-5b94-4153-8c39-3bd8ce0c65c0.png) is
    the average price over the path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3f973221-3119-4640-a6f2-e2e066b0c4f8.png)![](img/e4300d58-1141-4032-8b57-f16d53141e02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Write a Jupyter notebook to use the following definitions of Asian options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e51ae73-ccd6-44c8-b4be-02834221af98.png)![](img/32cafa72-593a-4f75-833d-89c737bd1aa0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this chapter, we mentioned that the following three lines could be used
    to download all R packages related to high-performance computing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Try this and report how many R packages are downloaded.
  prefs: []
  type: TYPE_NORMAL
- en: Find more examples associated with the R package called `Rmpi`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the sample Jupyter notebook called `Using MPI` with `IPython Parallel.ipynb`.
    Explain its meaning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The R `doRNG` package provides functions to perform reproducible parallel `foreach`
    loops, using independent random streams as generated by the package `rstream`,
    suitable for the different `foreach` back ends. Download and install this and
    other related packages. Show a few examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
