["```py\n> path<-\"http://canisius.edu/~yany/RData/\" \n> dataSet<-\"titanic\" \n> link<-paste(path,dataSet,\".RData\",sep='') \n> con<-url(link) \n> load(con)  \n> dim(.titanic) \n[1] 2201    4 \n> head(.titanic) \n CLASS   AGE GENDER SURVIVED \n1 First Adult   Male      Yes \n2 First Adult   Male      Yes \n3 First Adult   Male      Yes \n4 First Adult   Male      Yes \n5 First Adult   Male      Yes \n6 First Adult   Male      Yes \n> summary(.titanic) \n CLASS        AGE          GENDER     SURVIVED   \n Crew  :885   Adult:2092   Female: 470   No :1490   \n First :325   Child: 109   Male  :1731   Yes: 711   \n Second:285                                         \n Third :706   \n```", "```py\n> unique(.titanic$CLASS) \n[1] First  Second Third  Crew   \nLevels: Crew First Second Third\n> unique(.titanic$GENDER) \n[1] Male   Female \nLevels: Female Male \n> unique(.titanic$SURVIVED) \n[1] Yes No  \nLevels: No Yes\n```", "```py\n>library(rattle) \n>rattle() \n```", "```py\nlibrary(rpart, quietly=TRUE) \ncon<-url(\"http://canisius.edu/~yany/RData/titanic.RData\") \nload(con) \nx<-.titanic \nscoring  <- FALSE \nset.seed(42) \nrisk<-ident<-ignore<-weights<-numeric<-NULL \nstr(dataset) \nn<- nrow(dataset) \ntrain  <- sample <- sample(n,0.7*n) \nvalidate<- sample(setdiff(seq_len(n),train),0.15*n) \ntest<- setdiff(setdiff(seq_len(n), train), validate) \ninputVars<-categoric<-c(\"CLASS\",\"AGE\",\"GENDER\") \ntarget<-\"SURVIVED\"\noutput<-rpart(SURVIVED~.,data=x[train, c(inputVars, target)], \n   method=\"class\",parms=list(split=\"information\"),control= \n   rpart.control(usesurrogate=0,maxsurrogate=0)) \nfancyRpartPlot(output, main=\"Decision Tree for Titanic\") \n```", "```py\nimport sklearn as sk \nfrom sklearn import datasets \niris = datasets.load_iris() \nprint(\"data:n\",iris.data[0:4,]) \nprint(\"target\",iris.target[0:2,]) \nmylist=list(iris.target) \nused = [] \n[x for x in mylist if x not in used and used.append(x)] \nprint(\"unique values for targetsn\",used) \n```", "```py\ndata: \n [[ 5.1  3.5  1.4  0.2] \n [ 4.9  3\\.   1.4  0.2] \n [ 4.7  3.2  1.3  0.2] \n [ 4.6  3.1  1.5  0.2]] \ntarget [0 0] \nunique values for targets \n [0, 1, 2] \n```", "```py\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier as KNC\niris = datasets.load_iris()\nx= iris.data\ny= iris.target\nnp.unique(y)\nnp.random.seed(123)\nindices = np.random.permutation(len(x))\niris_x_train = x[indices[:-10]]\niris_y_train = y[indices[:-10]]\niris_x_test = x[indices[-10:]]\niris_y_test = y[indices[-10:]]\nknn = KNC()\nknn.fit(iris_x_train, iris_y_train)\nKNC(algorithm='auto',leaf_size=30, metric='minkowski',\nmetric_params=None,n_jobs=1,n_neighbors=5, p=2,weights='uniform')\nknn.predict(iris_x_test)\nout=knn.predict(iris_x_test)\nprint(\"predicted:\",out)\nprint(\"True :\",iris_y_test)\n```", "```py\npredicted: [1 1 2 1 2 0 1 1 2 2] \nTrue     : [1 1 2 2 1 0 1 1 2 2] \n```", "```py\nlibrary(ggvis) \nx<-ggvis \ny<-layer_points \niris %>% x(~Petal.Length,~Petal.Width,fill=~Species) %>% y() \n```", "```py\nlibrary(ElemStatLearn) \nrequire(class) \nx <- mixture.example$x \ny <- mixture.example$y \nxnew <- mixture.example$xnew \npx1 <- mixture.example$px1 \npx2 <- mixture.example$px2 \n# \ncolor1<-\"blue\" \ncolor2<-\"pink3\" \nkNearest<-5   \nmodel<- knn(x, xnew,y,k=kNearest,prob=TRUE) \ntitle<-paste(kNearest,\"-nearest neighbour\") \nprob <- attr(model,\"prob\") \nprob <- ifelse(model==\"1\",prob,1-prob) \nprob15 <- matrix(prob,length(px1),length(px2)) \npar(mar=rep(2,4)) \ncontour(px1,px2,prob15,levels=0.5,main=title,axes=FALSE) \npoints(x, col=ifelse(g==1,color1,color2)) \ngd <- expand.grid(x=px1, y=px2) \npoints(gd,pch=\".\",cex=1.5,col=ifelse(prob15>0.5,color1,color2)) \nbox() \n```", "```py\n> library(mlbench)\n> data(HouseVotes84)\n> head(HouseVotes84)\n```", "```py\nlibrary(e1071) \ndata(HouseVotes84, package = \"mlbench\") \nmodel<-naiveBayes(Class ~ ., data = HouseVotes84) \n# \npredict(model, HouseVotes84[1:10,]) \npredict(model, HouseVotes84[1:10,], type = \"raw\") \npred <- predict(model, HouseVotes84) \ntable(pred, HouseVotes84$Class) \n```", "```py\npred         democrat   republican \ndemocrat        238       13 \nrepublican      29        155 \n```", "```py\n> library(e1071) \n> data(Titanic) \n> m <- naiveBayes(Survived ~ ., data = Titanic) \n> output<-predict(m, as.data.frame(Titanic)) \n```", "```py\n> print(m) \nNaive Bayes Classifier for Discrete Predictors \nCall: \nnaiveBayes.formula(formula = Survived ~ ., data = Titanic) \nA-priori probabilities: \nSurvived \n      No      Yes  \n0.676965 0.323035  \nConditional probabilities: \n        Class \nSurvived        1st        2nd        3rd       Crew \n     No  0.08187919 0.11208054 0.35436242 0.45167785 \n     Yes 0.28551336 0.16596343 0.25035162 0.29817159 \n        Sex \nSurvived       Male     Female \n     No  0.91543624 0.08456376 \n     Yes 0.51617440 0.48382560 \n        Age \nSurvived      Child      Adult \n     No  0.03489933 0.96510067 \n     Yes 0.08016878 0.91983122 \n> print(output) \n[1] Yes No  No  No  Yes Yes Yes Yes No  No  No  No  Yes Yes \n[15] Yes Yes Yes No  No  No  Yes Yes Yes Yes No  No  No  No  \n[29] Yes Yes Yes Yes \nLevels: No Yes \n```", "```py\n> library(\"ReinforcementLearning\") \n> set.seed(123) \n> data <- sampleGridSequence(1000) \n> dim(data) \n[1] 1000    4 \n> head(data) \n  State Action Reward NextState \n1    s2   left     -1        s2 \n2    s4  right     -1        s4 \n3    s2   down     -1        s2 \n4    s4     up     -1        s4 \n5    s4     up     -1        s4 \n6    s1   left     -1        s1 \n> unique(data$State) \n[1] \"s2\" \"s4\" \"s1\" \"s3\" \n> unique(data$Action) \n[1] \"left\"  \"right\" \"down\"  \"up\"    \n> unique(data$NextState) \n[1] \"s2\" \"s4\" \"s1\" \"s3\" \n> unique(data$Reward) \n[1] -1 10  \n```", "```py\n> x<-subset(data,data$State==data$NextState) \n> head(x) \n State Action Reward NextState \n1    s2   left     -1        s2 \n2    s4  right     -1        s4 \n3    s2   down     -1        s2 \n4    s4     up     -1        s4 \n5    s4     up     -1        s4 \n6    s1   left     -1        s1 \n> unique(x$Reward) \n[1] -1  \n```", "```py\nlibrary(ReinforcementLearning) \nset.seed(123) \ndata <- sampleGridSequence(1000) \ncontrol <- list(alpha = 0.1, gamma = 0.1, epsilon = 0.1) \nmodel <- ReinforcementLearning(data,s=\"State\",a=\"Action\",r=\"Reward\",s_new=\"NextState\",control=control) \nprint(model) \n```", "```py\n> print(model) \nState-Action function Q \n         right        up        down      left \ns1 -1.09768561 -1.098833 -1.00284548 -1.098910 \ns2 -0.03076799 -1.099211 -1.00501562 -1.005837 \ns3 -0.02826295  9.808764 -0.02869875 -1.003904 \ns4 -1.10177002 -1.106688 -1.10776585 -1.106276 \n\nPolicy \n     s1      s2      s3      s4  \n \"down\" \"right\"    \"up\" \"right\"  \nReward (last iteration) \n[1] -505 \n```", "```py\n> library(rattle) \n> rattle() \n```", "```py\nlibrary(LogicReg) \ndata(logreg.testdat) \ny<-logreg.testdat[,1] \nx<-logreg.testdat[, 2:21] \nn=1000 \nn2=25000 \nset.seed(123) \nmyanneal<-logreg.anneal.control(start=-1,end=-4,iter=n2,update=n) \noutput<-logreg(resp=y,bin=x,type=2,select = 1,ntrees=2,anneal.control=myanneal) \nplot(output) \n```", "```py\n>library(RTextTools)  \n>data(NYTimes) \n>set.seed(123) # guarantees the same result  \n>data <- NYTimes[sample(1:3100,size=100,replace=FALSE),] \n>head(data) \n```", "```py\n>library(RTextTools) \n>print_algorithms() \n[1] \"BAGGING\"  \"BOOSTING\" \"GLMNET\"   \"MAXENT\"   \"NNET\"     \n[6] \"RF\"       \"SLDA\"     \"SVM\"      \"TREE\"     \n```", "```py\nlibrary(RTextTools) \ndata(NYTimes) \ndata <- NYTimes[sample(1:3100,size=100,replace=FALSE),] \nmatrix <- create_matrix(cbind(data[\"Title\"],data[\"Subject\"]), language=\"english\", \nremoveNumbers=TRUE, stemWords=FALSE, weighting=tm::weightTfIdf) \ncontainer <- create_container(matrix,data$Topic.Code,trainSize=1:75, testSize=76:100, \nvirgin=TRUE) \nmodels <- train_models(container, algorithms=c(\"MAXENT\",\"SVM\")) \nresults <- classify_models(container, models) \nanalytics <- create_analytics(container, results) \nsummary(analytics) \n```", "```py\nimport pandas as pd \nx=pd.read_pickle(\"c:/temp/ffcMonthly.pkl\") \nprint(x.head()) \nprint(x.tail()) \n```", "```py\nimport  scipy as sp \nimport pandas as pd \nimport quandl as qd \nimport statsmodels.api as sm \n#quandl.ApiConfig.api_key = 'YOUR_API_KEY' \na=qd.get(\"WIKI/IBM\")  \np=a['Adj. Close'] \nn=len(p) \nret=[] \n# \nfor i in range(n-1): \n    ret.append(p[i+1]/p[i]-1) \n# \nc=pd.DataFrame(ret,a.index[1:n],columns=['RET']) \nff=pd.read_pickle('c:/temp/ffDaily.pkl') \nfinal=pd.merge(c,ff,left_index=True,right_index=True) \ny=final['RET'] \nx=final[['MKT_RF','SMB','HML']] \n#x=final[['MKT_RF']] \nx=sm.add_constant(x) \nresults=sm.OLS(y,x).fit() \nprint(results.summary()) \n```", "```py\nfrom sklearn import metrics \nfrom sklearn import datasets \nfrom sklearn.tree import DecisionTreeClassifier \nx=datasets.load_iris() \nmodel=DecisionTreeClassifier() \nmodel.fit(x.data, x.target) \nprint(model) \ntrue=x.target \npredicted=model.predict(x.data) \nprint(\"------ output below --------- n\") \nprint(metrics.classification_report(true, predicted)) \nprint(metrics.confusion_matrix(true, predicted)) \n```", "```py\nimport matplotlib.pyplot as plt \nfrom sklearn import datasets, svm, metrics \nfrom sklearn.metrics import classification_report as report \n# \nformat1=\"Classification report for classifier %s:n%sn\" \nformat2=\"Confusion matrix:n%s\" \ndigits = datasets.load_digits() \nimageLabels = list(zip(digits.images, digits.target)) \nfor index,(image,label) in enumerate(imageLabels[:4]): \n    plt.subplot(2, 4, index + 1) \n    plt.axis('off') \n    plt.imshow(image,cmap=plt.cm.gray_r,interpolation='nearest') \n    plt.title('Training: %i' % label) \nn=len(digits.images) \ndata = digits.images.reshape((n,-1)) \nclassifier = svm.SVC(gamma=0.001) \nclassifier.fit(data[:n//2],digits.target[:n//2]) \nexpected = digits.target[n//2:] \npredicted = classifier.predict(data[n//2:]) \nprint(format1 % (classifier,report(expected, predicted))) \nprint(format2 % metrics.confusion_matrix(expected,predicted)) \nimageAndPrediction=list(zip(digits.images[n//2:],predicted)) \nfor index,(image,prediction) in enumerate(imageAndPrediction[:4]): \n    plt.subplot(2,4,index+5) \n    plt.axis('off') \n    plt.imshow(image,cmap=plt.cm.gray_r,interpolation='nearest') \n    plt.title('Prediction: %i' % prediction) \nplt.show() \n```", "```py\na=csvread(\"c:/temp/c9_input.csv\");\nx=a(:,2);\ny=a(:,3);\nfigure % open a window for graph\nplot(x, y, 'o');\nylabel('Annual returns for S&P500')\nxlabel('Annual returns for IBM')\n```", "```py\naddpath(genpath('C:\\Octave\\octave-ml-master\\octavelib'));\n```", "```py\nA = double(imread('bird_small.png')); \nA = A / 255; % Divide by 255, values in the range 0 - 1 \nimgSize = size(A); \nX = reshape(A, imgSize(1) * imgSize(2), 3); \nk = 16; % using 4-bit (16) colors,minimize cost \n[Centroids,idx,cost]=generateKMeansClustersMinCost(X,k,10,10); \nfprintf('Cost/Distortion of computed clusters:%.3fn', cost); \n% regenerate colors & image \nNewX = Centroids(idx, :); \nNewA = reshape(NewX, imgSize(1), imgSize(2), 3); \n% compare both the images \nfprintf('Comparing original & compressed imagesn'); \nsubplot(1, 2, 1); \nimagesc(A); \naxis(\"square\"); \ntitle('Original'); \nsubplot(1, 2, 2); \nimagesc(NewA); \naxis(\"square\"); \ntitle('Compressed'); \n```", "```py\nusing Gadfly \nusing RDatasets \nusing Clustering \niris = dataset(\"datasets\", \"iris\") \nhead(iris) \nfeatures=permutedims(convert(Array, iris[:,1:4]),[2, 1]) \nresult=kmeans(features,3)                           \nnameX=\"PetalLength\" \nnameY=\"PetalWidth\" \nassignments=result.assignments   \nplot(iris, x=nameX,y=nameY,color=assignments,Geom.point) \n```", "```py\nusing Clustering \nsrand(1234) \nnRow=5 \nnCol=1000 \nx = rand(nRow,nCol) \nmaxInter=200  #max interation  \nnCluster=20 \nR = kmeans(x,nCluster;maxiter=maxInter,display=:iter) \n@assert nclusters(R) ==nCluster \nc = counts(R) \nclusters=R.centers \n```", "```py\n>install.packages(\"ctv\") \n>library(\"ctv\") \n>install.views(\"MachineLearning\") \n```", "```py\n>install.packages(\"ctv\")\n>library(\"ctv\")\n>install.views(\"MachineLearning\")\n```", "```py\nfrom sklearn import datasets\nboston = datasets.load_boston()\nprint(boston.data.shape)\n(506, 13)\n```"]