<html><head></head><body><div><h1 class="header-title">Distributed Computing, Parallel Computing, and HPCC</h1>
                
            
            
                
<p>Since our society has entered a data-intensive era (that is, a big data era), we face larger and larger datasets. For this reason, companies and users are considering what kinds of tools they could use to speed up the process when dealing with data. One obvious solution is to increase their data storage capacity. Unfortunately, there is a huge cost associated with this. The other solutions include distributed computing and some ways to accelerate our process.</p>
<p>In this chapter, we'll cover the following topics:</p>
<ul>
<li>Introduction to distributed versus parallel computing</li>
<li>Understanding MPI</li>
<li>Parallel processing in Python</li>
<li>Compute nodes</li>
<li>Anaconda add-ons</li>
<li>Introduction to HPCC</li>
</ul>


            

            
        
    </div>
<div><h1 class="header-title">Introduction to distributed versus parallel computing</h1>
                
            
            
                
<p>Distributed computing is a subfield of computer science that studies distributed systems and models in which components located on networked computers communicate and coordinate their actions by passing messages. The components interact with each other in order to achieve a common goal.</p>
<p>It is worthwhile to discuss another phrase: parallel computing. Parallel computing is more tightly coupled to multi-threading, or how to make full use of a single CPU, while distributed computing refers to the notion of divide and conquer, executing subtasks on different machines, and then merging the results.</p>
<p>Since we have entered a so-called big data era, it seems that the distinction is melting. In fact, nowadays, many systems use a combination of parallel and distributed computing.</p>


            

            
        
    </div>
<div><h1 class="header-title">Task view for parallel processing</h1>
                
            
            
                
<p>For R, there is a task view called <strong>High-Performance and Parallel Computing with R</strong>. Recall that a task view is a set of R programs around a specific topic. To find the task view for <strong>High-Performance and parallel computing using R</strong>, we go to <a href="http://r-project.org">http://r-project.org</a>, click on CRAN on the left-hand side, choose a nearby mirror location, and click on Packages and Task Views.</p>
<p>After double-clicking on Task Views, we can see the following list—to save space, only the top task views are shown:</p>
<div><img src="img/d1975c7c-6005-4f4b-b03c-1d708bbc33b5.png" style="width:38.08em;height:15.92em;" width="623" height="260"/></div>
<p>After double-clicking on the related task view (<strong>HighPerformanceComputing</strong>), we can see the following screenshot—to save space, only the top few lines are shown:</p>
<div><strong><img src="img/d90b13fd-6d24-43a1-97a9-2d76077101a1.png" style="width:45.25em;height:21.75em;" width="732" height="352"/></strong></div>
<p>The preceding task view contains a list of packages, grouped by various topics that could be useful for <strong>high-performance computing</strong> (<strong>HPC</strong>) with R. In this context, the task view maintainers define high-performance computing quite loosely as just about anything related to pushing R a little further: using compiled code, parallel computing (in both explicit and implicit modes), working with large objects as well as profiling. The aforementioned task view can be downloaded at <a href="https://CRAN.R-project.org/view=HighPerformanceComputing">https://CRAN.R-project.org/view=HighPerformanceComputing</a>. As we have discussed in previous chapters, we could use the following command to install all related R packages:</p>
<pre>install.packages("ctv") 
library("ctv") 
install.views("HighPerformanceComputing") </pre>
<p>On May 14, 2018, by running the previous code, 217 related R packages were downloaded and installed.</p>


            

            
        
    </div>
<div><h1 class="header-title">Sample programs in Python</h1>
                
            
            
                
<p>For Python programs in parallel computing, we can visit the <em>IPython in-depth Tutorial</em> at <a href="https://github.com/ipython/ipython-in-depth">https://github.com/ipython/ipython-in-depth</a>. After downloading the ZIP file and unzipping it, we can find programs related to parallel computing (refer to the following two screenshot):</p>
<div><img src="img/73fc867a-93cb-481c-b9d0-39c60d129af7.png" style="width:31.50em;height:9.33em;" width="451" height="133"/></div>
<p>The preceding screenshot shows a few subdirectories, while the following one shows 19 programs including both Jupyter Notebook and Python programs:</p>
<div><img src="img/4c367b33-a0f3-4d3a-b1e4-570d71e336b9.png" style="width:34.58em;height:26.42em;" width="524" height="400"/></div>


            

            
        
    </div>
<div><h1 class="header-title">Understanding MPI</h1>
                
            
            
                
<p>Usually, a parallel algorithm needs to move data between different engines. One way to do so is by doing a pull and then a push using the direct view. However, this method is quite slow since all the data has to go through the controller to the client and then back through the controller, to its final destination. A much better way of moving data between engines is to use a message passing library, such as the <strong>Message Passing Interface</strong> (<strong>MPI</strong>). IPython's parallel computing architecture has been designed to integrate with MPI. To download and install Windows MPI, readers can refer to <a href="https://msdn.microsoft.com/en-us/library/bb524831%28v=vs.85%29.aspx">https://msdn.microsoft.com/en-us/library/bb524831%28v=vs.85%29.aspx.</a></p>
<p>In addition, you could install the <kbd>mpi4py</kbd> package.</p>


            

            
        
    </div>
<div><h1 class="header-title">R package Rmpi</h1>
                
            
            
                
<p>To find a demo associated with the <kbd>Rmpi</kbd> package, we could issue the following two lines:</p>
<pre>library(Rmpi) 
demo() </pre>
<p>After hitting <em>Enter</em>, we will see the following output:</p>
<div><img src="img/0a82682f-9ddf-4a78-b981-d30b6cd9321a.png" style="width:51.33em;height:21.75em;" width="829" height="352"/></div>
<p>For the first function, called <kbd>cslavePI</kbd>, we know that we should copy <kbd>cslavePI.c</kbd> in Rmpi library directory to your working directory and compile it as <kbd>mpicc -o cslavePI cslavePI.c</kbd>. To find the path of Rmpi, we issue the <kbd>find.package()</kbd> function (refer to the code and the result in the following code):</p>
<pre>&gt; find.package("Rmpi") 
<strong>[1] "C:/Users/yany/Documents/R/win-library/3.4/Rmpi"</strong> </pre>
<p>Note that different readers will get quite different paths. We can use <kbd>demo(cslavePI)</kbd> to find out its function. Refer to the code and the related output, as follows:</p>
<div><img src="img/682f484b-36c5-45e7-b627-2d1c26fa579f.png" style="width:33.17em;height:20.42em;" width="519" height="320"/></div>
<p>The mpicc software is used to compile and link MPI programs written in C.</p>


            

            
        
    </div>
<div><h1 class="header-title">R package plyr</h1>
                
            
            
                
<p>The objective of the <kbd>plyr</kbd> R package is about the split-apply-combine paradigm for R. This is quite common in data analysis: we solve a complex problem by breaking it down into small pieces, doing something to each piece, and then combining the results back together again. The following is an example borrowed from its menu with a minor modification:</p>
<pre>library(plyr) 
d1&lt;-c(rep('A', 8), rep('B', 15), rep('C', 6)) 
d2&lt;-sample(c("M", "F"), size = 29, replace = TRUE) 
d3&lt;-runif(n = 29, min = 18, max = 54) 
# 
dfx &lt;- data.frame(group =d1,sex=d2,age=d3) 
# 
ddply(dfx, .(group, sex), summarize, 
 mean = round(mean(age), 2), 
 sd = round(sd(age), 2)) 
# 
ddply(baseball[1:100,], ~ year, nrow) 
ddply(baseball, .(lg), c("nrow", "ncol")) 
rbi&lt;-ddply(baseball, .(year), summarise, 
      mean_rbi=mean(rbi, na.rm = TRUE)) 
plot(mean_rbi~year,type="l",data = rbi) 
base2&lt;-ddply(baseball,.(id),mutate, 
    career_year = year - min(year) + 1 
) </pre>
<p>To save space, only the graph is shown here:</p>
<div><img src="img/0a600249-cef2-49df-9ae0-b0c3f73ea2d1.png" style="width:21.00em;height:20.33em;" width="502" height="488"/></div>
<p>The <kbd>arrange()</kbd> function orders a data frame by its columns (refer to the following code):</p>
<pre>library(datasets) 
library(plyr) 
data(mtcars) 
# 
d1&lt;-mtcars[with(mtcars, order(cyl, disp)), ] 
d2&lt;-arrange(mtcars, cyl, disp) 
myCars = cbind(vehicle=row.names(mtcars), mtcars) 
d3&lt;-arrange(myCars, cyl, disp) 
d4&lt;-arrange(myCars, cyl, desc(disp)) </pre>


            

            
        
    </div>
<div><h1 class="header-title">R package parallel</h1>
                
            
            
                
<p>First, let's look at the simple usage of an R function called <kbd>lapply()</kbd> (refer to the following code):</p>
<pre>&gt; lapply(1:3,function(x) c(sin(x),x^2)) 
<strong>[[1]] 
[1] 0.841471 1.000000 
[[2]] 
[1] 0.9092974 4.0000000 
[[3]] 
[1] 0.14112 9.00000</strong> </pre>
<p>The meaning is clear: we have an input size of <kbd>1</kbd>, <kbd>2</kbd>, and <kbd>3</kbd> and we assign them to three functions. The following example is a slightly more complex one:</p>
<pre>myFunctions&lt;-c(sin(x),x^2+2,4*x^2-x^3-2) 
inputValue&lt;-1:10 
output&lt;-lapply(inputValue,function(x) myFunctions) </pre>
<p>The first couple of lines are shown here:</p>
<div><img src="img/c2d63458-d0d0-4d95-871a-572d99c2fc8f.png" style="width:39.92em;height:8.50em;" width="580" height="124"/></div>
<p>The following example is borrowed from <em>Gordon</em> (2015):</p>
<pre>library(parallel) 
n_cores &lt;- detectCores() - 1 
cl &lt;- makeCluster(n_cores) 
parLapply(cl, 2:4,function(exponent) 2^exponent) 
stopCluster(cl) </pre>
<p>In the preceding code, the <kbd>makeCluster()</kbd> function will set up the cluster. The <kbd>parLapply()</kbd> function calls the parallel version of <kbd>lapply()</kbd> or <kbd>parLapply()</kbd> functions. The output is shown here:</p>
<pre><strong> [[1]] 
[1] 4 
 
[[2]] 
[1] 8 
 
[[3]] 
[1] 16</strong> </pre>
<p>For the following code, we will see an error message:</p>
<pre>c2&lt;-makeCluster(n_cores) 
base &lt;- 2 
parLapply(c2, 2:4, function(exponent) base^exponent) 
stopCluster(c2) </pre>
<p>The error message is as follows:</p>
<pre><strong>Error in checkForRemoteErrors(val) :  
  3 nodes produced errors; first error: object 'base' not found</strong> </pre>
<p>To correct it, the base variable will be added (refer to the following code).</p>
<pre>c3&lt;-makeCluster(n_cores) 
base &lt;- 2 
clusterExport(c3, "base") 
parLapply(c3, 2:4, function(exponent)  base^exponent) 
stopCluster(c3) </pre>
<p>To save space, the output will not be shown here. The following is another example to see the difference between calling the <kbd>lapply()</kbd> and <kbd>mclapply()</kbd> functions:</p>
<pre>library(parallel) 
detectCores() 
myFunction&lt;- function(iter=1,n=5){ 
    x&lt;- rnorm(n, mean=0, sd=1 ) 
    eps &lt;- runif(n,-2,2) 
    y &lt;- 1 + 2*x + eps 
    result&lt;-lm( y ~ x ) 
    final&lt;-cbind(result$coef,confint(result)) 
    return(final)  
} 
# 
m&lt;-5000<br/>n2&lt;-5000<br/>system.time(lapply(1:m,myFunction,n=n2))<br/>system.time(mclapply(1:m,myFunction,n=n2))</pre>
<p>The output is shown here:</p>
<pre>&gt; system.time(lapply(1:n,myFunction,n=n2)) 
<strong>   user  system elapsed  
  63.97    3.26   22.49</strong>
&gt; system.time(mclapply(1:n,myFunction,n=n2)) 
<strong>   user  system elapsed  
  63.33 3.28 22.26 </strong> </pre>
<p>In the preceding code, the <kbd>lappy()</kbd> and <kbd>mclappy()</kbd> functions are used. The <kbd>mclapply()</kbd> function is a parallelized version of the <kbd>lapply()</kbd> function. It returns a list of the same length as <kbd>X</kbd>, each element of which is the result of applying <kbd>FUN</kbd> to the corresponding element of <kbd>X</kbd>. The following program is borrowed from <a href="http://www.smart-stats.org/wiki/parallel-computing-cluster-using-r">http://www.smart-stats.org/wiki/parallel-computing-cluster-using-r</a> with minor modifications. Note that the program is run on a UNIX instead of a PC:</p>
<pre>library(snow) 
library(parallel) 
#library(Rmpi) 
myFunction&lt;-function(n) { 
    a&lt;-rnorm(n) 
    final&lt;-log(abs(a))+a^3+2*a; 
    return(final) 
} 
nCores=11; 
#Using multicore 
system.time(mclapply(rep(5E6,11),myFunction,mc.cores=nCores)) 
#Using snow via MPI 
system.time(sapply(rep(5E6,11),myFunction)) 
#cl &lt;- getMPIcluster() 
cl &lt;- makeCluster(c("localhost","localhost"), type = "SOCK") 
system.time(parSapply(cl,rep(5E6,11),myFunction)) </pre>
<p>The related output is shown here:</p>
<pre>&gt; system.time(mclapply(rep(5E6,11),myFunction,mc.cores=nCores))<strong> 
   user  system elapsed 
  4.440   1.075   1.926 
</strong>&gt; system.time(sapply(rep(5E6,11),myFunction))<strong> 
   user  system elapsed 
 10.294   0.992  11.286 
</strong>&gt; system.time(parSapply(cl,rep(5E6,11),myFunction))<strong> 
   user  system elapsed 
  0.655   0.626   7.328 
</strong>&gt; proc.time()<strong> 
   user  system elapsed 
 15.621   2.936  22.134</strong> </pre>


            

            
        
    </div>
<div><h1 class="header-title">R package snow</h1>
                
            
            
                
<p>This package is for <strong>Simple Network of Workstations</strong> (<strong>SNOW</strong>). Let's take a look at a program associated with destruction:</p>
<pre>library(snow) 
cl &lt;- makeSOCKcluster(c("localhost","localhost")) 
clusterApply(cl, 1:2, get("+"), 3) 
clusterEvalQ(cl, library(boot)) 
x&lt;-1 
clusterExport(cl, "x") 
clusterCall(cl, function(y) x + y, 2) </pre>
<p>The <kbd>makeSOCKcluster()</kbd> function is used to start and stop a <kbd>snow</kbd> cluster and to set default cluster options. The <kbd>clusterApply()</kbd> calls the function on the first cluster node with arguments <kbd>seq[[1]] and ...</kbd>, on the second node with <kbd>seq[[2]] and ...</kbd>, and so on. If the length of <kbd>seq</kbd> is greater than the number of nodes in the cluster, then cluster nodes are recycled. A list of the results is returned; the length of the result list will be equal to the length of <kbd>seq</kbd>. The <kbd>clusterCall()</kbd> calls a function with identical arguments on each node in the cluster <kbd>cl</kbd> and returns a list of the results. The <kbd>clusterEvalQ()</kbd> function evaluates a literal expression on each cluster node. It's a cluster version of <kbd>evalq</kbd>, and is a convenience function defined in terms of <kbd>clusterCall()</kbd>.</p>


            

            
        
    </div>
<div><h1 class="header-title">Parallel processing in Python</h1>
                
            
            
                
<p>The following example is about computing π digits and is borrowed from the website <a href="http://ipyparallel.readthedocs.io/en/latest/demos.html#parallel-examples">http://ipyparallel.readthedocs.io/en/latest/demos.html#parallel-examples</a>. Since the first part needs a program called <kbd>one_digit_freqs()</kbd> function, we could run a Python program called <kbd>pidigits.py</kbd> contained at <kbd>.../ipython-ipython-in-depth-4d98937\examples\Parallel Computing\pi</kbd>, and this path depends on where the reader downloaded and saved his/her files. </p>
<p>To complete our part, we simply include it in the first part of the program, as shown here:</p>
<pre>import matplotlib.pyplot as plt<br/>import sympy<br/>import numpy as np <br/>#<br/>def plot_one_digit_freqs(f1):<br/>    """<br/>    Plot one digit frequency counts using matplotlib.<br/>    """<br/>    ax = plt.plot(f1,'bo-')<br/>    plt.title('Single digit counts in pi')<br/>    plt.xlabel('Digit')<br/>    plt.ylabel('Count')<br/>    return ax<br/>#<br/>def one_digit_freqs(digits, normalize=False):<br/>    """<br/>    Consume digits of pi and compute 1 digit freq. counts.<br/>    """<br/>    freqs = np.zeros(10, dtype='i4')<br/>    for d in digits:<br/>        freqs[int(d)] += 1<br/>    if normalize:<br/>        freqs = freqs/freqs.sum()<br/>    return freqs<br/>#<br/>pi = sympy.pi.evalf(40)<br/>pi<br/>pi = sympy.pi.evalf(10000)<br/>digits = (d for d in str(pi)[2:]) # create a sequence of digits<br/>freqs = one_digit_freqs(digits)<br/>plot_one_digit_freqs(freqs)<br/>plt.show()</pre>
<p>The related graph is shown here:</p>
<div><img src="img/b303d68c-8b04-4977-85cf-315fd84d7a40.png" style="width:27.08em;height:18.08em;" width="416" height="277"/></div>


            

            
        
    </div>
<div><h1 class="header-title">Parallel processing for word frequency</h1>
                
            
            
                
<p>First, let's look at a simple Python program to find out the first most frequently used word in an input text file. We randomly chose Da Vinci Code at <a href="http://www.gutenberg.org/files/5000/5000-8.txt">http://www.gutenberg.org/files/5000/5000-8.txt</a>. Assume that the downloaded novel is saved under <kbd>c:/temp/daVinci.txt</kbd>. The following Python code will list the top 10 most frequent words:</p>
<pre>text = open("c:/temp/daVinci.txt",'r').read().lower() 
def byFreq(pair): 
    return pair[1] 
for ch in '!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~': 
    text = text.replace(ch, ' ') 
    words = text.split() 
counts = {} 
for w in words: 
    counts[w] = counts.get(w,0) + 1 
n = 10   # for the first n most frequetly used words 
words= list(counts.items()) 
words.sort() 
words.sort(key=byFreq, reverse=True) 
for i in range(n): 
    word, count =words[i] 
    print("{0:&lt;15}{1:&gt;5}".format(word, count)) </pre>
<p>The related output is shown here:</p>
<pre><strong>the            22989 
of             11232 
and             8434 
in              5750 
to              5313 
a               4725 
is              4273 
it              3071 
that            2902 
which           2545</strong> </pre>


            

            
        
    </div>
<div><h1 class="header-title">Parallel Monte-Carlo options pricing</h1>
                
            
            
                
<p>This example is from the sample programs in Python discussed earlier. The notebook shows how to use the <kbd>ipyparallel</kbd> package to do Monte-Carlo options pricing in parallel. The notebook computes the price of a large number of options for different strike prices and volatilities.</p>
<p>To save space, only the first several lines of code are given here:</p>
<pre>%matplotlib inline 
import matplotlib.pyplot as plt 
import sys 
import time 
from ipyparallel import Client 
import numpy as np 
price = 100.0  # Initial price 
rate = 0.05  # Interest rate 
days = 260  # Days to expiration 
paths = 10000  # Number of MC paths 
n_strikes = 6  # Number of strike values 
min_strike = 90.0  # Min strike price 
max_strike = 110.0  # Max strike price 
n_sigmas = 5  # Number of volatility values 
min_sigma = 0.1  # Min volatility 
max_sigma = 0.4  # Max volatility 
# (more ....) </pre>
<p>To run it, click on IPython Clusters. After clicking IPython Clusters on the right-hand side, we will see the following screenshot:</p>
<div><img class="alignnone size-full wp-image-973 image-border" src="img/e6bb005d-ba65-4693-8d5e-3f64735b711b.png" style="width:64.75em;height:13.08em;" width="1139" height="229"/></div>
<p>We can click on Start for the default. The number of engines will show a value of <kbd>4</kbd>, as follows:</p>
<div><img src="img/cc7c234c-79b1-4f87-a690-0d9993eadedf.png" style="width:67.42em;height:13.25em;" width="1139" height="224"/></div>
<p>Now, we go back to our uploaded Jupyter Notebook for the parallel monte-carlo options pricing. In total, there are four output images. To save space, only the first one is shown here:</p>
<div><img src="img/14427f22-3abb-43e0-802b-841df9ec89c9.png" style="width:37.75em;height:26.92em;" width="573" height="408"/></div>
<p>Note that the payoff function for an Asian put using stock average price as the final terminal price is given here, where <em>Put(Asian)</em> is the Asian put option, <em>K</em> is the exercise price, and <img class="fm-editor-equation" src="img/0c79a345-2dd8-47c8-a4a3-7529f93e9281.png" style="width:1.08em;height:1.50em;" width="130" height="180"/> is the average price over the path:</p>
<div><img class="fm-editor-equation" src="img/e1554d15-5e87-4001-945f-93db9761280c.png" style="width:20.67em;height:1.25em;" width="3800" height="230"/></div>


            

            
        
    </div>
<div><h1 class="header-title">Compute nodes</h1>
                
            
            
                
<p>A compute node provides the ephemeral storage, networking, memory, and processing resources that can be consumed by virtual machine instances. The cloud system supports two types of compute nodes: <strong>ESX clusters</strong>, where clusters are created in VMware vCenter Server, and <strong>KVM compute nodes</strong>, where KVM compute nodes are created manually. In the previous chapter, we mentioned the concept of the cloud.</p>
<p>Within a cloud environment, which is quite useful for a more complex project, compute nodes form the core of resources. Typically, these notes supply the processing, memory, network, and storage that virtual machine instances need. When an instance is created, it is matched to a compute node with the available resources. A compute node can host multiple instances until all of its resources are consumed.</p>


            

            
        
    </div>
<div><h1 class="header-title">Anaconda add-on</h1>
                
            
            
                
<p>The following information is from the Anaconda Addon Development Guide.</p>
<p>An Anaconda add-on is a Python package containing a directory with an <kbd>__init__.py</kbd> file and other source directories (sub packages) inside. Because Python allows importing each package name only once, the package top-level directory name must be unique. At the same time, the name can be arbitrary, because add-ons are loaded regardless of their name; the only requirement is that they must be placed in a specific directory.</p>
<p>The suggested naming convention for add-ons is therefore similar to that of Java packages or D-Bus service names: prefix the add-on name with the reversed domain name of your organization, using underscores (_) instead of dots so that the directory name is a valid identifier for a Python package. An example add-on name following these suggestions would therefore be, for example, <kbd>org_fedora_hello_world</kbd>. This convention follows the recommended naming scheme for Python package and module names. Interested readers can find lots of information about Anaconda add-ons at <a href="https://rhinstaller.github.io/anaconda-addon-development-guide/index.html#sect-anaconda-introduction-addons">https://rhinstaller.github.io/anaconda-addon-development-guide/index.html#sect-anaconda-introduction-addons</a>.<a href="https://rhinstaller.github.io/anaconda-addon-development-guide/index.html#sect-anaconda-introduction-addons"/></p>


            

            
        
    </div>
<div><h1 class="header-title">Introduction to HPCC</h1>
                
            
            
                
<p>HPCC stands for <strong>High-Performance Computing Cluster</strong>. It is also known as <strong>Data Analytics Supercomputer</strong> (<strong>DAS</strong>), an open source, data-intensive computing system platform developed by LexisNexis Risk Solutions. The HPCC platform incorporates a software architecture implemented on computing clusters to provide high-performance, data-parallel processing design for various applications using big data. The HPCC platform includes system configurations to support both parallel batch data processing (Thor) and high-performance online query applications using indexed data files (Roxie). The HPCC platform also includes a data-centric declarative programming language for parallel data processing called ECL.</p>
<p>You can see a simple example of using Wharton's HPCC system at <a href="https://research-it.wharton.upenn.edu/documentation/">https://research-it.wharton.upenn.edu/documentation/</a>. Wharton's HPC Cluster (HPCC) provides access to advanced computational research hardware and software for Wharton faculty, faculty collaborators and research assistants, and Wharton doctoral candidates. It is designed for simple and parallel processing across a large set of tightly integrated hardware with dedicated networking and storage.</p>
<p>For more information about the hardware, refer to the Hardware page. HPCC users have access to a number of scientific, mathematics, and analytic software, including MATLAB, Mathematica, R, Stata, and SAS. MySQL server access can be provided as well. The HPCC also has Fortran, C, and C++ compilers in GNU and Intel versions. The following is a simple procedure to link to their HPCC platform:</p>
<ol>
<li>First, download and install FortClient and MobaXterm software</li>
<li>Connect to the Wharton VPN via FortClient (as shown in the following screenshot):</li>
</ol>
<div><img src="img/43d91c0d-f130-4be2-8c89-87cdbca7bb73.png" style="width:39.67em;height:28.75em;" width="588" height="427"/></div>
<ol start="3">
<li>After connecting, the following screen will appear:</li>
</ol>
<div><img class="alignnone size-full wp-image-974 image-border" src="img/5d7368e8-3d4f-4aa4-b302-7c36789c8295.png" style="width:19.08em;height:16.92em;" width="313" height="277"/></div>
<ol start="4">
<li>Use MobaXterm software to connect to Wharton's HPCC platform. Here, we assume that you have an account with Wharton (refer to the following screenshot):</li>
</ol>
<div><img src="img/076af6c0-16d0-4c35-b086-c4f286b5839f.png" style="width:33.08em;height:12.08em;" width="535" height="195"/></div>
<ol start="5">
<li>Now, we can connect, as shown in the following screenshot:</li>
</ol>
<div><img class="alignnone size-full wp-image-975 image-border" src="img/60abcd45-55a4-4969-8956-1104013ad39c.png" style="width:35.33em;height:41.75em;" width="634" height="749"/></div>
<ol start="6">
<li>Now, the users will write their own programs to take advantage of HPCC to speed up their research and experiments. For more information, refer to the documentation at <a href="https://research-it.wharton.upenn.edu/documentation/">https://research-it.wharton.upenn.edu/documentation/</a>.</li>
</ol>


            

            
        
    </div>
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we have discussed several R packages such as <kbd>plyr</kbd>, <kbd>snow</kbd>, <kbd>Rmpi</kbd>, and <kbd>parallel</kbd>, and the Python package <kbd>ipyparallel</kbd>. In addition, we mentioned compute nodes, project add-ons, parallel processing, and HPCC.</p>
<p>Now we've arrived at the end of our journey. We wish you good luck for the amazing endeavors you'll be taking up with the knowledge you've got from this book.</p>


            

            
        
    </div>
<div><h1 class="header-title">Review questions and exercises</h1>
                
            
            
                
<ol>
<li>What is distributed computing? Why it is useful?</li>
<li>From where could we get a task view for parallel computing?</li>
<li>From the task view related to parallel computing, we can find many R packages. Identify a few of them. Install two and find a few examples of using these two packages.</li>
<li>Conduct a word frequency analysis using: The Count of Monte Cristo by Alexandre Dumas (input file is at <a href="http://www.gutenberg.org/files/1184/1184-0.txt">http://www.gutenberg.org/files/1184/1184-0.txt</a>).</li>
<li>From where could we find more information about Anaconda add-ons?</li>
<li>What is HPCC and how does it work?</li>
<li>How do we find the path of an installed R package?</li>
<li>In the sample Jupyter notebook about parallel Monte-Carlo options pricing, the related Asian options are defined here, where <kbd>call(Asian)</kbd> is the Asian put option, <kbd>Put(Asian)</kbd>, <em>K</em> is the exercise price, and <img class="fm-editor-equation" src="img/67daf5c0-5b94-4153-8c39-3bd8ce0c65c0.png" style="width:1.08em;height:1.50em;" width="130" height="180"/> is the average price over the path:</li>
</ol>
<div><img class="fm-editor-equation" src="img/3f973221-3119-4640-a6f2-e2e066b0c4f8.png" style="width:11.67em;height:1.08em;" width="2480" height="230"/></div>
<div><img class="fm-editor-equation" src="img/e4300d58-1141-4032-8b57-f16d53141e02.png" style="width:11.50em;height:1.08em;" width="2450" height="230"/></div>
<p style="padding-left: 60px">Write a Jupyter notebook to use the following definitions of Asian options:</p>
<div><img class="fm-editor-equation" src="img/8e51ae73-ccd6-44c8-b4be-02834221af98.png" style="width:12.00em;height:1.08em;" width="2540" height="230"/></div>
<div><img class="fm-editor-equation" src="img/32cafa72-593a-4f75-833d-89c737bd1aa0.png" style="width:12.75em;height:1.17em;" width="2510" height="230"/></div>
<ol start="9">
<li>In this chapter, we mentioned that the following three lines could be used to download all R packages related to high-performance computing:</li>
</ol>
<pre style="padding-left: 90px">install.packages("ctv") 
library("ctv") 
install.views("HighPerformanceComputing") </pre>
<p style="padding-left: 60px">Try this and report how many R packages are downloaded.</p>
<ol start="10">
<li>Find more examples associated with the R package called <kbd>Rmpi</kbd>.</li>
<li>Run the sample Jupyter notebook called <kbd>Using MPI</kbd> with <kbd>IPython Parallel.ipynb</kbd>. Explain its meaning.</li>
<li>The R <kbd>doRNG</kbd> package provides functions to perform reproducible parallel <kbd>foreach</kbd> loops, using independent random streams as generated by the package <kbd>rstream</kbd>, suitable for the different <kbd>foreach</kbd> back ends. Download and install this and other related packages. Show a few examples.</li>
</ol>


            

            
        
    </div></body></html>