<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Testing and Debugging Spark</h1>
                
            
            
                
<p>"Everyone knows that debugging is twice as hard as writing a program in the first place. So if you're as clever as you can be when you write it, how will you ever debug it?"</p>
<p class="cdpalignright">- Brian W. Kernighan</p>
<p class="mce-root">In an ideal world, we write perfect Spark codes and everything runs perfectly all the time, right? Just kidding; in practice, we know that working with large-scale datasets is hardly ever that easy, and there are inevitably some data points that will expose any corner cases with your code.</p>
<p class="mce-root">Considering the aforementioned challenges, therefore, in this chapter, we will see how difficult it can be to test an application if it is distributed; then, we will see some ways to tackle this. In a nutshell, the following topics will be cover throughout this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Testing in a distributed environment</li>
<li class="mce-root1">Testing Spark application</li>
<li class="mce-root1">Debugging Spark application</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Testing in a distributed environment</h1>
                
            
            
                
<p class="mce-root">Leslie Lamport defined the term distributed system as follows:</p>
<p>"A distributed system is one in which I cannot get any work done because some machine I have never heard of has crashed."</p>
<p class="mce-root">Resource sharing through <strong class="calibre1">World Wide Web</strong> (aka <strong class="calibre1">WWW</strong>), a network of connected computers (aka a cluster), is a good example of distributed systems. These distributed environments are often complex and lots of heterogeneity occurs frequently. Testing in these kinds of the heterogeneous environments is also challenging. In this section, at first, we will observe some commons issues that are often raised while working with such system.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Distributed environment</h1>
                
            
            
                
<p class="mce-root">There are numerous definitions of distributed systems. Let's see some definition and then we will try to correlate the aforementioned categories afterward. Coulouris defines a distributed system as <em class="calibre8">a system in which hardware or software components located at networked computers communicate and coordinate their actions only by message passing</em>. On the other hand, Tanenbaum defines the term in several ways:</p>
<ul class="calibre9">
<li class="mce-root1"><em class="calibre8">A collection of independent computers that appear to the users of the system as a single computer.</em></li>
<li class="mce-root1"><em class="calibre8">A system that consists of a collection of two or more independent Computers which coordinate their processing through the exchange of synchronous or asynchronous message passing.</em></li>
<li class="mce-root1"><em class="calibre8">A distributed system is a collection of autonomous computers linked by a network with software designed to produce an integrated computing facility.</em></li>
</ul>
<p class="mce-root">Now, based on the preceding definition, distributed systems can be categorized as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Only hardware and software are distributed:The local distributed system is connected through LAN.</li>
<li class="mce-root1">Users are distributed, but there are computing and hardware resources that are running backend, for example, WWW.</li>
<li class="mce-root1">Both users and hardware/software are distributed: Distributed computing cluster that is connected through WAN. For example, you can get these types of computing facilities while using Amazon AWS, Microsoft Azure, Google Cloud, or Digital Ocean's droplets.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Issues in a distributed system</h1>
                
            
            
                
<p class="mce-root">Here we will discuss some major issues that need to be taken care of during the software and hardware testing so that Spark jobs run smoothly in cluster computing, which is essentially a distributed computing environment.</p>
<p class="mce-root">Note that all the issues are unavoidable, but we can at least tune them for betterment. You should follow the instructions and recommendations given in the previous chapter. According to <em class="calibre8">Kamal Sheel Mishra</em> and <em class="calibre8">Anil Kumar Tripathi</em>, <em class="calibre8">Some Issues, Challenges and Problems of Distributed Software System</em>, in <em class="calibre8">International Journal of Computer Science and Information Technologies</em>, Vol. 5 (4), 2014, 4922-4925. URL: <a href="https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf" class="calibre10">https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf</a>, there are several issues that need to be addressed while working with software or hardware in a distributed environment:</p>
<ul class="calibre9">
<li class="mce-root1">Scalability</li>
<li class="mce-root1">Heterogeneous languages, platform, and architecture</li>
<li class="mce-root1">Resource management</li>
<li class="mce-root1">Security and privacy</li>
<li class="mce-root1">Transparency</li>
<li class="mce-root1">Openness</li>
<li class="mce-root1">Interoperability</li>
<li class="mce-root1">Quality of service</li>
<li class="mce-root1">Failure management</li>
<li class="mce-root1">Synchronization</li>
<li class="mce-root1">Communications</li>
<li class="mce-root1">Software architectures</li>
<li class="mce-root1">Performance analysis</li>
<li class="mce-root1">Generating test data</li>
<li class="mce-root1">Component selection for testing</li>
<li class="mce-root1">Test sequence</li>
<li class="mce-root1">Testing for system scalability and performance</li>
<li class="mce-root1">Availability of source code</li>
<li class="mce-root1">Reproducibility of events</li>
<li class="mce-root1">Deadlocks and race conditions</li>
<li class="mce-root1">Testing for fault tolerance</li>
<li class="mce-root1">Scheduling issue for distributed system</li>
<li class="mce-root1">Distributed task allocation</li>
<li class="mce-root1">Testing distributed software</li>
<li class="mce-root1">Monitoring and control mechanism from the hardware abstraction level</li>
</ul>
<p class="mce-root">It's true that we cannot fully solve all of these issues, but However, using Spark, we can at least control a few of them that are related to distributed system. For example, scalability, resource management, quality of service, failure management, synchronization, communications, scheduling issue for distributed system, distributed task allocation, and monitoring and control mechanism in testing distributed software. Most of them were discussed in the previous two chapters. On the other hand, we can address some issues in the testing and software side: such as software architectures, performance analysis, generating test data, component selection for testing, test sequence, testing for system scalability and performance, and availability of source code. These will be covered explicitly or implicitly in this chapter at least.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Challenges of software testing in a distributed environment</h1>
                
            
            
                
<p class="mce-root">There are some common challenges associated with the tasks in an agile software development, and those challenges become more complex while testing the software in a distributed environment before deploying them eventually. Often team members need to merge the software components in parallel after the bugs proliferating. However, based on urgency, often the merging occurs before testing phase. Sometimes, many stakeholders are distributed across teams. Therefore, there's a huge potential for misunderstanding and teams often lose in between.</p>
<p class="mce-root">For example, Cloud Foundry (<a href="https://www.cloudfoundry.org/" class="calibre10">https://www.cloudfoundry.org/</a>) is an open source heavily distributed PaaS software system for managing deployment and scalability of applications in the Cloud. It promises different features such as scalability, reliability, and elasticity that come inherently to deployments on Cloud Foundry require the underlying distributed system to implement measures to ensure robustness, resiliency, and failover.</p>
<p class="mce-root">The process of software testing is long known to comprise <em class="calibre8">unit testing</em>, <em class="calibre8">integration testing</em>, <em class="calibre8">smoke testing</em>, <em class="calibre8">acceptance testing</em>, <em class="calibre8">scalability testing</em>, <em class="calibre8">performance testing</em>, and <em class="calibre8">quality of service testing</em>. In Cloud Foundry, the process of testing a distributed system is shown in the following figure:</p>
<div><img class="image-border257" src="img/00106.jpeg"/></div>
<div><strong class="calibre1">Figure 1:</strong> An example of software testing in a distributed environment like Cloud</div>
<p class="mce-root">As shown in the preceding figure (first column), the process of testing in a distributed environment like Cloud starts with running unit tests against the smallest points of contract in the system. Following successful execution of all the unit tests, integration tests are run to validate the behavior of interacting components as part of a single coherent software system (second column) running on a single box (for example, a <strong class="calibre1">Virtual Machine</strong> (<strong class="calibre1">VM</strong>) or bare metal). However, while these tests validate the overall behavior of the system as a monolith, they do not guarantee system validity in a distributed deployment. Once integration tests pass, the next step (third column) is to validate distributed deployment of the system and run the smoke tests.</p>
<p class="mce-root">As you know, that the successful configuration of the software and execution of unit tests prepares us to validate acceptability of system behavior. This verification is done by running acceptance tests (fourth column). Now, to overcome the aforementioned issues and challenges in distributed environments, there are also other hidden challenges that need to be solved by researchers and big data engineers, but those are actually out of the scope of this book.</p>
<p class="mce-root">Now that we know what real challenges are for the software testing in a distributed environment, now let's start testing our Spark code a bit. The next section is dedicated to testing Spark applications.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Testing Spark applications</h1>
                
            
            
                
<p class="mce-root">There are many ways to try to test your Spark code, depending on whether it's Java (you can do basic JUnit tests to test non-Spark pieces) or ScalaTest for your Scala code. You can also do full integration tests by running Spark locally or on a small test cluster. Another awesome choice from Holden Karau is using Spark-testing base. You probably know that there is no native library for unit testing in Spark as of yet. Nevertheless, we can have the following two alternatives to use two libraries:</p>
<ul class="calibre9">
<li class="mce-root1">ScalaTest</li>
<li class="mce-root1">Spark-testing base</li>
</ul>
<p class="mce-root">However, before starting to test your Spark applications written in Scala, some background knowledge about unit testing and testing Scala methods is a mandate.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Testing Scala methods</h1>
                
            
            
                
<p class="mce-root">Here, we will see some simple techniques for testing Scala methods. For Scala users, this is the most familiar unit testing framework (you can also use it for testing Java code and soon for JavaScript). ScalaTest supports a number of different testing styles, each designed to support a specific type of testing need. For details, see ScalaTest User Guide at <a href="http://www.scalatest.org/user_guide/selecting_a_style" class="calibre10">http://www.scalatest.org/user_guide/selecting_a_style</a>. Although ScalaTest supports many styles, one of the quickest ways to get started is to use the following ScalaTest traits and write the tests in the <strong class="calibre1">TDD</strong> (<strong class="calibre1">test-driven development</strong>) style:</p>
<ol class="calibre14">
<li value="1" class="mce-root1"><kbd class="calibre11">FunSuite</kbd></li>
<li value="2" class="mce-root1"><kbd class="calibre11">Assertions</kbd></li>
<li value="3" class="mce-root1"><kbd class="calibre11">BeforeAndAfter</kbd></li>
</ol>
<p class="mce-root">Feel free to browse the preceding URLs to learn more about these traits; that will make rest of this tutorial go smoothly.</p>
<p>It is to be noted that the TDD is a programming technique to develop software, and it states that you should start development from tests. Hence, it doesn't affect how tests are written, but when tests are written. There is no trait or testing style to enforce or encourage TDD in <kbd class="calibre22">ScalaTest.FunSuite</kbd>, <kbd class="calibre22">Assertions</kbd>, and <kbd class="calibre22">BeforeAndAfter</kbd> are only more similar to the xUnit testing frameworks.</p>
<p class="mce-root">There are three assertions available in the ScalaTest in any style trait:</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">assert</kbd>: This is used for general assertions in your Scala program.</li>
<li class="mce-root1"><kbd class="calibre11">assertResult</kbd>: This helps differentiate expected value from the actual values.</li>
<li class="mce-root1"><kbd class="calibre11">assertThrows</kbd>: This is used to ensure a bit of code throws an expected exception.</li>
</ul>
<p class="mce-root">The ScalaTest's assertions are defined in the trait <kbd class="calibre11">Assertions</kbd>, which is further extended by <kbd class="calibre11">Suite</kbd>. In brief, the <kbd class="calibre11">Suite</kbd> trait is the super trait for all the style traits. According to the ScalaTest documentation at <a href="http://www.scalatest.org/user_guide/using_assertions" class="calibre10">http://www.scalatest.org/user_guide/using_assertions</a>, the <kbd class="calibre11">Assertions</kbd> trait also provides the following features:</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">assume</kbd> to conditionally cancel a test</li>
<li class="mce-root1"><kbd class="calibre11">fail</kbd> to fail a test unconditionally</li>
<li class="mce-root1"><kbd class="calibre11">cancel</kbd> to cancel a test unconditionally</li>
<li class="mce-root1"><kbd class="calibre11">succeed</kbd> to make a test succeed unconditionally</li>
<li class="mce-root1"><kbd class="calibre11">intercept</kbd> to ensure a bit of code throws an expected exception and then make assertions about the exception</li>
<li class="mce-root1"><kbd class="calibre11">assertDoesNotCompile</kbd> to ensure a bit of code does not compile</li>
<li class="mce-root1"><kbd class="calibre11">assertCompiles</kbd> to ensure a bit of code does compile</li>
<li class="mce-root1"><kbd class="calibre11">assertTypeError</kbd> to ensure a bit of code does not compile because of a type (not parse) error</li>
<li class="mce-root1"><kbd class="calibre11">withClue</kbd> to add more information about a failure</li>
</ul>
<p class="mce-root">From the preceding list, we will show a few of them. In your Scala program, you can write assertions by calling <kbd class="calibre11">assert</kbd> and passing a <kbd class="calibre11">Boolean</kbd> expression in. You can simply start writing your simple unit test case using <kbd class="calibre11">Assertions</kbd>. The <kbd class="calibre11">Predef</kbd> is an object, where this behavior of assert is defined. Note that all the members of the <kbd class="calibre11">Predef</kbd> get imported into your every Scala source file. The following source code will print <kbd class="calibre11">Assertion success</kbd> for the following case:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>object SimpleScalaTest {<br class="title-page-name"/>  def main(args: Array[String]):Unit= {<br class="title-page-name"/>    val a = 5<br class="title-page-name"/>    val b = 5<br class="title-page-name"/>    assert(a == b)<br class="title-page-name"/>      println("Assertion success")       <br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="cdpalignleft1">However, if you make <kbd class="calibre11">a = 2</kbd> and <kbd class="calibre11">b = 1</kbd>, for example, the assertion will fail and you will experience the following output:</p>
<div><img class="image-border258" src="img/00271.jpeg"/></div>
<div><strong class="calibre1">Figure 2:</strong> An example of assertion fail</div>
<p class="cdpalignleft1">If you pass a true expression, assert will return normally. However, assert will terminate abruptly with an Assertion Error if the supplied expression is false. Unlike the <kbd class="calibre11">AssertionError</kbd> and <kbd class="calibre11">TestFailedException</kbd> forms, the ScalaTest's assert provides more information that will tell you exactly in which line the test case failed or for which expression. Therefore, ScalaTest's assert provides better error messages than Scala's assert.</p>
<p class="cdpalignleft1">For example, for the following source code, you should experience <kbd class="calibre11">TestFailedException</kbd> that will tell that 5 did not equal 4:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.scalatest.Assertions._<br class="title-page-name"/>object SimpleScalaTest {<br class="title-page-name"/>  def main(args: Array[String]):Unit= {<br class="title-page-name"/>    val a = 5<br class="title-page-name"/>    val b = 4<br class="title-page-name"/>    assert(a == b)<br class="title-page-name"/>      println("Assertion success")       <br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">The following figure shows the output of the preceding Scala test:</p>
<div><img class="image-border259" src="img/00180.jpeg"/></div>
<div><strong class="calibre1">Figure 3:</strong> An example of TestFailedException</div>
<p class="mce-root">The following source code explains the use of the <kbd class="calibre11">assertResult</kbd> unit test to test the result of your method:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.scalatest.Assertions._<br class="title-page-name"/>object AssertResult {<br class="title-page-name"/>  def main(args: Array[String]):Unit= {<br class="title-page-name"/>    val x = 10<br class="title-page-name"/>    val y = 6<br class="title-page-name"/>    assertResult(3) {<br class="title-page-name"/>      x - y<br class="title-page-name"/>    }<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="cdpalignleft1">The preceding assertion will be failed and Scala will throw an exception <kbd class="calibre11">TestFailedException</kbd> and prints <kbd class="calibre11">Expected 3 but got 4</kbd> (<em class="calibre8">Figure 4</em>):</p>
<div><img class="image-border260" src="img/00060.jpeg"/></div>
<div><strong class="calibre1">Figure 4:</strong> Another example of TestFailedException</div>
<p class="cdpalignleft1">Now, let's see a unit testing to show expected exception:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.scalatest.Assertions._<br class="title-page-name"/>object ExpectedException {<br class="title-page-name"/>  def main(args: Array[String]):Unit= {<br class="title-page-name"/>    val s = "Hello world!"<br class="title-page-name"/>    try {<br class="title-page-name"/>      s.charAt(0)<br class="title-page-name"/>      fail()<br class="title-page-name"/>    } catch {<br class="title-page-name"/>      case _: IndexOutOfBoundsException =&gt; // Expected, so continue<br class="title-page-name"/>    }<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">If you try to access an array element outside the index, the preceding code will tell you if you're allowed to access the first character of the preceding string <kbd class="calibre11">Hello world!</kbd>. If your Scala program can access the value in an index, the assertion will fail. This also means that the test case has failed. Thus, the preceding test case will fail naturally since the first index contains the character <kbd class="calibre11">H</kbd>, and you should experience the following error message (<em class="calibre8">Figure 5</em>):</p>
<div><img class="image-border261" src="img/00337.jpeg"/></div>
<div><strong class="calibre1">Figure 5:</strong> Third example of TestFailedException</div>
<p class="mce-root">However, now let's try to access the index at position <kbd class="calibre11">-1</kbd> as follows:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.scalatest.Assertions._<br class="title-page-name"/>object ExpectedException {<br class="title-page-name"/>  def main(args: Array[String]):Unit= {<br class="title-page-name"/>    val s = "Hello world!"<br class="title-page-name"/>    try {<br class="title-page-name"/>      s.charAt(-1)<br class="title-page-name"/>      fail()<br class="title-page-name"/>    } catch {<br class="title-page-name"/>      case _: IndexOutOfBoundsException =&gt; // Expected, so continue<br class="title-page-name"/>    }<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">Now the assertion should be true, and consequently, the test case will be passed. Finally, the code will terminate normally. Now, let's check our code snippets if it will compile or not. Very often, you may wish to ensure that a certain ordering of the code that represents emerging "user error" does not compile at all. The objective is to check the strength of the library against the error to disallow unwanted result and behavior. ScalaTest's <kbd class="calibre11">Assertions</kbd> trait includes the following syntax for that purpose:</p>
<pre class="calibre19">
assertDoesNotCompile("val a: String = 1")
</pre>
<p class="mce-root">If you want to ensure that a snippet of code does not compile because of a type error (as opposed to a syntax error), use the following:</p>
<pre class="calibre19">
assertTypeError("val a: String = 1")
</pre>
<p class="mce-root">A syntax error will still result on a thrown <kbd class="calibre11">TestFailedException</kbd>. Finally, if you want to state that a snippet of code does compile, you can make that more obvious with the following:</p>
<pre class="calibre19">
assertCompiles("val a: Int = 1")
</pre>
<p class="mce-root">A complete example is shown as follows:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.scalatest.Assertions._ <br class="title-page-name"/>object CompileOrNot {<br class="title-page-name"/>  def main(args: Array[String]):Unit= {<br class="title-page-name"/>    assertDoesNotCompile("val a: String = 1")<br class="title-page-name"/>    println("assertDoesNotCompile True")<br class="title-page-name"/>    <br class="title-page-name"/>    assertTypeError("val a: String = 1")<br class="title-page-name"/>    println("assertTypeError True")<br class="title-page-name"/>    <br class="title-page-name"/>    assertCompiles("val a: Int = 1")<br class="title-page-name"/>    println("assertCompiles True")<br class="title-page-name"/>    <br class="title-page-name"/>    assertDoesNotCompile("val a: Int = 1")<br class="title-page-name"/>    println("assertDoesNotCompile True")<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">The output of the preceding code is shown in the following figure:</p>
<div><img class="image-border262" src="img/00369.jpeg"/></div>
<div><strong class="calibre1">Figure 6:</strong> Multiple tests together</div>
<p class="mce-root">Now we would like to finish the Scala-based unit testing due to page limitation. However, for other unit test cases, you can refer the Scala test guideline at <a href="http://www.scalatest.org/user_guide" class="calibre10">http://www.scalatest.org/user_guide</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Unit testing</h1>
                
            
            
                
<p class="mce-root">In software engineering, often, individual units of source code are tested to determine whether they are fit for use or not. This way of software testing method is also called the unit testing. This testing ensures that the source code developed by a software engineer or developer meets the design specifications and works as intended.</p>
<p class="mce-root">On the other hand, the goal of unit testing is to separate each part of the program (that is, in a modular way). Then try to observe if all the individual parts are working normally. There are several benefits of unit testing in any software system:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Find problems early:</strong> It finds bugs or missing parts of the specification early in the development cycle.</li>
<li class="mce-root1"><strong class="calibre1">Facilitates change:</strong> It helps in refactoring and up gradation without worrying about breaking functionality.</li>
<li class="mce-root1"><strong class="calibre1">Simplifies integration:</strong> It makes integration tests easier to write.</li>
<li class="mce-root1"><strong class="calibre1">Documentation:</strong> It provides a living documentation of the system.</li>
<li class="mce-root1"><strong class="calibre1">Design:</strong> It can act as the formal design of the project.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Testing Spark applications</h1>
                
            
            
                
<p class="mce-root">We have already seen how to test your Scala code using built-in <kbd class="calibre11">ScalaTest</kbd> package of Scala. However, in this subsection, we will see how we could test our Spark application written in Scala. The following three methods will be discussed:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Method 1:</strong> Testing Spark applications using JUnit</li>
<li class="mce-root1"><strong class="calibre1">Method 2:</strong> Testing Spark applications using <kbd class="calibre11">ScalaTest</kbd> package</li>
<li class="mce-root1"><strong class="calibre1">Method 3:</strong> Testing Spark applications using Spark testing base</li>
</ul>
<p class="mce-root">Methods 1 and 2 will be discussed here with some practical codes. However, a detailed discussion on method 3 will be provided in the next subsection. To keep the understanding easy and simple, we will use the famous word counting applications to demonstrate methods 1 and 2.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Method 1: Using Scala JUnit test</h1>
                
            
            
                
<p class="mce-root">Suppose you have written an application in Scala that can tell you how many words are there in a document or text file as follows:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.apache.spark._<br class="title-page-name"/>import org.apache.spark.sql.SparkSession<br class="title-page-name"/>class wordCounterTestDemo {<br class="title-page-name"/>  val spark = SparkSession<br class="title-page-name"/>    .builder<br class="title-page-name"/>    .master("local[*]")<br class="title-page-name"/>    .config("spark.sql.warehouse.dir", "E:/Exp/")<br class="title-page-name"/>    .appName(s"OneVsRestExample")<br class="title-page-name"/>    .getOrCreate()<br class="title-page-name"/>  def myWordCounter(fileName: String): Long = {<br class="title-page-name"/>    val input = spark.sparkContext.textFile(fileName)<br class="title-page-name"/>    val counts = input.flatMap(_.split(" ")).distinct()<br class="title-page-name"/>    val counter = counts.count()<br class="title-page-name"/>    counter<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">The preceding code simply parses a text file and performs a <kbd class="calibre11">flatMap</kbd> operation by simply splitting the words. Then, it performs another operation to take only the distinct words into consideration. Finally, the <kbd class="calibre11">myWordCounter</kbd> method counts how many words are there and returns the value of the counter.</p>
<p class="mce-root">Now, before proceeding into formal testing, let's check if the preceding method works well. Just add the main method and create an object as follows:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.apache.spark._<br class="title-page-name"/>import org.apache.spark.sql.SparkSession<br class="title-page-name"/>object wordCounter {<br class="title-page-name"/>  val spark = SparkSession<br class="title-page-name"/>    .builder<br class="title-page-name"/>    .master("local[*]")<br class="title-page-name"/>    .config("spark.sql.warehouse.dir", "E:/Exp/")<br class="title-page-name"/>    .appName("Testing")<br class="title-page-name"/>    .getOrCreate()    <br class="title-page-name"/>  val fileName = "data/words.txt";<br class="title-page-name"/>  def myWordCounter(fileName: String): Long = {<br class="title-page-name"/>    val input = spark.sparkContext.textFile(fileName)<br class="title-page-name"/>    val counts = input.flatMap(_.split(" ")).distinct()<br class="title-page-name"/>    val counter = counts.count()<br class="title-page-name"/>    counter<br class="title-page-name"/>  }<br class="title-page-name"/>  def main(args: Array[String]): Unit = {<br class="title-page-name"/>    val counter = myWordCounter(fileName)<br class="title-page-name"/>    println("Number of words: " + counter)<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">If you execute the preceding code, you should observe the following output: <kbd class="calibre11">Number of words: 214</kbd>. Fantastic! It really works as a local application. Now, test the preceding test case using Scala JUnit test case.</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.scalatest.Assertions._<br class="title-page-name"/>import org.junit.Test<br class="title-page-name"/>import org.apache.spark.sql.SparkSession<br class="title-page-name"/>class wordCountTest {<br class="title-page-name"/>  val spark = SparkSession<br class="title-page-name"/>    .builder<br class="title-page-name"/>    .master("local[*]")<br class="title-page-name"/>    .config("spark.sql.warehouse.dir", "E:/Exp/")<br class="title-page-name"/>    .appName(s"OneVsRestExample")<br class="title-page-name"/>    .getOrCreate()   <br class="title-page-name"/>    @Test def test() {<br class="title-page-name"/>      val fileName = "data/words.txt"<br class="title-page-name"/>      val obj = new wordCounterTestDemo()<br class="title-page-name"/>      assert(obj.myWordCounter(fileName) == 214)<br class="title-page-name"/>           }<br class="title-page-name"/>    spark.stop()<br class="title-page-name"/>}
</pre>
<p class="mce-root">If you see the earlier code carefully, I have used the <kbd class="calibre11">Test</kbd> annotation before the <kbd class="calibre11">test()</kbd> method. Inside the <kbd class="calibre11">test()</kbd> method, I invoked the <kbd class="calibre11">assert()</kbd> method, where the actual testing occurs. Here we tried to check if the return value of the <kbd class="calibre11">myWordCounter()</kbd> method is equal to 214. Now run the earlier code as a Scala Unit test as follows (<em class="calibre8">Figure 7</em>):</p>
<div><img class="image-border263" src="img/00151.jpeg"/></div>
<div><strong class="calibre1">Figure 7:</strong> Running Scala code as Scala JUnit Test</div>
<p class="mce-root">Now if the test case passes, you should observe the following output on your Eclipse IDE (<em class="calibre8">Figure 8</em>):</p>
<div><img class="image-border264" src="img/00173.jpeg"/></div>
<div><strong class="calibre1">Figure 8:</strong> Word count test case passed</div>
<p class="mce-root">Now, for example, try to assert in the following way:</p>
<pre class="calibre19">
assert(obj.myWordCounter(fileName) == 210)
</pre>
<p class="mce-root">If the preceding test case fails, you should observe the following output (<em class="calibre8">Figure 9</em>):</p>
<div><img class="image-border265" src="img/00299.jpeg"/></div>
<div><strong class="calibre1">Figure 9:</strong> Test case failed</div>
<p class="mce-root">Now let's have a look at method 2 and how it helps us for the betterment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Method 2: Testing Scala code using FunSuite</h1>
                
            
            
                
<p class="mce-root">Now, let's redesign the preceding test case by returning only the RDD of the texts in the document, as follows:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.apache.spark._<br class="title-page-name"/>import org.apache.spark.rdd.RDD<br class="title-page-name"/>import org.apache.spark.sql.SparkSession<br class="title-page-name"/>class wordCountRDD {<br class="title-page-name"/>  def prepareWordCountRDD(file: String, spark: SparkSession): RDD[(String, Int)] = {<br class="title-page-name"/>    val lines = spark.sparkContext.textFile(file)<br class="title-page-name"/>    lines.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _)<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">So, the <kbd class="calibre11">prepareWordCountRDD()</kbd> method in the preceding class returns an RDD of string and integer values. Now, if we want to test the <kbd class="calibre11">prepareWordCountRDD()</kbd> method's functionality, we can do it more explicit by extending the test class with <kbd class="calibre11">FunSuite</kbd> and <kbd class="calibre11">BeforeAndAfterAll</kbd> from the <kbd class="calibre11">ScalaTest</kbd> package of Scala. The testing works in the following ways:</p>
<ul class="calibre9">
<li class="mce-root1">Extend the test class with <kbd class="calibre11">FunSuite</kbd> and <kbd class="calibre11">BeforeAndAfterAll</kbd> from the <kbd class="calibre11">ScalaTest</kbd> package of Scala</li>
<li class="mce-root1">Override the <kbd class="calibre11">beforeAll()</kbd> that creates Spark context</li>
<li class="mce-root1">Perform the test using the <kbd class="calibre11">test()</kbd> method and use the <kbd class="calibre11">assert()</kbd> method inside the <kbd class="calibre11">test()</kbd> method</li>
<li class="mce-root1">Override the <kbd class="calibre11">afterAll()</kbd> method that stops the Spark context</li>
</ul>
<p class="mce-root">Based on the preceding steps, let's see a class for testing the preceding <kbd class="calibre11">prepareWordCountRDD()</kbd> method:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.scalatest.{ BeforeAndAfterAll, FunSuite }<br class="title-page-name"/>import org.scalatest.Assertions._<br class="title-page-name"/>import org.apache.spark.sql.SparkSession<br class="title-page-name"/>import org.apache.spark.rdd.RDD<br class="title-page-name"/>class wordCountTest2 extends FunSuite with BeforeAndAfterAll {<br class="title-page-name"/>  var spark: SparkSession = null<br class="title-page-name"/>  def tokenize(line: RDD[String]) = {<br class="title-page-name"/>    line.map(x =&gt; x.split(' ')).collect()<br class="title-page-name"/>  }<br class="title-page-name"/>  override def beforeAll() {<br class="title-page-name"/>    spark = SparkSession<br class="title-page-name"/>      .builder<br class="title-page-name"/>      .master("local[*]")<br class="title-page-name"/>      .config("spark.sql.warehouse.dir", "E:/Exp/")<br class="title-page-name"/>      .appName(s"OneVsRestExample")<br class="title-page-name"/>      .getOrCreate()<br class="title-page-name"/>  }  <br class="title-page-name"/>  test("Test if two RDDs are equal") {<br class="title-page-name"/>    val input = List("To be,", "or not to be:", "that is the question-", "William Shakespeare")<br class="title-page-name"/>    val expected = Array(Array("To", "be,"), Array("or", "not", "to", "be:"), Array("that", "is", "the", "question-"), Array("William", "Shakespeare"))<br class="title-page-name"/>    val transformed = tokenize(spark.sparkContext.parallelize(input))<br class="title-page-name"/>    assert(transformed === expected)<br class="title-page-name"/>  }  <br class="title-page-name"/>  test("Test for word count RDD") {<br class="title-page-name"/>    val fileName = "C:/Users/rezkar/Downloads/words.txt"<br class="title-page-name"/>    val obj = new wordCountRDD<br class="title-page-name"/>    val result = obj.prepareWordCountRDD(fileName, spark)    <br class="title-page-name"/>    assert(result.count() === 214)<br class="title-page-name"/>  }<br class="title-page-name"/>  override def afterAll() {<br class="title-page-name"/>    spark.stop()<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">The first test says that if two RDDs materialize in two different ways, the contents should be the same. Thus, the first test should get passed. We will see this in following example. Now, for the second test, as we have seen previously, the word count of RDD is 214, but let's assume it unknown for a while. If it's 214 coincidentally, the test case should pass, which is its expected behavior.</p>
<p class="mce-root">Thus, we are expecting both tests to be passed. Now, on Eclipse, run the test suite as <kbd class="calibre11">ScalaTest-File</kbd>, as shown in the following figure:</p>
<div><img class="image-border266" src="img/00342.jpeg"/><strong class="calibre1"><br class="title-page-name"/>
Figure 10:</strong> running the test suite as ScalaTest-File</div>
<p class="mce-root">Now you should observe the following output (<em class="calibre8">Figure 11</em>). The output shows how many test cases we performed and how many of them passed, failed, canceled, ignored, or were (was) in pending. It also shows the time to execute the overall test.</p>
<div><img class="image-border267" src="img/00268.jpeg"/></div>
<div><strong class="calibre1">Figure 11:</strong> Test result when running the two test suites as ScalaTest-file</div>
<p class="mce-root">Fantastic! The test case passed. Now, let's try changing the compare value in the assertion in the two separate tests using the <kbd class="calibre11">test()</kbd> method as follows:</p>
<pre class="calibre19">
test("Test for word count RDD") { <br class="title-page-name"/>  val fileName = "data/words.txt"<br class="title-page-name"/>  val obj = new wordCountRDD<br class="title-page-name"/>  val result = obj.prepareWordCountRDD(fileName, spark)    <br class="title-page-name"/>  assert(result.count() === 210)<br class="title-page-name"/>}<br class="title-page-name"/>test("Test if two RDDs are equal") {<br class="title-page-name"/>  val input = List("To be", "or not to be:", "that is the question-", "William Shakespeare")<br class="title-page-name"/>  val expected = Array(Array("To", "be,"), Array("or", "not", "to", "be:"), Array("that", "is", "the", "question-"), Array("William", "Shakespeare"))<br class="title-page-name"/>  val transformed = tokenize(spark.sparkContext.parallelize(input))<br class="title-page-name"/>  assert(transformed === expected)<br class="title-page-name"/>}
</pre>
<p class="mce-root">Now, you should expect that the test case will be failed. Now run the earlier class as <kbd class="calibre11">ScalaTest-File</kbd> (<em class="calibre8">Figure 12</em>):</p>
<div><img class="image-border31" src="img/00029.jpeg"/></div>
<div><strong class="calibre1">Figure 12:</strong> Test result when running the preceding two test suites as ScalaTest-File</div>
<p class="mce-root">Well done! We have learned how to perform the unit testing using Scala's FunSuite. However, if you evaluate the preceding method carefully, you should agree that there are several disadvantages. For example, you need to ensure an explicit management of <kbd class="calibre11">SparkContext</kbd> creation and destruction. As a developer or programmer, you have to write more lines of code for testing a sample method. Sometimes, code duplication occurs as the <em class="calibre8">Before</em> and the <em class="calibre8">After</em> step has to be repeated in all test suites. However, this is debatable since the common code could be put in a common trait.</p>
<p class="mce-root">Now the question is how could we improve our experience? My recommendation is using the Spark testing base to make life easier and more straightforward. We will discuss how we could perform the unit testing the Spark testing base.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Method 3: Making life easier with Spark testing base</h1>
                
            
            
                
<p class="mce-root">Spark testing base helps you to test your most of the Spark codes with ease. So, what are the pros of this method then? There are many in fact. For example, using this the code is not verbose but we can get very succinct code. The API is itself richer than that of ScalaTest or JUnit. Multiple languages support, for example, Scala, Java, and Python. It has the support of built-in RDD comparators. You can also use it for testing streaming applications. And finally and most importantly, it supports both local and cluster mode testings. This is most important for the testing in a distributed environment.</p>
<p>The GitHub repo is located at <a href="https://github.com/holdenk/spark-testing-base" class="calibre21">https://github.com/holdenk/spark-testing-base</a>.</p>
<p class="mce-root">Before starting the unit testing with Spark testing base, you should include the following dependency in the Maven friendly <kbd class="calibre11">pom.xml</kbd> file in your project tree for Spark 2.x as follows:</p>
<pre class="calibre19">
&lt;dependency&gt;<br class="title-page-name"/>  &lt;groupId&gt;com.holdenkarau&lt;/groupId&gt;<br class="title-page-name"/>  &lt;artifactId&gt;spark-testing-base_2.10&lt;/artifactId&gt;<br class="title-page-name"/>  &lt;version&gt;2.0.0_0.6.0&lt;/version&gt;<br class="title-page-name"/>&lt;/dependency&gt;
</pre>
<p class="mce-root">For SBT, you can add the following dependency:</p>
<pre class="calibre19">
"com.holdenkarau" %% "spark-testing-base" % "2.0.0_0.6.0"
</pre>
<p class="mce-root">Note that it is recommended to add the preceding dependency in the <kbd class="calibre11">test</kbd> scope by specifying <kbd class="calibre11">&lt;scope&gt;test&lt;/scope&gt;</kbd> for both the Maven and SBT cases. In addition to these, there are other considerations such as memory requirements and OOMs and disabling the parallel execution. The default Java options in the SBT testing are too small to support for running multiple tests. Sometimes it's harder to test Spark codes if the job is submitted in local mode! Now you can naturally understand how difficult it would be in a real cluster mode -i.e. YARN or Mesos.</p>
<p class="mce-root">To get rid of this problem, you can increase the amount of memory in your <kbd class="calibre11">build.sbt</kbd> file in your project tree. Just add the following parameters as follows:</p>
<pre class="calibre19">
<strong class="calibre1">javaOptions ++= Seq("-Xms512M", "-Xmx2048M", "-XX:MaxPermSize=2048M", "-XX:+CMSClassUnloadingEnabled")</strong>
</pre>
<p class="mce-root">However, if you are using Surefire, you can add the following:</p>
<pre class="calibre19">
<strong class="calibre1">&lt;argLine&gt;-Xmx2048m -XX:MaxPermSize=2048m&lt;/argLine&gt;</strong>
</pre>
<p class="mce-root">In your Maven-based build, you can make it by setting the value in the environmental variable. For more on this issue, refer to <a href="https://maven.apache.org/configure.html" class="calibre10">https://maven.apache.org/configure.html</a>.</p>
<p class="mce-root">This is just an example to run spark testing base's own tests. Therefore, you might need to set bigger value. Finally, make sure that you have disabled the parallel execution in your SBT by adding the following line of code:</p>
<pre class="calibre19">
parallelExecution in Test := false
</pre>
<p class="mce-root">On the other hand, if you're using surefire, make sure that <kbd class="calibre11">forkCount</kbd> and <kbd class="calibre11">reuseForks</kbd> are set as 1 and true, respectively. Let's see an example of using Spark testing base. The following source code has three test cases. The first test case is the dummy that compares if 1 is equal to 1 or not, which obviously will be passed. The second test case counts the number of words from the sentence, say <kbd class="calibre11">Hello world! My name is Reza</kbd>, and compares if this has six words or not. The final and the last test case tries to compare two RDDs:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.scalatest.Assertions._<br class="title-page-name"/>import org.apache.spark.rdd.RDD<br class="title-page-name"/>import com.holdenkarau.spark.testing.SharedSparkContext<br class="title-page-name"/>import org.scalatest.FunSuite<br class="title-page-name"/>class TransformationTestWithSparkTestingBase extends FunSuite with SharedSparkContext {<br class="title-page-name"/>  def tokenize(line: RDD[String]) = {<br class="title-page-name"/>    line.map(x =&gt; x.split(' ')).collect()<br class="title-page-name"/>  }<br class="title-page-name"/>  test("works, obviously!") {<br class="title-page-name"/>    assert(1 == 1)<br class="title-page-name"/>  }<br class="title-page-name"/>  test("Words counting") {<br class="title-page-name"/>    assert(sc.parallelize("Hello world My name is Reza".split("\\W")).map(_ + 1).count == 6)<br class="title-page-name"/>  }<br class="title-page-name"/>  test("Testing RDD transformations using a shared Spark Context") {<br class="title-page-name"/>    val input = List("Testing", "RDD transformations", "using a shared", "Spark Context")<br class="title-page-name"/>    val expected = Array(Array("Testing"), Array("RDD", "transformations"), Array("using", "a", "shared"), Array("Spark", "Context"))<br class="title-page-name"/>    val transformed = tokenize(sc.parallelize(input))<br class="title-page-name"/>    assert(transformed === expected)<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">From the preceding source code, we can see that we can perform multiple test cases using Spark testing base. Upon successful execution, you should observe the following output (<em class="calibre8">Figure 13</em>):</p>
<div><img class="image-border268" src="img/00280.jpeg"/></div>
<div><img class="image-border269" src="img/00093.jpeg"/></div>
<div><strong class="calibre1">Figure 13:</strong> A successful execution and passed test using Spark testing base</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Configuring Hadoop runtime on Windows</h1>
                
            
            
                
<p class="mce-root">We have already seen how to test your Spark applications written in Scala on Eclipse or IntelliJ, but there is another potential issue that should not be overlooked. Although Spark works on Windows, Spark is designed to be run on the UNIX-like operating system. Therefore, if you are working on Windows environment, then extra care needs to be taken.</p>
<p class="mce-root">While using Eclipse or IntelliJ to develop your Spark applications for solving data analytics, machine learning, data science, or deep learning applications on Windows, you might face an I/O exception error and your application might not compile successfully or may be interrupted. Actually, the thing is that Spark expects that there is a runtime environment for Hadoop on Windows too. For example, if you run a Spark application, say <kbd class="calibre11">KMeansDemo.scala</kbd>, on Eclipse for the first time, you will experience an I/O exception saying the following:</p>
<pre class="calibre19">
<strong class="calibre1">17/02/26 13:22:00 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.</strong>
</pre>
<p class="mce-root">The reason is that by default, Hadoop is developed for the Linux environment, and if you are developing your Spark applications on Windows platform, a bridge is required that will provide an environment for the Hadoop runtime for Spark to be properly executed. The details of the I/O exception can be seen in the following figure:</p>
<div><img class="image-border270" src="img/00088.gif"/></div>
<div><strong class="calibre1">Figure 14:</strong> I/O exception occurred due to the failure of not to locate the winutils binary in the Hadoop binary path</div>
<p class="mce-root">Now, how to get rid of this problem then? The solution is straightforward. As the error message says, we need to have an executable, namely <kbd class="calibre11">winutils.exe</kbd>. Now download the <kbd class="calibre11">winutils.exe</kbd> file from <a href="https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin" class="calibre10">https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin</a>, paste it in the Spark distribution directory, and configure Eclipse. More specifically, suppose your Spark distribution containing Hadoop is located at <kbd class="calibre11">C:/Users/spark-2.1.0-bin-hadoop2.7</kbd>. Inside the Spark distribution, there is a directory named bin. Now, paste the executable there (that is, <kbd class="calibre11">path = C:/Users/spark-2.1.0-binhadoop2.7/bin/</kbd>).</p>
<p class="mce-root">The second phase of the solution is going to Eclipse and then selecting the main class (that is, <kbd class="calibre11">KMeansDemo.scala</kbd> in this case), and then going to the Run menu. From the Run menu, go to the Run Configurations option and from there select the Environment tab, as shown in the following figure:</p>
<div><img class="image-border271" src="img/00049.jpeg"/></div>
<div><strong class="calibre1">Figure 15:</strong> Solving the I/O exception occurred due to the absence of winutils binary in the Hadoop binary path</div>
<p class="mce-root">If you select the tab, you a will have the option to create a new environmental variable for Eclipse suing the JVM. Now create a new environmental variable named <kbd class="calibre11">HADOOP_HOME</kbd> and put the value as <kbd class="calibre11">C:/Users/spark-2.1.0-bin-hadoop2.7/</kbd>. Now press on Apply button and rerun your application, and your problem should be resolved.</p>
<p class="mce-root">It is to be noted that while working with Spark on Windows in a PySpark, the <kbd class="calibre11">winutils.exe</kbd> file is required too. For PySpark reference, refer to the <a href="part0571.html#H0HH61-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 19</a>, <em class="calibre8">PySpark and SparkR</em>.</p>
<p class="mce-root">Please make a note that the preceding solution is also applicable in debugging your applications. Sometimes, even if the preceding error occurs, your Spark application will run properly. However, if the size of the dataset is large, it is most likely that the preceding error will occur.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Debugging Spark applications</h1>
                
            
            
                
<p class="mce-root">In this section, we will see how to debug Spark applications that are running locally (on Eclipse or IntelliJ), standalone or cluster mode in YARN or Mesos. However, before diving deeper, it is necessary to know about logging in the Spark application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Logging with log4j with Spark recap</h1>
                
            
            
                
<p class="mce-root">We have already discussed this topic in <a href="part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 14</a>, <em class="calibre8">Time to Put Some Order - Cluster Your Data with Spark MLlib</em>. However, let's replay the same contents to make your brain align with the current discussion <em class="calibre8">Debugging Spark applications</em>. As stated earlier, Spark uses log4j for its own logging. If you configured Spark properly, Spark gets logged all the operation to the shell console. A sample snapshot of the file can be seen from the following figure:</p>
<div><img class="image-border272" src="img/00259.jpeg"/></div>
<div><strong class="calibre1">Figure 16:</strong> A snap of the log4j.properties file</div>
<p class="mce-root">Set the default spark-shell log level to WARN. When running the spark-shell, the log level for this class is used to overwrite the root logger's log level so that the user can have different defaults for the shell and regular Spark apps. We also need to append JVM arguments when launching a job executed by an executor and managed by the driver. For this, you should edit the <kbd class="calibre11">conf/spark-defaults.conf</kbd>. In short, the following options can be added:</p>
<pre class="calibre19">
spark.executor.extraJavaOptions=-Dlog4j.configuration=file:/usr/local/spark-2.1.1/conf/log4j.properties spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/usr/local/spark-2.1.1/conf/log4j.properties
</pre>
<p class="mce-root">To make the discussion clearer, we need to hide all the logs generated by Spark. We then can redirect them to be logged in the file system. On the other hand, we want our own logs to be logged in the shell and a separate file so that they don't get mixed up with the ones from Spark. From here, we will point Spark to the files where our own logs are, which in this particular case is <kbd class="calibre11">/var/log/sparkU.log</kbd>. This <kbd class="calibre11">log4j.properties</kbd> file is then picked up by Spark when the application starts, so we don't have to do anything aside of placing it in the mentioned location:</p>
<pre class="calibre19">
package com.chapter14.Serilazition<br class="title-page-name"/>import org.apache.log4j.LogManager<br class="title-page-name"/>import org.apache.log4j.Level<br class="title-page-name"/>import org.apache.spark.sql.SparkSession<br class="title-page-name"/>object myCustomLog {<br class="title-page-name"/>  def main(args: Array[String]): Unit = {   <br class="title-page-name"/>    val log = LogManager.getRootLogger    <br class="title-page-name"/>    //Everything is printed as INFO once the log level is set to INFO untill you set the level to new level for example WARN. <br class="title-page-name"/>    log.setLevel(Level.INFO)<br class="title-page-name"/>    log.info("Let's get started!")    <br class="title-page-name"/>    // Setting logger level as WARN: after that nothing prints other than WARN<br class="title-page-name"/>    log.setLevel(Level.WARN)    <br class="title-page-name"/>    // Creating Spark Session<br class="title-page-name"/>    val spark = SparkSession<br class="title-page-name"/>      .builder<br class="title-page-name"/>      .master("local[*]")<br class="title-page-name"/>      .config("spark.sql.warehouse.dir", "E:/Exp/")<br class="title-page-name"/>      .appName("Logging")<br class="title-page-name"/>      .getOrCreate()<br class="title-page-name"/>    // These will note be printed!<br class="title-page-name"/>    log.info("Get prepared!")<br class="title-page-name"/>    log.trace("Show if there is any ERROR!")<br class="title-page-name"/>    //Started the computation and printing the logging information<br class="title-page-name"/>    log.warn("Started")<br class="title-page-name"/>    spark.sparkContext.parallelize(1 to 20).foreach(println)<br class="title-page-name"/>    log.warn("Finished")<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">In the preceding code, everything is printed as INFO once the log level is set to <kbd class="calibre11">INFO</kbd> until you set the level to new level for example <kbd class="calibre11">WARN</kbd>. However, after that no info or trace and so on, that will note be printed. In addition to that, there are several valid logging levels supported by log4j with Spark. The successful execution of the preceding code should generate the following output:</p>
<pre class="calibre19">
<strong class="calibre1">17/05/13 16:39:14 INFO root: Let's get started!</strong><br class="title-page-name"/><strong class="calibre1">17/05/13 16:39:15 WARN root: Started</strong><br class="title-page-name"/><strong class="calibre1">4 </strong><br class="title-page-name"/><strong class="calibre1">1 </strong><br class="title-page-name"/><strong class="calibre1">2 </strong><br class="title-page-name"/><strong class="calibre1">5 </strong><br class="title-page-name"/><strong class="calibre1">3                                                                          </strong><br class="title-page-name"/><strong class="calibre1">17/05/13 16:39:16 WARN root: Finished</strong>
</pre>
<p class="mce-root">You can also set up the default logging for Spark shell in <kbd class="calibre11">conf/log4j.properties</kbd>. Spark provides a template of the log4j as a property file, and we can extend and modify that file for logging in Spark. Move to the <kbd class="calibre11">SPARK_HOME/conf</kbd> directory and you should see the <kbd class="calibre11">log4j.properties.template</kbd> file. You should use the following <kbd class="calibre11">conf/log4j.properties.template</kbd> after renaming it to <kbd class="calibre11">log4j.properties</kbd>. While developing your Spark application, you can put the <kbd class="calibre11">log4j.properties</kbd> file under your project directory while working on an IDE-based environment such as Eclipse. However, to disable logging completely, just set the <kbd class="calibre11">log4j.logger.org</kbd> flags as <kbd class="calibre11">OFF</kbd> as follows:</p>
<pre class="calibre19">
log4j.logger.org=OFF
</pre>
<p class="mce-root">So far, everything is very easy. However, there is a problem we haven't noticed yet in the preceding code segment. One drawback of the <kbd class="calibre11">org.apache.log4j.Logger</kbd> class is that it is not serializable, which implies that we cannot use it inside a closure while doing operations on some parts of the Spark API. For example, suppose we do the following in our Spark code:</p>
<pre class="calibre19">
object myCustomLogger {<br class="title-page-name"/>  def main(args: Array[String]):Unit= {<br class="title-page-name"/>    // Setting logger level as WARN<br class="title-page-name"/>    val log = LogManager.getRootLogger<br class="title-page-name"/>    log.setLevel(Level.WARN)<br class="title-page-name"/>    // Creating Spark Context<br class="title-page-name"/>    val conf = new SparkConf().setAppName("My App").setMaster("local[*]")<br class="title-page-name"/>    val sc = new SparkContext(conf)<br class="title-page-name"/>    //Started the computation and printing the logging information<br class="title-page-name"/>    //log.warn("Started")<br class="title-page-name"/>    val i = 0<br class="title-page-name"/>    val data = sc.parallelize(i to 100000)<br class="title-page-name"/>    data.map{number =&gt;<br class="title-page-name"/>      log.info(“My number”+ i)<br class="title-page-name"/>      number.toString<br class="title-page-name"/>    }<br class="title-page-name"/>    //log.warn("Finished")<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">You should experience an exception that says <kbd class="calibre11">Task</kbd> not serializable as follows:</p>
<pre class="calibre19">
<strong class="calibre1">org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: ...</strong><br class="title-page-name"/><strong class="calibre1">Exception in thread "main" org.apache.spark.SparkException: Task not serializable </strong><br class="title-page-name"/><strong class="calibre1">Caused by: java.io.NotSerializableException: org.apache.log4j.spi.RootLogger</strong><br class="title-page-name"/><strong class="calibre1">Serialization stack: object not serializable</strong>
</pre>
<p class="mce-root">At first, we can try to solve this problem in a naive way. What you can do is just make the Scala class (that does the actual operation) <kbd class="calibre11">Serializable</kbd> using <kbd class="calibre11">extends Serializable</kbd> . For example, the code looks as follows:</p>
<pre class="calibre19">
class MyMapper(n: Int) extends Serializable {<br class="title-page-name"/>  @transient lazy val log = org.apache.log4j.LogManager.getLogger("myLogger")<br class="title-page-name"/>  def logMapper(rdd: RDD[Int]): RDD[String] =<br class="title-page-name"/>    rdd.map { i =&gt;<br class="title-page-name"/>      log.warn("mapping: " + i)<br class="title-page-name"/>      (i + n).toString<br class="title-page-name"/>    }<br class="title-page-name"/>  }
</pre>
<p>This section is intended for carrying out a discussion on logging. However, we take the opportunity to make it more versatile for general purpose Spark programming and issues. In order to overcome the <kbd class="calibre22">task not serializable</kbd> error in a more efficient way, compiler will try to send the whole object (not only the lambda) by making it serializable and forces SPark to accept that. However, it increases shuffling significantly, especially for big objects! The other ways are making the whole class <kbd class="calibre22">Serializable</kbd> or by declaring the instance only within the lambda function passed in the map operation. Sometimes, keeping the not <kbd class="calibre22">Serializable</kbd> objects across the nodes can work. Lastly, use the <kbd class="calibre22">forEachPartition()</kbd> or <kbd class="calibre22">mapPartitions()</kbd> instead of just <kbd class="calibre22">map()</kbd> and create the not <kbd class="calibre22">Serializable</kbd> objects. In summary, these are the ways to solve the problem around:<br class="calibre23"/>
<ul class="calibre41">
<li class="calibre42">Serializable the class</li>
<li class="calibre42">Declare the instance only within the lambda function passed in the map</li>
<li class="calibre42">Make the NotSerializable object as a static and create it once per machine</li>
<li class="calibre42">Call the <kbd class="calibre22">forEachPartition ()</kbd> or <kbd class="calibre22">mapPartitions()</kbd> instead of <kbd class="calibre22">map()</kbd> and create the NotSerializable object</li>
</ul>
</p>
<p class="mce-root">In the preceding code, we have used the annotation <kbd class="calibre11">@transient lazy</kbd>, which marks the <kbd class="calibre11">Logger</kbd> class to be nonpersistent. On the other hand, object containing the method apply (i.e. <kbd class="calibre11">MyMapperObject</kbd>) that instantiate the object of the <kbd class="calibre11">MyMapper</kbd> class is as follows:</p>
<pre class="calibre19">
//Companion object <br class="title-page-name"/>object MyMapper {<br class="title-page-name"/>  def apply(n: Int): MyMapper = new MyMapper(n)<br class="title-page-name"/>}
</pre>
<p class="mce-root">Finally, the object containing the <kbd class="calibre11">main()</kbd> method is as follows:</p>
<pre class="calibre19">
//Main object<br class="title-page-name"/>object myCustomLogwithClosureSerializable {<br class="title-page-name"/>  def main(args: Array[String]) {<br class="title-page-name"/>    val log = LogManager.getRootLogger<br class="title-page-name"/>    log.setLevel(Level.WARN)<br class="title-page-name"/>    val spark = SparkSession<br class="title-page-name"/>      .builder<br class="title-page-name"/>      .master("local[*]")<br class="title-page-name"/>      .config("spark.sql.warehouse.dir", "E:/Exp/")<br class="title-page-name"/>      .appName("Testing")<br class="title-page-name"/>      .getOrCreate()<br class="title-page-name"/>    log.warn("Started")<br class="title-page-name"/>    val data = spark.sparkContext.parallelize(1 to 100000)<br class="title-page-name"/>    val mapper = MyMapper(1)<br class="title-page-name"/>    val other = mapper.logMapper(data)<br class="title-page-name"/>    other.collect()<br class="title-page-name"/>    log.warn("Finished")<br class="title-page-name"/>  }
</pre>
<p class="mce-root">Now, let's see another example that provides better insight to keep fighting the issue we are talking about. Suppose we have the following class that computes the multiplication of two integers:</p>
<pre class="calibre19">
class MultiplicaitonOfTwoNumber {<br class="title-page-name"/>  def multiply(a: Int, b: Int): Int = {<br class="title-page-name"/>    val product = a * b<br class="title-page-name"/>    product<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">Now, essentially, if you try to use this class for computing the multiplication in the lambda closure using <kbd class="calibre11">map()</kbd>, you will get the <kbd class="calibre11">Task Not Serializable</kbd> error that we described earlier. Now we simply can use <kbd class="calibre11">foreachPartition()</kbd> and the lambda inside as follows:</p>
<pre class="calibre19">
val myRDD = spark.sparkContext.parallelize(0 to 1000)<br class="title-page-name"/>    myRDD.foreachPartition(s =&gt; {<br class="title-page-name"/>      val notSerializable = new MultiplicaitonOfTwoNumber<br class="title-page-name"/>      println(notSerializable.multiply(s.next(), s.next()))<br class="title-page-name"/>    })
</pre>
<p class="mce-root">Now, if you compile it, it should return the desired result. For your ease, the complete code with the <kbd class="calibre11">main()</kbd> method is as follows:</p>
<pre class="calibre19">
package com.chapter16.SparkTesting<br class="title-page-name"/>import org.apache.spark.sql.SparkSession<br class="title-page-name"/>class MultiplicaitonOfTwoNumber {<br class="title-page-name"/>  def multiply(a: Int, b: Int): Int = {<br class="title-page-name"/>    val product = a * b<br class="title-page-name"/>    product<br class="title-page-name"/>  }<br class="title-page-name"/>}<br class="title-page-name"/>object MakingTaskSerilazible {<br class="title-page-name"/>  def main(args: Array[String]): Unit = {<br class="title-page-name"/>    val spark = SparkSession<br class="title-page-name"/>      .builder<br class="title-page-name"/>      .master("local[*]")<br class="title-page-name"/>      .config("spark.sql.warehouse.dir", "E:/Exp/")<br class="title-page-name"/>      .appName("MakingTaskSerilazible")<br class="title-page-name"/>      .getOrCreate()<br class="title-page-name"/> val myRDD = spark.sparkContext.parallelize(0 to 1000)<br class="title-page-name"/>    myRDD.foreachPartition(s =&gt; {<br class="title-page-name"/>      val notSerializable = new MultiplicaitonOfTwoNumber<br class="title-page-name"/>      println(notSerializable.multiply(s.next(), s.next()))<br class="title-page-name"/>    })<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">The output is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">0</strong><br class="title-page-name"/><strong class="calibre1">5700</strong><br class="title-page-name"/><strong class="calibre1">1406</strong><br class="title-page-name"/><strong class="calibre1">156</strong><br class="title-page-name"/><strong class="calibre1">4032</strong><br class="title-page-name"/><strong class="calibre1">7832</strong><br class="title-page-name"/><strong class="calibre1">2550</strong><br class="title-page-name"/><strong class="calibre1">650</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Debugging the Spark application</h1>
                
            
            
                
<p class="mce-root">In this section, we will discuss how to debug Spark applications running on locally on Eclipse or IntelliJ, as standalone or cluster mode in YARN or Mesos. Before getting started, you can also read the debugging documentation at <a href="https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/" class="calibre10">﻿https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Debugging Spark application on Eclipse as Scala debug</h1>
                
            
            
                
<p class="mce-root">To make this happen, just configure your Eclipse to debug your Spark applications as a regular Scala code debug. To configure select Run | Debug Configuration | Scala Application as shown in the following figure:</p>
<div><img class="image-border31" src="img/00022.jpeg"/></div>
<div><strong class="calibre1">Figure 17:</strong> Configuring Eclipse to debug Spark applications as a regular Scala code debug</div>
<p class="mce-root">Suppose we want to debug our <kbd class="calibre11">KMeansDemo.scala</kbd> and ask Eclipse (you can have similar options on InteliJ IDE) to start the execution at line 56 and set the breakpoint in line 95. To do so, run your Scala code as debugging and you should observe the following scenario on Eclipse:</p>
<div><img class="image-border31" src="img/00327.jpeg"/></div>
<div><strong class="calibre1">Figure 18:</strong> Debugging Spark applications on Eclipse</div>
<p class="mce-root">Then, Eclipse will pause on the line you ask it to stop the execution in line 95, as shown in the following screenshot:</p>
<div><img class="image-border273" src="img/00221.jpeg"/></div>
<div><strong class="calibre1">Figure 19:</strong> Debugging Spark applications on Eclipse (breakpoint)</div>
<p class="mce-root">In summary, to simplify the preceding example, if there is any error between line 56 and line 95, Eclipse will show where the error actually occurs. Otherwise, it will follow the normal workflow if not interrupted.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Debugging Spark jobs running as local and standalone mode</h1>
                
            
            
                
<p class="mce-root">While debugging your Spark application locally or as standalone mode, you should know that debugging the driver program and debugging one of the executors is different since using these two types of nodes requires different submission parameters passed to <kbd class="calibre11">spark-submit</kbd>. Throughout this section, I'll use port 4000 as the address. For example, if you want to debug the driver program, you can add the following to your <kbd class="calibre11">spark-submit</kbd> command:</p>
<pre class="calibre19">
<strong class="calibre1">--driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000</strong>
</pre>
<p class="mce-root">After that, you should set your remote debugger to connect to the node where you have submitted the driver program. For the preceding case, port number 4000 was specified. However, if something (that is, other Spark jobs, other applications or services, and so on) is already running on that port, you might also need to customize that port, that is, change the port number.</p>
<p class="mce-root">On the other hand, connecting to an executor is similar to the preceding option, except for the address option. More specifically, you will have to replace the address with your local machine's address (IP address or host name with the port number). However, it is always a good practice and recommended to test that you can access your local machine from the Spark cluster where the actual computing occurs. For example, you can use the following options to make the debugging environment enable to your <kbd class="calibre11">spark-submit</kbd> command:</p>
<p class="mce-root"> </p>
<pre class="calibre19">
<strong class="calibre1">--num-executors 1\<br class="title-page-name"/>--executor-cores 1 \<br class="title-page-name"/>--conf "spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=n,address=localhost:4000,suspend=n"</strong>
</pre>
<p class="mce-root">In summary, use the following command to submit your Spark jobs (the <kbd class="calibre11">KMeansDemo</kbd> application in this case):</p>
<p class="mce-root"> </p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-submit \<br class="title-page-name"/>--class "com.chapter13.Clustering.KMeansDemo" \<br class="title-page-name"/>--master spark://ubuntu:7077 \<br class="title-page-name"/>--num-executors 1\<br class="title-page-name"/>--executor-cores 1 \<br class="title-page-name"/>--conf "spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=n,address= host_name_to_your_computer.org:5005,suspend=n" \<br class="title-page-name"/>--driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000 \<br class="title-page-name"/> KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar \<br class="title-page-name"/>Saratoga_NY_Homes.txt</strong>
</pre>
<p class="mce-root">Now, start your local debugger in a listening mode and start your Spark program. Finally, wait for the executor to attach to your debugger. You will observe the following message on your terminal:</p>
<pre class="calibre19">
<strong class="calibre1">Listening for transport dt_socket at address: 4000 </strong>
</pre>
<p class="mce-root">It is important to know that you need to set the number of executors to 1 only. Setting multiple executors will all try to connect to your debugger and will eventually create some weird problems. It is to be noted that sometimes setting the <kbd class="calibre11">SPARK_JAVA_OPTS</kbd> helps in debugging your Spark applications that are running locally or as standalone mode. The command is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$ export SPARK_JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,address=4000,suspend=y,onuncaught=n</strong>
</pre>
<p class="mce-root">However, since Spark release 1.0.0, <kbd class="calibre11">SPARK_JAVA_OPTS</kbd> has been deprecated and replaced by <kbd class="calibre11">spark-defaults.conf</kbd> and command line arguments to Spark-submit or Spark-shell. It is also to be noted that setting <kbd class="calibre11">spark.driver.extraJavaOptions</kbd> and <kbd class="calibre11">spark.executor.extraJavaOptions</kbd>, which we saw in the previous section, in <kbd class="calibre11">spark-defaults.conf</kbd> is not a replacement for <kbd class="calibre11">SPARK_JAVA_OPTS</kbd>. But to be frank, <kbd class="calibre11">SPARK_JAVA_OPTS</kbd>, it still works pretty well and you can try as well.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Debugging Spark applications on YARN or Mesos cluster</h1>
                
            
            
                
<p class="mce-root">When you run a Spark application on YARN, there is an option that you can enable by modifying <kbd class="calibre11">yarn-env.sh</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">YARN_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4000 $YARN_OPTS"</strong>
</pre>
<p class="mce-root">Now, the remote debugging will be available through port 4000 on your Eclipse or IntelliJ IDE. The second option is by setting the <kbd class="calibre11">SPARK_SUBMIT_OPTS</kbd>. You can use either Eclipse or IntelliJ to develop your Spark applications that can be submitted to be executed on remote multinode YARN clusters. What I do is that I create a Maven project on Eclipse or IntelliJ and package my Java or Scala application as a jar file and then submit it as a Spark job. However, in order to attach your IDE such as Eclipse or IntelliJ debugger to your Spark application, you can define all the submission parameters using the <kbd class="calibre11">SPARK_SUBMIT_OPTS</kbd> environment variable as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$ export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000</strong>
</pre>
<p class="mce-root">Then submit your Spark job as follows (please change the values accordingly based on your requirements and setup):</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-submit \<br class="title-page-name"/>--class "com.chapter13.Clustering.KMeansDemo" \<br class="title-page-name"/>--master yarn \<br class="title-page-name"/>--deploy-mode cluster \<br class="title-page-name"/>--driver-memory 16g \<br class="title-page-name"/>--executor-memory 4g \<br class="title-page-name"/>--executor-cores 4 \<br class="title-page-name"/>--queue the_queue \<br class="title-page-name"/>--num-executors 1\<br class="title-page-name"/>--executor-cores 1 \<br class="title-page-name"/>--conf "spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=n,address= host_name_to_your_computer.org:4000,suspend=n" \<br class="title-page-name"/>--driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000 \<br class="title-page-name"/> KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar \<br class="title-page-name"/>Saratoga_NY_Homes.txt</strong>
</pre>
<p class="mce-root">After running the preceding command, it will wait until you connect your debugger, as shown in the following: <kbd class="calibre11">Listening for transport dt_socket at address: 4000</kbd>. Now you can configure your Java remote application (Scala application will work too) on the IntelliJ debugger, as shown in the following screenshot:</p>
<div><img class="image-border274" src="img/00331.jpeg"/></div>
<div><strong class="calibre1">Figure 20:</strong> Configuring remote debugger on IntelliJ</div>
<p class="mce-root">For the preceding case, 10.200.1.101 is the IP address of the remote computing node where your Spark job is basically running. Finally, you will have to start the debugger by clicking on Debug under IntelliJ's Run menu. Then, if the debugger connects to your remote Spark app, you will see the logging info in the application console on IntelliJ. Now if you can set the breakpoints and the rests of them are normal debugging. The following figure shows an example how will you see on the IntelliJ when pausing a Spark job with a breakpoint:</p>
<div><img class="image-border275" src="img/00007.jpeg"/></div>
<div><strong class="calibre1">Figure 21:</strong> An example how will you see on the IntelliJ when pausing a Spark job with a breakpoint</div>
<p class="mce-root">Although it works well, but sometimes I experienced that using <kbd class="calibre11">SPARK_JAVA_OPTS</kbd> won't help you much in the debug process on Eclipse or even IntelliJ. Instead, use and export <kbd class="calibre11">SPARK_WORKER_OPTS</kbd> and <kbd class="calibre11">SPARK_MASTER_OPTS</kbd> while running your Spark jobs on a real cluster (YARN, Mesos, or AWS) as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$ export SPARK_WORKER_OPTS="-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n"<br class="title-page-name"/>$ export SPARK_MASTER_OPTS="-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n"</strong>
</pre>
<p class="mce-root">Then start your Master node as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARKH_HOME/sbin/start-master.sh</strong>
</pre>
<p class="cdpalignleft1">Now open an SSH connection to your remote machine where the Spark job is actually running and map your localhost at 4000 (aka <kbd class="calibre11">localhost:4000</kbd>) to <kbd class="calibre11">host_name_to_your_computer.org:5000</kbd>, assuming the cluster is at <kbd class="calibre11">host_name_to_your_computer.org:5000</kbd> and listening on port 5000. Now that your Eclipse will consider that you're just debugging your Spark application as a local Spark application or process. However, to make this happen, you will have to configure the remote debugger on Eclipse, as shown in the following figure:</p>
<div><img class="image-border276" src="img/00141.jpeg"/></div>
<div><strong class="calibre1">Figure 22:</strong> Connecting remote host on Eclipse for debugging Spark application</div>
<p class="cdpalignleft1">That's it! Now you can debug on your live cluster as if it were your desktop. The preceding examples are for running with the Spark Master set as YARN-client. However, it should also work when running on a Mesos cluster. If you're running using YARN-cluster mode, you may have to set the driver to attach to your debugger rather than attaching your debugger to the driver since you won't necessarily know in advance what mode the driver will be executing on.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Debugging Spark application using SBT</h1>
                
            
            
                
<p class="mce-root">The preceding setting works mostly on Eclipse or IntelliJ using the Maven project. Suppose that you already have your application done and are working on your preferred IDEs such as IntelliJ or Eclipse as follows:</p>
<pre class="calibre19">
object DebugTestSBT {<br class="title-page-name"/>  def main(args: Array[String]): Unit = {<br class="title-page-name"/>    val spark = SparkSession<br class="title-page-name"/>      .builder<br class="title-page-name"/>      .master("local[*]")<br class="title-page-name"/>      .config("spark.sql.warehouse.dir", "C:/Exp/")<br class="title-page-name"/>      .appName("Logging")<br class="title-page-name"/>      .getOrCreate()      <br class="title-page-name"/>    spark.sparkContext.setCheckpointDir("C:/Exp/")<br class="title-page-name"/>    println("-------------Attach debugger now!--------------")<br class="title-page-name"/>    Thread.sleep(8000)<br class="title-page-name"/>    // code goes here, with breakpoints set on the lines you want to pause<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">Now, if you want to get this job to the local cluster (standalone), the very first step is packaging the application with all its dependencies into a fat JAR. For doing this, use the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ sbt assembly</strong>
</pre>
<p class="mce-root">This will generate the fat JAR. Now the task is to submit the Spark job to a local cluster. You need to have spark-submit script somewhere on your system:</p>
<pre class="calibre19">
<strong class="calibre1">$ export SPARK_JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005</strong>
</pre>
<p class="mce-root">The preceding command exports a Java argument that will be used to start Spark with the debugger:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-submit --class Test --master local[*] --driver-memory 4G --executor-memory 4G /path/project-assembly-0.0.1.jar</strong>
</pre>
<p class="mce-root">In the preceding command, <kbd class="calibre11">--class</kbd> needs to point to a fully qualified class path to your job. Upon successful execution of this command, your Spark job will be executed without breaking at the breakpoints. Now to get the debugging facility on your IDE, say IntelliJ, you need to configure to connect to the cluster. For more details on the official IDEA documentation, refer to <a href="http://stackoverflow.com/questions/21114066/attach-intellij-idea-debugger-to-a-running-java-process" class="calibre10">http://stackoverflow.com/questions/21114066/attach-intellij-idea-debugger-to-a-running-java-process</a>.</p>
<p class="mce-root">It is to be noted that if you just create a default remote run/debug configuration and leave the default port of 5005, it should work fine. Now, when you submit the job for the next time and see the message to attach the debugger, you have eight seconds to switch to IntelliJ IDEA and trigger this run configuration. The program will then continue to execute and pause at any breakpoint you defined. You can then step through it like any normal Scala/Java program. You can even step into Spark functions to see what it's doing under the hood.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, you saw how difficult the testing and debugging your Spark applications are. These can even be more critical in a distributed environment. We also discussed some advanced ways to tackle them altogether. In summary, you learned the way of testing in a distributed environment. Then you learned a better way of testing your Spark application. Finally, we discussed some advanced ways of debugging Spark applications.</p>
<p class="mce-root">We believe that this book will help you to gain some good understanding of Spark. Nevertheless, due to page limitation, we could not cover many APIs and their underlying functionalities. If you face any issues, please don't forget to report this to Spark user mailing list at <kbd class="calibre11">user@spark.apache.org</kbd>. Before doing so, make sure that you have subscribed to it.</p>
<p class="mce-root">This is more or less the end of our little journey with advanced topics on Spark. Now, a general suggestion from our side to you as readers or if you are relatively newer to the data science, data analytics, machine learning, Scala, or Spark is that you should at first try to understand what types of analytics you want to perform. To be more specific, for example, if your problem is a machine learning problem, try to guess what type of learning algorithms should be the best fit, that is, classification, clustering, regression, recommendation, or frequent pattern mining. Then define and formulate the problem, and after that, you should generate or download the appropriate data based on the feature engineering concept of Spark that we have discussed earlier. On the other hand, if you think that you can solve your problem using deep learning algorithms or APIs, you should use other third-party algorithms and integrate with Spark and work straight away.</p>
<p class="mce-root">Our final recommendation to the readers is to browse the Spark website (at <a href="http://spark.apache.org/" class="calibre10">http://spark.apache.org/</a>) regularly to get the updates and also try to incorporate the regular Spark-provided APIs with other third-party applications or tools to get the best result of the collaboration.</p>


            

            
        
    </body></html>