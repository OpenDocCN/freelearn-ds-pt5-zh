- en: Chapter 5. Learning Data Analytics with R and Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters we learned about the installation, configuration, and
    integration of R and Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to perform data analytics operations over
    an integrated R and Hadoop environment. Since this chapter is designed for data
    analytics, we will understand this with an effective data analytics cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data analytics project life cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data analytics problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the data analytics project life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While dealing with the data analytics projects, there are some fixed tasks that
    should be followed to get the expected output. So here we are going to build a
    data analytics project cycle, which will be a set of standard data-driven processes
    to lead data to insights effectively. The defined data analytics processes of
    a project life cycle should be followed by sequences for effectively achieving
    the goal using input datasets. This data analytics process may include identifying
    the data analytics problems, designing, and collecting datasets, data analytics,
    and data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data analytics project life cycle stages are seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the data analytics project life cycle](img/3282OS_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's get some perspective on these stages for performing data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Today, business analytics trends change by performing data analytics over web
    datasets for growing business. Since their data size is increasing gradually day
    by day, their analytical application needs to be scalable for collecting insights
    from their datasets.
  prefs: []
  type: TYPE_NORMAL
- en: With the help of web analytics, we can solve the business analytics problems.
    Let's assume that we have a large e-commerce website, and we want to know how
    to increase the business. We can identify the important pages of our website by
    categorizing them as per popularity into high, medium, and low. Based on these
    popular pages, their types, their traffic sources, and their content, we will
    be able to decide the roadmap to improve business by improving web traffic, as
    well as content.
  prefs: []
  type: TYPE_NORMAL
- en: Designing data requirement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To perform the data analytics for a specific problem, it needs datasets from
    related domains. Based on the domain and problem specification, the data source
    can be decided and based on the problem definition; the data attributes of these
    datasets can be defined.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we are going to perform social media analytics (problem specification),
    we use the data source as Facebook or Twitter. For identifying the user characteristics,
    we need user profile information, likes, and posts as data attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In data analytics, we do not use the same data sources, data attributes, data
    tools, and algorithms all the time as all of them will not use data in the same
    format. This leads to the performance of data operations, such as data cleansing,
    data aggregation, data augmentation, data sorting, and data formatting, to provide
    the data in a supported format to all the data tools as well as algorithms that
    will be used in the data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, preprocessing is used to perform data operation to translate
    data into a fixed data format before providing data to algorithms or tools. The
    data analytics process will then be initiated with this formatted data as the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: In case of Big Data, the datasets need to be formatted and uploaded to **Hadoop
    Distributed File System** (**HDFS**) and used further by various nodes with Mappers
    and Reducers in Hadoop clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Performing analytics over data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After data is available in the required format for data analytics algorithms,
    data analytics operations will be performed. The data analytics operations are
    performed for discovering meaningful information from data to take better decisions
    towards business with data mining concepts. It may either use descriptive or predictive
    analytics for business intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Analytics can be performed with various machine learning as well as custom algorithmic
    concepts, such as regression, classification, clustering, and model-based recommendation.
    For Big Data, the same algorithms can be translated to MapReduce algorithms for
    running them on Hadoop clusters by translating their data analytics logic to the
    MapReduce job which is to be run over Hadoop clusters. These models need to be
    further evaluated as well as improved by various evaluation stages of machine
    learning concepts. Improved or optimized algorithms can provide better insights.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data visualization is used for displaying the output of data analytics. Visualization
    is an interactive way to represent the data insights. This can be done with various
    data visualization softwares as well as R packages. R has a variety of packages
    for the visualization of datasets. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ggplot2`: This is an implementation of the Grammar of Graphics by *Dr. Hadley
    Wickham* ([http://had.co.nz/](http://had.co.nz/)). For more information refer
    [http://cran.r-project.org/web/packages/ggplot2/](http://cran.r-project.org/web/packages/ggplot2/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rCharts`: This is an R package to create, customize, and publish interactive
    JavaScript visualizations from R by using a familiar lattice-style plotting interface
    by *Markus Gesmann* and *Diego de Castillo*. For more information refer [http://ramnathv.github.io/rCharts/](http://ramnathv.github.io/rCharts/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some popular examples of visualization with R are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Plots for facet scales** (`ggplot`): The following figure shows the comparison
    of males and females with different measures; namely, education, income, life
    expectancy, and literacy, using `ggplot`:![Visualizing data](img/3282OS_05_20.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dashboard charts**: This is an `rCharts` type. Using this we can build interactive
    animated dashboards with R.![Visualizing data](img/3282OS_05_21.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data analytics problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we have included three practical data analytics problems with
    various stages of data-driven activity with R and Hadoop technologies. These data
    analytics problem definitions are designed such that readers can understand how
    Big Data analytics can be done with the analytical power of functions, packages
    of R, and the computational powers of Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data analytics problem definitions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the categorization of web pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the frequency of changes in the stock market
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the sale price of a blue book for bulldozers (case study)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring web pages categorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This data analytics problem is designed to identify the category of a web page
    of a website, which may categorized popularity wise as high, medium, or low (regular),
    based on the visit count of the pages. While designing the data requirement stage
    of the data analytics life cycle, we will see how to collect these types of data
    from **Google Analytics**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring web pages categorization](img/3282OS_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Identifying the problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As this is a web analytics problem, the goal of the problem is to identify the
    importance of web pages designed for websites. Based on this information, the
    content, design, or visits of the lower popular pages can be improved or increased.
  prefs: []
  type: TYPE_NORMAL
- en: Designing data requirement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will be working with data requirement as well as data collection
    for this data analytics problem. First let's see how the requirement for data
    can be achieved for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a web analytics problem, we will use Google Analytics data source.
    To retrieve this data from Google Analytics, we need to have an existent Google
    Analytics account with web traffic data stored on it. To increase the popularity,
    we will require the visits information of all of the web pages. Also, there are
    many other attributes available in Google Analytics with respect to dimensions
    and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the required Google Analytics data attributes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The header format of the dataset to be extracted from Google Analytics is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`date`: This is the date of the day when the web page was visited'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`source`: This is the referral to the web page'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pageTitle`: This is the title of the web page'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pagePath`: This is the URL of the web page'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As we are going to extract the data from Google Analytics, we need to use `RGoogleAnalytics`,
    which is an R library for extracting Google Analytics datasets within R. To extract
    data, you need this plugin to be installed in R. Then you will be able to use
    its functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code for the extraction process from Google Analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding file will be available with the chapter contents for download.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, we have the raw data for Google Analytics available in a CSV file. We need
    to process this data before providing it to the MapReduce algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main changes that need to be performed into the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Query parameters needs to be removed from the column `pagePath` as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The new CSV file needs to be created as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Performing analytics over data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To perform the categorization over website pages, we will build and run the
    MapReduce algorithm with R and Hadoop integration. As already discussed in the
    [Chapter 2](ch02.html "Chapter 2. Writing Hadoop MapReduce Programs"), *Writing
    Hadoop MapReduce Programs*, sometimes we need to use multiple Mappers and Reducers
    for performing data analytics; this means using the chained MapReduce jobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In case of chaining MapReduce jobs, multiple Mappers and Reducers can communicate
    in such a way that the output of the first job will be assigned to the second
    job as input. The MapReduce execution sequence is described in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performing analytics over data](img/3282OS_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Chaining MapReduce
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s start with the programming task to perform analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize by setting Hadoop variables and loading the `rmr2` and `rhdfs` packages
    of the RHadoop libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload the datasets to HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we will see the development of Hadoop MapReduce job 1 for these analytics.
    We will divide this job into Mapper and Reducer. Since, there are two MapReduce
    jobs, there will be two Mappers and Reducers. Also note that here we need to create
    only one file for both the jobs with all Mappers and Reducers. Mapper and Reducer
    will be established by defining their separate functions.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see MapReduce job 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mapper 1**: The code for this is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Reducer 1**: The code for this is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Output of MapReduce job 1**: The intermediate output for the information
    is shown in the following screenshot:![Performing analytics over data](img/3282OS_05_18.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output in the preceding screenshot is only for information about the output
    of this MapReduce job 1\. This can be considered an intermediate output where
    only 100 data rows have been considered from the whole dataset for providing output.
    In these rows, 23 URLs are unique; so the output has provided 23 URLs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see Hadoop MapReduce job 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mapper 2**: The code for this is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Reducer 2**: The code for this is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Before executing the MapReduce job, please start all the Hadoop daemons and
    check the HDFS connection via the `hdfs.init()` method. If your Hadoop daemons
    have not been started, you can start them by `$hduser@ubuntu :~ $HADOOP_HOME/bin/start-all.sh`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once we are ready with the logic of the Mapper and Reducer, MapReduce jobs can
    be executed by the MapReduce method of the `rmr2` package. Here we have developed
    multiple MapReduce jobs, so we need to call the `mapreduce` function within the
    `mapreduce` function with the required parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command for calling a chained MapReduce job is seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performing analytics over data](img/3282OS_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the command for retrieving the generated output from HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: While executing Hadoop MapReduce, the execution log output will be printed over
    the terminal for the purpose of monitoring. We will understand MapReduce job 1
    and MapReduce job 2 by separating them into different parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The details for MapReduce job 1 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tracking the MapReduce job metadata**: With this initial portion of log,
    we can identify the metadata for the Hadoop MapReduce job. We can also track the
    job status with the web browser by calling the given `Tracking URL`.![Performing
    analytics over data](img/3282OS_05_05.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracking status of Mapper and Reducer tasks**: With this portion of log,
    we can monitor the status of the Mapper or Reducer task being run on Hadoop cluster
    to get details such as whether it was a success or a failure.![Performing analytics
    over data](img/3282OS_05_06.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracking HDFS output location**: Once the MapReduce job is completed, its
    output location will be displayed at the end of logs.![Performing analytics over
    data](img/3282OS_05_07.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For MapReduce job 2.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tracking the MapReduce job metadata**: With this initial portion of log,
    we can identify the metadata for the Hadoop MapReduce job. We can also track the
    job status with the web browser by calling the given `Tracking URL`.![Performing
    analytics over data](img/3282OS_05_08.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracking status of the Mapper and Reducer tasks**: With this portion of log,
    we can monitor the status of the Mapper or Reducer tasks being run on the Hadoop
    cluster to get the details such as whether it was successful or failed.![Performing
    analytics over data](img/3282OS_05_09.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracking HDFS output location**: Once the MapReduce job is completed, its
    output location will be displayed at the end of the logs.![Performing analytics
    over data](img/3282OS_05_10.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output of this chained MapReduce job is stored at an HDFS location, which
    can be retrieved by the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The response to the preceding command is shown in the following figure (output
    only for the top 1000 rows of the dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Performing analytics over data](img/3282OS_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We collected the web page categorization output using the three categories.
    I think the best thing we can do is simply list the URLs. But if we have more
    information, such as sources, we can represent the web pages as nodes of a graph,
    colored by popularity with directed edges when users follow the links. This can
    lead to more informative insights.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the frequency of stock market change
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This data analytics MapReduce problem is designed for calculating the frequency
    of stock market changes.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since this is a typical stock market data analytics problem, it will calculate
    the frequency of past changes for one particular symbol of the stock market, such
    as a **Fourier Transformation**. Based on this information, the investor can get
    more insights on changes for different time periods. So the goal of this analytics
    is to calculate the frequencies of percentage change.
  prefs: []
  type: TYPE_NORMAL
- en: '![Identifying the problem](img/3282OS_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Designing data requirement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this stock market analytics, we will use Yahoo! Finance as the input dataset.
    We need to retrieve the specific symbol''s stock information. To retrieve this
    data, we will use the Yahoo! API with the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: From month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To day
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Symbol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on this API, visit [http://developer.yahoo.com/finance/](http://developer.yahoo.com/finance/).
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To perform the analytics over the extracted dataset, we will use R to fire
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Or you can also download via the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then upload it to HDFS by creating a specific Hadoop directory for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Performing analytics over data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To perform the data analytics operations, we will use streaming with R and Hadoop
    (without the `HadoopStreaming` package). So, the development of this MapReduce
    job can be done without any RHadoop integrated library/package.
  prefs: []
  type: TYPE_NORMAL
- en: In this MapReduce job, we have defined Map and Reduce in different R files to
    be provided to the Hadoop streaming function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mapper**: `stock_mapper.R`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Reducer**: `stock_reducer.R`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From the following codes, we run MapReduce in R without installing or using
    any R library/package. There is one `system()` method in R to fire the system
    command within R console to help us direct the firing of Hadoop jobs within R.
    It will also provide the repose of the commands into the R console.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also run this same program via the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: While running this program, the output at your R console or terminal will be
    as given in the following screenshot, and with the help of this we can monitor
    the status of the Hadoop MapReduce job. Here we will see them sequentially with
    the divided parts. Please note that we have separated the logs output into parts
    to help you understand them better.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MapReduce log output contains (when run from terminal):'
  prefs: []
  type: TYPE_NORMAL
- en: With this initial portion of log, we can identify the metadata for the Hadoop
    MapReduce job. We can also track the job status with the web browser, by calling
    the given `Tracking URL`. This is how the MapReduce job metadata is tracked.![Performing
    analytics over data](img/3282OS_05_13.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this portion of log, we can monitor the status of the Mapper or Reducer
    tasks being run on the Hadoop cluster to get the details like whether it was successful
    or failed. This is how we track the status of the Mapper and Reducer tasks.![Performing
    analytics over data](img/3282OS_05_14.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the MapReduce job is completed, its output location will be displayed at
    the end of the logs. This is known as tracking the HDFS output location.![Performing
    analytics over data](img/3282OS_05_15.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the terminal, the output of the Hadoop MapReduce program can be called
    using the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The headers of the output of your MapReduce program will look as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The following figure shows the sample output of MapReduce problem:![Performing
    analytics over data](img/3282OS_05_16.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can get more insights if we visualize our output with various graphs in R.
    Here, we have tried to visualize the output with the help of the `ggplot2` package.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing data](img/3282OS_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the previous graph, we can quickly identify that most of the time the stock
    price has changed from around 0 to 1.5\. So, the stock's price movements in the
    history will be helpful at the time of investing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The required code for generating this graph is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we have included the case study on how Big Data analytics
    is performed with R and Hadoop for the **Kaggle** data competition.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the sale price of blue book for bulldozers – case study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a case study for predicting the auction sale price for a piece of heavy
    equipment to create a blue book for bulldozers.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, I have included a case study by Cloudera data scientists on
    how large datasets can be resampled, and applied the random forest model with
    R and Hadoop. Here, I have considered the Kaggle blue book for bulldozers competition
    for understanding the types of Big Data problem definitions. Here, the goal of
    this competition is to predict the sale price of a particular piece of heavy equipment
    at a usage auction based on its usage, equipment type, and configuration. This
    solution has been provided by *Uri Laserson* (Data Scientist at Cloudera). The
    provided data contains the information about auction result posting, usage, and
    equipment configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a trick to model the Big Data sets and divide them into the smaller datasets.
    Fitting the model on that dataset is a traditional machine learning technique
    such as random forests or bagging. There are possibly two reasons for random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: Large datasets typically live in a cluster, so any operations will have some
    level of parallelism. Separate models fit on separate nodes that contain different
    subsets of the initial data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if you can use the entire initial dataset to fit a single model, it turns
    out that ensemble methods, where you fit multiple smaller models by using subsets
    of data, generally outperform single models. Indeed, fitting a single model with
    100M data points can perform worse than fitting just a few models with 10M data
    points each (so smaller total data outperforms larger total data).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling with replacement is the most popular method for sampling from the initial
    dataset for producing a collection of samples for model fitting. This method is
    equivalent to sampling from a multinomial distribution, where the probability
    of selecting any individual input data point is uniform over the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kaggle is a Big Data platform where data scientists from all over the world
    compete to solve Big Data analytics problems hosted by data-driven organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Designing data requirement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this competition, Kaggle has provided real-world datasets that comprises
    approximately 4,00,000 training data points. Each data point represents the various
    attributes of sales, configuration of the bulldozer, and sale price. To find out
    where to predict the sales price, the random forest regression model needs to
    be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The reference link for this Kaggle competition is [http://www.kaggle.com/c/bluebook-for-bulldozers](http://www.kaggle.com/c/bluebook-for-bulldozers).
    You can check the data, information, forum, and leaderboard as well as explore
    some other Big Data analytics competitions and participate in them to evaluate
    your data analytics skills.
  prefs: []
  type: TYPE_NORMAL
- en: We chose this model because we are interested in predicting the sales price
    in numeric values from random sets of a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The datasets are provided in the terms of the following data files:'
  prefs: []
  type: TYPE_NORMAL
- en: '| File name | Description format (size) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Train` | This is a training set that contains data for 2011. |'
  prefs: []
  type: TYPE_TB
- en: '| `Valid` | This is a validation set that contains data from January 1, 2012
    to April 30, 2012. |'
  prefs: []
  type: TYPE_TB
- en: '| `Data dictionary` | This is the metadata of the training dataset variables.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Machine_Appendix` | This contains the correct year of manufacturing for
    a given machine along with the make, model, and product class details. |'
  prefs: []
  type: TYPE_TB
- en: '| `Test` | This tests datasets. |'
  prefs: []
  type: TYPE_TB
- en: '| `random_forest_benchmark_test` | This is the benchmark solution provided
    by the host. |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In case you want to learn and practice Big Data analytics, you can acquire the
    Big Data sets from the Kaggle data source by participating in the Kaggle data
    competitions. These contain the datasets of various fields from industries worldwide.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To perform the analytics over the provided Kaggle datasets, we need to build
    a predictive model. To predict the sale price for the auction, we will fit the
    model over provided datasets. But the datasets are provided with more than one
    file. So we will merge them as well as perform data augmentation for acquiring
    more meaningful data. We are going to build a model from `Train.csv` and `Machine_Appendix.csv`
    for better prediction of the sale price.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the data preprocessing tasks that need to be performed over the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Performing analytics over data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we are going to perform analytics with sampled datasets, we need to understand
    how many datasets need to be sampled.
  prefs: []
  type: TYPE_NORMAL
- en: 'For random sampling, we have considered three model parameters, which are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We have N data points in our initial training set. This is very large (106-109)
    and is distributed over an HDFS cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to train a set of M different models for an ensemble classifier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the M models will be fitted with K data points, where typically K <<
    N. (For example, K may be 1-10 percent of N.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have N numbers of training datasets, which are fixed and generally outside
    our control. As we are going to handle this via **Poisson** sampling, we need
    to define the total number of input vectors to be consumed into the random forest
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three cases to be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '**KM < N**: In this case, we are not using the full amount of data available
    to us'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KM = N**: In this case, we can exactly partition our dataset to produce totally
    independent samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KM > N**: In this case, we must resample some of our data with replacements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Poisson sampling method described in the following section handles all the
    three cases in the same framework. However, note that for the case KM = N, it
    does not partition the data, but simply resamples it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Poisson-approximation resampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generalized linear models are an extension of the general linear model. Poisson
    regression is a situation of generalized models. The dependent variable obeys
    Poisson distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Poisson sampling will be run on the Map of the MapReduce task because it occurs
    for input data points. This doesn't guarantee that every data point will be considered
    into the model, which is better than multinomial resampling of full datasets.
    But it will guarantee the generation of independent samples by using N training
    input points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following graph indicates the amount of missed datasets that can
    be retrieved in the Poisson sampling with the function of KM/N:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Poisson-approximation resampling](img/3282OS_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The grey line indicates the value of KM=N. Now, let''s look at the pseudo code
    of the MapReduce algorithm. We have used three parameters: N, M, and K where K
    is fixed. We used T=K/N to eliminate the need for the value of N in advance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**An example of sampling parameters**: Here, we will implement the preceding
    logic with a pseudo code. We will start by defining two model input parameters
    as `frac.per.model` and `num.models`, where `frac.per.model` is used for defining
    the fraction of the full dataset that can be used, and `num.models` is used for
    defining how many models will be fitted from the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Logic of Mapper**: Mapper will be designed for generating the samples of
    the full dataset by data wrangling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Logic of Reducer**: Reducer will take a data sample as input and fit the
    random forest model over it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Fitting random forests with RHadoop
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In machine learning, fitting a model means fitting the best line into our data.
    Fitting a model can fall under several types, namely, under fitting, over fitting,
    and normal fitting. In case of under and over fitting, there are chances of high
    bias (cross validation and training errors are high) and high variance (cross
    validation error is high but training error is low) effects, which is not good.
    We will normally fit the model over the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the diagrams for fitting a model over datasets with three types of
    fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Under fitting**: In this cross validation and training errors are high![Fitting
    random forests with RHadoop](img/3282OS_05_22.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normal fitting**: In this cross-validation and training errors are normal![Fitting
    random forests with RHadoop](img/3282OS_05_23.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Over fitting**: In this the cross-validation error is high but training error
    is low![Fitting random forests with RHadoop](img/3282OS_05_24.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will fit the model over the data using the random forest technique of machine
    learning. This is a type of recursive partitioning method, particularly well suited
    for small and large problems. It involves an ensemble (or set) of classification
    (or regression) trees that are calculated on random subsets of the data, using
    a subset of randomly restricted and selected predictors for every split in each
    classification tree.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the results of an ensemble of classification/regression trees have
    been used to produce better predictions instead of using the results of just one
    classification tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now implement our Poisson sampling strategy with RHadoop. We will start
    by setting global values for our parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Let's check how to implement Mapper as per the specifications in the pseudo
    code with RHadoop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mapper is implemented in the the following manner:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since we are using R, it's tricky to fit the model with the random forest model
    over the collected sample dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Reducer is implemented in the following manner:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To fit the model, we need `model.formula`, which is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`SalePrice` is defined as a response variable and the rest of them are defined
    as predictor variables for the random forest model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Random forest model with R doesn't support factor with level more than 32.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The MapReduce job can be executed using the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The resulting trees are dumped in HDFS at `/poisson/output`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we can load the trees, merge them, and use them to classify new test
    points:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Each of the 50 samples produced a random forest with 10 trees, so the final
    random forest is a collection of 500 trees, fitted in a distributed fashion over
    a Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The full set of source files is available on the official Cloudera blog at [http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/](http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/).
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, we have learned a scalable approach for training ensemble classifiers
    or bootstrapping in a parallel fashion by using a Poisson approximation for multinomial
    sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to perform Big Data analytics with various data
    driven activities over an R and Hadoop integrated environment.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn more about how R and Hadoop can be used to
    perform machine learning techniques.
  prefs: []
  type: TYPE_NORMAL
