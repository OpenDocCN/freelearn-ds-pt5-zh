<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Learning Data Analytics with R and Hadoop</h1></div></div></div><p>In the previous chapters we learned about the installation, configuration, and integration of R and Hadoop.</p><p>In this <a id="id515" class="indexterm"/>chapter, we will learn how to perform data analytics operations <a id="id516" class="indexterm"/>over an integrated R and Hadoop environment. Since this chapter is designed for data analytics, we will understand this with an effective data analytics cycle.</p><p>In this chapter we will learn about:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the data analytics project life cycle</li><li class="listitem" style="list-style-type: disc">Understanding data analytics problems</li></ul></div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec36"/>Understanding the data analytics project life cycle</h1></div></div></div><p>While dealing <a id="id517" class="indexterm"/>with the data analytics projects, there are some fixed tasks that should be followed to get the expected output. So here we are going to build a data analytics project cycle, which will be a set of standard data-driven processes to lead data to insights effectively. The defined data analytics processes of a project life cycle should be followed by sequences for effectively achieving the goal using input datasets. This data analytics process may include identifying the data analytics problems, designing, and collecting datasets, data analytics, and data visualization.</p><p>The data analytics project life cycle stages are seen in the following diagram:</p><div><img src="img/3282OS_05_01.jpg" alt="Understanding the data analytics project life cycle"/></div><p>Let's <a id="id518" class="indexterm"/>get some perspective on these stages for performing data analytics.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec51"/>Identifying the problem</h2></div></div></div><p>Today, business analytics trends change by performing data analytics over web datasets for growing <a id="id519" class="indexterm"/>business. Since their data <a id="id520" class="indexterm"/>size is increasing gradually day by day, their analytical application needs to be scalable for collecting insights from their datasets.</p><p>With the help of web analytics, we can solve the business analytics problems. Let's assume that we have a large e-commerce website, and we want to know how to increase the business. We can identify the important pages of our website by categorizing them as per popularity into high, medium, and low. Based on these popular pages, their types, their traffic sources, and their content, we will be able to decide the roadmap to improve business by improving web traffic, as well as content.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec52"/>Designing data requirement</h2></div></div></div><p>To perform the data analytics for a specific problem, it needs datasets from related domains. Based <a id="id521" class="indexterm"/>on the domain <a id="id522" class="indexterm"/>and problem specification, the data source can be decided and based on the problem definition; the data attributes of these datasets can be defined.</p><p>For example, if we are going to perform social media analytics (problem specification), we use the data source as Facebook or Twitter. For identifying the user characteristics, we need user profile information, likes, and posts as data attributes.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec53"/>Preprocessing data</h2></div></div></div><p>In data analytics, we do not use the same data sources, data attributes, data tools, and algorithms <a id="id523" class="indexterm"/>all the time as all of them will <a id="id524" class="indexterm"/>not use data in the same format. This leads to the performance of data operations, such as data cleansing, data aggregation, data augmentation, data sorting, and data formatting, to provide the data in a supported format to all the data tools as well as algorithms that will be used in the data analytics.</p><p>In simple terms, preprocessing is used to perform data operation to translate data into a fixed data format before providing data to algorithms or tools. The data analytics process will then be initiated with this formatted data as the input.</p><p>In case of Big Data, the <a id="id525" class="indexterm"/>datasets need to be formatted and uploaded to <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>) and used further by various nodes with Mappers and Reducers in Hadoop clusters.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec54"/>Performing analytics over data</h2></div></div></div><p>After<a id="id526" class="indexterm"/> data is available in the required format for data analytics algorithms, data analytics operations will be performed. The data analytics operations are performed for discovering meaningful information <a id="id527" class="indexterm"/>from data to take better decisions towards business with data mining concepts. It may either use descriptive or predictive analytics for business intelligence.</p><p>Analytics can be performed with various machine learning as well as custom algorithmic concepts, such as regression, classification, clustering, and model-based recommendation. For Big Data, the same algorithms can be translated to MapReduce algorithms for running them on Hadoop clusters by translating their data analytics logic to the<a id="id528" class="indexterm"/> MapReduce job which is to <a id="id529" class="indexterm"/>be run over Hadoop clusters. These models need to be further evaluated as well as improved by various evaluation stages of machine learning concepts. Improved or optimized algorithms can provide better insights.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec55"/>Visualizing data</h2></div></div></div><p>Data visualization <a id="id530" class="indexterm"/>is used for displaying the <a id="id531" class="indexterm"/>output of data analytics. Visualization is an interactive way to represent the data insights. This can be done with various data visualization softwares as well as R packages. R has a variety of packages for the visualization of datasets. They are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">ggplot2</code>: This<a id="id532" class="indexterm"/> is an implementation of the Grammar <a id="id533" class="indexterm"/>of Graphics by <em>Dr. Hadley Wickham</em> (<a class="ulink" href="http://had.co.nz/">http://had.co.nz/</a>). For more information refer <a class="ulink" href="http://cran.r-project.org/web/packages/ggplot2/">http://cran.r-project.org/web/packages/ggplot2/</a>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rCharts</code>: This is an R package to create, customize, and publish interactive JavaScript <a id="id534" class="indexterm"/>visualizations from R by using a familiar lattice-style plotting interface <a id="id535" class="indexterm"/>by <em>Markus Gesmann</em> and <em>Diego de Castillo</em>. For more information refer <a class="ulink" href="http://ramnathv.github.io/rCharts/">http://ramnathv.github.io/rCharts/</a>.</li></ul></div><p>Some popular examples of visualization with R are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Plots for facet scales</strong> (<code class="literal">ggplot</code>): The following figure shows the comparison of males and <a id="id536" class="indexterm"/>females with different measures; namely, education, income, life expectancy, and literacy, using <code class="literal">ggplot</code>:<div><img src="img/3282OS_05_20.jpg" alt="Visualizing data"/></div></li><li class="listitem" style="list-style-type: disc"><strong>Dashboard charts</strong>: This<a id="id537" class="indexterm"/> is an <code class="literal">rCharts</code> type. Using this we can build interactive<a id="id538" class="indexterm"/> animated <a id="id539" class="indexterm"/>dashboards with R.<div><img src="img/3282OS_05_21.jpg" alt="Visualizing data"/></div></li></ul></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec37"/>Understanding data analytics problems</h1></div></div></div><p>In this section, we have included three practical data analytics problems with various stages of data-driven activity with R and Hadoop technologies. These data analytics problem <a id="id540" class="indexterm"/>definitions are designed such that readers can understand how Big Data analytics can be done with the analytical power of functions, packages of R, and the computational powers of Hadoop.</p><p>The data analytics problem definitions are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Exploring the categorization of web pages</li><li class="listitem" style="list-style-type: disc">Computing the frequency of changes in the stock market</li><li class="listitem" style="list-style-type: disc">Predicting the sale price of a blue book for bulldozers (case study)</li></ul></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec56"/>Exploring web pages categorization</h2></div></div></div><p>This data analytics problem is designed to identify the category of a web page of a website, which <a id="id541" class="indexterm"/>may categorized<a id="id542" class="indexterm"/> popularity wise as high, medium, or low (regular), based on the visit count of the pages. While designing the data requirement stage of the data analytics life cycle, we will see how to collect these types of data from <a id="id543" class="indexterm"/>
<strong>Google Analytics</strong>.</p><div><img src="img/3282OS_05_02.jpg" alt="Exploring web pages categorization"/></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec33"/>Identifying the problem</h3></div></div></div><p>As this is a <a id="id544" class="indexterm"/>web analytics problem, the goal of the problem is to identify the importance of web pages designed for websites. Based on this information, the content, design, or visits of the lower popular pages can be improved or increased.</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec34"/>Designing data requirement</h3></div></div></div><p>In this <a id="id545" class="indexterm"/>section, we will be working with data requirement as well as data collection for this data analytics problem. First let's see how the requirement for data can be achieved for this problem.</p><p>Since this is a web analytics problem, we will use Google Analytics data source. To retrieve this <a id="id546" class="indexterm"/>data from Google Analytics, we need to have an existent Google Analytics account with web traffic data stored on it. To increase the popularity, we will require the visits information of all of the web pages. Also, there are many other attributes available in Google Analytics with respect to dimensions and metrics.</p><div><div><div><div><h4 class="title"><a id="ch05lvl4sec03"/>Understanding the required Google Analytics data attributes</h4></div></div></div><p>The header <a id="id547" class="indexterm"/>format of the dataset to be extracted from<a id="id548" class="indexterm"/> Google Analytics is as follows:</p><div><pre class="programlisting">date, source, pageTitle, pagePath</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">date</code>: This is the date of the day when the web page was visited</li><li class="listitem" style="list-style-type: disc"><code class="literal">source</code>: This is the referral to the web page</li><li class="listitem" style="list-style-type: disc"><code class="literal">pageTitle</code>: This is the title of the web page</li><li class="listitem" style="list-style-type: disc"><code class="literal">pagePath</code>: This is the URL of the web page</li></ul></div><div><div><div><div><h5 class="title"><a id="ch05lvl5sec01"/>Collecting data</h5></div></div></div><p>As we are <a id="id549" class="indexterm"/>going to extract the data from Google Analytics, we need to use <code class="literal">RGoogleAnalytics</code>, which is an R library for extracting<a id="id550" class="indexterm"/> Google Analytics datasets within R. To extract data, you need this plugin to be installed in R. Then you will be able to use its functions.</p><p>The following is the code for the extraction process from Google Analytics:</p><div><pre class="programlisting"># Loading the RGoogleAnalytics library
require("RGoogleAnalyics")

# Step 1. Authorize your account and paste the access_token
query &lt;- QueryBuilder()
access_token &lt;- query$authorize()

# Step 2. Create a new Google Analytics API object
ga &lt;- RGoogleAnalytics()

# To retrieve profiles from Google Analytics
ga.profiles &lt;- ga$GetProfileData(access_token)

# List the GA profiles 
ga.profiles

# Step 3. Setting up the input parameters
profile &lt;- ga.profiles$id[3] 
startdate &lt;- "2010-01-08"
enddate &lt;- "2013-08-23"
dimension &lt;- "ga:date,ga:source,ga:pageTitle,ga:pagePath"
metric &lt;- "ga:visits"
sort &lt;- "ga:visits"
maxresults &lt;- 100099

# Step 4. Build the query string, use the profile by setting its index value
query$Init(start.date = startdate,
           end.date = enddate,
           dimensions = dimension,
           metrics = metric,
           
           max.results = maxresults,
           table.id = paste("ga:",profile,sep="",collapse=","),
           access_token=access_token)

# Step 5. Make a request to get the data from the API
ga.data &lt;- ga$GetReportData(query)

# Look at the returned data
head(ga.data)
write.csv(ga.data,"webpages.csv", row.names=FALSE)</pre></div><p>The <a id="id551" class="indexterm"/>preceding file will be available with the chapter<a id="id552" class="indexterm"/> contents for download.</p></div></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec35"/>Preprocessing data</h3></div></div></div><p>Now, we have the raw data for Google Analytics available in a CSV file. We need to process this <a id="id553" class="indexterm"/>data before providing it to the MapReduce algorithm.</p><p>There are two main changes that need to be performed into the dataset:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Query parameters needs to be removed from the column <code class="literal">pagePath</code> as follows:<div><pre class="programlisting">pagePath &lt;- as.character(data$pagePath)
pagePath &lt;- strsplit(pagePath,"\\?")
pagePath &lt;- do.call("rbind", pagePath)
pagePath &lt;- pagePath [,1]</pre></div></li><li class="listitem" style="list-style-type: disc">The<a id="id554" class="indexterm"/> new CSV file needs to be created as follows:<div><pre class="programlisting">data  &lt;- data.frame(source=data$source, pagePath=d,visits =)
write.csv(data, "webpages_mapreduce.csv" , row.names=FALSE)</pre></div></li></ul></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec36"/>Performing analytics over data</h3></div></div></div><p>To perform <a id="id555" class="indexterm"/>the categorization over website pages, we will build and run the MapReduce algorithm with R and Hadoop integration. As already discussed in the <a class="link" href="ch02.html" title="Chapter 2. Writing Hadoop MapReduce Programs">Chapter 2</a>, <em>Writing Hadoop MapReduce Programs</em>, sometimes we need to use multiple Mappers and Reducers for performing data analytics; this means using the chained MapReduce jobs.</p><p>In case of chaining MapReduce jobs, multiple Mappers and Reducers can communicate in such a way that the output of the first job will be assigned to the second job as input. The MapReduce execution sequence is described in the following diagram:</p><div><img src="img/3282OS_05_03.jpg" alt="Performing analytics over data"/><div><p>Chaining MapReduce</p></div></div><p>Now let's start with the programming task to perform analytics:</p><div><ol class="orderedlist arabic"><li class="listitem">Initialize by setting Hadoop variables and loading the <code class="literal">rmr2</code> and <code class="literal">rhdfs</code> packages of the RHadoop libraries:<div><pre class="programlisting"># setting up the Hadoop variables need by RHadoop
Sys.setenv(HADOOP_HOME="/usr/local/hadoop/")
Sys.setenv(HADOOP_CMD="/usr/local/hadoop/bin/hadoop")

# Loading the RHadoop libraries rmr2 and rhdfs
library(rmr2)
library(rhdfs)

# To initializing hdfs
hdfs.init()</pre></div></li><li class="listitem">Upload the datasets to HDFS:<div><pre class="programlisting"># First uploading the data to R console,
webpages &lt;- read.csv("/home/vigs/Downloads/webpages_mapreduce.csv")

# saving R file object to HDFS,
webpages.hdfs &lt;- to.dfs(webpages) </pre></div></li></ol></div><p>Now we <a id="id556" class="indexterm"/>will see the development of Hadoop MapReduce job 1 for these analytics. We will divide this job into Mapper and Reducer. Since, there are two MapReduce jobs, there will be two Mappers and Reducers. Also note that here we need to create only one file for both the jobs with all Mappers and Reducers. Mapper and Reducer will be established by defining their separate functions.</p><p>Let's see MapReduce job 1.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Mapper 1</strong>: The code for this is as follows:<div><pre class="programlisting">mapper1 &lt;- function(k,v) {

 # To storing pagePath column data in to key object
 key &lt;- v[2]

 # To store visits column data into val object
 Val &lt;- v[3]

 # emitting key and value for each row
 keyval(key, val)
}
totalvisits &lt;- sum(webpages$visits)</pre></div></li><li class="listitem" style="list-style-type: disc"><strong>Reducer 1</strong>: The code for this is as follows:<div><pre class="programlisting">reducer1 &lt;- function(k,v) {

  # Calculating percentage visits for the specific URL
  per &lt;- (sum(v)/totalvisits)*100
  # Identify the category of URL
  if (per &lt;33 )
 {
val &lt;- "low"
}
 if (per &gt;33 &amp;&amp; per &lt; 67)
 {
 val &lt;- "medium"
 }
 if (per &gt; 67)
 {
 val &lt;- "high"
 }

 # emitting key and values
 keyval(k, val)
}</pre></div></li><li class="listitem" style="list-style-type: disc"><strong>Output of MapReduce job 1</strong>: The intermediate output for the information is shown <a id="id557" class="indexterm"/>in the following screenshot:<div><img src="img/3282OS_05_18.jpg" alt="Performing analytics over data"/></div></li></ul></div><p>The output in the preceding screenshot is only for information about the output of this MapReduce job 1. This can be considered an intermediate output where only 100 data rows have<a id="id558" class="indexterm"/> been considered from the whole dataset for providing output. In these rows, 23 URLs are unique; so the output has provided 23 URLs.</p><p>Let's see Hadoop MapReduce job 2:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Mapper 2</strong>: The code for this is as follows:<div><pre class="programlisting">#Mapper:
mapper2 &lt;- function(k, v) {

# Reversing key and values and emitting them 
 keyval(v,k)
}</pre></div></li><li class="listitem" style="list-style-type: disc"><strong>Reducer 2</strong>: The code for this is as follows:<div><pre class="programlisting">key &lt;- NA
val &lt;- NULL
# Reducer:
reducer2  &lt;-  function(k, v) {


# for checking whether key-values are already assigned or not.
 if(is.na(key)) {
 key &lt;- k
 val &lt;- v
  } else {
   if(key==k) {
 val &lt;- c(val,v)
  } else{
   key &lt;- k
   val &lt;- v
  }
 }
# emitting key and list of values 
keyval(key,list(val))
}</pre></div><div><div><h3 class="title"><a id="tip15"/>Tip</h3><p>Before executing the MapReduce job, please start all the Hadoop daemons and check the HDFS connection via the <code class="literal">hdfs.init()</code> method. If your Hadoop daemons have not been started, you can start them by <code class="literal">$hduser@ubuntu :~ $HADOOP_HOME/bin/start-all.sh</code>.</p></div></div></li></ul></div><p>Once we <a id="id559" class="indexterm"/>are ready with the logic of the Mapper and Reducer, MapReduce jobs can be executed by the MapReduce method of the <code class="literal">rmr2</code> package. Here we have developed multiple MapReduce jobs, so we need to call the <code class="literal">mapreduce</code> function within the <code class="literal">mapreduce</code> function with the required parameters.</p><p>The command for calling a chained MapReduce job is seen in the following figure:</p><div><img src="img/3282OS_05_04.jpg" alt="Performing analytics over data"/></div><p>The following is the command for retrieving the generated output from HDFS:</p><div><pre class="programlisting">
<strong>from.dfs(output)</strong>
</pre></div><p>While executing Hadoop MapReduce, the execution log output will be printed over the terminal<a id="id560" class="indexterm"/> for the purpose of monitoring. We will understand MapReduce job 1 and MapReduce job 2 by separating them into different parts.</p><p>The details for MapReduce job 1 is as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Tracking the MapReduce job metadata</strong>: With this initial portion of log, we can identify <a id="id561" class="indexterm"/>the metadata for the Hadoop MapReduce job. We can also track the job status with the web browser by calling the given <code class="literal">Tracking URL</code>.<div><img src="img/3282OS_05_05.jpg" alt="Performing analytics over data"/></div></li><li class="listitem" style="list-style-type: disc"><strong>Tracking status of Mapper and Reducer tasks</strong>: With <a id="id562" class="indexterm"/>this portion of log, we can<a id="id563" class="indexterm"/> monitor the status of the Mapper <a id="id564" class="indexterm"/>or Reducer task<a id="id565" class="indexterm"/> being run on Hadoop cluster to get details such as whether it was a success or a failure.<div><img src="img/3282OS_05_06.jpg" alt="Performing analytics over data"/></div></li><li class="listitem" style="list-style-type: disc"><strong>Tracking HDFS output location</strong>: Once the MapReduce job is completed, its output <a id="id566" class="indexterm"/>location will be <a id="id567" class="indexterm"/>displayed at the end of logs.<div><img src="img/3282OS_05_07.jpg" alt="Performing analytics over data"/></div></li></ul></div><p>For MapReduce job 2.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Tracking the MapReduce job metadata</strong>: With this initial portion of log, we can<a id="id568" class="indexterm"/> identify the metadata for the Hadoop MapReduce job. We can also track the job status with the web browser by calling the given <code class="literal">Tracking URL</code>.<div><img src="img/3282OS_05_08.jpg" alt="Performing analytics over data"/></div></li><li class="listitem" style="list-style-type: disc"><strong>Tracking status of the Mapper and Reducer tasks</strong>: With <a id="id569" class="indexterm"/>this <a id="id570" class="indexterm"/>portion of log, we can monitor the status of the Mapper or Reducer tasks being run on the Hadoop cluster to get the details such as whether it was successful or failed.<div><img src="img/3282OS_05_09.jpg" alt="Performing analytics over data"/></div></li><li class="listitem" style="list-style-type: disc"><strong>Tracking HDFS output location</strong>: Once the MapReduce job is completed, its output <a id="id571" class="indexterm"/>location will be <a id="id572" class="indexterm"/>displayed at the end of the logs.<div><img src="img/3282OS_05_10.jpg" alt="Performing analytics over data"/></div></li></ul></div><p>The output of this chained MapReduce job is stored at an HDFS location, which can be retrieved by the command:</p><div><pre class="programlisting">
<strong>from.dfs(output)</strong>
</pre></div><p>The response to the preceding command is shown in the following figure (output only for the top 1000 rows of the dataset):</p><div><img src="img/3282OS_05_11.jpg" alt="Performing analytics over data"/></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec37"/>Visualizing data</h3></div></div></div><p>We collected the web page categorization output using the three categories. I think the best thing <a id="id573" class="indexterm"/>we can do is simply list the URLs. But if we have more information, such as sources, we can represent the web pages as nodes of a graph, colored by popularity with directed edges when users follow the links. This can lead to more informative insights.</p></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec57"/>Computing the frequency of stock market change</h2></div></div></div><p>This <a id="id574" class="indexterm"/>data analytics <a id="id575" class="indexterm"/>MapReduce problem is designed for calculating the frequency of stock market changes.</p><div><div><div><div><h3 class="title"><a id="ch05lvl3sec38"/>Identifying the problem</h3></div></div></div><p>Since this is a typical stock market data analytics problem, it will calculate the frequency of past changes for one particular symbol of the stock market, such as a <strong>Fourier Transformation</strong><a id="id576" class="indexterm"/>. Based on this information, the investor can get<a id="id577" class="indexterm"/> more insights on changes for different time periods. So the goal of this analytics is to calculate the frequencies of percentage change.</p><div><img src="img/3282OS_05_12.jpg" alt="Identifying the problem"/></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec39"/>Designing data requirement</h3></div></div></div><p>For this <a id="id578" class="indexterm"/>stock market analytics, we will use Yahoo! Finance as the input dataset. We need to retrieve the specific symbol's stock information. To retrieve this data, we will use the Yahoo! API with the following parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">From month</li><li class="listitem" style="list-style-type: disc">From day</li><li class="listitem" style="list-style-type: disc">From year</li><li class="listitem" style="list-style-type: disc">To month</li><li class="listitem" style="list-style-type: disc">To day</li><li class="listitem" style="list-style-type: disc">To year</li><li class="listitem" style="list-style-type: disc">Symbol</li></ul></div><div><div><h3 class="title"><a id="tip16"/>Tip</h3><p>For more information on this API, visit <a class="ulink" href="http://developer.yahoo.com/finance/">http://developer.yahoo.com/finance/</a>.</p></div></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec40"/>Preprocessing data</h3></div></div></div><p>To perform<a id="id579" class="indexterm"/> the analytics over the extracted dataset, we will use R to fire the following command:</p><div><pre class="programlisting">
<strong>stock_BP &lt;- read.csv("http://ichart.finance.yahoo.com/table.csv?s=BP")</strong>
</pre></div><p>Or you can also download via the terminal:</p><div><pre class="programlisting">
<strong>wget http://ichart.finance.yahoo.com/table.csv?s=BP</strong>
<strong>#exporting to csv file</strong>

<strong>write.csv(stock_BP,"table.csv", row.names=FALSE)</strong>
</pre></div><p>Then upload it to HDFS by creating a specific Hadoop directory for this:</p><div><pre class="programlisting">
<strong># creating /stock directory in hdfs</strong>
<strong>bin/hadoop dfs -mkdir /stock</strong>

<strong># uploading table.csv to hdfs in /stock directory</strong>
<strong>bin/hadoop dfs -put /home/Vignesh/downloads/table.csv /stock/ </strong>
</pre></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec41"/>Performing analytics over data</h3></div></div></div><p>To perform<a id="id580" class="indexterm"/> the data analytics operations, we will use streaming with R and Hadoop (without the <code class="literal">HadoopStreaming</code> package). So, the development of this MapReduce job can be done without any RHadoop integrated library/package.</p><p>In this MapReduce job, we have defined Map and Reduce in different R files to be provided to the Hadoop streaming function.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Mapper</strong>: <code class="literal">stock_mapper.R</code><div><pre class="programlisting">
<strong>#! /usr/bin/env/Rscript</strong>
<strong># To disable the warnings</strong>
<strong>options(warn=-1)</strong>
<strong># To take input the data from streaming</strong>
<strong>input &lt;- file("stdin", "r")</strong>

<strong># To reading the each lines of documents till the end</strong>
<strong>while(length(currentLine &lt;-readLines(input, n=1, warn=FALSE)) &gt; 0)</strong>
<strong>{</strong>

<strong># To split the line by "," seperator</strong>
<strong>fields &lt;- unlist(strsplit(currentLine, ","))</strong>

<strong># Capturing open column value</strong>
<strong> open &lt;- as.double(fields[2])</strong>

<strong># Capturing close columns value</strong>
<strong> close &lt;- as.double(fields[5])</strong>

<strong># Calculating the difference of close and open attribute</strong>
<strong>  change &lt;- (close-open)</strong>

<strong># emitting change as key and value as 1</strong>
<strong>write(paste(change, 1, sep="\t"), stdout())</strong>
<strong>}</strong>

<strong>close(input)</strong>
</pre></div></li><li class="listitem" style="list-style-type: disc"><strong>Reducer</strong>: <code class="literal">stock_reducer.R</code><div><pre class="programlisting">
<strong>#! /usr/bin/env Rscript</strong>
<strong>stock.key &lt;- NA</strong>
<strong>stock.val &lt;- 0.0</strong>

<strong>conn &lt;- file("stdin", open="r")</strong>
<strong>while (length(next.line &lt;- readLines(conn, n=1)) &gt; 0) {</strong>
<strong> split.line &lt;- strsplit(next.line, "\t")</strong>
<strong> key &lt;- split.line[[1]][1]</strong>
<strong> val &lt;- as.numeric(split.line[[1]][2])</strong>
<strong> if (is.na(current.key)) {</strong>
<strong> current.key &lt;- key</strong>
<strong> current.val &lt;- val</strong>
<strong> }</strong>
<strong> else {</strong>
<strong> if (current.key == key) {</strong>
<strong>current.val &lt;- current.val + val</strong>
<strong>}</strong>
<strong>else {</strong>
<strong>write(paste(current.key, current.val, sep="\t"), stdout())</strong>
<strong>current.key &lt;- key</strong>
<strong>current.val&lt;- val</strong>
<strong>}</strong>
<strong>}</strong>
<strong>}</strong>
<strong>write(paste(current.key, current.val, sep="\t"), stdout())</strong>
<strong>close(conn)</strong>
</pre></div></li></ul></div><p>From the following codes, we run MapReduce in R without installing or using any R library/package. There is one <code class="literal">system()</code> method in R to fire the system command within R console to<a id="id581" class="indexterm"/> help us direct the firing of <a id="id582" class="indexterm"/>Hadoop jobs within R. It will also provide the repose of the commands into the R console.</p><div><pre class="programlisting">
<strong># For locating at Hadoop Directory</strong>
<strong>system("cd $HADOOP_HOME")</strong>

<strong># For listing all HDFS first level directorysystem("bin/hadoop dfs -ls /")</strong>

<strong># For running Hadoop MapReduce with streaming parameters</strong>
<strong>system(paste("bin/hadoop jar </strong>
<strong>/usr/local/hadoop/contrib/streaming/hadoop-streaming-1.0.3.jar ",</strong>

<strong>" -input /stock/table.csv",</strong>
<strong>" -output /stock/outputs",</strong>
<strong>" -file /usr/local/hadoop/stock/stock_mapper.R",</strong>
<strong>" -mapper /usr/local/hadoop/stock/stock_mapper.R",</strong>
<strong>" -file /usr/local/hadoop/stock/stock_reducer.R",</strong>
<strong>" -reducer /usr/local/hadoop/stock/stock_reducer.R"))</strong>

<strong># For storing the output of list command </strong>
<strong>dir &lt;- system("bin/hadoop dfs -ls /stock/outputs", intern=TRUE)</strong>
<strong>dir</strong>

<strong># For storing the output from part-oooo (output file)</strong>
<strong>out &lt;- system("bin/hadoop dfs -cat /stock/outputs/part-00000", intern=TRUE)</strong>

<strong># displaying Hadoop MapReduce output data out</strong>
</pre></div><p>You can also run this same program via the terminal:</p><div><pre class="programlisting">
<strong>bin/hadoop jar /usr/local/hadoop/contrib/streaming/hadoop-streaming-1.0.3.jar \</strong>

<strong> -input /stock/table.csv \</strong>
<strong> -output /stock/outputs\</strong>
<strong> -file /usr/local/hadoop/stock/stock_mapper.R \</strong>
<strong> -mapper /usr/local/hadoop/stock/stock_mapper.R \</strong>
<strong> -file /usr/local/hadoop/stock/stock_reducer.R \</strong>
<strong> -reducer /usr/local/hadoop/stock/stock_reducer.R </strong>
</pre></div><p>While running this program, the output at your R console or terminal will be as given in the following screenshot, and with the help of this we can monitor the status of the Hadoop MapReduce job. Here we will see them sequentially with the divided parts. Please note <a id="id583" class="indexterm"/>that we have separated the logs output into parts to help you understand them better.</p><p>The MapReduce log output contains (when run from terminal):</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">With this initial portion of log, we can identify the metadata for the Hadoop MapReduce job. We can also track the job status with the web browser, by calling the given <code class="literal">Tracking URL</code>. This is how the MapReduce job metadata is tracked.<div><img src="img/3282OS_05_13.jpg" alt="Performing analytics over data"/></div></li><li class="listitem" style="list-style-type: disc">With this portion of log, we can monitor the status of the Mapper or Reducer tasks being run on the Hadoop cluster to get the details like whether it was successful or failed. This is how we track the status of the Mapper and Reducer tasks.<div><img src="img/3282OS_05_14.jpg" alt="Performing analytics over data"/></div></li><li class="listitem" style="list-style-type: disc">Once<a id="id584" class="indexterm"/> the MapReduce job is completed, its output location will be displayed at the end of the logs. This is known as tracking the HDFS output location.<div><img src="img/3282OS_05_15.jpg" alt="Performing analytics over data"/></div></li><li class="listitem" style="list-style-type: disc">From the terminal, the output of the Hadoop MapReduce program can be called using the following command:<div><pre class="programlisting">
<strong>bin/hadoop dfs -cat /stock/outputs/part-00000</strong>
</pre></div></li><li class="listitem" style="list-style-type: disc">The headers of the output of your MapReduce program will look as follows:<div><pre class="programlisting">
<strong>change    frequency</strong>
</pre></div></li><li class="listitem" style="list-style-type: disc">The <a id="id585" class="indexterm"/>following figure shows the sample output of MapReduce problem:<div><img src="img/3282OS_05_16.jpg" alt="Performing analytics over data"/></div></li></ul></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec42"/>Visualizing data</h3></div></div></div><p>We can get<a id="id586" class="indexterm"/> more insights if we visualize our output with various graphs in R. Here, we have tried to visualize the output with the help of the <code class="literal">ggplot2</code> package.</p><div><img src="img/3282OS_05_19.jpg" alt="Visualizing data"/></div><p>From the previous graph, we can quickly identify that most of the time the stock price has changed from around 0 to 1.5. So, the stock's price movements in the history will be helpful at the time of investing.</p><p>The required code for generating this graph is as follows:</p><div><pre class="programlisting"># Loading ggplot2 library
library(ggplot2);

# we have stored above terminal output to stock_output.txt file

#loading it to R workspace
myStockData &lt;- read.delim("stock_output.txt", header=F, sep="", dec=".");

# plotting the data with ggplot2 geom_smooth function
ggplot(myStockData, aes(x=V1, y=V2)) + geom_smooth() + geom_point();</pre></div><p>In the<a id="id587" class="indexterm"/> next section, we have included the case study on how Big Data analytics is performed with R and Hadoop for the <strong>Kaggle</strong> data<a id="id588" class="indexterm"/> competition.</p></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec58"/>Predicting the sale price of blue book for bulldozers – case study</h2></div></div></div><p>This is a case study<a id="id589" class="indexterm"/> for predicting the auction sale price for a piece of heavy equipment to create a blue book for bulldozers.</p><div><div><div><div><h3 class="title"><a id="ch05lvl3sec43"/>Identifying the problem</h3></div></div></div><p>In this <a id="id590" class="indexterm"/>example, I have included a case study by Cloudera data scientists on how large datasets can be resampled, and applied the random forest model with R and Hadoop. Here, I have considered the Kaggle blue book for bulldozers competition for understanding the types of Big Data problem definitions. Here, the goal of this competition is to predict the sale price of a particular piece of heavy equipment at a usage auction based on its usage, equipment type, and configuration. This solution has been provided by <em>Uri Laserson</em> (Data Scientist at Cloudera). The provided data contains the information about auction result posting, usage, and equipment configuration.</p><p>It's a trick to model the Big Data sets and divide them into the smaller datasets. Fitting the model on that dataset is a traditional machine learning technique such as random forests or bagging. There are possibly two reasons for random forests:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Large datasets typically live in a cluster, so any operations will have some level of parallelism. Separate models fit on separate nodes that contain different subsets of the initial data.</li><li class="listitem" style="list-style-type: disc">Even if you can use the entire initial dataset to fit a single model, it turns out that ensemble methods, where you fit multiple smaller models by using subsets of data, generally outperform single models. Indeed, fitting a single model with 100M data points can perform worse than fitting just a few models with 10M data points each (so smaller total data outperforms larger total data).</li></ul></div><p>Sampling <a id="id591" class="indexterm"/>with replacement is the most popular method for sampling from the initial dataset for producing a collection of samples for model fitting. This method is equivalent to sampling from a multinomial distribution, where the probability of selecting any individual input data point is uniform over the entire dataset.</p><div><div><h3 class="title"><a id="tip17"/>Tip</h3><p>Kaggle<a id="id592" class="indexterm"/> is a Big Data platform where data scientists from all over the world compete to solve Big Data analytics problems hosted by data-driven organizations.</p></div></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec44"/>Designing data requirement</h3></div></div></div><p>For<a id="id593" class="indexterm"/> this competition, Kaggle has provided real-world datasets that comprises approximately 4,00,000 training data points. Each data point represents the various attributes of sales, configuration of the bulldozer, and sale price. To find out where to predict the sales price, the random forest regression model needs to be implemented.</p><div><div><h3 class="title"><a id="note03"/>Note</h3><p>The reference link for this Kaggle competition is <a class="ulink" href="http://www.kaggle.com/c/bluebook-for-bulldozers">http://www.kaggle.com/c/bluebook-for-bulldozers</a>. You can check the data, information, forum, and leaderboard as well as explore some other Big Data analytics competitions and participate in them to evaluate your data analytics skills.</p></div></div><p>We chose this model because we are interested in predicting the sales price in numeric values from random sets of a large dataset.</p><p>The datasets are provided in the terms of the following data files:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>File name</p>
</th><th style="text-align: left" valign="bottom">
<p>Description format (size)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">Train</code></p>
</td><td style="text-align: left" valign="top">
<p>This is a training set that contains data for 2011.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">Valid</code></p>
</td><td style="text-align: left" valign="top">
<p>This is a validation set that contains data from January 1, 2012 to April 30, 2012.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">Data dictionary</code></p>
</td><td style="text-align: left" valign="top">
<p>This is the metadata of the training dataset variables.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">Machine_Appendix</code></p>
</td><td style="text-align: left" valign="top">
<p>This contains the correct year of manufacturing for a given machine along with the make, model, and product class details.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">Test</code></p>
</td><td style="text-align: left" valign="top">
<p>This tests datasets.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">random_forest_benchmark_test</code></p>
</td><td style="text-align: left" valign="top">
<p>This is the benchmark solution provided by the host.</p>
</td></tr></tbody></table></div><div><div><h3 class="title"><a id="tip18"/>Tip</h3><p>In <a id="id594" class="indexterm"/>case you want to learn and practice Big Data analytics, you can acquire the Big Data sets from the Kaggle data source by participating in the Kaggle data competitions. These contain the datasets of various fields from industries worldwide.</p></div></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec45"/>Preprocessing data</h3></div></div></div><p>To <a id="id595" class="indexterm"/>perform the analytics over the provided Kaggle datasets, we need to build a predictive model. To predict the sale price for the auction, we will fit the model over provided datasets. But the datasets are provided with more than one file. So we will merge them as well as perform data augmentation for acquiring more meaningful data. We are going to build a model from <code class="literal">Train.csv</code> and <code class="literal">Machine_Appendix.csv</code> for better prediction of the sale price.</p><p>Here are the data preprocessing tasks that need to be performed over the datasets:</p><div><pre class="programlisting"># Loading Train.csv dataset which includes the Sales as well as machine identifier data attributes.

transactions &lt;- read.table(file="~/Downloads/Train.csv",
header=TRUE,
sep=",",
quote="\"",
row.names=1,
fill=TRUE,
colClasses=c(MachineID="factor",
 ModelID="factor",
datasource="factor",
YearMade="character",
SalesID="character",
auctioneerID="factor",
UsageBand="factor",
saledate="custom.date.2",
Tire_Size="tire.size",
Undercarriage_Pad_Width="undercarriage",
Stick_Length="stick.length"),
na.strings=na.values)

# Loading Machine_Appendix.csv for machine configuration information

machines &lt;- read.table(file="~/Downloads/Machine_Appendix.csv",
header=TRUE,
sep=",",
quote="\"",
fill=TRUE,
colClasses=c(MachineID="character",
ModelID="factor",
fiManufacturerID="factor"),
na.strings=na.values)


# Updating the values to numeric 
# updating sale data number
transactions$saledatenumeric &lt;- as.numeric(transactions$saledate)
transactions$ageAtSale &lt;- as.numeric(transactions$saledate - as.Date(transactions$YearMade, format="%Y"))

transactions$saleYear &lt;- as.numeric(format(transactions$saledate, "%Y"))

# updating the month of sale from transaction
transactions$saleMonth &lt;- as.factor(format(transactions$saledate, "%B"))

# updating the date of sale from transaction
transactions$saleDay &lt;- as.factor(format(transactions$saledate, "%d"))

# updating the day of week of sale from transaction
transactions$saleWeekday &lt;- as.factor(format(transactions$saledate, "%A"))

# updating the year of sale from transaction
transactions$YearMade &lt;- as.integer(transactions$YearMade)

# deriving the model price from transaction
transactions$MedianModelPrice &lt;- unsplit(lapply(split(transactions$SalePrice, 
transactions$ModelID), median), transactions$ModelID)


# deriving the model count from transaction
transactions$ModelCount &lt;- unsplit(lapply(split(transactions$SalePrice, transactions$ModelID), length), transactions$ModelID)

# Merging the transaction and machine data in to dataframe 
training.data &lt;- merge(x=transactions, y=machines, by="MachineID")

# write denormalized data out
write.table(x=training.data,
file="~/temp/training.csv",
sep=",",
quote=TRUE,
row.names=FALSE,
eol="\n",
col.names=FALSE)
# Create poisson directory at HDFS
bin/hadoop dfs -mkdir /poisson

# Uploading file training.csv at HDFS
bin/hadoop dfs -put ~/temp/training.csv /poisson/</pre></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec46"/>Performing analytics over data</h3></div></div></div><p>As we<a id="id596" class="indexterm"/> are going to perform analytics with sampled datasets, we need to understand how many datasets need to be sampled.</p><p>For random sampling, we have considered three model parameters, which are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We have N data points in our initial training set. This is very large (106-109) and is distributed over an HDFS cluster.</li><li class="listitem" style="list-style-type: disc">We are going to train a set of M different models for an ensemble classifier.</li><li class="listitem" style="list-style-type: disc">Each of the M models will be fitted with K data points, where typically K &lt;&lt; N. (For example, K may be 1-10 percent of N.).</li></ul></div><p>We have N numbers of training datasets, which are fixed and generally outside our control. As we are going to handle this via <strong>Poisson</strong> sampling, we need to define the total number of<a id="id597" class="indexterm"/> input vectors to be consumed into the random forest model.</p><p>There<a id="id598" class="indexterm"/> are three cases to be considered:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>KM &lt; N</strong>: In this case, we are not using the full amount of data available to us</li><li class="listitem" style="list-style-type: disc"><strong>KM = N</strong>: In this case, we can exactly partition our dataset to produce totally independent samples</li><li class="listitem" style="list-style-type: disc"><strong>KM &gt; N</strong>: In this case, we must resample some of our data with replacements</li></ul></div><p>The Poisson sampling method described in the following section handles all the three cases in the same framework. However, note that for the case KM = N, it does not partition the data, but simply resamples it.</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec47"/>Understanding Poisson-approximation resampling</h3></div></div></div><p>Generalized linear models are an extension of the general linear model. Poisson regression is a situation of generalized models. The dependent variable obeys Poisson distribution.</p><p>Poisson sampling will be run on the Map of the MapReduce task because it occurs for input data points. This doesn't guarantee that every data point will be considered into the model, which is better than multinomial resampling of full datasets. But it will guarantee the generation of independent samples by using N training input points.</p><p>Here, the following graph indicates the amount of missed datasets that can be retrieved in the Poisson sampling with the function of KM/N:</p><div><img src="img/3282OS_05_17.jpg" alt="Understanding Poisson-approximation resampling"/></div><p>The grey <a id="id599" class="indexterm"/>line indicates the value of KM=N. Now, let's look at the pseudo code of the MapReduce algorithm. We have used three parameters: N, M, and K where K is fixed. We used T=K/N to eliminate the need for the value of N in advance.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>An example of sampling parameters</strong>: Here, we will implement the preceding logic with a pseudo code. We will start by defining two model input parameters as <code class="literal">frac.per.model</code> and <code class="literal">num.models</code>, where <code class="literal">frac.per.model</code> is used for defining the fraction of the full dataset that can be used, and <code class="literal">num.models</code> is used for defining how many models will be fitted from the dataset.<div><pre class="programlisting">T = 0.1  # param 1: K / N-average fraction of input data in each model 10%

M = 50   # param 2: number of models</pre></div></li><li class="listitem" style="list-style-type: disc"><strong>Logic of Mapper</strong>: Mapper will be designed for generating the samples of the full dataset by data wrangling.<div><pre class="programlisting">def map(k, v):
// for each input data point
    for i in 1:M  
    // for each model
        m = Poisson(T)  
    // num times curr point should appear in this sample
        if m &gt; 0
            for j in 1:m
   // emit current input point proper num of times
                emit (i, v)</pre></div></li><li class="listitem" style="list-style-type: disc"><strong>Logic of Reducer</strong>: Reducer will take a data sample as input and fit the random forest model over it.<div><pre class="programlisting">def reduce(k, v):
    fit model or calculate statistic with the sample in v</pre></div></li></ul></div><div><div><div><div><h4 class="title"><a id="ch05lvl4sec04"/>Fitting random forests with RHadoop</h4></div></div></div><p>In machine <a id="id600" class="indexterm"/>learning, fitting a model means fitting the best line <a id="id601" class="indexterm"/>into our data. Fitting a model<a id="id602" class="indexterm"/> can fall under several types, namely, under fitting, over fitting, and normal fitting. In case of under and over fitting, there are chances of high bias (cross validation and training errors are high) and high variance (cross validation error is high but training error is low) effects, which is not good. We will normally fit the model over the datasets.</p><p>Here are the diagrams for fitting a model over datasets with three types of fitting:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Under fitting</strong>: In <a id="id603" class="indexterm"/>this cross validation and training <a id="id604" class="indexterm"/>errors are high<div><img src="img/3282OS_05_22.jpg" alt="Fitting random forests with RHadoop"/></div></li><li class="listitem" style="list-style-type: disc"><strong>Normal fitting</strong>: In<a id="id605" class="indexterm"/> this <a id="id606" class="indexterm"/>cross-validation and<a id="id607" class="indexterm"/> training errors are <a id="id608" class="indexterm"/>normal<div><img src="img/3282OS_05_23.jpg" alt="Fitting random forests with RHadoop"/></div></li><li class="listitem" style="list-style-type: disc"><strong>Over fitting</strong>: In <a id="id609" class="indexterm"/>this <a id="id610" class="indexterm"/>the cross-validation error<a id="id611" class="indexterm"/> is high but training<a id="id612" class="indexterm"/> error is low<div><img src="img/3282OS_05_24.jpg" alt="Fitting random forests with RHadoop"/></div></li></ul></div><p>We will fit the model over the data using the random forest technique of machine learning. This is a type of recursive partitioning method, particularly well suited for small and large problems. It involves an ensemble (or set) of classification (or regression) trees that are calculated on random subsets of the data, using a subset of randomly restricted and selected predictors for every split in each classification tree.</p><p>Furthermore, the results of an ensemble of classification/regression trees have been used to produce better predictions instead of using the results of just one classification tree.</p><p>We will now implement our Poisson sampling strategy with RHadoop. We will start by setting global values for our parameters:</p><div><pre class="programlisting">#10% of input data to each sample on avg
frac.per.model &lt;- 0.1  
num.models &lt;- 50</pre></div><p>Let's check how to implement Mapper as per the specifications in the pseudo code with RHadoop.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Mapper is implemented in the the following manner:<div><pre class="programlisting">poisson.subsample &lt;- function(k, input) {
  # this function is used to generate a sample from the current block of data
  generate.sample &lt;- function(i) {
    # generate N Poisson variables
    draws &lt;- rpois(n=nrow(input), lambda=frac.per.model)
    # compute the index vector for the corresponding rows,
    # weighted by the number of Poisson draws
    indices &lt;- rep((1:nrow(input)), draws)
    # emit the rows; RHadoop takes care of replicating the key appropriately
    # and rbinding the data frames from different mappers together for the
    # reducer
    keyval(i, input[indices, ])
  }

  # here is where we generate the actual sampled data
  c.keyval(lapply(1:num.models, generate.sample))
}</pre></div><p>Since we are using R, it's tricky to fit the model with the random forest model over the collected sample dataset.</p></li><li class="listitem" style="list-style-type: disc">Reducer <a id="id613" class="indexterm"/>is implemented<a id="id614" class="indexterm"/> in the following manner:<div><pre class="programlisting"># REDUCE function
fit.trees &lt;- function(k, v) {
  # rmr rbinds the emitted values, so v is a dataframe
  # note that do.trace=T is used to produce output to stderr to keep the reduce task from timing out
  rf &lt;- randomForest(formula=model.formula,
                        data=v,
                        na.action=na.roughfix,
                        ntree=10,
                        do.trace=FALSE)
 
 # rf is a list so wrap it in another list to ensure that only
 # one object gets emitted. this is because keyval is vectorized
  keyval(k, list(forest=rf))
}</pre></div></li><li class="listitem" style="list-style-type: disc">To fit the model, we need <code class="literal">model.formula</code>, which is as follows:<div><pre class="programlisting">
<strong>model.formula &lt;- SalePrice ~ datasource + auctioneerID + YearMade + saledatenumeric + ProductSize + ProductGroupDesc.x + Enclosure + Hydraulics + ageAtSale + saleYear + saleMonth + saleDay + saleWeekday + MedianModelPrice + ModelCount + MfgYear</strong>
</pre></div><p><code class="literal">SalePrice</code> is defined as a response variable and the rest of them are defined as predictor variables for the random forest model.</p><div><div><h3 class="title"><a id="tip19"/>Tip</h3><p>Random forest model with R doesn't support factor with level more than 32.</p></div></div></li><li class="listitem" style="list-style-type: disc">The <a id="id615" class="indexterm"/>MapReduce job can <a id="id616" class="indexterm"/>be executed using the following command:<div><pre class="programlisting">
<strong>mapreduce(input="/poisson/training.csv",</strong>
<strong>               input.format=bulldozer.input.format,</strong>
<strong>               map=poisson.subsample,</strong>
<strong>               reduce=fit.trees,</strong>
<strong>               output="/poisson/output")</strong>
</pre></div><p>The resulting trees are dumped in HDFS at <code class="literal">/poisson/output</code>.</p></li><li class="listitem" style="list-style-type: disc">Finally, we can load the trees, merge them, and use them to classify new test points:<div><pre class="programlisting">
<strong>mraw.forests &lt;- values(from.dfs("/poisson/output"))</strong>
<strong>forest &lt;- do.call(combine, raw.forests)</strong>
</pre></div></li></ul></div><p>Each of the 50 samples produced a random forest with 10 trees, so the final random forest is a collection of 500 trees, fitted in a distributed fashion over a Hadoop cluster.</p><div><div><h3 class="title"><a id="note04"/>Note</h3><p>The full set of source files is available on the official Cloudera blog at <a class="ulink" href="http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/">http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/</a>.</p></div></div><p>Hopefully, we have learned a scalable approach for training ensemble classifiers or bootstrapping in a parallel fashion by using a Poisson approximation for multinomial sampling.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Summary</h1></div></div></div><p>In this chapter, we learned how to perform Big Data analytics with various data driven activities over an R and Hadoop integrated environment.</p><p>In the next chapter, we will learn more about how R and Hadoop can be used to perform machine learning techniques.</p></div></body></html>