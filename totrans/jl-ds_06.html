<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch06" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 6. Supervised Machine Learning</h1></div></div></div><p class="calibre11">It is often believed that data science is machine learning, which means in data science, we only train models of machine learning. But data science is much more than that. Data science involves understanding data, gathering data, munging data, taking the meaning out of that data, and then machine learning if needed.</p><p class="calibre11">In my opinion, machine learning is the most exciting field that exists today. With huge amounts of data that is readily available, we can gather invaluable knowledge. Lots of companies have made their machine learning libraries accessible and there are lots of open source alternatives that exist.</p><p class="calibre11">In this chapter, you will study the following topics:</p><div><ul class="itemizedlist"><li class="listitem">What is machine learning?</li><li class="listitem">Types of machine learning</li><li class="listitem">What is overfitting and underfitting?</li><li class="listitem">Bias-variance trade-off</li><li class="listitem">Feature extraction and selection</li><li class="listitem">Decision trees</li><li class="listitem">Naïve Bayes classifier</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec55" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>What is machine learning?</h1></div></div></div><p class="calibre11">Generally, when we talk about machine learning, we get into the idea of us fighting wars with intelligent machines that we created but went out of control. These machines are able to outsmart the human race and become a threat to human existence. These theories are just created for our entertainment. We are still very far away from such machines.</p><p class="calibre11">So, the question is: what is machine learning? Tom M. Mitchell gave a formal definition:</p><div><blockquote class="blockquote"><p class="calibre22">
<em class="calibre23">"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."</em>
</p></blockquote></div><p class="calibre11">This implies that machine learning is teaching computers to generate algorithms using data without programming them explicitly. It transforms data into actionable knowledge. Machine learning has close association with statistics, probability, and mathematical optimization.</p><p class="calibre11">As technology grows, there is one thing that grows with it exponentially—data. We have huge amounts of unstructured and structured data growing at a very great pace. Lots of data is generated by space observatories, meteorologists, biologists, fitness sensors, surveys, and so on. It is not possible to manually go through this much amount of data and find patterns or gain insights. This data is very important for scientists, domain experts, governments, health officials, and even businesses. To gain knowledge out of this data, we need self-learning algorithms that can help us in decision making.</p><p class="calibre11">Machine learning evolved as a subfield of artificial intelligence, which eliminates the need to manually analyze large amounts of data. Instead of using machine learning, we make data-driven decisions by gaining knowledge using self-learning predictive models. Machine learning has become important in our daily lives. Some common use cases include search engines, games, spam filters, and image recognition. Self-driving cars also uses machine learning.</p><p class="calibre11">Some basic terminologies used in machine learning include:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Features</strong>: Distinctive characteristics of the data point or record</li><li class="listitem"><strong class="calibre19">Training set</strong>: This is the dataset that we feed to train the algorithm that helps us to find relationships or build a model</li><li class="listitem"><strong class="calibre19">Testing set</strong>: The algorithm generated using the training dataset is tested on the testing dataset to find the accuracy</li><li class="listitem"><strong class="calibre19">Feature vector</strong>: An n-dimensional vector that contains the features defining an object</li><li class="listitem"><strong class="calibre19">Sample</strong>: An item from the dataset or the record</li></ul></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec79" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Uses of machine learning</h2></div></div></div><p class="calibre11">Machine learning in one way or another is used everywhere. Its applications are endless. Let's discuss some very common use cases:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">E-mail spam filtering</strong>: Every major e-mail service provider uses machine learning to filter out spam messages from the Inbox to the Spam folder.</li><li class="listitem"><strong class="calibre19">Predicting storms and natural disasters</strong>: Machine learning is used by meteorologists and geologists to predict the natural disasters using weather data, which can help us to take preventive measures.</li><li class="listitem"><strong class="calibre19">Targeted promotions/campaigns and advertising</strong>: On social sites, search engines, and maybe in mailboxes, we see advertisements that somehow suit our tastes. This is made feasible using machine learning on the data from our past searches, our social profile, or e-mail contents.</li><li class="listitem"><strong class="calibre19">Self-driving cars</strong>: Technology giants are currently working on self-driving cars. This is made possible using machine learning on the feed of the actual data from human drivers, image and sound processing, and various other factors.</li><li class="listitem">Machine learning is also used by businesses to predict the market.</li><li class="listitem">It can also be used to predict the outcomes of elections and the sentiment of voters towards a particular candidate.</li><li class="listitem">Machine learning is also being used to prevent crime. By understanding the pattern of different criminals, we can predict a crime that can happen in the future and can prevent it.</li></ul></div><p class="calibre11">One case that got a huge amount of attention was of a big retail chain in the United States using machine learning to identify pregnant women. The retailer thought of a strategy to give discounts on multiple maternity products, so that the women would become loyal customers and would baby purchase items with a high profit margin.</p><p class="calibre11">The retailer worked on the algorithm to predict the pregnancy using useful patterns in purchases of different products which are useful for pregnant women.</p><p class="calibre11">Once a man approached the retailer and asked for the reason that his teenage daughter is receiving discount coupons for maternity items. The retail chain offered an apology but later the father himself apologized when he got to know that his daughter was indeed pregnant.</p><p class="calibre11">This story may or may not be completely true, but retailers do analyze their customers' data routinely to find out patterns for targeted promotions, campaigns, and inventory management.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec80" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Machine learning and ethics</h2></div></div></div><p class="calibre11">Let's see where machine learning is used very frequently:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Retailers</strong>: In the previous example, we mentioned how retail chains use data for machine learning to increase their revenue as well as to retain their customers</li><li class="listitem"><strong class="calibre19">Spam filtering</strong>: E-mails are processed using various machine learning algorithms for spam filtering</li><li class="listitem"><strong class="calibre19">Targeted advertisements</strong>: In our mailbox, social sites, or search engines, we see advertisements of our liking</li></ul></div><p class="calibre11">These are only some of the actual use cases that are implemented in the world today. One thing that is common between them is the user data.</p><p class="calibre11">In the first example, retailers are using the history of transactions done by the user for targeted promotions and campaigns and for inventory management, among other things. Retail giants do this by providing users a loyalty or sign-up card.</p><p class="calibre11">In the second example, the e-mail service provider uses trained machine learning algorithms to detect and flag spam. It does so by going through the contents of e-mail/attachments and classifying the sender of the e-mail.</p><p class="calibre11">In the third example, again the e-mail provider, social network, or search engine will go through our cookies, our profiles, or our mails to do the targeted advertising.</p><p class="calibre11">In all of these examples, it is mentioned in the terms and conditions of the agreement when we sign up with the retailer, e-mail provider, or social network that the user's data will be used but privacy will not be violated.</p><p class="calibre11">It is really important that before using data that is not publicly available, we take the required permissions. Also, our machine learning models shouldn't discriminate on the basis of region, race, and sex, or of any other kind. The data provided should not be used for purposes not mentioned in the agreement or if it is illegal in the region or country of existence.</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec56" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Machine learning – the process</h1></div></div></div><p class="calibre11">Machine learning algorithms are trained in keeping with the idea of how the human brain works. They are somewhat similar. Let's discuss the whole process.</p><p class="calibre11">The machine learning process can be described in three steps:</p><div><ol class="orderedlist"><li class="listitem1">Input</li><li class="listitem1">Abstraction</li><li class="listitem1">Generalization</li></ol></div><p class="calibre11">These three steps are the core of how the machine learning algorithm works. Although the algorithm may or may not be divided or represented in such a way, this explains the overall approach:</p><div><ol class="orderedlist"><li class="listitem1">The first step concentrates on what data should be there and what shouldn't. On the basis of that, it gathers, stores, and cleans the data as per the requirements.</li><li class="listitem1">The second step entails the data being translated to represent the bigger class of data. This is required as we cannot capture everything and our algorithm should not be applicable for only the data that we have.</li><li class="listitem1">The third step focuses on the creation of the model or an action that will use this abstracted data, which will be applicable for the broader mass.</li></ol></div><p class="calibre11">So, what should be the flow of approaching a machine learning problem?</p><p class="calibre11">
</p><div><img src="img/B05321_06_01.jpg" alt="Machine learning – the process" class="calibre178"/></div><p class="calibre11">
</p><p class="calibre11">In this particular figure, we see that the data goes through the abstraction process before it can be used to create the machine learning algorithm. This process itself is cumbersome. We studied this process in the chapter related to data munging.</p><p class="calibre11">The process follows the training of the model, which is fitting the model into the dataset that we have. The computer does not pick up the model on its own, but it is dependent on the learning task. The learning task also includes generalizing the knowledge gained on the data that we don't have yet.</p><p class="calibre11">Therefore, training the model is based on the data that we currently have and the learning task includes generalization of the model for future data.</p><p class="calibre11">It depends on how our model deduces knowledge from the dataset that we currently have. We need to make such a model that can gather insights into something that wasn't known to us before can be useful and can be linked to future data.</p><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec81" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Different types of machine learning</h2></div></div></div><p class="calibre11">Machine learning is divided mainly into three categories:</p><div><ul class="itemizedlist"><li class="listitem">Supervised learning</li><li class="listitem">Unsupervised learning</li><li class="listitem">Reinforcement learning</li></ul></div><p class="calibre11">In supervised learning, the model/machine is presented with inputs and the outputs corresponding to those inputs. The machine learns from these inputs and applies this learning in further unseen data to generate outputs.</p><p class="calibre11">Unsupervised learning doesn't have the required outputs; therefore it is up to the machine to learn and find patterns that were previously unseen.</p><p class="calibre11">In reinforcement learning, the machine continuously interacts with the environment and learns through this process. This includes a feedback loop.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec82" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>What is bias-variance trade-off?</h2></div></div></div><p class="calibre11">Let's understand what bias and variance are. First we will go through bias in our model:</p><div><ul class="itemizedlist"><li class="listitem">Bias is the difference between the predictions that have been generated by the model and the correct value that was expected or we should have received.</li><li class="listitem">When we get the new data, the model will work out and give predictions. Therefore, it means our model has a range of predictions it can generate.</li><li class="listitem">Bias is the correctness of this range of predictions.</li></ul></div><p class="calibre11">Now, let's understand variance and how it affects the model:</p><div><ul class="itemizedlist"><li class="listitem">Variance is the variability of the model when the data points are changed or new data is introduced</li><li class="listitem">It shouldn't be required to tweak the model every time new data is introduced</li></ul></div><p class="calibre11">As per our understanding of bias and variance, we can conclude that these affect each other. Therefore, while creating the model, we keep this trade-off in consideration.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec83" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Effects of overfitting and underfitting on a model</h2></div></div></div><p class="calibre11">Overfitting happens when the model that we have created also starts considering the outliers or noise in our dataset. Therefore, it means our model is fitting the dataset rather too well.</p><p class="calibre11">The drawback of such a model is that it will not be able to generalize well. Such models have low bias and high variance.</p><p class="calibre11">Underfitting happens when the model that we have created is not able to find out the patterns or trend of the data as is desired. Therefore, it means the model is not fitting to the dataset well.</p><p class="calibre11">The drawback of such a model is that it is not able to give good predictions. Such models have high bias and low variance.</p><p class="calibre11">We should try to reduce both underfitting and overfitting. This is done through various techniques. Ensemble models are very good in avoiding underfitting and overfitting. We will study ensemble models in upcoming chapters.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec57" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding decision trees</h1></div></div></div><p class="calibre11">A decision tree is a very good example of "divide and conquer". It is one of the most practical and widely used methods for inductive inference. It is a supervised learning method that can be used for both classification and regression. It is non-parametric and its aim is to learn by inferring simple decision rules from the data and create such a model that can predict the value of the target variable.</p><p class="calibre11">Before taking a decision, we analyze the probability of the pros and cons by weighing the different options that we have. Let's say we want to purchase a phone and we have multiple choices in the price segment. Each of the phones has something really good, and maybe better than the other. To make a choice, we start by considering the most important feature that we want. And as such, we create a series of features that it has to pass to become the ultimate choice.</p><p class="calibre11">In this section, we will learn about:</p><div><ul class="itemizedlist"><li class="listitem">Decision trees</li><li class="listitem">Entropy measures</li><li class="listitem">Random forests</li></ul></div><p class="calibre11">We will also learn about famous decision tree learning algorithms such as ID3 and C5.0.</p><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec84" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Building decision trees – divide and conquer</h2></div></div></div><p class="calibre11">A heuristic called recursive partitioning is used to build decision trees. In this approach, our data is split into similar classes of smaller and smaller subsets as we move along.</p><p class="calibre11">A decision tree is actually an inverted tree. It starts from the root and ends up at the leaf nodes, which are the terminal nodes. The splitting of the node into branches is based on logical decisions. The whole dataset is represented at the root node. A feature is chosen by the algorithm that is most predictive of the target class. Then it partitions the examples into distinct value groups of this particular feature. This represents the first set of the branches of our tree.</p><p class="calibre11">The divide-and-conquer approach is followed until the end point is reached. At each step, the algorithm continues to choose the best candidate feature.</p><p class="calibre11">The end point is defined when:</p><div><ul class="itemizedlist"><li class="listitem">At a particular node, nearly all the examples belong to the same class</li><li class="listitem">The feature list is exhausted</li><li class="listitem">A predefined size limit of the tree is reached</li></ul></div><p class="calibre11">
</p><div><img src="img/B05321_06_02.jpg" alt="Building decision trees – divide and conquer" class="calibre179"/></div><p class="calibre11">
</p><p class="calibre11">The preceding image is a very famous example of decision tree. Here, a decision tree is made to find out whether to go out or not:</p><div><ul class="itemizedlist"><li class="listitem">Outlook is the root node. This refers to all the possible classes of the environment</li><li class="listitem">Sunny, overcast, and Rain are the branches.</li><li class="listitem">Humidity and Wind are the leaf nodes, which are again split into branches, and a decision is taken depending on the favorable environment.</li></ul></div><p class="calibre11">These trees can also be re-represented as if-then rules, which would be easily understandable. Decision trees are one of the very successful and popular algorithms, with a broad range of applications.</p><p class="calibre11">The following are some of the applications of decision trees:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Decision of credit card / loan approval</strong>: Credit scoring models are based on decision trees, where every applicant's information is fed to decide whether a credit card / loan should be approved or not.</li><li class="listitem"><strong class="calibre19">Medical diagnosis</strong>: Various diseases are diagnosed using well-defined and tested decision trees based on symptoms, measurements, and tests.</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec85" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Where should we use decision tree learning?</h2></div></div></div><p class="calibre11">Although there are various decision tree learning methods that can be used for a variety of problems, decision trees are best suited for the following scenarios:</p><div><ul class="itemizedlist"><li class="listitem">Attribute-value pairs are scenarios where instances are described by attributes from a fixed set and values. In the previous example, we had the attribute as "Wind" and the values as "Strong" and "Weak". These disjoint possible values make it easy to create decision tree learning, although attributes with real values can also be used.</li><li class="listitem">The final output of the target function has a discreet value, like the previous example, where we had "Yes" or "No". The decision tree algorithm can be extended to have more than two possible target values. Decision trees can also be extended to have real values as outputs, but this is rarely used.</li><li class="listitem">The decision tree algorithm is robust to errors in the training dataset. These errors can be in the attribute values of the examples or the classification of the examples or both.</li><li class="listitem">Decision tree learning is also suited for missing values in a dataset. If the values are missing in some examples where they are available for attributes in other examples, then decision trees can be used.</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec86" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Advantages of decision trees</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">It is easy to understand and interpret decision trees. Visualizing decision trees is easy too.</li><li class="listitem">In other algorithms, data normalization needs to be done before they can be applied. Normalization refers to the creation of dummy variables and removing blank values. Decision trees, on the other hand, require very less data preparation.</li><li class="listitem">The cost involved in prediction using a decision tree is logarithmic with respect to the number of examples used in training the tree.</li><li class="listitem">Decision trees, unlike other algorithms, can be applied to both numerical and categorical data. Other algorithms are generally specialized to be used for only one type of variable.</li><li class="listitem">Decision trees can easily take care of the problems where multiple outputs is a possibility.</li><li class="listitem">Decision trees follow the white box model, which means a condition is easily explained using Boolean logic if the situation is observable in the model. On the other hand, results are comparatively difficult to interpret in a black box model, such as artificial neural networks.</li><li class="listitem">Statistical tests can be used to validate the model. Therefore, we can test the reliability of the model.</li><li class="listitem">It is able to perform well even if there is a violation in the assumptions from the true model that was the source of the data.</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec87" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Disadvantages of decision trees</h2></div></div></div><p class="calibre11">We've covered where decision trees are suited and their advantages. Now we will go through the disadvantages of decision trees:</p><div><ul class="itemizedlist"><li class="listitem">There is always a possibility of overfitting the data in decision trees. This generally happens when we create trees that are over-complex and are not possible to generalize well.</li><li class="listitem">To avoid this, various steps can be taken. One method is pruning. As the name suggests, it is a method where we set the maximum depth to which the tree can grow.</li><li class="listitem">Instability is always a concern with decision trees as a small variation in the data can result in generation of a different tree altogether.</li><li class="listitem">The solution to such a scenario is ensemble learning, which we will study in the next chapter.</li><li class="listitem">Decision tree learning may sometimes lead to the creation of biased trees, where some classes are dominant over others. The solution to such a scenario is balancing the dataset prior to fitting it to the decision tree algorithm.</li><li class="listitem">Decision tree learning is known to be NP-complete, considering several aspects of optimality. This holds even for the basic concepts.</li><li class="listitem">Usually, heuristic algorithms like greedy algorithms are used where a locally optimal decision is made at each node. This doesn't guarantee that we will have a decision tree that is globally optimal.</li><li class="listitem">Learning can be hard for the concepts such as Parity, XOR, and multiplexer problems, where decision trees cannot represent them easily.</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec88" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Decision tree learning algorithms</h2></div></div></div><p class="calibre11">There are various decision tree learning algorithms that are actually variations of the core algorithm. The core algorithm is actually a top-down, greedy search through all possible trees.</p><p class="calibre11">We are going to discuss two algorithms:</p><div><ul class="itemizedlist"><li class="listitem">ID3</li><li class="listitem">C4.5 and C5.0</li></ul></div><p class="calibre11">The first algorithm, <strong class="calibre19">ID3</strong> (<strong class="calibre19">Iterative Dichotomiser 3</strong>), was developed by Ross Quinlan in 1986. The algorithm proceeds by creating a multiway tree, where it uses a greedy search to find each node and the features that can yield maximum information gain for the categorical targets. As trees can grow to the maximum size, which can result in over-fitting of data, pruning is used to make the generalized model.</p><p class="calibre11">C4.5 came after ID3 and eliminated the restriction that all features must be categorical. It does this by defining dynamically a discrete attribute based on the numerical variables. This partitions into a discrete set of intervals from the continuous attribute value. C4.5 creates sets of if-then rules from the trained trees of the ID3 algorithm. C5.0 is the latest version; it builds smaller rule sets and uses comparatively lesser memory.</p><div><div><div><div><h3 class="title5"><a id="ch06lvl3sec35" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>How a decision tree algorithm works</h3></div></div></div><p class="calibre11">The decision tree algorithm constructs the top-down tree. It follows these steps:</p><div><ol class="orderedlist"><li class="listitem1">To know which element should come at the root of the tree, a statistical test is done on each instance of the attribute to determine how well the training examples can be classified using this attribute alone.</li><li class="listitem1">This leads to the selection of the best attribute at the root node of the tree.</li><li class="listitem1">Now at this root node, for each possible value of the attribute, the descendants are created.</li><li class="listitem1">The examples in our training dataset are sorted to each of these descendant nodes.</li><li class="listitem1">Now for these individual descendant nodes, all the previous steps are repeated for the remaining examples in our training dataset.</li><li class="listitem1">This leads to the creation of an acceptable tree for our training dataset using a greedy search. The algorithm never backtracks, which means it never reconsiders the previous choices and follows the tree downwards.</li></ol></div></div><div><div><div><div><h3 class="title5"><a id="ch06lvl3sec36" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding and measuring purity of node</h3></div></div></div><p class="calibre11">The decision tree is built top-down. It can be tough to decide on which attribute to split on each node. Therefore, we find the feature that best splits the target class. Purity is the measure of a node containing only one class.</p><p class="calibre11">Purity in C5.0 is measured using entropy. The entropy of the sample of the examples is the indication of how class values are mixed across the examples:</p><div><ul class="itemizedlist"><li class="listitem">0: The minimum value is an indication of the homogeneity in the class values in the sample</li><li class="listitem">1: The maximum value is an indication that there is maximum amount of disorder in the class values in the sample</li></ul></div><p class="calibre11">Entropy is given by:</p><p class="calibre11">
</p><div><img src="img/image_06_003.jpg" alt="Understanding and measuring purity of node" class="calibre180"/></div><p class="calibre11">
</p><p class="calibre11">In the preceding formula, <em class="calibre23">S</em> refers to the dataset that we have and <em class="calibre23">c</em> refers to the class levels. For a given class <em class="calibre23">i</em>, <em class="calibre23">p</em> is the proportion of the values.</p><p class="calibre11">When the purity measure is determined, the algorithm has to decide on which feature the data should be split. To decide this, the entropy measure is used by the algorithm to calculate how the homogeneity differs on splitting on each possible feature. This particular calculation done by the algorithm is the information gain:</p><p class="calibre11">
</p><div><img src="img/image_06_004.jpg" alt="Understanding and measuring purity of node" class="calibre181"/></div><p class="calibre11">
</p><p class="calibre11">The difference between the entropy before splitting the dataset (<em class="calibre23">S1</em>) and the resulting partitions from splitting (<em class="calibre23">S2</em>) is called information gain (<em class="calibre23">F</em>).</p></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec89" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>An example</h2></div></div></div><p class="calibre11">Let's apply what we've learned to create a decision tree using Julia. We will be using the example available for Python on <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/">http://scikit-learn.org/</a> and Scikitlearn.jl by Cedric St-Jean.</p><p class="calibre11">We will first have to add the required packages:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Pkg.update() 
julia&gt; Pkg.add("DecisionTree") 
julia&gt; Pkg.add("ScikitLearn") 
julia&gt; Pkg.add("PyPlot") 
</strong>
</pre><p class="calibre11">ScikitLearn provides the interface to the much-famous library of machine learning for Python to Julia:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; using ScikitLearn 
julia&gt; using DecisionTree 
julia&gt; using PyPlot 
</strong>
</pre><p class="calibre11">After adding the required packages, we will create the dataset that we will be using in our example:</p><pre class="programlisting">j<strong class="calibre19">ulia&gt; # Create a random dataset 
julia&gt; srand(100) 
julia&gt; X = sort(5 * rand(80)) 
julia&gt; XX = reshape(X, 80, 1) 
julia&gt; y = sin(X) 
julia&gt; y[1:5:end] += 3 * (0.5 - rand(16)) 
</strong>
</pre><p class="calibre11">This will generate a 16-element <code class="literal">Array{Float64,1}</code>.</p><p class="calibre11">Now we will create instances of two different models. One model is where we will not limit the depth of the tree, and in other model, we will prune the decision tree on the basis of purity:</p><p class="calibre11">
</p><div><img src="img/image_06_005.jpg" alt="An example" class="calibre182"/></div><p class="calibre11">
</p><p class="calibre11">We will now fit the models to the dataset that we have. We will fit both the models.</p><p class="calibre11">
</p><div><img src="img/image_06_006.jpg" alt="An example" class="calibre183"/></div><p class="calibre11">
</p><p class="calibre11">This is the first model. Here our decision tree has <code class="literal">25</code> leaf nodes and a depth of <code class="literal">8</code>.</p><p class="calibre11">
</p><div><img src="img/image_06_007.jpg" alt="An example" class="calibre184"/></div><p class="calibre11">
</p><p class="calibre11">This is the second model. Here we prune our decision tree. This has <code class="literal">6</code> leaf nodes and a depth of <code class="literal">4</code>.</p><p class="calibre11">Now we will use the models to predict on the test dataset:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; # Predict 
julia&gt; X_test = 0:0.01:5.0 
julia&gt; y_1 = predict(regr_1, hcat(X_test)) 
julia&gt; y_2 = predict(regr_2, hcat(X_test)) 
</strong>
</pre><p class="calibre11">This creates a 501-element <code class="literal">Array{Float64,1}</code>.</p><p class="calibre11">To better understand the results, let's plot both the models on the dataset that we have:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; # Plot the results 
julia&gt; scatter(X, y, c="k", label="data") 
julia&gt; plot(X_test, y_1, c="g", label="no pruning", linewidth=2) 
julia&gt; plot(X_test, y_2, c="r", label="pruning_purity_threshold=0.05", linewidth=2) 
 
julia&gt; xlabel("data") 
julia&gt; ylabel("target") 
julia&gt; title("Decision Tree Regression") 
julia&gt; legend(prop=Dict("size"=&gt;10)) 
</strong>
</pre><p class="calibre11">Decision trees can tend to overfit data. It is required to prune the decision tree to make it more generalized. But if we do more pruning than required, then it may lead to an incorrect model. So, it is required that we find the most optimized pruning level.</p><p class="calibre11">
</p><div><img src="img/image_06_008.jpg" alt="An example" class="calibre185"/></div><p class="calibre11">
</p><p class="calibre11">It is quite evident that the first decision tree overfits to our dataset, whereas the second decision tree model is comparatively more generalized.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec58" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Supervised learning using Naïve Bayes</h1></div></div></div><p class="calibre11">Naïve Bayes is one of most famous machine learning algorithms to date. It is widely used in text classification techniques.</p><p class="calibre11">Naïve Bayes methods come under the set of supervised learning algorithms. It is a probabilistic classifier and is based on Bayes' theorem. It takes the "naïve" assumption that every pair of features is independent of one another.</p><p class="calibre11">And in spite of these assumptions, Naïve Bayes classifiers work really well. Their most famous use case is spam filtering. The effectiveness of this algorithm is justified by the requirement of quite a small amount of training data for estimating the required parameters.</p><p class="calibre11">These classifiers and learners are quite fast when compared to other methods.</p><p class="calibre11">
</p><div><img src="img/image_06_009.jpg" alt="Supervised learning using Naïve Bayes" class="calibre186"/></div><p class="calibre11">
</p><p class="calibre11">In this given formula:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre23">A</em> and <em class="calibre23">B</em> are events.</li><li class="listitem"><em class="calibre23">P(A)</em> and <em class="calibre23">P(B)</em> are probabilities of <em class="calibre23">A</em> and <em class="calibre23">B</em>.</li><li class="listitem">These are prior probabilities and are independent of each other.</li><li class="listitem"><em class="calibre23">P(A | B)</em> is the probability of <em class="calibre23">A</em> with the condition that <em class="calibre23">B</em> is true. It is the posterior probability of class (<em class="calibre23">A</em>, target) given predictor (<em class="calibre23">B</em>, attributes).</li><li class="listitem"><em class="calibre23">P(B | A)</em> is the probability of <em class="calibre23">B</em> with the condition that <em class="calibre23">A</em> is true. It is the likelihood, which is the probability of the predictor given class.</li></ul></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec90" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Advantages of Naïve Bayes</h2></div></div></div><p class="calibre11">Following are some of the advantages of Naïve Bayes:</p><div><ul class="itemizedlist"><li class="listitem">It is relatively simple to build and understand</li><li class="listitem">It can be trained easily and doesn't require a huge dataset</li><li class="listitem">It is comparatively fast</li><li class="listitem">Is not affected by irrelevant features</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec91" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Disadvantages of Naïve Bayes</h2></div></div></div><p class="calibre11">The disadvantage of Naïve Bayes is the "naïve" assumption that every feature is independent. This is not always true.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec92" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Uses of Naïve Bayes classification</h2></div></div></div><p class="calibre11">Here are a few uses of Naïve Bayes classification:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Naïve Bayes text classification</strong>: This is used as a probabilistic learning method and is actually one of the most successful algorithms to classify documents.</li><li class="listitem"><strong class="calibre19">Spam filtering</strong>: This is the best known use case of Naïve Bayes. Naïve Bayes is used to identify spam e-mail from legitimate e-mail. Many server-side e-mail filtering mechanisms use this with other algorithms.</li><li class="listitem"><strong class="calibre19">Recommender systems</strong>: Naïve Bayes can also be used to build recommender systems. Recommender systems are used to predict and suggest products the user may like in the future. It is based on unseen data and is used with collaborative filtering to do so. This method is more scalable and generally performs better than other algorithms.</li></ul></div><p class="calibre11">To understand how Naïve Bayes classifiers actually work, we should understand the Bayesian rule. It was formed by Thomas Bayes in the 18<sup class="calibre187">th</sup> century. He developed various mathematical principles, which are known to us as Bayesian methods. These very effectively describe probabilities of events and how the probabilities should be revised when we have additional information.</p><p class="calibre11">Classifiers, based on Bayesian methods, use the training dataset to find out the observed probability of every class based on the values of all the features. So, when this classifier is used on unlabeled or unseen data, it makes use of the observed probabilities to predict to which class the new features belong. Although it is a very simple algorithm, its performance is comparable or better than most other algorithms.</p><p class="calibre11">Bayesian classifiers are best used for these cases:</p><div><ul class="itemizedlist"><li class="listitem">A dataset containing numerous attributes, where all of them should be considered simultaneously to calculate the probability of an outcome.</li><li class="listitem">Features with weak effects are generally ignored, but Bayesian classifiers use them too to generate predictions. Many such weak features can lead to a big change in the decision.</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec93" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>How Bayesian methods work</h2></div></div></div><p class="calibre11">Bayesian methods are dependent on the concept that the estimation of likelihood of an event is based on the evidence at hand. The possible outcome of the situation is the event; for example, in a coin toss, we get heads or tails. Similarly, a mail can be "ham" or "spam". The trial refers to a single opportunity in which an event occurs. In our previous example, the coin toss is the trial.</p><div><div><div><div><h3 class="title5"><a id="ch06lvl3sec37" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Posterior probabilities</h3></div></div></div><div><blockquote class="blockquote"><p class="calibre22">
<em class="calibre23">posterior probability = conditional probability * prior probability/evidence</em>
</p></blockquote></div><p class="calibre11">In terms of classification, posterior probability refers to the probability that a particular object belongs to a class x when the observed feature values are given. For example, "what is the probability that it will rain given the temperature and percentage of humidity?"</p><div><blockquote class="blockquote"><p class="calibre22">
<em class="calibre23">P(rain | xi), xi = [45degrees, 95%humidity]</em>
</p></blockquote></div><div><ul class="itemizedlist"><li class="listitem">Let <em class="calibre23">xi</em> be the feature vector of the sample <em class="calibre23">i</em>, where <em class="calibre23">i</em>" belongs to<em class="calibre23"> {1,2,3,.....n</em>}</li><li class="listitem">Let <em class="calibre23">wj</em> be the notation of the class <em class="calibre23">j</em>, where <em class="calibre23">j</em> belongs to <em class="calibre23">{1,2,3,......n}</em></li><li class="listitem"><em class="calibre23">P(xi | wi)</em> is the probability of the observing sample <em class="calibre23">xi</em> when it is given that it belongs to class <em class="calibre23">wj</em></li></ul></div><p class="calibre11">The general notation of posterior probabilities is:</p><div><blockquote class="blockquote"><p class="calibre22">
<em class="calibre23">P(</em>wj<em class="calibre23"> | xi) = P(xi | wj) * P(wj)/P(xi)</em>
</p></blockquote></div><p class="calibre11">The main objective of Naïve Bayes is to maximize the probability of the posterior probability on the given training data so that a decision rule can be formed.</p></div><div><div><div><div><h3 class="title5"><a id="ch06lvl3sec38" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Class-conditional probabilities</h3></div></div></div><p class="calibre11">Bayesian classifiers assume that all the samples in the dataset are independent and identically distributed. Here, independence means that the probability of one observation is not affected by the probability of the other observation.</p><p class="calibre11">One very famous example that we discussed is the coin toss. Here the outcome of the first coin toss doesn't affect the subsequent coin tosses. The probability of getting the head or tail always remains 0.5 for an unbiased coin.</p><p class="calibre11">An added assumption is that the features have conditional independence. This is another "naïve" assumption, which means that the estimation of the likelihood or the class-conditional probabilities can be done directly from the training data without needing to evaluate all the probabilities of x.</p><p class="calibre11">Let's understand with an example. Suppose we have to create a server-side e-mail filtering application to decide if the mails are spam or not. Let's say we have around 1,000 e-mails and 100 e-mails are spam.</p><p class="calibre11">Now, we received a new mail with the text "Hello Friend". So, how should we calculate the class-conditional probability of the new message?</p><p class="calibre11">The pattern of the text consists of two features: "hello" and "friend". Now, we will calculate the class-conditional probability of the new mail.</p><p class="calibre11">Class-conditional probability is the probability of encountering "hello" when the mail is spam * the probability of encountering "friend" when the mail is spam:</p><div><blockquote class="blockquote"><p class="calibre22">
<em class="calibre23">P(X=[hello,world] | w=spam) = P(hello | spam) * P(friend | spam)</em>
</p></blockquote></div><p class="calibre11">We can easily find out how many mails contained the word "hello" and how many mails contained the word "spam". However, we took the "naïve" assumption that one word doesn't influence the occurrence of the other. We know that "hello" and "friend" often occur together. Therefore, our assumption is violated.</p></div><div><div><div><div><h3 class="title5"><a id="ch06lvl3sec39" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Prior probabilities</h3></div></div></div><p class="calibre11">Prior probability is the prior knowledge of the occurrence of an event. It is the general probability of the occurrence of the particular class. If the priors follow a uniform distribution, the posterior probabilities are determined using the class-conditional probabilities and also using the evidence term.</p><p class="calibre11">Prior knowledge is obtained using the estimation on the training data, when the training data is the sample of the entire population.</p></div><div><div><div><div><h3 class="title5"><a id="ch06lvl3sec40" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Evidence</h3></div></div></div><p class="calibre11">There is one more required value to calculate posterior probability, and that is "evidence". The evidence P(x) is the probability of occurrence of the particular pattern x, which is independent of the class label.</p></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec94" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The bag of words</h2></div></div></div><p class="calibre11">In the previous example, we were doing classification of e-mails. For that, we classify a pattern. To classify a pattern, the most important tasks are:</p><div><ul class="itemizedlist"><li class="listitem">Feature extraction</li><li class="listitem">Feature selection</li></ul></div><p class="calibre11">But how are good features recognized? There are some characteristics of good features:</p><div><ul class="itemizedlist"><li class="listitem">The features must be important to the use case that we are building the classifier for</li><li class="listitem">The selected features should have enough information to distinguish well between the different patterns and can be used to train the classifier</li><li class="listitem">The features should not be susceptible to distortion or scaling</li></ul></div><p class="calibre11">We need to first represent the e-mail text document as a feature vector before we can fit it to our model and apply machine learning algorithms. The classification of the text document uses the bag-of-words model. In this model, we create the vocabulary, which is a collection of different words that occur in all the e-mails (training set) and then count how many times each word occurred.</p><div><div><div><div><h3 class="title5"><a id="ch06lvl3sec41" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Advantages of using Naïve Bayes as a spam filter</h3></div></div></div><p class="calibre11">Here are the advantages of using Naïve Bayes as a spam filter:</p><div><ul class="itemizedlist"><li class="listitem">It can be personalized. It means that it can be trained on a per user basis. We sometime subscribe to newsletters or mailing lists or update about products, which may be spam to other users. Also, the e-mails that I receive have some words related to my work, which may be categorized as spam for other users. So, being a legitimate user, I would not like my mails going into spam. We may try to use the rules or filters, but Bayesian spam filtering is far more superior than these mechanisms.</li><li class="listitem">Bayesian spam filters are effective in avoiding false positives, by which it is very less probable that legitimate e-mail will be classified as spam. For example, we all get mails with the word "Nigeria" or claiming to be from Nigeria, which are actually phishing scams. But there is the possibility that I have a relative or a friend there, or I have some business there; therefore that mail may not be illegitimate to me.</li></ul></div></div><div><div><div><div><h3 class="title5"><a id="ch06lvl3sec42" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Disadvantages of Naïve Bayes filters</h3></div></div></div><p class="calibre11">Bayesian filters are vulnerable to Bayesian poisoning, which is a technique in which a large amount of legitimate text is sent with the spam mail. Therefore, the Bayesian filter fails there and marks it as "ham" or legitimate mail.</p></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec95" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Examples of Naïve Bayes</h2></div></div></div><p class="calibre11">Let us create some Naïve Bayes models using Julia:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Pkg.update 
julia&gt; Pkg.add("NaiveBayes") 
</strong>
</pre><p class="calibre11">We added the required <code class="literal">NaiveBayes</code> package.</p><p class="calibre11">Now, let's create some dummy datasets:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; X = [1 1 0 2 1; 
     0 0 3 1 0; 
     1 0 1 0 2] 
julia&gt; y = [:a, :b, :b, :a, :a] 
</strong>
</pre><p class="calibre11">We created two arrays of <code class="literal">X</code> and <code class="literal">y</code>, where an element in <code class="literal">y</code> represents the column in <code class="literal">X</code>:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; m = MultinomialNB(unique(y), 3) 
julia&gt; fit(m, X, y) 
</strong>
</pre><p class="calibre11">We loaded an instance of MultinomialNB and fit our dataset to it:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Xtest = [0 4 1; 
      2 2 0; 
      1 1 1] 
</strong>
</pre><p class="calibre11">Now we will use it to predict it on our test dataset:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; predict(m, Xtest) 
</strong>
</pre><p class="calibre11">The output that I got was:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; 3-element Array{Symbol,1}: 
   :b 
   :a 
   :a 
</strong>
</pre><p class="calibre11">Which means the first column is <code class="literal">b</code>, second is <code class="literal">a</code>, and third is also <code class="literal">a</code>.</p><p class="calibre11">This example was on a dummy dataset. Let's apply Naïve Bayes on an actual dataset. We will be using the famous iris dataset in this example:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; #import necessary libraries 
 
julia&gt; using NaiveBayes 
julia&gt; using RDatasets 
 
julia&gt; iris = dataset("datasets", "iris") 
 
julia&gt; #observations in columns and variables in rows 
 
julia&gt; x = array(iris[:, 1:4]) 
 
julia&gt; p,n = size(x) 
julia&gt; # By default species is a PooledDataArray 
 
julia&gt; y = [species for species in iris[:,5]] 
</strong>
</pre><p class="calibre11">We loaded RDatasets, which contains the iris dataset. We created arrays for the feature vectors (sepal length, sepal width, petal length, and petal width).</p><p class="calibre11">
</p><div><img src="img/image_06_010.jpg" alt="Examples of Naïve Bayes" class="calibre188"/></div><p class="calibre11">
</p><p class="calibre11">Now we will split the dataset for training and testing.</p><p class="calibre11">
</p><div><img src="img/image_06_011.jpg" alt="Examples of Naïve Bayes" class="calibre189"/></div><p class="calibre11">
</p><p class="calibre11">This is quite straightforward, fitting the dataset to a Naïve Bayes classifier. We are also calculating the accuracy to which our model worked. We can see that the accuracy was 1.0, which is 100%.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec59" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we learned about machine learning and its uses. Providing computers the ability to learn and improve has far-reaching uses in this world. It is used in predicting disease outbreaks, predicting weather, games, robots, self-driving cars, personal assistants, and much more.</p><p class="calibre11">There are three different types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.</p><p class="calibre11">In this chapter, we learned about supervised learning, especially about Naïve Bayes and decision trees. In further chapters, we will learn more about ensemble learning and unsupervised learning.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec60" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/JuliaStats/MLBase.jl">https://github.com/JuliaStats/MLBase.jl</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://julialang.org/">http://julialang.org/</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/johnmyleswhite/NaiveBayes.jl">https://github.com/johnmyleswhite/NaiveBayes.jl</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/bensadeghi/DecisionTree.jl">https://github.com/bensadeghi/DecisionTree.jl</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/bicycle1885/RandomForests.jl">https://github.com/bicycle1885/RandomForests.jl</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/">http://scikit-learn.org/stable/</a></li></ul></div></div></div>



  </body></html>