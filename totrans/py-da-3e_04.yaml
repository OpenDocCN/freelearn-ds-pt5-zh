- en: Statistics
  prefs: []
  type: TYPE_NORMAL
- en: '**Explo****ratory data analysis** (**EDA**) is the first step toward data analysis
    and building a machine learning model. Statistics provide fundamental knowledge
    and a set of tools for exploratory or descriptive data analysis. This chapter
    is designed to make you data-ready. For any kind of data professional role, you
    need to understand real-world data that is generally noisy, has missing values,
    and is collected from various sources.'
  prefs: []
  type: TYPE_NORMAL
- en: Before performing any kind of preprocessing and analysis, you need to get familiar
    with the data present, and statistics is the only tool that will help you here.
    This makes statistics a primary and very necessary skill for data professionals,
    helping them gain initial insights and an understanding of the data. For example,
    the arithmetic mean of the monthly working hours of an employee can help us to
    understand the load of an employee in an organization. Similarly, the standard
    deviation of monthly working hours can help us to infer the range of working hours.
    Correlation between two variables such as blood pressure and age of patients can
    help us understand the relationship between blood pressure and age. Sampling methods
    can be useful in any kind of primary data collection. We can also perform parametric
    and non-parametric hypothesis tests to infer facts about the population.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding attributes and their types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring central tendency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring dispersion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skewness and kurtosis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding relationships using covariance and correlation coefficients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Central limit theorem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing parametric tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing non-parametric tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, the following technical information is available:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code and the dataset at the following GitHub link: [https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter03](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter03).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code blocks are available in `ch3.ipynb`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will use the NumPy, Pandas, and SciPy Python libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding attributes and their types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is the collection of raw facts and statistics such as numbers, words, and
    observations. An attribute is a column or data field or series that represents
    the characteristics of an object and is also known as a variable, a feature, or
    a dimension. Statisticians use the term *variable*, while machine learning engineers
    prefer the term *feature*. The term *dimension* is used in data warehousing, while
    database professionals use the term *attribute*.
  prefs: []
  type: TYPE_NORMAL
- en: Types of attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data type of attributes is more crucial for data analysis because certain
    situations require certain data types. The data type of attributes helps analysts
    select the correct method for data analysis and visualization plots. The following
    list shows the various attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nominal attributes:** Nominal refers to names or labels of categorized variables.
    The value of a nominal attribute can be the symbol or name of items. The values
    are categorical, qualitative, and unordered in nature such as product name, brand
    name, zip code, state, gender, and marital status. Finding the mean and median
    values of qualitative and categorical values will not make any sense but data
    analysts can calculate the mode, which is the most commonly occurring value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ordinal attributes:** Ordinal refers to names or labels with a meaningful
    order or ranking, but the magnitude of values is not known. These types of attributes
    measure subjective qualities alone. That is why they are used in surveys for customer
    satisfaction ratings, product ratings, and movie rating reviews. Customer satisfaction
    ratings appear in the following order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '1: Very dissatisfied'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '2: Somewhat dissatisfied'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '3: Neutral'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '4: Satisfied'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '5: Very satisfied'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another example could be the size of a drink: small, medium, or large. Ordinal
    attributes can only be measured via the mode and the median. The mean cannot be
    calculated for ordinal attributes because of their qualitative nature. Ordinal
    attributes can also be recreated by discretization of a quantitative variable
    by dividing their values in a range of finite numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numeric attributes:** A numeric attribute is quantitatively presented as
    integer or real values. Numeric attributes can be of two types: interval-scaled
    or ratio-scaled.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interval-scaled attributes are measured on an ordered scale of equal-sized units.
    The meaningful difference between the two values of an interval-scaled attribute
    can be calculated, and this allows a comparison between the two values—for example,
    the birth year, and the temperature in °C. The main problem with interval-scaled
    attribute values is that they don't have a "true zero"—for example, if the temperature
    in °C is 0 then it doesn't mean that temperature doesn't exist. Interval-scaled
    data can add and subtract but can't multiply and divide because of no true zero.
    We can also calculate the mean value of an interval-scaled attribute, in addition
    to the median and mode.
  prefs: []
  type: TYPE_NORMAL
- en: Ratio-scaled attributes are measured on an ordered scale of equal-sized units,
    similar to an interval scale with an inherent zero point. Examples of ratio-scaled
    attributes are height, weight, latitude, longitude, years of experience, and the
    number of words in a document. We can perform multiplication and division, and
    calculate the difference between ratio-scaled values. We can also compute central
    tendency measures such as mean, median, and mode. The Celsius and Fahrenheit temperature
    scales are measured on an interval scale, while the Kelvin temperature scale is
    measured on a ratio scale because it has a true zero point.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete and continuous attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are various ways to classify attributes. In the previous sub-section,
    we have seen nominal, ordinal, and numeric attributes. In this sub-section, we
    will see another type of attribute classification. Here, we will talk about discrete
    or continuous attributes. A discrete variable accepts only a countable finite
    number, such as how many students are present in a class, how many cars are sold,
    and how many books are published. It can be obtained by counting numbers. A continuous
    variable accepts an infinite number of possible values, such as the weight and
    height of students. It can be obtained by measuring.
  prefs: []
  type: TYPE_NORMAL
- en: A discrete variable accepts integral values, while a continuous variable accepts
    real values. In other words, we can say a discrete variable accepts values whose
    fraction doesn't make sense, whereas a continuous variable accepts values whose
    fraction makes sense. A discrete attribute uses a limited number of values, while
    a continuous attribute uses an unlimited number of values.
  prefs: []
  type: TYPE_NORMAL
- en: After understanding the attributes and their types, it's time to focus on basic
    statistical descriptions such as central tendency measures.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring central tendency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Central tendency is the trend of values clustered around the averages such as
    the mean, mode, and median values of data. The main objective of central tendency
    is to compute the center-leading value of observations. Central tendency determines
    the descriptive summary and provides quantitative information about a group of
    observations. It has the capability to represent a whole set of observations.
    Let's see each type of central tendency measure in detail in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Mean
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The mean value is the arithmetic mean or average, which is computed by the
    sum of observations divided by the number of observations. It is sensitive to
    outliers and noise, with the result that whenever uncommon or unusual values are
    added to a group, its mean gets deviated from the typical central value. Assume
    x[1], x[²], . . . , x [N] is *N* observations. The formula for the mean of these
    values is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2e54702-8e82-4603-aa97-cfae46ae7ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s compute the mean value of the communication skill score column using
    the `pandas` library, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have created one DataFrame named `data` that
    has four columns (`name`, `gender`, `communication_skill_score`, and `quantitative_skill_score`)
    and computed the mean using the `mean(axis=0)` function. Here, `axis=0` represents
    the mean along the rows.
  prefs: []
  type: TYPE_NORMAL
- en: Mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mode is the highest-occurring item in a group of observations. The mode
    value occurs frequently in data and is mostly used for categorical values. If
    all the values in a group are unique or non-repeated, then there is no mode. It
    is also possible that more than one value has the same occurrence frequency. In
    such cases, there can be multiple modes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compute the mode value of the communication skill score column using
    the `pandas` library, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we have computed the mode of the communication
    skill score column using the `mode()` function. Let''s compute another central
    tendency measure: the median.'
  prefs: []
  type: TYPE_NORMAL
- en: Median
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The median is the midpoint or middle value in a group of observations. It is
    also called the 50^(th) percentile. The median is less affected by outliers and
    noise than the mean, and that is why it is considered a more suitable statistic
    measure for reporting. It is much near to a typical central value. Let''s compute
    the median value of the communication skill score column using the `pandas` library,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have computed the median of the communication
    skill score column using the `median()` function. Let's understand dispersion
    measures and compute them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring dispersion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen, central tendency presents the middle value of a group of observations
    but does not provide the overall picture of an observation. Dispersion metrics
    measure the deviation in observations. The most popular dispersion metrics are
    range, **interquartile range** (**IQR**), variance, and standard deviation. These
    dispersion metrics value the variability in observations or the spread of observations.
    Let''s see each dispersion measure in detail, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Range:** The range is the difference between the maximum and minimum value
    of an observation. It is easy to compute and easy to understand. Its unit is the
    same as the unit of observations. Let''s compute the range of communication skill
    scores, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have computed the range of communication skill
    scores by finding the difference between the maximum and minimum scores. The maximum
    and minimum scores were computed using the `max()` and `min()` functions.
  prefs: []
  type: TYPE_NORMAL
- en: '**IQR:** IQR is the difference between the third and first quartiles. It is
    easy to compute and easy to understand. Its unit is the same as the unit of observations.
    It measures the middle 50% in the observation. It represents the range where most
    of the observation lies. IQR is also known as midspread or middle 50%, or H-spread.
    Let''s compute the IQR of communication skill scores, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have computed the IQR of communication skill
    scores by finding the difference between the first and third quartile of scores.
    Both the first and third quartile scores were computed using the `quantile(.25)`
    and `quantile(.75)` functions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variance:** The variance measures the deviation from the mean. It is the
    average value of the squared difference between observed values and the mean.
    The main problem with the variance is its unit of measurement because of squaring
    the difference between observations and mean. Let''s assume x[1] , x[²], . . .
    , x [N] are *N* observations. The formula for the variance of these values will
    then be the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0744c65a-7790-418b-9d99-55061aac3071.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s compute the variance of communication skill scores, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have computed the variance of communication
    skill scores using the `var()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Standard deviation:** This is the square root of the variance. Its unit is
    the same as for the original observations. This makes it easier for an analyst
    to evaluate the exact deviation from the mean. The lower value of standard deviation
    represents the lesser distance of observations from the mean; this means observations
    are less widely spread. The higher value of standard deviation represents a large
    distance of observations from the mean—that is, observations are widely spread.
    Standard deviation is mathematically represented by the Greek letter sigma (Σ).
    Assume x[1], x[²], . . . , x [N] are *N* observations. The formula for the standard
    deviation of these values is the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/80967c25-ffab-4a46-8fd3-487420d8c829.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s compute the standard deviation of communication skill scores, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have computed the standard deviation of communication
    skill scores using the `std()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also try to describe the function to get all the summary statistics
    in a single command. The `describe()` function returns the count, mean, standard
    deviation, first quartile, median, third quartile, and minimum and maximum values
    for each numeric column in the DataFrame, and is illustrated in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have generated a descriptive statistics summary
    of data using the `describe()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Skewness and kurtosis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Skewness measures the symmetry of a distribution. It shows how much the distribution
    deviates from a normal distribution. Its values can be zero, positive, and negative.
    A zero value represents a perfectly normal shape of a distribution. Positive skewness
    is shown by the tails pointing toward the right—that is, outliers are skewed to
    the right and data stacked up on the left. Negative skewness is shown by the tails
    pointing toward the left—that is, outliers are skewed to the left and data stacked
    up on the right. Positive skewness occurs when the mean is greater than the median
    and the mode. Negative skewness occurs when the mean is less than the median and
    mode. Let''s compute skewness in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have computed the skewness of the communication
    skill score column using the `skew()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kurtosis measures the tailedness (thickness of tail) compared to a normal distribution.
    High kurtosis is heavy-tailed, which means more outliers are present in the observations,
    and low values of kurtosis are light-tailed, which means fewer outliers are present
    in the observations. There are three types of kurtosis shapes: mesokurtic, platykurtic,
    and leptokurtic. Let''s define them one by one, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A normal distribution having zero kurtosis is known as a mesokurtic distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A platykurtic distribution has a negative kurtosis value and is thin-tailed
    compared to a normal distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A leptokurtic distribution has a kurtosis value greater than 3 and is fat-tailed
    compared to a normal distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see the type of kurtosis shapes in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd7c3330-09d7-44a6-8517-5efa18a505da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A histogram is an effective medium to present skewness and kurtosis. Let''s
    compute the kurtosis of the communication skill score column, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have computed the kurtosis of the communication
    skill score column using the `kurtosis()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding relationships using covariance and correlation coefficients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Measuring the relationship between variables will be helpful for data analysts
    to understand the dynamics between variables—for example, an HR manager needs
    to understand the strength of the relationship between employee performance score
    and satisfaction score. Statistics offers two measures of covariance and correlation
    to understand the relationship between variables. Covariance measures the relationship
    between a pair of variables. It shows the degree of change in the variables—that
    is, how the change in one variable affects the other variable. Its value ranges
    from -infinity to + infinity. The problem with covariance is that it does not
    provide effective conclusions because it is not normalized. Let''s find the relationship
    between the communication and quantitative skill score using covariance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b8b6b19-eae9-4d77-aea7-df83e333cb34.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code block, covariance is computed using the `cov()` method.
    Here, the output of this method is the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Pearson's correlation coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Correlation shows how variables are correlated with each other. Correlation
    offers a better understanding than covariance and is a normalized version of covariance.
    Correlation ranges from -1 to 1\. A negative value represents the increase in
    one variable, causing a decrease in other variables or variables to move in the
    same direction. A positive value represents the increase in one variable, causing
    an increase in another variable, or a decrease in one variable causes decreases
    in another variable. A zero value means that there is no relationship between
    the variable or that variables are independent of each other. Have a look at the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/910acc6b-7f60-4164-8dd9-41a562e70855.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `''method''` parameter can take one of the following three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pearson`: Standard correlation coefficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kendall`: Kendall''s tau correlation coefficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spearman`: Spearman''s rank correlation coefficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spearman's rank correlation coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spearman's rank correlation coefficient is Pearson's correlation coefficient
    on the ranks of the observations. It is a non-parametric measure for rank correlation.
    It assesses the strength of the association between two ranked variables. Ranked
    variables are ordinal numbers, arranged in order. First, we rank the observations
    and then compute the correlation of ranks. It can apply to both continuous and
    discrete ordinal variables. When the distribution of data is skewed or an outlier
    is affected, then Spearman's rank correlation is used instead of Pearson's correlation
    because it doesn't have any assumptions for data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Kendall's rank correlation coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kendall's rank correlation coefficient or Kendall's tau coefficient is a non-parametric
    statistic used to measure the association between two ordinal variables. It is
    a type of rank correlation. It measures the similarity or dissimilarity between
    two variables. If both the variables are binary, then Pearson's = Spearman's =
    Kendall's tau.
  prefs: []
  type: TYPE_NORMAL
- en: Till now, we have seen descriptive statistics topics such as central measures,
    dispersion measures, distribution measures, and variable relationship measures.
    It's time to jump to the inferential statistics topics such as the central limit
    theorem, sampling techniques, and parametric and non-parametric tests.
  prefs: []
  type: TYPE_NORMAL
- en: Central limit theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data analysis methods involve hypothesis testing and deciding confidence intervals.
    All statistical tests assume that the population is normally distributed. The
    central limit theorem is the core of hypothesis testing. According to this theorem,
    the sampling distribution approaches a normal distribution with an increase in
    the sample size. Also, the mean of the sample gets closer to the population means
    and the standard deviation of the sample gets reduced. This theorem is essential
    for working with inferential statistics, helping data analysts figure out how
    samples can be useful in getting insights about the population.
  prefs: []
  type: TYPE_NORMAL
- en: 'Does it provide answers to questions such as what size of sample should be
    taken or which sample size is an accurate representation of the population? You
    can understand this with the help of the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df7154da-c951-47c8-8478-7dfb8bafe6ce.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, you can see four histograms for different-different
    sample sizes 50, 100, 200, and 500\. If you observe here, as the sample size increases,
    the histogram approaches a normal curve. Let's learn sampling techniques in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting samples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A sample is a small set of the population used for data analysis purposes.
    Sampling is a method or process of collecting sample data from various sources.
    It is the most crucial part of data collection. The success of an experiment depends
    upon how well the data is collected. If anything goes wrong with sampling, it
    will hugely affect the final interpretations. Also, it is impossible to collect
    data for the whole population. Sampling helps researchers to infer the population
    from the sample and reduces the survey cost and workload to collect and manage
    data. There are lots of sampling techniques available, for various purposes. These
    techniques can be categorized into two categories: probability sampling and non-probability
    sampling, described in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability sampling:** With this technique, there is a random selection
    of every respondent of the population, with an equal chance of the selected sample.
    Such types of sampling techniques are more time-consuming and expensive, and include
    the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple random sampling:** With this technique, each respondent is selected
    by chance, meaning that each respondent has an equal chance of being selected.
    It is a simple and straightforward method—for example, 20 products being randomly
    selected from 500 products for quality testing.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stratified sampling:** With this technique, the whole population is divided
    into small groups known as strata that are based on some similarity criteria.
    These strata can be of unequal size. This technique improves accuracy by reducing
    selection bias.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Systematic sampling:** With this technique, respondents are selected at regular
    intervals. In other words, we can say respondents are selected in systematic order
    from the target population, such as every *n*th respondent from the population.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster sampling:** With this sampling technique, the entire population is
    divided into clusters or sections. Clusters are formed based on gender, location,
    occupation, and so on. These entire clusters are used for sampling rather than
    the individual respondent.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-probability sampling:** This sampling non-randomly selects every respondent
    of the population, with an unequal chance of the selected sample. Its outcome
    might be biased. Such types of sampling techniques are cheaper and more convenient,
    and include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convenience sampling:** This is the easiest technique for data collection.
    It selects respondents based on their availability and willingness to participate.
    Statisticians prefer this technique for the initial survey due to cost and fast
    collection of data, but the results are more prone to bias.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purposive sampling:** This is also known as judgmental sampling because it
    depends upon the statistician''s judgment. Statisticians decide at runtime who
    will participate in the survey based on certain predefined characteristics. News
    reporters use this technique to select people whose opinions they wish to obtain.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quota sampling:** This technique predefines the properties of strata and
    proportions for the sample. Sample respondents are selected until a definitive
    proportion is met. It differs from stratified sampling in terms of selection strategy;
    it selects items in strata using random sampling.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snowball sampling:** This technique is used in a situation where finding
    respondents in a population is rare and difficult to trace, in areas such as illegal
    immigration or HIV. Statisticians contact volunteers to reach out to the victims.
    It is also known as referral sampling because the initial person taking part in
    the survey refers to another person who fits the sample description.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we have seen sampling methods and their types: probability
    sampling and non-probability sampling. Now, it''s time to jump to hypothesis testing
    techniques. In upcoming sections, we will focus on parametric and non-parametric
    hypothesis testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Performing parametric tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The hypothesis is the main core topic of inferential statistics. In this section,
    we will focus on parametric tests. The basic assumption of a parametric test is
    the underlying statistical distribution. Most elementary statistical methods are
    parametric in nature. Parametric tests are used for quantitative and continuous
    data. Parameters are numeric quantities that represent the whole population. Parametric
    tests are more powerful and reliable than non-parametric tests. The hypothesis
    is developed on the parameters of the population distribution. Here are some examples
    of parametric tests:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A t-test is a kind of parametric test that is used for checking if there is
    a significant difference between the means of the two groups concerned. It is
    the most commonly used inferential statistic that follows the normal distribution.
    A t-test has two types: a one-sample t-test and a two-sample t-test. A one-sample
    t-test is used for checking if there is a significant difference between a sample
    and hypothesized population means. Let''s take 10 students and check whether their
    average weight is 68 kg or not by using a t-test, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have created an array of 10 students' weight
    and computed its arithmetic mean using `numpy.mean()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform a one-sample t-test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have tested the null hypothesis (average weight
    of 10 students is 68 kg) by using `ttest_1samp()`. The output results have shown
    that the null hypothesis is accepted with a 95% confidence interval, which means
    that the average weight of 10 students is 68 kg.
  prefs: []
  type: TYPE_NORMAL
- en: 'A two-sample t-test is used for comparing the significant difference between
    two independent groups. This test is also known as an independent samples t-test.
    Let''s compare the average weight of two independent student groups, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Null Hypothesis H[0]:** Sample means are equal—μ [1] = μ [2]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Alternative Hypothesis H[a]:** Sample means are not equal—μ [1] > μ [2] or
    μ [2] > μ [1]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have created two arrays of 10 students' weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform a two-sample t-test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have tested the hypothesis average weight of
    two groups using the `ttest_ind()` method, and results show that the null hypothesis
    is rejected with a 95% confidence interval, which means that the sample means
    are different.
  prefs: []
  type: TYPE_NORMAL
- en: 'A paired sample t-test is a dependent sample t-test, which is used to decide
    whether the mean difference between two observations of the same group is zero—for
    example, to compare the difference in blood pressure level for a group of patients
    before and after some drug treatment. This is equivalent to a one-sample t-test
    and is also known as a dependent sample t-test. Let''s perform a paired t-test
    to assess the impact of weight loss treatment. We have collected the weight of
    patients before and after treatment. This can be represented using the following
    hypothesis:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Null Hypothesis H[0]*:** Mean difference between the two dependent samples
    is 0.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Alternative Hypothesis H[a]*:** Mean difference between the two dependent
    samples is not 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we have created two arrays of 10 patients'' weights
    before and after treatment. Let''s perform a paired sample t-test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have tested the hypothesis of the average weight
    of two groups before and after treatment using the `ttest_rel()` method. Results
    show that the null hypothesis is rejected with a 95% confidence interval, which
    means that weight loss treatment has a significant impact on the patient's weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'ANOVA: A t-test only deals with two groups, but sometimes we have more than
    two groups or multiple groups at the same time to compare. **ANOVA** (**ANalysis
    Of** **VAriance**) is a statistical inference test used for comparing multiple
    groups. It analyzes the variance between and within multiple groups and tests
    several null hypotheses at the same time. It usually compares more than two sets
    of data and checks statistical significance. We can use ANOVA in three ways: one-way
    ANOVA, two-way ANOVA, and N-way multivariate ANOVA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the one-way ANOVA method, we compare multiple groups based on only one
    independent variable—for example, an IT company wants to compare multiple employee
    groups'' or teams'' productivity based on performance score. In our example, we
    are comparing the performance of employees in an IT company based in three locations:
    Mumbai, Chicago, and London. Here, we will perform a one-way ANOVA test and check
    for a significant difference in performance. Let''s define the null and alternative
    hypotheses, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Null Hypothesis H[0]*:** There is no difference between the mean performance
    score of multiple locations.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Alternative Hypothesis H[a]*:** There is a difference between the mean performance
    score of multiple locations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we have created three lists of employee performance
    scores for three locations: Mumbai, Chicago, and London.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform a one-way ANOVA test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have tested the hypothesis that there is no
    difference between the mean performance score of various locations using the `f_oneway()`
    method. The preceding results show that the null hypothesis is accepted with a
    95% confidence interval, which means that there is no significant difference between
    the mean performance score of all the locations.
  prefs: []
  type: TYPE_NORMAL
- en: With the two-way ANOVA method, we compare multiple groups based on two independent
    variables—for example, if an IT company wants to compare multiple employee groups'
    or teams' productivity based on working hours and project complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In N-way ANOVA, we compare multiple groups based on *N* independent variables—for
    example, if an IT company wants to compare multiple employee groups' or teams'
    productivity based on working hours, project complexity, employee training, and
    other employee perks and facilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we have explored parametric tests such as the t-test and ANOVA
    tests in detail. Let's jump to the non-parametric hypothesis test.
  prefs: []
  type: TYPE_NORMAL
- en: Performing non-parametric tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A non-parametric test doesn''t rely on any statistical distribution; that is
    why it is known as a "distribution-free" hypothesis test. Non-parametric tests
    don''t have parameters of the population. Such types of tests are used for order
    and rank of observations and require special ranking and counting methods. Here
    are some examples of non-parametric tests:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A **Chi-Square test** is determined by a significant difference or relationship
    between two categorical variables from a single population. In general, this test
    assesses whether distributions of categorical variables differ from each other.
    It is also known as a Chi-Square goodness of fit test or a Chi-Square test for
    independence. A small value of the Chi-Square statistic means observed data fit
    with expected data, and a larger value of the Chi-Square statistic means observed
    data doesn''t fit with expected data. For example, the impact of gender on voting
    preference or the impact of company size on health insurance coverage can be assessed
    by a Chi-Square test:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/104b9b36-4efc-4d4f-9c58-1c99f988bb85.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *O* is the observed value, *E* is the expected value, and "*i*" is the
    "i^(th)" position in the contingency table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the Chi-Square test using an example. Suppose we have done
    a survey in a company of 200 employees and asked about their highest qualification
    such as High School, Higher Secondary, Graduate, Post-Graduate, and compare it
    with performance levels such as Average and Outstanding. Here is the hypothesis
    and contingency criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Null Hypothesis H[0]*:** The two categorical variables are independent—that
    is, employee performance is independent of the highest qualification level.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Alternative Hypothesis H[a]*:** The two categorical variables are not independent—that
    is, employee performance is not independent of the highest qualification level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The contingency table can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | High School | Higher Secondary | Graduate | Post-Graduate |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 20 | 16 | 13 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| Outstanding | 31 | 40 | 50 | 13 |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s perform a Chi-Square test and check for a significant difference in
    the association between variables, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have created two lists of average and outstanding
    performing employees and created a contingency table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform a Chi-Square test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have tested the hypothesis that employee performance
    is independent of the highest qualification level. The preceding results show
    that the null hypothesis is accepted with a 95% confidence interval, which means
    that employee performance is independent of the highest qualification level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Mann-Whitney U test** is the non-parametric counterpart of the t-test
    for two samples. It doesn''t assume that the difference between the samples is
    normally distributed. The Mann-Whitney U test is used when the observation is
    ordinal and assumptions of the t-test were not met—for example, comparing two
    groups of movie test preferences from their given movie ratings. Let''s compare
    two groups of movie ratings using the following criteria:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Null Hypothesis H[0]*:** There is no difference between the two sample distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Alternative Hypothesis H[a]*:** There is a difference between the two sample
    distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have created two lists of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform a Mann-Whitney U test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have tested the hypothesis that there is no
    difference between the distribution of two movie rating groups using the `mannwhitneyu()`
    method. The results show that the null hypothesis is rejected with a 99% confidence
    interval, which means that there is a significant difference between the two movie
    rating groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Wilcoxon signed-rank test** compares two paired samples. It is a non-parametric
    counterpart version of the paired t-test. It tests the null hypothesis as to whether
    the two paired samples belong to the same distribution or not—for example, to
    compare the difference between two treatment observations for multiple groups.
    Let''s compare the difference between two treatment observations using the following
    criteria:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Null Hypothesis H[0]*:** There is no difference between the dependent sample
    distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Alternative Hypothesis H[a]*:** There is a difference between the dependent
    sample distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have created two lists of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform a Wilcoxon signed-rank test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have tested the hypothesis that there is no
    difference between the distribution of groups before and after treatment using
    the `wilcoxon()` method. The preceding results show that the null hypothesis is
    accepted with a 99% confidence interval, which means that there is no significant
    difference between the groups before and after treatment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Kruskal-Wallis** test is the non-parametric version of one-way ANOVA,
    to assess whether samples belong to the same distribution or not. It compares
    two or more independent samples. It extends the limit of the Mann-Whitney U test,
    which compares only two groups. Let''s compare three sample groups using the following
    code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we have created three lists of data. Let''s perform
    a Kruskal-Wallis test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we have tested the hypothesis that there is no
    difference between the three sample groups using the `kruskal()` method. The preceding
    results show that the null hypothesis is accepted with a 99% confidence interval,
    which means that there is no difference between the three sample groups. Let''s
    compare both parametric and non-parametric tests, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Features** | **Parametric Tests** | **Non-Parametric Tests** |'
  prefs: []
  type: TYPE_TB
- en: '| **Test Statistic** | Distribution | Arbitrary or "Distribution-Free" |'
  prefs: []
  type: TYPE_TB
- en: '| **Attribute Type** | Numeric | Nominal and Ordinal |'
  prefs: []
  type: TYPE_TB
- en: '| **Central Tendency Measures** | Mean | Median |'
  prefs: []
  type: TYPE_TB
- en: '| **Correlation Tests** | Pearson''s Correlation | Spearman''s Correlation
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Information about Population** | Complete Information | No Information
    |'
  prefs: []
  type: TYPE_TB
- en: In the preceding table, you have seen examples of parametric and non-parametric
    tests based on various features such as test statistic, attribute type, central
    tendency measures, correlation tests, and population information. Finally, you
    made it to the end. In this chapter, we have explored the fundamentals of descriptive
    as well as inferential statistics with Python.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core fundamentals of statistics will provide the foundation for data analysis,
    facilitating how data is described and understood. In this chapter, you have learned
    the basics of statistics such as attributes and their different types such as
    nominal, ordinal, and numeric. You have also learned about mean, median, and mode
    for measuring central tendency. Range, IQR, variance, and standard deviation measures
    are used to estimate variability in the data; skewness and kurtosis are used for
    understanding data distribution; covariance and correlation are used to understand
    the relationship between variables. You have also seen inferential statistics
    topics such as the central limit theorem, collecting samples, and parametric and
    non-parametric tests. You have also performed hands-on coding on statistics concepts
    using the `pandas` and `scipy.stats` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter, [Chapter 4](0f9d094c-3d35-438c-bff9-368f2d6a9dd6.xhtml), *Linear
    Algebra*, will help us to learn how to solve the linear system of equations, find
    Eigenvalues and Eigenvectors, and learn about binomial and normal distribution,
    normality tests, and masked arrays using the Python packages NumPy and SciPy.
  prefs: []
  type: TYPE_NORMAL
