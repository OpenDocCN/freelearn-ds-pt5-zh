<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer315" class="Content">
			<h1 id="_idParaDest-205"><em class="italics"><a id="_idTextAnchor225"/>Appendix</em></h1>
		</div>
		<div>
			<div id="_idContainer316" class="Content">
			</div>
		</div>
		<div id="_idContainer317" class="Content">
			<h2>About</h2>
			<p>This section is included to assist you in performing the activities present in the book. It includes detailed steps that are to be performed by the students to complete and achieve the objectives of the book.</p>
		</div>
		<div id="_idContainer379" class="Content">
			<h2 id="_idParaDest-206"><a id="_idTextAnchor226"/>Chapter 1: Introduction to Data Science and Data Preprocessing</h2>
			<h3 id="_idParaDest-207"><a id="_idTextAnchor227"/>Activity 1: Pre-Processing Using the Bank Marketing Subscription Dataset</h3>
			<p>Solution</p>
			<p>Let's perform various pre-processing tasks on the<strong class="bold"> </strong><strong class="inline">Bank Marketing Subscription </strong>dataset. We'll also be splitting the dataset into training and testing data. Follow these steps to complete this activity:</p>
			<ol>
				<li>Open a Jupyter notebook and add a new cell to import the pandas library and load the dataset into a pandas dataframe. To do so, you first need to import the library, and then use the <strong class="inline">pd.read_csv()</strong> function, as shown here:<p class="snippet">import pandas as pd</p><p class="snippet">Link = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv'</p><p class="snippet">#reading the data into the dataframe into the object data</p><p class="snippet">df = pd.read_csv(Link, header=0)</p></li>
				<li>To find the number of rows and columns in the dataset, add the following code:<p class="snippet">#Finding number of rows and columns</p><p class="snippet">print("Number of rows and columns : ",df.shape)</p><p>The preceding code generates the following output:</p><div id="_idContainer318" class="IMG---Figure"><img src="Images/C13322_01_61.jpg" alt="Figure 1.60: Number of rows and columns in the dataset" width="1144" height="51"/></div><h6>Figure 1.60: Number of rows and columns in the dataset</h6></li>
				<li>To print the list of all columns, add the following code:<p class="snippet">#Printing all the columns</p><p class="snippet">print(list(df.columns))</p><p>The preceding code generates the following output:</p><div id="_idContainer319" class="IMG---Figure"><img src="Images/C13322_01_62.jpg" alt="Figure 1.61: List of columns present in the dataset" width="1259" height="99"/></div><h6>Figure 1.61: List of columns present in the dataset</h6></li>
				<li>To overview the basic statistics of each column, such as the count, mean, median, standard deviation, minimum value, maximum value, and so on, add the following code:<p class="snippet">#Basic Statistics of each column</p><p class="snippet">df.describe().transpose()</p><p>The preceding code generates the following output:</p><div id="_idContainer320" class="IMG---Figure"><img src="Images/C13322_01_63.jpg" alt="Figure 1.62: Basic statistics of each column" width="1576" height="790"/></div><h6>Figure 1.62: Basic statistics of each column</h6></li>
				<li>To print the basic information of each column, add the following code:<p class="snippet">#Basic Information of each column</p><p class="snippet">print(df.info())</p><p>The preceding code generates the following output:</p><div id="_idContainer321" class="IMG---Figure"><img src="Images/C13322_01_64.jpg" alt="Figure 1.63: Basic information of each column&#13;&#10;" width="1574" height="1098"/></div><h6>Figure 1.63: Basic information of each column</h6><p>In the preceding figure, you can see that none of the columns contains any null values. Also, the type of each column is provided.</p></li>
				<li>Now let's check for missing values and the type of each feature. Add the following code to do this:<p class="snippet">#finding the data types of each column and checking for null</p><p class="snippet">null_ = df.isna().any()</p><p class="snippet">dtypes = df.dtypes</p><p class="snippet">sum_na_ = df.isna().sum()</p><p class="snippet">info = pd.concat([null_,sum_na_,dtypes],axis = 1,keys = ['isNullExist','NullSum','type'])</p><p class="snippet">info</p><p>Have a look at the output for this in the following figure:</p><div id="_idContainer322" class="IMG---Figure"><img src="Images/C13322_01_65.jpg" alt="Figure 1.64: Information of each column stating the number of null values and the data types&#13;&#10;" width="1405" height="1230"/></div><h6>Figure 1.64: Information of each column stating the number of null values and the data types</h6></li>
				<li>Since we have loaded the dataset into the <strong class="inline">data</strong> object, we will remove the null values from the dataset. To remove the null values from the dataset, add the following code: <p class="snippet">#removing Null values</p><p class="snippet">df = df.dropna()</p><p class="snippet">#Total number of null in each column</p><p class="snippet">print(df.isna().sum())# No NA</p><p>Have a look at the output for this in the following figure:</p><div id="_idContainer323" class="IMG---Figure"><img src="Images/C13322_01_66.jpg" alt="Figure 1.65: Features of dataset with no null values&#13;&#10;" width="1095" height="641"/></div><h6>Figure 1.65: Features of dataset with no null values</h6></li>
				<li>Now we check the frequency distribution of the <strong class="inline">education</strong> column in the dataset. Use the <strong class="inline">value_counts()</strong> function to implement this:<p class="snippet">df.education.value_counts()</p><p>Have a look at the output for this in the following figure:</p><div id="_idContainer324" class="IMG---Figure"><img src="Images/C13322_01_67.jpg" alt="Figure 1.66: Frequency distribution of the education column&#13;&#10;" width="1314" height="330"/></div><h6>Figure 1.66: Frequency distribution of the education column</h6></li>
				<li>In the preceding figure, we can see that the <strong class="inline">education</strong> column of the dataset has many categories. We need to reduce the categories for better modeling. To check the various categories in the <strong class="inline">education</strong> column, we use the <strong class="inline">unique()</strong> function. Type the following code to implement this:<p class="snippet">df.education.unique()  </p><p>The output is as follows:</p><div id="_idContainer325" class="IMG---Figure"><img src="Images/C13322_01_68.jpg" alt="Figure 1.67: Various categories of the education column&#13;&#10;" width="1280" height="134"/></div><h6>Figure 1.67: Various categories of the education column</h6></li>
				<li>Now let's group the <strong class="inline">basic.4y</strong>, <strong class="inline">basic.9y</strong>, and <strong class="inline">basic.6y</strong> categories together and call them <strong class="inline">basic</strong>. To do this, we can use the <strong class="inline">replace</strong> function from pandas:<p class="snippet">df.education.replace({"basic.9y":"Basic","basic.6y":"Basic","basic.4y":"Basic"},inplace=True)</p></li>
				<li>To check the list of categories after grouping, add the following code:<p class="snippet">df.education.unique()  </p><div id="_idContainer326" class="IMG---Figure"><img src="Images/C13322_01_69.jpg" alt="Figure 1.68: Various categories of the education column&#13;&#10;" width="1168" height="88"/></div><h6>Figure 1.68: Various categories of the education column</h6><p>In the preceding figure, you can see that <strong class="inline">basic.9y</strong>, <strong class="inline">basic.6y</strong>, and <strong class="inline">basic.4y</strong> are grouped together as <strong class="inline">Basic</strong>.</p></li>
				<li>Now we select and perform a suitable encoding method for the data. Add the following code to implement this: <p class="snippet">#Select all the non numeric data using select_dtypes function</p><p class="snippet">data_column_category = df.select_dtypes(exclude=[np.number]).columns</p><p>The preceding code generates the following output:</p><div id="_idContainer327" class="IMG---Figure"><img src="Images/C13322_01_70.jpg" alt="Figure 1.69: Various columns of the dataset&#13;&#10;" width="1456" height="122"/></div><h6>Figure 1.69: Various columns of the dataset</h6></li>
				<li>Now we define a list with all the names of the categorical features in the data. Also, we loop through every variable in the list, getting dummy variable encoded output. Add the following code to do this:<p class="snippet">cat_vars=data_column_category</p><p class="snippet">for var in cat_vars:</p><p class="snippet">    cat_list='var'+'_'+var</p><p class="snippet">    cat_list = pd.get_dummies(df[var], prefix=var)</p><p class="snippet">    data1=df.join(cat_list)</p><p class="snippet">    df=data1</p><p class="snippet"> df.columns</p><p>The preceding code generates the following output:</p><div id="_idContainer328" class="IMG---Figure"><img src="Images/C13322_01_71.jpg" alt="Figure 1.70: List of categorical features in the data&#13;&#10;" width="1426" height="730"/></div><h6>Figure 1.70: List of categorical features in the data</h6></li>
				<li>Now we neglect the categorical column for which we have done encoding. We'll select only the numerical and encoded categorical columns. Add the code to do this:<p class="snippet">#Categorical features</p><p class="snippet">cat_vars=data_column_category</p><p class="snippet">#All features</p><p class="snippet">data_vars=df.columns.values.tolist()</p><p class="snippet">#neglecting the categorical column for which we have done encoding</p><p class="snippet">to_keep = []</p><p class="snippet">for i in data_vars:</p><p class="snippet">    if i not in cat_vars:</p><p class="snippet">        to_keep.append(i)</p><p class="snippet">        </p><p class="snippet">#selecting only the numerical and encoded catergorical column</p><p class="snippet">data_final=df[to_keep]</p><p class="snippet">data_final.columns</p><p>The preceding code generates the following output:</p><div id="_idContainer329" class="IMG---Figure"><img src="Images/C13322_01_72.jpg" alt="Figure 1.71: List of numerical and encoded categorical columns&#13;&#10;" width="1415" height="675"/></div><h6>Figure 1.71: List of numerical and encoded categorical columns</h6></li>
				<li>Finally, we split the data into train and test sets. Add the following code to implement this:<p class="snippet">#Segregating Independent and Target variable</p><p class="snippet">X=data_final.drop(columns='y')</p><p class="snippet">y=data_final['y']</p><p class="snippet">from sklearn. model_selection import train_test_split</p><p class="snippet">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)</p><p class="snippet">print("FULL Dateset X Shape: ", X.shape )</p><p class="snippet">print("Train Dateset X Shape: ", X_train.shape )</p><p class="snippet">print("Test Dateset X Shape: ", X_test.shape )</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer330" class="IMG---Figure">
					<img src="Images/C13322_01_73.jpg" alt="Figure 1.72: Shape of the full, train, and test datasets&#13;&#10;" width="1222" height="113"/>
				</div>
			</div>
			<h6>Figure 1.72: Shape of the full, train, and test datasets</h6>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor228"/>Chapter 2: Data Visualization</h2>
			<h3 id="_idParaDest-209"><a id="_idTextAnchor229"/>Activity 2: Line Plot</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Create a list of 6 strings for each month, January through June, and save it as x using:<p class="snippet">x = ['January','February','March','April','May','June']</p></li>
				<li>Create a list of 6 values for '<strong class="inline">Items Sold</strong>' that starts at 1000 and increases by 200, so the final value is 2000 and save it as y as follows:<p class="snippet">y = [1000, 1200, 1400, 1600, 1800, 2000]</p></li>
				<li>Plot y ('<strong class="inline">Items Sold</strong>') by x ('<strong class="inline">Month</strong>') with a dotted blue line and star markers using the following:<p class="snippet">plt.plot(x, y, '*:b')</p></li>
				<li>Set the x-axis to '<strong class="inline">Month</strong>' using the following code:<p class="snippet">plt.xlabel('Month')</p></li>
				<li>Set the y-axis to '<strong class="inline">Items Sold</strong>' as follows:<p class="snippet">plt.ylabel('Items Sold')</p></li>
				<li>To set the title to read '<strong class="inline">Items Sold has been Increasing Linearly</strong>', refer to the following code:<p class="snippet">plt.title('Items Sold has been Increasing Linearly')</p><p>Check out the following screenshot for the resultant output:</p></li>
			</ol>
			<div>
				<div id="_idContainer331" class="IMG---Figure">
					<img src="Images/C13322_02_33.jpg" alt="Figure 2.33: Line plot of items sold by month&#13;&#10;" width="509" height="271"/>
				</div>
			</div>
			<h6>Figure 2.33: Line plot of items sold by month</h6>
			<h3 id="_idParaDest-210"><a id="_idTextAnchor230"/>Activity 3: Bar Plot</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Create a list of five strings for <strong class="inline">x</strong> containing the names of NBA franchises with the most titles using the following code:<p class="snippet">x = ['Boston Celtics','Los Angeles Lakers', 'Chicago Bulls', 'Golden State Warriors', 'San Antonio Spurs']</p></li>
				<li>Create a list of five values for <strong class="inline">y</strong> containing values for '<strong class="inline">Titles Won</strong>' that correspond with the strings in <strong class="inline">x</strong> using the following code:<p class="snippet">y = [17, 16, 6, 6, 5]</p></li>
				<li>Place <strong class="inline">x</strong> and <strong class="inline">y</strong> into a data frame with the column names '<strong class="inline">Team</strong>' and '<strong class="inline">Titles</strong>', respectively, as follows:<p class="snippet">import pandas as pd</p><p class="snippet"> </p><p class="snippet">df = pd.DataFrame({'Team': x,</p><p class="snippet">                   'Titles': y})</p></li>
				<li>To sort the data frame descending by 'Titles' and save it as df_sorted, refer to the following code:<p class="snippet">df_sorted = df.sort_values(by=('Titles'), ascending=False)</p><h4>Note </h4><p class="callout">If we sort with <strong class="inline">ascending=True</strong>, the plot will have larger values to the right. Since we want the larger values on the left, we will be using <strong class="inline">ascending=False</strong>.</p></li>
				<li>Make a programmatic title and save it as title by first finding the team with the most titles and saving it as the <strong class="inline">team_with_most_titles</strong> object using the following code:<p class="snippet">team_with_most_titles = df_sorted['Team'][0]</p></li>
				<li>Then, retrieve the number of titles for the team with the most titles using the following code:<p class="snippet">most_titles = df_sorted['Titles'][0]</p></li>
				<li>Lastly, create a string that reads '<strong class="inline">The Boston Celtics have the most titles with 17</strong>' using the following code:<p class="snippet">title = 'The {} have the most titles with {}'.format(team_with_most_titles, most_titles)</p></li>
				<li>Use a bar graph to plot the number of titles by team using the following code: <p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet"> </p><p class="snippet">plt.bar(df_sorted['Team'], df_sorted['Titles'], color='red')</p></li>
				<li>Set the x-axis label to '<strong class="inline">Team</strong>' using the following:<p class="snippet">plt.xlabel('Team')</p></li>
				<li>Set the y-axis label to '<strong class="inline">Number of Championships</strong>' using the following:<p class="snippet">plt.ylabel('Number of Championships')</p></li>
				<li>To prevent the x tick labels from overlapping by rotating them 45 degrees, refer to the following code:<p class="snippet">plt.xticks(rotation=45)</p></li>
				<li>Set the title of the plot to the programmatic <strong class="inline">title</strong> object we created as follows: <p class="snippet">plt.title(title)</p></li>
				<li>Save the plot to our current working directory as '<strong class="inline">Titles_by_Team.png</strong>' using the following code: <p class="snippet">plt.savefig('Titles_by_Team)</p></li>
				<li>Print the plot using <strong class="inline">plt.show()</strong>. To understand this better, check out the following output screenshot:<div id="_idContainer332" class="IMG---Figure"><img src="Images/C13322_02_34.jpg" alt="Figure 2.34: The bar plot of the number of titles held by an NBA team&#13;&#10;" width="674" height="346"/></div><h6>Figure 2.34: The bar plot of the number of titles held by an NBA team</h6><h4>Note</h4><p class="callout">When we print the plot to the console using <strong class="inline">plt.show()</strong>, it appears as intended; however, when we open the file we created titled '<strong class="inline">Titles_by_Team.png</strong>', we see that it crops the x tick labels.</p><p>The following figure displays the bar plot with the cropped x tick labels.</p><div id="_idContainer333" class="IMG---Figure"><img src="Images/C13322_02_35.jpg" alt="Figure 2.35: 'Titles_by_Team.png' with x tick labels cropped&#13;&#10;" width="537" height="302"/></div><h6>Figure 2.35: 'Titles_by_Team.png' with x tick labels cropped</h6></li>
				<li>To fix the cropping issue, add <strong class="inline">bbox_inches='tight'</strong> as an argument inside of <strong class="inline">plt.savefig()</strong> as follows:<p class="snippet">plt.savefig('Titles_by_Team', bbox_inches='tight')</p></li>
				<li>Now, when we open the saved '<strong class="inline">Titles_by_Team.png</strong>' file from our working directory, we see that the x tick labels are not cropped.<p>Check out the following output for the final result:</p></li>
			</ol>
			<div>
				<div id="_idContainer334" class="IMG---Figure">
					<img src="Images/C13322_02_36.jpg" alt="Figure 2.36: 'Titles_by_Team.png' without cropped x tick labels&#13;&#10;" width="610" height="349"/>
				</div>
			</div>
			<h6>Figure 2.36: 'Titles_by_Team.png' without cropped x tick labels</h6>
			<h3 id="_idParaDest-211"><a id="_idTextAnchor231"/>Activity 4: Multiple Plot Types Using Subplots</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Import the '<strong class="inline">Items_Sold_by_Week.csv</strong>' file and save it as the <strong class="inline">Items_by_Week</strong> data frame object using the following code:<p class="snippet">import pandas as pd</p><p class="snippet"> </p><p class="snippet">Items_by_Week = pd.read_csv('Items_Sold_by_Week.csv')</p></li>
				<li>Import the '<strong class="inline">Weight_by_Height.csv</strong>' file and save it as the <strong class="inline">Weight_by_Height</strong> data frame object as follows:<p class="snippet">Weight_by_Height = pd.read_csv('Weight_by_Height.csv')</p></li>
				<li>Generate an array of 100 normally distributed numbers to use as data for the histogram and box-and-whisker plots and save it as y using the following code:<p class="snippet">y = np.random.normal(loc=0, scale=0.1, size=100)</p></li>
				<li>To generate a figure with six subplots organized in three rows and two columns that do not overlap refer to the following code:<p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet"> </p><p class="snippet">fig, axes = plt.subplots(nrows=3, ncols=2)</p><p class="snippet">plt.tight_layout()</p></li>
				<li>Set the respective axes' titles to match those in Figure 2.32 using the following code:<p class="snippet">axes[0,0].set_title('Line') </p><p class="snippet">axes[0,1].set_title('Bar') </p><p class="snippet">axes[1,0].set_title('Horizontal Bar') </p><p class="snippet">axes[1,1].set_title('Histogram') </p><p class="snippet">axes[2,0].set_title('Scatter') </p><p class="snippet">axes[2,1].set_title('Box-and-Whisker') </p><div id="_idContainer335" class="IMG---Figure"><img src="Images/C13322_02_37.jpg" alt="Figure 2.37: Titled, non-overlapping empty subplots&#13;&#10;" width="572" height="276"/></div><h6>Figure 2.37: Titled, non-overlapping empty subplots</h6></li>
				<li>On the '<strong class="inline">Line</strong>', '<strong class="inline">Bar</strong>', and '<strong class="inline">Horizontal Bar</strong>' axes, plot '<strong class="inline">Items_Sold</strong>' by '<strong class="inline">Week</strong>' from '<strong class="inline">Items_by_Week</strong>' using:<p class="snippet">axes[0,0].plot(Items_by_Week['Week'], Items_by_Week['Items_Sold'])</p><p class="snippet">axes[0,1].bar(Items_by_Week['Week'], Items_by_Week['Items_Sold'])</p><p class="snippet">axes[1,0].barh(Items_by_Week['Week'], Items_by_Week['Items_Sold'])</p><p>See the resultant output in the following figure:</p><div id="_idContainer336" class="IMG---Figure"><img src="Images/C13322_02_38.jpg" alt="Figure 2.38: Line, bar, and horizontal bar plots added&#13; &#10;" width="608" height="277"/></div><h6>Figure 2.38: Line, bar, and horizontal bar plots added</h6></li>
				<li>On the '<strong class="inline">Histogram</strong>' and '<strong class="inline">Box-and-Whisker</strong>' axes, plot the array of 100 normally distributed numbers using the following code:<p class="snippet">axes[1,1].hist(y, bins=20)axes[2,1].boxplot(y)</p><p>The resultant output is displayed here:</p><div id="_idContainer337" class="IMG---Figure"><img src="Images/C13322_02_39.jpg" alt="Figure 2.39: The histogram and box-and-whisker added&#13;&#10;" width="578" height="273"/></div><h6>Figure 2.39: The histogram and box-and-whisker added</h6></li>
				<li>Plot '<strong class="inline">Weight</strong>' by '<strong class="inline">Height</strong>' on the '<strong class="inline">Scatterplot</strong>' axes from the '<strong class="inline">Weight_by_Height</strong>' data frame using the following code:<p class="snippet">axes[2,0].scatter(Weight_by_Height['Height'], Weight_by_Height['Weight'])</p><p>See the figure here for the resultant output:</p><div id="_idContainer338" class="IMG---Figure"><img src="Images/C13322_02_40.jpg" alt="Figure 2.40: Scatterplot added&#13;&#10;" width="623" height="284"/></div><h6>Figure 2.40: Scatterplot added</h6></li>
				<li>Label the x- and y-axis for each subplot using <strong class="inline">axes[row, column].set_xlabel('X-Axis Label')</strong> and <strong class="inline">axes[row, column].set_ylabel('Y-Axis Label')</strong>, respectively.<p>See the figure here for the resultant output:</p><div id="_idContainer339" class="IMG---Figure"><img src="Images/C13322_02_41.jpg" alt="Figure 2.41: X and y axes have been labeled&#13;&#10;" width="641" height="279"/></div><h6>Figure 2.41: X and y axes have been labeled</h6></li>
				<li>Increase the size of the figure with the <strong class="inline">figsize</strong> argument in the subplots function as follows:<p class="snippet">fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8,8))</p></li>
				<li>Save the figure to the current working directory as '<strong class="inline">Six_Subplots</strong>' using the following code:<p class="snippet">fig.savefig('Six_Subplots')</p><p>The following figure displays the '<strong class="inline">Six_Subplots.png</strong>' file:</p></li>
			</ol>
			<div>
				<div id="_idContainer340" class="IMG---Figure">
					<img src="Images/C13322_02_42.jpg" alt="Figure 2.42: The Six_Subplots.png file&#13;&#10;" width="591" height="571"/>
				</div>
			</div>
			<h6>Figure 2.42: The Six_Subplots.png file</h6>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor232"/>Chapter 3: Introduction to Machine Learning via Scikit-Learn</h2>
			<h3 id="_idParaDest-213"><a id="_idTextAnchor233"/>Activity 5: Generating Predictions and Evaluating the Performance of a Multiple Linear Regression Model</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Generate predictions on the test data using the following:<p class="snippet">predictions = model.predict(X_test)</p><p class="snippet">2.    Plot the predicted versus actual values on a scatterplot using the following code:</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from scipy.stats import pearsonr</p><p class="snippet"> </p><p class="snippet">plt.scatter(y_test, predictions)</p><p class="snippet">plt.xlabel('Y Test (True Values)')</p><p class="snippet">plt.ylabel('Predicted Values')</p><p class="snippet">plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, predictions)[0], 2))</p><p class="snippet">plt.show()</p><p>Refer to the resultant output here:</p><div id="_idContainer341" class="IMG---Figure"><img src="Images/C13322_03_33.jpg" alt="Figure 3.33: A scatterplot of predicted versus actual values from a multiple linear regression model&#13;&#10;" width="576" height="272"/></div><h6>Figure 3.33: A scatterplot of predicted versus actual values from a multiple linear regression model</h6><h4>Note</h4><p class="callout">There is a much stronger linear correlation between the predicted and actual values in the multiple linear regression model (r = 0.93) relative to the simple linear regression model (r = 0.62).</p></li>
				<li>To plot the distribution of the residuals, refer to the code here:<p class="snippet">import seaborn as sns</p><p class="snippet">from scipy.stats import shapiro</p><p class="snippet"> </p><p class="snippet">sns.distplot((y_test - predictions), bins = 50)</p><p class="snippet">plt.xlabel('Residuals')</p><p class="snippet">plt.ylabel('Density')</p><p class="snippet">plt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - predictions)[1]))</p><p class="snippet">plt.show()</p><p>Refer to the resultant output here:</p><div id="_idContainer342" class="IMG---Figure"><img src="Images/C13322_03_34.jpg" alt="Figure 3.34: The distribution of the residuals from a multiple linear regression model&#13;&#10;" width="569" height="276"/></div><h6>Figure 3.34: The distribution of the residuals from a multiple linear regression model</h6><h4>Note</h4><p class="callout">Our residuals are negatively skewed and non-normal, but this is less skewed than in the simple linear model.</p></li>
				<li>Calculate the metrics for mean absolute error, mean squared error, root mean squared error, and R-squared, and put them into a DataFrame as follows:<p class="snippet">from sklearn import metrics</p><p class="snippet">import numpy as np</p><p class="snippet"> </p><p class="snippet">metrics_df = pd.DataFrame({'Metric': ['MAE', </p><p class="snippet">                                      'MSE', </p><p class="snippet">                                      'RMSE', </p><p class="snippet">                                      'R-Squared'],</p><p class="snippet">                          'Value': [metrics.mean_absolute_error(y_test, predictions),</p><p class="snippet">                                      metrics.mean_squared_error(y_test, predictions),</p><p class="snippet">                                    np.sqrt(metrics.mean_squared_error(y_test, predictions)),</p><p class="snippet">                                    metrics.explained_variance_score(y_test, predictions)]}).round(3)</p><p class="snippet">print(metrics_df)</p><p>Please refer to the resultant output:</p><div id="_idContainer343" class="IMG---Figure"><img src="Images/C13322_03_35.jpg" alt="Figure 3.35: Model evaluation metrics from a multiple linear regression model&#13;&#10;" width="482" height="79"/></div></li>
			</ol>
			<h6> </h6>
			<h6>Figure 3.35: Model evaluation metrics from a multiple linear regression model</h6>
			<p>The multiple linear regression model performed better on every metric relative to the simple linear regression model.</p>
			<h3 id="_idParaDest-214"><a id="_idTextAnchor234"/>Activity 6: Generating Predictions and Evaluating Performance of a Tuned Logistic Regression Model</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Generate the predicted probabilities of rain using the following code:<p class="snippet">predicted_prob = model.predict_proba(X_test)[:,1]</p></li>
				<li>Generate the predicted class of rain using <strong class="inline">predicted_class = model.predict(X_test)</strong>.</li>
				<li>Evaluate performance using a confusion matrix and save it as a DataFrame using the following code:<p class="snippet">from sklearn.metrics import confusion_matrix</p><p class="snippet">import numpy as np</p><p class="snippet"> </p><p class="snippet">cm = pd.DataFrame(confusion_matrix(y_test, predicted_class))</p><p class="snippet">cm['Total'] = np.sum(cm, axis=1)</p><p class="snippet">cm = cm.append(np.sum(cm, axis=0), ignore_index=True)</p><p class="snippet">cm.columns = ['Predicted No', 'Predicted Yes', 'Total']</p><p class="snippet">cm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])</p><p class="snippet">print(cm)</p><h6> </h6><div id="_idContainer344" class="IMG---Figure"><img src="Images/C13322_03_36.jpg" alt="Figure 3.36: The confusion matrix from our logistic regression grid search model&#13;&#10;" width="503" height="64"/></div><h6>Figure 3.36: The confusion matrix from our logistic regression grid search model</h6><h4>Note</h4><p class="callout">Nice! We have decreased our number of false positives from 6 to 2. Additionally, our false negatives were lowered from 10 to 4 (see in <em class="italics">Exercise 26</em>). Be aware that results may vary slightly. </p></li>
				<li>For further evaluation, print a classification report as follows:<p class="snippet">from sklearn.metrics import classification_report</p><p class="snippet"> </p><p class="snippet">print(classification_report(y_test, predicted_class))</p></li>
			</ol>
			<h6>            </h6>
			<div>
				<div id="_idContainer345" class="IMG---Figure">
					<img src="Images/C13322_03_37.jpg" alt="Figure 3.37: The classification report from our logistic regression grid search model&#13;&#10;" width="567" height="122"/>
				</div>
			</div>
			<h6>Figure 3.37: The classification report from our logistic regression grid search model</h6>
			<p>By tuning the hyperparameters of the logistic regression model, we were able to improve upon a logistic regression model that was already performing very well.</p>
			<h3 id="_idParaDest-215"><a id="_idTextAnchor235"/>Activity 7: Generating Predictions and Evaluating the Performance of the SVC Grid Search Model</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Extract predicted classes of rain using the following code:<p class="snippet">predicted_class = model.predict(X_test)</p></li>
				<li>Create and print a confusion matrix using the code here:<p class="snippet">from sklearn.metrics import confusion_matrix</p><p class="snippet">import numpy as np</p><p class="snippet"> </p><p class="snippet">cm = pd.DataFrame(confusion_matrix(y_test, predicted_class))</p><p class="snippet">cm['Total'] = np.sum(cm, axis=1)</p><p class="snippet">cm = cm.append(np.sum(cm, axis=0), ignore_index=True)</p><p class="snippet">cm.columns = ['Predicted No', 'Predicted Yes', 'Total']</p><p class="snippet">cm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])</p><p class="snippet">print(cm)</p><p>See the resultant output here:</p><h6>                 </h6><div id="_idContainer346" class="IMG---Figure"><img src="Images/C13322_03_38.jpg" alt="Figure 3.38: The confusion matrix from our SVC grid search model&#13;&#10;" width="567" height="62"/></div><h6>Figure 3.38: The confusion matrix from our SVC grid search model</h6></li>
				<li>Generate and print a classification report as follows:<p class="snippet">from sklearn.metrics import classification_report</p><p class="snippet"> </p><p class="snippet">print(classification_report(y_test, predicted_class))</p><p>See the resultant output here:</p></li>
			</ol>
			<h6> </h6>
			<div>
				<div id="_idContainer347" class="IMG---Figure">
					<img src="Images/C13322_03_39.jpg" alt="Figure 3.39: The classification report from our SVC grid search model&#13;&#10;" width="538" height="122"/>
				</div>
			</div>
			<h6>Figure 3.39: The classification report from our SVC grid search model</h6>
			<p>Here, we demonstrated how to tune the hyperparameters of an SVC model using grid search.</p>
			<h3 id="_idParaDest-216"><a id="_idTextAnchor236"/>Activity 8: Preparing Data for a Decision Tree Classifier</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Import <strong class="inline">weather.csv</strong> and store it as a DataFrame using the following:<p class="snippet">import pandas as pd</p><p class="snippet"> </p><p class="snippet">df = pd.read_csv('weather.csv')</p></li>
				<li>Dummy code the <strong class="inline">Description</strong> column as follows:<p class="snippet">import pandas as pd</p><p class="snippet"> </p><p class="snippet">df_dummies = pd.get_dummies(df, drop_first=True)</p></li>
				<li>Shuffle <strong class="inline">df_dummies</strong> using the following code:<p class="snippet">from sklearn.utils import shuffle</p><p class="snippet"> </p><p class="snippet">df_shuffled = shuffle(df_dummies, random_state=42)</p></li>
				<li>Split <strong class="inline">df_shuffled</strong> into X and y as follows:<p class="snippet">DV = 'Rain'</p><p class="snippet">X = df_shuffled.drop(DV, axis=1)</p><p class="snippet">y = df_shuffled[DV]</p></li>
				<li>Split <strong class="inline">X</strong> and <strong class="inline">y</strong> into testing and training data:<p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet"> </p><p class="snippet">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</p></li>
				<li>Scale <strong class="inline">X_train</strong> and <strong class="inline">X_test</strong> using the following code:<p class="snippet">from sklearn.preprocessing import StandardScaler</p><p class="snippet"> </p><p class="snippet">model = StandardScaler()</p><p class="snippet">X_train_scaled = model.fit_transform(X_train)</p><p class="snippet">X_test_scaled = model.transform(X_test)</p></li>
			</ol>
			<h3 id="_idParaDest-217"><a id="_idTextAnchor237"/>Activity 9: Generating Predictions and Evaluating the Performance of a Decision Tree Classifier Model</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Generate the predicted probabilities of rain using the following:<p class="snippet">predicted_prob = model.predict_proba(X_test_scaled)[:,1]</p></li>
				<li>Generate the predicted classes of rain using the following:<p class="snippet">predicted_class = model.predict(X_test)</p></li>
				<li>Generate and print a confusion matrix with the code here:<p class="snippet">from sklearn.metrics import confusion_matrix</p><p class="snippet">import numpy as np</p><p class="snippet"> </p><p class="snippet">cm = pd.DataFrame(confusion_matrix(y_test, predicted_class))</p><p class="snippet">cm['Total'] = np.sum(cm, axis=1)</p><p class="snippet">cm = cm.append(np.sum(cm, axis=0), ignore_index=True)</p><p class="snippet">cm.columns = ['Predicted No', 'Predicted Yes', 'Total']</p><p class="snippet">cm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])</p><p class="snippet">print(cm)</p><p>Refer to the resultant output here:</p><div id="_idContainer348" class="IMG---Figure"><img src="Images/C13322_03_40.jpg" alt="Figure 3.40: The confusion matrix from our tuned decision tree classifier model&#13;&#10;" width="501" height="61"/></div><h6>Figure 3.40: The confusion matrix from our tuned decision tree classifier model</h6></li>
				<li>Print a classification report as follows:<p class="snippet">from sklearn.metrics import classification_report</p><p class="snippet">print(classification_report(y_test, predicted_class))</p><p>Refer to the resultant output here:</p></li>
			</ol>
			<div>
				<div id="_idContainer349" class="IMG---Figure">
					<img src="Images/C13322_03_41.jpg" alt="Figure 3.41: The classification report from our tuned decision tree classifier model&#13;&#10;" width="517" height="121"/>
				</div>
			</div>
			<h6>Figure 3.41: The classification report from our tuned decision tree classifier model</h6>
			<p>There was only one misclassified observation. Thus, by tuning a decision tree classifier model on our <strong class="inline">weather.csv</strong> dataset, we were able to predict rain (or snow) with great accuracy. We can see that the sole driving feature was temperature in Celsius. This makes sense due to the way in which decision trees use recursive partitioning to make predictions.</p>
			<h3 id="_idParaDest-218"><a id="_idTextAnchor238"/>Activity 10: Tuning a Random Forest Regressor</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Specify the hyperparameter space as follows:<p class="snippet">import numpy as np</p><p class="snippet"> </p><p class="snippet">grid = {'criterion': ['mse','mae'],</p><p class="snippet">        'max_features': ['auto', 'sqrt', 'log2', None],</p><p class="snippet">        'min_impurity_decrease': np.linspace(0.0, 1.0, 10),</p><p class="snippet">        'bootstrap': [True, False],</p><p class="snippet">        'warm_start': [True, False]}</p></li>
				<li>Instantiate the <strong class="inline">GridSearchCV</strong> model, optimizing the explained variance using the following code:<p class="snippet">from sklearn.model_selection import GridSearchCV</p><p class="snippet">from sklearn.ensemble import RandomForestRegressor</p><p class="snippet"> </p><p class="snippet">model = GridSearchCV(RandomForestRegressor(), grid, scoring='explained_variance', cv=5)</p></li>
				<li>Fit the grid search model to the training set using the following (note that this may take a while):<p class="snippet">model.fit(X_train_scaled, y_train)</p><p>See the output here:</p><div id="_idContainer350" class="IMG---Figure"><img src="Images/C13322_03_42.jpg" alt="Figure 3.42: The output from our tuned random forest regressor grid search model&#13;&#10;" width="671" height="196"/></div><h6>Figure 3.42: The output from our tuned random forest regressor grid search model</h6></li>
				<li>Print the tuned parameters as follows:<p class="snippet">best_parameters = model.best_params_</p><p class="snippet">print(best_parameters)</p><p>See the resultant output below: </p></li>
			</ol>
			<div>
				<div id="_idContainer351" class="IMG---Figure">
					<img src="Images/C13322_03_43.jpg" alt="Figure 3.43: The tuned hyperparameters from our random forest regressor grid search model&#13;&#10;" width="779" height="19"/>
				</div>
			</div>
			<h6>Figure 3.43: The tuned hyperparameters from our random forest regressor grid search model</h6>
			<h3 id="_idParaDest-219"><a id="_idTextAnchor239"/>Activity 11: Generating Predictions and Evaluating the Performance of a Tuned Random Forest Regressor Model</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Generate predictions on the test data using the following:<p class="snippet">predictions = model.predict(X_test_scaled)</p></li>
				<li>Plot the correlation of predicted and actual values using the following code:<p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from scipy.stats import pearsonr</p><p class="snippet"> </p><p class="snippet">plt.scatter(y_test, predictions)</p><p class="snippet">plt.xlabel('Y Test (True Values)')</p><p class="snippet">plt.ylabel('Predicted Values')</p><p class="snippet">plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, predictions)[0], 2))</p><p class="snippet">plt.show()</p><p>Refer to the resultant output here:</p><div id="_idContainer352" class="IMG---Figure"><img src="Images/C13322_03_44.jpg" alt="Figure 3.44: A scatterplot of predicted and actual values from our random forest regression model with tuned hyperparameters&#13;&#10;" width="545" height="271"/></div><p class="Normal" lang="en-US" xml:lang="en-US"> </p><h6>Figure 3.44: A scatterplot of predicted and actual values from our random forest regression model with tuned hyperparameters</h6></li>
				<li>Plot the distribution of residuals as follows:<p class="snippet">import seaborn as sns</p><p class="snippet">from scipy.stats import shapiro</p><p class="snippet"> </p><p class="snippet">sns.distplot((y_test - predictions), bins = 50)</p><p class="snippet">plt.xlabel('Residuals')</p><p class="snippet">plt.ylabel('Density')</p><p class="snippet">plt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - predictions)[1]))</p><p class="snippet">plt.show()</p><p>Refer to the resultant output here:</p><h6>                 </h6><div id="_idContainer353" class="IMG---Figure"><img src="Images/C13322_03_45.jpg" alt="Figure 3.45: A histogram of residuals from a random forest regression model with &#13;&#10;tuned hyperparameters&#13;&#10;" width="572" height="273"/></div><h6>Figure 3.45: A histogram of residuals from a random forest regression model with tuned hyperparameters</h6></li>
				<li>Compute metrics, place them in a DataFrame, and print it using the code here:<p class="snippet">from sklearn import metrics</p><p class="snippet">import numpy as np</p><p class="snippet"> </p><p class="snippet">metrics_df = pd.DataFrame({'Metric': ['MAE', </p><p class="snippet">                                      'MSE', </p><p class="snippet">                                      'RMSE', </p><p class="snippet">                                      'R-Squared'],</p><p class="snippet">                          'Value': [metrics.mean_absolute_error(y_test, predictions),</p><p class="snippet">                                    metrics.mean_squared_error(y_test, predictions),</p><p class="snippet">                                    np.sqrt(metrics.mean_squared_error(y_test, predictions)),</p><p class="snippet">                                    metrics.explained_variance_score(y_test, predictions)]}).round(3)</p><p class="snippet">print(metrics_df)</p><p>Find the resultant output here:</p></li>
			</ol>
			<h6>                 </h6>
			<div>
				<div id="_idContainer354" class="IMG---Figure">
					<img src="Images/C13322_03_46.jpg" alt="Figure 3.46: Model evaluation metrics from our random forest regression model with tuned hyperparameters&#13;&#10;" width="489" height="75"/>
				</div>
			</div>
			<h6>Figure 3.46: Model evaluation metrics from our random forest regression model with tuned hyperparameters</h6>
			<p>The random forest regressor model seems to underperform compared to the multiple linear regression, as evidenced by greater MAE, MSE, and RMSE values, as well as less explained variance. Additionally, there was a weaker correlation between the predicted and actual values, and the residuals were further from being normally distributed. Nevertheless, by leveraging ensemble methods using a random forest regressor, we constructed a model that explains 75.8% of the variance in temperature and predicts temperature in Celsius + 3.781 degrees.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor240"/>Chapter 4: Dimensionality Reduction and Unsupervised Learning</h2>
			<h3 id="_idParaDest-221"><a id="_idTextAnchor241"/>Activity 12: Ensemble k-means Clustering and Calculating Predictions</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<p>After the glass dataset has been imported, shuffled, and standardized (see Exercise 58):</p>
			<ol>
				<li value="1">Instantiate an empty data frame to append each model and save it as the new data frame object <strong class="inline">labels_df</strong> with the following code:<p class="snippet">import pandas as pd</p><p class="snippet">labels_df = pd.DataFrame()</p></li>
				<li>Import the <strong class="inline">KMeans</strong> function outside of the loop using the following:<p class="snippet">from sklearn.cluster import KMeans</p></li>
				<li>Complete 100 iterations as follows:<p class="snippet">for i in range(0, 100):</p></li>
				<li>Save a KMeans model object with two clusters (arbitrarily decided upon, a priori) using:<p class="snippet">model = KMeans(n_clusters=2)</p></li>
				<li>Fit the model to <strong class="inline">scaled_features</strong> using the following:<p class="snippet">model.fit(scaled_features)</p></li>
				<li>Generate the labels array and save it as the labels object, as follows:<p class="snippet">labels = model.labels_</p></li>
				<li>Store labels as a column in <strong class="inline">labels_df</strong> named after the iteration using the code:<p class="snippet">labels_df['Model_{}_Labels'.format(i+1)] = labels</p></li>
				<li>After labels have been generated for each of the 100 models (see Activity 21), calculate the mode for each row using the following code:<p class="snippet">row_mode = labels_df.mode(axis=1)</p></li>
				<li>Assign <strong class="inline">row_mode</strong> to a new column in <strong class="inline">labels_df</strong>, as shown in the following code:<p class="snippet">labels_df['row_mode'] = row_mode</p></li>
				<li>View the first five rows of labels_df <p class="snippet">print(labels_df.head(5))</p></li>
			</ol>
			<div>
				<div id="_idContainer355" class="IMG---Figure">
					<img src="Images/C13322_04_24.jpg" alt="Figure 4.24: First five rows of labels_df&#13;&#10;" width="836" height="124"/>
				</div>
			</div>
			<h6>Figure 4.24: First five rows of labels_df</h6>
			<p>We have drastically increased the confidence in our predictions by iterating through numerous models, saving the predictions at each iteration, and assigning the final predictions as the mode of these predictions. However, these predictions were generated by models using a predetermined number of clusters. Unless we know the number of clusters a priori, we will want to discover the optimal number of clusters to segment our observations.</p>
			<h3 id="_idParaDest-222"><a id="_idTextAnchor242"/>Activity 13: Evaluating Mean Inertia by Cluster after PCA Transformation</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Instantiate a PCA model with the value for the <strong class="inline">n_components</strong> argument equal to <strong class="inline">best_n_components</strong> (that is, remember, <strong class="inline">best_n_components = 6</strong>) as follows:<p class="snippet">from sklearn.decomposition import PCA</p><p class="snippet">model = PCA(n_components=best_n_components)</p></li>
				<li>Fit the model to <strong class="inline">scaled_features</strong> and transform them into the six components, as shown here:<p class="snippet">df_pca = model.fit_transform(scaled_features)</p></li>
				<li>Import <strong class="inline">numpy</strong> and the <strong class="inline">KMeans</strong> function outside the loop using the following code:<p class="snippet">from sklearn.cluster import KMeans</p><p class="snippet">import numpy as np</p></li>
				<li>Instantiate an empty list, <strong class="inline">inertia_list</strong>, for which we will append inertia values after each iteration using the following code:<p class="snippet">inertia_list = []</p></li>
				<li>In the inside for loop, we will iterate through 100 models as follows:<p class="snippet">for i in range(100):</p></li>
				<li>Build our <strong class="inline">KMeans</strong> model with <strong class="inline">n_clusters=x</strong> using:<p class="snippet">model = KMeans(n_clusters=x)</p><h4>Note</h4><p class="callout">The value for x will be dictated by the outer loop which is covered in detail here.</p></li>
				<li>Fit the model to <strong class="inline">df_pca</strong> as follows:<p class="snippet">model.fit(df_pca)</p></li>
				<li>Get the inertia value and save it to the object inertia using the following code:<p class="snippet">inertia = model.inertia_</p></li>
				<li>Append inertia to <strong class="inline">inertia_list</strong> using the following code:<p class="snippet">inertia_list.append(inertia)</p></li>
				<li>Moving to the outside loop, instantiate another empty list to store the average inertia values using the following code:<p class="snippet">mean_inertia_list_PCA = []</p></li>
				<li>Since we want to check the average inertia over 100 models for <strong class="inline">n_clusters</strong> 1 through 10, we will instantiate the outer loop as follows:<p class="snippet">for x in range(1, 11):</p></li>
				<li>After the inside loop has run through its 100 iterations, and the inertia value for each of the 100 models have been appended to <strong class="inline">inertia_list</strong>, compute the mean of this list, and save the object as <strong class="inline">mean_inertia</strong> using the following code:<p class="snippet">mean_inertia = np.mean(inertia_list)</p></li>
				<li>Append <strong class="inline">mean_inertia</strong> to <strong class="inline">mean_inertia_list_PCA</strong> using the following code:<p class="snippet">mean_inertia_list_PCA.append(mean_inertia)</p></li>
				<li>	Print <strong class="inline">mean_inertia_list_PCA</strong> to the console using the following code:<p class="snippet">print(mean_inertia_list_PCA)</p></li>
				<li> Notice the output in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer356" class="IMG---Figure">
					<img src="Images/C13322_04_25.jpg" alt="Figure 4.25: mean_inertia_list_PCA&#13;&#10;" width="835" height="32"/>
				</div>
			</div>
			<h6>Figure 4.25: mean_inertia_list_PCA</h6>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor243"/>Chapter 5: Mastering Structured Data</h2>
			<h3 id="_idParaDest-224"><a id="_idTextAnchor244"/>Activity 14: Training and Predicting the Income of a Person</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Import the libraries and load the income dataset using pandas. First, import pandas and then read the data using <strong class="inline">read_csv</strong>.<p class="snippet">import pandas as pd</p><p class="snippet">import xgboost as xgb</p><p class="snippet">import numpy as np</p><p class="snippet">from sklearn.metrics import accuracy_score</p><p class="snippet">data = pd.read_csv("../data/adult-data.csv", names=['age', 'workclass', 'education-num', 'occupation', 'capital-gain', 'capital-loss', 'hours-per-week', 'income'])</p><p>The reason we are passing the names of the columns is because the data doesn't contain them. We do this to make our lives easy.</p></li>
				<li>Use Label Encoder from sklearn to encode strings. First, import <strong class="inline">Label Encoder</strong>. Then, encode all string categorical columns one by one.<p class="snippet">from sklearn.preprocessing import LabelEncoder</p><p class="snippet">data['workclass'] = LabelEncoder().fit_transform(data['workclass'])</p><p class="snippet">data['occupation'] = LabelEncoder().fit_transform(data['occupation'])</p><p class="snippet">data['income'] = LabelEncoder().fit_transform(data['income'])</p><p>Here, we encode all the categorical string data that we have. There is another method we can use to prevent writing the same piece of code again and again. See if you can find it.</p></li>
				<li>We first separate the dependent and independent variables.<p class="snippet">X = data.copy()</p><p class="snippet">X.drop("income", inplace = True, axis = 1)</p><p class="snippet">Y = data.income</p></li>
				<li>Then, we divide them into training and testing sets with an 80:20 split.<p class="snippet">X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values</p><p class="snippet">Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values</p></li>
				<li>Next, we convert them into DMatrix, a data structure that the library supports.<p class="snippet">train = xgb.DMatrix(X_train, label=Y_train)</p><p class="snippet">test = xgb.DMatrix(X_test, label=Y_test)</p></li>
				<li>Then, we use the following parameters to train the model using XGBoost.<p class="snippet">param = {'max_depth':7, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'} num_round = 50</p><p class="snippet">model = xgb.train(param, train, num_round)</p></li>
				<li>Check the accuracy of the model.<p class="snippet">preds = model.predict(test)</p><p class="snippet">accuracy = accuracy_score(Y[int(Y.shape[0]*0.8):].values, preds)</p><p class="snippet">print("Accuracy: %.2f%%" % (accuracy * 100.0))</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer357" class="IMG---Figure">
					<img src="Images/C13322_05_36.jpg" alt="Figure 5.36: Final model accuracy&#13;&#10;" width="602" height="20"/>
				</div>
			</div>
			<h6>Figure 5.36: Final model accuracy</h6>
			<h3 id="_idParaDest-225"><a id="_idTextAnchor245"/>Activity 15: Predicting the Loss of Customers</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Load the income dataset using pandas. First, import pandas, and then read the data using <strong class="inline">read_csv</strong>.<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">data = data = pd.read_csv("data/telco-churn.csv")</p></li>
				<li>The <strong class="inline">customerID</strong> variable is not required because any future prediction will have a unique <strong class="inline">customerID</strong>, making this variable useless for prediction.<p class="snippet">data.drop('customerID', axis = 1, inplace = True)</p></li>
				<li>Convert all categorical variables to integers using scikit. One example is given below.<p class="snippet">from sklearn.preprocessing import LabelEncoder</p><p class="snippet">data['gender'] = LabelEncoder().fit_transform(data['gender'])</p></li>
				<li>Check the data types of the variables in the dataset.<p class="snippet">data.dtypes</p><p>The data types of the variables will be shown as follows:</p><div id="_idContainer358" class="IMG---Figure"><img src="Images/C13322_05_37.jpg" alt="Figure 5.37: Data types of variables&#13;&#10;" width="930" height="448"/></div><h6>Figure 5.37: Data types of variables</h6></li>
				<li>As you can see, <strong class="inline">TotalCharges</strong> is an object. So, convert the data type of <strong class="inline">TotalCharges</strong> from object to numeric. coerce will make the missing values null.<p class="snippet">data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')</p></li>
				<li>Convert the data frame to an XGBoost variable and find the best parameters for the dataset using the previous exercises as reference.<p class="snippet">import xgboost as xgb</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">X = data.copy()</p><p class="snippet">X.drop("Churn", inplace = True, axis = 1)</p><p class="snippet">Y = data.Churn</p><p class="snippet">X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values</p><p class="snippet">Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values</p><p class="snippet">train = xgb.DMatrix(X_train, label=Y_train)</p><p class="snippet">test = xgb.DMatrix(X_test, label=Y_test)</p><p class="snippet">test_error = {}</p><p class="snippet">for i in range(20):</p><p class="snippet">    param = {'max_depth':i, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'}</p><p class="snippet">    num_round = 50</p><p class="snippet">    model_metrics = xgb.cv(param, train, num_round, nfold = 10)</p><p class="snippet">    test_error[i] = model_metrics.iloc[-1]['test-error-mean']</p><p class="snippet"> </p><p class="snippet">plt.scatter(test_error.keys(),test_error.values())</p><p class="snippet">plt.xlabel('Max Depth')</p><p class="snippet">plt.ylabel('Test Error')</p><p class="snippet">plt.show()</p><p>Check out the output in the following screenshot:</p><div id="_idContainer359" class="IMG---Figure"><img src="Images/C13322_05_38.jpg" alt="Figure 5.38: Graph of max depth to test error for telecom churn dataset&#13;&#10;" width="507" height="255"/></div><h6>Figure 5.38: Graph of max depth to test error for telecom churn dataset</h6><p>From the graph, it is clear that a max depth of 4 gives the least error. So, we will be using that to train our model.</p></li>
				<li>Create the model using the <strong class="inline">max_depth</strong> parameter that we chose from the previous steps.<p class="snippet">param = {'max_depth':4, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'}</p><p class="snippet">num_round = 100</p><p class="snippet">model = xgb.train(param, train, num_round)</p><p class="snippet">preds = model.predict(test)</p><p class="snippet">from sklearn.metrics import accuracy_score</p><p class="snippet">accuracy = accuracy_score(Y[int(Y.shape[0]*0.8):].values, preds)</p><p class="snippet">print("Accuracy: %.2f%%" % (accuracy * 100.0))</p><p>The output is as follows:</p><div id="_idContainer360" class="IMG---Figure"><img src="Images/C13322_05_39.jpg" alt="Figure 5.39: Final accuracy &#13;&#10;" width="579" height="18"/></div><h6>Figure 5.39: Final accuracy </h6></li>
				<li>Save the model for future use using the following code:<p class="snippet">model.save_model('churn-model.model')</p></li>
			</ol>
			<h3 id="_idParaDest-226"><a id="_idTextAnchor246"/>Activity 16: Predicting a Customer's Purchase Amount</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Load the <strong class="inline">Black Friday</strong> dataset using pandas. First, import <strong class="inline">pandas</strong>, and then, read the data using <strong class="inline">read_csv</strong>.<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">data = data = pd.read_csv("data/BlackFriday.csv")</p></li>
				<li>The <strong class="inline">User_ID</strong> variable is not required to allow predictions on new user Ids, so we drop it. <p class="snippet">data.isnull().sum()</p><p class="snippet">data.drop(['User_ID', 'Product_Category_2', 'Product_Category_3'], axis = 1, inplace = True)</p><p>The product category variables have high null values, so we drop them as well.</p></li>
				<li>Convert all categorical variables to integers using scikit-learn.<p class="snippet">from collections import defaultdict</p><p class="snippet">from sklearn.preprocessing import LabelEncoder, MinMaxScaler</p><p class="snippet">label_dict = defaultdict(LabelEncoder)</p><p class="snippet">data[['Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1']] = data[['Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1']].apply(lambda x: label_dict[x.name].fit_transform(x)) </p></li>
				<li>Split the data into training and testing sets and convert it into the form required by the embedding layers. <p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet">X = data</p><p class="snippet">y = X.pop('Purchase')</p><p class="snippet">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=9)</p><p class="snippet"> </p><p class="snippet">cat_cols_dict = {col: list(data[col].unique()) for col in ['Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1']}</p><p class="snippet">train_input_list = []</p><p class="snippet">test_input_list = []</p><p class="snippet"> </p><p class="snippet">for col in cat_cols_dict.keys():</p><p class="snippet">    raw_values = np.unique(data[col])</p><p class="snippet">    value_map = {}</p><p class="snippet">    for i in range(len(raw_values)):</p><p class="snippet">        value_map[raw_values[i]] = i       </p><p class="snippet">    train_input_list.append(X_train[col].map(value_map).values)</p><p class="snippet">    test_input_list.append(X_test[col].map(value_map).fillna(0).values)</p></li>
				<li>Create the network using the embedding and dense layers in Keras and perform hyperparameter tuning to get the best accuracy. <p class="snippet">from keras.models import Model</p><p class="snippet">from keras.layers import Input, Dense, Concatenate, Reshape, Dropout</p><p class="snippet">from keras.layers.embeddings import Embedding</p><p class="snippet">cols_out_dict = {</p><p class="snippet">    'Product_ID': 20,</p><p class="snippet">    'Gender': 1,</p><p class="snippet">    'Age': 2,</p><p class="snippet">    'Occupation': 6,</p><p class="snippet">    'City_Category': 1,</p><p class="snippet">    'Stay_In_Current_City_Years': 2,</p><p class="snippet">    'Marital_Status': 1,</p><p class="snippet">    'Product_Category_1': 9</p><p class="snippet">}</p><p class="snippet"> </p><p class="snippet">inputs = []</p><p class="snippet">embeddings = []</p><p class="snippet"> </p><p class="snippet">for col in cat_cols_dict.keys():</p><p class="snippet"> </p><p class="snippet">    inp = Input(shape=(1,), name = 'input_' + col)</p><p class="snippet">    embedding = Embedding(len(cat_cols_dict[col]), cols_out_dict[col], input_length=1, name = 'embedding_' + col)(inp)</p><p class="snippet">    embedding = Reshape(target_shape=(cols_out_dict[col],))(embedding)</p><p class="snippet">    inputs.append(inp)</p><p class="snippet">    embeddings.append(embedding)</p></li>
				<li>Now, we create a three-layer network after the embedding layers.<p class="snippet">x = Concatenate()(embeddings)</p><p class="snippet">x = Dense(4, activation='relu')(x)</p><p class="snippet">x = Dense(2, activation='relu')(x)</p><p class="snippet">output = Dense(1, activation='relu')(x)</p><p class="snippet"> </p><p class="snippet">model = Model(inputs, output)</p><p class="snippet"> </p><p class="snippet">model.compile(loss='mae', optimizer='adam')</p><p class="snippet"> </p><p class="snippet">model.fit(train_input_list, y_train, validation_data = (test_input_list, y_test), epochs=20, batch_size=128)</p></li>
				<li>Check the RMSE of the model on the test set.<p class="snippet">from sklearn.metrics import mean_squared_error</p><p class="snippet">y_pred = model.predict(test_input_list)</p><p class="snippet">np.sqrt(mean_squared_error(y_test, y_pred))</p><p>The RMSE is:</p><div id="_idContainer361" class="IMG---Figure"><img src="Images/C13322_05_40.jpg" alt="Figure 5.40: RMSE model&#13;&#10;" width="525" height="15"/></div><h6>Figure 5.40: RMSE model</h6></li>
				<li>Visualize the product ID embedding.<p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.decomposition import PCA</p><p class="snippet">embedding_Product_ID = model.get_layer('embedding_Product_ID').get_weights()[0]</p><p class="snippet">pca = PCA(n_components=2) </p><p class="snippet">Y = pca.fit_transform(embedding_Product_ID[:40])</p><p class="snippet">plt.figure(figsize=(8,8))</p><p class="snippet">plt.scatter(-Y[:, 0], -Y[:, 1])</p><p class="snippet">for i, txt in enumerate(label_dict['Product_ID'].inverse_transform(cat_cols_dict['Product_ID'])[:40]):</p><p class="snippet">    plt.annotate(txt, (-Y[i, 0],-Y[i, 1]), xytext = (-20, 8), textcoords = 'offset points')</p><p class="snippet">plt.show()</p><p>The plot is as follows:</p><h6> </h6><div id="_idContainer362" class="IMG---Figure"><img src="Images/C13322_05_41.jpg" alt="" width="533" height="463"/></div><h6>Figure 5.41: Plot of clustered model</h6><p>From the plot, you can see that similar products have been clustered together by the model.</p></li>
				<li>Save the model for future use.<p class="snippet">model.save ('black-friday.model')</p></li>
			</ol>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor247"/>Chapter 6: Decoding Images</h2>
			<h3 id="_idParaDest-228"><a id="_idTextAnchor248"/>Activity 17: Predict if an Image Is of a Cat or a Dog</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">If you look at the name of the images in the dataset, you will find that the images of dogs start with dog followed by '.' and then a number, for example – "dog.123.jpg". Similarly, the images of cats start with cat. So, let's create a function to get the label from the name of the file:<p class="snippet">def get_label(file):</p><p class="snippet">    class_label = file.split('.')[0]</p><p class="snippet">    if class_label == 'dog': label_vector = [1,0]</p><p class="snippet">    elif class_label == 'cat': label_vector = [0,1]</p><p class="snippet">    return label_vector</p><p>Then, create a function to read, resize, and preprocess the images:</p><p class="snippet">import os</p><p class="snippet">import numpy as np</p><p class="snippet">from PIL import Image</p><p class="snippet">from tqdm import tqdm</p><p class="snippet">from random import shuffle</p><p class="snippet"> </p><p class="snippet">SIZE = 50</p><p class="snippet"> </p><p class="snippet">def get_data():</p><p class="snippet">    data = []</p><p class="snippet">    files = os.listdir(PATH)</p><p class="snippet"> </p><p class="snippet">    for image in tqdm(files): </p><p class="snippet">        label_vector = get_label(image) </p><p class="snippet">        img = Image.open(PATH + image).convert('L')</p><p class="snippet">        img = img.resize((SIZE,SIZE)) </p><p class="snippet">        data.append([np.asarray(img),np.array(label_vector)])</p><p class="snippet">        </p><p class="snippet">    shuffle(data)</p><p class="snippet">    return data</p><p><strong class="inline">SIZE</strong> here refers to the dimension of the final square image we will input to the model. We resize the image to have the length and breadth equal to <strong class="inline">SIZE</strong>.</p><h4>Note</h4><p class="callout">When running <strong class="inline">os.listdir(PATH)</strong>, you will find that all the images of cats come first, followed by images of dogs. </p></li>
				<li>To have the same distribution of both the classes in the training and testing sets, we will shuffle the data.</li>
				<li>Define the size of the image and read the data. Split the loaded data into training and testing sets:<p class="snippet">data = get_data()</p><p class="snippet">train = data[:7000]</p><p class="snippet">test = data[7000:]</p><p class="snippet">x_train = [data[0] for data in train]</p><p class="snippet">y_train = [data[1] for data in train]</p><p class="snippet">x_test = [data[0] for data in test]</p><p class="snippet">y_test = [data[1] for data in test]</p></li>
				<li>Transform the lists to numpy arrays and reshape the images to a format that Keras will accept:<p class="snippet">y_train = np.array(y_train)</p><p class="snippet">y_test = np.array(y_test)</p><p class="snippet">x_train = np.array(x_train).reshape(-1, SIZE, SIZE, 1)</p><p class="snippet">x_test = np.array(x_test).reshape(-1, SIZE, SIZE, 1)</p></li>
				<li>Create a CNN model that makes use of regularization to perform training:<p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization</p><p class="snippet">model = Sequential() </p><p>Add the convolutional layers:</p><p class="snippet">model.add(Conv2D(48, (3, 3), activation='relu', padding='same', input_shape=(50,50,1)))    </p><p class="snippet">model.add(Conv2D(48, (3, 3), activation='relu'))  </p><p>Add the pooling layer:</p><p class="snippet">model.add(MaxPool2D(pool_size=(2, 2)))</p></li>
				<li>Add the batch normalization layer along with a dropout layer using the following code:<p class="snippet">model.add(BatchNormalization())</p><p class="snippet">model.add(Dropout(0.10))</p></li>
				<li>Flatten the 2D matrices into 1D vectors:<p class="snippet">model.add(Flatten())</p></li>
				<li>Use dense layers as the final layers for the model:<p class="snippet">model.add(Dense(512, activation='relu'))</p><p class="snippet">model.add(Dropout(0.5))</p><p class="snippet">model.add(Dense(2, activation='softmax'))</p></li>
				<li>Compile the model and then train it using the training data:<p class="snippet">model.compile(loss='categorical_crossentropy', </p><p class="snippet">              optimizer='adam',</p><p class="snippet">              metrics = ['accuracy'])</p><p class="snippet">Define the number of epochs you want to train the model for:</p><p class="snippet">EPOCHS = 10</p><p class="snippet">model_details = model.fit(x_train, y_train,</p><p class="snippet">                    batch_size = 128, </p><p class="snippet">                    epochs = EPOCHS, </p><p class="snippet">                    validation_data= (x_test, y_test),</p><p class="snippet">                    verbose=1)</p></li>
				<li>Print the model's accuracy on the test set:<p class="snippet">score = model.evaluate(x_test, y_test)</p><p class="snippet">print("Accuracy: {0:.2f}%".format(score[1]*100)) </p><div id="_idContainer363" class="IMG---Figure"><img src="Images/C13322_06_39.jpg" alt="Figure 6.39: Model accuracy on the test set&#13;&#10;" width="572" height="22"/></div><h6>Figure 6.39: Model accuracy on the test set</h6></li>
				<li>Print the model's accuracy on the training set:<p class="snippet">score = model.evaluate(x_train, y_train)</p><p class="snippet">print("Accuracy: {0:.2f}%".format(score[1]*100)) </p></li>
			</ol>
			<div>
				<div id="_idContainer364" class="IMG---Figure">
					<img src="Images/C13322_06_40.jpg" alt="Figure 6.40: Model accuracy on the train set&#13;&#10;" width="546" height="23"/>
				</div>
			</div>
			<h6>Figure 6.40: Model accuracy on the train set</h6>
			<p>The test set accuracy for this model is 70.4%. The training set accuracy is really high, at 96%. This means that the model has started to overfit. Improving the model to get the best possible accuracy is left for you as an exercise. You can plot the incorrectly predicted images using the code from previous exercises to get a sense of how well the model performs:</p>
			<p class="snippet">import matplotlib.pyplot as plt</p>
			<p class="snippet">y_pred = model.predict(x_test)</p>
			<p class="snippet">incorrect_indices = np.nonzero(np.argmax(y_pred,axis=1) != np.argmax(y_test,axis=1))[0]</p>
			<p class="snippet">labels = ['dog', 'cat']</p>
			<p class="snippet">image = 5</p>
			<p class="snippet">plt.imshow(x_test[incorrect_indices[image]].reshape(50,50),  cmap=plt.get_cmap('gray'))</p>
			<p class="snippet">plt.show()</p>
			<p class="snippet">print("Prediction: {0}".format(labels[np.argmax(y_pred[incorrect_indices[image]])]))</p>
			<div>
				<div id="_idContainer365" class="IMG---Figure">
					<img src="Images/C13322_06_41.jpg" alt="Figure 6.41: Incorrect prediction of a dog by the regularized CNN model&#13;&#10;" width="780" height="352"/>
				</div>
			</div>
			<h6>Figure 6.41: Incorrect prediction of a dog by the regularized CNN model</h6>
			<h3 id="_idParaDest-229"><a id="_idTextAnchor249"/>Activity 18: Identifying and Augmenting an Image</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Create functions to get the images and the labels of the dataset:<p class="snippet">from PIL import Image</p><p class="snippet">def get_input(file):</p><p class="snippet">    return Image.open(PATH+file)</p><p class="snippet"> </p><p class="snippet">def get_output(file):</p><p class="snippet">    class_label = file.split('.')[0]</p><p class="snippet">    if class_label == 'dog': label_vector = [1,0]</p><p class="snippet">    elif class_label == 'cat': label_vector = [0,1]</p><p class="snippet">    return label_vector</p></li>
				<li>Create functions to preprocess and augment images:<p class="snippet">SIZE = 50</p><p class="snippet">def preprocess_input(image):</p><p class="snippet">    # Data preprocessing</p><p class="snippet">    image = image.convert('L')</p><p class="snippet">    image = image.resize((SIZE,SIZE))</p><p class="snippet">    </p><p class="snippet">    </p><p class="snippet">    # Data augmentation</p><p class="snippet">    random_vertical_shift(image, shift=0.2)</p><p class="snippet">    random_horizontal_shift(image, shift=0.2)</p><p class="snippet">    random_rotate(image, rot_range=45)</p><p class="snippet">    random_horizontal_flip(image)</p><p class="snippet">    </p><p class="snippet">    return np.array(image).reshape(SIZE,SIZE,1)</p></li>
				<li>Implement the augmentation functions to randomly execute the augmentation when passed an image and return the image with the result. <p>This is for horizontal flip:</p><p class="snippet">import random</p><p class="snippet">def random_horizontal_flip(image):</p><p class="snippet">    toss = random.randint(1, 2)</p><p class="snippet">    if toss == 1:</p><p class="snippet">        return image.transpose(Image.FLIP_LEFT_RIGHT)</p><p class="snippet">    else:</p><p class="snippet">        return image</p><p>This is for rotation:</p><p class="snippet">def random_rotate(image, rot_range):</p><p class="snippet">    value = random.randint(-rot_range,rot_range)</p><p class="snippet">    return image.rotate(value)</p><p>This is for image shift:</p><p class="snippet">import PIL</p><p class="snippet">def random_horizontal_shift(image, shift):</p><p class="snippet">    width, height = image.size</p><p class="snippet">    rand_shift = random.randint(0,shift*width)</p><p class="snippet">    image = PIL.ImageChops.offset(image, rand_shift, 0)</p><p class="snippet">    image.paste((0), (0, 0, rand_shift, height))</p><p class="snippet">    return image</p><p class="snippet"> def random_vertical_shift(image, shift):</p><p class="snippet">    width, height = image.size</p><p class="snippet">    rand_shift = random.randint(0,shift*height)</p><p class="snippet">    image = PIL.ImageChops.offset(image, 0, rand_shift)</p><p class="snippet">    image.paste((0), (0, 0, width, rand_shift))</p><p class="snippet">    return image</p></li>
				<li>Finally, create the generator that will generate images batches to be used to train the model:<p class="snippet">import numpy as np</p><p class="snippet">def custom_image_generator(images, batch_size = 128):</p><p class="snippet">    while True:</p><p class="snippet">        # Randomly select images for the batch</p><p class="snippet">        batch_images = np.random.choice(images, size = batch_size)</p><p class="snippet">        batch_input = []</p><p class="snippet">        batch_output = [] </p><p class="snippet">        </p><p class="snippet">        # Read image, perform preprocessing and get labels</p><p class="snippet">        for file in batch_images:</p><p class="snippet">            # Function that reads and returns the image</p><p class="snippet">            input_image = get_input(file)</p><p class="snippet">            # Function that gets the label of the image</p><p class="snippet">            label = get_output(file)</p><p class="snippet">            # Function that pre-processes and augments the image</p><p class="snippet">            image = preprocess_input(input_image)</p><p class="snippet"> </p><p class="snippet">            batch_input.append(image)</p><p class="snippet">            batch_output.append(label)</p><p class="snippet"> </p><p class="snippet">        batch_x = np.array(batch_input)</p><p class="snippet">        batch_y = np.array(batch_output)</p><p class="snippet"> </p><p class="snippet">        # Return a tuple of (images,labels) to feed the network</p><p class="snippet">        yield(batch_x, batch_y)</p></li>
				<li>Create functions to load the test dataset's images and labels:<p class="snippet">def get_label(file):</p><p class="snippet">    class_label = file.split('.')[0]</p><p class="snippet">    if class_label == 'dog': label_vector = [1,0]</p><p class="snippet">    elif class_label == 'cat': label_vector = [0,1]</p><p class="snippet">    return label_vector</p><p>This <strong class="inline">get_data</strong> function is similar to the one we used in <em class="italics">Activity 1</em>. The modification here is that we get the list of images to be read as an input parameter, and we return a tuple of images and their labels:</p><p class="snippet">def get_data(files):</p><p class="snippet">    data_image = []</p><p class="snippet">    labels = []</p><p class="snippet">    for image in tqdm(files):</p><p class="snippet">        </p><p class="snippet">        label_vector = get_label(image)</p><p class="snippet">        </p><p class="snippet"> </p><p class="snippet">        img = Image.open(PATH + image).convert('L')</p><p class="snippet">        img = img.resize((SIZE,SIZE))</p><p class="snippet">        </p><p class="snippet">        </p><p class="snippet">        labels.append(label_vector)</p><p class="snippet">        data_image.append(np.asarray(img).reshape(SIZE,SIZE,1))</p><p class="snippet">        </p><p class="snippet">    data_x = np.array(data_image)</p><p class="snippet">    data_y = np.array(labels)</p><p class="snippet">        </p><p class="snippet">    return (data_x, data_y)</p></li>
				<li>Now, create the test train split and load the test dataset:<p class="snippet">import os</p><p class="snippet">files = os.listdir(PATH)</p><p class="snippet">random.shuffle(files)</p><p class="snippet">train = files[:7000]</p><p class="snippet">test = files[7000:]</p><p class="snippet">validation_data = get_data(test)</p></li>
				<li>Create the model and perform training:<p class="snippet">from keras.models import Sequential</p><p class="snippet">model = Sequential()</p><p>Add the convolutional layers</p><p class="snippet">from keras.layers import Input, Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization</p><p class="snippet">model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(50,50,1)))    </p><p class="snippet">model.add(Conv2D(32, (3, 3), activation='relu'))</p><p>Add the pooling layer:</p><p class="snippet">model.add(MaxPool2D(pool_size=(2, 2)))</p></li>
				<li>Add the batch normalization layer along with a dropout layer:<p class="snippet">model.add(BatchNormalization())</p><p class="snippet">model.add(Dropout(0.10))</p></li>
				<li>Flatten the 2D matrices into 1D vectors:<p class="snippet">model.add(Flatten())</p></li>
				<li>Use dense layers as the final layers for the model:<p class="snippet">model.add(Dense(512, activation='relu'))</p><p class="snippet">model.add(Dropout(0.5))</p><p class="snippet"> </p><p class="snippet">model.add(Dense(2, activation='softmax'))</p></li>
				<li>Compile the model and train it using the generator that you created:<p class="snippet">EPOCHS = 10</p><p class="snippet">BATCH_SIZE = 128</p><p class="snippet">model.compile(loss='categorical_crossentropy', </p><p class="snippet">              optimizer='adam',</p><p class="snippet">              metrics = ['accuracy']) </p><p class="snippet">model_details = model.fit_generator(custom_image_generator(train, batch_size = BATCH_SIZE),</p><p class="snippet">                    steps_per_epoch = len(train) // BATCH_SIZE, </p><p class="snippet">                    epochs = EPOCHS, </p><p class="snippet">                    validation_data= validation_data,</p><p class="snippet">                    verbose=1) </p></li>
			</ol>
			<p>The test set accuracy for this model is 72.6%, which is an improvement on the model in <em class="italics">Activity 21</em>. You will observe that the training accuracy is really high, at 98%. This means that this model has started to overfit, much like the one in <em class="italics">Activity 21</em>. This could be due to a lack of data augmentation. Try changing the data augmentation parameters to see if there is any change in accuracy. Alternatively, you can modify the architecture of the neural network to get better results. You can plot the incorrectly predicted images to get a sense of how well the model performs.</p>
			<p class="snippet">import matplotlib.pyplot as plt</p>
			<p class="snippet">y_pred = model.predict(validation_data[0])</p>
			<p class="snippet">incorrect_indices = np.nonzero(np.argmax(y_pred,axis=1) != np.argmax(validation_data[1],axis=1))[0]</p>
			<p class="snippet">labels = ['dog', 'cat']</p>
			<p class="snippet">image = 7</p>
			<p class="snippet">plt.imshow(validation_data[0][incorrect_indices[image]].reshape(50,50), cmap=plt.get_cmap('gray'))</p>
			<p class="snippet">plt.show()</p>
			<p class="snippet">print("Prediction: {0}".format(labels[np.argmax(y_pred[incorrect_indices[image]])]))</p>
			<div>
				<div id="_idContainer366" class="IMG---Figure">
					<img src="Images/C13322_06_42.jpg" alt="Figure 6.42: Incorrect prediction of a cat by the data augmentation CNN model &#13;&#10;" width="982" height="348"/>
				</div>
			</div>
			<h6>Figure 6.42: Incorrect prediction of a cat by the data augmentation CNN model</h6>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor250"/>Chapter 7: Processing Human Language</h2>
			<h3 id="_idParaDest-231"><a id="_idTextAnchor251"/>Activity 19: Predicting Sentiments of Movie Reviews</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Read the IMDB movie review dataset using pandas in Python:<p class="snippet">import pandas as pd</p><p class="snippet">data = pd.read_csv('../../chapter 7/data/movie_reviews.csv', encoding='latin-1')</p></li>
				<li>Convert the tweets to lowercase to reduce the number of unique words: <p class="snippet">data.text = data.text.str.lower()</p><h4>Note</h4><p class="callout">Keep in mind that "<strong class="inline">Hello</strong>" and "<strong class="inline">hellow</strong>" are not the same to a computer.</p></li>
				<li>Clean the reviews using RegEx with the <strong class="inline">clean_str</strong> function:<p class="snippet">import re</p><p class="snippet">def clean_str(string):</p><p class="snippet">    </p><p class="snippet">    string = re.sub(r"https?\://\S+", '', string)</p><p class="snippet">    string = re.sub(r'\&lt;a href', ' ', string)</p><p class="snippet">    string = re.sub(r'&amp;amp;', '', string) </p><p class="snippet">    string = re.sub(r'&lt;br /&gt;', ' ', string)</p><p class="snippet">    string = re.sub(r'[_"\-;%()|+&amp;=*%.,!?:#$@\[\]/]', ' ', string)</p><p class="snippet">    string = re.sub('\d','', string)</p><p class="snippet">    string = re.sub(r"can\'t", "cannot", string)</p><p class="snippet">    string = re.sub(r"it\'s", "it is", string)</p><p class="snippet">    return string</p><p class="snippet">data.SentimentText = data.SentimentText.apply(lambda x: clean_str(str(x)))</p></li>
				<li>Next, remove stop words and other frequently occurring unnecessary words from the reviews: <h4>Note</h4><p class="callout">To see how we found these, words refer to <em class="italics">Exercise 51</em>. </p></li>
				<li>This step converts strings into toke<a id="_idTextAnchor252"/>ns (which will be helpful in the next step):<p class="snippet">from nltk.corpus import stopwords</p><p class="snippet">from nltk.tokenize import word_tokenize,sent_tokenize</p><p class="snippet">stop_words = stopwords.words('english') + ['movie', 'film', 'time']</p><p class="snippet">stop_words = set(stop_words)</p><p class="snippet">remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]</p><p class="snippet">data['SentimentText'] = data['SentimentText'].apply(remove_stop_words)</p></li>
				<li>Create the word embedding of the reviews with the tokens created in the previous step. Here, we will use genism Word2Vec to create these embedding vectors:<p class="snippet">from gensim.models import Word2Vec</p><p class="snippet">model = Word2Vec(</p><p class="snippet">        data['SentimentText'].apply(lambda x: x[0]),</p><p class="snippet">        iter=10,</p><p class="snippet">        size=16,</p><p class="snippet">        window=5,</p><p class="snippet">        min_count=5,</p><p class="snippet">        workers=10)</p><p class="snippet">model.wv.save_word2vec_format('movie_embedding.txt', binary=False)</p></li>
				<li>Combine the tokens to get a string and then drop any review that does not have anything in it after stop word removal:<p class="snippet">def combine_text(text):    </p><p class="snippet">    try:</p><p class="snippet">        return ' '.join(text[0])</p><p class="snippet">    except:</p><p class="snippet">        return np.nan</p><p class="snippet"> </p><p class="snippet">data.SentimentText = data.SentimentText.apply(lambda x: combine_text(x))</p><p class="snippet">data = data.dropna(how='any')</p></li>
				<li>Tokenize the reviews using the Keras Tokenizer and convert them into numbers: <p class="snippet">from keras.preprocessing.text import Tokenizer</p><p class="snippet">tokenizer = Tokenizer(num_words=5000)</p><p class="snippet">tokenizer.fit_on_texts(list(data['SentimentText']))</p><p class="snippet">sequences = tokenizer.texts_to_sequences(data['SentimentText'])</p><p class="snippet">word_index = tokenizer.word_index</p></li>
				<li>Finally, pad the tweets to have a maximum of 100 words. This will remove any words after the 100-word limit and add 0s if the number of words is less than 100:<p class="snippet">from keras.preprocessing.sequence import pad_sequences</p><p class="snippet">reviews = pad_sequences(sequences, maxlen=100)</p></li>
				<li>Load the created embedding to get the embedding matrix using the <strong class="inline">load_embedding</strong> function discussed in the <em class="italics">Text Processing</em> section:<p class="snippet">import numpy as np</p><p class="snippet"> </p><p class="snippet">def load_embedding(filename, word_index , num_words, embedding_dim):</p><p class="snippet">    embeddings_index = {}</p><p class="snippet">    file = open(filename, encoding="utf-8")</p><p class="snippet">    for line in file:</p><p class="snippet">        values = line.split()</p><p class="snippet">        word = values[0]</p><p class="snippet">        coef = np.asarray(values[1:])</p><p class="snippet">        embeddings_index[word] = coef</p><p class="snippet">    file.close()</p><p class="snippet">    </p><p class="snippet">    embedding_matrix = np.zeros((num_words, embedding_dim))</p><p class="snippet">    for word, pos in word_index.items():</p><p class="snippet">        if pos &gt;= num_words:</p><p class="snippet">            continue</p><p class="snippet">        embedding_vector = embeddings_index.get(word)</p><p class="snippet">        if embedding_vector is not None:</p><p class="snippet">            embedding_matrix[pos] = embedding_vector</p><p class="snippet">    return embedding_matrix</p><p class="snippet"> </p><p class="snippet">embedding_matrix = load_embedding('movie_embedding.txt', word_index, len(word_index), 16)</p></li>
				<li>Convert the label into one-hot vector using pandas' <strong class="inline">get_dummies</strong> function and split the dataset into testing and training sets with an 80:20 split:<p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet">labels = pd.get_dummies(data.Sentiment)</p><p class="snippet">X_train, X_test, y_train, y_test = train_test_split(reviews,labels, test_size=0.2, random_state=9)</p></li>
				<li>Create the neural network model starting with the input and embedding layers. This layer converts the input words into their embedding vectors:<p class="snippet">from keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten</p><p class="snippet">from keras.models import Model</p><p class="snippet">inp = Input((100,))</p><p class="snippet">embedding_layer = Embedding(len(word_index),</p><p class="snippet">                    16,</p><p class="snippet">                    weights=[embedding_matrix],</p><p class="snippet">                    input_length=100,</p><p class="snippet">                    trainable=False)(inp)</p></li>
				<li>Create the rest of the fully connected neural network using Keras:<p class="snippet">model = Flatten()(embedding_layer)</p><p class="snippet">model = BatchNormalization()(model)</p><p class="snippet">model = Dropout(0.10)(model)</p><p class="snippet">model = Dense(units=1024, activation='relu')(model)</p><p class="snippet">model = Dense(units=256, activation='relu')(model)</p><p class="snippet">model = Dropout(0.5)(model)</p><p class="snippet">predictions = Dense(units=2, activation='softmax')(model)</p><p class="snippet">model = Model(inputs = inp, outputs = predictions)</p></li>
				<li>Compile and train the model for 10 epochs. You can modify the model and the hyperparameters to try and get a better accuracy:<p class="snippet">model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = ['acc'])</p><p class="snippet">model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=256)</p></li>
				<li>Calculate the accuracy of the model on the test set to see how well our model performs on previously unseen data by using the following:<p class="snippet">from sklearn.metrics import accuracy_score</p><p class="snippet">preds = model.predict(X_test)</p><p class="snippet">accuracy_score(np.argmax(preds, 1), np.argmax(y_test.values, 1))</p><p>The accuracy of the model is:</p><div id="_idContainer367" class="IMG---Figure"><img src="Images/C13322_07_39.jpg" alt="Figure 7.39: Model accuracy&#13;&#10;" width="537" height="24"/></div><h6>Figure 7.39: Model accuracy</h6></li>
				<li>Plot the confusion matrix of the model to get a proper sense of the model's prediction:<p class="snippet">y_actual = pd.Series(np.argmax(y_test.values, axis=1), name='Actual')</p><p class="snippet">y_pred = pd.Series(np.argmax(preds, axis=1), name='Predicted')</p><p class="snippet">pd.crosstab(y_actual, y_pred, margins=True)</p><p>Check the following </p><h6> </h6><div id="_idContainer368" class="IMG---Figure"><img src="Images/C13322_07_40.jpg" alt="Figure 7.40: Confusion matrix of the model (0 = negative sentiment, 1 = positive sentiment)&#13;&#10;" width="642" height="147"/></div><h6>Figure 7.40: Confusion matrix of the model (0 = negative sentiment, 1 = positive sentiment)</h6></li>
				<li>Check the performance of the model by seeing the sentiment predictions on random reviews using the following code:<p class="snippet">review_num = 111</p><p class="snippet">print("Review: \n"+tokenizer.sequences_to_texts([X_test[review_num]])[0])</p><p class="snippet">sentiment = "Positive" if np.argmax(preds[review_num]) else "Negative"</p><p class="snippet">print("\nPredicted sentiment = "+ sentiment)</p><p class="snippet">sentiment = "Positive" if np.argmax(y_test.values[review_num]) else "Negative"</p><p class="snippet">print("\nActual sentiment = "+ sentiment)</p><p>Check that you receive the following output:</p></li>
			</ol>
			<h6>                </h6>
			<div>
				<div id="_idContainer369" class="IMG---Figure">
					<img src="Images/C13322_07_41.jpg" alt="Figure 7.41: A review from the IMDB dataset&#13;&#10;" width="781" height="143"/>
				</div>
			</div>
			<h6>Figure 7.41: A review from the IMDB dataset</h6>
			<h3 id="_idParaDest-232"><a id="_idTextAnchor253"/>Activity 20: Predicting Sentiments from Tweets</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Read the tweet dataset using pandas and rename the columns with those given in the following code:<p class="snippet">import pandas as pd</p><p class="snippet">data = pd.read_csv('tweet-data.csv', encoding='latin-1', header=None)</p><p class="snippet">data.columns = ['sentiment', 'id', 'date', 'q', 'user', 'text']</p></li>
				<li>Drop the following columns as we won't be using them. You can analyze and use them if you want when trying to improve the accuracy:<p class="snippet">data = data.drop(['id', 'date', 'q', 'user'], axis=1)</p></li>
				<li>We perform this activity only on a subset (400,000 tweets) of the data to save time. If you want, you can work on the whole dataset:<p class="snippet">data = data.sample(400000).reset_index(drop=True)</p></li>
				<li>Convert the tweets to lowercase to reduce the number of unique words. Keep in mind that "<strong class="inline">Hello</strong>" and "<strong class="inline">hellow</strong>" are not the same to a computer:<p class="snippet">data.text = data.text.str.lower()</p></li>
				<li>Clean the tweets using the <strong class="inline">clean_str</strong> function: <p class="snippet">import re</p><p class="snippet">def clean_str(string):</p><p class="snippet">    string = re.sub(r"https?\://\S+", '', string)</p><p class="snippet">    string = re.sub(r"@\w*\s", '', string)</p><p class="snippet">    string = re.sub(r'\&lt;a href', ' ', string)</p><p class="snippet">    string = re.sub(r'&amp;amp;', '', string) </p><p class="snippet">    string = re.sub(r'&lt;br /&gt;', ' ', string)</p><p class="snippet">    string = re.sub(r'[_"\-;%()|+&amp;=*%.,!?:#$@\[\]/]', ' ', string)</p><p class="snippet">    string = re.sub('\d','', string)</p><p class="snippet">    return string</p><p class="snippet"> </p><p class="snippet">data.text = data.text.apply(lambda x: clean_str(str(x)))</p></li>
				<li>Remove all the stop words from the tweets, as was done in the <strong class="inline">Text Preprocessing</strong> section:<p class="snippet">from nltk.corpus import stopwords</p><p class="snippet">from nltk.tokenize import word_tokenize,sent_tokenize</p><p class="snippet">stop_words = stopwords.words('english')</p><p class="snippet">stop_words = set(stop_words)</p><p class="snippet">remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]</p><p class="snippet">data['text'] = data['text'].apply(remove_stop_words)</p><p class="snippet"> </p><p class="snippet">def combine_text(text):    </p><p class="snippet">    try:</p><p class="snippet">        return ' '.join(text[0])</p><p class="snippet">    except:</p><p class="snippet">        return np.nan</p><p class="snippet"> </p><p class="snippet">data.text = data.text.apply(lambda x: combine_text(x))</p><p class="snippet"> </p><p class="snippet">data = data.dropna(how='any')</p></li>
				<li>Tokenize the tweets and convert them to numbers using the Keras Tokenizer:<p class="snippet">from keras.preprocessing.text import Tokenizer</p><p class="snippet">tokenizer = Tokenizer(num_words=5000)</p><p class="snippet">tokenizer.fit_on_texts(list(data['text']))</p><p class="snippet">sequences = tokenizer.texts_to_sequences(data['text'])</p><p class="snippet">word_index = tokenizer.word_index</p></li>
				<li>Finally, pad the tweets to have a maximum of 50 words. This will remove any words after the 50-word limit and add 0s if the number of words is less than 50:<p class="snippet">from keras.preprocessing.sequence import pad_sequences</p><p class="snippet">tweets = pad_sequences(sequences, maxlen=50)</p></li>
				<li>Create the embedding matrix from the GloVe embedding file that we downloaded using the <strong class="inline">load_embedding</strong> function:<p class="snippet">import numpy as np</p><p class="snippet">def load_embedding(filename, word_index , num_words, embedding_dim):</p><p class="snippet">    embeddings_index = {}</p><p class="snippet">    file = open(filename, encoding="utf-8")</p><p class="snippet">    for line in file:</p><p class="snippet">        values = line.split()</p><p class="snippet">        word = values[0]</p><p class="snippet">        coef = np.asarray(values[1:])</p><p class="snippet">        embeddings_index[word] = coef</p><p class="snippet">    file.close()</p><p class="snippet">    </p><p class="snippet">    embedding_matrix = np.zeros((num_words, embedding_dim))</p><p class="snippet">    for word, pos in word_index.items():</p><p class="snippet">        if pos &gt;= num_words:</p><p class="snippet">            continue</p><p class="snippet">        embedding_vector = embeddings_index.get(word)</p><p class="snippet">        if embedding_vector is not None:</p><p class="snippet">            embedding_matrix[pos] = embedding_vector</p><p class="snippet">    return embedding_matrix</p><p class="snippet"> </p><p class="snippet">embedding_matrix = load_embedding('../../embedding/glove.twitter.27B.50d.txt', word_index, len(word_index), 50)</p></li>
				<li>Split the dataset into training and testing sets with an 80:20 spilt. You can experiment with different splits: <p class="snippet">from sklearn.model_selection import train_test_split  </p><p class="snippet">X_train, X_test, y_train, y_test = train_test_split(tweets, pd.get_dummies(data.sentiment), test_size=0.2, random_state=9)</p></li>
				<li>Create the LSTM model that will predict the sentiment. You can modify this to create your own neural network:<p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.layers import Dense, Dropout, BatchNormalization, Embedding, Flatten, LSTM</p><p class="snippet">embedding_layer = Embedding(len(word_index),</p><p class="snippet">                           50,</p><p class="snippet">                           weights=[embedding_matrix],</p><p class="snippet">                           input_length=50,</p><p class="snippet">                            trainable=False)</p><p class="snippet">model = Sequential()</p><p class="snippet">model.add(embedding_layer)</p><p class="snippet">model.add(Dropout(0.5))</p><p class="snippet">model.add(LSTM(100, dropout=0.2))</p><p class="snippet">model.add(Dense(2, activation='softmax'))</p><p class="snippet"> </p><p class="snippet">model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = ['acc'])</p></li>
				<li>Train the model. Here, we train it only for 10 epochs. You can increase the number of epochs to try and get a better accuracy:<p class="snippet">model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=256)</p></li>
				<li>Check how well the model is performing by predicting the sentiment of a few tweets in the test set:<p class="snippet">preds = model.predict(X_test)</p><p class="snippet">review_num = 1</p><p class="snippet">print("Tweet: \n"+tokenizer.sequences_to_texts([X_test[review_num]])[0])</p><p class="snippet">sentiment = "Positive" if np.argmax(preds[review_num]) else "Negative"</p><p class="snippet">print("\nPredicted sentiment = "+ sentiment)</p><p class="snippet">sentiment = "Positive" if np.argmax(y_test.values[review_num]) else "Negative"</p><p class="snippet">print("\nActual sentiment = "+ sentiment)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer370" class="IMG---Figure">
					<img src="Images/C13322_07_42.jpg" alt="Figure 7.42: Positive (left) and negative (right) tweets and their predictions&#13;&#10;" width="570" height="112"/>
				</div>
			</div>
			<h6>Figure 7.42: Positive (left) and negative (right) tweets and their predictions</h6>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor254"/>Chapter 8: Tips and Tricks of the Trade</h2>
			<h3 id="_idParaDest-234"><a id="_idTextAnchor255"/>Activity 21: Classifying Images using InceptionV3</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Create functions to get images and labels. Here <strong class="inline">PATH</strong> variable contains the path to the training dataset.<p class="snippet">from PIL import Image</p><p class="snippet">def get_input(file):</p><p class="snippet">    return Image.open(PATH+file)</p><p class="snippet">def get_output(file):</p><p class="snippet">    class_label = file.split('.')[0]</p><p class="snippet">    if class_label == 'dog': label_vector = [1,0]</p><p class="snippet">    elif class_label == 'cat': label_vector = [0,1]</p><p class="snippet">    return label_vector</p></li>
				<li>Set <strong class="inline">SIZE</strong> and <strong class="inline">CHANNELS. SIZE</strong> is the dimension of the square image input. <strong class="inline">CHANNELS</strong> is the number of channels in the training data images. There are 3 channels in a RGB image.<p class="snippet">SIZE = 200</p><p class="snippet">CHANNELS = 3</p></li>
				<li>Create a function to preprocess and augment images:<p class="snippet">def preprocess_input(image):</p><p class="snippet">    </p><p class="snippet">    # Data preprocessing</p><p class="snippet">    image = image.resize((SIZE,SIZE))</p><p class="snippet">    image = np.array(image).reshape(SIZE,SIZE,CHANNELS)</p><p class="snippet">    </p><p class="snippet">    # Normalize image </p><p class="snippet">    image = image/255.0</p><p class="snippet">    </p><p class="snippet">    return image</p></li>
				<li>Finally, develop the generator that will generate the batches:<p class="snippet">import numpy as np</p><p class="snippet">def custom_image_generator(images, batch_size = 128):</p><p class="snippet">    </p><p class="snippet">    while True:</p><p class="snippet">        # Randomly select images for the batch</p><p class="snippet">        batch_images = np.random.choice(images, size = batch_size)</p><p class="snippet">        batch_input = []</p><p class="snippet">        batch_output = [] </p><p class="snippet">        </p><p class="snippet">        # Read image, perform preprocessing and get labels</p><p class="snippet">        for file in batch_images:</p><p class="snippet">            # Function that reads and returns the image</p><p class="snippet">            input_image = get_input(file)</p><p class="snippet">            # Function that gets the label of the image</p><p class="snippet">            label = get_output(file)</p><p class="snippet">            # Function that pre-processes and augments the image</p><p class="snippet">            image = preprocess_input(input_image)</p><p class="snippet"> </p><p class="snippet">            batch_input.append(image)</p><p class="snippet">            batch_output.append(label)</p><p class="snippet"> </p><p class="snippet">        batch_x = np.array(batch_input)</p><p class="snippet">        batch_y = np.array(batch_output)</p><p class="snippet"> </p><p class="snippet">        # Return a tuple of (images,labels) to feed the network</p><p class="snippet">        yield(batch_x, batch_y)</p></li>
				<li>Next, we will read the validation data. Create a function to read the images and their labels:<p class="snippet">from tqdm import tqdm</p><p class="snippet">def get_data(files):</p><p class="snippet">    data_image = []</p><p class="snippet">    labels = []</p><p class="snippet">    for image in tqdm(files):</p><p class="snippet">        label_vector = get_output(image)</p><p class="snippet">        </p><p class="snippet">        img = Image.open(PATH + image)</p><p class="snippet">        img = img.resize((SIZE,SIZE))</p><p class="snippet">        </p><p class="snippet">        labels.append(label_vector)</p><p class="snippet">        img = np.asarray(img).reshape(SIZE,SIZE,CHANNELS)</p><p class="snippet">        img = img/255.0</p><p class="snippet">        data_image.append(img)</p><p class="snippet">        </p><p class="snippet">    data_x = np.array(data_image)</p><p class="snippet">    data_y = np.array(labels)</p><p class="snippet">        </p><p class="snippet">    return (data_x, data_y)</p></li>
				<li>Read the validation files:<p class="snippet">import os</p><p class="snippet">files = os.listdir(PATH)</p><p class="snippet">random.shuffle(files)</p><p class="snippet">train = files[:7000]</p><p class="snippet">test = files[7000:]</p><p class="snippet">validation_data = get_data(test)</p><p class="snippet">7.    Plot a few images from the dataset to see whether you loaded the files correctly:</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">plt.figure(figsize=(20,10))</p><p class="snippet">columns = 5</p><p class="snippet">for i in range(columns):</p><p class="snippet">    plt.subplot(5 / columns + 1, columns, i + 1)</p><p class="snippet">    plt.imshow(validation_data[0][i])</p><p>A random sample of the images is shown here:</p><div id="_idContainer371" class="IMG---Figure"><img src="Images/C13322_08_16.jpg" alt="Figure 8.16: Sample images from the loaded dataset&#13;&#10;" width="1301" height="262"/></div><h6>Figure 8.16: Sample images from the loaded dataset</h6></li>
				<li>Load the Inception model and pass the shape of the input images:<p class="snippet">from keras.applications.inception_v3 import InceptionV3</p><p class="snippet">base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(SIZE,SIZE,CHANNELS))</p></li>
				<li>Add the output dense layer according to our problem:<p class="snippet">from keras.layers import GlobalAveragePooling2D, Dense, Dropout</p><p class="snippet">from keras.models import Model</p><p class="snippet">x = base_model.output</p><p class="snippet">x = GlobalAveragePooling2D()(x)</p><p class="snippet">x = Dense(256, activation='relu')(x)</p><p class="snippet">x = Dropout(0.5)(x)</p><p class="snippet">predictions = Dense(2, activation='softmax')(x)</p><p class="snippet"> </p><p class="snippet">model = Model(inputs=base_model.input, outputs=predictions)</p></li>
				<li>Next, compile the model to make it ready for training:<p class="snippet">model.compile(loss='categorical_crossentropy', </p><p class="snippet">              optimizer='adam',</p><p class="snippet">              metrics = ['accuracy'])</p><p class="snippet">And then perform the training of the model:</p><p class="snippet">EPOCHS = 50</p><p class="snippet">BATCH_SIZE = 128</p><p class="snippet"> </p><p class="snippet">model_details = model.fit_generator(custom_image_generator(train, batch_size = BATCH_SIZE),</p><p class="snippet">                    steps_per_epoch = len(train) // BATCH_SIZE, </p><p class="snippet">                    epochs = EPOCHS, </p><p class="snippet">                    validation_data= validation_data,</p><p class="snippet">                    verbose=1)</p></li>
				<li>Evaluate the model and get the accuracy:<p class="snippet">score = model.evaluate(validation_data[0], validation_data[1])</p><p class="snippet">print("Accuracy: {0:.2f}%".format(score[1]*100))</p><p>The accuracy is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer372" class="IMG---Figure">
					<img src="Images/C13322_08_17.jpg" alt="Figure 8.17: Model accuracy&#13;&#10;" width="539" height="19"/>
				</div>
			</div>
			<h6>Figure 8.17: Model accuracy</h6>
			<h3 id="_idParaDest-235"><a id="_idTextAnchor256"/>Activity 22: Using Transfer Learning to Predict Images</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">First, set the random number seed so that the results are reproducible:<p class="snippet">from numpy.random import seed</p><p class="snippet">seed(1)</p><p class="snippet">from tensorflow import set_random_seed</p><p class="snippet">set_random_seed(1)</p></li>
				<li>Set <strong class="inline">SIZE</strong> and <strong class="inline">CHANNELS</strong><p><strong class="inline">SIZE</strong> is the dimension of the square image input. <strong class="inline">CHANNELS</strong> is the number of channels in the training data images. There are 3 channels in a RGB image.</p><p class="snippet">SIZE = 200</p><p class="snippet">CHANNELS = 3</p></li>
				<li>Create functions to get images and labels. Here <strong class="inline">PATH</strong> variable contains the path to the training dataset.<p class="snippet">from PIL import Image</p><p class="snippet">def get_input(file):</p><p class="snippet">    return Image.open(PATH+file)</p><p class="snippet">def get_output(file):</p><p class="snippet">    class_label = file.split('.')[0]</p><p class="snippet">    if class_label == 'dog': label_vector = [1,0]</p><p class="snippet">    elif class_label == 'cat': label_vector = [0,1]</p><p class="snippet">    return label_vector</p></li>
				<li>Create a function to preprocess and augment images:<p class="snippet">def preprocess_input(image):</p><p class="snippet">    </p><p class="snippet">    # Data preprocessing</p><p class="snippet">    image = image.resize((SIZE,SIZE))</p><p class="snippet">    image = np.array(image).reshape(SIZE,SIZE,CHANNELS)</p><p class="snippet">    </p><p class="snippet">    # Normalize image </p><p class="snippet">    image = image/255.0</p><p class="snippet">    </p><p class="snippet">    return image</p></li>
				<li>Finally, create the generator that will generate the batches:<p class="snippet">import numpy as np</p><p class="snippet">def custom_image_generator(images, batch_size = 128):</p><p class="snippet">    </p><p class="snippet">    while True:</p><p class="snippet">        # Randomly select images for the batch</p><p class="snippet">        batch_images = np.random.choice(images, size = batch_size)</p><p class="snippet">        batch_input = []</p><p class="snippet">        batch_output = [] </p><p class="snippet">        </p><p class="snippet">        # Read image, perform preprocessing and get labels</p><p class="snippet">        for file in batch_images:</p><p class="snippet">            # Function that reads and returns the image</p><p class="snippet">            input_image = get_input(file)</p><p class="snippet">            # Function that gets the label of the image</p><p class="snippet">            label = get_output(file)</p><p class="snippet">            # Function that pre-processes and augments the image</p><p class="snippet">            image = preprocess_input(input_image)</p><p class="snippet"> </p><p class="snippet">            batch_input.append(image)</p><p class="snippet">            batch_output.append(label)</p><p class="snippet"> </p><p class="snippet">        batch_x = np.array(batch_input)</p><p class="snippet">        batch_y = np.array(batch_output)</p><p class="snippet"> </p><p class="snippet">        # Return a tuple of (images,labels) to feed the network</p><p class="snippet">        yield(batch_x, batch_y)</p></li>
				<li>Next, we will read the development and test data. Create a function to read the images and their labels:<p class="snippet">from tqdm import tqdm</p><p class="snippet">def get_data(files):</p><p class="snippet">    data_image = []</p><p class="snippet">    labels = []</p><p class="snippet">    for image in tqdm(files):</p><p class="snippet">        </p><p class="snippet">        label_vector = get_output(image)</p><p class="snippet">        </p><p class="snippet"> </p><p class="snippet">        img = Image.open(PATH + image)</p><p class="snippet">        img = img.resize((SIZE,SIZE))</p><p class="snippet">        </p><p class="snippet">       </p><p class="snippet">        labels.append(label_vector)</p><p class="snippet">        img = np.asarray(img).reshape(SIZE,SIZE,CHANNELS)</p><p class="snippet">        img = img/255.0</p><p class="snippet">        data_image.append(img)</p><p class="snippet">        </p><p class="snippet">    data_x = np.array(data_image)</p><p class="snippet">    data_y = np.array(labels)</p><p class="snippet">        </p><p class="snippet">    return (data_x, data_y)</p></li>
				<li>Now read the development and test files. The split for the train/dev/test set is <strong class="inline">70%/15%/15%</strong>.<p class="snippet">import random</p><p class="snippet">random.shuffle(files)</p><p class="snippet">train = files[:7000]</p><p class="snippet">development = files[7000:8500]</p><p class="snippet">test = files[8500:]</p><p class="snippet">development_data = get_data(development)</p><p class="snippet">test_data = get_data(test)</p></li>
				<li>Plot a few images from the dataset to see whether you loaded the files correctly:<p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">plt.figure(figsize=(20,10))</p><p class="snippet">columns = 5</p><p class="snippet">for i in range(columns):</p><p class="snippet">    plt.subplot(5 / columns + 1, columns, i + 1)</p><p class="snippet">    plt.imshow(validation_data[0][i])</p><p>Check the output in the following screenshot:</p><div id="_idContainer373" class="IMG---Figure"><img src="Images/C13322_08_18.jpg" alt="Figure 8.18: Sample images from the loaded dataset&#13;&#10;" width="1235" height="244"/></div><h6>Figure 8.18: Sample images from the loaded dataset</h6></li>
				<li>Load the Inception model and pass the shape of the input images:<p class="snippet">from keras.applications.inception_v3 import InceptionV3</p><p class="snippet">base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(200,200,3))</p><p class="snippet">10.  Add the output dense layer according to our problem:</p><p class="snippet">from keras.models import Model</p><p class="snippet">from keras.layers import GlobalAveragePooling2D, Dense, Dropout</p><p class="snippet">x = base_model.output</p><p class="snippet">x = GlobalAveragePooling2D()(x)</p><p class="snippet">x = Dense(256, activation='relu')(x)</p><p class="snippet">keep_prob = 0.5</p><p class="snippet">x = Dropout(rate = 1 - keep_prob)(x)</p><p class="snippet">predictions = Dense(2, activation='softmax')(x)</p><p class="snippet"> </p><p class="snippet">model = Model(inputs=base_model.input, outputs=predictions)</p></li>
				<li>This time around, we will freeze the first five layers of the model to help with the training time:<p class="snippet">for layer in base_model.layers[:5]:</p><p class="snippet">    layer.trainable = False</p></li>
				<li>Compile the model to make it ready for training:<p class="snippet">model.compile(loss='categorical_crossentropy', </p><p class="snippet">              optimizer='adam',</p><p class="snippet">              metrics = ['accuracy'])</p></li>
				<li>Create callbacks for Keras:<p class="snippet">from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard</p><p class="snippet">callbacks = [</p><p class="snippet">    TensorBoard(log_dir='./logs',</p><p class="snippet">                update_freq='epoch'),</p><p class="snippet">    EarlyStopping(monitor = "val_loss",</p><p class="snippet">                 patience = 18,</p><p class="snippet">                 verbose = 1,</p><p class="snippet">                 min_delta = 0.001,</p><p class="snippet">                 mode = "min"),</p><p class="snippet">    ReduceLROnPlateau(monitor = "val_loss",</p><p class="snippet">                     factor = 0.2,</p><p class="snippet">                     patience = 8,</p><p class="snippet">                     verbose = 1,</p><p class="snippet">                     mode = "min"),</p><p class="snippet">    ModelCheckpoint(monitor = "val_loss",</p><p class="snippet">                   filepath = "Dogs-vs-Cats-InceptionV3-{epoch:02d}-{val_loss:.2f}.hdf5", </p><p class="snippet">                   save_best_only=True,</p><p class="snippet">                   period = 1)]</p><h4>Note</h4><p class="callout">Here, we are making use of four callbacks: <strong class="inline">TensorBoard</strong>, <strong class="inline">EarlyStopping</strong>, <strong class="inline">ReduceLROnPlateau</strong>, and <strong class="inline">ModelCheckpoint</strong>.</p><p>Perform training on the model. Here we train our model for 50 epochs only and with a batch size of 128:</p><p class="snippet">EPOCHS = 50</p><p class="snippet">BATCH_SIZE = 128</p><p class="snippet">model_details = model.fit_generator(custom_image_generator(train, batch_size = BATCH_SIZE),</p><p class="snippet">                   steps_per_epoch = len(train) // BATCH_SIZE, </p><p class="snippet">                   epochs = EPOCHS, </p><p class="snippet">                   callbacks = callbacks,</p><p class="snippet">                   validation_data= development_data,</p><p class="snippet">                   verbose=1)</p><p>The training logs on TensorBoard are shown here:</p><div id="_idContainer374" class="IMG---Figure"><img src="Images/C13322_08_19.jpg" alt="Figure 8.19: Training set logs from TensorBoard&#13;&#10;" width="652" height="593"/></div><h6>Figure 8.19: Training set logs from TensorBoard</h6></li>
				<li>You can now fine-tune the hyperparameters taking accuracy of the development set as the metric.<p>The logs of the development set from the TensorBoard tool are shown here:</p><div id="_idContainer375" class="IMG---Figure"><img src="Images/C13322_08_20.jpg" alt="Figure 8.20: Validation set logs from TensorBoard" width="610" height="596"/></div><h6>Figure 8.20: Validation set logs from TensorBoard</h6><p>The learning rate decrease can be observed from the following plot:</p><div id="_idContainer376" class="IMG---Figure"><img src="Images/C13322_08_21.jpg" alt="Figure 8.21: Learning rate log from TensorBoard&#13;&#10;" width="602" height="257"/></div><h6>Figure 8.21: Learning rate log from TensorBoard</h6></li>
				<li>Evaluate the model on the test set and get the accuracy:<p class="snippet">score = model.evaluate(test_data[0], test_data[1])</p><p class="snippet">print("Accuracy: {0:.2f}%".format(score[1]*100))</p><p>To understand fully, refer to the following output screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer377" class="IMG---Figure">
					<img src="Images/C13322_08_22.jpg" alt="Figure 8.22: The final accuracy of the model on the test set&#13;&#10;" width="635" height="30"/>
				</div>
			</div>
			<h6>Figure 8.22: The final accuracy of the model on the test set</h6>
			<p>As you can see, the model gets an accuracy of 93.6% on the test set, which is different from the accuracy of the development set (93.3% from the TensorBoard training logs). The early stopping callback stopped training when there wasn't a significant improvement in the loss of the development set; this helped us save some time. The learning rate was reduced after nine epochs, which helped training, as can be seen here:</p>
			<div>
				<div id="_idContainer378" class="IMG---Figure">
					<img src="Images/C13322_08_23.jpg" alt="Figure 8.23: A snippet of the training logs of the model" width="1244" height="171"/>
				</div>
			</div>
			<h6>Figure 8.23: A snippet of the training logs of the model</h6>
		</div>
	</div>
<div id="sbo-rt-content"><nav id="toc" epub:type="toc">
		<h2>Contents</h2>
			<ol>
				<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-1">Preface</a>
					<ol>
						<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-2">About the Book</a>
							<ol>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-3">About the Authors </a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-4">Learning Objectives</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-5">Audience</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-6">Approach</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-7">Minimum Hardware Requirements</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-8">Software Requirements </a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-9">Installation and Setup</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-10">Using Kaggle for Faster Experimentation</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-11">Conventions</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-12">Installing the Code Bundle</a></li>
							</ol>
						</li>
					</ol>
				</li>
				<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-13">Chapter 1</a></li>
				<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-14">Introduction to Data Science and Data Pre-Processing</a>
					<ol>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-15">Introduction</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-16">Python Libraries</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-17">Roadmap for Building Machine Learning Models</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-18">Data Representation</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-19">Independent and Target Variables</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-20">Exercise 1: Loading a Sample Dataset and Creating the Feature Matrix and Target Matrix</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-21">Data Cleaning </a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-22">Exercise 2: Removing Missing Data</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-23">Exercise 3: Imputing Missing Data</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-24">Exercise 4: Finding and Removing Outliers in Data</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-25">Data Integration</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-26">Exercise 5: Integrating Data</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-27">Data Transformation</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-28">Handling Categorical Data</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-29">Exercise 6: Simple Replacement of Categorical Data with a Number</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-30">Exercise 7: Converting Categorical Data to Numerical Data Using Label Encoding</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-31">Exercise 8: Converting Categorical Data to Numerical Data Using One-Hot Encoding</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-32">Data in Different Scales</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-33">Exercise 9: Implementing Scaling Using the Standard Scaler Method</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-34">Exercise 10: Implementing Scaling Using the MinMax Scaler Method</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-35">Data Discretization</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-36">Exercise 11: Discretization of Continuous Data </a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-37">Train and Test Data</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-38">Exercise 12: Splitting Data into Train and Test Sets</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-39">Activity 1: Pre-Processing Using the Bank Marketing Subscription Dataset</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-40">Supervised Learning</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-41">Unsupervised Learning </a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-42">Reinforcement Learning</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-43">Performance Metrics</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-44">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-45">Chapter 2</a></li>
				<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-46">Data Visualization</a>
					<ol>
						<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-47">Introduction</a></li>
						<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-48">Functional Approach</a>
							<ol>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-49">Exercise 13: Functional Approach – Line Plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-50">Exercise 14: Functional Approach – Add a Second Line to the Line Plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-51">Activity 2: Line Plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-52">Exercise 15: Creating a Bar Plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-53">Activity 3: Bar Plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-54">Exercise 16: Functional Approach – Histogram</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-55">Exercise 17: Functional Approach – Box-and-Whisker plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-56">Exercise 18: Scatterplot</a></li>
							</ol>
						</li>
						<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-57">Object-Oriented Approach Using Subplots</a>
							<ol>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-58">Exercise 19: Single Line Plot using Subplots</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-59">Exercise 20: Multiple Line Plots Using Subplots</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-60">Activity 4: Multiple Plot Types Using Subplots</a></li>
							</ol>
						</li>
						<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-61">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-62">Chapter 3</a></li>
				<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-63">Introduction to Machine Learning via Scikit-Learn</a>
					<ol>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-64">Introduction</a></li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-65">Introduction to Linear and Logistic Regression</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-66">Simple Linear Regression</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-67">Exercise 21: Preparing Data for a Linear Regression Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-68">Exercise 22: Fitting a Simple Linear Regression Model and Determining the Intercept and Coefficient</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-69">Exercise 23: Generating Predictions and Evaluating the Performance of a Simple Linear Regression Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-70">Multiple Linear Regression</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-71">Exercise 24: Fitting a Multiple Linear Regression Model and Determining the Intercept and Coefficients</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-72">Activity 5: Generating Predictions and Evaluating the Performance of a Multiple Linear Regression Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-73">Logistic Regression</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-74">Exercise 25: Fitting a Logistic Regression Model and Determining the Intercept and Coefficients</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-75">Exercise 26: Generating Predictions and Evaluating the Performance of a Logistic Regression Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-76">Exercise 27: Tuning the Hyperparameters of a Multiple Logistic Regression Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-77">Activity 6: Generating Predictions and Evaluating Performance of a Tuned Logistic Regression Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-78">Max Margin Classification Using SVMs</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-79">Exercise 28: Preparing Data for the Support Vector Classifier (SVC) Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-80">Exercise 29: Tuning the SVC Model Using Grid Search</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-81">Activity 7: Generating Predictions and Evaluating the Performance of the SVC Grid Search Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-82">Decision Trees</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-83">Activity 8: Preparing Data for a Decision Tree Classifier</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-84">Exercise 30: Tuning a Decision Tree Classifier Using Grid Search</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-85">Exercise 31: Programmatically Extracting Tuned Hyperparameters from a Decision Tree Classifier Grid Search Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-86">Activity 9: Generating Predictions and Evaluating the Performance of a Decision Tree Classifier Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-87">Random Forests</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-88">Exercise 32: Preparing Data for a Random Forest Regressor</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-89">Activity 10: Tuning a Random Forest Regressor</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-90">Exercise 33: Programmatically Extracting Tuned Hyperparameters and Determining Feature Importance from a Random Forest Regressor Grid Search Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-91">Activity 11: Generating Predictions and Evaluating the Performance of a Tuned Random Forest Regressor Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-92">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-93">Chapter 4</a></li>
				<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-94">Dimensionality Reduction and Unsupervised Learning</a>
					<ol>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-95">Introduction</a></li>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-96">Hierarchical Cluster Analysis (HCA)</a>
							<ol>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-97">Exercise 34: Building an HCA Model</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-98">Exercise 35: Plotting an HCA Model and Assigning Predictions</a></li>
							</ol>
						</li>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-99">K-means Clustering</a>
							<ol>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-100">Exercise 36: Fitting k-means Model and Assigning Predictions</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-101">Activity 12: Ensemble k-means Clustering and Calculating Predictions</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-102">Exercise 37: Calculating Mean Inertia by n_clusters</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-103">Exercise 38: Plotting Mean Inertia by n_clusters</a></li>
							</ol>
						</li>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-104">Principal Component Analysis (PCA)</a>
							<ol>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-105">Exercise 39: Fitting a PCA Model</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-106">Exercise 40: Choosing n_components using Threshold of Explained Variance</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-107">Activity 13: Evaluating Mean Inertia by Cluster after PCA Transformation</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-108">Exercise 41: Visual Comparison of Inertia by n_clusters</a></li>
							</ol>
						</li>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-109">Supervised Data Compression using Linear Discriminant Analysis (LDA)</a>
							<ol>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-110">Exercise 42: Fitting LDA Model</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-111">Exercise 43: Using LDA Transformed Components in Classification Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-112">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-113">Chapter 5</a></li>
				<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-114">Mastering Structured Data</a>
					<ol>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-115">Introduction</a></li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-116">Boosting Algorithms</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-117">Gradient Boosting Machine (GBM)</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-118">XGBoost (Extreme Gradient Boosting)</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-119">Exercise 44: Using the XGBoost library to Perform Classification</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-120">XGBoost Library</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-121">Controlling Model Overfitting</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-122">Handling Imbalanced Datasets</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-123">Activity 14: Training and Predicting the Income of a Person</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-124">External Memory Usage</a></li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-125">Cross-validation</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-126">Exercise 45: Using Cross-validation to Find the Best Hyperparameters</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-127">Saving and Loading a Model</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-128">Exercise 46: Creating a Python Pcript that Predicts Based on Real-time Input</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-129">Activity 15: Predicting the Loss of Customers</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-130">Neural Networks</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-131">What Is a Neural Network?</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-132">Optimization Algorithms</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-133">Hyperparameters</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-134">Keras</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-135">Exercise 47: Installing the Keras library for Python and Using it to Perform Classification</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-136">Keras Library</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-137">Exercise 48: Predicting Avocado Price Using Neural Networks</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-138">Categorical Variables</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-139">One-hot Encoding</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-140">Entity Embedding</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-141">Exercise 49: Predicting Avocado Price Using Entity Embedding</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-142">Activity 16: Predicting a Customer's Purchase Amount</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-143">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-144">Chapter 6</a></li>
				<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-145">Decoding Images</a>
					<ol>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-146">Introduction</a></li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-147">Images</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-148">Exercise 50: Classify MNIST Using a Fully Connected Neural Network</a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-149">Convolutional Neural Networks</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-150">Convolutional Layer</a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-151">Pooling Layer</a></li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-152">Adam Optimizer</a></li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-153">Cross-entropy Loss</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-154">Exercise 51: Classify MNIST Using a CNN</a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-155">Regularization</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-156">Dropout Layer</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-157">L1 and L2 Regularization</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-158">Batch Normalization</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-159">Exercise 52: Improving Image Classification Using Regularization Using CIFAR-10 images </a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-160">Image Data Preprocessing</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-161">Normalization</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-162">Converting to Grayscale</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-163">Getting All Images to the Same Size</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-164">Other Useful Image Operations</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-165">Activity 17: Predict if an Image Is of a Cat or a Dog</a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-166">Data Augmentation</a></li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-167">Generators</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-168">Exercise 53: Classify CIFAR-10 Images with Image Augmentation</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-169">Activity 18: Identifying and Augmenting an Image</a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-170">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-171">Chapter 7</a></li>
				<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-172">Processing Human Language</a>
					<ol>
						<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-173">Introduction</a></li>
						<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-174">Text Data Processing </a>
							<ol>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-175">Regular Expressions</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-176">Exercise 54: Using RegEx for String Cleaning </a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-177">Basic Feature Extraction</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-178">Text Preprocessing</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-179">Exercise 55: Preprocessing the IMDB Movie Review Dataset</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-180">Text Processing</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-181">Exercise 56: Creating Word Embeddings Using Gensim</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-182">Activity 19: Predicting Sentiments of Movie Reviews</a></li>
							</ol>
						</li>
						<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-183">Recurrent Neural Networks (RNNs)</a>
							<ol>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-184">LSTMs</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-185">Exercise 57: Performing Sentiment Analysis Using LSTM </a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-186">Activity 20: Predicting Sentiments from Tweets</a></li>
							</ol>
						</li>
						<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-187">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-188">Chapter 8</a></li>
				<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-189">Tips and Tricks of the Trade</a>
					<ol>
						<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-190">Introduction</a></li>
						<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-191">Transfer Learning</a>
							<ol>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-192">Transfer Learning for Image Data</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-193">Exercise 58: Using InceptionV3 to Compare and Classify Images</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-194">Activity 21: Classifying Images using InceptionV3</a></li>
							</ol>
						</li>
						<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-195">Useful Tools and Tips</a>
							<ol>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-196">Train, Development, and Test Datasets</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-197">Working with Unprocessed Datasets</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-198">pandas Profiling</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-199">TensorBoard</a></li>
							</ol>
						</li>
						<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-200">AutoML</a>
							<ol>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-201">Exercise 59: Get a Well-Performing Network Using Auto-Keras</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-202">Model Visualization Using Keras</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-203">Activity 22: Using Transfer Learning to Predict Images</a></li>
							</ol>
						</li>
						<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-204">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-205">Appendix</a>
					<ol>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-206">Chapter 1: Introduction to Data Science and Data Preprocessing</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-207">Activity 1: Pre-Processing Using the Bank Marketing Subscription Dataset</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-208">Chapter 2: Data Visualization</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-209">Activity 2: Line Plot</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-210">Activity 3: Bar Plot</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-211">Activity 4: Multiple Plot Types Using Subplots</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-212">Chapter 3: Introduction to Machine Learning via Scikit-Learn</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-213">Activity 5: Generating Predictions and Evaluating the Performance of a Multiple Linear Regression Model</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-214">Activity 6: Generating Predictions and Evaluating Performance of a Tuned Logistic Regression Model</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-215">Activity 7: Generating Predictions and Evaluating the Performance of the SVC Grid Search Model</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-216">Activity 8: Preparing Data for a Decision Tree Classifier</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-217">Activity 9: Generating Predictions and Evaluating the Performance of a Decision Tree Classifier Model</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-218">Activity 10: Tuning a Random Forest Regressor</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-219">Activity 11: Generating Predictions and Evaluating the Performance of a Tuned Random Forest Regressor Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-220">Chapter 4: Dimensionality Reduction and Unsupervised Learning</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-221">Activity 12: Ensemble k-means Clustering and Calculating Predictions</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-222">Activity 13: Evaluating Mean Inertia by Cluster after PCA Transformation</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-223">Chapter 5: Mastering Structured Data</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-224">Activity 14: Training and Predicting the Income of a Person</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-225">Activity 15: Predicting the Loss of Customers</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-226">Activity 16: Predicting a Customer's Purchase Amount</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-227">Chapter 6: Decoding Images</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-228">Activity 17: Predict if an Image Is of a Cat or a Dog</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-229">Activity 18: Identifying and Augmenting an Image</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-230">Chapter 7: Processing Human Language</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-231">Activity 19: Predicting Sentiments of Movie Reviews</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-232">Activity 20: Predicting Sentiments from Tweets</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-233">Chapter 8: Tips and Tricks of the Trade</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-234">Activity 21: Classifying Images using InceptionV3</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-235">Activity 22: Using Transfer Learning to Predict Images</a></li>
							</ol>
						</li>
					</ol>
				</li>
			</ol>
		</nav>
		<nav epub:type="landmarks">
		<h2>Landmarks</h2>
			<ol>
				<li><a epub:type="cover" href="Images/cover.xhtml">Cover</a></li>
				<li><a epub:type="toc" href="C13322_FM_Epub_Final_SW.xhtml#_idContainer004">Table of Contents</a></li>
			</ol>
		</nav>
	</div></body></html>