- en: '2'
  prefs: []
  type: TYPE_NORMAL
- en: Data Cleaning and Advanced Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Plan a machine learning classification strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess data to prepare it for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use validation curves to tune model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use dimensionality reduction to enhance model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will learn data preprocessing and machine learning by completing
    several practical exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a small food-delivery business that is looking to optimize their product.
    An analyst might look at the appropriate data and determine what type of food
    people are enjoying most. Perhaps they find a large amount of people are ordering
    the spiciest food options, indicating the business might be losing out on customers
    who desire something even more spicy. This is quite basic, or as some might say,
    "vanilla" analytics.
  prefs: []
  type: TYPE_NORMAL
- en: In a separate task, the analyst could employ predictive analytics by modeling
    the order volumes over time. With enough data, they could predict the future order
    volumes and therefore guide the restaurant as to how many staff are required each
    day. This model could take factors such as the weather into account to make the
    best predictions. For instance, a heavy rainstorm could be an indicator to staff
    more delivery personnel to make up for slow travel times. With historical weather
    data, that type of signal could be encoded into the model. This prediction model
    would save a business the time of having to consider these problems manually,
    and money by keeping customers happy and thereby increasing customer retention.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of data analytics in general is to uncover actionable insights that
    result in positive business outcomes. In the case of predictive analytics, the
    aim is to do this by determining the most likely future outcome of a target, based
    on previous trends and patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of predictive analytics are not restricted to big technology companies.
    Any business can find ways to benefit from machine learning, given the right data.
  prefs: []
  type: TYPE_NORMAL
- en: Companies all around the world are collecting massive amounts of data and using
    predictive analytics to cut costs and increase profits. Some of the most prevalent
    examples of this are from the technology giants Google, Facebook, and Amazon,
    who utilize big data on a huge scale. For example, Google and Facebook serve you
    personalized ads based on predictive algorithms that guess what you are most likely
    to click on. Similarly, Amazon recommends personalized products that you are most
    likely to buy, given your previous purchases.
  prefs: []
  type: TYPE_NORMAL
- en: Modern predictive analytics is done with machine learning, where computer models
    are trained to learn patterns from data. As we saw briefly in the previous chapter,
    software such as scikit-learn can be used with Jupyter Notebooks to efficiently
    build and test machine learning models. As we will continue to see, Jupyter Notebooks
    are an ideal environment for doing this type of work, as we can perform ad-hoc
    testing and analysis, and easily save the results for reference later.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will again take a hands-on approach by running through various
    examples and activities in a Jupyter Notebook. Where we saw a couple of examples
    of machine learning in the previous chapter, here we'll take a much slower and
    more thoughtful approach. Using an employee retention problem as our overarching
    example for the chapter, we will discuss how to approach predictive analytics,
    what things to consider when preparing the data for modeling, and how to implement
    and compare a variety of models using Jupyter Notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing to Train a Predictive Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we will cover the preparation required to train a predictive model. Although
    not as technically glamorous as training the models themselves, this step should
    not be taken lightly. It's very important to ensure you have a good plan before
    proceeding with the details of building and training a reliable model. Furthermore,
    once you've decided on the right plan, there are technical steps in preparing
    the data for modeling that should not be overlooked.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We must be careful not to go so deep into the weeds of technical tasks that
    we lose sight of the goal. Technical tasks include things that require programming
    skills, for example, constructing visualizations, querying databases, and validating
    predictive models. It's easy to spend hours trying to implement a specific feature
    or get the plots looking just right. Doing this sort of thing is certainly beneficial
    to our programming skills, but we should not forget to ask ourselves if it's really
    worth our time with respect to the current project.
  prefs: []
  type: TYPE_NORMAL
- en: Also, keep in mind that Jupyter Notebooks are particularly well-suited for this
    step, as we can use them to document our plan, for example, by writing rough notes
    about the data or a list of models we are interested in training. Before starting
    to train models, its good practice to even take this a step further and write
    out a well- structured plan to follow. Not only will this help you stay on track
    as you build and test the models, but it will allow others to understand what
    you're doing when they see your work.
  prefs: []
  type: TYPE_NORMAL
- en: After discussing the preparation, we will also cover another step in preparing
    to train the predictive model, which is cleaning the dataset. This is another
    thing that Jupyter Notebooks are well-suited for, as they offer an ideal testing
    ground for performing dataset transformations and keeping track of the exact changes.
    The data transformations required for cleaning raw data can quickly become intricate
    and convoluted; therefore, it's important to keep track of your work. As discussed
    in the first chapter, tools other than Jupyter Notebooks just don't offer very
    good options for doing this efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Before we progress to the next section, let's pause and think about these ideas
    in the context of a real-life example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following situation:'
  prefs: []
  type: TYPE_NORMAL
- en: You are hired by an online video game marketplace who want to increase the conversion
    rate of people visiting their site. They ask you to use predictive analytics to
    determine what genre of game the user will like, so they can display specialized
    content that will encourage the user to make a purchase. They want to do this
    without having to ask the customer their preference of game genre.
  prefs: []
  type: TYPE_NORMAL
- en: Is this a problem that can be solved? What type of data would be required? What
    would be the business implications?
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, we could consider making the prediction based on
    users' browsing cookies. For example, if they have a cookie from previously visiting
    a World of Warcraft website, this would act as an indicator that they like role
    playing games.
  prefs: []
  type: TYPE_NORMAL
- en: Another valuable piece of data would be a history of the games that user has
    previously bought in the marketplace. This could be the target variable in a machine
    learning algorithm, for example, a model that could predict which games the user
    would be interested in, based on the type of cookies in their browsing session.
    An alternate target variable could be constructed by setting up a survey in the
    marketplace to collect data on user preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the business implications, being able to accurately predict the
    genre of game is very important to the success of the campaign. In fact, getting
    the prediction wrong is doubly problematic: not only do we miss out on the opportunity
    to target users, but we may end up showing users content that would be negatively
    perceived. This could lead to more people leaving the site and fewer sales.'
  prefs: []
  type: TYPE_NORMAL
- en: Determining a Plan for Predictive Analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When formulating a plan for doing predictive modeling, one should start by considering
    stakeholder needs. A perfect model will be useless if it doesn't solve a relevant
    problem. Planning a strategy around business needs ensures that a successful model
    will lead to actionable insights.
  prefs: []
  type: TYPE_NORMAL
- en: Although it may be possible in principle to solve many business problems, the
    ability to deliver the solution will always depend on the availability of the
    necessary data. Therefore, it's important to consider the business needs in the
    context of the available data sources. When data is plentiful, this will have
    little effect, but as the amount of available data becomes smaller, so too does
    the scope of problems that can be solved.
  prefs: []
  type: TYPE_NORMAL
- en: 'These ideas can be formed into a standard process for determining a predictive
    analytics plan, which goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Look at the available data** to understand the range of realistically solvable
    business problems. At this stage, it might be too early to think about the exact
    problems that can be solved. Make sure you understand the data fields available
    and the timeframes they apply to.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Determine the business needs** by speaking with key stakeholders. Seek out
    a problem where the solution will lead to actionable business decisions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assess the data for suitability** by considering the availability of a sufficiently
    diverse and large feature space. Also, take into account the condition of the
    data: are there large chunks of missing values for certain variables or time ranges?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 2 and 3 should be repeated until a realistic plan has taken shape. At
    this point, you will already have a good idea of what the model input will be
    and what you might expect as output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you''ve identified a problem that can be solved with machine learning,
    along with the appropriate data sources, we should answer the following questions
    to lay a framework for the project. Doing this will help us determine which types
    of machine learning models we can use to solve the problem. The following image
    provides an overview of the choices available depending on the type of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: A flow chart for machine learning strategy based on the type
    of data](img/C13018_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: A flow chart for machine learning strategy based on the type of
    data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The above image describes the path you can choose depending of the type of
    data: labeled or unlabeled.'
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, either one can chose supervised or unsupervised learning. Supervised
    learning comprises either classification or regression problem. In regression,
    variables are continuous; for example, the amount of rainfall. In regression,
    the variables are discrete and we predict class labels. Simplest type of classification
    problem is binary; for example, will it rain today? (yes/no)
  prefs: []
  type: TYPE_NORMAL
- en: For unsupervised learning, cluster analysis is a commonly used method. Here,
    labels are assigned to the nearest cluster for each sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, not only the type but also the size and origin of data sources would
    be a factor while deciding on machine learning strategy. Specifically, following
    points should be note:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of data in terms of the width (no. of columns) and height (no. of rows)
    should be considered before apply a machine learning algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certain algorithms are better at handling certain features than the others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General, the larger the dataset, the better in terms of accuracy. However, this
    can be time consuming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One can reduce time by using dimensionality reduction techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For multiple data sources, one can consider merging them in a single table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If this cannot be done, we can train models for each and consider an ensemble
    average for final prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example where we may want to do this is with various sets of times series
    data on different scales. Consider we have the following data sources: a table
    with the AAPL stock closing prices on a daily time scale and iPhone sales data
    on a monthly time scale. We could merge the data by adding the monthly sales data
    to each sample in the daily time scale table, or grouping the daily data by month,
    but it might be better to build two models, one for each dataset, and use a combination
    of the results from each in the final prediction model.'
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing has a huge impact on machine learning. Like the saying "you
    are what you eat," the model's performance is a direct reflection of the data
    it's trained on. Many models depend on the data being transformed so that the
    continuous feature values have comparable limits. Similarly, categorical features
    should be encoded into numerical values. Although important, these steps are relatively
    simple and do not take very long.
  prefs: []
  type: TYPE_NORMAL
- en: 'The aspect of preprocessing that usually takes the longest is cleaning up messy
    data. Some estimates suggest that data scientists spend around two thirds of their
    work time cleaning and organizing datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: A pie chart distribution of the time spend on different data
    tasks](img/C13018_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: A pie chart distribution of the time spend on different data tasks'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To know more about the preprocessing stage, refer to: [https://www.forbes.com/sites/gilpress/2016/03/23/data-](http://www.forbes.com/sites/gilpress/2016/03/23/data-)
    preparation-most-time-consuming-least-enjoyable-data- science-task-survey-says/2/#17c66c7e1492.'
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to consider is the size of the datasets being used by many data
    scientists. As the dataset size increases, the prevalence of messy data increases
    as well, along with the difficulty in cleaning it.
  prefs: []
  type: TYPE_NORMAL
- en: Simply dropping the missing data is usually not the best option, because it's
    hard to justify throwing away samples where most of the fields have values. In
    doing so, we could lose valuable information that may hurt final model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this exercise, we practice preprocessing the data by creating two DataFrames,
    and performing an inner merge and outer merge on the DataFrames and remove the
    null (`NaN`) values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in data preprocessing can be grouped as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Merging data** sets on common fields to bring all data into a single table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering** to improve the quality of data, for example, the use
    of dimensionality reduction techniques to build new features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cleaning the data** by dealing with duplicate rows, incorrect or missing
    values, and other issues that arise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building the training data sets** by standardizing or normalizing the required
    data and splitting it into training and testing sets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's explore some of the tools and methods for doing the preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 8: Explore Data Preprocessing Tools and Methods'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Start the `NotebookApp` from the project directory by executing `jupyter notebook`.
    Navigate to the `Lesson-2` directory and open up the `lesson- 2-workbook.ipynb`
    file. Find the cell near the top where the packages are loaded, and run it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are going to start by showing off some basic tools from Pandas and sci-kit
    learn. Then, we'll take a deeper dive into methods for rebuilding missing data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scroll down to `Subtopic B: Preparing data for machine learning` and run the
    cell containing `pd.merge?` to display the docstring for the merge function in
    the notebook:![Figure 2.3: Docstring for the merge function](img/C13018_02_03.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.3: Docstring for the merge function'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see, the function accepts a left and right DataFrame to merge. You
    can specify one or more columns to group on as well as how they are grouped, that
    is, to use the left, right, outer, or inner sets of values. Let's see an example
    of this in use.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Exit the help popup and run the cell containing the following sample DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we will build two simple DataFrames from scratch. As can be seen, they
    contain a `product` column with some shared entries.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the next cell to perform the inner merge:![Figure 2.4: Inner merge of columns](img/C13018_02_04.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.4: Inner merge of columns'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note how only the shared items, **red shirt** and **white dress**, are included.
    To include all entries from both tables, we can do an outer merge instead. Let's
    do this now.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the next cell to perform an outer merge:![Figure 2.5: Outer merge of columns](img/C13018_02_05.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.5: Outer merge of columns'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This returns all of the data from each table where missing values have been
    labeled with `NaN`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.6: Code for using NumPy to test for quality](img/C13018_02_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.6: Code for using NumPy to test for quality'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: You may have noticed that our most recently merged table has duplicated data
    in the first few rows. This will be addressed in the next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell containing `df.drop_duplicates()` to return a version of the DataFrame
    with no duplicate rows:![Figure 2.7: Table with dropped duplicate rows](img/C13018_02_07.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.7: Table with dropped duplicate rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This is the easiest and "standard" way to drop duplicate rows. To apply these
    changes to `df`, we can either set `inplace=True` or do something like `df = df.drop_duplicated()`.
    Let's see another method, which uses masking to select or drop duplicate rows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell containing `df.duplicated()` to print the True/False series, marking
    duplicate rows:![Figure 2.8: Printing True/False values for duplicate rows](img/C13018_02_08.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.8: Printing True/False values for duplicate rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Sum the result to determine how many rows have been duplicated by running the
    following code:![Figure 2.9: Summing the result to check the number of duplicate
    rows](img/C13018_02_09.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.9: Summing the result to check the number of duplicate rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Run the following code and convince yourself the output is the same as that
    from `df.drop_duplicates()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.10: Output from the df.[~df.duplicated()] function](img/C13018_02_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.10: Output from the df.[~df.duplicated()] function'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Run the cell containing the following code to drop duplicates from a subset
    of the full DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.11: Output after dropping duplicates](img/C13018_02_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.11: Output after dropping duplicates'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here, we are doing the following things:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: creating a mask (a True/False series) for the product row, where duplicates
    are marked with `True`;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: using the tilde (~) to take the opposite of that mask, so that duplicates are
    instead marked with False and everything else is `True`;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: using that mask to filter out the `False` rows of `df`, which correspond to
    the duplicated products.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As expected, we now see that only the first `df` with a deduplicated version
    of itself. This can be done by running `drop_duplicates` and passing the parameter
    `inplace=True`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Deduplicate the DataFrame and save the result by running the cell containing
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Continuing on to other preprocessing methods, let's ignore the duplicated rows and
    first deal with the missing data. This is necessary because models cannot be trained
    on incomplete samples. Using the missing price data for blue pants and white tuxedo
    as an example, let's show some different options for handling `NaN` values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Drop rows, especially if your `NaN` samples are missing data, by running the
    cell containing `df.dropna()`:![Figure 2.12: Output after dropping incomplete
    rows](img/C13018_02_12.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.12: Output after dropping incomplete rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Drop entire columns that have most values missing for a feature. Do this by
    running the cell containing the same method as before, but this time with the
    axes parameter passed to indicate columns instead of rows:![Figure 2.13: Output
    after dropping entire columns with missing values for a feature](img/C13018_02_13.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.13: Output after dropping entire columns with missing values for a
    feature'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Simply dropping the `NaN` values is usually not the best option, because losing
    data is never good, especially if only a small fraction of the sample values is
    missing. Pandas offers a method for filling in `NaN` entries in a variety of different
    ways, some of which we'll illustrate now.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell containing `df.fillna?` to print the docstring for the Pandas
    `NaN-fill` method:![Figure 2.14: Docstring for the NaN-fill method](img/C13018_02_14.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.14: Docstring for the NaN-fill method'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note the options for the value parameter; this could be, for example, a single
    value or a dictionary/series type map based on index. Alternatively, we can leave
    the value as `None` and pass a `fill` method instead. We'll see examples of each
    in this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fill in the missing data with the average product price by running the cell
    containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.15: Output after filling missing data with average product price](img/C13018_02_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.15: Output after filling missing data with average product price'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fill in the missing data using the pad method by running the cell containing
    the following code instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.16: Output after filling data using the pad method](img/C13018_02_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.16: Output after filling data using the pad method'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how the **white dress** price was used to pad the missing values below
    it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To conclude this exercise, we will prepare our simple table to be used for training
    a machine learning algorithm. Don't worry, we won't actually try to train any
    models on such a small dataset! We start this process by encoding the class labels
    for the categorical data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the first cell in the `Building training data sets` section to add another
    column of data representing the average product ratings before encoding the labels:![Figure
    2.17: Output after adding the rating column](img/C13018_02_17.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.17: Output after adding the rating column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Considering we want to use this table to train a predictive model, we should
    first think about changing all the variables to numeric types.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Convert the handle `in_stock`., which is a Boolean list, to numeric values;
    for example, `0` and `1`. This should be done before using it to train a predictive
    model. This can be done in many ways, for example, by running the cell containing
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.18: Output after converting in_stock to binary](img/C13018_02_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.18: Output after converting in_stock to binary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Run the cell containing the following code to map class labels to integers
    at a higher level. We use sci-kit learn''s `LabelEncoder` for this purpose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.19: Output after mapping class labels to integers](img/C13018_02_19_.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.19: Output after mapping class labels to integers'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This might bring to mind the preprocessing we did in the previous chapter, when
    building the polynomial model. Here, we instantiate a label encoder and then "train"
    it and "transform" our data using the `fit_transform` method. We apply the result
    to a copy of our DataFrame, `_df`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Re-convert the features using the class we reference with the variable `rating_encoder`,
    by running `rating_encoder.inverse_ transform(df.rating)`:![Figure 2.20: Output
    after performing inverse transform](img/C13018_02_20.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 2.20: Output after performing inverse transform'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: You may notice a problem here. We are working with a so-called "ordinal" feature,
    where there's an inherent order to the labels. In this case, we should expect
    that a rating of "low" would be encoded with a 0 and a rating of "high" would
    be encoded with a 2\. However, this is not the result we see. In order to achieve
    proper ordinal label encoding, we should again use map, and build the dictionary
    ourselves.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Encode the ordinal labels properly by running the cell containing the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.21: Output after encoding ordinal labels](img/C13018_02_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.21: Output after encoding ordinal labels'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We first create the mapping dictionary. This is done using a dictionary comprehension
    and enumeration, but looking at the result, we see that it could just as easily
    be defined manually instead. Then, as done earlier for the `in_stock` column,
    we apply the dictionary mapping to the feature. Looking at the result, we see
    that rating now makes more sense than before, where `low` is labeled with `0`,
    `medium` with `1`, and `high` with `2`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that you've discussed ordinal features, let's touch on another type called
    nominal features. These are fields with no inherent order, and in our case, we
    see that `product` is a perfect example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Most scikit-learn models can be trained on data like this, where we have strings
    instead of integer-encoded labels. In this situation, the necessary conversions
    are done under the hood. However, this may not be the case for all models in scikit-learn,
    or other machine learning and deep learning libraries. Therefore, it's good practice
    to encode these ourselves during preprocessing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Convert the class labels from strings to numerical values by running the cell
    containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final DataFrame then looks as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.22: Final DataFrame](img/C13018_02_201.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.22: Final DataFrame'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here, we see the result of one-hot encoding: the `product` column has been
    split into 4, one for each unique value. Within each column, we find either a
    `1` or `0` representing whether that row contains the particular value or product.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Moving on and ignoring any data scaling (which should usually be done), the
    final step is to split the data into training and test sets to use for machine
    learning. This can be done using scikit-learn's `train_test_split`. Let's assume
    we are going to try to predict whether an item is in stock, given the other feature
    values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: When we call the values attribute in the preceding code, we are converting the
    Pandas series (that is, the DataFrame column) into a NumPy array. This is good
    practice because it strips out unnecessary information from the series object,
    such as the index and name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into training and test sets by running the cell containing the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.23: Splitting data intro training and test sets](img/C13018_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.23: Splitting data intro training and test sets'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we are selecting subsets of the data and feeding them into the `train_test_split`
    function. This function has four outputs, which are unpacked into the training
    and testing splits for features (`X`) and the target (`y`).
  prefs: []
  type: TYPE_NORMAL
- en: Observe the shape of the output data, where the test set has roughly 30% of
    the samples and the training set has roughly 70%.
  prefs: []
  type: TYPE_NORMAL
- en: We'll see similar code blocks later, when preparing real data to use for training
    predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the training exercise on cleaning data for use in machine learning
    applications. Let's take a minute to note how effective our Jupyter Notebook was
    for testing various methods of transforming the data, and ultimately documenting
    the pipeline we decided upon. This could easily be applied to an updated version
    of the data by altering only specific cells of code, prior to processing. Also,
    should we desire any changes to the processing, these can easily be tested in
    the notebook, and specific cells may be changed to accommodate the alterations.
    The best way to achieve this would probably be to copy the notebook over to a
    new file, so that we can always keep a copy of the original analysis for reference.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to an activity, we'll now apply the concepts from this section to
    a large dataset as we prepare it for use in training predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2: Preparing to Train a Predictive Model for the Employee-Retention
    Problem'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose you are hired to do freelance work for a company who wants to find insights
    into why their employees are leaving. They have compiled a set of data they think
    will be helpful in this respect. It includes details on employee satisfaction
    levels, evaluations, time spent at work, department, and salary.
  prefs: []
  type: TYPE_NORMAL
- en: The company shares their data with you by sending you a file called hr_data.csv
    and asking what you think can be done to help stop employees from leaving.
  prefs: []
  type: TYPE_NORMAL
- en: Our aim is to
  prefs: []
  type: TYPE_NORMAL
- en: 'apply the concepts you''ve learned thus far to a real-life problem. In particular,
    we seek to:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine a plan for using predictive analytics to provide impactful business
    insights, given the available data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the data for use in machine learning models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Starting with this activity and continuing through the remainder of this chapter,
    we''ll be using Human Resources Analytics dataset, which is a Kaggle dataset.
    The link to the dataset can be found here: [https://bit.ly/2OXWFUs](https://bit.ly/2OXWFUs).
    The data is simulated, meaning the samples are artificially generated and do not
    represent real people. We''ll ignore this fact as we analyze and model the data.
    There is a small difference between the dataset we use in this book and the online
    version. Our human resource analytics data contains some NaN values. These were
    manually removed from the online version of the dataset, for the purposes of illustrating
    data cleaning techniques. We have also added a column of data called is_smoker,
    for the same purposes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to achieve this, following steps have to be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: Scroll to the `Activity A` section of the `lesson-2-workbook.ipynb` notebook
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the head of the table to verify that it is in standard CSV format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the data with Pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect the columns by printing `df.columns` and make sure the data has loaded
    as expected by printing the DataFrame `head` and `tail` with `df.head()` and `df.tail()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the number of rows (including the header) in the CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare this result to `len(df)` to make sure we''ve loaded all the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assess the target variable and check the distribution and number of missing entries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the data type of each feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the feature distributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check how many `NaN` values are in each column by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop the `is_smoker` column as there is barely any information in this metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill the `NaN` values in the `time_spend_company` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a boxplot of `average_montly_hours` segmented by `number_project`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the mean of each group by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill the `NaN` values in `average_montly_hours`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Confirm that `df` has no more `NaN` values by running the assertion test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the string and Boolean fields into integer representations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print `df.columns` to show the fields
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save our preprocessed data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The detailed steps along with the solutions are presented in the *Appendix
    A*     (pg. no. 150).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Again, we pause here to note how well the Jupyter Notebook suited our needs
    when performing this initial data analysis and clean-up. Imagine, for example,
    we left this project in its current state for a few months. Upon returning to
    it, we would probably not remember what exactly was going on when we left it.
    Referring back to this notebook though, we would be able to retrace our steps
    and quickly recall what we previously learned about the data. Furthermore, we
    could update the data source with any new data and re-run the notebook to prepare
    the new set of data for use in our machine learning algorithms. Recall that in
    this situation, it would be best to make a copy of the notebook first, so as not
    to lose the initial analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, you''ve learned and applied methods for preparing to train a
    machine learning model. We started by discussing steps for identifying a problem
    that can be solved with predictive analytics. This consisted of:'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the available data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the business needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing the data for suitability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also discussed how to identify supervised versus unsupervised and regression
    versus classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: After identifying our problem, we learned techniques for using Jupyter Notebooks
    to build and test a data transformation pipeline. These techniques included methods
    and best practices for filling missing data, transforming categorical features,
    and building train/test data sets.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this chapter, we will use this preprocessed data to train
    a variety of classification models. To avoid blindly applying algorithms we don't
    understand, we start by introducing them and overviewing how they work. Then,
    we use Jupyter to train and compare their predictive capabilities. Here, we have
    the opportunity to discuss more advanced topics in machine learning like overfitting,
    k-fold cross-validation, and validation curves.
  prefs: []
  type: TYPE_NORMAL
- en: Training Classification Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you've already seen in the previous chapter, using libraries such as scikit-learn
    and platforms such as Jupyter, predictive models can be trained in just a few
    lines of code. This is possible by abstracting away the difficult computations
    involved with optimizing model parameters. In other words, we deal with a black
    box where the internal operations are hidden instead. With this simplicity also
    comes the danger of misusing algorithms, for example, by overfitting during training
    or failing to properly test on unseen data. We'll show how to avoid these pitfalls
    while training classification models and produce trustworthy results with the
    use of k-fold cross validation and validation curves.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Classification Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall the two types of supervised machine learning: regression and classification.
    In regression, we predict a continuous target variable. For example, recall the
    linear and polynomial models from the first chapter. In this chapter, we focus
    on the other type of supervised machine learning: classification. Here, the goal
    is to predict the class of a sample using the available metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest case, there are only two possible classes, which means we are
    doing binary classification. This is the case for the example problem in this
    chapter, where we try to predict whether an employee has left or not. If we have
    more than two class labels instead, we are doing multi-class classification.
  prefs: []
  type: TYPE_NORMAL
- en: Although there is little difference between binary and multi-class classification
    when training models with scikit-learn, what's done inside the "black box" is
    notably different. In particular, multi-class classification models often use
    the one-versus-rest method. This works as follows for a case with three class
    labels. When the model is "fit" with the data, three models are trained, and each
    model predicts whether the sample is part of an individual class or part of some
    other class. This might bring to mind the one-hot encoding for features that we
    did earlier. When a prediction is made for a sample, the class label with the
    highest confidence level is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll train three types of classification models: Support
    Vector Machines, Random Forests, and k-Nearest Neighbors classifiers. Each of
    these algorithms are quite different. As we will see, however, they are quite
    similar to train and use for predictions thanks to scikit-learn. Before swapping
    over to the Jupyter Notebook and implementing these, we''ll briefly see how they
    work.'
  prefs: []
  type: TYPE_NORMAL
- en: SVMs attempt to find the best hyperplane to divide classes by. This is done
    by maximizing the distance between the hyperplane and the closest samples of each
    class, which are called support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: This linear method can also be used to model nonlinear classes using the kernel
    trick. This method maps the features into a higher-dimensional space in which
    the hyperplane is determined. This hyperplane is also referred to as the decision
    surface, and we'll visualize it when training our models.
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbors classification algorithms memorize the training data and
    make predictions depending on the K nearest samples in the feature space. With
    three features, this can be visualized as a sphere surrounding the prediction
    sample. Often, however, we are dealing with more than three features and therefore
    hyperspheres are drawn to find the closest K samples.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests are an ensemble of decision trees, where each has been trained
    on different subsets of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree algorithm classifies a sample based on a series of decisions.
    For example, the first decision might be "if feature x_1 is less than or greater
    than 0." The data would then be split on this condition and fed into descending
    branches of the tree. Each step in the decision tree is decided based on the feature
    split that maximizes the information gain. Essentially, this term describes the
    mathematics that attempts to pick the best possible split of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Random Forest consists of creating bootstrapped (that is, randomly
    sampled data with replacement) datasets for a set of decision trees. Predictions
    are then made based on the majority vote. These have the benefit of less overfitting
    and better generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Decision trees can be used to model a mix of continuous and categorical data,
    which make them very useful. Furthermore, as we will see later in this chapter,
    the tree depth can be limited to reduce overfitting. For a detailed (but brief)
    look into the decision tree algorithm, check out this popular StackOverflow answer:
    [https://stackoverflow.com/a/1859910/3511819](https://stackoverflow.com/a/1859910/3511819).
    There, the author shows a simple example and discusses concepts such as node purity,
    information gain, and entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9: Training Two-Feature Classification Models With Scikit-learn'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll continue working on the employee retention problem that we introduced
    in the first topic. We previously prepared a dataset for training a classification
    model, in which we predicted whether an employee has left or not. Now, we''ll
    take that data and use it to train classification models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the `NotebookApp` and open the `lesson-2-workbook.ipynb` file. Scroll
    down to `Topic B: Training classification models`. Run the first couple of cells
    to set the default figure size and load the processed data that we previously
    saved to a CSV file. For this example, we''ll be training classification models
    on two continuous features: `satisfaction_level and last_evaluation`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Draw the bivariate and univariate graphs of the continuous target variables
    by running the cell with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.24: Bivariate and univariate graphs for satisfaction_level and last_evaluation](img/C13018_02_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.24: Bivariate and univariate graphs for satisfaction_level and last_evaluation'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see in the preceding image, there are some very distinct patterns
    in the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Re-plot the bivariate distribution, segmenting on the target variable, by running
    the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.25: Bivariate distribution for satisfaction_level and last_evaluation](img/C13018_02_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.25: Bivariate distribution for satisfaction_level and last_evaluation'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, we can see how the patterns are related to the target variable. For the
    remainder of this exercise, we'll try to exploit these patterns to train effective
    classification models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into training and test sets by running the cell containing the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our first two models, the Support Vector Machine and k-Nearest Neighbors algorithm,
    are most effective when the input data is scaled so that all of the features are
    on the same order. We'll accomplish this with scikit-learn's `StandardScaler`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load `StandardScaler` and create a new instance, as referenced by the scaler
    variable. Fit the scaler on the training set and transform it. Then, transform
    the test set. Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: An easy mistake to make when doing machine learning is to "fit" the scaler on
    the whole dataset, when in fact it should only be "fit" to the training data.
    For example, scaling the data before splitting into training and testing sets
    is a mistake. We don't want this because the model training should not be influenced
    in any way by the test data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import the scikit-learn support vector machine class and fit the model on the
    training data by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the accuracy of this model on unseen data by running the cell containing
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We predict the targets for our test samples and then use scikit-learn's `accuracy_score`
    function to determine the accuracy. The result looks promising at ~75%! Not bad
    for our first model. Recall, though, the target is imbalanced. Let's see how accurate
    the predictions are for each class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the confusion matrix and then determine the accuracy within each
    class by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It looks like the model is simply classifying every sample as 0, which is clearly not
    helpful at all. Let's use a contour plot to show the predicted class at each point
    in the feature space. This is commonly known as the decision- regions plot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the decision regions using a helpful function from the `mlxtend` library.
    Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.26: Plot of the decision regions](img/C13018_02_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.26: Plot of the decision regions'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The function plots decision regions along with a set of samples passed as arguments.
    In order to see the decision regions properly without too many samples obstructing
    our view, we pass only a 200-sample subset of the test data to the `plot_decision_regions`
    function. In this case, of course, it does not matter. We see the result is entirely
    red, indicating every point in the feature space would be classified as 0.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It shouldn't be surprising that a linear model can't do a good job of describing
    these nonlinear patterns. Recall earlier we mentioned the kernel trick for using
    SVMs to classify nonlinear problems. Let's see if doing this can improve the result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the docstring for scikit-learn''s SVM by running the cell containing
    SVC. Scroll down and check out the parameter descriptions. Notice the kernel option,
    which is actually enabled by default as `rbf`. Use this `kernel` option to train
    a new SVM by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.27: Training a new SVM ](img/C13018_02_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.27: Training a new SVM'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 2.28: Enhanced results with non-linear patterns](img/C13018_02_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.28: Enhanced results with non-linear patterns'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The result is much better. Now, we are able to capture some of the non-linear
    patterns in the data and correctly classify the majority of the employees who
    have left.
  prefs: []
  type: TYPE_NORMAL
- en: The plot_decision_regions Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `plot_decision_regions` function is provided by `mlxtend`, a Python library
    developed by Sebastian Raschka. It's worth taking a peek at the source code (which
    is of course written in Python) to understand how these plots are drawn. It's
    really not too complicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Jupyter Notebook, import the function with `from mlxtend.plotting import
    plot_decision_regions` and then pull up the help with `plot_decision_regions?`
    and scroll to the bottom to see the local file path:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.29: Local file path](img/C13018_02_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.29: Local file path'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Then, open up the file and read through it. For example, you could run `cat`
    in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.30: Running cat in the notebook](img/C13018_02_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.30: Running cat in the notebook'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is okay, but not ideal as there's no color markup for the code. It's better
    to copy it (so you don't accidentally alter the original) and open it with your
    favorite text editor.
  prefs: []
  type: TYPE_NORMAL
- en: When drawing attention to the code responsible for mapping the decision regions,
    we see a contour plot of predictions Z over an array `X_predict` that spans the
    feature space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.31: The screenshot of the code for mapping decision regions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_02_31.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.31: The screenshot of the code for mapping decision regions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's move to training our model on k-Nearest Neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10: Training K-nearest Neighbors for Our Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Load the scikit-learn KNN classification model and print the docstring by running
    the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `n_neighbors` parameter decides how many samples to use when making a classification.
    If the weights parameter is set to uniform, then class labels are decided by majority
    vote. Another useful choice for the weights is distance, where closer samples
    have a higher weight in the voting. Like most model parameters, the best choice
    for this depends on the particular dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train the KNN classifier with `n_neighbors=3`, and then compute the accuracy
    and decision regions. Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.32: Training the kNN classifier with n_negihbours=3](img/C13018_02_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.32: Training the kNN classifier with n_negihbours=3'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 2.33: Enhanced results after training](img/C13018_02_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.33: Enhanced results after training'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We see an increase in overall accuracy and a significant improvement for class
    1 in particular. However, the decision region plot would indicate we are overfitting
    the data. This is evident by the hard, "choppy" decision boundary, and small pockets
    of blue everywhere. We can soften the decision boundary and decrease overfitting
    by increasing the number of nearest neighbors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train a KNN model with `n_neighbors=25` by running the cell containing the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.34: Training the kNN classifier with n_negihbours=25](img/C13018_02_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.34: Training the kNN classifier with n_negihbours=25'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 2.35: Output after training with n_neighbours=25 ](img/C13018_02_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.35: Output after training with n_neighbours=25'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see, the decision boundaries are significantly less choppy, and there
    are far less pockets of blue. The accuracy for class 1 is slightly less, but we
    would need to use a more comprehensive method such as k-fold cross validation
    to decide if there's a significant difference between the two models.
  prefs: []
  type: TYPE_NORMAL
- en: Note that increasing `n_neighbors` has no effect on training time, as the model
    is simply memorizing the data. The prediction time, however, will be greatly affected.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When doing machine learning with real-world data, it's important for the algorithms
    to run quick enough to serve their purposes. For example, a script to predict
    tomorrow's weather that takes longer than a day to run is completely useless!
    Memory is also a consideration that should be taken into account when dealing
    with substantial amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: We will now train a Random Forest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11: Training a Random Forest'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Observe how similar it is to train and make predictions on each model, despite
    them each being so different internally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a Random Forest classification model composed of 50 decision trees, each
    with a max depth of 5\. Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.36: Training a Random Forest with a max depth of 5](img/C13018_02_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.36: Training a Random Forest with a max depth of 5'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 2.37: Output after training with a max depth of 5](img/C13018_02_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.37: Output after training with a max depth of 5'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note the distinctive axes-parallel decision boundaries produced by decision
    tree machine learning algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can access any of the individual decision trees used to build the Random
    Forest. These trees are stored in the `estimators_attribute` of the model. Let's
    draw one of these decision trees to get a feel for what's going on. Doing this
    requires the **graphviz** dependency, which can sometimes be difficult to install.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Draw one of the decision trees in the Jupyter Notebook by running the cell
    containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.38: Decision tree obtained using graphviz](img/C13018_02_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.38: Decision tree obtained using graphviz'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that each path is limited to five nodes as a result of setting max_depth=5\.
    The orange boxes represent predictions of no (has not left the company), and the
    blue boxes represent yes (has left the company). The shade of each box (light,
    dark, and so on) indicates the confidence level, which is related to the gini
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, we have accomplished two of the learning objectives in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: We gained a qualitative understanding of support vector machines (SVMs), k-Nearest
    Neighbor classifiers (kNNs), and Random Forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are now able to train a variety of models using scikit-learn and Jupyter
    Notebooks so that we can confidently build and compare predictive models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In particular, we used the preprocessed data from our employee retention problem
    to train classification models to predict whether an employee has left the company
    or not. For the purposes of keeping things simple and focusing on the algorithms,
    we built models to predict this given only two features: the satisfaction level
    and last evaluation value. This two-dimensional feature space also allowed us
    to visualize the decision boundaries and identify what overfitting looks like.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section, we will introduce two important topics in machine
    learning: k-fold cross-validation and validation curves.'
  prefs: []
  type: TYPE_NORMAL
- en: Assessing Models With K-fold Cross-Validation and Validation Curves
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thus far, we have trained models on a subset of the data and then assessed performance
    on the unseen portion, called the test set. This is good practice because the
    model performance on training data is not a good indicator of its effectiveness
    as a predictor. It's very easy to increase accuracy on a training dataset by overfitting
    a model, which can result in poorer performance on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: That said, simply training models on data split in this way is not good enough.
    There is a natural variance in data that causes accuracies to be different (if
    even slightly) depending on the training and test splits. Furthermore, using only
    one training/test split to compare models can introduce bias towards certain models
    and lead to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**K-fold cross validation** offers a solution to this problem and allows the
    variance to be accounted for by way of an error estimate on each accuracy calculation.
    This, in turn, naturally leads to the use of validation curves for tuning model
    parameters. These plot the accuracy as a function of a hyper parameter such as
    the number of decision trees used in a Random Forest or the max depth.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is our first time using the term hyperparameter. It references a parameter
    that is defined when initializing a model, for example, the C parameter of the
    SVM. This is in contradistinction to a parameter of the trained model, such as
    the equation of the decision boundary hyperplane for a trained SVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method is illustrated in the following diagram, where we see how the k-folds
    can be selected from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.39: Selecting k-folds from a data set](img/C13018_02_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.39: Selecting k-folds from a data set'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The k-fold cross validation algorithm goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Split data into k "folds" of near-equal size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test and train k models on different fold combinations. Each model will include
    k - 1 folds of training data and the left-out fold is used for testing. In this
    method, each fold ends up being used as the validation data exactly once.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the model accuracy by taking the mean of the k values. The standard
    deviation is also calculated to provide error bars on the value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It's standard to set *k = 10*, but smaller values for k should be considered
    if using a big data set.
  prefs: []
  type: TYPE_NORMAL
- en: This validation method can be used to reliably compare model performance with
    different hyperparameters (for example, the C parameter for an SVM or the number
    of nearest neighbors in a KNN classifier). It's also suitable for comparing entirely
    different models.
  prefs: []
  type: TYPE_NORMAL
- en: Once the *best model* has been identified, it should be re-trained on the entirety
    of the dataset before being used to predict actual classifications.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing this with scikit-learn, it's common to use a slightly improved
    variation of the normal k-fold algorithm instead. This is called **stratified
    k-fold**. The improvement is that stratified k-fold cross validation maintains
    roughly even class label populations in the folds. As you can imagine, this reduces
    the overall variance in the models and decreases the likelihood of highly unbalanced
    models causing bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Validation curves** are plots of a training and validation metric as a function
    of some model parameter. They allow to us to make good model parameter selections.
    In this book, we will use the accuracy score as our metric for these plots.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The documentation for plot validation curves is available here: [http://scikit-learn.org/stable/auto_examples/](http://scikit-learn.org/stable/auto_examples/)
    model_selection/plot_validation_curve.html.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this validation curve, where the accuracy score is plotted as a function
    of the gamma SVM parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.40: Validation curve with SVM](img/C13018_02_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.40: Validation curve with SVM'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Starting on the left side of the plot, we can see that both sets of data are
    agreeing on the score, which is good. However, the score is also quite low compared
    to other gamma values, so therefore we say the model is underfitting the data.
    Increasing the gamma, we can see a point where the error bars of these two lines
    no longer overlap. From this point on, we see the classifier overfitting the data
    as the models behave increasingly well on the training set compared to the validation
    set. The optimal value for the gamma parameter can be found by looking for a high
    validation score with overlapping error bars on the two lines.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that a learning curve for some parameter is only valid while the
    other parameters remain constant. For example, if training the SVM in this plot,
    we could decide to pick gamma on the order of. However, we may want to optimize
    the `C` parameter as well. With a different value for `C`, the preceding plot
    would be different and our selection for gamma may no longer be optimal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12: Using K-fold Cross Validation and Validation Curves in Python
    With Scikit-learn'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Start the `NotebookApp` and open the `lesson-2-workbook.ipynb` file. Scroll
    down to `Subtopic B: K-fold cross-validation and validation curves`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The training data should already be in the notebook's memory, but let's reload
    it as a reminder of what exactly we're working with.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the data and select the `satisfaction_level` and `last_evaluation` features
    for the training/validation set. We will not use the train-test split this time
    because we are going to use k-fold validation instead. Run the cell containing
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a Random Forest model by running the cell containing the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To train the model with stratified k-fold cross validation, we'll use the `model_
    selection.cross_val_score` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train 10 variations of our model `clf` using stratified k-fold validation.
    Note that scikit-learn''s `cross_val_score` does this type of validation by default.
    Run the cell containing the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note how we use `np.random.seed` to set the seed for the random number generator,
    therefore ensuring reproducibility with respect to the randomly selected samples
    for each fold and decision tree in the Random Forest.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the accuracy as the average of each fold. We can also see the individual
    accuracies for each fold by printing scores. To see these, `run print(scores)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using `cross_val_score` is very convenient, but it doesn''t tell us about the
    accuracies within each class. We can do this manually with the `model_ selection.StratifiedKFold`
    class. This class takes the number of folds as an initialization parameter, then
    the split method is used to build randomly sampled "masks" for the data. A mask
    is simply an array containing indexes of items in another array, where the items
    can then be returned by doing this: `data[mask]`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a custom class for calculating k-fold cross validation class accuracies.
    Run the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the complete code, refer to the following: [https://bit.ly/2O5uP3h](https://bit.ly/2O5uP3h).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can then calculate the class accuracies with code that''s very similar to
    step 4\. Do this by running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the complete code, refer to the following: [https://bit.ly/2EKK7Lp](https://bit.ly/2EKK7Lp).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now we can see the class accuracies for each fold! Pretty neat, right?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate a validation curve using `model_selection.validation_curve`. This
    function uses stratified k-fold cross validation to train models for various values
    of a given parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Do the calculations required to plot a validation curve by training Random
    Forests over a range of `max_depth` values. Run the cell containing the following
    code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will return arrays with the cross validation scores for each model, where
    the models have different max depths. In order to visualize the results, we'll
    leverage a function provided in the scikit-learn documentation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell in which `plot_validation_curve` is defined. Then, run the cell
    containing the following code to draw the plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.41: Plot validation curve](img/C13018_02_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.41: Plot validation curve'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recall how setting the max depth for decision trees limits the amount of overfitting.
    This is reflected in the validation curve, where we see overfitting taking place
    for large max depth values to the right. A good value for `max_depth` appears
    to be 6, where we see the training and validation accuracies in agreement. When
    `max_depth` is equal to 3, we see the model underfitting the data as training
    and validation accuracies are lower.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we have learned and implemented two important techniques for building
    reliable predictive models. The first such technique was k-foldcross-validation,
    which is used to split the data into various train/test batches and generate a
    set accuracy. From this set, we then calculated the average accuracy and the standard
    deviation as a measure of the error. This is important so that we have a gauge
    of the variability of our model and we can produce trustworthy accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also learned about another such technique to ensure we have trustworthy
    results: validation curves. These allow us to visualize when our model is overfitting
    based on comparing training and validation accuracies. By plotting the curve over
    a range of our selected hyperparameter, we are able to identify its optimal value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final section of this chapter, we take everything we have learned so
    far and put it together in order to build our final predictive model for the employee
    retention problem. We seek to improve the accuracy, compared to the models trained
    thus far, by including all of the features from the dataset in our model. We''ll
    see now-familiar topics such as k-fold cross-validation and validation curves,
    but we''ll also introduce something new: dimensionality reduction techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dimensionality reduction can simply involve removing unimportant features from
    the training data, but more exotic methods exist, such as **Principal Component
    Analysis** (**PCA**) and Linear Discriminant Analysis (LDA). These techniques
    allow for data compression, where the most important information from a large
    group of features can be encoded in just a few features.
  prefs: []
  type: TYPE_NORMAL
- en: In this subtopic, we'll focus on PCA. This technique transforms the data by
    projecting it into a new subspace of orthogonal "principal components," where
    the components with the highest eigenvalues encode the most information for training
    the model. Then, we can simply select a few of these principal components in place
    of the original high-dimensional dataset. For example, PCA could be used to encode
    the information from every pixel in an image. In this case, the original feature
    space would have dimensions equal to the number of pixels in the image. This high-dimensional
    space could then be reduced with PCA, where the majority of useful information
    for training predictive models might be reduced to just a few dimensions. Not
    only does this save time when training and using models, it allows them to perform
    better by removing noise in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Like the models you've seen, it's not necessary to have a detailed understanding
    of PCA in order to leverage the benefits. However, we'll dig into the technical
    details of PCA just a bit further so that we can conceptualize it better. The
    key insight of PCA is to identify patterns between features based on correlations,
    so the PCA algorithm calculates the covariance matrix and then decomposes this
    into eigenvectors and eigenvalues. The vectors are then used to transform the
    data into a new subspace, from which a fixed number of principal components can
    be selected.
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we'll see an example of how PCA can be used to improve
    our Random Forest model for the employee retention problem we have been working
    on. This will be done after training a classification model on the full feature
    space, to see how our accuracy is affected by dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 13: Training a Predictive Model for the Employee Retention Problem'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already spent considerable effort planning a machine learning strategy,
    preprocessing the data, and building predictive models for the employee retention
    problem. Recall that our business objective was to help the client prevent employees
    from leaving. The strategy we decided upon was to build a classification model
    that would predict the probability of employees leaving. This way, the company
    can assess the likelihood of current employees leaving and take action to prevent
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given our strategy, we can summarize the type of predictive modeling we are
    doing as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning on labeled training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification problems with two class labels (binary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In particular, we are training models to determine whether an employee has left
    the company, given a set of continuous and categorical features. After preparing
    the data for machine learning in *Activity 1, Preparing to Train a Predictive
    Model for the Employee-Retention Problem*, we went on to implement SVM, k-Nearest
    Neighbors, and Random Forest algorithms using just two features. These models
    were able to make predictions with over 90% overall accuracy. When looking at
    the specific class accuracies, however, we found that employees who had left (`class-
    label 1`) could only be predicted with 70-80% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how much this can be improved by utilizing the full feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down to the code for this section in the `lesson-2-workbook.ipynb` notebook.
    We should already have the preprocessed data loaded from the previous exercises,
    but this can be done again, if desired, by executing `df = pd.read_csv`(`'../data/hr-analytics/hr_data_processed.csv'`).
    Then, print the DataFrame columns with `print(df.columns)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define a list of all the features by copy and pasting the output from `df.columns`
    into a new list (making sure to remove the target variable `left`). Then, define
    `X` and `Y` as we have done before. This goes as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the complete code, refer to the following: [https://bit.ly/2D3WOQ2](https://bit.ly/2D3WOQ2).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Looking at the feature names, recall what the values look like for each one.
    Scroll up to the set of histograms we made in the first activity to help jog your
    memory. The first two features are continuous; these are what we used for training
    models in the previous two exercises. After that, we have a few discrete features,
    such as `number_project` and `time_spend_company`, followed by some binary fields
    such as `work_accident` and `promotion_last_5years`. We also have a bunch of binary
    features, such as `department_ IT` and `department_accounting`, which were created
    by one-hot encoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given a mix of features like this, Random Forests are a very attractive type
    of model. For one thing, they're compatible with feature sets composed of both
    continuous and categorical data, but this is not particularly special; for instance,
    an SVM can be trained on mixed feature types as well (given proper preprocessing).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you''re interested in training an SVM or k-Nearest Neighbors classifier
    on mixed-type input features, you can use the data-scaling prescription from this
    StackExchange answer: [https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm/83086#83086](https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm/83086#83086).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A simple approach would be to preprocess data as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: standardize continuous variables; one-hot-encode categorical features; shift
    binary values to -1 and 1 instead of 0 and 1\. Finally, the mixed-feature data
    could be used to train a variety of classification models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Tune the `max_depth` hyperparameter using a validation curve to figure out
    the best parameters for our Random Forest model. Calculate the training and validation
    accuracies by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are testing 10 models with k-fold cross validation. By setting k = 5, we
    produce five estimates of the accuracy for each model, from which we extract the
    mean and standard deviation to plot in the validation curve. In total, we train
    50 models, and since `n_estimators` is set to 20, we are training a total of 1,000
    decision trees! All in roughly 10 seconds!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the validation curve using our custom `plot_validation_curve` function
    from the last exercise. Run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.42: Plot validation curve for different values of max_depths](img/C13018_02_42.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.42: Plot validation curve for different values of max_depths'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: For small max depths, we see the model underfitting the data. Total accuracies
    dramatically increase by allowing the decision trees to be deeper and encode more
    complicated patterns in the data. As the max depth is increased further and the
    accuracy approaches 100%, we find the model overfits the data, causing the training
    and validation accuracies to grow apart. Based on this figure, let's select a
    `max_depth` of 6 for our model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We should really do the same for `n_estimators`, but in the spirit of saving
    time, we'll skip it. You are welcome to plot it on your own; you should find agreement
    between training and validation sets for a large range of values. Usually it's
    better to use more decision tree estimators in the random forest, but this comes
    at the cost of increased training times. We'll use 200 estimators to train our
    model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use `cross_val_class_score`, the k-fold cross validation by class function we
    created earlier, to test the selected model, a Random Forest with `max_ depth
    = 6` and `n_estimators = 200:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracies are way higher now that we're using the full feature set, compared
    to before when we only had the two continuous features!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the accuracies with a boxplot by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.43: Visualizing the accuracy with a box plot](img/C13018_02_43.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.43: Visualizing the accuracy with a box plot'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Random forests can provide an estimate of the feature performances.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The feature importance in scikit-learn is calculated based on how the node
    impurity changes with respect to each feature. For a more detailed explanation,
    take a look at the following StackOverflow thread about how feature importance
    is determined in Random Forest Classifier: [https://stackoverflow.com](https://stackoverflow.com)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the feature importance, as stored in the attribute `feature_importances_`,
    by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.44: Plot of feature_importance](img/C13018_02_44.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.44: Plot of feature_importance'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It doesn''t look like we''re getting much in the way of useful contribution
    from the one-hot encoded variables: department and salary. Also, the `promotion_last_5years`
    and `work_accident` features don''t appear to be very useful.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's use PCA to condense all of these weak features into just a few principal
    components.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Import the `PCA` class from scikit-learn and transform the features. Run the
    following code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since we asked for the top three components, we get three vectors returned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add the new features to our DataFrame with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select our reduced-dimension feature set to train a new Random Forest with.
    Run the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assess the new model''s accuracy with k-fold cross validation. This can be
    done by running the same code as before, where X now points to different features.
    The code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the result in the same way as before, using a box plot. The code
    is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.45: Box plot to visualize accuracy](img/C13018_02_45.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.45: Box plot to visualize accuracy'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Comparing this to the previous result, we find an improvement in the class 1
    accuracy! Now, the majority of the validation sets return an accuracy greater
    than 90%. The average accuracy of 90.6% can be compared to the accuracy of 85.6%
    prior to dimensionality reduction!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's select this as our final model. We'll need to re-train it on the full
    sample space before using it in production.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train the final predictive model by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the trained model to a binary file using `externals.joblib.dump`. Run
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check that it''s saved into the working directory, for example, by running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`!ls *.pkl`. Then, test that we can load the model from the file by running
    the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You've trained the final predictive model! Now, let's see an
    example of how it can be used to provide business insights for the client. Say
    we have a particular employee, who we'll call Sandra. Management has noticed she
    is working very hard and reported low job satisfaction in a recent survey. They
    would therefore like to know how likely it is that she will quit. For the sake
    of simplicity, let's take her feature values as a sample from the training set
    (but pretend that this is unseen data instead).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'List the feature values for Sandra by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The next step is to ask the model which group it thinks she should be in.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Predict the class label for Sandra by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model classifies her as having already left the company; not a good sign!
    We can take this a step further and calculate the probabilities of each class
    label.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use `clf.predict_proba` to predict the probability of our model predicting
    that Sandra has quit. Run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We see the model predicting that she has quit with 93% accuracy. Since this
    is clearly a red flag for management, they decide on a plan to reduce her number
    of monthly hours to 100 and the time spent at the company to 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the new probabilities with Sandra''s newly planned metrics. Run the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Excellent! We can now see that the model returns a mere 38% likelihood that
    she has quit! Instead, it now predicts she will not have left the company.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our model has allowed management to make a data-driven decision. By reducing
    her amount of time with the company by this particular amount, the model tells
    us that she will most likely remain an employee at the company!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have seen how predictive models can be trained in Jupyter
    Notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, we talked about how to plan a machine learning strategy. We thought
    about how to design a plan that can lead to actionable business insights and stressed
    the importance of using the data to help set realistic business goals. We also
    explained machine learning terminology such as supervised learning, unsupervised
    learning, classification, and regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we discussed methods for preprocessing data using scikit-learn and pandas.
    This included lengthy discussions and examples of a surprisingly time-consuming
    part of machine learning: dealing with missing data.'
  prefs: []
  type: TYPE_NORMAL
- en: In the latter half of the chapter, we trained predictive classification models
    for our binary problem, comparing how decision boundaries are drawn for various
    models such as the SVM, k-Nearest Neighbors, and Random Forest. We then showed
    how validation curves can be used to make good parameter choices and how dimensionality
    reduction can improve model performance. Finally, at the end of our activity,
    we explored how the final model can be used in practice to make data-driven decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on data acquisition. Specifically, we will
    analyze HTTP requests, scrape tabular data from a web page, build and transform
    Pandas DataFrames, and finally create visualizations.
  prefs: []
  type: TYPE_NORMAL
