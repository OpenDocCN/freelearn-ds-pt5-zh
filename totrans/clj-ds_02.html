<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch02" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 2. Inference</h1></div></div></div><div><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote1" summary="Block quote"><tr class="calibre17"><td class="calibre18"> </td><td class="calibre18"><p class="calibre19"><em class="calibre13">"I can see nothing," said I, handing it back to my friend.</em></p><p class="calibre19"><em class="calibre13">"On the contrary, Watson, you can see everything. You fail, however, to reason from what you see. You are too timid in drawing your inferences."</em></p></td><td class="calibre18"> </td></tr><tr class="calibre17"><td class="calibre18"> </td><td colspan="2" class="calibre20">--<em class="calibre13">Sir Arthur Conan Doyle, The Adventure of the Blue Carbuncle</em></td></tr></table></div><p class="calibre11">In the previous chapter, we introduced a variety of numerical and visual approaches to understand the normal distribution. We discussed descriptive statistics, such as the mean and standard deviation, and how they can be used to summarize large amounts of data succinctly.</p><p class="calibre11">A dataset is usually a <a id="id129" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sample of some larger population. Sometimes, this population is too large to be measured in its entirety. Sometimes, it is intrinsically unmeasurable, either because it is infinite in size or it otherwise cannot be accessed directly. In either case, we are forced to generalize from the data that we have.</p><p class="calibre11">In this chapter, we consider statistical inference: how we can go beyond simply describing the samples of data and instead describe the population from which they were sampled. We'll look in detail at how confident we can be about the inferences we make from the samples of data. We'll cover hypothesis testing: a robust approach to data analysis that puts the science in data science. We'll also implement an interactive web page with ClojureScript to simulate the relationship between samples and the population from which they are taken.</p><p class="calibre11">To help illustrate the principles, we'll invent a fictional company, AcmeContent, that has recently hired us as a data scientist.</p><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec28" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Introducing AcmeContent</h1></div></div></div><p class="calibre11">To help <a id="id130" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>illustrate the concepts in this chapter, let's imagine that we've recently been appointed for the data scientist role at AcmeContent. The company runs a website that lets visitors share video clips that they've enjoyed online.</p><p class="calibre11">One of the metrics AcmeContent tracks through its web analytics is <strong class="calibre12">dwell time</strong>. This is a measure of how long <a id="id131" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a visitor stays on the site. Clearly, visitors who spend a long time on the site are enjoying themselves and AcmeContent wants its visitors to stay as long as possible. If the mean dwell time increases, our CEO will be very happy.</p><div><div><h3 class="title4"><a id="note13" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Dwell time is the length of time between the time a visitor first arrives at a website and the time they make their last request to your site.</p><p class="calibre22">A <strong class="calibre12">bounce</strong> is a visitor <a id="id132" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>who makes only one request—their dwell time is zero.</p></div></div><p class="calibre11">As the company's <a id="id133" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>new data scientist, it falls to us to analyze the dwell time reported by the website's analytics and measure the success of AcmeContent's site.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec29" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Download the sample code</h1></div></div></div><p class="calibre11">The code for this <a id="id134" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chapter is available at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clojuredatascience/ch2-inference">https://github.com/clojuredatascience/ch2-inference</a> or from the Packt Publishing's website.</p><p class="calibre11">The example data has been generated specifically for this chapter. It's small enough that it has been included with the book's sample code inside the data directory. Consult the book's wiki at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://wiki.clojuredatascience.com">http://wiki.clojuredatascience.com</a> for links to further read about dwell time analysis.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec30" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Load and inspect the data</h1></div></div></div><p class="calibre11">In the previous chapter, we <a id="id135" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>used Incanter to load Excel spreadsheets with the <code class="literal">incanter.excel/load-xls</code> function. In this chapter, we will load a dataset from a <a id="id136" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>tab-separated text file. For this, we'll make use of <code class="literal">incanter.io/read-dataset</code> that expects to receive either a URL object or a file path represented as a string.</p><p class="calibre11">The file has been helpfully reformatted by AcmeContent's web team to contain just two columns—the date of the request and the dwell time in seconds. There are column headings in the first row, so we pass <code class="literal">:header true</code> to <code class="literal">read-dataset</code>:</p><div><pre class="programlisting">(defn load-data [file]
  (-&gt; (io/resource file)
      (iio/read-dataset :header true :delim \tab)))

(defn ex-2-1 []
  (-&gt; (load-data "dwell-times.tsv")
      (i/view)))</pre></div><p class="calibre11">If you run this <a id="id137" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>code (either in the REPL or on the command line with <code class="literal">lein run –e 2.1</code>), you <a id="id138" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>should see an output similar to the following:</p><div><img src="img/7180OS_02_100.jpg" alt="Load and inspect the data" class="calibre52"/></div><p class="calibre11">Let's see what the dwell times look like as a histogram.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec31" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Visualizing the dwell times</h1></div></div></div><p class="calibre11">We can plot a <a id="id139" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>histogram of dwell times by simply extracting the <code class="literal">:dwell-time</code> column with <code class="literal">i/$</code>:</p><div><pre class="programlisting">(defn ex-2-2 []
  (-&gt; (i/$ :dwell-time (load-data "dwell-times.tsv"))
      (c/histogram :x-label "Dwell time (s)"
                   :nbins 50)
      (i/view)))</pre></div><p class="calibre11">The earlier code generates the following histogram:</p><div><img src="img/7180OS_02_110.jpg" alt="Visualizing the dwell times" class="calibre45"/></div><p class="calibre11">This is clearly not a normally distributed data, nor even a very skewed normal distribution. There is no tail to the left of the peak (a visitor clearly can't be on our site for less than zero seconds). While the data tails off steeply to the right at first, it extends much further along the <em class="calibre13">x</em> axis than we would expect from normally distributed data.</p><p class="calibre11">When confronted <a id="id140" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with distributions like this, where values are mostly small but occasionally extreme, it can be useful to plot the <em class="calibre13">y</em> axis as a <strong class="calibre12">log scale</strong>. Log scales are used to represent events that cover a very large range. Chart axes are ordinarily linear and they partition a range into equally sized steps like the "number line" we learned at school. Log scales partition the range into steps that get larger and larger as they go further away from the origin.</p><p class="calibre11">Some systems of measurement for natural phenomena that cover a very large range are represented on a log scale. For example, the Richter magnitude scale for earthquakes is a base-10 logarithmic scale, which means that an earthquake measuring 5 on the Richter scale is 10 times the magnitude of an earthquake measuring 4. The decibel scale is also a logarithmic scale with a different base—a sound wave of 30 decibels has 10 times the magnitude of a sound wave of 20 decibels. In each case, the principle is the same—the use of a log scale allows a very large range of values to be compressed into a much smaller range.</p><p class="calibre11">Plotting our <em class="calibre13">y</em> axis <a id="id141" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>on <code class="literal">log-axis</code> is simple with Incanter with <code class="literal">c/set-axis</code>:</p><div><pre class="programlisting">(defn ex-2-3 []
  (-&gt; (i/$ :dwell-time (load-data "dwell-times.tsv"))
      (c/histogram :x-label "Dwell time (s)"
                   :nbins 20)
      (c/set-axis :y (c/log-axis :label "Log Frequency"))
      (i/view)))</pre></div><p class="calibre11">By default Incanter will use a base-10 log scale, meaning that each tick on the axis represents a range that is 10 <a id="id142" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>times the previous step. A chart like this—where only one axis is shown on a log scale—is called <strong class="calibre12">log-linear</strong>. Unsurprisingly, a chart showing two log <a id="id143" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>axes is called a <strong class="calibre12">log-log chart</strong>.</p><div><img src="img/7180OS_02_120.jpg" alt="Visualizing the dwell times" class="calibre45"/></div><p class="calibre11">Plotting dwell times on a log-linear plot shows hidden consistency in the data—there is a linear relationship between the dwell time and the logarithm of the frequency. The clarity of the relationship breaks down to the right of the plot where there are fewer than 10 visitors but, aside <a id="id144" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>from this, the relationship is remarkably consistent.</p><p class="calibre11">A straight line on a log-linear plot is a clear indicator of an exponential distribution.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec32" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The exponential distribution</h1></div></div></div><p class="calibre11">The exponential <a id="id145" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>distribution occurs frequently when considering situations where there are many small positive quantities and much fewer larger quantities. Given what we have learned about the Richter scale, it won't be a surprise to learn that the magnitude of earthquakes follows an exponential distribution.</p><p class="calibre11">The distribution also frequently occurs in waiting times—the time until the next earthquake of any magnitude roughly follows an exponential distribution as well. The distribution is often used to model failure rates, which is essentially the waiting time until a machine breaks down. Our exponential distribution models a process similar to failure—the waiting time until a visitor gets bored and leaves our site.</p><p class="calibre11">The exponential distribution has a number of interesting properties. One relates to the mean and standard deviation:</p><div><pre class="programlisting">(defn ex-2-4 []
  (let [dwell-times (-&gt;&gt; (load-data "dwell-times.tsv")
                         (i/$ :dwell-time))]
    (println "Mean:  " (s/mean dwell-times))
    (println "Median:" (s/median dwell-times))
    (println "SD:    " (s/sd dwell-times))))

Mean:   93.2014074074074
Median: 64.0
SD:     93.96972402519796</pre></div><p class="calibre11">The mean and standard deviations are very similar. In fact, for an ideal exponential distribution, they are exactly the same. This property holds true for all the exponential distributions—as the mean increases, so does the standard deviation.</p><div><div><h3 class="title4"><a id="note14" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">For exponential distributions, the mean and standard deviations are equal.</p></div></div><p class="calibre11">A second property <a id="id146" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the exponential distribution is that it is <strong class="calibre12">memoryless</strong>. This is a counterintuitive property best illustrated by an example. We expect that as a visitor continues to browse our site, the probability of them getting bored and leaving increases. Since the mean dwell time is 93 seconds, it might appear that beyond 93 seconds, they are less and less likely to continue browsing.</p><p class="calibre11">The memoryless property of exponential distributions tells us that the probability of a visitor staying on our site for another 93 seconds is exactly the same whether they have already been browsing the site for 93 seconds, 5 minutes, an hour, or they have just arrived.</p><div><div><h3 class="title4"><a id="note15" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">For a memoryless distribution, the probability of continuing for an additional <em class="calibre13">x</em> minutes is not affected by how much time has already elapsed.</p></div></div><p class="calibre11">The memoryless property of exponential distributions goes some way towards explaining why it is so difficult to predict when an earthquake will occur next. We must rely on other evidence (such as a disturbance in geomagnetism) rather than the elapsed time.</p><p class="calibre11">Since the median dwell time is 64 seconds, about half of our visitors are staying on the site for only around a <a id="id147" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>minute. A mean of 93 seconds shows that some visitors are staying much longer than that. These statistics have been calculated on all the visitors over the last 6 months. It might be interesting to see how these statistics vary per day. Let's calculate this now.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec21" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The distribution of daily means</h2></div></div></div><p class="calibre11">The file provided <a id="id148" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>by the web team includes the timestamp of the visit. In order to aggregate by day, it's necessary to remove the time portion from the date. While we could do this with string manipulation, a more flexible approach would be to use a date and time library such as <code class="literal">clj-time</code> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clj-time/clj-time">https://github.com/clj-time/clj-time</a>) to parse the string. This will allow us to not only remove the time, but also perform arbitrarily complex filters (such as filtering to particular days of the week or the first or last day of the month, for example).</p><p class="calibre11">The <code class="literal">clj-time.predicates</code> namespace contains a variety of useful predicates and the <code class="literal">clj-time.format</code> namespace contains parsing functions that will attempt to convert the string to a date-time object using predefined standard formats. If our timestamp wasn't already in a standard format, we could use the same namespace to build a custom formatter. Consult the <code class="literal">clj-time</code> documentation for more information and many usage examples:</p><div><pre class="programlisting">(defn with-parsed-date [data]
  (i/transform-col data :date (comp tc/to-local-date f/parse)))

(defn filter-weekdays [data]
  (i/$where {:date {:$fn p/weekday?}} data))

(defn mean-dwell-times-by-date [data]
  (i/$rollup :mean :dwell-time :date data))

(defn daily-mean-dwell-times [data]
  (-&gt;&gt; (with-parsed-date data)
       (filter-weekdays)
       (mean-dwell-times-by-date)))</pre></div><p class="calibre11">Combining the previous functions allows us to calculate the mean, median, and standard deviation for the <a id="id149" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>daily mean dwell times:</p><div><pre class="programlisting">(defn ex-2-5 []
  (let [means (-&gt;&gt; (load-data "dwell-times.tsv")
                   (daily-mean-dwell-times)
                   (i/$ :dwell-time))]
    (println "Mean:   " (s/mean means))
    (println "Median: " (s/median means))
    (println "SD:     " (s/sd means))))

;; Mean:    90.210428650562
;; Median:  90.13661202185791
;; SD:      3.722342905320035</pre></div><p class="calibre11">The mean value of our daily means is 90.2 seconds. This is close to the mean value we calculated previously on the whole dataset, including weekends. The standard deviation is much lower though, just 3.7 seconds. In other words, the distribution of daily means has a much lower standard deviation than the entire dataset. Let's plot the daily mean dwell times on a chart:</p><div><pre class="programlisting">(defn ex-2-6 []
  (let [means (-&gt;&gt; (load-data "dwell-times.tsv")
                   (daily-mean-dwell-times)
                   (i/$ :dwell-time))]
    (-&gt; (c/histogram means
                     :x-label "Daily mean dwell time (s)"
                     :nbins 20)
        (i/view))))</pre></div><p class="calibre11">This code generates the following histogram:</p><div><img src="img/7180OS_02_130.jpg" alt="The distribution of daily means" class="calibre45"/></div><p class="calibre11">The distribution of sample means is distributed symmetrically around the overall grand mean value of 90 <a id="id150" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>seconds with a standard deviation of 3.7 seconds. Unlike the distribution from which these means were sampled—the exponential distribution—the distribution of sample means is normally distributed.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec33" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The central limit theorem</h1></div></div></div><p class="calibre11">We <a id="id151" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>encountered the central limit theorem in the previous chapter when we took samples from a uniform distribution and averaged them. In fact, the central limit theorem works for any distribution of values, provided the distribution has a finite standard deviation.</p><div><div><h3 class="title4"><a id="note16" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The central limit theorem states that the distribution of sample means will be normally distributed irrespective of the distribution from which they were calculated.</p></div></div><p class="calibre11">It doesn't matter that the underlying distribution is exponential—the central limit theorem shows that the mean of random samples taken from any distribution will closely approximate a normal distribution. Let's plot a normal curve over our histogram to see how closely it matches.</p><p class="calibre11">To plot a normal curve over our histogram, we have to plot our histogram as a density histogram. This <a id="id152" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>plots the proportion of all the points that have been put in each bucket rather than the frequency. We can then overlay the normal probability density with the same mean and standard deviation:</p><div><pre class="programlisting">(defn ex-2-7 []
  (let [means (-&gt;&gt; (load-data "dwell-times.tsv")
                   (daily-mean-dwell-times)
                   (i/$ :dwell-time))
        mean (s/mean means)
        sd   (s/sd means)
        pdf  (fn [x]
               (s/pdf-normal x :mean mean :sd sd))]
    (-&gt; (c/histogram means
                     :x-label "Daily mean dwell time (s)"
                     :nbins 20
                     :density true)
        (c/add-function pdf 80 100)
        (i/view))))</pre></div><p class="calibre11">This code generates the following chart:</p><div><img src="img/7180OS_02_140.jpg" alt="The central limit theorem" class="calibre45"/></div><p class="calibre11">The normal curve plotted over the histogram has a standard deviation of approximately 3.7 seconds. In other words, this quantifies the variation of each daily mean being relative to the grand mean <a id="id153" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of 90 seconds. We can think of each day's mean as a sample from the overall population with the earlier curve representing the distribution of the sample means. Because 3.7 seconds is the amount that the sample's mean differs from the grand mean, it's referred to as the <strong class="calibre12">standard error</strong>.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec34" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Standard error</h1></div></div></div><p class="calibre11">While the <a id="id154" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>standard deviation measures the amount of variation there is within a sample, the standard error measures the amount of variation there is between the means of samples taken from the same population.</p><div><div><h3 class="title4"><a id="note17" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The standard error is the standard deviation of the distribution of the sample means.</p></div></div><p class="calibre11">We have calculated the standard error of dwell time empirically by looking at the previous 6 months of data. But there is an equation that allows us to calculate it from only a single sample:</p><div><img src="img/7180OS_02_01.jpg" alt="Standard error" class="calibre53"/></div><p class="calibre11">Here, <em class="calibre13">σ<sub class="calibre25">x</sub></em> is the standard deviation and <em class="calibre13">n</em> is the sample size. This is unlike the descriptive statistics that we studied in the previous chapter. While they described a single sample, the standard error attempts to describe a property of samples in general—the amount of variation in the sample means that variations can be expected for samples of a given size:</p><div><pre class="programlisting">(defn standard-deviation [xs]
  (Math/sqrt (variance xs)))

(defn standard-error [xs]
  (/ (standard-deviation xs)
     (Math/sqrt (count xs))))</pre></div><p class="calibre11">The standard error of the mean is thus related to two factors:</p><div><ul class="itemizedlist"><li class="listitem">The size of the sample</li><li class="listitem">The population standard deviation</li></ul></div><p class="calibre11">The size of the sample has the largest impact on the standard error. Since we take the square root of the sample size, we have to increase the size of the sample by four to halve the size of the standard error.</p><p class="calibre11">It may seem curious that the proportion of the population sampled has no effect on the size of the standard error. This is just as well, since some populations could be infinite in size.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec35" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Samples and populations</h1></div></div></div><p class="calibre11">The words "sample" and "population" mean something very particular to statisticians. A population is the <a id="id155" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>entire collection of entities that a researcher wishes to understand <a id="id156" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>or draw conclusions about. For example, in the second half of the 19th century, Gregor Johann Mendel, the originator of genetics, recorded observations about pea plants. Although he was studying specific plants in a laboratory, his objective was to understand the underlying mechanisms behind heredity in all possible pea plants.</p><div><div><h3 class="title4"><a id="note18" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Statisticians refer to the group of entities from which a sample is drawn as the population, whether or not the objects being studied are people.</p></div></div><p class="calibre11">Since populations may be large—or in the case of Mendel's pea plants, infinite—we must study representative samples and draw inferences about the population from them. To distinguish the measurable attributes of our samples from the inaccessible attributes of the population, we use the word <em class="calibre13">statistics</em> to refer to the <em class="calibre13">sample</em> attributes and parameters to refer to the population attributes.</p><div><div><h3 class="title4"><a id="note19" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Statistics are the attributes we can measure from our samples. Parameters are the attributes of the population we are trying to infer.</p></div></div><p class="calibre11">In fact, statistics and parameters are distinguished through the use of different symbols in mathematical formulae:</p><div><table border="1" class="calibre27"><colgroup class="calibre28"><col class="calibre29"/><col class="calibre29"/><col class="calibre29"/></colgroup><thead class="calibre30"><tr class="calibre31"><th valign="bottom" class="calibre32">
<p class="calibre19">Measure</p>
</th><th valign="bottom" class="calibre32">
<p class="calibre19">Sample statistic</p>
</th><th valign="bottom" class="calibre32">
<p class="calibre19">Population parameter</p>
</th></tr></thead><tbody class="calibre33"><tr class="calibre34"><td class="calibre35">
<p class="calibre19">Number of items</p>
</td><td class="calibre35">
<p class="calibre19">
<em class="calibre13">n</em>
</p>
</td><td class="calibre35">
<p class="calibre19">
<em class="calibre13">N</em>
</p>
</td></tr><tr class="calibre36"><td class="calibre35">
<p class="calibre19">Mean</p>
</td><td class="calibre35">
<img src="img/7180OS_02_02.jpg" alt="Samples and populations" class="calibre54"/>
</td><td class="calibre35">
<p class="calibre19">
<em class="calibre13">µ<sub class="calibre25">x</sub></em>
</p>
</td></tr><tr class="calibre34"><td class="calibre35">
<p class="calibre19">Standard deviation</p>
</td><td class="calibre35">
<p class="calibre19">
<em class="calibre13">S<sub class="calibre25">x</sub></em>
</p>
</td><td class="calibre35">
<p class="calibre19">
<em class="calibre13">σ<sub class="calibre25">x</sub></em>
</p>
</td></tr><tr class="calibre51"><td class="calibre35">
<p class="calibre19">Standard error</p>
</td><td class="calibre35">
<img src="img/7180OS_02_03.jpg" alt="Samples and populations" class="calibre55"/>
</td><td class="calibre35"> </td></tr></tbody></table></div><p class="calibre11">Here, <img src="img/7180OS_02_02.jpg" alt="Samples and populations" class="calibre24"/> is pronounced as "x-bar," <em class="calibre13">µ<sub class="calibre25">x</sub></em> is pronounced as "mu x," and <em class="calibre13">σ<sub class="calibre25">x</sub></em> is pronounced as "sigma x."</p><p class="calibre11">If you refer back to the equation for the standard error, you'll notice that it is calculated from the population standard deviation <em class="calibre13">σ<sub class="calibre25">x</sub></em>, not the sample standard deviation <em class="calibre13">S<sub class="calibre25">x</sub></em>. This presents us with a paradox—we can't calculate the sample statistic using population parameters when the population parameters are precisely the values we are trying to infer. In practice, though, the sample and population standard deviations are assumed to be the same above a sample size of about 30.</p><p class="calibre11">Let's calculate the <a id="id157" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>standard error from a particular day's means. For example, let's take a particular day, say May 1:</p><div><pre class="programlisting">(defn ex-2-8 []
  (let [may-1 (f/parse-local-date "2015-05-01")]
    (-&gt;&gt; (load-data "dwell-times.tsv")
         (with-parsed-date)
         (filtered-times {:date {:$eq may-1}})
         (standard-error))))

;; 3.627</pre></div><p class="calibre11">Although we have <a id="id158" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>only taken a sample from one day, the standard error we calculate is very close to the standard deviation of all the sample means—3.6 compared to 3.7s. It's as if, like a cell containing DNA, each sample encodes information about the entire population within it.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec36" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Confidence intervals</h1></div></div></div><p class="calibre11">Since the standard <a id="id159" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>error of our sample measures how closely we expect our sample mean to match the population mean, we could also consider the inverse—the standard error measures how closely we expect the population mean to match our measured sample mean. In other words, based on our standard error, we can infer that the population mean lies within some expected range of the sample mean with a certain degree of confidence.</p><p class="calibre11">Taken together, the "degree of confidence" and the "expected range" define a <strong class="calibre12">confidence interval</strong>. While stating confidence intervals, it is fairly standard to state the 95 percent interval—we are 95 percent sure that the population parameter lies within the interval. Of course, there remains a 5 percent possibility that it does not.</p><div><img src="img/7180OS_02_150.jpg" alt="Confidence intervals" class="calibre56"/></div><p class="calibre11">Whatever the <a id="id160" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>standard error, 95 percent of the population mean will lie between -1.96 and 1.96 standard deviations of the sample mean. 1.96 is therefore the <em class="calibre13">critical z-value</em> for a 95 percent confidence interval.</p><div><div><h3 class="title4"><a id="note20" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The name <em class="calibre13">z</em>-value comes from the fact that the normal distribution is also called the <em class="calibre13">z</em>-distribution.</p></div></div><p class="calibre11">The number 1.96 is so commonly used that it's a number worth remembering, but we can also calculate the critical value using the <code class="literal">s/quantile-normal</code> function. Our <code class="literal">confidence-interval</code> function that follows expects a value for <code class="literal">p</code> between zero and one. This will be 0.95 for our 95 percent confidence interval. We need to subtract it from one and divide it by two to calculate the site of each of the two tails (2.5 percent for the 95 percent confidence interval):</p><div><pre class="programlisting">(defn confidence-interval [p xs]
  (let [x-bar  (s/mean xs)
        se     (standard-error xs)
        z-crit (s/quantile-normal (- 1 (/ (- 1 p) 2)))]
    [(- x-bar (* se z-crit))
     (+ x-bar (* se z-crit))]))

(defn ex-2-9 []
  (let [may-1 (f/parse-local-date "2015-05-01")]
    (-&gt;&gt; (load-data "dwell-times.tsv")
         (with-parsed-date)
         (filtered-times {:date {:$eq may-1}})
         (confidence-interval 0.95))))

;; [83.53415272762004 97.75306531749274]</pre></div><p class="calibre11">The result tells us that <a id="id161" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we can be 95 percent confident that the population mean lies between 83.53 and 97.75 seconds. Indeed, the population mean we calculated previously lies well within this range.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec22" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Sample comparisons</h2></div></div></div><p class="calibre11">After a viral <a id="id162" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>marketing campaign, the web team at AcmeContent take a sample of dwell times for us to analyze from a single day. They'd like to know whether their latest campaign has brought more engaged visitors to the site. Confidence intervals provide us with an intuitive way to compare the two samples.</p><p class="calibre11">We load the dwell times from the campaign as we did earlier and summarize them in the same way:</p><div><pre class="programlisting">(defn ex-2-10 []
  (let [times (-&gt;&gt; (load-data "campaign-sample.tsv")
                   (i/$ :dwell-time))]
    (println "n:      " (count times))
    (println "Mean:   " (s/mean times))
    (println "Median: " (s/median times))
    (println "SD:     " (s/sd times))
    (println "SE:     " (standard-error times))))

;; n:       300
;; Mean:    130.22
;; Median:  84.0
;; SD:      136.13370714388046
;; SE:      7.846572839994115</pre></div><p class="calibre11">The mean seems to be much larger than the means we have been looking at previously—130s compared to 90s. It could be that there is some significant difference here, although the standard error is over twice the size of our previous one day sample, owing to a smaller sample size and larger standard deviation. We can calculate the 95 percent confidence interval for the population mean based on this data using the same <code class="literal">confidence-interval</code> function like before:</p><div><pre class="programlisting">(defn ex-2-11 []
  (-&gt;&gt; (load-data "campaign-sample.tsv")
       (i/$ :dwell-time)
       (confidence-interval 0.95)))

;; [114.84099983154137 145.59900016845864]</pre></div><p class="calibre11">The 95 percent <a id="id163" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>confidence interval for the population mean is 114.8s to 145.6s. This doesn't overlap with the 90s population mean we calculated previously at all. There appears to be a large underlying population difference that is unlikely to have occurred just through a sampling error alone. Our task now is to find out why.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec23" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Bias</h2></div></div></div><p class="calibre11">A sample should be <a id="id164" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>representative of the population from which it is drawn. In other words, it should avoid bias that would result in certain kinds of population members being systematically excluded (or included) over others.</p><p class="calibre11">A famous example of sample bias is the 1936 Literary Digest poll for the US Presidential Election. It was one of the largest and most expensive polls ever conducted with 2.4 million people being surveyed by mail. The results were decisive—Republican governor of Kansas Alfred Landon would defeat Franklin D. Roosevelt, taking 57 percent of the vote. In the event, Roosevelt won the election with 62 percent of the vote.</p><p class="calibre11">The primary cause of the magazine's huge sampling error was sample selection bias. In their attempt to gather as many voter addresses as possible, the Literary Digest scoured telephone directories, magazine subscription lists, and club membership lists. In an era when telephones were more of a luxury item, this process was guaranteed to be biased in favor of upper- and middle-class voters and was not representative of the electorate as a whole. A secondary <a id="id165" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cause of bias was <strong class="calibre12">nonresponse bias</strong>—less than a quarter of those who were approached actually responded to the survey. This is a kind of selection bias that favors only those respondents who actually wish to participate.</p><p class="calibre11">A common way to avoid sample selection bias is to ensure that the sampling is randomized in some way. Introducing chance into the process makes it less likely that experimental factors will unfairly influence the quality of the sample. The Literary Digest poll was focused on getting the largest sample possible, but an unbiased small sample is much more useful than a badly chosen large sample.</p><p class="calibre11">If we open up the <code class="literal">campaign-sample.tsv</code> file, we'll discover that our sample has come exclusively from June 6, 2015. This was a weekend, a fact we can easily confirm with <code class="literal">clj-time</code>:</p><div><pre class="programlisting">(p/weekend? (t/date-time 2015 6 6))
;; true</pre></div><p class="calibre11">Our summary statistics so far have all been based on the data we filtered just to include weekdays. This is a bias <a id="id166" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>in our sample, and if the weekend visitor behavior turns out to be different from the weekday behavior—a very likely scenario—then we would say that the samples represent two different populations.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec37" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Visualizing different populations</h1></div></div></div><p class="calibre11">Let's remove the <a id="id167" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>filter for weekdays and plot the daily mean dwell time for both week days and weekends:</p><div><pre class="programlisting">(defn ex-2-12 []
  (let [means (-&gt;&gt; (load-data "dwell-times.tsv")
                   (with-parsed-date)
                   (mean-dwell-times-by-date)
                   (i/$ :dwell-time))]
    (-&gt; (c/histogram means
                     :x-label "Daily mean dwell time unfiltered (s)"
                     :nbins 20)
        (i/view))))</pre></div><p class="calibre11">The code generates the following histogram:</p><div><img src="img/7180OS_02_160.jpg" alt="Visualizing different populations" class="calibre45"/></div><p class="calibre11">The distribution is no longer a <a id="id168" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>normal distribution. In fact, the distribution is <strong class="calibre12">bimodal</strong>—there are two peaks. The second smaller peak, which corresponds to the newly <a id="id169" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>added weekend data, is lower both because there are not as many weekend days as weekdays and because the distribution has a larger standard error.</p><div><div><h3 class="title4"><a id="note21" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">In general, distributions with more than one peak are referred to as <strong class="calibre12">multimodal</strong>. They can be an <a id="id170" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>indicator that two or more normal distributions have been combined, and therefore, that two or more populations may have been combined. A classic example of bimodality is the distribution of people's heights, since the modal height for men is larger than that for women.</p></div></div><p class="calibre11">The weekend data has different characteristics than the weekday data. We should make sure that we're comparing like with like. Let's filter our original dataset just to weekends:</p><div><pre class="programlisting">(defn ex-2-13 []
  (let [weekend-times (-&gt;&gt; (load-data "dwell-times.tsv")
                           (with-parsed-date)
                           (i/$where {:date {:$fn p/weekend?}})
                           (i/$ :dwell-time))]
    (println "n:      " (count weekend-times))
    (println "Mean:   " (s/mean weekend-times))
    (println "Median: " (s/median weekend-times))
    (println "SD:     " (s/sd weekend-times))
    (println "SE:     " (standard-error weekend-times))))

;; n:       5860
;; Mean:    117.78686006825939
;; Median:  81.0
;; SD:      120.65234077179436
;; SE:      1.5759770362547665</pre></div><p class="calibre11">The grand mean value at weekends (based on 6 months of data) is 117.8s, which falls within the 95 percent confidence interval of the marketing sample. In other words, although 130s is a high <a id="id171" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>mean dwell time, even for a weekend, the difference is not so big that it couldn't simply be attributed to chance variation within the sample.</p><p class="calibre11">The approach we have just taken to establish a genuine difference in populations (between the visitors to our site on weekends compared to the visitors during the week) is not the way statistical testing would conventionally proceed. A more usual approach is to begin with a theory, and then to test that theory against the data. The statistical method defines a rigorous <a id="id172" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>approach for this called <strong class="calibre12">hypothesis testing</strong>.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec38" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Hypothesis testing</h1></div></div></div><p class="calibre11">Hypothesis testing is a <a id="id173" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>formal process for statisticians and data scientists. The standard approach to hypothesis testing is to define an area of research, decide which variables are necessary to measure what is being studied, and then to set out two competing hypotheses. In order to avoid only looking at the data that confirms our biases, researchers will state their hypothesis clearly ahead of time. Statistics can then be used to confirm or refute this hypothesis, based on the data.</p><p class="calibre11">In order to help retain our visitors, designers go to work on a variation of our home page that uses all the latest techniques to keep the attention of our audience. We'd like to be sure that our effort isn't in vain, so we will look for an increase in dwell time on the new site.</p><p class="calibre11">Therefore, our research question is "does the new site cause the visitor's dwell time to increase"? We decide that this should be tested with reference to the mean dwell time. Now, we need to set out our two hypotheses. By convention, the data is assumed not to contain what the researcher is looking for. The conservative opinion is that the data would not show anything <a id="id174" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>unusual. This is called the <strong class="calibre12">null hypothesis</strong> and is <a id="id175" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>normally denoted <em class="calibre13">H<sub class="calibre25">0</sub></em>.</p><div><div><h3 class="title4"><a id="note22" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Hypothesis testing assumes that the null hypothesis is true until the weight of the evidence makes this proposition unlikely. This "back to front" way of looking for proof is driven partly by the simple psychological fact that when people go looking for something, they tend to find it.</p></div></div><p class="calibre11">The researcher then forms an alternate hypothesis, denoted by <em class="calibre13">H<sub class="calibre25">1</sub></em>. This could simply be that the population mean is different from the baseline. Or, it could be that the population mean is greater or lesser than the baseline, or even greater or lesser by some specified value. We'd like to test whether the new site increases dwell time, so these will be our null and alternate hypotheses:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre13">H<sub class="calibre25">0</sub></em>: The dwell time for the new site is no different than the dwell time of the existing site</li><li class="listitem"><em class="calibre13">H<sub class="calibre25">1</sub></em>: The dwell time is greater for the new site compared to the existing site</li></ul></div><p class="calibre11">Our conservative assumption is that the new site has no effect on the dwell time of users. The null hypothesis doesn't have to be nil hypothesis (that there is no effect), but in this case, we have no reasonable justification to assume otherwise. If the sample data does not support the null hypothesis (if the data differs from its prediction by a margin too large to be by chance alone), then we will reject the null hypothesis and propose the alternative hypothesis as the best alternative explanation.</p><p class="calibre11">Having set out the null and alternative hypotheses, we must set a significance level at which we are looking for an effect.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec24" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Significance</h2></div></div></div><p class="calibre11">Significance testing <a id="id176" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>was originally developed independent <a id="id177" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of hypothesis testing, but the two approaches are now very often used in concert together. The purpose of significance testing is to set the threshold beyond which we determine that the observed data no longer supports the null hypothesis.</p><p class="calibre11">There are therefore two risks:</p><div><ul class="itemizedlist"><li class="listitem">We may accept a difference as significant when in fact, it arose by chance</li><li class="listitem">We may attribute a difference to chance when, in fact, it indicates a true population difference</li></ul></div><p class="calibre11">These two possibilities are respectively referred to as Type I and Type II errors:</p><div><table border="1" class="calibre27"><colgroup class="calibre28"><col class="calibre29"/><col class="calibre29"/><col class="calibre29"/></colgroup><thead class="calibre30"><tr class="calibre31"><th valign="bottom" class="calibre32"> </th><th valign="bottom" class="calibre32">
<p class="calibre19">
<em class="calibre13">H<sub class="calibre25">0</sub></em> false</p>
</th><th valign="bottom" class="calibre32">
<p class="calibre19">
<em class="calibre13">H<sub class="calibre25">0</sub></em> true</p>
</th></tr></thead><tbody class="calibre33"><tr class="calibre34"><td class="calibre35">
<p class="calibre19">
<strong class="calibre12">Reject H<sub class="calibre25">0</sub></strong>
</p>
</td><td class="calibre35">
<p class="calibre19">True negative</p>
</td><td class="calibre35">
<p class="calibre19">Type I error</p>
</td></tr><tr class="calibre51"><td class="calibre35">
<p class="calibre19">
<strong class="calibre12">Accept </strong>
<em class="calibre13">H<sub class="calibre25">0</sub></em>
</p>
</td><td class="calibre35">
<p class="calibre19">Type II error</p>
</td><td class="calibre35">
<p class="calibre19">True positive</p>
</td></tr></tbody></table></div><p class="calibre11">The more we reduce our risk of making Type I errors, the more we increase our risk of making Type II errors. In other words, the more confident we wish to be to not claim a real difference when there is none, the bigger the difference we'll demand between our samples to claim statistical significance. This increases the probability that we'll disregard a genuine difference when we encounter it.</p><p class="calibre11">Two significance thresholds are commonly used by statisticians. These are the 5 percent and 1 percent levels. A difference at 5 percent is commonly called <em class="calibre13">significant</em> and at 1 percent is called <em class="calibre13">highly significant</em>. The choice of threshold is often referred to in formulae by the Greek letter alpha, <em class="calibre13">α</em>. Since finding no effect might be regarded as a failure (either of the experiment or of the new site), we might be tempted to adjust <em class="calibre13">α</em> until we find an effect. Because of this, the textbook approach to significance testing requires us to set a significance level before we look at our data. A level of 5 percent is often chosen, so let's go with it.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec39" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Testing a new site design</h1></div></div></div><p class="calibre11">The web team at <a id="id178" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>AcmeContent have been hard at work, developing a new site to encourage visitors to stick around for an extended period of time. They've used all the latest techniques and, as a result, we're pretty confident that the site will show a marked improvement in dwell time.</p><p class="calibre11">Rather than launching it to all users at once, AcmeContent would like to test the site on a small sample of visitors first. We've educated them about sample bias, and as a result, the web team diverts a random 5 percent of the site traffic to the new site for one day. The result is <a id="id179" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>provided to us as a single text file containing all the day's traffic. Each row shows the dwell time for a visitor who is given a value of either "0" if they used the original site design, or "1" if they saw the new (and hopefully improved) site.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec25" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Performing a z-test</h2></div></div></div><p class="calibre11">While testing <a id="id180" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with the confidence intervals previously, we had a single population mean to compare to.</p><p class="calibre11">With <em class="calibre13">z</em>-testing, we have the option of comparing two samples. The people who saw the new site were randomized, and the data for both groups was collected on the same day to rule out other time-dependent factors.</p><p class="calibre11">Since we have two samples, we also have two standard errors. The <em class="calibre13">z</em>-test is performed against the pooled standard error, which is simply the square root of the sum of the variances divided by the sample sizes. This is the same as the result we would get if we took the standard error of the samples combined:</p><div><img src="img/7180OS_02_04.jpg" alt="Performing a z-test" class="calibre57"/></div><p class="calibre11">Here, <img src="img/7180OS_02_05.jpg" alt="Performing a z-test" class="calibre58"/> is the variance of sample <em class="calibre13">a</em> and <img src="img/7180OS_02_06.jpg" alt="Performing a z-test" class="calibre58"/> is the variance of sample <em class="calibre13">b</em>. <em class="calibre13">n<sub class="calibre25">a</sub></em> and <em class="calibre13">n<sub class="calibre25">b</sub></em> are the sample sizes of <em class="calibre13">a</em> and <em class="calibre13">b</em>, respectively. The pooled standard error can be calculated in Clojure like this:</p><div><pre class="programlisting">(defn pooled-standard-error [a b]
  (i/sqrt (+ (/ (i/sq (standard-deviation a)) (count a))
             (/ (i/sq (standard-deviation b)) (count b)))))</pre></div><p class="calibre11">To determine if the difference we're seeing is unexpectedly large, we can take the ratio of the observed difference between the means over the pooled standard error. This quantity is given the variable name <em class="calibre13">z</em>:</p><div><img src="img/7180OS_02_07.jpg" alt="Performing a z-test" class="calibre59"/></div><p class="calibre11">Using our <code class="literal">pooled-standard-error</code> function, the <em class="calibre13">z</em>-statistic can be calculated like this:</p><div><pre class="programlisting">(defn z-stat [a b]
  (-&gt; (- (mean a)
         (mean b))
      (/ (pooled-standard-error a b))))</pre></div><p class="calibre11">The ratio <em class="calibre13">z</em> captures how much the means differ relative to the amount we would expect given the standard error. The <em class="calibre13">z</em>-statistic therefore tells us how many standard errors apart the means are. Since <a id="id181" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the standard error has a normal probability distribution, we can associate this difference with a probability by looking up the <em class="calibre13">z</em>-statistic in the normal CDF:</p><div><pre class="programlisting">(defn z-test [a b]
  (s/cdf-normal (z-stat a b)))</pre></div><p class="calibre11">The following example uses the <em class="calibre13">z</em>-test to compare the performance of the two sites. We do this by grouping the rows by site, returning a map that indexes the site to the collection of rows for the site. We call <code class="literal">map-vals</code> with <code class="literal">(partial map :dwell-time)</code> to convert the collection of rows into a collection of dwell times. <code class="literal">map-vals</code> is a function defined in <a id="id182" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Medley (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/weavejester/medley">https://github.com/weavejester/medley</a>), a library of lightweight utility functions:</p><div><pre class="programlisting">(defn ex-2-14 []
    (let [data (-&gt;&gt; (load-data "new-site.tsv")
                    (:rows)
                    (group-by :site)
                    (map-vals (partial map :dwell-time)))
          a (get data 0)
          b (get data 1)]
      (println "a n:" (count a))
      (println "b n:" (count b))
      (println "z-stat: " (z-stat a b))
      (println "p-value:" (z-test a b))))

;; a n: 284
;; b n: 16
;; z-stat:  -1.6467438180091214
;; p-value: 0.049805356789022426</pre></div><p class="calibre11">Setting a significance level of 5 percent is much like setting a confidence interval of 95 percent. In essence, we're looking to see if the observed difference falls outside the 95 percent confidence interval. If it does, we can claim to have found a result that's significant at the 5 <a id="id183" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>percent level.</p><div><div><h3 class="title6"><a id="note23" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The <em class="calibre13">p</em>-value is the probability of making a Type I error by wrongly rejecting the null hypothesis if it is, in fact, true. The smaller the <em class="calibre13">p</em>-value, the more certainty we have that the null hypothesis is false, and that we have found a genuine effect.</p></div></div><p class="calibre11">This code returns a value of 0.0498, equating to 4.98 percent. As it is just less than our significance threshold of 5 percent, we can claim to have found something significant.</p><p class="calibre11">Let's remind ourselves of the null and alternative hypotheses:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre13">H<sub class="calibre25">0</sub></em>: The dwell time for the new site is no different from the dwell time of the existing site</li><li class="listitem"><em class="calibre13">H<sub class="calibre25">1</sub></em>: The dwell time is greater for the new site compared to the existing site</li></ul></div><p class="calibre11">Our alternate hypothesis is that the dwell time is greater for the new site.</p><p class="calibre11">We are ready to claim statistical significance, and that the dwell time is greater for the new site compared to the existing site, but we have a problem—with a smaller sample, there is an increased uncertainty that the sample standard deviation matches the population standard deviation. Our new site sample has only 16 visitors, as shown in the output of the previous example. Samples as small as this invalidate the assumption that the standard error is normally distributed.</p><p class="calibre11">Fortunately, there is a statistical test and an associated distribution which models the increased uncertainty of standard errors for smaller sample sizes.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec26" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Student's t-distribution</h2></div></div></div><p class="calibre11">The <em class="calibre13">t</em>-distribution <a id="id184" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>was popularized by William Sealy Gossett, a chemist working for the Guinness Brewery in Ireland, who incorporated it into his analysis of Stout.</p><div><div><h3 class="title6"><a id="note24" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">William Gosset published the test in Biometrika in 1908, but was forced to use a pen name by his employer, who regarded the fact that they were using statistics as a trade secret. The pen name he chose was "Student".</p></div></div><p class="calibre11">While the normal distribution is completely described by two parameters—the mean and standard deviation, the <a id="id185" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><em class="calibre13">t</em>-distribution is described by only one parameter called the <strong class="calibre12">degrees of freedom</strong>. The larger the degrees of freedom, the closer the <em class="calibre13">t</em>-distribution resembles <a id="id186" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the normal distribution with a mean of zero and a standard deviation of one. As the degrees of freedom decreases, the distribution becomes wider with tails that are fatter than the normal distribution.</p><div><img src="img/7180OS_02_170.jpg" alt="Student's t-distribution" class="calibre60"/></div><p class="calibre11">The earlier chart shows how the <em class="calibre13">t</em>-distribution varies with respect to the normal distribution for different <a id="id187" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>degrees of freedom. Fatter tails for smaller sample sizes correspond to an increased chance of observing larger deviations from the mean.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec27" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Degrees of freedom</h2></div></div></div><p class="calibre11">The degrees of freedom, often abbreviated to <em class="calibre13">df</em>, is closely related to the sample size. It is a useful statistic and <a id="id188" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>an intuitive property of the series that can be demonstrated simply by example.</p><p class="calibre11">If you were told that the mean of two values is 10 and that one of the values is 8, you would not need any additional information to be able to infer that the other value is 12. In other words, for a sample size of two and a given mean, one of the values is constrained if the other is known.</p><p class="calibre11">If instead you're told that the mean of three values is 10 and the first value is also 10, you would not be able to deduce what the remaining two values are. Since there are an infinite number of sets of three numbers beginning with 10 whose mean is 10, the second value must also be specified before you can infer the value of the third.</p><p class="calibre11">For any set of three numbers, the constraint is simple: you can freely pick the first two numbers, but the final number is constrained. The degrees of freedom can thus be generalized in the following way: for any single sample, the degrees of freedom is one less than the sample size.</p><p class="calibre11">When comparing two samples of data, the degrees of freedom is two less than the sum of the sample sizes, which is the same as the sum of their individual degrees of freedom.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec40" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The t-statistic</h1></div></div></div><p class="calibre11">While using the <em class="calibre13">t</em>-distribution, we look up the <em class="calibre13">t</em>-statistic. Like the <em class="calibre13">z</em>-statistic, this value quantifies how <a id="id189" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>unlikely a particular observed deviation is. For a dual sample <em class="calibre13">t</em>-test, the <em class="calibre13">t</em>-statistic is calculated in the following way:</p><div><img src="img/7180OS_02_08.jpg" alt="The t-statistic" class="calibre61"/></div><p class="calibre11">Here, <img src="img/7180OS_02_09.jpg" alt="The t-statistic" class="calibre62"/> is the pooled standard error. We could calculate the pooled standard error in the same way as we did earlier:</p><div><img src="img/7180OS_02_10.jpg" alt="The t-statistic" class="calibre57"/></div><p class="calibre11">However, the equation assumes knowledge of the population parameters <em class="calibre13">σ<sub class="calibre25">a</sub></em> and <em class="calibre13">σ<sub class="calibre25">b</sub></em>, which can only be approximated from large samples. The <em class="calibre13">t</em>-test is designed for small samples and does not require us to make assumptions about population variance.</p><p class="calibre11">As a result, for <a id="id190" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the <em class="calibre13">t</em>-test, we write the pooled standard error as the square root of the sum of the standard errors:</p><div><img src="img/7180OS_02_11.jpg" alt="The t-statistic" class="calibre63"/></div><p class="calibre11">In practice, the earlier two equations for the pooled standard error yield identical results, given the same input sequences. The difference in notation just serves to illustrate that with the <em class="calibre13">t</em>-test, we depend only on sample statistics as input. The pooled standard error <img src="img/7180OS_02_09.jpg" alt="The t-statistic" class="calibre62"/> can be calculated in the following way:</p><div><pre class="programlisting">(defn pooled-standard-error [a b]
  (i/sqrt (+ (i/sq (standard-error a))
             (i/sq (standard-error b)))))</pre></div><p class="calibre11">Although they are represented differently in mathematical notation, in practice, the calculation of <em class="calibre13">t</em>-statistic is identical to <em class="calibre13">z</em>-statistic:</p><div><pre class="programlisting">(def t-stat z-stat)

(defn ex-2-15 []
    (let [data (-&gt;&gt; (load-data "new-site.tsv")
                    (:rows)
                    (group-by :site)
                    (map-vals (partial map :dwell-time)))
          a (get data 0)
          b (get data 1)]
      (t-stat a b)))

;; -1.647</pre></div><p class="calibre11">The difference between the two statistics is conceptual rather than algorithmic—the <em class="calibre13">z</em>-statistic is only <a id="id191" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>applicable when the samples follow a normal distribution.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec41" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Performing the t-test</h1></div></div></div><p class="calibre11">The difference in the <a id="id192" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>way <em class="calibre13">t</em>-test works stems from the probability distribution from which our <em class="calibre13">p</em>-value is calculated. Having calculated our <em class="calibre13">t</em>-statistic, we need to look up the value in the <em class="calibre13">t</em>-distribution parameterized by the degrees of freedom of our data:</p><div><pre class="programlisting">(defn t-test [a b]
  (let [df (+ (count a) (count b) -2)]
    (- 1 (s/cdf-t (i/abs (t-stat a b)) :df df))))</pre></div><p class="calibre11">The degrees of freedom are two less than the sizes of the samples combined, which is 298 for our samples.</p><div><img src="img/7180OS_02_180.jpg" alt="Performing the t-test" class="calibre56"/></div><p class="calibre11">Recall that we are <a id="id193" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>performing a hypothesis test. So, let's state our null and alternate hypotheses:</p><div><ul class="itemizedlist"><li class="listitem"><em class="calibre13">H<sub class="calibre25">0</sub></em>: This sample is drawn from a population with a supplied mean</li><li class="listitem"><em class="calibre13">H<sub class="calibre25">1</sub></em>: This sample is drawn from a population with a greater mean</li></ul></div><p class="calibre11">Let's run the example:</p><div><pre class="programlisting">(defn ex-2-16 []
  (let [data (-&gt;&gt; (load-data "new-site.tsv")
                  (:rows)
                  (group-by :site)
                  (map-vals (partial map :dwell-time)))
        a (get data 0)
        b (get data 1)]
    (t-test a b)))

;; 0.0503</pre></div><p class="calibre11">This returns a <em class="calibre13">p</em>-value of over 0.05. Since this is greater than the <em class="calibre13">α</em> of 5% we set for our hypothesis test, we are not able to reject the null hypothesis. Our test for the difference between the means has <a id="id194" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>not discovered a significant difference using the <em class="calibre13">t</em>-test. Our barely significant result of the <em class="calibre13">z</em>-test was therefore partly due to it having such a small sample.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec28" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Two-tailed tests</h2></div></div></div><p class="calibre11">There has been an <a id="id195" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>implicit assumption in our alternate hypothesis that the new site would perform better than the previous site. The process of hypothesis testing goes to great lengths to ensure that we don't encode hidden assumptions while looking for statistical significance.</p><p class="calibre11">Tests where we look only for a significant increase or decrease in quantity are called <strong class="calibre12">one-tailed tests</strong> and are <a id="id196" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>generally frowned upon, except in the case where a change in the opposite direction would be impossible. The name comes from the fact that a one-tailed test allocates all of the <em class="calibre13">α</em> to a single tail of the distribution. By not testing in the other direction, the test has more power to reject the null hypothesis in a particular direction and, in essence, lowers the threshold by which we would judge a result as significant.</p><div><div><h3 class="title6"><a id="note25" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Statistical power is the probability of correctly accepting the alternative hypothesis. This can be thought of as the ability of the test to detect an effect, where there is an effect to be detected.</p></div></div><p class="calibre11">While higher statistical power sounds desirable, it comes at the cost of there being a greater probability of making a Type I error. A more correct approach would be to entertain the possibility that the new site could realistically be worse than the existing site. This allocates our <em class="calibre13">α</em> equally to both tails of the distribution and ensures a significant outcome that is not biased by a prior assumption of improvement.</p><div><img src="img/7180OS_02_190.jpg" alt="Two-tailed tests" class="calibre64"/></div><p class="calibre11">In fact, Incanter already provides functions to perform two-sample <em class="calibre13">t</em>-tests with the <code class="literal">s/t-test</code> function. We <a id="id197" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>provide a sample of data as the first argument and a sample to compare against with the <code class="literal">:y</code> keyword argument. Incanter will assume that we want to perform a two-tailed test, unless we pass the <code class="literal">:alternative</code> keyword with a value of <code class="literal">:greater </code>or <code class="literal">:lower</code>, in which case a one-tailed test will be performed.</p><div><pre class="programlisting">(defn ex-2-17 []
  (let [data (-&gt;&gt; (load-data "new-site.tsv")
                  (:rows)
                  (group-by :site)
                  (map-vals (partial map :dwell-time)))
        a (get data 0)
        b (get data 1)]
    (clojure.pprint/print (s/t-test a :y b))))

;; {:p-value 0.12756432502462456,
;;  :df 17.7613823496861,
;;  :n2 16,
;;  :x-mean 87.95070422535211,
;;  :y-mean 122.0,
;;  :x-var 10463.941024237305,
;;  :conf-int [-78.9894629402365 10.890871390940724],
;;  :y-var 6669.866666666667,
;;  :t-stat -1.5985205593851322,
;;  :n1 284}</pre></div><p class="calibre11">Incanter's <em class="calibre13">t</em>-test returns a lot of information, including the <em class="calibre13">p</em>-value. The <em class="calibre13">p</em>-value is around twice what we <a id="id198" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculated for the one-tailed test. In fact, the only reason it's not exactly double is because Incanter implements a slight variant of the <em class="calibre13">t</em>-test called <strong class="calibre12">Welch's t-test</strong>, which is slightly more robust when two samples have different standard <a id="id199" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>deviations. Since we know that, for exponential distributions, the mean and the variance are intimately related, the test is slightly more rigorous to apply and returns an even lower significance.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec42" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>One-sample t-test</h1></div></div></div><p class="calibre11">Independent <a id="id200" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>samples of <em class="calibre13">t</em>-tests are the most common sort of statistical analysis, which provide a very flexible and generic way of comparing whether two samples represent the same or different population. However, in cases where the population mean is already known, there is an even simpler test represented by <code class="literal">s/simple-t-test</code>.</p><p class="calibre11">We pass a sample and a population mean to test against with the <code class="literal">:mu</code> keyword. So, if we simply want to see whether our new site is significantly different from the previous population mean dwell time of 90s, we can run a test like this:</p><div><pre class="programlisting">(defn ex-2-18 []
  (let [data (-&gt;&gt; (load-data "new-site.tsv")
                  (:rows)
                  (group-by :site)
                  (map-vals (partial map :dwell-time)))
        b (get data 1)]
    (clojure.pprint/pprint (s/t-test b :mu 90))))

;; {:p-value 0.13789520958229406,
;;  :df 15,
;;  :n2 nil,
;;  :x-mean 122.0,
;;  :y-mean nil,
;;  :x-var 6669.866666666667,
;;  :conf-int [78.48152745280898 165.51847254719104],
;;  :y-var nil,
;;  :t-stat 1.5672973291495713,
;;  :n1 16}</pre></div><p class="calibre11">The <code class="literal">simple-t-test</code> function returns not only the <em class="calibre13">p</em>-value for the test, but also the confidence interval for <a id="id201" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the population mean. It is wide, running from 78.5s to 165.5s, certainly overlapping with the 90s of our test. This explains why we were not able to reject the null hypothesis.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec43" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Resampling</h1></div></div></div><p class="calibre11">To develop an <a id="id202" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>intuition as to how the <em class="calibre13">t</em>-test can confirm and calculate these statistics from so little data, we can apply an approach called <strong class="calibre12">resampling</strong>. Resampling is based on the premise that each sample is just one of an infinite number of possible samples from a population. We can gain an insight into the nature of what these other samples could have been, and therefore have a better understanding of the underlying population, by taking many new samples from our existing sample.</p><p class="calibre11">There are actually several resampling techniques, and we'll discuss one of the simplest—bootstrapping. In bootstrapping, we generate a new sample by repeatedly taking a random value from the original sample with replacement until we generate a sample that is of the same size as the original. Because these values are replaced between each random selection, the same source value can appear multiple times in the new sample. It is as if we were drawing a random card from a deck of playing cards repeatedly, but replacing the card after each draw. Occasionally, we will pick a card that we have previously selected.</p><p class="calibre11">We can bootstrap our sample easily in Incanter to generate many resamples with the bootstrap function. The <code class="literal">bootstrap</code> function takes two arguments—the original sample and a summary statistic to be calculated on the bootstrapped samples as well as the number of optional arguments—<code class="literal">:size</code> (the number of bootstrapped samples to be calculated on, each sample being the size of the original sample), <code class="literal">:smooth</code> (whether to smooth the output of discrete statistics such as the median), <code class="literal">:smooth-sd</code>, and <code class="literal">:replacement</code>, which defaults to true:</p><div><pre class="programlisting">(defn ex-2-19 []
  (let [data (-&gt;&gt; (load-data "new-site.tsv")
                  (i/$where {:site {:$eq 1}})
                  (i/$ :dwell-time ))]
    (-&gt; (s/bootstrap data s/mean :size 10000)
        (c/histogram :nbins 20
                     :x-label "Bootstrapped mean dwell times (s)")
        (i/view))))</pre></div><p class="calibre11">Let's visualize <a id="id203" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the output in a histogram:</p><div><img src="img/7180OS_02_200.jpg" alt="Resampling" class="calibre45"/></div><p class="calibre11">The histogram shows how the values of the mean value have changed with repeated (re) samples of the new site dwell times. Although the input was just a single sample of 16 visitors, the bootstrapped samples have simulated the standard error of our original sample very clearly and visualized the confidence interval (78s to 165s) calculated earlier by our single sample <em class="calibre13">t</em>-test.</p><p class="calibre11">Through bootstrapping, we simulated by taking multiple samples, even though we only had one sample as our <a id="id204" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>input. It's a generally useful technique to estimate parameters that we cannot or do not know to calculate analytically.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec44" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Testing multiple designs</h1></div></div></div><p class="calibre11">It's been disappointing to <a id="id205" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>discover that there is no statistical significance behind the increased dwell time of users on the new site design. Better that we discovered this on a small sample of users before we rolled it out to the world though.</p><p class="calibre11">Not to be discouraged, AcmeContent's web team works overtime and devises a suite of alternative site designs. Taking the best elements from the other designs, they devise 19 variations to be tested. Together with our original site, which will act as a control, there are 20 different sites to direct visitors to.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec29" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating sample means</h2></div></div></div><p class="calibre11">The web team <a id="id206" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>deploys the 19 new site designs alongside the original site. As mentioned earlier, each receives a random 5 percent of the visitors. We let the test run for 24 hours.</p><p class="calibre11">The next day, we receive a file that shows the dwell times for visitors to each of the site designs. Each has been labeled with a number, with site <code class="literal">0</code> corresponding to the original unaltered design, and numbers <code class="literal">1</code> to <code class="literal">19</code> representing the other designs:</p><div><pre class="programlisting">(defn ex-2-20 []
  (-&gt;&gt; (i/transform-col (load-data "multiple-sites.tsv")
                        :dwell-time float)
       (i/$rollup :mean :dwell-time :site)
       (i/$order :dwell-time :desc)
       (i/view)))</pre></div><p class="calibre11">This code generates the following table:</p><div><img src="img/7180OS_02_210.jpg" alt="Calculating sample means" class="calibre65"/></div><p class="calibre11">We would like to test out each of the site designs to see if any generate a statistically significant result. To <a id="id207" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>do so, we could compare the sites with each other as follows:</p><div><pre class="programlisting">(defn ex-2-21 []
  (let [data (-&gt;&gt; (load-data "multiple-sites.tsv")
                  (:rows)
                  (group-by :site)
                  (map-vals (partial map :dwell-time)))
        alpha 0.05]
    (doseq [[site-a times-a] data
            [site-b times-b] data
            :when (&gt; site-a site-b)
            :let [p-val (-&gt; (s/t-test times-a :y times-b)
                            (:p-value))]]
      (when (&lt; p-val alpha)
        (println site-b "and" site-a
                 "are significantly different:"
                 (format "%.3f" p-val))))))</pre></div><p class="calibre11">However, this would be a bad idea. We are very likely to see a statistical difference between the pages that performed particularly well against the pages that performed particularly poorly, even if these differences were by chance. If you run the earlier example, you'll see that many of the pages are statistically different from each other.</p><p class="calibre11">Alternatively, we could compare each site against our current baseline—the mean dwell time of 90 seconds currently measured for our site:</p><div><pre class="programlisting">(defn ex-2-22 []
  (let [data (-&gt;&gt; (load-data "multiple-sites.tsv")
                  (:rows)
                  (group-by :site)
                  (map-vals (partial map :dwell-time)))
        baseline (get data 0)
        alpha 0.05]
    (doseq [[site-a times-a] data
            :let [p-val (-&gt; (s/t-test times-a :y baseline)
                            (:p-value))]]
      (when (&lt; p-val alpha)
        (println site-a
                 "is significantly different from baseline:"
                 (format "%.3f" p-val))))))</pre></div><p class="calibre11">This test determines two sites as being significantly different from the baseline:</p><div><pre class="programlisting">;; 6 is significantly different from baseline: 0.007
;; 10 is significantly different from baseline: 0.006</pre></div><p class="calibre11">The small <em class="calibre13">p</em>-values (smaller than 1 percent) indicate that there is a very statistically significant difference. This looks very promising, but we have an issue. We have performed a <em class="calibre13">t</em>-test on 20 samples of data with an <em class="calibre13">α</em> of 0.05. The definition of <em class="calibre13">α</em> is that it is the probability of wrongly rejecting the null hypothesis. By running a <em class="calibre13">t</em>-test 20 times, it actually becomes probable that we would wrongly reject the null hypothesis for at least one of the pages.</p><p class="calibre11">By comparing multiple <a id="id208" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>pages at once like this, we invalidate the results of the <em class="calibre13">t</em>-test. There exist a variety of alternative techniques to address the problem of making multiple comparisons in statistical tests, which we'll introduce in a later section.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec45" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Multiple comparisons</h1></div></div></div><p class="calibre11">The fact that with <a id="id209" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>repeated trials, we increase the probability of discovering a significant effect is called the multiple comparisons problem. In general, the solution to the problem is to demand more significant effects when comparing many samples. There is no straightforward solution to this issue though; even with an <em class="calibre13">α</em> of 0.01, we will make a Type I error on an average of 1 percent of the time.</p><p class="calibre11">To develop our intuition about how multiple comparisons and statistical significance relate to each other, let's build an interactive web page to simulate the effect of taking multiple samples. It's one of the advantages of using a powerful and general-purpose programming language like Clojure for data analysis that we can run our data processing code in a diverse array of environments.</p><p class="calibre11">The code we've written and run so far for this chapter has been compiled for the Java Virtual Machine. But <a id="id210" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>since 2013, there has been an alternative target environment for our compiled code: the web browser. ClojureScript extends the reach of Clojure even further to any computer that has a JavaScript-enabled web browser.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec30" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Introducing the simulation</h2></div></div></div><p class="calibre11">To help visualize the <a id="id211" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>problems associated with multiple significance testing, we'll use ClojureScript to build an interactive simulation, looking for statistically significant differences between the samples drawn at random from two exponential distributions. To see how other factors relate to our hypothesis testing, our simulation will allow us to change the underlying population mean for each of the two distributions, as well as set the sample size and desired confidence level.</p><p class="calibre11">If you have downloaded the sample code for this chapter, you will see, in the resources directory, an <code class="literal">index.html</code> file. If you open this code in a web browser, you should see a message prompting you to compile the JavaScript. We can do this with the Leiningen plugin called <code class="literal">cljsbuild</code>.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec31" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Compile the simulation</h2></div></div></div><p class="calibre11">
<code class="literal">cljsbuild</code> is a <a id="id212" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Leiningen plugin that compiles ClojureScript to JavaScript. To use it, we simply have to let the compiler know where we would like to output the JavaScript file. While Clojure code outputs to a <code class="literal">.jar</code> file (short for Java Archive), ClojureScript outputs to a single <code class="literal">.js</code> file. We specify the name of the output file and the compiler settings to use with the <code class="literal">:cljsbuilds</code> section of <code class="literal">project.clj</code>.</p><p class="calibre11">The plugin is accessible on the command line as <code class="literal">lein cljsbuild</code>. In the root of the project directory, run the following command:</p><div><pre class="programlisting">
<strong class="calibre12">lein cljsbuild once</strong>
</pre></div><p class="calibre11">This command will compile a JavaScript file for us. An alternative command is as follows:</p><div><pre class="programlisting">
<strong class="calibre12">lein cljsbuild auto</strong>
</pre></div><p class="calibre11">The preceding will compile the code, but will remain active, monitoring changes to the source files. If any of these files are updated, the output will be recompiled.</p><p class="calibre11">Open the <a id="id213" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>file <code class="literal">resources/index.html</code> in a web browser now to see the effect of the JavaScript.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec46" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The browser simulation</h1></div></div></div><p class="calibre11">An HTML <a id="id214" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>page has been supplied in the resources directory of the sample project. Open the page in any modern browser and you should see something similar to the following image:</p><div><img src="img/7180OS_02_220.jpg" alt="The browser simulation" class="calibre66"/></div><p class="calibre11">The left of the page shows a dual histogram with the distribution of two samples, both taken from an exponential distribution. The means of the populations from which the samples are generated are controlled by the sliders at the top right corner of the web page in the box marked as <strong class="calibre12">Parameters</strong>. Underneath the histogram is a plot showing the two probability densities for the population means based on the samples. These are calculated using the <em class="calibre13">t</em>-distribution, parameterized by the degrees of freedom of the sample. Below these sliders, in a box marked as <strong class="calibre12">Settings</strong>, are another pair of sliders that set the sample size and confidence intervals for the test. Adjusting the confidence intervals will crop the tails of the <em class="calibre13">t</em>-distributions; at the 95 percent confidence interval, only the central 95 percent of the probability distributions are displayed. Finally, in a box marked as <strong class="calibre12">Statistics</strong>, are the sliders that show the mean of both the samples. These cannot be changed; their values are measured from the samples. A button marked as <strong class="calibre12">New Sample</strong> can be used to generate two new random samples. Observe how the sample means fluctuate with each new pair of samples being generated. Keep generating samples and you'll occasionally observe significant differences between sample means, even when the underlying population means are identical.</p><p class="calibre11">While we explore <a id="id215" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the effects of changing the sample size and the confidence for different population means, let's look at how the simulation was constructed with the libraries <code class="literal">jStat</code>, <code class="literal">Reagent</code>, and <code class="literal">B1</code>.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec47" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>jStat</h1></div></div></div><p class="calibre11">As ClojureScript <a id="id216" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>compiles to JavaScript, we can't make use of the libraries that have Java dependencies. Incanter is heavily reliant on several underlying Java libraries, so we have to find an alternative to Incanter for our browser-based statistical analysis.</p><div><div><h3 class="title4"><a id="note26" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">While building ClojureScript applications, we can't make use of the libraries that depend on Java libraries, as they won't be available in the JavaScript engine which executes our code.</p></div></div><p class="calibre11">
<code class="literal">jStat</code> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/jstat/jstat">https://github.com/jstat/jstat</a>) is a JavaScript statistical library. It provides functions to <a id="id217" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>generate sequences according to specific distributions, including the exponential and <em class="calibre13">t</em>-distributions.</p><p class="calibre11">To use it, we have to make sure it's available on our webpage. We can do this either by linking it to a remote <a id="id218" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><strong class="calibre12">content distribution network</strong> (<strong class="calibre12">CDN</strong>) or by hosting the file ourselves. The advantage of linking it to a CDN is that visitors, who previously downloaded <code class="literal">jStat</code> for another website, can make use of their cached version. However, since our simulation is for local use, we've included the file so that the page works even when our browser is offline.</p><p class="calibre11">The <code class="literal">jstat.min.js</code> file has been downloaded in the <code class="literal">resources/js/vendor</code> directory. The file is loaded in the main body of <code class="literal">index.html</code> with a standard HTML tag.</p><p class="calibre11">To make use of jStat's distribution generating functions, we have to interact with the JavaScript library from ClojureScript. As with the Java interop, Clojure provides pragmatic syntax to interact with the libraries written in the host language.</p><p class="calibre11">
<code class="literal">jStat</code> provides a variety of distributions documented at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://jstat.github.io/distributions.html">https://jstat.github.io/distributions.html</a>. To generate samples from an exponential <a id="id219" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>distribution, we'd like to call the <code class="literal">jStat.exponential.sample(lambda)</code> function. The JavaScript interop for it is very straightforward; we prefix the expression with <code class="literal">js/</code> to ensure that we access JavaScript's namespace and move the position of the brackets:</p><div><pre class="programlisting">(defn randexp [lambda]
  (js/jStat.exponential.sample lambda))</pre></div><p class="calibre11">Once we have the ability to generate samples from an exponential distribution, creating a lazy sequence of samples will be as simple as calling the function repeatedly:</p><div><pre class="programlisting">(defn exponential-distribution [lambda]
  (repeatedly #(randexp lambda)))</pre></div><p class="calibre11">ClojureScript exposes almost all of Clojure, including lazy sequences. Refer to the book's wiki at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://wiki.clojuredatascience.com">http://wiki.clojuredatascience.com</a> for links to resources on the JavaScript interop.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec48" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>B1</h1></div></div></div><p class="calibre11">Now that we can generate samples of data in ClojureScript, we'd like to be able to plot them on a histogram. We <a id="id220" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>need a pure Clojure alternative to Incanter that will draw histograms in a web-accessible format; the B1 library (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/henrygarner/b1">https://github.com/henrygarner/b1</a>) provides just this functionality. The name is derived from the fact <a id="id221" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that it is adapted and simplified from the ClojureScript library <code class="literal">C2</code>, which in turn is a simplification of the popular JavaScript data visualization framework <code class="literal">D3</code>.</p><p class="calibre11">We'll be using B1's simple utility functions in <code class="literal">b1.charts</code> to build histograms out of our data in ClojureScript. B1 does not mandate a particular display format; we could use it to draw on a canvas element or even to build diagrams directly out of the HTML elements. However, B1 does contain functions to convert charts to SVG in <code class="literal">b1.svg</code> and these can be displayed in all modern web browsers.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec32" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Scalable Vector Graphics</h2></div></div></div><p class="calibre11">SVG stands for Scalable Vector Graphics and defines a set of tags that represent drawing instructions. The <a id="id222" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>advantage of SVG is that results can be rendered at any size without the blurring associated with raster (pixel-based) graphics that are scaled up. An additional benefit is that modern browsers know how to render SVG drawing instructions to produce images directly in the web page and can style and animate the images with CSS.</p><p class="calibre11">Although a detailed discussion of SVG and CSS is beyond the scope of this book, B1 does provide syntax that is very much like Incanter's to build simple charts and graphs using SVG. Given a sequence of values, we call the <code class="literal">c/histogram</code> function to convert it into an internal representation of the data structure. We can add additional histograms with the <code class="literal">c/add-histogram</code> function and call <code class="literal">svg/as-svg </code>to render the chart to an SVG representation:</p><div><pre class="programlisting">(defn sample-histograms [sample-a sample-b]
  (-&gt; (c/histogram sample-a :x-axis [0 200] :bins 20)
      (c/add-histogram sample-b)
      (svg/as-svg :width 550 :height 400)))</pre></div><p class="calibre11">Unlike Incanter, when we choose to render our histogram, we must also specify the desired width and height of the chart.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec49" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Plotting probability densities</h1></div></div></div><p class="calibre11">In addition to using <a id="id223" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>jStat to generate samples from the exponential distribution, we'll also use it to calculate the probability density for the <em class="calibre13">t</em>-distribution. We can construct a simple function to wrap the <code class="literal">jStat.studentt.pdf(t, df)</code> function, providing the correct <em class="calibre13">t</em>-statistic and degrees of freedom to parameterize the distribution:</p><div><pre class="programlisting">(defn pdf-t [t &amp; {:keys [df]}]
  (js/jStat.studentt.pdf t df))</pre></div><p class="calibre11">An advantage of using ClojureScript is that we have already written the code to calculate the <em class="calibre13">t</em>-statistic from a sample. The code, which worked in Clojure, can be compiled to ClojureScript with no changes whatsoever:</p><div><pre class="programlisting">(defn t-statistic [test {:keys [mean n sd]}]
  (/ (- mean test)
     (/ sd (Math/sqrt n))))</pre></div><p class="calibre11">To render the probability density, we can use B1's <code class="literal">c/function-area-plot</code>. This will generate an area plot from the line described by a function. The provided function simply needs to accept an <em class="calibre13">x</em> and return the corresponding <em class="calibre13">y</em>.</p><p class="calibre11">A slight complication is that the value of <em class="calibre13">y</em> we return will be different for different samples. This is because <code class="literal">t-pdf</code> will be highest at the sample mean (corresponding to a <em class="calibre13">t</em>-statistic of zero). Because of this, we'll need to generate a different function for each sample to be passed to <code class="literal">function-area-plot</code>. This is accomplished by the <code class="literal">probability-density</code> <a id="id224" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function, as follows:</p><div><pre class="programlisting">(defn probability-density [sample alpha]
  (let [mu (mean sample)
        sd (standard-deviation sample)
        n  (count sample)]
    (fn [x]
      (let [df     (dec (count sample))
            t-crit (threshold-t 2 df alpha)
            t-stat (t-statistic x {:mean mu
                                   :sd sd
                                   :n n})]
        (if (&lt; (Math/abs t-stat) t-crit)
          (pdf-t t-stat :df df)
          0)))))</pre></div><p class="calibre11">Here, we're defining a higher-order function called <code class="literal">probability-density</code> that accepts a single value, <code class="literal">sample</code>. We calculate some simple summary statistics and then return an anonymous function that calculates the probability density for a given value in the distribution.</p><p class="calibre11">This anonymous function is what will be passed to <code class="literal">function-area-plot</code>. It accepts an <em class="calibre13">x</em> and calculates a <em class="calibre13">t</em>-statistic for the given sample from it. The <em class="calibre13">y</em> value returned is the probability of the <em class="calibre13">t</em>-distribution associated with the <em class="calibre13">t</em>-statistic:</p><div><pre class="programlisting">(defn sample-means [sample-a sample-b alpha]
  (-&gt; (c/function-area-plot (probability-density sample-a alpha)
                            :x-axis [0 200])
      (c/add-function (probability-density sample-b alpha))
      (svg/as-svg :width 550 :height 250)))</pre></div><p class="calibre11">As with histograms, generating multiple plots is as straightforward as calling <code class="literal">add-function </code>with the chart, and the new function we'd like to add.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec50" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>State and Reagent</h1></div></div></div><p class="calibre11">State in <a id="id225" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>ClojureScript is managed in the same way as Clojure applications—through the use of atoms, refs, or agents. Atoms provide uncoordinated, synchronous access to a single identity and are an excellent choice for storing the application state. Using an atom ensures that the application always sees a single, consistent view of the data.</p><p class="calibre11">Reagent is a <a id="id226" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>ClojureScript library that provides a mechanism to update the content of a web page in response to changing the value of an atom. Markup and state are bound together, so that markup is regenerated whenever the application state is updated.</p><p class="calibre11">Reagent also provides syntax to render HTML in an idiomatic way using Clojure data structures. This means that both the content and the interactivity of the page can be handled in one language.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec33" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Updating state</h2></div></div></div><p class="calibre11">With data held in a <a id="id227" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Reagent atom, updating the state is achieved by calling the <code class="literal">swap!</code> function with two arguments—the atom we wish to update and a function to transform the state of the atom. The provided function needs to accept the current state of the atom and return the new state. The exclamation mark indicates that the function has side effects and, in this case, the side effects are desirable; in addition to updating the atom, Reagent will ensure that relevant sections of our HTML page are updated.</p><p class="calibre11">The exponential distribution has a single parameter—the rate symbolized by lambda, <em class="calibre13">λ</em>. The rate of an exponential distribution is the reciprocal of the mean, so we calculate <code class="literal">(/ 1 mean-a) </code>to pass it as the argument to the exponential distribution function:</p><div><pre class="programlisting">(defn update-sample [{:keys [mean-a mean-b sample-size]
                      :as state}]
  (let [sample-a (-&gt;&gt; (float (/ 1 mean-a))
                      (exponential-distribution)
                      (take sample-size))
        sample-b (-&gt;&gt; (float (/ 1 mean-b))
                      (exponential-distribution)
                      (take sample-size))]
    (-&gt; state
        (assoc :sample-a sample-a)
        (assoc :sample-b sample-b)
        (assoc :sample-mean-a (int (mean sample-a)))
        (assoc :sample-mean-b (int (mean sample-b))))))

(defn update-sample! [state]
  (swap! state update-sample))</pre></div><p class="calibre11">In the preceding code, we have defined an <code class="literal">update-sample</code> function that accepts a map containing <code class="literal">:sample-size</code>, <code class="literal">:mean-a</code>, and <code class="literal">:mean-b</code>, and returns a new map with the associated new samples and sample means.</p><p class="calibre11">The <code class="literal">update-sample</code> <a id="id228" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function is pure in the sense that it doesn't have side effects, which makes it easier to test. The <code class="literal">update-sample!</code> function wraps it with a call to <code class="literal">swap!</code>. Reagent ensures that any code that depends on the value contained in this atom will be executed when the value in the atom changes. This causes our interface to be re-rendered in response to the new samples.</p></div><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec34" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Binding the interface</h2></div></div></div><p class="calibre11">To bind the interface <a id="id229" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to the state, Reagent defines a <code class="literal">render-component</code> function. This links a particular function (in this case, our <code class="literal">layout-interface</code> function) with a particular HTML node (the element with the ID <code class="literal">root</code> on our page):</p><div><pre class="programlisting">(defn layout-interface []
  (let [sample-a (get @state :sample-a)
        sample-b (get @state :sample-b)
        alpha (/ (get @state :alpha) 100)]
    [:div
     [:div.row
      [:div.large-12.columns
       [:h1 "Parameters &amp; Statistics"]]]
     [:div.row
      [:div.large-5.large-push-7.columns
       [controllers state]]
      [:div.large-7.large-pull-5.columns {:role :content}
       [sample-histograms sample-a sample-b]
       [sample-means sample-a sample-b alpha]]]]))

(defn run []
  (r/render-component
   [layout-interface]
   (.getElementById js/document "root")))</pre></div><p class="calibre11">Our <code class="literal">layout-interface</code> function contains an HTML markup expressed as nested Clojure data structures. Amongst the calls to <code class="literal">:div</code> and <code class="literal">:h1</code>, elements are calls to our two <code class="literal">sample-histograms</code> and <code class="literal">sample-means</code> functions. They will be substituted with their return values—the SVG representations of the histograms and the probability densities of the means.</p><p class="calibre11">For the sake of brevity, we <a id="id230" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have omitted the implementation of the <code class="literal">controllers</code> function, which handles the rendering of the sliders and the <strong class="calibre12">New Sample</strong> button. Consult the <code class="literal">cljds.ch2.app</code> namespace in the sample code to see how this is implemented.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec51" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Simulating multiple tests</h1></div></div></div><p class="calibre11">Each time the <strong class="calibre12">New </strong><a id="id231" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><strong class="calibre12">Sample</strong> button is pressed, a pair of new samples from an exponential distribution with population means taken from the sliders are generated. The samples are plotted on a histogram and, underneath, a probability density function is drawn showing the standard error for the sample. As the confidence intervals are changed, observe how the acceptable deviation of the standard error changes as well.</p><p class="calibre11">Each time the button is pressed, we could think of it as a significance test with an alpha set to the complement of the confidence interval. In other words, if the probability distributions for the sample means overlap at the 95 percent confidence interval, we cannot reject the null hypothesis at the 5 percent significance level.</p><p class="calibre11">Observe how, even when the population means are identical, occasional large deviations in the means will occur. Where samples differ by more than our standard error, we can accept the alternate hypothesis. With a confidence level of 95 percent, we will discover a significant result around one in 20 trials, even when the population means of the distributions are identical. When this happens, we are making a Type 1 error in mistaking a sampling error for a real population difference.</p><div><img src="img/7180OS_02_230.jpg" alt="Simulating multiple tests" class="calibre67"/></div><p class="calibre11">Despite the identical <a id="id232" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>population parameters, large sample differences are occasionally observed.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec52" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The Bonferroni correction</h1></div></div></div><p class="calibre11">We therefore <a id="id233" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>require an alternative approach while conducting multiple tests that will account for an increased probability of discovering a significant effect through repeated trials. The Bonferroni correction is a very simple adjustment that ensures we are unlikely to make Type I errors. It does this by adjusting the alpha for our tests.</p><p class="calibre11">The adjustment is a simple one—the Bonferroni correction simply divides our desired alpha by the number of tests we are performing. For example, if we had <em class="calibre13">k</em> site designs to test and an experimental alpha of <em class="calibre13">0.05</em>, the Bonferroni correction is expressed as:</p><div><img src="img/7180OS_02_12.jpg" alt="The Bonferroni correction" class="calibre68"/></div><p class="calibre11">This is a safe way to mitigate the increased probability of making a Type I error in multiple testing. The following example is identical to <code class="literal">ex-2-22</code>, except the alpha value has been divided by the number of groups:</p><div><pre class="programlisting">(defn ex-2-23 []
  (let [data (-&gt;&gt; (load-data "multiple-sites.tsv")
                  (:rows)
                  (group-by :site)
                  (map-vals (partial map :dwell-time)))
        alpha (/ 0.05 (count data))]
    (doseq [[site-a times-a] data
            [site-b times-b] data
            :when (&gt; site-a site-b)
            :let [p-val (-&gt; (s/t-test times-a :y times-b)
                            (:p-value))]]
      (when (&lt; p-val alpha)
        (println site-b "and" site-a
                 "are significantly different:"
                 (format "%.3f" p-val))))))</pre></div><p class="calibre11">If you run the preceding example, you'll see that none of the pages count as statistically significant any longer using the Bonferroni correction.</p><p class="calibre11">Significance testing is a balancing act—the lower our chances of making a Type I error, the greater our risk <a id="id234" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of making a Type II error. The Bonferroni correction is very conservative and it's possible that we're missing a genuine difference due to being so cautious.</p><p class="calibre11">In the final part of this chapter, we'll investigate an alternative approach to significance testing that strikes a balance between making Type I and Type II errors while allowing us to test all the 20 pages simultaneously.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec53" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Analysis of variance</h1></div></div></div><p class="calibre11">Analysis of variance, often <a id="id235" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>shortened to <strong class="calibre12">ANOVA</strong>, is a series of statistical methods used to measure the statistical significance of the difference between groups. It was developed by Ronald Fisher, an extremely gifted statistician, who also popularized significance testing through his work on biological testing.</p><p class="calibre11">Our tests, using the <em class="calibre13">z</em>-statistic and <em class="calibre13">t</em>-statistic, have focused on sample means as the primary mechanism to draw a distinction between the two samples. In each case, we looked for a difference in the means divided by the level of difference we could reasonably expect and quantified by the standard error.</p><p class="calibre11">The mean isn't the <a id="id236" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>only statistic that might indicate a difference between samples. In fact, it is also possible to use the sample variance as an indicator of statistical difference.</p><div><img src="img/7180OS_02_240.jpg" alt="Analysis of variance" class="calibre69"/></div><p class="calibre11">To illustrate how this might work, consider the preceding diagram. Each of the three groups on the left could represent samples of dwell times for a specific page with its own mean and standard deviation. If the dwell times for all the three groups are combined into one, the variance is larger than the average variance for the groups taken individually.</p><p class="calibre11">The statistical significance of an ANOVA test is derived from the ratio of two variances—the variance <em class="calibre13">between</em> the groups of interest and the variance <em class="calibre13">within</em> the groups of interest. If there is a significant difference between the groups that is not reflected within the groups, then those groupings help explain some of the variance between the groups. Conversely, if the <a id="id237" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>variance within the groups is identical to the variance between the groups, the groups are not statistically different from one another.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec54" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The F-distribution</h1></div></div></div><p class="calibre11">The <em class="calibre13">F</em>-distribution is parameterized by two degrees of freedom—those of the sample size and those of the number of groups.</p><p class="calibre11">The first degree of <a id="id238" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>freedom is the count of groups less one and the second degree of freedom is the size of the sample less the number of groups. If <em class="calibre13">k</em> represents the number of groups, and <em class="calibre13">n</em> represents the sample size:</p><div><img src="img/7180OS_02_13.jpg" alt="The F-distribution" class="calibre70"/></div><div><img src="img/7180OS_02_14.jpg" alt="The F-distribution" class="calibre71"/></div><p class="calibre11">We can visualize different <em class="calibre13">F</em>-distributions with an Incanter function plot:</p><div><img src="img/7180OS_02_250.jpg" alt="The F-distribution" class="calibre45"/></div><p class="calibre11">The lines of the <a id="id239" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>preceding diagram show various <em class="calibre13">F</em>-distributions for a sample of 100 points split into 5, 10, and 50 groups.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec55" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The F-statistic</h1></div></div></div><p class="calibre11">The test statistic that <a id="id240" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>represents the ratio of the variance within and between the groups is called the <em class="calibre13">F</em>-statistic. The closer <em class="calibre13">F</em>-statistic is to one, the more alike the two variances are. The <em class="calibre13">F</em>-statistic is calculated very simply as follows:</p><div><img src="img/7180OS_02_15.jpg" alt="The F-statistic" class="calibre72"/></div><p class="calibre11">Here, <img src="img/7180OS_02_16.jpg" alt="The F-statistic" class="calibre73"/> is the <em class="calibre13">variance between</em> the groups and <img src="img/7180OS_02_17.jpg" alt="The F-statistic" class="calibre73"/> is the <em class="calibre13">variance within</em> the groups.</p><p class="calibre11">As the ratio <em class="calibre13">F</em> gets larger, the larger the variance between the groups is compared to the variance within the groups. This implies that the grouping is doing a good job in explaining the variance observed in the sample as a whole. Where this ratio exceeds a critical threshold, we <a id="id241" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can say that the difference is statistically significant.</p><div><div><h3 class="title4"><a id="note27" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The <em class="calibre13">F</em>-test is always a one-tailed test, because any variance among the groups tends to make <em class="calibre13">F</em> large. It is impossible for <em class="calibre13">F</em> to decrease below zero.</p></div></div><p class="calibre11">The <em class="calibre13">variance within</em> for an <em class="calibre13">F</em>-test is calculated as the mean squared deviation from the mean. We calculate this as the sum of squared deviations from the mean divided by the first degree of freedom. For example, if there are <em class="calibre13">k</em> groups, each with a mean of <img src="img/7180OS_02_18.jpg" alt="The F-statistic" class="calibre74"/>, we could calculate the variance within like this:</p><div><img src="img/7180OS_02_19.jpg" alt="The F-statistic" class="calibre75"/></div><p class="calibre11">Here, <em class="calibre13">SSW</em> represents the <em class="calibre13">sum of squares within</em> and <em class="calibre13">x<sub class="calibre25">jk</sub></em> represents the value of the <em class="calibre13">j<sup class="calibre42">th</sup></em> element in group <em class="calibre13">k</em>.</p><p class="calibre11">The preceding formula for calculating the <em class="calibre13">SSW</em> looks intimidating. But, in fact, Incanter defines a useful <code class="literal">s/sum-of-square-devs-from-mean</code> function that makes calculating the sum of squares within as trivial as:</p><div><pre class="programlisting">(defn ssw [groups]
  (-&gt;&gt; (map s/sum-of-square-devs-from-mean groups)
       (reduce +)))</pre></div><p class="calibre11">The <em class="calibre13">variance between</em> for an <em class="calibre13">F</em>-test has a similar formula:</p><div><img src="img/7180OS_02_20.jpg" alt="The F-statistic" class="calibre76"/></div><p class="calibre11">Here, <em class="calibre13">SST</em> is the <em class="calibre13">total sum of squares</em> and <em class="calibre13">SSW</em> is the value we just calculated. The total sum of the squares is the <a id="id242" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sum of squared differences from the "grand" mean that can be calculated like this:</p><div><img src="img/7180OS_02_21.jpg" alt="The F-statistic" class="calibre77"/></div><p class="calibre11">Thus, <em class="calibre13">SST</em> is simply the overall sum of the squares without any grouping. We can calculate both the SST and SSW in Clojure, like this:</p><div><pre class="programlisting">(defn sst [groups]
  (-&gt;&gt; (apply concat groups)
       (s/sum-of-square-devs-from-mean)))

(defn ssb [groups]
  (- (sst groups)
     (ssw groups)))</pre></div><p class="calibre11">The <em class="calibre13">F</em>-statistic is calculated as the ratio of the variance between and the variance within the groups. Combining both our <code class="literal">ssb</code> and <code class="literal">ssw</code> functions defined previously and the two degrees of freedom, we can calculate the <em class="calibre13">F</em>-statistic in Clojure as follows.</p><p class="calibre11">Thus, we can calculate the <em class="calibre13">F</em>-statistic from our groups and our two degrees of freedom as follows:</p><div><pre class="programlisting">(defn f-stat [groups df1 df2]
  (let [msb (/ (ssb groups) df1)
        msw (/ (ssw groups) df2)]
    (/ msbmsw)))</pre></div><p class="calibre11">Now that we can <a id="id243" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculate the <em class="calibre13">F</em>-statistic from our groups, we're ready to use it in an <em class="calibre13">F</em>-test.</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec56" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The F-test</h1></div></div></div><p class="calibre11">As with all of the <a id="id244" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>hypothesis tests we have looked at in this chapter, once we have a statistic and a distribution, we simply need to pick a value of <em class="calibre13">α</em> and see if our data has exceeded the critical value for the test.</p><p class="calibre11">Incanter provides an <code class="literal">s/f-test</code> function, but this only measures the variance between and within the two groups. To run an <em class="calibre13">F</em>-test on our 20 different groups, we will need to implement our own <em class="calibre13">F</em>-test function. Fortunately, we've already done the hard work in the previous sections by calculating an appropriate <em class="calibre13">F</em>-statistic. We can perform the <em class="calibre13">F</em>-test by looking up the <em class="calibre13">F</em>-statistic in an <em class="calibre13">F</em>-distribution parameterized with the correct degrees of freedom. In the following code, we will write an <code class="literal">f-test</code> function, which uses this to perform the test on an arbitrary number of groups:</p><div><pre class="programlisting">(defn f-test [groups]
  (let [n (count (apply concat groups))
        m (count groups)
        df1 (- m 1)
        df2 (- n m)
        f-stat (f-stat groups df1 df2)]
    (s/cdf-f f-stat :df1 df1 :df2 df2 :lower-tail? false)))</pre></div><p class="calibre11">In the last line of the preceding function, we convert the value of the <em class="calibre13">F</em>-statistic into a <em class="calibre13">p</em>-value using Incanter's <code class="literal">s/cdf-f</code> function parameterized by the correct degrees of freedom. This <em class="calibre13">p</em>-value is a measure of the whole model, how well the different pages explain the variance of the dwell times overall. All that remains for us to do is to choose a significance level and run the test. Let's stick with a 5 percent significance level:</p><div><pre class="programlisting">(defn ex-2-24 []
  (let [grouped (-&gt;&gt; (load-data "multiple-sites.tsv")
                     (:rows)
                     (group-by :site)
                     (vals)
                     (map (partial map :dwell-time)))]
    (f-test grouped)))

;; 0.014</pre></div><p class="calibre11">The test returns a <em class="calibre13">p</em>-value of 0.014, which is a significant result. The different pages indeed have different variances that cannot simply be explained away by random sampling error alone.</p><div><img src="img/7180OS_02_260.jpg" alt="The F-test" class="calibre78"/></div><p class="calibre11">We could use a box <a id="id245" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>plot to visualize the distributions of each site together in one chart to compare them side by side:</p><div><pre class="programlisting">(defn ex-2-25 []
  (let [grouped (-&gt;&gt; (load-data "multiple-sites.tsv")
                     (:rows)
                     (group-by :site)
                     (sort-by first)
                     (map second)
                     (map (partial map :dwell-time)))
        box-plot (c/box-plot (first grouped)
                             :x-label "Site number"
                             :y-label "Dwell time (s)")
        add-box (fn [chart dwell-times]
                  (c/add-box-plot chart dwell-times))]
    (-&gt; (reduce add-box box-plot (rest grouped))
        (i/view))))</pre></div><p class="calibre11">In the preceding code, we reduce over the groups, calling <code class="literal">c/add-box-plot</code> for each group. The groups are sorted by their site ID before plotting, so our original page 0 is to the extreme left of the chart.</p><div><img src="img/7180OS_02_270.jpg" alt="The F-test" class="calibre45"/></div><p class="calibre11">It might appear that site ID <code class="literal">10</code> has the longest dwell times, since its interquartile range extends furthest up the <a id="id246" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chart. However, if you look closely, you'll see its mean value is lower than site 6, having a mean dwell time of over 144 seconds:</p><div><pre class="programlisting">(defn ex-2-26 []
  (let [data (load-data "multiple-sites.tsv")
        site-0 (-&gt;&gt; (i/$where {:site {:$eq 0}} data)
                    (i/$ :dwell-time))
        site-10 (-&gt;&gt; (i/$where {:site {:$eq 10}} data)
                     (i/$ :dwell-time))]
    (s/t-test site-10 :y site-0)))

;; 0.0069</pre></div><p class="calibre11">Now that we have confirmed a statistically significant effect using the <em class="calibre13">F</em>-test, we're justified in claiming that site ID <code class="literal">6</code> is statistically different from the baseline:</p><div><pre class="programlisting">(defn ex-2-27 []
  (let [data (load-data "multiple-sites.tsv")
        site-0 (-&gt;&gt; (i/$where {:site {:$eq 0}} data)
                    (i/$ :dwell-time))
        site-6 (-&gt;&gt; (i/$where {:site {:$eq 6}} data)
                    (i/$ :dwell-time))]
    (s/t-test site-6 :y site-0)))

;; 0.007</pre></div><p class="calibre11">Finally, we have evidence to suggest that page ID 6 is a genuine improvement over the current site. As a result of our analysis, the AcmeContent CEO authorizes the launch of a new look website. The web team is delighted!</p></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec57" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Effect size</h1></div></div></div><p class="calibre11">In this chapter, we focused on statistical significance—the methods employed by statisticians to ensure a difference is discovered, which cannot be easily explained as chance variation. We must always remember that finding a significant effect isn't the same as finding a large effect. With very large samples, even a tiny difference in sample means will count as significant. To get a better sense of whether our discovery is both significant and important, we should state the effect size as well.</p><div><div><div><div><h2 class="title5"><a id="ch02lvl2sec35" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Cohen's d</h2></div></div></div><p class="calibre11">Cohen's d is an adjustment that can be applied to see whether the difference we have observed is not just statistically significant, but actually large. Like the Bonferroni correction, the adjustment is a straightforward one:</p><div><img src="img/7180OS_02_22.jpg" alt="Cohen's d" class="calibre79"/></div><p class="calibre11">Here, <em class="calibre13">S<sub class="calibre25">ab</sub></em> is the pooled standard deviation (not the pooled standard error) of the samples. It can be calculated in a way similar to the pooled standard error:</p><div><pre class="programlisting">(defn pooled-standard-deviation [a b]
  (i/sqrt (+ (i/sq (standard-deviation a))
             (i/sq (standard-deviation b)))))</pre></div><p class="calibre11">Thus, we can calculate Cohen's d for our page 6, as follows:</p><div><pre class="programlisting">(defn ex-2-28 []
  (let [data (load-data "multiple-sites.tsv")
        a (-&gt;&gt; (i/$where {:site {:$eq 0}} data)
               (i/$ :dwell-time))
        b (-&gt;&gt; (i/$where {:site {:$eq 6}} data)
               (i/$ :dwell-time))]
    (/ (- (s/mean b)
          (s/mean a))
       (pooled-standard-deviation a b))))

;; 0.389</pre></div><p class="calibre11">In contrast with the <em class="calibre13">p</em>-values, there is no absolute threshold for Cohen's d. Whether an effect can be considered large is partly dependent on the context, but it does provide a useful, normalized measure of the effect size. Values above 0.5 are typically considered large, so 0.38 is a moderate effect. It certainly represents a meaningful increase in the dwell time on our site and is certainly worth the effort of a site upgrade.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch02lvl1sec58" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we've learned about the difference between descriptive and inferential statistics. Once again, we've seen the importance of normal distribution and the central limit theorem, and learned how to quantify population differences with <em class="calibre13">z</em>-tests, <em class="calibre13">t</em>-tests, and <em class="calibre13">F</em>-tests.</p><p class="calibre11">We've learned about how the techniques of inferential statistics analyze the samples themselves to make claims about the population that was sampled. We've seen a variety of techniques—confidence intervals, bootstrapping, and significance tests—that can yield insight into the underlying population parameters. By simulating repeated tests with ClojureScript, we've also gained an insight into the difficulty of significance testing with multiple comparisons and seen how the <em class="calibre13">F</em>-test attempts to address the issue and strike a balance between Type I and Type II errors.</p><p class="calibre11">In the next chapter, we'll apply the lessons we've learned on variance and <em class="calibre13">F</em>-testing to single samples. We'll introduce the technique of regression analysis and use it to find correlations among variables within a sample of Olympic athletes.</p></div></div>



  </body></html>