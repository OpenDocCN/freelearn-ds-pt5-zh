["```py\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import  classification_report\nfrom sklearn.preprocessing import scale\nimport numpy as np\n\ndef get_data(batch_size):\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    b_size = 0\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.8*no_features)\n    repeated_features = int(0.1*no_features)\n\n    while b_size < batch_size:\n        x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\\\n                n_informative = informative_features, n_redundant = redundant_features \\\n                ,n_repeated = repeated_features, random_state=51)\n        y_indx = y < 1\n        y[y_indx] = -1\n        x = scale(x,with_mean=True,with_std=True)\n\n        yield x,y\n        b_size+=1\n```", "```py\ndef build_model(x,y,weights,epochs,alpha=0.5):\n    \"\"\"\n    Simple Perceptron\n    \"\"\"\n\n    for i in range(epochs):\n\n        # Shuffle the dataset\n        shuff_index = np.random.shuffle(range(len(y)))\n        x_train = x[shuff_index,:].reshape(x.shape)\n        y_train = np.ravel(y[shuff_index,:])\n\n        # Build weights one instance at a time\n        for index in range(len(y)):\n            prediction = np.sign( np.sum(x_train[index,:] * weights) ) \n            if prediction != y_train[index]:\n                weights = weights + alpha * (y_train[index] * x_train[index,:])\n\n    return weights\n\ndef model_worth(x,y,weights):\n    prediction = np.sign(np.sum(x * weights,axis=1))\nprint classification_report(y,prediction)\n```", "```py\nif __name__ == \"__main__\":\n    data = get_data(10)    \n    x,y = data.next()\n    weights = np.zeros(x.shape[1])    \n    for i in range(10):\n        epochs = 100\n        weights = build_model(x,y,weights,epochs)\n        print\n        print \"Model worth after receiving dataset batch %d\"%(i+1)    \n        model_worth(x,y,weights)\n        print\n        if i < 9:\n            x,y = data.next()\n```", "```py\n    data = get_data(10)    \n```", "```py\n    x,y = data.next()\n```", "```py\n        x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\\\n                n_informative = informative_features, n_redundant = redundant_features \\\n                ,n_repeated = repeated_features, random_state=51)\n```", "```py\n        y_indx = y < 1\n        y[y_indx] = -1\n```", "```py\n    weights = np.zeros(x.shape[1])    \n```", "```py\n    for i in range(10):\n        epochs = 100\n        weights = build_model(x,y,weights,epochs)\n```", "```py\ndef build_model(x,y,weights,epochs,alpha=0.5)\n```", "```py\n        # Shuffle the dataset\n        shuff_index = np.random.shuffle(range(len(y)))\n        x_train = x[shuff_index,:].reshape(x.shape)\n        y_train = np.ravel(y[shuff_index,:])\n```", "```py\n\n # Build weights one instance at a time\n        for index in range(len(y)):\n            prediction = np.sign( np.sum(x_train[index,:] * weights) ) \n            if prediction != y_train[index]:\n                weights = weights + alpha * (y_train[index] * x_train[index,:])\n```", "```py\n            prediction = np.sign( np.sum(x_train[index,:] * weights) ) \n```", "```py\n                weights = weights + alpha * (y_train[index] * x_train[index,:])\n```", "```py\n        print\n        print \"Model worth after receiving dataset batch %d\"%(i+1)    \n        model_worth(x,y,weights)\n```", "```py\ndef get_data(batch_size):\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    b_size = 0\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.8*no_features)\n    repeated_features = int(0.1*no_features)\n    poly = PolynomialFeatures(degree=2)\n\n    while b_size < batch_size:\n        x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\\\n                n_informative = informative_features, n_redundant = redundant_features \\\n                ,n_repeated = repeated_features, random_state=51)\n        y_indx = y < 1\n        y[y_indx] = -1\n        x = poly.fit_transform(x)\n        yield x,y\n        b_size+=1\n```", "```py\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.cross_validation import train_test_split\n\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    no_features = 30\n\n    x,y = make_regression(n_samples=1000,n_features=no_features,\\\n             random_state=51)\n    return x,y\n```", "```py\ndef build_model(x,y):\n    estimator = SGDRegressor(n_iter = 10, shuffle=True,loss = \"squared_loss\", \\\n            learning_rate='constant',eta0=0.01,fit_intercept=True, \\\n            penalty='none')\n    estimator.fit(x,y)\n\n    return estimator\n\ndef model_worth(model,x,y):\n    predicted_y = model.predict(x)\n    print \"\\nMean absolute error = %0.2f\"%mean_absolute_error(y,predicted_y)\n    print \"Mean squared error = %0.2f\"%mean_squared_error(y,predicted_y)\n\ndef inspect_model(model):\n    print \"\\nModel Itercept {0}\".format(model.intercept_)\n    print\n    for i,coef in enumerate(model.coef_):\n        print \"Coefficient {0} = {1:.3f}\".format(i+1,coef)\n```", "```py\nif __name__ == \"__main__\":\n    x,y = get_data()\n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    model = build_model(x_train,y_train)\n\n    inspect_model(model)\n\n    print \"Model worth on train data\"\n    model_worth(model,x_train,y_train)\n    print \"Model worth on dev data\"\n    model_worth(model,x_dev,y_dev)\n\n    # Building model with l2 regularization\n    model = build_model_regularized(x_train,y_train)\n    inspect_model(model)\n```", "```py\n    x,y = get_data()\n```", "```py\n    no_features = 30\n    x,y = make_regression(n_samples=1000,n_features=no_features,\\\n             random_state=51)\t\n```", "```py\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n```", "```py\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n```", "```py\n    model = build_model(x_train,y_train)\n```", "```py\n    estimator = SGDRegressor(n_iter = 10, shuffle=True,loss = \"squared_loss\", \\\n            learning_rate='constant',eta0=0.01,fit_intercept=True, \\\n            penalty='none')\n    estimator.fit(x,y)\n```", "```py\n    inspect_model(model)\n```", "```py\n    print \"Model worth on train data\"\n    model_worth(model,x_train,y_train)\n```", "```py\ndef build_model_regularized(x,y):\n    estimator = SGDRegressor(n_iter = 10,shuffle=True,loss = \"squared_loss\", \\\n            learning_rate='constant',eta0=0.01,fit_intercept=True, \\\n            penalty='l2',alpha=0.01)\n    estimator.fit(x,y)\n\n    return estimator\n```", "```py\nmodel = build_model_regularized(x_train,y_train)\ninspect_model(model)\n```", "```py\n    estimator = SGDRegressor(n_iter = 10,shuffle=True,loss = \"squared_loss\", \\\n            learning_rate='constant',eta0=0.01,fit_intercept=True, \\\n            penalty='l2',alpha=0.01)\n```", "```py\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import  accuracy_score\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import SGDClassifier\n\nimport numpy as np\n\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n    x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\\\n            n_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n    return x,y\n```", "```py\ndef build_model(x,y,x_dev,y_dev):\n    estimator = SGDClassifier(n_iter=50,shuffle=True,loss=\"log\", \\\n                learning_rate = \"constant\",eta0=0.0001,fit_intercept=True, penalty=\"none\")\n    estimator.fit(x,y)\n    train_predcited = estimator.predict(x)\n    train_score = accuracy_score(y,train_predcited)\n    dev_predicted = estimator.predict(x_dev)\n    dev_score = accuracy_score(y_dev,dev_predicted)\n\n    print \n    print \"Training Accuracy = %0.2f Dev Accuracy = %0.2f\"%(train_score,dev_score)\n```", "```py\nif __name__ == \"__main__\":\n    x,y = get_data()    \n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    build_model(x_train,y_train,x_dev,y_dev)\n```", "```py\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\n            n_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n    return x,y\n```", "```py\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n```", "```py\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n```", "```py\nbuild_model(x_train,y_train,x_dev,y_dev)\n```", "```py\n    estimator = SGDClassifier(n_iter=50,shuffle=True,loss=\"log\", \\\n                learning_rate = \"constant\",eta0=0.0001,fit_intercept=True, penalty=\"none\")\n```", "```py\n estimator.fit(x,y)\n    train_predcited = estimator.predict(x)\n    train_score = accuracy_score(y,train_predcited)\n    dev_predicted = estimator.predict(x_dev)\n    dev_score = accuracy_score(y_dev,dev_predicted)\n\n    print \n    print \"Training Accuracy = %0.2f Dev Accuracy = %0.2f\"%(train_score,dev_score)\n```", "```py\nestimator = SGDClassifier(n_iter=50,shuffle=True,loss=\"log\", \\\nlearning_rate = \"invscaling\",eta0=0.001,fit_intercept=True, penalty=\"none\")\n```"]