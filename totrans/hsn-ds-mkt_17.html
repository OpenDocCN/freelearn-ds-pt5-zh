<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">A/B Testing for Better Marketing Strategy</h1>
                </header>
            
            <article>
                
<p>When building different marketing strategies, whether your idea is going to work or not. Typically, there is a lot of guesswork involved when coming up with new marketing ideas, and often there is a lack of tools, resources, or even motivation to test whether any of your marketing ideas will work. However, this way of putting your marketing strategy ideas into work is risky and can be very costly. What if you spent lots of money on your new marketing campaign and it did not help you reach your marketing goal at all? What if you spent hundreds of hours refining your marketing message and it never attracted your prospects to engage with your marketing message?</p>
<p>In this chapter, we are going to discuss a way of testing your marketing ideas before you fully commit to them. More specifically, we are going to learn about what A/B testing is, <span>why running A/B tests is important,</span> and <span>how it can help you reach your marketing goal in a more efficient and less expensive way</span>.</p>
<p>I<span>n this chapte</span><span>r, we will cover the following topics:</span></p>
<ul>
<li>A/B testing for marketing</li>
<li>Statistical hypothesis testing</li>
<li>Evaluating A/B testing results with Python</li>
<li><span>Evaluating A/B testing results </span>with R</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A/B testing for marketing</h1>
                </header>
            
            <article>
                
<p><strong>A/B testing</strong> plays a critical role in decision-making processes across various industries. A/B testing is essentially a method of comparing and testing the effectiveness and benefits of two different business strategies. It can be considered as an experiment where two or more variants are tested for a set period of time and then the experiment results are evaluated to find the strategy that works best. Running A/B testing before fully committing to a single option helps businesses take the guesswork out of their decision-making processes and saves valuable resources, such as time and capital, that could have been wasted if the chosen strategy did not work.</p>
<p>In a typical A/B testing setting, you would create and test two or more versions of marketing strategies for their effectiveness in achieving your marketing goal. Consider a case where your goal is to improve marketing email open rates. If your hypothesis is that email subject line B will result in higher open rates than email subject line A, then you would run an A/B test with these two subject lines. You will randomly select half of the users and send out marketing emails with subject line A. The other half of randomly selected users will receive emails with subject line B. You will run this test for a predetermined period of time (which could be one week, two weeks, or one month, for instance) or until a predetermined number of users receive the two versions of emails (which is a minimum of 1,000 users to receive each version of the subject line). Once your tests are complete, then you analyze and evaluate the experiment results. When analyzing the results, you will need to check whether there is a statistically significant difference between the results of the two versions. We will cover more about statistical hypothesis testing and statistical significance in the following section. If your experiment results show a clear winner between the two versions of subject line, you can use the winning subject line in your future marketing emails.</p>
<p>Aside from the aforementioned email subject line scenario, A/B testing can be applied in many different areas of marketing. For instance, you can run A/B testing on your advertisements on social media. You can have two or more variants of your ads and run A/B tests to see which variation works better for click-through rates or conversion rates. As another example, you can use A/B testing to test whether product recommendations on your web page result in higher purchase rates. If you have built a different version of your product recommendation algorithm, then you can use and expose the initial version of your product recommendation algorithm to some randomly selected users and the second version to some other randomly selected users. You can gather the A/B test results and evaluate which version of your product recommendation algorithm helps you bring in more revenue.</p>
<p>As you can see from these example use cases, A/B testing plays an important role in decision-making. As you test different scenarios before you fully commit to one, it helps you save your energy, time, and capital that you could have wasted if you had fully committed to it but failed. A/B tests also help you take your guesswork away and quantify the performance gains (or losses) of your future marketing strategy. Whenever you have a new marketing idea that you would like to iterate on, you should consider running A/B tests first.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Statistical hypothesis testing</h1>
                </header>
            
            <article>
                
<p>When you run A/B tests, it is important to test your hypothesis and seek for statistically significant differences among the test groups. Student's t-test, or simply the <strong>t-test</strong>, is frequently used to test whether the difference between two tests is statistically significant. The t-test compares the two averages and examines whether they are significantly different from each other. </p>
<p>There are two important statistics in a t-test—the <strong>t-value</strong> and <strong>p-value</strong>. The t-value measures the degree of difference relative to the variation in the data. The larger the t-value is, the more difference there is between the two groups. On the other hand, the p-value measures the probability that the results would occur by chance. The smaller the p-value is, the more statistically significant difference there will be between the two groups. The equation to compute the t-value is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7e564400-c20a-46a9-bf2e-08b039624cf2.png" style="width:10.08em;height:5.83em;"/></p>
<p>In this equation, <em>M</em><sub>1</sub> and <em>M<sub>2</sub></em> are the averages of group <em>1</em> and <em>2</em>. <em>S<sub>1</sub></em> and <em>S<sub>2</sub></em> are the standard deviations of group <em>1</em> and <em>2</em><em>,</em> and <em>N<sub>1</sub></em> and <em>N<sub>2</sub></em> are number of samples in group <em>1</em> and <em>2</em> respectively.</p>
<p><span>There is a concept of the null hypothesis and the alternate hypothesis, which you should be familiar with. Generally speaking, the null hypothesis is that the two groups show no statistically significant difference. On the other hand, the alternate hypothesis states that the two groups show a statistically significant difference. When the t-value is larger than a threshold and the p-value is smaller than a threshold, we say that we can reject the null hypothesis and that the two groups show a statistically significant difference. Typically,</span> 0.01 or 0.05 are used as the p-value thresholds for testing statistical significance. If the p-value is less than 0.05, t<span>hen it suggests that there is less than 5% probability that the difference between the two groups occurs by chance. In other words, the difference is highly unlikely to be by chance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating A/B testing results with Python</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to discuss how to evaluate A/B testing results to decide which marketing strategy works the best. By the end of this section, we will have covered how to run statistical hypothesis testing and compute the statistical significance. We will be mainly using the<span> </span><kbd>pandas</kbd><span>,</span><span> </span><kbd>matplotlib</kbd><span>, and <kbd>scipy</kbd> </span>packages to analyze and visualize the data, and evaluate the A/B testing results. </p>
<div class="packt_tip"><span>For those readers who would like to use R instead of Python for this exercise, you can skip to the next section.</span></div>
<p><span>For this exercise, we will be using one of the publicly available datasets from</span> the IBM Watson Analytics community, wh<span>ich can be found at this link: <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-campaign-eff-usec_-fastf/">https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-campaign-eff-usec_-fastf/</a></span><span>. You can follow this link and download the data, which is available in XLSX format, named <kbd>WA_Fn-UseC_-Marketing-Campaign-Eff-UseC_-FastF.xlsx</kbd>. Once you have downloaded this data, you can load it into your Jupyter Notebook by running the following command:</span></p>
<pre>import pandas as pd<br/><br/>df = pd.read_excel('../data/WA_Fn-UseC_-Marketing-Campaign-Eff-UseC_-FastF.xlsx')</pre>
<p>The <kbd>df</kbd> DataFrame looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cba7050d-9fa8-4ffc-b7c9-6e01e6c7d49f.png" style="width:40.25em;height:31.33em;"/></p>
<p>There are a total of seven variables in the dataset. You can find the descriptions of these variables on the IBM Watson Analytics Community page, but we will reiterate in the following:</p>
<ul>
<li><kbd>MarketID</kbd>: unique identifier for market</li>
<li><kbd>MarketSize</kbd>: size of market area by sales</li>
<li><kbd>LocationID</kbd>: unique identifier for store location</li>
<li><kbd>AgeOfStore</kbd>: age of store in years</li>
<li><kbd>Promotion</kbd>: one of three promotions that was tested</li>
<li><kbd>week</kbd>: one of four weeks when the promotions were run</li>
<li><kbd>SalesInThousands</kbd>: sales amount for specific <kbd>LocationID</kbd>, <kbd>Promotion</kbd>, and <kbd>week</kbd></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data analysis</h1>
                </header>
            
            <article>
                
<p>Let's take a deeper look at the data. In this section, we are going to focus on understanding the distributions of sales, market sizes, store locations, and store ages used to test different promotions. The goal of this analysis is to make sure the controls and attributes of each of the promotion groups are symmetrically distributed, so that the promotion performances among different groups are comparable to each other. </p>
<p>The total sales distributions across different promotions can be visualized using the following code:</p>
<pre>ax = df.groupby(<br/>    'Promotion'<br/>).sum()[<br/>    'SalesInThousands'<br/>].plot.pie(<br/>    figsize=(7, 7),<br/>    autopct='%1.0f%%'<br/>)<br/><br/>ax.set_ylabel('')<br/>ax.set_title('sales distribution across different promotions')<br/><br/>plt.show()</pre>
<p>As you can see from this code, we are grouping the data by the <kbd>Promotion</kbd> <span>column </span>and aggregating the total sales amount by summing over the <kbd>SalesInThousands</kbd> column. Using a pie chart, we can easily visualize how much of the pie each group takes.</p>
<p>The resulting pie chart looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/154b46df-a5f7-4828-b304-3a2bcb3c7be5.png" style="width:23.50em;height:25.08em;"/></p>
<p>As is easily visible from this pie chart, promotion group <strong>3</strong> has the largest aggregate sales among the three groups. However, each promotion group takes roughly about one third of the total sales during the promotion weeks. Similarly, we can also visualize the compositions of different market sizes in each promotion group. Take a look at the following code:</p>
<pre>ax = df.groupby([<br/>    'Promotion', 'MarketSize'<br/>]).count()[<br/>    'MarketID'<br/>].unstack(<br/>    'MarketSize'<br/>).plot(<br/>    kind='bar',<br/>    figsize=(12,10),<br/>    grid=True,<br/>)<br/><br/>ax.set_ylabel('count')<br/>ax.set_title('breakdowns of market sizes across different promotions')<br/><br/>plt.show()</pre>
<p>The bar plot looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/43368d36-9f76-4c0b-a58a-4c9c50372d88.png" style="width:50.08em;height:41.42em;"/></p>
<p>If you think a stacked bar chart will be easier to view, you can use the following code to display this data in a stacked bar plot:</p>
<pre>ax = df.groupby([<br/>    'Promotion', 'MarketSize'<br/>]).sum()[<br/>    'SalesInThousands'<br/>].unstack(<br/>    'MarketSize'<br/>).plot(<br/>    kind='bar',<br/>    figsize=(12,10),<br/>    grid=True,<br/>    stacked=True<br/>)<br/><br/>ax.set_ylabel('Sales (in Thousands)')<br/>ax.set_title('breakdowns of market sizes across different promotions')<br/><br/>plt.show()</pre>
<p>You may notice that the only difference between this code and the previous code is the <kbd>stacked=True</kbd> flag. The result looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/925c9804-6231-4cdb-81e8-164466db21de.png" style="width:51.58em;height:42.00em;"/></p>
<p>As you can see from this bar chart, the medium market size occupies the most among all three promotion groups, while the small market size occupies the least. We can verify that the compositions of different market sizes are similar among the three promotion groups from this plot.</p>
<p>Another attribute, <kbd>AgeOfStore</kbd>, and its overall distribution across all different promotions groups, can be visualized by using the following code:</p>
<pre>ax = df.groupby(<br/>    'AgeOfStore'<br/>).count()[<br/>    'MarketID'<br/>].plot(<br/>    kind='bar', <br/>    color='skyblue',<br/>    figsize=(10,7),<br/>    grid=True<br/>)<br/><br/>ax.set_xlabel('age')<br/>ax.set_ylabel('count')<br/>ax.set_title('overall distributions of age of store')<br/><br/>plt.show()</pre>
<p>And the result looks as in the following bar plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c17e98dd-0db7-4db6-b748-782b367030cd.png" style="width:38.83em;height:28.17em;"/></p>
<p>As you can see from this plot, a large number of stores are <strong>1</strong> year old and the majority of stores are <strong>10</strong> years old or less. However, what we are more interested in is whether the stores in the three different promotion groups have similar store age profiles. Take a look at the following code:</p>
<pre>ax = df.groupby(<br/>    ['AgeOfStore', 'Promotion']<br/>).count()[<br/>    'MarketID'<br/>].unstack(<br/>    'Promotion'<br/>).iloc[::-1].plot(<br/>    kind='barh', <br/>    figsize=(12,15),<br/>    grid=True<br/>)<br/><br/>ax.set_ylabel('age')<br/>ax.set_xlabel('count')<br/>ax.set_title('overall distributions of age of store')<br/><br/>plt.show()</pre>
<p>Using this code, you will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/203b97a7-78a0-42d0-8713-8fb94d496e30.png" style="width:43.08em;height:52.67em;"/></p>
<p class="mce-root"/>
<p>The store age distributions across the three different promotion groups seem to align with each other, but it is quite difficult to digest the information presented from this plot. It will be easier to look at the summary statistics of store ages across the three promotion groups. Take a look at the following code:</p>
<pre>df.groupby('Promotion').describe()['AgeOfStore']</pre>
<p>The output of this code looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bc132c1f-8897-4faf-b738-89a283ac1438.png" style="width:30.33em;height:9.83em;"/></p>
<p>As you may notice from this output, it is much easier to understand the overall store age distributions from these summary statistics. We can see that all three test groups seem to have similar store age profiles. The average ages of stores for the three groups are 8–9 years old and the majority of the stores are 10–12 years old or younger.</p>
<p>By analyzing how each promotion or test group is comprised, we could verify that the store profiles are similar to each other. This suggests that the sample groups are well controlled and the A/B testing results will be meaningful and trustworthy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Statistical hypothesis testing</h1>
                </header>
            
            <article>
                
<p>The ultimate goal of A/B testing of different marketing strategies is to find out which strategy is the most efficient and works the best among the others. As briefly discussed in an earlier section, a strategy having a higher response number does not necessarily mean that it outperforms the rest. We will discuss how we can use the t-test to evaluate the relative performances of different marketing strategies and see which strategy wins over the others with significance.</p>
<p>In Python, there are two approaches to computing the t-value and p-value in a t-test. We will demonstrate both approaches in this section, and it is up to you to decide which one works more conveniently for you. The t<span>wo approaches to compute the t-value and p-value for a t-test are as follows:</span></p>
<ul>
<li><strong>Computing t-value and p-value from the equations</strong>: The first approach is to manually calculate the t-value using the equation we have learned in the previous section. As you may recall, there are three things we need to compute to get the t-value—the mean, the standard deviation, and the number of samples. Take a look at the following code:</li>
</ul>
<pre>        means = df.groupby('Promotion').mean()['SalesInThousands']<br/>        stds = df.groupby('Promotion').std()['SalesInThousands']<br/>        ns = df.groupby('Promotion').count()['SalesInThousands']</pre>
<p style="padding-left: 60px">As you can see from this code, you can easily compute the mean, the standard deviation, and the number of samples in each test group by using the <kbd>mean</kbd>, <kbd>std</kbd>, and <kbd>count</kbd> functions respectively. With these, we can compute the t-value using the previously discussed equation. Take a look at the following code:</p>
<pre>      import numpy as np<br/><br/>        t_1_vs_2 = (<br/>            means.iloc[0] - means.iloc[1]<br/>        )/ np.sqrt(<br/>            (stds.iloc[0]**2/ns.iloc[0]) + (stds.iloc[1]**2/ns.iloc[1])<br/>        )</pre>
<p style="padding-left: 60px">Using this code, we can compute the t-value for comparing the performances of promotion 1 and promotion 2. The t-value we get from running the code is <kbd>6.4275</kbd>. From this t-value, we can get the p-value with the following code:</p>
<pre>        from scipy import stats<br/><br/>        df_1_vs_1 = ns.iloc[0] + ns.iloc[1] - 2<br/><br/>        p_1_vs_2 = (1 - stats.t.cdf(t_1_vs_2, df=df_1_vs_1))*2</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">As you can see from this code, we first compute the degrees of freedom, which is the sum of the number of samples in both groups minus two. With the t-value calculated previously, we can compute the p-value, using the <kbd>t.cdf</kbd> function from <kbd>scipy</kbd> package's <kbd>stats</kbd> module. The p-value we get from running this code is <kbd>4.143e-10</kbd>. This is an extremely small number that is close to <kbd>0</kbd>. As discussed earlier, a p-value closer to 0 suggests that there is a strong evidence against the null hypothesis and that the difference between the two test groups is significant. <br/>
<br/>
The average sales (in thousands) for promotion group 1 is about <kbd>58.1</kbd>, and for promotion group 2 it's about <kbd>47.33</kbd>. From our t-test, we have shown that the marketing performances for these two groups are significantly different and that promotion group 1 outperforms promotion group 2. However, if we run a t-test between the promotion group 1 and promotion group 3, we see different results.<br/>
<br/>
On the surface, the average sales from promotion group 1 (<kbd>58.1</kbd>) looks higher than those from promotion group 2 (<kbd>55.36</kbd>). However, when we run a t-test between these two groups, we get a t-value of <kbd>1.556</kbd> and a p-value of <kbd>0.121</kbd>. The computed p-value is much higher than <kbd>0.05</kbd>, which is a generally accepted cut-off line. This suggests that the marketing performance from promotion group 1 is not statistically different from the marketing performance from promotion group 2. Thus, even though promotion group 1's average sales number is higher than the promotion group 2's from the A/B test, the difference is not statistically significant and we cannot conclude that promotion group 1 performs much better than promotion group 2. From these evaluation results, we can conclude that promotion groups 1 and 3 outperform promotion group 2, but the difference between promotion groups 1 and 3 is not statistically significant.</p>
<ul>
<li><strong>Computing the t-value and p-value using scipy</strong>: Another approach to computing the t-value and p-value is by using the <kbd>stats</kbd> module from the <kbd>scipy</kbd> package. Take a look at the following code:</li>
</ul>
<pre>        t, p = stats.ttest_ind(<br/>            df.loc[df['Promotion'] == 1, 'SalesInThousands'].values, <br/>            df.loc[df['Promotion'] == 2, 'SalesInThousands'].values, <br/>            equal_var=False<br/>         )</pre>
<p style="padding-left: 60px">As you can see from this code, the <kbd>stats</kbd> module from the <kbd>scipy</kbd> package has a function named <kbd>ttest_ind</kbd>. This function computes t-value and p-value, given the data. Using this function, we can easily compute t-values and p-values to compare the marketing performances of different promotion or test groups. The results are the same in both approaches. Whether we use the previous approach of manually computing the t-values and p-values from the equation or the approach of using the <kbd>ttest_ind</kbd> function in the <kbd>scipy</kbd> package, the t-values we get to compare promotion group 1 against 2 and promotion group 1 against 3 are <kbd>6.4275</kbd> and <kbd>1.556</kbd>; whereas, the p-values we get are <kbd>4.29e-10</kbd> and <kbd>0.121</kbd> respectively. And, of course, the interpretations of these t-test results are the same as before.</p>
<p>We have shown two approaches to computing t-values and p-values. It may look easier to use the <kbd>scipy</kbd> package's out-of-the-box solution to compute those values, but it is always helpful to have the equation in the back in your mind.</p>
<div class="packt_infobox">The full code for this Python exercise can be found at the following link: <a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.12/python/ABTesting.ipynb">https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.12/python/ABTesting.ipynb</a>.<a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.12/python/ABTesting.ipynb"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating A/B testing results with R</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to discuss how to evaluate A/B testing results to decide which marketing strategy works the best. By the end of this section, we will have covered how to run statistical hypothesis testing and compute the statistical significance. We will be mainly using<span> </span><kbd>dplyr</kbd><span> and</span><span> </span><kbd>ggplot2</kbd> to analyze and visualize the data and evaluate the A/B testing results. </p>
<div class="packt_tip"><span>For those readers who would like to use Python instead of R for this exercise, you can refer to the previous section.</span></div>
<p><span>For this exercise, we will be using one of the publicly available datasets from</span> the IBM Watson Analytics community, which can be foun<span>d at this link: <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-campaign-eff-usec_-fastf/">https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-campaign-eff-usec_-fastf/</a></span><span>. You can follow this link and download the data, which is available in XLSX format, named <kbd>WA_Fn-UseC_-Marketing-Campaign-Eff-UseC_-FastF.xlsx</kbd>. Once you have downloaded this data, you can load it into your RStudio by running the following command:</span></p>
<pre>library(dplyr)<br/>library(readxl)<br/>library(ggplot2)<br/><br/>#### 1. Load Data ####<br/>df &lt;- read_excel(<br/>  path="~/Documents/data-science-for-marketing/ch.12/data/WA_Fn-UseC_-Marketing-Campaign-Eff-UseC_-FastF.xlsx"<br/>)</pre>
<p>The<span> </span><kbd>df</kbd> DataFrame looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/789290db-bc30-45b4-b6cb-024f2145de85.png" style="width:56.75em;height:30.50em;"/></p>
<p>There are a total of seven variables in the dataset. You can find the descriptions of these variables on the IBM Watson Analytics Community page, but we will reiterate in the following:</p>
<ul>
<li><kbd>MarketID</kbd><span>:</span> unique identifier for market</li>
<li><kbd>MarketSize</kbd><span>:</span><span> size of market area by sales</span></li>
<li><kbd>LocationID</kbd><span>:</span><span> unique identifier for store location</span></li>
<li><kbd>AgeOfStore</kbd><span>:</span><span> age of store in years</span></li>
<li><kbd>Promotion</kbd><span>:</span><span> one of three promotions that was tested</span></li>
<li><kbd>week</kbd><span>:</span><span> one of four weeks when the promotions were run</span></li>
<li><kbd>SalesInThousands</kbd><span>:</span><span> sales amount for a specific</span> <kbd>LocationID</kbd><span>,</span> <kbd>Promotion</kbd><span>, and</span> <kbd>week</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data analysis</h1>
                </header>
            
            <article>
                
<p>Let's take a deeper look at the data. In this section, we are going to focus on understanding the distributions of sales, market sizes, store locations, and store ages used to test different promotions. The goal of this analysis is to make sure that the controls and attributes of each promotion groups are symmetrically distributed, so that the promotion performances among different groups are comparable to each other. </p>
<p>The total sales distributions across different promotions can be visualized using the following code:</p>
<pre>salesPerPromo &lt;- df %&gt;% <br/>  group_by(Promotion) %&gt;%<br/>  summarise(Sales=sum(SalesInThousands))<br/><br/>ggplot(salesPerPromo, aes(x="", y=Sales, fill=Promotion)) + <br/>  geom_bar(width=1, stat = "identity", position=position_fill()) +<br/>  geom_text(aes(x=1.25, label=Sales), position=position_fill(vjust = 0.5), color='white') +<br/>  coord_polar("y") +<br/>  ggtitle('sales distribution across different promotions')</pre>
<p>As you can see from this code, we are grouping the data by the <kbd>Promotion</kbd> <span>column</span><span> </span>and aggregating the total sales amount by summing over the <kbd>SalesInThousands</kbd> column. Using a pie chart, we can easily visualize how much of the pie each group takes.</p>
<p>The resulting pie chart looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/55644525-9f11-4aa8-a57b-b747c89df7c8.png" style="width:31.75em;height:27.25em;"/></p>
<p>As is easily visible from this pie chart, promotion group <span class="packt_screen">3</span> has the largest aggregate sales among the three groups. However, each promotion group takes roughly one third of the total sales during the promotion weeks. Similarly, we can also visualize the compositions of different market sizes in each promotion group. Take a look at the following code:</p>
<pre>marketSizePerPromo &lt;- df %&gt;% <br/>  group_by(Promotion, MarketSize) %&gt;%<br/>  summarise(Count=n())<br/><br/>ggplot(marketSizePerPromo, aes(x=Promotion, y=Count, fill=MarketSize)) + <br/>  geom_bar(width=0.5, stat="identity", position="dodge") +<br/>  ylab("Count") +<br/>  xlab("Promotion") +<br/>  ggtitle("breakdowns of market sizes across different promotions") +<br/>  theme(plot.title=element_text(hjust=0.5))</pre>
<p>The bar plot looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/86c3d24a-7177-4acf-9dfe-ebaa156e40d1.png"/></p>
<p>If you think a stacked bar chart will be easier to view, you can use the following code to display this data in a stacked bar plot:</p>
<pre>ggplot(marketSizePerPromo, aes(x=Promotion, y=Count, fill=MarketSize)) + <br/>  geom_bar(width=0.5, stat="identity", position="stack") +<br/>  ylab("Count") +<br/>  xlab("Promotion") +<br/>  ggtitle("breakdowns of market sizes across different promotions") +<br/>  theme(plot.title=element_text(hjust=0.5))</pre>
<p>You may notice that the only difference between this code and the previous code is the <kbd>position="stack"</kbd> <span>flag</span><span> </span>in the <kbd>geom_bar</kbd> function. The result looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bfa1e513-4576-42dd-990b-79b111d71d4b.png"/></p>
<p>As you can see from this bar chart, the medium market size occupies the most among all three promotion groups, while small market size occupies the least. We can verify that the compositions of different market sizes are similar among the three promotion groups from this plot.</p>
<p>Another attribute,<span> </span><kbd>AgeOfStore</kbd>, and its overall distributions across all different promotions groups, can be visualized by using the following code:</p>
<pre>overallAge &lt;- df %&gt;%<br/>  group_by(AgeOfStore) %&gt;%<br/>  summarise(Count=n())<br/><br/>ggplot(overallAge, aes(x=AgeOfStore, y=Count)) + <br/>  geom_bar(width=0.5, stat="identity") +<br/>  ylab("Count") +<br/>  xlab("Store Age") +<br/>  ggtitle("overall distributions of age of store") +<br/>  theme(plot.title=element_text(hjust=0.5))</pre>
<p>And the result looks like the following bar plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2545b6a9-074f-4164-b4b7-38798c1b60d9.png" style="width:57.92em;height:34.67em;"/></p>
<p>As you can see from this plot, a large number of stores are <strong>1</strong> year old and the majority of stores are <strong>10</strong> years old or less. However, what we are more interested in is whether the stores in the three different promotion groups have similar store age profiles. Take a look at the following code:</p>
<pre>AgePerPromo &lt;- df %&gt;% <br/>  group_by(Promotion, AgeOfStore) %&gt;%<br/>  summarise(Count=n())<br/><br/>ggplot(AgePerPromo, aes(x=AgeOfStore, y=Count, fill=Promotion)) + <br/>  geom_bar(width=0.5, stat="identity", position="dodge2") +<br/>  ylab("Count") +<br/>  xlab("Store Age") +<br/>  ggtitle("distributions of age of store") +<br/>  theme(plot.title=element_text(hjust=0.5))</pre>
<p>Using this code, you will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/73f3b9ec-bf12-432c-99c1-e051ab9bb93d.png" style="width:51.42em;height:31.25em;"/></p>
<p>The store age distributions across the three different promotion groups seem to align with each other, but it is quite difficult to digest the information presented from this plot. It will be easier to look at the summary statistics of store ages across the three promotion groups. Take a look at the following code:</p>
<pre>tapply(df$AgeOfStore, df$Promotion, summary)</pre>
<p>The output of this code looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/200bc692-cd85-41ff-87a6-a7b086bf35bb.png" style="width:21.33em;height:12.42em;"/></p>
<p>As you may notice from this output, it is much easier to understand the overall store age distributions from these summary statistics. We can see that all three test groups seem to have similar store age profiles. The average ages of stores for the three groups are 8-9 years old and the majority of the stores are 10-12 years old or younger.</p>
<p>By analyzing how each promotion or test group is comprised, we could verify that the store profiles are similar to each other. This suggests that the sample groups are well controlled and the A/B testing results will be meaningful and trustworthy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Statistical hypothesis testing</h1>
                </header>
            
            <article>
                
<p>The ultimate goal of A/B testing of different marketing strategies is to find out which strategy is the most efficient and works the best among the others. As briefly discussed in an earlier section, a strategy with a higher response number does not necessarily mean that it outperforms the rest. We will discuss how we can use the t-test<span> </span>to evaluate the relative performances of different marketing strategies and see which strategy wins over the others with significance.</p>
<p>In R, there are two approaches to compute the t-value and p-value for a t-test. We will demonstrate both approaches in this section, and it is up to you to decide which one works more conveniently for you. The two approaches to compute the t-value and p-value for a t-test are as follows:</p>
<ul>
<li><strong>Computing the t-value and p-value from the equations</strong><span>:</span> The first approach is to manually calculate the t-value using the equation we have learned in the previous section. As you may recall, there are three things we need to compute to get the t-value: the mean, the standard deviation, and the number of samples. Take a look at the following code:</li>
</ul>
<pre>        promo_1 &lt;- df[which(df$Promotion == 1),]$SalesInThousands<br/>        promo_2 &lt;- df[which(df$Promotion == 2),]$SalesInThousands<br/><br/>        mean_1 &lt;- mean(promo_1)<br/>        mean_2 &lt;- mean(promo_2)<br/>        std_1 &lt;- sd(promo_1)<br/>        std_2 &lt;- sd(promo_2)<br/>        n_1 &lt;- length(promo_1)<br/>        n_2 &lt;- length(promo_2)</pre>
<p class="mce-root"/>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">As you can see from this code, you can easily compute the mean, the standard deviation, and the number of samples in each test group by using the<span> </span><kbd>mean</kbd>,<span> </span><kbd>sd</kbd>, and<span> </span><kbd>length</kbd><span> </span>functions respectively. With these, we can compute the t-value using the previously discussed equation. Take a look at the following code:</p>
<pre>        t_val &lt;- (<br/>          mean_1 - mean_2<br/>        ) / sqrt(<br/>         (std_1**2/n_1 + std_2**2/n_2)<br/>        )</pre>
<p style="padding-left: 60px">Using this code, we can compute the t-value for comparing the performances of promotion 1 and promotion 2. The t-value we get from running the code is <kbd>6.4275</kbd>. From this t-value, we can get the p-value with the following code:</p>
<pre>        df_1_2 &lt;- n_1 + n_2 - 2<br/> <br/>        p_val &lt;- 2 * pt(t_val, df_1_2, lower=FALSE)</pre>
<p style="padding-left: 60px">As you can see from this code, we first compute the degrees of freedom, which is the sum of the number of samples in both groups minus two. With the t-value calculated previously, we can compute the p-value using the<span> </span><kbd>pt</kbd><span> </span>function, which returns a probability value from the t-distribution, given the t-value and degree of freedom. The p-value we get from running this code is <kbd>4.143e-10</kbd>. This is an extremely small number that is close to 0. As discussed earlier, a p-value close to 0 suggests that there is strong evidence against the null hypothesis and that the difference between the two test groups is significant. <br/>
<br/>
The average sales (in thousands) for promotion group 1 is about<span> </span><kbd>58.1</kbd>,<span> </span>and for promotion group 2 it's about<span> </span><kbd>47.33</kbd>. From our t-test, we have shown that the marketing performances for these two groups are significantly different and that promotion group 1 outperforms promotion group 2. However, if we run a t-test between promotion group 1 and promotion group 3, we see different results.</p>
<p style="padding-left: 60px">On the surface, the average sales from promotion group 1 (<kbd>58.1</kbd>) looks higher than those from promotion group 2 (<kbd>55.36</kbd>). However, when we run a t-test between these two groups, we get a t-value of<span> </span><kbd>1.556</kbd><span> </span>and a p-value of<span> </span><kbd>0.121</kbd>. The computed p-value is much higher than<span> </span><kbd>0.05</kbd>, which is a generally accepted cut-off line. This suggests that the marketing performance for promotion group 1 is not statistically different from the marketing performance of promotion group 2. Thus, even though promotion group 1's average sales number is higher than promotion group 2's from the A/B test, the difference is not statistically significant, and we cannot conclude that promotion group 1 performs much better than promotion group 2. From these evaluation results, we can conclude that promotions groups 1 and 3 outperform promotion group 2, but the difference between promotion groups 1 and 3 is not statistically significant.</p>
<ul>
<li><strong>Computing the t-value and p-value using t.test</strong><span>:</span> Another approach to compute the t-value and p-value is by using the<span> </span><kbd>t.test</kbd><span> function in R</span>. Take a look at the following code:</li>
</ul>
<pre>        # using t.test<br/>        t.test(<br/>          promo_1, <br/>          promo_2<br/>        )</pre>
<p style="padding-left: 60px">As you can see from this code, R has a <kbd>t.test</kbd> function, which computes the t-value and p-value, given data. Using this function, we can easily compute t-values and p-values to compare the marketing performances of different promotions or test groups. The results are the same in both approaches. Whether we use the previous approach of manually computing the t-values and p-values from the equation or the approach of using the<span> </span><kbd>ttest_ind</kbd><span> </span>function in the<span> </span><kbd>scipy</kbd><span> </span>package, the t-values we get to compare promotion group 1 against <span>promotion group</span> 2 and promotion group 1 against <span>promotion group </span>3 are<span> </span><kbd>6.4275</kbd><span> </span>and<span> </span><kbd>1.556</kbd>; whereas, the p-values we get are <kbd>4.29e-10</kbd><span> </span>and<span> </span><kbd>0.121</kbd><span> </span>respectively. And, of course, the interpretations of these t-test results are the same as before.</p>
<p>We have shown two approaches to computing t-values and p-values. It may look easier to use the<span> </span><kbd>t.test</kbd><span> </span>function in R, but it is always helpful to have the equation in the back of your mind.</p>
<div class="packt_infobox">The full code for this R exercise can be found at the following link: <a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.12/R/ABTesting.R">https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.12/R/ABTesting.R</a>.<a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.12/R/ABTesting.R"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned about one of the most frequently used testing methods in marketing for making decisions on future marketing strategies. We have discussed what A/B testing is, why it is important to run A/B tests before you fully commit to one marketing strategy, and <span>how it can help you reach your marketing goal in a more efficient and less expensive way. By working through a sample use case, where your goal was to choose the best email subject line, we learned what a typical process for running A/B tests looks like. A/B testing does not need to happen only once. A/B tests are best used when you consistently test your new ideas against currently running strategies or against other ideas through experiments. Simply put, whenever there is a new idea, it should be A/B tested. Using the t-test and the Python and R tools that we have learned about in this chapter, you should be able to easily evaluate A/B test results and identify which strategy is the winning strategy.</span></p>
<p>This chapter was the last technical chapter with case studies and programming exercises. In the next chapter, we are going to summarize and review all the topics that we have covered throughout this book. Then, we will discuss some common data science and machine learning applications in marketing and some other Python and R libraries that you can benefit from in your future projects that have not been covered in this book.</p>


            </article>

            
        </section>
    </body></html>