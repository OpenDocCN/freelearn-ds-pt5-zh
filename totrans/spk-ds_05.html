<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 5. Data Analysis on Spark"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch05" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 5. Data Analysis on Spark</h1></div></div></div><p class="calibre11">The field of data analytics at scale has been evolving like never before. Various libraries and tools were developed for data analysis with a rich set of algorithms. On a parallel line, distributed computing techniques were evolving with time, to process huge datasets at scale. These two traits had to converge, and that was the primary intention behind the development of Spark.</p><p class="calibre11">The previous two chapters outlined the technology aspects of data science. It covered some fundamentals on the DataFrame API, Datasets, streaming data  and how it facilitated data representation through DataFrames that R and Python users were familiar with. After introducing this API, we saw how operating on datasets became easier than ever. We also looked at how Spark SQL played a background role in supporting the DataFrame API with its robust features and optimization techniques. In this chapter, we are going to cover the scientific aspect of big data analysis and learn various data analytics techniques that can be executed on Spark.</p><p class="calibre11">As a prerequisite for this chapter, a basic understanding of the DataFrame API and statistics fundamentals is good to have. However, we have tried to make the content as simple as possible and covered some important fundamentals in detail so that anyone can get started with statistical analysis on Spark. The topics covered in this chapter are as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Data analytics life cycle</li><li class="listitem">Data acquisition</li><li class="listitem">Data preparation<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Data consolidation</li><li class="listitem">Data cleansing</li><li class="listitem">Data transformation</li></ul></div><p class="calibre31">
</p></li></ul></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Basics of statistics<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Sampling</li><li class="listitem">Data distributions</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Descriptive statistics<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Measures of location</li><li class="listitem">Measures of spread</li><li class="listitem">Summary statistics</li><li class="listitem">Graphical techniques</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Inferential statistics<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Discrete probability distributions</li><li class="listitem">Continuous probability distributions</li><li class="listitem">Standard error</li><li class="listitem">Confidence level</li><li class="listitem">Margin of error and confidence interval</li><li class="listitem">Variability in population</li><li class="listitem">Estimating sample size</li><li class="listitem">Hypothesis testing</li><li class="listitem">Chi-square test</li><li class="listitem">F-test</li><li class="listitem">Correlations</li></ul></div><p class="calibre31">
</p></li></ul></div><div class="calibre2" title="Data analytics life cycle"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch05lvl1sec34" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data analytics life cycle</h1></div></div></div><p class="calibre11">For most real-world projects, there is some defined sequence of steps to be followed. However, there are no universally agreed upon definitions or boundaries for data analytics and data science. Generally, the term "data analytics" encompasses the techniques and processes involved in examining data, discovering useful insights, and communicating them. The term "data science" can be best treated as an interdisciplinary field drawing from <span class="strong"><em class="calibre22">statistics</em></span>, <span class="strong"><em class="calibre22">computer science</em></span>, and <span class="strong"><em class="calibre22">mathematics</em></span>. Both terms deal with processing raw data to derive knowledge or insights, usually in an iterative fashion, and some people use them interchangeably.</p><p class="calibre11">Based on diverse business requirements, there are different ways of approaching problems but there is no unique standard process that fits in well with all possible scenarios. A typical process workflow can be summarized as a cycle of formulating a question, exploring, hypothesizing, validating the hypothesis, analyzing the results, and starting all over again. This is depicted in the following figure with the thick arrows. From a data perspective, the workflow consists of data acquisition, preprocessing, exploring the data, modeling, and communicating the results. This is shown in the figure as circles. Analysis and visualization happen at every stage, right from data collection to results communication. The data analytics workflow encompasses all the activities shown in both views:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_001.jpg" alt="Data analytics life cycle" class="calibre46"/></div><p class="calibre11">
</p><p class="calibre11">The most important thing in the entire life cycle is the question at hand. Data that might contain an answer (relevant data!) to that question comes next. Depending on the question, the first task is to collect the right data from one or more data sources as needed. Organizations often maintain <span class="strong"><strong class="calibre19">data lakes</strong></span>, which are humongous repositories of data in their original format.</p><p class="calibre11">The next step is to clean/transform the data to the desired format. Data cleansing is also called data munging, data wrangling, or data dredging. This involves activities such as missing value treatment and outlier treatment upon assessing the quality of the data at hand. You may also have to aggregate/plot the data for better understanding. This process of formulating the final data matrix to work with is touted as the most time-consuming step. This also happens to be an underestimated component that is considered to be part of preprocessing, along with other activities such as feature extraction and data transformation.</p><p class="calibre11">The crux of data science, that is, training models and extracting patterns, comes next, which requires heavy use of statistics and machine learning. The final step is publishing the results.</p><p class="calibre11">The remaining sections in this chapter delve deeper into each of these steps and how they can be implemented using Spark. Some basics of statistics are also included so as to enable the reader to follow the code snippets with ease.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Data acquisition"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch05lvl1sec35" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data acquisition</h1></div></div></div><p class="calibre11">Data acquisition, or data collection, is the very first step in any data science project. Usually, you won't find the complete set of required data in one place as it is distributed across <span class="strong"><strong class="calibre19">line-of-business</strong></span> (<span class="strong"><strong class="calibre19">LOB</strong></span>) applications and systems.</p><p class="calibre11">The majority of this section has already been covered in the previous chapter, which outlined how to source data from different data sources and store the data in DataFrames for easier analysis. There is a built-in mechanism in Spark to fetch data from some of the common data sources and the <span class="strong"><em class="calibre22">Data Source API</em></span> is provided for the ones not supported out of the box on Spark.</p><p class="calibre11">To get a better understanding of the data acquisition and preparation phases, let us assume a scenario and try to address all the steps involved with example code snippets. The scenario is such that employee data is present across native RDDs, JSON files, and on a SQL server. So, let's see how we can get those to Spark DataFrames:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">// From RDD: Create an RDD and convert to DataFrame
&gt;&gt;&gt; employees = sc.parallelize([(1, "John", 25), (2, "Ray", 35), (3, "Mike", 24), (4, "Jane", 28), (5, "Kevin", 26), (6, "Vincent", 35), (7, "James", 38), (8, "Shane", 32), (9, "Larry", 29), (10, "Kimberly", 29), (11, "Alex", 28), (12, "Garry", 25), (13, "Max", 31)]).toDF(["emp_id","name","age"])
&gt;&gt;&gt;

// From JSON: reading a JSON file
&gt;&gt;&gt; salary = sqlContext.read.json("./salary.json")
&gt;&gt;&gt; designation = sqlContext.read.json("./designation.json")</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">// From RDD: Create an RDD and convert to DataFrame
scala&gt; val employees = sc.parallelize(List((1, "John", 25), (2, "Ray", 35), (3, "Mike", 24), (4, "Jane", 28), (5, "Kevin", 26), (6, "Vincent", 35), (7, "James", 38), (8, "Shane", 32), (9, "Larry", 29), (10, "Kimberly", 29), (11, "Alex", 28), (12, "Garry", 25), (13, "Max", 31))).toDF("emp_id","name","age")
employees: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 1 more field]
scala&gt; // From JSON: reading a JSON file
scala&gt; val salary = spark.read.json("./salary.json")
salary: org.apache.spark.sql.DataFrame = [e_id: bigint, salary: bigint]
scala&gt; val designation = spark.read.json("./designation.json")
designation: org.apache.spark.sql.DataFrame = [id: bigint, role: string]</pre></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Data preparation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch05lvl1sec36" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data preparation</h1></div></div></div><p class="calibre11">Data quality has always been a pervasive problem in the industry. The presence of incorrect or inconsistent data can produce misleading results of your analysis. Implementing better algorithm or building better models will not help much if the data is not cleansed and prepared well, as per the requirement. There is an industry jargon called <span class="strong"><strong class="calibre19">data engineering</strong></span> that refers to data sourcing and preparation. This is typically done by data scientists and in a few organizations, there is a dedicated team for this purpose. However, while preparing data, a scientific perspective is often needed to do it right. As an example, you may not just do <span class="strong"><em class="calibre22">mean substitution</em></span> to treat missing values and look into data distribution to find more appropriate values to substitute. Another such example is that you may not just look at a box plot or scatter plot to look for outliers, as there could be multivariate outliers which are not visible if you plot a single variable. There are different approaches, such as <span class="strong"><strong class="calibre19">Gaussian Mixture Models</strong></span> (<span class="strong"><strong class="calibre19">GMMs</strong></span>) and <span class="strong"><strong class="calibre19">Expectation Maximization</strong></span> (<span class="strong"><strong class="calibre19">EM</strong></span>) algorithms that use <span class="strong"><strong class="calibre19">Mahalanobis distance</strong></span> to look for multivariate outliers.</p><p class="calibre11">The data preparation phase is an extremely important phase, not only for the algorithms to work properly, but also for you to develop a better understanding of your data so that you can take the right approach while implementing an algorithm.</p><p class="calibre11">Once the data has been acquired from different sources, the next step is to consolidate them all so that the data as a whole can be cleaned, formatted, and transformed to the format needed for your analysis. Please note that you might have to take samples of data from the sources, depending on the scenario, and then prepare the data for further analysis. Various sampling techniques that can be used are discussed later in this chapter.</p><div class="calibre2" title="Data consolidation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec45" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data consolidation</h2></div></div></div><p class="calibre11">In this section, we will take a look at how to combine data acquired from various data sources:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">// Creating the final data matrix using the join operation
&gt;&gt;&gt; final_data = employees.join(salary, employees.emp_id == salary.e_id).join(designation, employees.emp_id == designation.id).select("emp_id", "name", "age", "role", "salary")
&gt;&gt;&gt; final_data.show(5)
+------+-----+---+---------+------+
|emp_id| name|age|     role|salary|
+------+-----+---+---------+------+
|     1| John| 25|Associate| 10000|
|     2|  Ray| 35|  Manager| 12000|
|     3| Mike| 24|  Manager| 12000|
|     4| Jane| 28|Associate|  null|
|     5|Kevin| 26|  Manager|   120|
+------+-----+---+---------+------+
only showing top 5 rows</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">// Creating the final data matrix using the join operation
scala&gt; val final_data = employees.join(salary, $"emp_id" === $"e_id").join(designation, $"emp_id" === $"id").select("emp_id", "name", "age", "role", "salary")
final_data: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 3 more fields]</pre><p class="calibre11">After integrating data from those sources, the final dataset (in this case it is <code class="literal">final_data</code>) should be of the following format (just example data):</p><div class="informaltable"><table border="1" class="calibre12"><colgroup class="calibre13"><col class="calibre14"/><col class="calibre14"/><col class="calibre14"/><col class="calibre14"/><col class="calibre14"/></colgroup><tbody class="calibre15"><tr class="calibre16"><td class="calibre17">
<p class="calibre18">
<span class="strong"><strong class="calibre19">emp_id</strong></span>
</p>
</td><td class="calibre17">
<p class="calibre18">
<span class="strong"><strong class="calibre19">name</strong></span>
</p>
</td><td class="calibre17">
<p class="calibre18">
<span class="strong"><strong class="calibre19">age</strong></span>
</p>
</td><td class="calibre17">
<p class="calibre18">
<span class="strong"><strong class="calibre19">role</strong></span>
</p>
</td><td class="calibre17">
<p class="calibre18">
<span class="strong"><strong class="calibre19">salary</strong></span>
</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">1</p>
</td><td class="calibre17">
<p class="calibre18">John</p>
</td><td class="calibre17">
<p class="calibre18">25</p>
</td><td class="calibre17">
<p class="calibre18">Associate</p>
</td><td class="calibre17">
<p class="calibre18">10,000 $</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">2</p>
</td><td class="calibre17">
<p class="calibre18">Ray</p>
</td><td class="calibre17">
<p class="calibre18">35</p>
</td><td class="calibre17">
<p class="calibre18">Manager</p>
</td><td class="calibre17">
<p class="calibre18">12,000 $</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">3</p>
</td><td class="calibre17">
<p class="calibre18">Mike</p>
</td><td class="calibre17">
<p class="calibre18">24</p>
</td><td class="calibre17">
<p class="calibre18">Manager</p>
</td><td class="calibre17">
<p class="calibre18">12,000 $</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">4</p>
</td><td class="calibre17">
<p class="calibre18">Jane</p>
</td><td class="calibre17">
<p class="calibre18">28</p>
</td><td class="calibre17">
<p class="calibre18">Associate</p>
</td><td class="calibre17">
<p class="calibre18">null</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">5</p>
</td><td class="calibre17">
<p class="calibre18">Kevin</p>
</td><td class="calibre17">
<p class="calibre18">26</p>
</td><td class="calibre17">
<p class="calibre18">Manager</p>
</td><td class="calibre17">
<p class="calibre18">12,000 $</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">6</p>
</td><td class="calibre17">
<p class="calibre18">Vincent</p>
</td><td class="calibre17">
<p class="calibre18">35</p>
</td><td class="calibre17">
<p class="calibre18">Senior Manager</p>
</td><td class="calibre17">
<p class="calibre18">22,000 $</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">7</p>
</td><td class="calibre17">
<p class="calibre18">James</p>
</td><td class="calibre17">
<p class="calibre18">38</p>
</td><td class="calibre17">
<p class="calibre18">Senior Manager</p>
</td><td class="calibre17">
<p class="calibre18">20,000 $</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">8</p>
</td><td class="calibre17">
<p class="calibre18">Shane</p>
</td><td class="calibre17">
<p class="calibre18">32</p>
</td><td class="calibre17">
<p class="calibre18">Manager</p>
</td><td class="calibre17">
<p class="calibre18">12,000 $</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">9</p>
</td><td class="calibre17">
<p class="calibre18">Larry</p>
</td><td class="calibre17">
<p class="calibre18">29</p>
</td><td class="calibre17">
<p class="calibre18">Manager</p>
</td><td class="calibre17">
<p class="calibre18">10,000 $</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">10</p>
</td><td class="calibre17">
<p class="calibre18">Kimberly</p>
</td><td class="calibre17">
<p class="calibre18">29</p>
</td><td class="calibre17">
<p class="calibre18">Associate</p>
</td><td class="calibre17">
<p class="calibre18">8,000 $</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">11</p>
</td><td class="calibre17">
<p class="calibre18">Alex</p>
</td><td class="calibre17">
<p class="calibre18">28</p>
</td><td class="calibre17">
<p class="calibre18">Manager</p>
</td><td class="calibre17">
<p class="calibre18">12,000 $</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">12</p>
</td><td class="calibre17">
<p class="calibre18">Garry</p>
</td><td class="calibre17">
<p class="calibre18">25</p>
</td><td class="calibre17">
<p class="calibre18">Manager</p>
</td><td class="calibre17">
<p class="calibre18">12.000 $</p>
</td></tr><tr class="calibre21"><td class="calibre17">
<p class="calibre18">13</p>
</td><td class="calibre17">
<p class="calibre18">Max</p>
</td><td class="calibre17">
<p class="calibre18">31</p>
</td><td class="calibre17">
<p class="calibre18">Manager</p>
</td><td class="calibre17">
<p class="calibre18">12,000 $</p>
</td></tr></tbody></table></div></div><div class="calibre2" title="Data cleansing"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec46" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data cleansing</h2></div></div></div><p class="calibre11">Once you have the data consolidated in one place, it is extremely important that you spend enough time and effort in cleaning it before analyzing it. This is an iterative process because you have to validate the actions you have taken on the data and continue till you are satisfied with the data quality. It is advisable that you spend time analyzing the causes of anomalies you detect in the data.</p><p class="calibre11">Some level of impurity in data usually exists in any dataset. There can be various kinds of issues with data, but we are going to address a few common cases, such as missing values, duplicate values, transforming, or formatting (adding or removing digits from a number, splitting a column into two, merging two columns into one).</p><div class="calibre2" title="Missing value treatment"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec25" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Missing value treatment</h3></div></div></div><p class="calibre11">There are various ways of handling missing values. One way is dropping rows containing missing values. We may want to drop a row even if a single column has missing value, or may have different strategies for different columns. We may want to retain the row as long as the total number of missing values in that row are under a threshold. Another approach may be to replace nulls with a constant value, say the mean value in case of numeric variables.</p><p class="calibre11">In this section, we will not be providing some examples in both Scala and Python and will try to cover various scenarios to give you a broader perspective.</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">// Dropping rows with missing value(s)
&gt;&gt;&gt; clean_data = final_data.na.drop()
&gt;&gt;&gt; 
// Replacing missing value by mean
&gt;&gt;&gt; import math
&gt;&gt;&gt; from pyspark.sql import functions as F
&gt;&gt;&gt; mean_salary = math.floor(salary.select(F.mean('salary')).collect()[0][0])
&gt;&gt;&gt; clean_data = final_data.na.fill({'salary' : mean_salary})
&gt;&gt;&gt; 
//Another example for missing value treatment
&gt;&gt;&gt; authors = [['Thomas','Hardy','June 2, 1840'],
       ['Charles','Dickens','7 February 1812'],
        ['Mark','Twain',None],
        ['Jane','Austen','16 December 1775'],
      ['Emily',None,None]]
&gt;&gt;&gt; df1 = sc.parallelize(authors).toDF(
       ["FirstName","LastName","Dob"])
&gt;&gt;&gt; df1.show()
+---------+--------+----------------+
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|   Thomas|   Hardy|    June 2, 1840|
|  Charles| Dickens| 7 February 1812|
|     Mark|   Twain|            null|
|     Jane|  Austen|16 December 1775|
|    Emily|    null|            null|
+---------+--------+----------------+

// Drop rows with missing values
&gt;&gt;&gt; df1.na.drop().show()
+---------+--------+----------------+
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|   Thomas|   Hardy|    June 2, 1840|
|  Charles| Dickens| 7 February 1812|
|     Jane|  Austen|16 December 1775|
+---------+--------+----------------+

// Drop rows with at least 2 missing values
&gt;&gt;&gt; df1.na.drop(thresh=2).show()
+---------+--------+----------------+
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|   Thomas|   Hardy|    June 2, 1840|
|  Charles| Dickens| 7 February 1812|
|     Mark|   Twain|            null|
|     Jane|  Austen|16 December 1775|
+---------+--------+----------------+

// Fill all missing values with a given string
&gt;&gt;&gt; df1.na.fill('Unknown').show()
+---------+--------+----------------+
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|   Thomas|   Hardy|    June 2, 1840|
|  Charles| Dickens| 7 February 1812|
|     Mark|   Twain|         Unknown|
|     Jane|  Austen|16 December 1775|
|    Emily| Unknown|         Unknown|
+---------+--------+----------------+

// Fill missing values in each column with a given string
&gt;&gt;&gt; df1.na.fill({'LastName':'--','Dob':'Unknown'}).show()
+---------+--------+----------------+
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|   Thomas|   Hardy|    June 2, 1840|
|  Charles| Dickens| 7 February 1812|
|     Mark|   Twain|         Unknown|
|     Jane|  Austen|16 December 1775|
|    Emily|      --|         Unknown|
+---------+--------+----------------+</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">//Missing value treatment
// Dropping rows with missing value(s)
scala&gt; var clean_data = final_data.na.drop() //Note the var declaration instead of val
clean_data: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 3 more fields]
scala&gt;

// Replacing missing value by mean
scal&gt; val mean_salary = final_data.select(floor(avg("salary"))).
            first()(0).toString.toDouble
mean_salary: Double = 20843.0
scal&gt; clean_data = final_data.na.fill(Map("salary" -&gt; mean_salary)) 

//Reassigning clean_data
clean_data: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 3 more fields]
scala&gt;

//Another example for missing value treatment
scala&gt; case class Author (FirstName: String, LastName: String, Dob: String)
defined class Author
scala&gt; val authors = Seq(
        Author("Thomas","Hardy","June 2, 1840"),
        Author("Charles","Dickens","7 February 1812"),
        Author("Mark","Twain",null),
        Author("Emily",null,null))
authors: Seq[Author] = List(Author(Thomas,Hardy,June 2, 1840),
   Author(Charles,Dickens,7 February 1812), Author(Mark,Twain,null),
   Author(Emily,null,null))
scala&gt; val ds1 = sc.parallelize(authors).toDS()
ds1: org.apache.spark.sql.Dataset[Author] = [FirstName: string, LastName: string ... 1 more field]
scala&gt; ds1.show()
+---------+--------+---------------+
|FirstName|LastName|            Dob|
+---------+--------+---------------+
|   Thomas|   Hardy|   June 2, 1840|
|  Charles| Dickens|7 February 1812|
|     Mark|   Twain|           null|
|    Emily|    null|           null|
+---------+--------+---------------+
scala&gt;

// Drop rows with missing values
scala&gt; ds1.na.drop().show()
+---------+--------+---------------+
|FirstName|LastName|            Dob|
+---------+--------+---------------+
|   Thomas|   Hardy|   June 2, 1840|
|  Charles| Dickens|7 February 1812|
+---------+--------+---------------+
scala&gt;

//Drop rows with at least 2 missing values
//Note that there is no direct scala function to drop rows with at least n missing values
//However, you can drop rows containing under specified non nulls
//Use that function to achieve the same result
scala&gt; ds1.na.drop(minNonNulls = df1.columns.length - 1).show()
//Fill all missing values with a given string
scala&gt; ds1.na.fill("Unknown").show()
+---------+--------+---------------+
|FirstName|LastName|            Dob|
+---------+--------+---------------+
|   Thomas|   Hardy|   June 2, 1840|
|  Charles| Dickens|7 February 1812|
|     Mark|   Twain|        Unknown|
|    Emily| Unknown|        Unknown|
+---------+--------+---------------+
scala&gt;

//Fill missing values in each column with a given string
scala&gt; ds1.na.fill(Map("LastName"-&gt;"--",
                    "Dob"-&gt;"Unknown")).show()
+---------+--------+---------------+
|FirstName|LastName|            Dob|
+---------+--------+---------------+
|   Thomas|   Hardy|   June 2, 1840|
|  Charles| Dickens|7 February 1812|
|     Mark|   Twain|        Unknown|
|    Emily|      --|        Unknown|
+---------+--------+---------------+</pre></div><div class="calibre2" title="Outlier treatment"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec26" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Outlier treatment</h3></div></div></div><p class="calibre11">Understanding what an outlier is also important to treat it well. To put it simply, an outlier is a data point that does not share the same characteristics as the rest of the data points. Example: if you have a dataset of schoolchildren and there are a few age values in the range of 30-40 then they could be outliers. Let us look into a different example now: if you have a dataset where a variable can have data points only in two ranges, say, in the 10-20 or 80-90 range, then the data points (say, 40 or 55) with values in between these two ranges could also be outliers. In this example, 40 or 55 do not belong to the 10-20 range, nor do they belong to the 80-90 range, and are outliers.</p><p class="calibre11">Also, there can be univariate outliers and there can be multivariate outliers as well. We will focus on univariate outliers in this book for simplicity's sake as Spark MLlib may not have all the algorithms needed at the time of writing this book.</p><p class="calibre11">In order to treat the outliers, you have to first see if there are outliers. There are different ways, such as summary statistics and plotting techniques, to find the outliers. You can use the built-in library functions such as <code class="literal">matplotlib</code> of Python to visualize your data. You can do so by connecting to Spark through a notebook (for example, Jupyter) so that the visuals can be generated, which may not be possible on a command shell.</p><p class="calibre11">Once you find outliers, you can either delete the rows containing outliers or impute the mean values in place of outliers or do something more relevant, as applicable to your case. Let us have a look at the mean substitution method here:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">// Identify outliers and replace them with mean
//The following example reuses the clean_data dataset and mean_salary computed in previous examples
&gt;&gt;&gt; mean_salary
20843.0
&gt;&gt;&gt; 
//Compute deviation for each row
&gt;&gt;&gt; devs = final_data.select(((final_data.salary - mean_salary) ** 2).alias("deviation"))

//Compute standard deviation
&gt;&gt;&gt; stddev = math.floor(math.sqrt(devs.groupBy().
          avg("deviation").first()[0]))

//check standard deviation value
&gt;&gt;&gt; round(stddev,2)
30351.0
&gt;&gt;&gt; 
//Replace outliers beyond 2 standard deviations with the mean salary
&gt;&gt;&gt; no_outlier = final_data.select(final_data.emp_id, final_data.name, final_data.age, final_data.salary, final_data.role, F.when(final_data.salary.between(mean_salary-(2*stddev), mean_salary+(2*stddev)), final_data.salary).otherwise(mean_salary).alias("updated_salary"))
&gt;&gt;&gt; 
//Observe modified values
&gt;&gt;&gt; no_outlier.filter(no_outlier.salary != no_outlier.updated_salary).show()
+------+----+---+------+-------+--------------+
|emp_id|name|age|salary|   role|updated_salary|
+------+----+---+------+-------+--------------+
|    13| Max| 31|120000|Manager|       20843.0|
+------+----+---+------+-------+--------------+
&gt;&gt;&gt;
 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">// Identify outliers and replace them with mean
//The following example reuses the clean_data dataset and mean_salary computed in previous examples
//Compute deviation for each row
scala&gt; val devs = clean_data.select(((clean_data("salary") - mean_salary) *
        (clean_data("salary") - mean_salary)).alias("deviation"))
devs: org.apache.spark.sql.DataFrame = [deviation: double]

//Compute standard deviation
scala&gt; val stddev = devs.select(sqrt(avg("deviation"))).
            first().getDouble(0)
stddev: Double = 29160.932595617614

//If you want to round the stddev value, use BigDecimal as shown
scala&gt; scala.math.BigDecimal(stddev).setScale(2,
             BigDecimal.RoundingMode.HALF_UP)
res14: scala.math.BigDecimal = 29160.93
scala&gt;

//Replace outliers beyond 2 standard deviations with the mean salary
scala&gt; val outlierfunc = udf((value: Long, mean: Double) =&gt; {if (value &gt; mean+(2*stddev)
            || value &lt; mean-(2*stddev)) mean else value})

//Use the UDF to compute updated_salary
//Note the usage of lit() to wrap a literal as a column
scala&gt; val no_outlier = clean_data.withColumn("updated_salary",
            outlierfunc(col("salary"),lit(mean_salary)))

//Observe modified values
scala&gt; no_outlier.filter(no_outlier("salary") =!=  //Not !=
             no_outlier("updated_salary")).show()
+------+----+---+-------+------+--------------+
|emp_id|name|age|   role|salary|updated_salary|
+------+----+---+-------+------+--------------+
|    13| Max| 31|Manager|120000|       20843.0|
+------+----+---+-------+------+--------------+</pre></div><div class="calibre2" title="Duplicate values treatment"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec27" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Duplicate values treatment</h3></div></div></div><p class="calibre11">There are different ways of treating the duplicate records in a dataset. We will demonstrate those in the following code snippets:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">// Deleting the duplicate rows
&gt;&gt;&gt; authors = [['Thomas','Hardy','June 2,1840'],
    ['Thomas','Hardy','June 2,1840'],
    ['Thomas','H',None],
    ['Jane','Austen','16 December 1775'],
    ['Emily',None,None]]
&gt;&gt;&gt; df1 = sc.parallelize(authors).toDF(
      ["FirstName","LastName","Dob"])
&gt;&gt;&gt; df1.show()
+---------+--------+----------------+
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|   Thomas|   Hardy|    June 2, 1840|
|   Thomas|   Hardy|    June 2, 1840|
|   Thomas|       H|            null|
|     Jane|  Austen|16 December 1775|
|    Emily|    null|            null|
+---------+--------+----------------+

// Drop duplicated rows
&gt;&gt;&gt; df1.dropDuplicates().show()
+---------+--------+----------------+
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|    Emily|    null|            null|
|     Jane|  Austen|16 December 1775|
|   Thomas|       H|            null|
|   Thomas|   Hardy|    June 2, 1840|
+---------+--------+----------------+

// Drop duplicates based on a sub set of columns
&gt;&gt;&gt; df1.dropDuplicates(subset=["FirstName"]).show()
+---------+--------+----------------+
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|    Emily|    null|            null|
|   Thomas|   Hardy|    June 2, 1840|
|     Jane|  Austen|16 December 1775|
+---------+--------+----------------+
&gt;&gt;&gt; </pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//Duplicate values treatment
// Reusing the Author case class
// Deleting the duplicate rows
scala&gt; val authors = Seq(
            Author("Thomas","Hardy","June 2,1840"),
            Author("Thomas","Hardy","June 2,1840"),
            Author("Thomas","H",null),
            Author("Jane","Austen","16 December 1775"),
            Author("Emily",null,null))
authors: Seq[Author] = List(Author(Thomas,Hardy,June 2,1840), Author(Thomas,Hardy,June 2,1840), Author(Thomas,H,null), Author(Jane,Austen,16 December 1775), Author(Emily,null,null))
scala&gt; val ds1 = sc.parallelize(authors).toDS()
ds1: org.apache.spark.sql.Dataset[Author] = [FirstName: string, LastName: string ... 1 more field]
scala&gt; ds1.show()
+---------+--------+----------------+
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|   Thomas|   Hardy|     June 2,1840|
|   Thomas|   Hardy|     June 2,1840|
|   Thomas|       H|            null|
|     Jane|  Austen|16 December 1775|
|    Emily|    null|            null|
+---------+--------+----------------+
scala&gt;

// Drop duplicated rows
scala&gt; ds1.dropDuplicates().show()
+---------+--------+----------------+                                          
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|     Jane|  Austen|16 December 1775|
|    Emily|    null|            null|
|   Thomas|   Hardy|     June 2,1840|
|   Thomas|       H|            null|
+---------+--------+----------------+
scala&gt;

// Drop duplicates based on a sub set of columns
scala&gt; ds1.dropDuplicates("FirstName").show()
+---------+--------+----------------+                                           
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|    Emily|    null|            null|
|     Jane|  Austen|16 December 1775|
|   Thomas|   Hardy|     June 2,1840|
+---------+--------+----------------+</pre></div></div><div class="calibre2" title="Data transformation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec47" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data transformation</h2></div></div></div><p class="calibre11">There can be various kinds of data transformation needs and every case is mostly unique. We are going to cover some basic types of transformations, as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Merging two columns into one</li><li class="listitem">Adding characters/numbers to the existing ones</li><li class="listitem">Deleting or replacing characters/numbers from the existing ones</li><li class="listitem">Changing date formats</li></ul></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">// Merging columns
//Create a udf to concatenate two column values
&gt;&gt;&gt; import pyspark.sql.functions
&gt;&gt;&gt; concat_func = pyspark.sql.functions.udf(lambda name, age: name + "_" + str(age))

//Apply the udf to create merged column
&gt;&gt;&gt; concat_df = final_data.withColumn("name_age", concat_func(final_data.name, final_data.age))
&gt;&gt;&gt; concat_df.show(4)
+------+----+---+---------+------+--------+
|emp_id|name|age|     role|salary|name_age|
+------+----+---+---------+------+--------+
|     1|John| 25|Associate| 10000| John_25|
|     2| Ray| 35|  Manager| 12000|  Ray_35|
|     3|Mike| 24|  Manager| 12000| Mike_24|
|     4|Jane| 28|Associate|  null| Jane_28|
+------+----+---+---------+------+--------+
only showing top 4 rows
// Adding constant to data
&gt;&gt;&gt; data_new = concat_df.withColumn("age_incremented",concat_df.age + 10)
&gt;&gt;&gt; data_new.show(4)
+------+----+---+---------+------+--------+---------------+
|emp_id|name|age|     role|salary|name_age|age_incremented|
+------+----+---+---------+------+--------+---------------+
|     1|John| 25|Associate| 10000| John_25|             35|
|     2| Ray| 35|  Manager| 12000|  Ray_35|             45|
|     3|Mike| 24|  Manager| 12000| Mike_24|             34|
|     4|Jane| 28|Associate|  null| Jane_28|             38|
+------+----+---+---------+------+--------+---------------+
only showing top 4 rows
&gt;&gt;&gt; 

//Replace values in a column
&gt;&gt;&gt; df1.replace('Emily','Charlotte','FirstName').show()
+---------+--------+----------------+
|FirstName|LastName|             Dob|
+---------+--------+----------------+
|   Thomas|   Hardy|    June 2, 1840|
|  Charles| Dickens| 7 February 1812|
|     Mark|   Twain|            null|
|     Jane|  Austen|16 December 1775|
|Charlotte|    null|            null|
+---------+--------+----------------+

// If the column name argument is omitted in replace, then replacement is applicable to all columns
//Append new columns based on existing values in a column
//Give 'LastName' instead of 'Initial' if you want to overwrite
&gt;&gt;&gt; df1.withColumn('Initial',df1.LastName.substr(1,1)).show()
+---------+--------+----------------+-------+
|FirstName|LastName|             Dob|Initial|
+---------+--------+----------------+-------+
|   Thomas|   Hardy|    June 2, 1840|      H|
|  Charles| Dickens| 7 February 1812|      D|
|     Mark|   Twain|            null|      T|
|     Jane|  Austen|16 December 1775|      A|
|    Emily|    null|            null|   null|
+---------+--------+----------------+-------+</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">// Merging columns
//Create a udf to concatenate two column values
scala&gt; val concatfunc = udf((name: String, age: Integer) =&gt;
                           {name + "_" + age})
concatfunc: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function2&gt;,StringType,Some(List(StringType, IntegerType)))
scala&gt;

//Apply the udf to create merged column
scala&gt; val concat_df = final_data.withColumn("name_age",
                         concatfunc($"name", $"age"))
concat_df: org.apache.spark.sql.DataFrame =
         [emp_id: int, name: string ... 4 more fields]
scala&gt; concat_df.show(4)
+------+----+---+---------+------+--------+
|emp_id|name|age|     role|salary|name_age|
+------+----+---+---------+------+--------+
|     1|John| 25|Associate| 10000| John_25|
|     2| Ray| 35|  Manager| 12000|  Ray_35|
|     3|Mike| 24|  Manager| 12000| Mike_24|
|     4|Jane| 28|Associate|  null| Jane_28|
+------+----+---+---------+------+--------+
only showing top 4 rows
scala&gt;

// Adding constant to data
scala&gt; val addconst = udf((age: Integer) =&gt; {age + 10})
addconst: org.apache.spark.sql.expressions.UserDefinedFunction =
      UserDefinedFunction(&lt;function1&gt;,IntegerType,Some(List(IntegerType)))
scala&gt; val data_new = concat_df.withColumn("age_incremented",
                 addconst(col("age")))
data_new: org.apache.spark.sql.DataFrame =
     [emp_id: int, name: string ... 5 more fields]
scala&gt; data_new.show(4)
+------+----+---+---------+------+--------+---------------+
|emp_id|name|age|     role|salary|name_age|age_incremented|
+------+----+---+---------+------+--------+---------------+
|     1|John| 25|Associate| 10000| John_25|             35|
|     2| Ray| 35|  Manager| 12000|  Ray_35|             45|
|     3|Mike| 24|  Manager| 12000| Mike_24|             34|
|     4|Jane| 28|Associate|  null| Jane_28|             38|
+------+----+---+---------+------+--------+---------------+
only showing top 4 rows

// Replace values in a column
//Note: As of Spark 2.0.0, there is no replace on DataFrame/ Dataset does not work so .na. is a work around
scala&gt; ds1.na.replace("FirstName",Map("Emily" -&gt; "Charlotte")).show()
+---------+--------+---------------+
|FirstName|LastName|            Dob|
+---------+--------+---------------+
|   Thomas|   Hardy|   June 2, 1840|
|  Charles| Dickens|7 February 1812|
|     Mark|   Twain|           null|
|Charlotte|    null|           null|
+---------+--------+---------------+
scala&gt;

// If the column name argument is "*" in replace, then replacement is applicable to all columns
//Append new columns based on existing values in a column
//Give "LastName" instead of "Initial" if you want to overwrite
scala&gt; ds1.withColumn("Initial",ds1("LastName").substr(1,1)).show()
+---------+--------+---------------+-------+
|FirstName|LastName|            Dob|Initial|
+---------+--------+---------------+-------+
|   Thomas|   Hardy|   June 2, 1840|      H|
|  Charles| Dickens|7 February 1812|      D|
|     Mark|   Twain|           null|      T|
|    Emily|    null|           null|   null|
+---------+--------+---------------+-------+</pre><p class="calibre11">Now that we are familiar with basic examples, let us put together a somewhat complex example. You might have noticed that the date column in Authors data has different date formats. In some cases, month is followed by day, and vice versa. Such anomalies are common in the real world, wherein data might be collected from different sources. Here, we are looking at a case where the date column has data points with many different date formats. We need to standardize all the different date formats into one format. To do so, we first have to create a <span class="strong"><strong class="calibre19">user-defined function</strong></span> (<span class="strong"><strong class="calibre19">udf</strong></span>) that can take care of the different formats and convert those to one common format.</p><pre class="programlisting">// Date conversions
//Create udf for date conversion that converts incoming string to YYYY-MM-DD format
// The function assumes month is full month name and year is always 4 digits
// Separator is always a space or comma
// Month, date and year may come in any order
//Reusing authors data
&gt;&gt;&gt; authors = [['Thomas','Hardy','June 2, 1840'],
        ['Charles','Dickens','7 February 1812'],
        ['Mark','Twain',None],
        ['Jane','Austen','16 December 1775'],
        ['Emily',None,None]]
&gt;&gt;&gt; df1 = sc.parallelize(authors).toDF(
      ["FirstName","LastName","Dob"])
&gt;&gt;&gt; 

// Define udf
//Note: You may create this in a script file and execute with execfile(filename.py)
&gt;&gt;&gt; def toDate(s):
 import re
 year = month = day = ""
 if not s:
  return None
 mn = [0,'January','February','March','April','May',
  'June','July','August','September',
  'October','November','December']

 //Split the string and remove empty tokens
 l = [tok for tok in re.split(",| ",s) if tok]
 
//Assign token to year, month or day
 for a in l:
  if a in mn:
   month = "{:0&gt;2d}".format(mn.index(a))
  elif len(a) == 4:
   year = a
  elif len(a) == 1:
   day = '0' + a
  else:
   day = a
 return year + '-' + month + '-' + day
&gt;&gt;&gt; 

//Register the udf
&gt;&gt;&gt; from pyspark.sql.functions import udf
&gt;&gt;&gt; from pyspark.sql.types import StringType
&gt;&gt;&gt; toDateUDF = udf(toDate, StringType())

//Apply udf
&gt;&gt;&gt; df1.withColumn("Dob",toDateUDF("Dob")).show()
+---------+--------+----------+
|FirstName|LastName|       Dob|
+---------+--------+----------+
|   Thomas|   Hardy|1840-06-02|
|  Charles| Dickens|1812-02-07|
|     Mark|   Twain|      null|
|     Jane|  Austen|1775-12-16|
|    Emily|    null|      null|
+---------+--------+----------+
&gt;&gt;&gt; </pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">//Date conversions
//Create udf for date conversion that converts incoming string to YYYY-MM-DD format
// The function assumes month is full month name and year is always 4 digits
// Separator is always a space or comma
// Month, date and year may come in any order
//Reusing authors case class and data
&gt;&gt;&gt; val authors = Seq(
        Author("Thomas","Hardy","June 2, 1840"),
        Author("Charles","Dickens","7 February 1812"),
        Author("Mark","Twain",null),
        Author("Jane","Austen","16 December 1775"),
        Author("Emily",null,null))
authors: Seq[Author] = List(Author(Thomas,Hardy,June 2, 1840), Author(Charles,Dickens,7 February 1812), Author(Mark,Twain,null), Author(Jane,Austen,16 December 1775), Author(Emily,null,null))
scala&gt; val ds1 = sc.parallelize(authors).toDS()
ds1: org.apache.spark.sql.Dataset[Author] = [FirstName: string, LastName: string ... 1 more field]
scala&gt;

// Define udf
//Note: You can type :paste on REPL to paste  multiline code. CTRL + D signals end of paste mode
def toDateUDF = udf((s: String) =&gt; {
    var (year, month, day) = ("","","")
    val mn = List("","January","February","March","April","May",
        "June","July","August","September",
        "October","November","December")
    //Tokenize the date string and remove trailing comma, if any
    if(s != null) {
      for (x &lt;- s.split(" ")) {
        val token = x.stripSuffix(",")
        token match {
        case "" =&gt;
        case x if (mn.contains(token)) =&gt;
            month = "%02d".format(mn.indexOf(token))
        case x if (token.length() == 4) =&gt;
            year = token
        case x =&gt;
            day = token
        }
     }   //End of token processing for
     year + "-" + month + "-" + day=
   } else {
       null
   }
})
toDateUDF: org.apache.spark.sql.expressions.UserDefinedFunction
scala&gt;

//Apply udf and convert date strings to standard form YYYY-MM-DD
scala&gt; ds1.withColumn("Dob",toDateUDF(ds1("Dob"))).show()
+---------+--------+----------+
|FirstName|LastName|       Dob|
+---------+--------+----------+
|   Thomas|   Hardy| 1840-06-2|
|  Charles| Dickens| 1812-02-7|
|     Mark|   Twain|      null|
|     Jane|  Austen|1775-12-16|
|    Emily|    null|      null|
+---------+--------+----------+</pre><p class="calibre11">That lines up the date of birth strings neatly. We can keep fine-tuning the udf as we encounter more varieties of date formats.</p><p class="calibre11">At this stage, before getting started with data analysis, it is extremely important that you should pause for a moment and re-evaluate the actions you have taken from starting data acquisition to cleaning and transforming it. There have been a lot of cases where tremendous time and effort involved went for a toss and led to project failure because of incorrect data being analyzed and modeled. Such cases became perfect examples of a famous computer adage - <span class="strong"><strong class="calibre19">Garbage In, Garbage Out</strong></span> (<span class="strong"><strong class="calibre19">GIGO</strong></span>).</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Basics of statistics"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch05lvl1sec37" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Basics of statistics</h1></div></div></div><p class="calibre11">The field of statistics is all about using mathematical procedures to summarize the raw facts and figures of a dataset in some meaningful way so that it makes sense to you. This includes, and is not limited to: gathering data, analyzing it, interpreting it, and representing it.</p><p class="calibre11">The field of statistics exists mainly because it is usually impossible to collect data for the entire population. So using statistical techniques, we estimate the population parameters using the sample statistics by addressing the uncertainties.</p><p class="calibre11">In this section, we will cover some basic statistics and analysis techniques on which we are going to build up our complete understanding of the concepts covered in this book.</p><p class="calibre11">The study of statistics can be broadly categorized into two main branches:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Descriptive statistics</li><li class="listitem">Inferential statistics</li></ul></div><p class="calibre11">The following diagram depicts these two terms and shows how we estimate the population parameters from samples:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_002.jpg" alt="Basics of statistics" class="calibre47"/></div><p class="calibre11">
</p><p class="calibre11">Before we get started on these, it is important to get some familiarity with sampling and distributions.</p><div class="calibre2" title="Sampling"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec48" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sampling</h2></div></div></div><p class="calibre11">Through sampling techniques, we just take a portion of the population dataset and work on it:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_003.jpg" alt="Sampling" class="calibre48"/></div><p class="calibre11">
</p><p class="calibre11">But why do we sample? The following are various reasons for sampling:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Difficult to get the entire population's data; for example, the heights of the citizens of any country.</li><li class="listitem">Difficult to process the entire dataset. When we talk about big data computing platforms such as Spark, the scope of this challenge nearly disappears. However, there can be scenarios where you have to treat the entire data at hand as a sample and extrapolate your analysis result to a future time or to a larger population.</li><li class="listitem">Difficult to plot voluminous data to visualize it. There can be technical limitations to it.</li><li class="listitem">For validation of your analysis or validation of your predictive models - especially when you are working with small datasets and you have to rely on cross-validation.</li></ul></div><p class="calibre11">For effective sampling, there are two important constraints: one is determining the sample size and the other is the technique to choose for sampling. The sample size greatly influences the estimation of population parameters. We will cover this aspect later in this chapter after covering some of the prerequisite basics. In this section, we will focus on sampling techniques.</p><p class="calibre11">There are various probability-based (the probability of each sample being selected is known) and non-probability-based (the probability of each sample being selected is not known) sampling techniques available, but we are going to limit our discussion to probability-based techniques only.</p><div class="calibre2" title="Simple random sample"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec28" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Simple random sample</h3></div></div></div><p class="calibre11">The <span class="strong"><strong class="calibre19">simple random sample</strong></span> (<span class="strong"><strong class="calibre19">SRS</strong></span>) is the most basic type of probability sampling method, where every element has the same probability of being chosen. This means that every possible sample of <span class="strong"><em class="calibre22">n</em></span> elements has an equal chance of selection.</p></div><div class="calibre2" title="Systematic sampling"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec29" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Systematic sampling</h3></div></div></div><p class="calibre11">Systematic sampling is probably the simplest of all probability-based sampling techniques, where every <span class="strong"><em class="calibre22">kth</em></span> element of the population is sampled. So this is otherwise known as interval sampling. It starts with a fixed starting point chosen at random and then an interval is estimated (the <span class="strong"><em class="calibre22">kth</em></span> element, where <span class="strong"><em class="calibre22">k = (population size)/(sample size)</em></span>). Here, the progression through the elements is circled to start from the beginning when it reaches the end till your sample size is reached.</p></div><div class="calibre2" title="Stratified sampling"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec30" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Stratified sampling</h3></div></div></div><p class="calibre11">This sampling technique is preferred when the subgroups or subpopulations within the population vary, because other sampling techniques might not help extract a sample that is a good representative of the population. Through stratified sampling, the population is divided into homogeneous subgroups called <span class="strong"><strong class="calibre19">strata</strong></span> and a sample is taken by randomly selecting the subjects from those strata in proportion to the population. So, the stratum size to population size ratio is maintained in the sample as well:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">/* ”Sample” function is defined for DataFrames (not RDDs) which takes three parameters:
withReplacement - Sample with replacement or not (input: True/False)
fraction - Fraction of rows to generate (input: any number between 0 and 1 as per your requirement of sample size)
seed - Seed for sampling (input: Any random seed)
*/
&gt;&gt;&gt; sample1 = data_new.sample(False, 0.6) //With random seed as no seed value specified
&gt;&gt;&gt; sample2 = data_new.sample(False, 0.6, 10000) //With specific seed value of 10000</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>:</p><pre class="programlisting">scala&gt; val sample1 = data_new.sample(false, 0.6) //With random seed as no seed value specified
sample1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [emp_id: int, name: string ... 5 more fields]
scala&gt; val sample2 = data_new.sample(false, 0.6, 10000) //With specific seed value of 10000
sample2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [emp_id: int, name: string ... 5 more fields]</pre><div class="note" title="Note"><div class="inner"><h3 class="title6"><a id="note8" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Note</h3><p class="calibre25">We only looked at sampling on DataFrames; there are MLlib library functions such as <code class="literal">sampleByKey</code> and <code class="literal">sampleByKeyExact</code> to do stratified sampling on RDDs of key-value pairs. Check out <code class="literal">spark.util.random</code> package for Bernoulli , Poisson or Random samplers.</p></div></div></div></div><div class="calibre2" title="Data distributions"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec49" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data distributions</h2></div></div></div><p class="calibre11">Understanding how your data is distributed is one of the primary tasks you need to perform to turn data into information. Analyzing the distributions of the variables helps detect the outliers, visualize the trends in the data, and can also shape up your understanding for the data at hand. This helps in thinking right and taking the right approaches in solving a business problem. Plotting the distributions makes it visually more intuitive and we will cover this aspect in the <span class="strong"><em class="calibre22">Descriptive statistics</em></span> section.</p><div class="calibre2" title="Frequency distributions"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec31" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Frequency distributions</h3></div></div></div><p class="calibre11">Frequency distribution explains which values a variable takes and how often it takes those values. It is usually represented with a table with each possible value with its corresponding number of occurrences.</p><p class="calibre11">Let's consider an example where we roll a six-sided die 100 times and observe the following frequencies:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/Chapter-5_NEw.jpg" alt="Frequency distributions" class="calibre49"/><div class="caption">Frequency Table</div></div><p class="calibre11">
</p><p class="calibre11">Similarly, you might observe different distributions on every set of 100 rolls of the die because it would depend on chance.</p><p class="calibre11">At times, you might be interested in the proportions of occurrences instead of just occurrences. In the preceding die roll example, we rolled the die 100 times in total, so the proportionate distribution or the <span class="strong"><strong class="calibre19">relative frequency distribution</strong></span> would appear as follows:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/Final-5-RT-3.jpg" alt="Frequency distributions" class="calibre50"/><div class="caption">Relative Frequency Table</div></div><p class="calibre11">
</p></div><div class="calibre2" title="Probability distributions"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec32" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Probability distributions</h3></div></div></div><p class="calibre11">In the same example of die rolling, we know that a total probability of 1 is distributed across all faces of the die. This means that a probability of 1/6 (approximately 0.167) is associated with face 1 through face 6. Irrespective of the number of times you roll a die (a fair die!), the same probability of 1/6 would be distributed evenly on all sides of the die. So, if you plot this distribution, it would appear as follows:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/Chapter-new.jpg" alt="Probability distributions" class="calibre51"/><div class="caption">Probability Distribution</div></div><p class="calibre11">
</p><p class="calibre11">We looked at three kinds of distributions here - frequency distributions, relative frequency distribution, and probability distribution.</p><p class="calibre11">This probability distribution is actually the distribution of the population. In real-world cases, at times we have prior knowledge of the population distribution (in our example, it is the probability of 0.167 on all six sides of a fair die) and at times we don't. In scenarios where we don't have the population distribution, finding the distribution of the population itself becomes part of your inferential statistics. Also, unlike the fair die example, where the same probability is associated with all the sides, there can be different probabilities associated with the values a variable can take and they can follow a particular type of distribution as well.</p><p class="calibre11">Now it's time to reveal the secret! The relation between the relative frequency distribution and the probability distribution is the basis of statistical inference. The relative frequency distributions are also called empirical distributions based on what we observe in the samples we take (here, it is a sample of 100). As discussed earlier, the empirical distributions of every 100 rolls of the die would differ depending on chance. Now, the larger the number of rolls, the closer will be the relative frequency distribution to the probability distribution. So, the relative frequencies of an infinite number of die rolls is the probability distribution, which in turn is the population distribution.</p><p class="calibre11">There are various kinds of probability distributions, which are again categorized into two, based on the type of variable - categorical or continuous. We will cover these distributions in detail in the subsequent sections of this chapter. However, we should know what these categories imply! Categorical variables can take on only a few categories; for example, pass/fail, zero/one, cancer/malignant are examples of categorical variables with two categories. Similarly, a categorical variable can have more categories, such as red/green/blue, type1/type2/type3/type4, and so on. Continuous variables can take on any value in a given range and measured on a continuous scale, for example, age, height, salary, and so on. Theoretically, there can be an infinite number of possible values between any two values of a continuous variable. For example, between 5'6" and 6'4" height values (foot and inch scale), there can be many fractional values possible. The same holds true when measured in a centimeter scale as well.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Descriptive statistics"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch05lvl1sec38" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Descriptive statistics</h1></div></div></div><p class="calibre11">In the previous section, we learnt how distributions are formed. In this section, we will learn how to describe them through descriptive statistics. There are two important components of a distribution that can help describe it, which are its location and its spread.</p><div class="calibre2" title="Measures of location"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec50" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Measures of location</h2></div></div></div><p class="calibre11">A measure of location is a single value that describes where the center of the data lies. The three most common measures of location are mean, median, and mode.</p><div class="calibre2" title="Mean"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec33" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Mean</h3></div></div></div><p class="calibre11">By far the most common and widely used measure of central tendency is the <span class="strong"><strong class="calibre19">mean</strong></span>, which is otherwise known as the average. Whether it is a sample or a population, the mean or average is the summation of all the elements divided by the total number of elements.</p></div><div class="calibre2" title="Median"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec34" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Median</h3></div></div></div><p class="calibre11">The <span class="strong"><strong class="calibre19">median</strong></span> is the middle value of a series of data when sorted in any order so that half of the data is greater than the median and the other half smaller. When there are two middle values (with an even number of data items), the median is the average of those middle two. Medians are better measures of location when the data has outliers (extreme values).</p></div><div class="calibre2" title="Mode"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec35" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Mode</h3></div></div></div><p class="calibre11">The <span class="strong"><strong class="calibre19">mode</strong></span> is the most frequent data item. It can be determined for both qualitative and quantitative data.</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><p class="calibre11">//Reusing data_new created in duplicated value treatment</p><pre class="programlisting">&gt;&gt;&gt; mean_age = data_new.agg({'age': 'mean'}).first()[0]
&gt;&gt;&gt; age_counts = data_new.groupBy("age").agg({"age": "count"}).alias("freq")
&gt;&gt;&gt; mode_age = age_counts.sort(age_counts["COUNT(age)"].desc(), age_counts.age.asc()).first()[0]
&gt;&gt;&gt; print(mean_age, mode_age)
(29.615384615384617, 25)
&gt;&gt;&gt; age_counts.sort("count(age)",ascending=False).show(2)
+---+----------+                                                               
|age|count(age)|
+---+----------+
| 28|         3|
| 29|         2|
+---+----------+
only showing top 2 rows</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">//Reusing data_new created 
scala&gt; val mean_age = data_new.select(floor(avg("age"))).first().getLong(0)
mean_age: Long = 29
scala&gt; val mode_age = data_new.groupBy($"age").agg(count($"age")).
                 sort($"count(age)".desc, $"age").first().getInt(0)
mode_age: Int = 28
scala&gt; val age_counts = data_new.groupBy("age").agg(count($"age") as "freq")
age_counts: org.apache.spark.sql.DataFrame = [age: int, freq: bigint]
scala&gt; age_counts.sort($"freq".desc).show(2)
+---+----+                                                                     
|age|freq|
+---+----+
| 35|   2|
| 28|   2|
+---+----+</pre></div></div><div class="calibre2" title="Measures of spread"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec51" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Measures of spread</h2></div></div></div><p class="calibre11">Measures of spread describe how close or scattered the data is for a particular variable or data item.</p><div class="calibre2" title="Range"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec36" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Range</h3></div></div></div><p class="calibre11">The range is the difference between the smallest and largest values of a variable. One disadvantage to it is that it does not take into account every value in the data.</p></div><div class="calibre2" title="Variance"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec37" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Variance</h3></div></div></div><p class="calibre11">To find the variability in the dataset, we can subtract each value from the mean, square them up so it gets rid of the negative signs (also scales up the magnitude), and then sum them all and divide by the total number of values:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_007.jpg" alt="Variance" class="calibre52"/></div><p class="calibre11">
</p><p class="calibre11">If the data is more spread out, the variance will be a large number. One disadvantage to it is that it gives undue weight to the outliers.</p></div><div class="calibre2" title="Standard deviation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec38" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Standard deviation</h3></div></div></div><p class="calibre11">Like variance, standard deviation is also a measure of dispersion within the data. Variance had the limitation that the unit of data was also squared along with the data, so it was difficult to relate the variance with the values in the dataset. So, standard deviation is calculated as the square root of the variance:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_008.jpg" alt="Standard deviation" class="calibre53"/></div><p class="calibre11">
</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">//Reusing data_new created before
import math
&gt;&gt;&gt; range_salary = data_new.agg({'salary': 'max'}).first()[0] - data_new.agg({'salary': 'min'}).first()[0]
&gt;&gt;&gt; mean_salary = data_new.agg({'salary': 'mean'}).first()[0]
&gt;&gt;&gt; salary_deviations = data_new.select(((data_new.salary - mean_salary) *
       (data_new.salary - mean_salary)).alias("deviation"))
&gt;&gt;&gt; stddev_salary = math.sqrt(salary_deviations.agg({'deviation' : 
'avg'}).first()[0])
&gt;&gt;&gt; variance_salary = salary_deviations.groupBy().avg("deviation").first()[0]
&gt;&gt;&gt; print(round(range_salary,2), round(mean_salary,2),
      round(variance_salary,2), round(stddev_salary,2))
(119880.0, 20843.33, 921223322.22, 30351.66)
&gt;&gt;&gt; </pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">//Reusing data_new created before
scala&gt; val range_salary = data_new.select(max("salary")).first().
          getLong(0) - data_new.select(min("salary")).first().getLong(0)
range_salary: Long = 119880
scala&gt; val mean_salary = data_new.select(floor(avg("salary"))).first().getLong(0)
mean_salary: Long = 20843
scala&gt; val salary_deviations = data_new.select(((data_new("salary") - mean_salary)
                     * (data_new("salary") - mean_salary)).alias("deviation"))
salary_deviations: org.apache.spark.sql.DataFrame = [deviation: bigint]
scala&gt; val variance_salary = { salary_deviations.select(avg("deviation"))
                                       .first().getDouble(0) }
variance_salary: Double = 9.212233223333334E8
scala&gt; val stddev_salary = { salary_deviations
                    .select(sqrt(avg("deviation")))
                    .first().getDouble(0) }
stddev_salary: Double = 30351.660948510435</pre></div></div><div class="calibre2" title="Summary statistics"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec52" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary statistics</h2></div></div></div><p class="calibre11">The summary statistics of a dataset is extremely useful information that gives us a quick insight into the data at hand. Using the function <code class="literal">colStats</code> available in statistics, we can obtain a multivariate statistical summary of <code class="literal">RDD[Vector]</code> which contains column-wise max, min, mean, variance, number of non-zeros, and the total count. Let us explore this through some code examples:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">&gt;&gt;&gt; import numpy
&gt;&gt;&gt; from pyspark.mllib.stat import Statistics
// Create an RDD of number vectors
//This example creates an RDD with 5 rows with 5 elements each
&gt;&gt;&gt; observations = sc.parallelize(numpy.random.random_integers(0,100,(5,5)))
// Compute column summary statistics.
//Note that the results may vary because of random numbers
&gt;&gt;&gt; summary = Statistics.colStats(observations)
&gt;&gt;&gt; print(summary.mean())       // mean value for each column
&gt;&gt;&gt; print(summary.variance())  // column-wise variance
&gt;&gt;&gt; print(summary.numNonzeros())// number of nonzeros in each column</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">scala&gt; import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Vectors
scala&gt; import org.apache.spark.mllib.stat.{
          MultivariateStatisticalSummary, Statistics}
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
// Create an RDD of number vectors
//This example creates an RDD with 5 rows with 5 elements each
scala&gt; val observations = sc.parallelize(Seq.fill(5)(Vectors.dense(Array.fill(5)(
                    scala.util.Random.nextDouble))))
observations: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[43] at parallelize at &lt;console&gt;:27
scala&gt;
// Compute column summary statistics.
//Note that the results may vary because of random numbers
scala&gt; val summary = Statistics.colStats(observations)
summary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@36836161
scala&gt; println(summary.mean)  // mean value for each column
[0.5782406967737089,0.5903954680966121,0.4892908815930067,0.45680701799234835,0.6611492334819364]
scala&gt; println(summary.variance)    // column-wise variance
[0.11893608153330748,0.07673977181967367,0.023169197889513014,0.08882605965192601,0.08360159585590332]
scala&gt; println(summary.numNonzeros) // number of nonzeros in each column
[5.0,5.0,5.0,5.0,5.0]</pre><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="tip9" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Tip</h3><p class="calibre25">
</p><p class="calibre25">Apache Spark MLlib RDD-based API is in maintenance mode starting Spark 2.0. They are expected to deprecated in 2.2+ and removed in Spark 3.0.</p><p class="calibre25">
</p></div></div></div><div class="calibre2" title="Graphical techniques"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec53" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Graphical techniques</h2></div></div></div><p class="calibre11">To understand the behavior of your data points, you may have to plot them and see. You need a platform, however, to visualize your data in terms of <span class="strong"><em class="calibre22">box plots</em></span>, <span class="strong"><em class="calibre22">scatter plots</em></span>, or <span class="strong"><em class="calibre22">histograms</em></span>, to name a few. The iPython/Jupyter notebook or any other third-party notebook supported by Spark can be used for data visualization in your browser itself. Databricks provides their own notebook. Visualization is covered in its own chapter and this chapter focuses on the complete life cycle. However, Spark provides histogram data preparation out of the box so that bucket ranges and frequencies may be transferred to the client machine as against the complete dataset. The following example shows the same.</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">//Histogram
&gt;&gt;&gt;from random import randint
&gt;&gt;&gt; numRDD = sc.parallelize([randint(0,9) for x in xrange(1,1001)])
// Generate histogram data for given bucket count
&gt;&gt;&gt; numRDD.histogram(5)
([0.0, 1.8, 3.6, 5.4, 7.2, 9], [202, 213, 215, 188, 182])
//Alternatively, specify ranges
&gt;&gt;&gt; numRDD.histogram([0,3,6,10])
([0, 3, 6, 10], [319, 311, 370])</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//Histogram
scala&gt; val numRDD = sc.parallelize(Seq.fill(1000)(
                    scala.util.Random.nextInt(10)))
numRDD: org.apache.spark.rdd.RDD[Int] =
     ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24
// Generate histogram data for given bucket count
scala&gt; numRDD.histogram(5)
res10: (Array[Double], Array[Long]) = (Array(0.0, 1.8, 3.6, 5.4, 7.2, 9.0),Array(194, 209, 215, 195, 187))
scala&gt;
//Alternatively, specify ranges
scala&gt; numRDD.histogram(Array(0,3.0,6,10))
res13: Array[Long] = Array(293, 325, 382)</pre></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Inferential statistics"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch05lvl1sec39" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Inferential statistics</h1></div></div></div><p class="calibre11">We saw that descriptive statistics were extremely useful in describing and presenting data, but they did not provide a way to use the sample statistics to infer the population parameters or to validate any hypothesis we might have made. So, the techniques of inferential statistics surfaced to address such requirements. Some of the important uses of inferential statistics are:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Estimation of population parameters</li><li class="listitem">Hypothesis testing</li></ul></div><p class="calibre11">Please note that a sample can never represent a population perfectly because every time we sample, it naturally incurs sampling errors, hence the need for inferential statistics! Let us spend some time understanding the various types of probability distributions that can help infer the population parameters.</p><div class="calibre2" title="Discrete probability distributions"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec54" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Discrete probability distributions</h2></div></div></div><p class="calibre11">Discrete probability distributions are used to model data that is discrete in nature, which means that data can only take on certain values, such as integers. Unlike categorical variables, discrete variables can take on only numeric data, especially count data from a set of distinct whole values. Also, the sum of probabilities of all possible values of a random variable is one. The discrete probability distributions are described in terms of probability mass function. There can be various types of discrete probability distributions. The following are a few examples.</p><div class="calibre2" title="Bernoulli distribution"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec39" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Bernoulli distribution</h3></div></div></div><p class="calibre11">Bernoulli distribution is a type of distribution that describes the trials having only two possible outcomes, such as success/failure, head/tail, the face value of a six-sided die is 4 or not, the message sent was received or not, and so on. Bernoulli distribution can be generalized for any categorical variable with two or more possible outcomes.</p><p class="calibre11">Let's take the example of "students' pass rate for an exam" where 0.6 (60 percent) is the probability <span class="strong"><strong class="calibre19">P</strong></span> of the students passing the exam and 0.4 (40 percent) is the probability (<span class="strong"><strong class="calibre19">1-P</strong></span>) for the students to fail in the exam. Let us denote fail as <span class="strong"><strong class="calibre19">0</strong></span> and pass as <span class="strong"><strong class="calibre19">1</strong></span>:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_011.jpg" alt="Bernoulli distribution" class="calibre54"/></div><p class="calibre11">
</p><p class="calibre11">Such distributions cannot answer questions such as the expected pass rate of a student, because the expected value (μ) is going to be some fraction that this distribution cannot take. It can only mean that if you sample 1,000 students, then 600 would pass and 400 would fail.</p></div><div class="calibre2" title="Binomial distribution"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec40" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Binomial distribution</h3></div></div></div><p class="calibre11">This distribution can describe a series of Bernoulli trials (each with only two possible outcomes). Also, it assumes that the outcome of one trial does not affect the subsequent trials and that the probability of any event occurring is the same on every trial. An example of binomial distribution is tossing a coin five times. Here, the outcome of the first toss does not influence the outcome of the second toss, and the probability associated with each outcome is the same on all tosses.</p><p class="calibre11">If <span class="strong"><em class="calibre22">n</em></span> is the number of trials and <span class="strong"><em class="calibre22">p</em></span> is the probability of success in every trial, then the mean (μ) of this binomial distribution would be given by:</p><p class="calibre11">
<span class="strong"><em class="calibre22">μ = n * p</em></span>
</p><p class="calibre11">The variance (σ2x) would be given by:</p><p class="calibre11">
<span class="strong"><em class="calibre22">σ2x = n*p*(1-p).</em></span>
</p><p class="calibre11">In general, a random variable <span class="strong"><em class="calibre22">X</em></span> that follows binomial distribution with parameters <span class="strong"><em class="calibre22">n</em></span> and <span class="strong"><em class="calibre22">p</em></span>, we can write as <span class="strong"><em class="calibre22">X ~ B(n, p)</em></span>. For such a distribution, the probability of getting exactly <span class="strong"><em class="calibre22">k</em></span> successes in <span class="strong"><em class="calibre22">n</em></span> trials can be described by the probability mass function as follows:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_012.jpg" alt="Binomial distribution" class="calibre55"/></div><p class="calibre11">
</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_013.jpg" alt="Binomial distribution" class="calibre56"/></div><p class="calibre11">
</p><p class="calibre11">here, k = 0, 1, 2, ..., n</p><div class="calibre2" title="Sample problem"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title7"><a id="ch05lvl4sec0" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sample problem</h4></div></div></div><p class="calibre11">Let us assume a hypothetical scenario. Suppose 24 percent of companies in a city announced they would provide support to the tsunami-affected areas of the country as part of their CSR activity. In a sample of 20 companies chosen at random, find the probability of the number of companies that have announced they will help tsunami-affected areas:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Exactly three</li><li class="listitem">Less than three</li><li class="listitem">Three or more</li></ul></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Solution</strong></span>:</p><p class="calibre11">The sample size = <span class="strong"><em class="calibre22">n</em></span> = 20.</p><p class="calibre11">The probability that a company chosen at random has announced it will help = <span class="strong"><em class="calibre22">P = 0.24</em></span>.</p><p class="calibre11">a) P(x = 3) = <sup class="calibre57">20</sup>C<sub class="calibre58">3</sub> (0.24)<sup class="calibre57">3</sup> (0.76)<sup class="calibre57"> 17</sup> = 0.15</p><p class="calibre11">b) P(x &lt; 3) = P(0) + P(1) + P(2)</p><p class="calibre11">= (0.76)<sup class="calibre57"> 20</sup> + <sup class="calibre57">20</sup>C<sub class="calibre58">1</sub> (0.24) (0.76)<sup class="calibre57">19 </sup>+ <sup class="calibre57">20</sup>C<sub class="calibre58">2</sub> (0.24)<sup class="calibre57">2</sup> (0.76)<sup class="calibre57">18</sup></p><p class="calibre11"> = 0.0041 + 0.0261 + 0.0783 = 0.11</p><p class="calibre11">c) P(x &gt;= 3) = 1 - P(x &lt;= 2) = 1- 0.11 = 0.89</p><p class="calibre11">Note that binomial distribution is widely used in scenarios where you want to model the success rate in a sample of size <span class="strong"><em class="calibre22">n</em></span> drawn from a population of size <span class="strong"><em class="calibre22">N</em></span>, with replacement. If it is done without replacement then the draws will no longer be independent and hence will not follow binomial distribution rightly. However, such scenarios do exist and can be modeled using different types of distributions, such as hypergeometric distributions.</p></div></div><div class="calibre2" title="Poisson distribution"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec41" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Poisson distribution</h3></div></div></div><p class="calibre11">Poisson distribution can describe the probability of a given number of independent events that occur with a known average rate in a fixed interval of time or space. Please note that the events should only have binary outcomes such as success or failure, for example, the number of phone calls you receive per day or the number of cars passing a signal per hour. You need to carefully take a closer look at these examples. Please note here that you do not have the opposite half of this information, that is, how many phone calls you did not receive per day or how many cars did not pass that signal. Such data points do not have the other half of the information. On the contrary, if I say that 30 out of 50 students passed in an exam, you can easily infer that 20 students have failed! You have this other half of the information.</p><p class="calibre11">If <span class="strong"><em class="calibre22">µ</em></span> is the mean number of events occurring (a known average rate in a fixed interval of time or space) then the probability of <span class="strong"><em class="calibre22">k</em></span> events occurring at the same interval can be described by the probability mass function:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_014.jpg" alt="Poisson distribution" class="calibre59"/></div><p class="calibre11">
</p><p class="calibre11">here, <span class="strong"><em class="calibre22">k</em></span> = 0, 1, 2, 3...</p><p class="calibre11">The preceding equation describes the Poisson distribution.</p><p class="calibre11">For a Poisson distribution, mean and variance are the same. Also, the Poisson distribution tends to be more symmetric as its mean or variance increases.</p><div class="calibre2" title="Sample problem"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title7"><a id="ch05lvl4sec1" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sample problem</h4></div></div></div><p class="calibre11">Suppose you knew that the mean number of calls to a fire station on a weekday is eight. What is the probability that on a given weekday there would be 11 calls? This problem can be solved using the following formula based on the Poisson distribution:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_015.jpg" alt="Sample problem" class="calibre60"/></div><p class="calibre11">
</p></div></div></div><div class="calibre2" title="Continuous probability distributions"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec55" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Continuous probability distributions</h2></div></div></div><p class="calibre11">Continuous probability distributions are used to model data that is continuous in nature, which means that data can only take on any value within a specified range. So we deal with probabilities associated with intervals and not with any particular value as it is zero. Continuous probability distributions are the theoretical models of experiments; it is a relative frequency distribution built from an infinite number of observations. This means that when you reduce the interval, the number of observations increases, and as the number of observations increases more and more and approaches infinity, it forms a continuous probability distribution. The total area under the curve is one and to find the probability associated with any particular range, we have to find the area under the curve. Therefore, continuous distributions are normally described in terms of <span class="strong"><strong class="calibre19">probability density function</strong></span> (<span class="strong"><strong class="calibre19">PDF</strong></span>) which is of the following type:</p><p class="calibre11">P(a ≤ X ≤ b) = a∫<sup class="calibre57">b</sup> f(x) dx</p><p class="calibre11">There can be various types of continuous probability distributions. The following sections are a few examples.</p><div class="calibre2" title="Normal distribution"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec42" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Normal distribution</h3></div></div></div><p class="calibre11">A normal distribution is a simple, straightforward, yet very important continuous probability distribution. It is otherwise known as a Gaussian distribution or <span class="strong"><strong class="calibre19">bell curve</strong></span> because of its appearance when plotted. Also, for a perfect normal distribution, the mean, median, and mode are all the same.</p><p class="calibre11">Many naturally occurring phenomena follow a normal distribution (they may follow a different distribution as well!), such as the heights of people, errors in measurement, and so on. However, normal distributions are not suitable to model variables that are highly skewed or are inherently positive (for example, share prices or students' test scores where the difficulty level was minimal). Such variables may be better described by different distributions or by the normal distribution after a data transformation (like logarithmic transformation).</p><p class="calibre11">Normal distributions can be described using two descriptors: mean for the location of the center and standard deviation for the spread (height and width). The probability density function that represents a normal distribution is as follows:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_016.jpg" alt="Normal distribution" class="calibre61"/></div><p class="calibre11">
</p><p class="calibre11">One of the reasons this normal distribution tops the chart for popularity is because of the <span class="strong"><strong class="calibre19">Central Limit Theorem</strong></span> (<span class="strong"><strong class="calibre19">CLT</strong></span>). It states that, regardless of the population distribution, the mean of samples independently drawn from same population distribution is distributed almost normally and this normality increases more and more with the increase in sample size. This behavior is actually the basis of statistical hypothesis testing.</p><p class="calibre11">Additionally, every normal distribution, irrespective of its mean and standard deviation, follows an empirical rule (68-95-99.7 rule) which states that about 68 percent of the area under the curve falls within one standard deviation of the mean, 95 percent of the area under the curve falls within two standard deviations of the mean, and around 99.7 percent of the area under the curve falls within three standard deviations of the mean.</p><p class="calibre11">Now, to find the probability of an event, you can either use integral calculus or transform the distribution into a standard normal distribution as explained in the next section.</p></div><div class="calibre2" title="Standard normal distribution"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec43" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Standard normal distribution</h3></div></div></div><p class="calibre11">A standard normal distribution is a type of normal distribution with mean <span class="strong"><em class="calibre22">0</em></span> and standard deviation <span class="strong"><em class="calibre22">1</em></span>. Such a distribution is rarely found naturally. It is designed mainly to find the area under the curve of a normal distribution (instead of integrating using calculus) or to normalize the data points.</p><p class="calibre11">Suppose a random variable <span class="strong"><em class="calibre22">X</em></span> is normally distributed with mean (<span class="strong"><em class="calibre22">μ</em></span>) and standard deviation (<span class="strong"><em class="calibre22">σ</em></span>), then the random variable <span class="strong"><em class="calibre22">Z</em></span> will have a standard normal distribution with mean <span class="strong"><em class="calibre22">0</em></span> and standard deviation <span class="strong"><em class="calibre22">1</em></span>. The value of <span class="strong"><em class="calibre22">Z</em></span> can be found as:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_017.jpg" alt="Standard normal distribution" class="calibre62"/></div><p class="calibre11">
</p><p class="calibre11">Since data can be standardized in this manner, the data points can be represented and interpreted as <span class="strong"><em class="calibre22">how many standard deviations away from the mean</em></span> they lie in the distribution. It helps in comparing two distributions with different scales.</p><p class="calibre11">You can find the applications of a normal distribution in scenarios where one wants to find what percent would fall under a specified range - assuming that the distribution is approximately normal.</p><p class="calibre11">Consider the following example:</p><p class="calibre11">If the time a shopkeeper operates the shop on a given day follows the normal distribution with <span class="strong"><em class="calibre22">μ</em></span> = <span class="strong"><em class="calibre22">8</em></span> hours and <span class="strong"><em class="calibre22">σ</em></span> = <span class="strong"><em class="calibre22">0.5</em></span> hours, what is the probability that he stays at the shop for less than 7.5 hours?</p><p class="calibre11">The probability distribution would look as follows:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_018.jpg" alt="Standard normal distribution" class="calibre63"/><div class="caption">Data distribution</div></div><p class="calibre11">
</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/Capture.jpg" alt="Standard normal distribution" class="calibre64"/></div><p class="calibre11">
</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_020.jpg" alt="Standard normal distribution" class="calibre65"/><div class="caption">Standard normal distribution </div></div><p class="calibre11">
</p><p class="calibre11">So the probability that the shopkeeper stays at the shop for less than 7.5 hours is given by:</p><p class="calibre11">
<span class="strong"><em class="calibre22">P(z = -1) = 0.1587 = 15.87</em></span>
</p><div class="note" title="Note"><div class="inner"><h3 class="title6"><a id="note10" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Note</h3><p class="calibre25">This was figured out using the Z-table.</p></div></div><p class="calibre11">Please note that normality in a dataset is mostly an approximation. You first need to check the normality of the data and then proceed further if your analysis is based on the assumption of normality in data. There are various different ways to check for normality: you can opt for techniques such as histogram (with a curve fitted with the mean and standard deviation of the data), normal probability plot, or QQ plot.</p></div><div class="calibre2" title="Chi-square distribution"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec44" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chi-square distribution</h3></div></div></div><p class="calibre11">Chi-square distribution is one of the most widely used distributions in statistical inference. It is a special case of gamma distribution, which is useful in modeling skewed distributions of the variables that are not negative. It states that, if a random variable <span class="strong"><em class="calibre22">X</em></span> is normally distributed and <span class="strong"><em class="calibre22">Z</em></span> is one of its standard normal variables, then <span class="strong"><em class="calibre22">Z<sub class="calibre58">2</sub></em></span> will have a X<sub class="calibre58"><sup class="calibre66">2</sup></sub> distribution with one degree of freedom. Similarly, if we take many such random independent standard normal variables from the same distribution, square them and add them up, then that will also follow X<sub class="calibre58"><sup class="calibre66">2</sup></sub> distribution as follows:</p><p class="calibre11">
<span class="strong"><em class="calibre22">Z<sub class="calibre58">12</sub> + Z<sub class="calibre58">22</sub> + ... + Z<sub class="calibre58">k2</sub></em></span> will have X<sub class="calibre58">2</sub> distribution with <span class="strong"><em class="calibre22">k</em></span> degrees of freedom.</p><p class="calibre11">Chi-square distribution is mainly used for the inference of population variance or population standard deviation given the sample variance or standard deviation. This is because X<sub class="calibre58">2</sub> distribution is defined in an alternative way, in terms of the ratio of sample variance to population variance.</p><p class="calibre11">To justify this point, let us take a random sample (<span class="strong"><em class="calibre22">x<sub class="calibre58">1</sub></em></span>, <span class="strong"><em class="calibre22">x<sub class="calibre58">2</sub></em></span>,...,<span class="strong"><em class="calibre22">xn</em></span>) from a normal distribution with variance <span class="inlinemediaobject"><img src="Images/Ch.jpg" alt="Chi-square distribution" class="calibre67"/></span>
</p><p class="calibre11">The sample mean would be given by:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_021.jpg" alt="Chi-square distribution" class="calibre68"/></div><p class="calibre11">
</p><p class="calibre11">However, the sample variance is given by:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_022.jpg" alt="Chi-square distribution" class="calibre69"/></div><p class="calibre11">
</p><p class="calibre11">Considering the preceding mentioned facts, we can define the chi-square statistic as follows:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_023.jpg" alt="Chi-square distribution" class="calibre70"/></div><p class="calibre11">
</p><p class="calibre11">(Remember <span class="inlinemediaobject"><img src="Images/image_05_024.jpg" alt="Chi-square distribution" class="calibre71"/></span> and <span class="strong"><em class="calibre22">Z<sub class="calibre58">2</sub></em></span> will have X<sub class="calibre58">2</sub> distribution.)</p><p class="calibre11">So , <span class="inlinemediaobject"><img src="Images/image_05_025.jpg" alt="Chi-square distribution" class="calibre72"/></span>
</p><p class="calibre11">Therefore, the sampling distribution of the chi-square statistic will follow a chi-square distribution with <span class="strong"><em class="calibre22">(n-1)</em></span> degrees of freedom.</p><p class="calibre11">The probability density function of a chi-square distribution with <span class="strong"><em class="calibre22">n</em></span> degrees of freedom and gamma function <span class="strong"><em class="calibre22">Г</em></span> is given by:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_026.jpg" alt="Chi-square distribution" class="calibre73"/></div><p class="calibre11">
</p><p class="calibre11">For a <span class="strong"><em class="calibre22">χ2</em></span> distribution with <span class="strong"><em class="calibre22">k</em></span> degrees of freedom, mean (<span class="strong"><em class="calibre22">µ</em></span>) = <span class="strong"><em class="calibre22">k</em></span> and variance (<span class="strong"><em class="calibre22">σ2</em></span>) = <span class="strong"><em class="calibre22">2k.</em></span>
</p><p class="calibre11">Please note that chi-square distributions are positively skewed, but the degree of skewness decreases with the increase in the degree of freedom and approaches a normal distribution.</p><div class="calibre2" title="Sample problem"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title7"><a id="ch05lvl4sec2" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sample problem</h4></div></div></div><p class="calibre11">Find the 90 percent confidence interval for the variance and standard deviation for the price in dollars for adult single movie tickets. The data given represents a selected sample of nationwide movie theaters. Assume the variable is normally distributed.</p><p class="calibre11">Given sample (in $): 10, 08, 07, 11, 12, 06, 05, 09, 15, 12</p><p class="calibre11">Solution:</p><p class="calibre11">
<span class="strong"><em class="calibre22">N</em></span> = <span class="strong"><em class="calibre22">10</em></span>
</p><p class="calibre11">Mean of sample:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/Mean-of-sample.jpg" alt="Sample problem" class="calibre74"/></div><p class="calibre11">
</p><p class="calibre11">Variance of sample:</p><p class="calibre11"> </p><div class="mediaobject"><img src="Images/Variance.jpg" alt="Sample problem" class="calibre75"/></div><p class="calibre11">
</p><p class="calibre11">Standard deviation of sample:</p><p class="calibre11">S = sqrt(9.61)</p><p class="calibre11">Degree of freedom:</p><p class="calibre11">10-1 = 9</p><p class="calibre11">Now we need to find the 90 percent confidence interval, which means that 10 percent of the data will be left over in the tails.</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_027.jpg" alt="Sample problem" class="calibre76"/></div><p class="calibre11">
</p><p class="calibre11">Now, let us use the formula:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_028.jpg" alt="Sample problem" class="calibre77"/></div><p class="calibre11">
</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_029.jpg" alt="Sample problem" class="calibre78"/></div><p class="calibre11">
</p><p class="calibre11">Then we can either find the chi-square value using a table or a computer program.</p><p class="calibre11">To find the middle 90 percent confidence interval, we can consider the left 95 percent and right 5 percent.</p><p class="calibre11">So after substituting the numbers, we get:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_030.jpg" alt="Sample problem" class="calibre79"/></div><p class="calibre11">
</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_031.jpg" alt="Sample problem" class="calibre79"/></div><p class="calibre11">
</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_032.jpg" alt="Sample problem" class="calibre80"/></div><p class="calibre11">
</p><p class="calibre11">So, we can conclude that we are 90 percent confident that the standard deviation for the price of a single movie ticket of the population (all tickets in the nation) is between $2.26 and $5.10 based on a sample of 10 nationwide movie ticket prices.</p></div></div><div class="calibre2" title="Student's t-distribution"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec45" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Student's t-distribution</h3></div></div></div><p class="calibre11">Student's t-distribution is used in estimating the mean of a normally distributed population in the case where the population standard deviation is not known or the sample size is too small. In such cases, both <span class="strong"><em class="calibre22">μ</em></span> and <span class="strong"><em class="calibre22">σ</em></span> are unknown and population parameters are estimated only through the sample.</p><p class="calibre11">This distribution is bell-shaped and symmetric like normal distribution, but has heavier tails. The t-distribution becomes a normal distribution when the sample size is large.</p><p class="calibre11">Let us take a random sample (<span class="strong"><em class="calibre22">x1</em></span>, <span class="strong"><em class="calibre22">x2</em></span>,...,<span class="strong"><em class="calibre22">xn</em></span>) from a normal distribution with mean <span class="strong"><em class="calibre22">μ</em></span> and variance <span class="strong"><em class="calibre22">σ2</em></span>.</p><p class="calibre11">The sample mean would be <span class="inlinemediaobject"><img src="Images/image_05_033.jpg" alt="Student's t-distribution" class="calibre81"/></span> and sample variance <span class="inlinemediaobject"><img src="Images/image_05_034.jpg" alt="Student's t-distribution" class="calibre82"/></span>
</p><p class="calibre11">Considering the above-mentioned facts, the t-statistic can be defined as:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_035.jpg" alt="Student's t-distribution" class="calibre83"/></div><p class="calibre11">
</p><p class="calibre11">The sampling distribution of the t-statistic will follow a t-distribution with <span class="strong"><em class="calibre22">(n-1)</em></span>
<span class="strong"><strong class="calibre19">degrees of freedom</strong></span> (<span class="strong"><strong class="calibre19">df</strong></span> ). The higher the degree of freedom, the closer will be the t-distribution to the standard normal distribution.</p><p class="calibre11">The mean of a t-distribution (<span class="strong"><em class="calibre22">μ</em></span>) = <span class="strong"><em class="calibre22">0</em></span> and variance <span class="strong"><em class="calibre22">(σ2) = df/df-2</em></span>
</p><p class="calibre11">Now, just to make things clearer, let us look back for a moment and consider the scenario where the population <span class="strong"><em class="calibre22">σ</em></span> is known. When the population is normally distributed, the sample mean <span class="strong"><em class="calibre22">x̄</em></span> is mostly normally distributed regardless of the sample size and any linear transformation of <span class="strong"><em class="calibre22">x̄</em></span> such as <span class="inlinemediaobject"><img src="Images/image_05_037.jpg" alt="Student's t-distribution" class="calibre84"/></span> will also follow a normal distribution.</p><p class="calibre11">What if the population is not normally distributed? Even then, the distribution of <span class="strong"><em class="calibre22">x̄</em></span> (which is the sampling distribution) or <span class="inlinemediaobject"><img src="Images/image_05_037.jpg" alt="Student's t-distribution" class="calibre84"/></span> will follow a normal distribution as per CLT when the sample size is large enough!</p><p class="calibre11">The other scenario is that the population <span class="strong"><em class="calibre22">σ</em></span> is unknown. With this, if the population is normally distributed, the sample mean <span class="strong"><em class="calibre22">x̄</em></span> is mostly normally distributed, but the random variable <span class="inlinemediaobject"><img src="Images/image_05_039.jpg" alt="Student's t-distribution" class="calibre85"/></span> will not follow a normal distribution; it follows a t-distribution with <span class="strong"><em class="calibre22">(n-1)</em></span> degrees of freedom. The reason is because of the randomness of <span class="strong"><em class="calibre22">S</em></span> in the denominator, it is different for different samples.</p><p class="calibre11">In the above case, if the population is not normally distributed, the distribution of <span class="inlinemediaobject"><img src="Images/image_05_040.jpg" alt="Student's t-distribution" class="calibre85"/></span> will follow a normal distribution as per CLT with sufficiently large sample sizes (and not with the small sample size!). So, with the large sample size, the distribution of <span class="inlinemediaobject"><img src="Images/image_05_040.jpg" alt="Student's t-distribution" class="calibre85"/></span> follows a normal distribution, and it is safe to assume that it follows t-distribution because t-distribution approaches normality with an increase in the sample size.</p></div><div class="calibre2" title="F-distribution"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec46" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>F-distribution</h3></div></div></div><p class="calibre11">In statistical inference, F-distribution is used to study the variance of two normally distributed populations. It states that the sampling distribution of the sample variances from two independent normally distributed populations with the same population variance follow F-distribution.</p><p class="calibre11">If the sample variance of sample 1 is <span class="inlinemediaobject"><img src="Images/image_05_041.jpg" alt="F-distribution" class="calibre86"/></span> and if the sample variance of sample 2 is <span class="inlinemediaobject"><img src="Images/image_05_042.jpg" alt="F-distribution" class="calibre87"/></span> then, <span class="inlinemediaobject"><img src="Images/image_05_043.jpg" alt="F-distribution" class="calibre88"/></span> will have F-distribution (<span class="strong"><em class="calibre22">σ12 = σ22</em></span>).</p><p class="calibre11">From the above fact, we can also say that <span class="inlinemediaobject"><img src="Images/image_05_044.jpg" alt="F-distribution" class="calibre89"/></span> will also follow F-distribution.</p><p class="calibre11">In the previous section of chi-square distribution, we can also say that</p><p class="calibre11">
<span class="inlinemediaobject"><img src="Images/image_05_045.jpg" alt="F-distribution" class="calibre90"/></span> will also follow F-distribution with <span class="strong"><em class="calibre22">n1-1</em></span> and <span class="strong"><em class="calibre22">n2-1</em></span> degrees of freedom. For each combination of these degrees of freedoms, there would be different F-distributions.</p></div></div><div class="calibre2" title="Standard error"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec56" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Standard error</h2></div></div></div><p class="calibre11">The standard deviation of the sampling distribution of a statistic (such as mean or variance) is called the <span class="strong"><strong class="calibre19">standard error</strong></span> (<span class="strong"><strong class="calibre19">SE</strong></span>), a measure of variability. In other words, the <span class="strong"><strong class="calibre19">standard error of the mean</strong></span> (<span class="strong"><strong class="calibre19">SEM</strong></span>) can be defined as the standard deviation of the sample mean's estimate of a population mean.</p><p class="calibre11">As you increase the sample sizes, the sampling distribution of the mean gets more and more normal and the standard deviation gets smaller. It is proved that:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_046.jpg" alt="Standard error" class="calibre91"/></div><p class="calibre11">
</p><p class="calibre11">(<span class="strong"><em class="calibre22">n</em></span> being the sample size)</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_047.jpg" alt="Standard error" class="calibre92"/></div><p class="calibre11">
</p><p class="calibre11">The smaller the standard error, the more representative the sample will be of the overall population. Also, the larger the sample size, the smaller the standard error.</p><p class="calibre11">SE is very important in other measures of statistical inference, such as margin of error and confidence interval.</p></div><div class="calibre2" title="Confidence level"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec57" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Confidence level</h2></div></div></div><p class="calibre11">It is a measure of how certain you would like to be (the probability) in estimating the population parameter through sample statistics so that the expected values would fall within a desired range or confidence interval. It is calculated by subtracting the significance level (<span class="strong"><em class="calibre22">α</em></span>) from <span class="strong"><em class="calibre22">1</em></span> (that is, <span class="strong"><em class="calibre22">confidence level = 1 - α</em></span>). So, if <span class="strong"><em class="calibre22">α = 0.05</em></span>, the confidence level would be <span class="strong"><em class="calibre22">1-0.05 = 0.95</em></span>
</p><p class="calibre11">Usually, the higher the confidence level, the higher the sample size required. However, there are often trade-offs and you have to decide on how confident you would like to be so that you can estimate the sample size needed for your confidence level.</p></div><div class="calibre2" title="Margin of error and confidence interval"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec58" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Margin of error and confidence interval</h2></div></div></div><p class="calibre11">As discussed already, since a sample can never be a 100 percent representative of the population, estimating the population parameter through inference will always have some margin of error due to sampling errors. Usually, the bigger the sample size, the smaller the margin of error. However, you have to decide on how much error to allow, and estimating a proper sample size required would depend on that.</p><p class="calibre11">So, the range of values below and above the sample statistic based on the margin of error is called the <span class="strong"><strong class="calibre19">confidence interval</strong></span>. In other words, a confidence interval is a range of numbers within which we believe the true population parameter to fall a certain percentage of the time (confidence level).</p><p class="calibre11">Please note here that a statement such as "I am 95 percent confident that the confidence interval contains the true value" could be misleading! The right way of stating this could be "<span class="strong"><em class="calibre22">If I take an infinite number of samples of the same size, then 95 percent of the time the confidence interval would contains the true value".</em></span>
</p><p class="calibre11">For example, when you put the confidence level as 95 percent and the confidence interval as 4 percent for a sample statistic 58 (here, 58 is any sample statistic such as mean, variance, or standard deviation), you can say that you are 95 percent sure that the true percentage of the population is between 58 - 4 = 54 percent and 58 + 4 = 62 percent.</p></div><div class="calibre2" title="Variability in the population"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec59" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Variability in the population</h2></div></div></div><p class="calibre11">The variability in the population is one of the most important factors we should consider in our inferential statistics. It plays an important role in estimating the sample size. No matter what sampling algorithm you choose that can best represent the population, the sample size still plays a crucial role - and this is obvious!</p><p class="calibre11">If the variation in the population is more, then the sample size required would also be more.</p></div><div class="calibre2" title="Estimating sample size"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec60" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Estimating sample size</h2></div></div></div><p class="calibre11">We already covered sampling techniques in the previous sections. In this section, we will discuss how to estimate the sample size. Assume you have to prove a concept or to assess the result of some action, then you take some relevant data and try to prove your point. However, how would you ensure you have enough data? Samples that are too big waste time and resources, and samples that are too small may lead to misleading results. Estimating the sample size depends majorly on factors such as the margin of error or confidence interval, confidence level, and variability in the population.</p><p class="calibre11">Consider the following example:</p><p class="calibre11">The college president asks the statistics teacher to estimate the average age of the students at their college. How large a sample is necessary? The statistics teacher would like to be 99 percent confident that the estimate should be accurate within 1 year. From a previous study, the standard deviation of the ages is known to be 3 years.</p><p class="calibre11">Solution:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_048.jpg" alt="Estimating sample size" class="calibre93"/></div><p class="calibre11">
</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_05_049.jpg" alt="Estimating sample size" class="calibre94"/></div><p class="calibre11">
</p></div><div class="calibre2" title="Hypothesis testing"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch05lvl2sec61" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Hypothesis testing</h2></div></div></div><p class="calibre11">Hypothesis testing is about testing the assumptions made for the population parameters. This helps in determining whether a result is statistically significant or has occurred by chance. It is the most important instrument of statistical research. We will discuss some of the testing to see how variables are related to each other in the population.</p><div class="calibre2" title="Null and alternate hypotheses"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec47" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Null and alternate hypotheses</h3></div></div></div><p class="calibre11">The null hypothesis (denoted as H0) is often the initial claim about the population parameter, and it is mostly indicative of <span class="strong"><em class="calibre22">no effect</em></span> or <span class="strong"><em class="calibre22">no relation</em></span>. In our hypothesis testing, our aim is to invalidate and reject the null hypothesis to be able to accept the alternate hypothesis (denoted as H1). The alternate hypothesis is indicative of some effect in your experiment. While experimenting, please note here that you either reject the null hypothesis or fail to reject the null hypothesis. If you are successful in rejecting the null hypothesis then the alternate hypothesis is to be considered and if you fail to reject the null hypothesis then the null hypothesis is considered (though it may not be true!).</p><p class="calibre11">So, we usually hope to get a very small P-value (lower than the defined significance level alpha) to be able to reject the null hypothesis. If the P-value is greater than alpha, then you fail to reject the null hypothesis.</p></div><div class="calibre2" title="Chi-square test"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec48" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chi-square test</h3></div></div></div><p class="calibre11">Most of the statistical inference techniques are used to estimate the population parameters or to test a hypothesis using the sample statistics such as <span class="strong"><em class="calibre22">mean</em></span>. However, the chi-square statistic takes a completely different approach by examining the whole distribution or the relationship between two distributions. In the field of inferential statistics, many test statistics resemble a chi-square distribution. The most common tests using this distribution are the chi-square test of goodness of fit (one-way tables) and chi-square test of independence (two-way tables). The <span class="strong"><em class="calibre22">goodness of fit</em></span> test is done when you want to see if the sample data follows the same distribution in the population and the <span class="strong"><em class="calibre22">independence</em></span> test is done when you want to see if two categorical variables are related to each other in the population.</p><p class="calibre11">The input data types determine whether to conduct a <span class="strong"><em class="calibre22">goodness of fit</em></span> or <span class="strong"><em class="calibre22">independence</em></span> test without specifying them as switches explicitly. So, if you provide a vector as input, then the <span class="strong"><em class="calibre22">goodness of fit</em></span> test is conducted and if you provide a matrix as input, then the <span class="strong"><em class="calibre22">independence</em></span> test is conducted. In either case, a vector of frequencies of events or a contingency matrix is provided as input which you need to compute first. Let us explore these through examples:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting"> //Chi-Square test
&gt;&gt;&gt; from pyspark.mllib.linalg import Vectors, Matrices
&gt;&gt;&gt; from pyspark.mllib.stat import Statistics
&gt;&gt;&gt; import random
&gt;&gt;&gt; 
//Make a vector of frequencies of events
&gt;&gt;&gt; vec = Vectors.dense( random.sample(xrange(1,101),10))
&gt;&gt;&gt; vec
DenseVector([45.0, 40.0, 93.0, 66.0, 56.0, 82.0, 36.0, 30.0, 85.0, 15.0])
// Get Goodnesss of fit test results
&gt;&gt;&gt; GFT_Result = Statistics.chiSqTest(vec)
// Here the ‘goodness of fit test’ is conducted because your input is a vector
//Make a contingency matrix
&gt;&gt;&gt; mat = Matrices.dense(5,6,random.sample(xrange(1,101),30))\
//Get independense test results\\
&gt;&gt;&gt; IT_Result = Statistics.chiSqTest(mat)
// Here the ‘independence test’ is conducted because your input is a vector
//Examine the independence test results
&gt;&gt;&gt; print(IT_Result)
Chi squared test summary:
method: pearson
degrees of freedom = 20
statistic = 285.9423808343265
pValue = 0.0
Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">scala&gt; import org.apache.spark.mllib.linalg.{Vectors, Matrices}
import org.apache.spark.mllib.linalg.{Vectors, Matrices} 

scala&gt; import org.apache.spark.mllib.stat.Statistics 

scala&gt; val vec = Vectors.dense( Array.fill(10)(               scala.util.Random.nextDouble))vec: org.apache.spark.mllib.linalg.Vector = [0.4925741159101148,....] 

scala&gt; val GFT_Result = Statistics.chiSqTest(vec)GFT_Result: org.apache.spark.mllib.stat.test.ChiSqTestResult =Chi squared test summary:
method: pearson
degrees of freedom = 9
statistic = 1.9350768763253192
pValue = 0.9924531181394086
No presumption against null hypothesis: observed follows the same distribution as expected..
// Here the ‘goodness of fit test’ is conducted because your input is a vector
scala&gt; val mat = Matrices.dense(5,6, Array.fill(30)(scala.util.Random.nextDouble)) // a contingency matrix
mat: org.apache.spark.mllib.linalg.Matrix =..... 
scala&gt; val IT_Result = Statistics.chiSqTest(mat)
IT_Result: org.apache.spark.mllib.stat.test.ChiSqTestResult =Chi squared test summary:
method: pearson
degrees of freedom = 20
statistic = 2.5401190679900663
pValue = 0.9999990459111089
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..
// Here the ‘independence test’ is conducted because your input is a vector
</pre></div><div class="calibre2" title="F-test"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec49" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>F-test</h3></div></div></div><p class="calibre11">We have already covered how to calculate the F-statistic in the previous sections. Now we will solve a sample problem.</p><div class="calibre2" title="Problem:"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title7"><a id="ch05lvl4sec3" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Problem:</h4></div></div></div><p class="calibre11">You want to test the belief that the income of Master's degree holders shows greater variability than the income of Bachelor's degree holders. A random sample of 21 graduates and a random sample of 30 Masters were taken. The standard deviation of the graduates sample was $180 and that of the Masters sample was $112.</p><p class="calibre11">Solution:</p><p class="calibre11">The null hypothesis is: <span class="strong"><em class="calibre22">H<sub class="calibre58">0</sub> : σ<sub class="calibre58">1</sub><sup class="calibre57">2 </sup>=σ<sub class="calibre58">2</sub><sup class="calibre57">2 </sup></em></span>
</p><p class="calibre11">Given that<span class="strong"><em class="calibre22"> S<sub class="calibre58">1</sub> = $180</em></span>, <span class="strong"><em class="calibre22">n<sub class="calibre58">1</sub> = 21</em></span>, and <span class="strong"><em class="calibre22">S<sub class="calibre58">2</sub> = $112</em></span>, <span class="strong"><em class="calibre22">n<sub class="calibre58">2</sub> = 30</em></span>
</p><p class="calibre11">Considering the level of significance to be <span class="strong"><em class="calibre22">α = 0.05</em></span>
</p><p class="calibre11">
<span class="strong"><em class="calibre22">F = S<sub class="calibre58">1</sub><sup class="calibre57">2 </sup>/S<sub class="calibre58">2</sub><sup class="calibre57">2 </sup>= 180<sup class="calibre57">2</sup>/112<sup class="calibre57">2 </sup>= 2.58</em></span>
</p><p class="calibre11">From the F-table with the significance level 0.05, df1=20 and df2=29, we can see that the F-value is 1.94</p><p class="calibre11">Since the computed value of F is greater than the table value of F, we can reject the null hypothesis and conclude that <span class="strong"><em class="calibre22">σ<sub class="calibre58">1</sub><sup class="calibre57">2 </sup>&gt;σ<sub class="calibre58">2</sub></em></span>
<sup class="calibre57"><span class="emphasis"><em class="calibre95">2</em></span>
</sup>.</p></div></div><div class="calibre2" title="Correlations"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch05lvl3sec50" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Correlations</h3></div></div></div><p class="calibre11">Correlations provide a way to measure the statistical dependence between two random variables that are numeric in nature. This shows the extent to which the two variables change with each other. There are basically two types of correlation measures: Pearson and Spearman. Pearson is more appropriate for interval scale data, such as temperature, height, and so on. Spearman is more appropriate for ordinal scale, such as a satisfaction survey where 1 is less satisfied and 5 is most satisfied. Also, Pearson is calculated based on true values and is useful in finding linear relationships whereas Spearman is based on rank order and is useful in finding monotonic relationships. The monotonic relationship means that the variables do change together, but not at a constant rate. Please note that both of these correlation measures can only measure linear or monotonic relationships and are not capable of depicting any other kind of relationships such as non-linear relationships.</p><p class="calibre11">In Spark, both of these are supported. If you input two <code class="literal">RDD[Double]</code>, the output is a <span class="strong"><em class="calibre22">Double</em></span> and if you input an <code class="literal">RDD[Vector]</code>, the output is a <span class="strong"><em class="calibre22">Correlation Matrix</em></span>. In both Scala and Python implementations, if you do not provide the type of correlation as input, then the default considered is always Pearson.</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">&gt;&gt;&gt; from pyspark.mllib.stat import Statistics
&gt;&gt;&gt; import random 
// Define two series
//Number of partitions and cardinality of both Ser_1 and Ser_2 should be the same
&gt;&gt;&gt; Ser_1 = sc.parallelize(random.sample(xrange(1,101),10))       
// Define Series_1&gt;&gt;&gt; Ser_2 = sc.parallelize(random.sample(xrange(1,101),10))       
// Define Series_2 
&gt;&gt;&gt; correlation = Statistics.corr(Ser_1, Ser_2, method = "pearson") 
//if you are interested in Spearman method, use “spearman” switch instead
&gt;&gt;&gt; round(correlation,2)-0.14
&gt;&gt;&gt; correlation = Statistics.corr(Ser_1, Ser_2, method ="spearman")
&gt;&gt;&gt; round(correlation,2)-0.19//Check on matrix//The following statement creates 100 rows of 5 elements each
&gt;&gt;&gt; data = sc.parallelize([random.sample(xrange(1,51),5) for x in range(100)])
&gt;&gt;&gt; correlMatrix = Statistics.corr(data, method = "pearson") 
//method may be spearman as per you requirement
&gt;&gt;&gt; correlMatrix
array([[ 1.        ,  0.09889342, -0.14634881,  0.00178334,  0.08389984],       [ 0.09889342,  1.        , -0.07068631, -0.02212963, -0.1058252 ],       [-0.14634881, -0.07068631,  1.        , -0.22425991,  0.11063062],       [ 0.00178334, -0.02212963, -0.22425991,  1.        , -0.04864668],       [ 0.08389984, -0.1058252 ,  0.11063062, -0.04864668,  1.        
]])
&gt;&gt;&gt; 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">scala&gt; val correlation = Statistics.corr(Ser_1, Ser_2, "pearson")correlation: Double = 0.43217145308272087 
//if you are interested in Spearman method, use “spearman” switch instead
scala&gt; val correlation = Statistics.corr(Ser_1, Ser_2, "spearman")correlation: Double = 0.4181818181818179 
scala&gt;
//Check on matrix
//The following statement creates 100 rows of 5 element Vectors
scala&gt; val data = sc.parallelize(Seq.fill(100)(Vectors.dense(Array.fill(5)(              scala.util.Random.nextDouble))))
data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:27 
scala&gt; val correlMatrix = Statistics.corr(data, method="pearson") 
//method may be spearman as per you requirement
correlMatrix: org.apache.spark.mllib.linalg.Matrix =1.0                    -0.05478051936343809  ... (5 total)-0.05478051936343809   1.0                   ..........</pre></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch05lvl1sec40" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we briefly covered the steps involved in the data science life cycle, such as data acquisition, data preparation, and data exploration through descriptive statistics. We also learnt to estimate the population parameters through sample statistics using some popular tools and techniques.</p><p class="calibre11">We explained the basics of statistics from both theoretical and practical aspects by going deeper into the fundamentals in a few areas to be able to solve business problems. Finally, we learnt a few examples on how statistical analysis can be performed on Apache Spark, leveraging the out-of-the-box features, which was basically the objective behind this chapter.</p><p class="calibre11">We will discuss more details of the machine learning part of data science in the next chapter as we have already built statistical understanding in this chapter. Learnings from this chapter should help connect to the machine learning algorithms in a more informed way.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch05lvl1sec41" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><p class="calibre11">Supported statistics by Spark:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://spark.apache.org/docs/latest/mllib-statistics.html">http://spark.apache.org/docs/latest/mllib-statistics.html</a>
</p><p class="calibre11">Plotting features of Databricks:</p><p class="calibre11"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://docs.cloud.databricks.com/docs/latest/databricks_guide/04%20Visualizations/4%20Matplotlib%20and%20GGPlot.html">https://docs.cloud.databricks.com/docs/latest/databricks_guide/04%20Visualizations/4%20Matplotlib%20and%20GGPlot.html</a></p><p class="calibre11">Detailed information on OOTB library functions of MLLIB stats:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.Statistics%24">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.Statistics$</a>
</p></div></div>



  </body></html>