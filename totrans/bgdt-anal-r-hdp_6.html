<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Understanding Big Data Analysis with Machine Learning</h1></div></div></div><p>In this chapter, we <a id="id617" class="indexterm"/>are going to learn about different machine learning techniques that can be used with R and Hadoop to perform Big Data analytics with the help of the following points:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introduction to machine learning</li><li class="listitem" style="list-style-type: disc">Types of machine-learning algorithms</li><li class="listitem" style="list-style-type: disc">Supervised machine-learning algorithms</li><li class="listitem" style="list-style-type: disc">Unsupervised machine-learning algorithms</li><li class="listitem" style="list-style-type: disc">Recommendation algorithms</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec39"/>Introduction to machine learning</h1></div></div></div><p>Machine learning is a branch of artificial intelligence that allows us to make our application intelligent without being explicitly programmed. Machine learning concepts are used to<a id="id618" class="indexterm"/> enable applications to take a decision from the available datasets. A combination of machine learning and data mining can be used to develop spam mail detectors, self-driven cars, speech recognition, face recognition, and online transactional fraud-activity detection.</p><p>There are many popular organizations that are using machine-learning algorithms to make their service or product understand the need of their users and provide services as per their behavior. Google has its intelligent web search engine, which provides a number one search, spam classification in Google Mail, news labeling in Google News, and Amazon for recommender systems. There are many open source frameworks available for developing these types of applications/frameworks, such as R, Python, Apache Mahout, and Weka.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec59"/>Types of machine-learning algorithms</h2></div></div></div><p>There are <a id="id619" class="indexterm"/>three different types of machine-learning <a id="id620" class="indexterm"/>algorithms for intelligent system development:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Supervised machine-learning algorithms</li><li class="listitem" style="list-style-type: disc">Unsupervised machine-learning algorithms</li><li class="listitem" style="list-style-type: disc">Recommender systems</li></ul></div><p>In <a id="id621" class="indexterm"/>this <a id="id622" class="indexterm"/>chapter, we are <a id="id623" class="indexterm"/>going to discuss well-known<a id="id624" class="indexterm"/> business problems with classification, regression, and clustering, as well as how to perform these machine-learning<a id="id625" class="indexterm"/> techniques over Hadoop to overcome memory issues.</p><p>If you load a dataset that won't be able to fit into your machine memories and you try to run it, the predictive analysis will throw an error related to machine memory, such as <strong>Error: cannot allocate vector of size 990.1 MB</strong>. The solution is to increase the machine configuration or parallelize with commodity hardware.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec40"/>Supervised machine-learning algorithms</h1></div></div></div><p>In this section, we will be learning about supervised machine-learning algorithms. The algorithms<a id="id626" class="indexterm"/> are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Linear regression</li><li class="listitem" style="list-style-type: disc">Logistic regression</li></ul></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec60"/>Linear regression</h2></div></div></div><p>Linear <a id="id627" class="indexterm"/>regression is mainly <a id="id628" class="indexterm"/>used for predicting and forecasting values based on historical <a id="id629" class="indexterm"/>information. Regression is a supervised machine-learning technique to identify the linear relationship between target variables and explanatory variables. We can say it is used for predicting the target variable values in numeric form.</p><p>In the following section, we will be learning about linear regression with R and linear regression with R and Hadoop.</p><p>Here, the variables that are <a id="id630" class="indexterm"/>going to be predicted are considered as target variables and the variables that are going to help predict the target variables are called explanatory variables. With the linear relationship, we can identify the<a id="id631" class="indexterm"/> impact of a change in explanatory variables on the target variable.</p><p>In mathematics, regression can be formulated as follows:</p><p>y = ax +e</p><p>Other formulae include:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The slope of the regression line is given by:<p>a= (NΣxy - (Σx)(Σy)) / (NΣx<sup>2</sup> - (Σx)<sup>2</sup>)</p></li><li class="listitem" style="list-style-type: disc">The intercept point of regression is given by:<p>e = (Σy - b(Σx)) / N</p></li></ul></div><p>Here, <em>x</em> and <em>y</em> are variables that form a dataset and <em>N</em> is the total numbers of values.</p><p>Suppose we have the data shown in the following table:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>x</p>
</th><th style="text-align: left" valign="bottom">
<p>y</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>63</p>
</td><td style="text-align: left" valign="top">
<p>3.1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>64</p>
</td><td style="text-align: left" valign="top">
<p>3.6</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>65</p>
</td><td style="text-align: left" valign="top">
<p>3.8</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>66</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr></tbody></table></div><p>If we have a new value of <em>x</em>, we can get the value of <em>y</em> with it with the help of the regression formula.</p><p>Applications of linear regression include:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Sales forecasting</li><li class="listitem" style="list-style-type: disc">Predicting optimum product price</li><li class="listitem" style="list-style-type: disc">Predicting the next online purchase from various sources and campaigns</li></ul></div><p>Let's look at the statistical technique to implement the regression model for the provided dataset. Assume that we have been given n number of statistical data units.</p><div><img src="img/3282OS_06_01.jpg" alt="Linear regression"/></div><p>Its formula is as follows:</p><p>Y = e<sub>0</sub> + a<sub>0</sub>x<sub>0</sub> + a<sub>1</sub>x<sub>1</sub> + a<sub>2</sub>x<sub>2</sub> +a<sub>3</sub>x<sub>3</sub> + a<sub>4</sub>x<sub>4</sub></p><p>Here, <em>Y</em> is the <a id="id632" class="indexterm"/>target variable (response variable), <em>xi</em> are explanatory variables, and <em>e<sub>0</sub></em> is the sum of the squared error term, which can be considered as noise. To get a more accurate prediction, we need to reduce this error term as <a id="id633" class="indexterm"/>soon as possible with the help of the <code class="literal">call</code> function.</p><div><div><div><div><h3 class="title"><a id="ch06lvl3sec48"/>Linear regression with R</h3></div></div></div><p>Now we will see how to perform linear regression in R. We can use the in-built <code class="literal">lm()</code> method to <a id="id634" class="indexterm"/>build a linear regression model with R.</p><div><pre class="programlisting">Model &lt;-lm(target ~ ex_var1, data=train_dataset)</pre></div><p>It will build a <a id="id635" class="indexterm"/>regression model based on the property of the provided dataset and store all of the variables' coefficients and model parameters used for predicting and identifying of data pattern from the model variable values.</p><div><pre class="programlisting"># Defining data variables
X = matrix(rnorm(2000), ncol = 10)
y = as.matrix(rnorm(200))

# Bundling data variables into dataframe
train_data &lt;- data.frame(X,y)

# Training model for generating prediction
lmodel&lt;- lm(y~ train_data $X1 + train_data $X2 + train_data $X3 + train_data $X4 + train_data $X5 + train_data $X6 + train_data $X7 + train_data $X8 + train_data $X9 + train_data $X10,data= train_data)

summary(lmodel)</pre></div><p>The following are the<a id="id636" class="indexterm"/> various model parameters that can be displayed with the preceding <code class="literal">summary</code> command:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>RSS</strong>: This is equal to ∑(yactual - y)<sup>2</sup>.</li><li class="listitem" style="list-style-type: disc"><strong>Degrees of Freedom</strong> (<strong>DOF</strong>): This is used for identifying the degree of fit for the prediction model, which should be as small as possible (logically, the value 0 means perfect prediction).</li><li class="listitem" style="list-style-type: disc"><strong>Residual standard error</strong> (<strong>RSS/DF</strong>): This is used for identifying the goodness of fit for the prediction model, which should be as small as possible (logically, the value 0 means perfect prediction).</li><li class="listitem" style="list-style-type: disc"><strong>pr</strong>: This is the probability for a variable to be included into the model; it should be less than 0.05 for a variable to be included.</li><li class="listitem" style="list-style-type: disc"><strong>t-value</strong>: This is equal to 15.</li><li class="listitem" style="list-style-type: disc"><strong>f</strong>: This is the statistic that checks whether R square is a value other than zero.</li></ul></div><div><img src="img/3282OS_06_03.jpg" alt="Linear regression with R"/></div></div><div><div><div><div><h3 class="title"><a id="ch06lvl3sec49"/>Linear regression with R and Hadoop</h3></div></div></div><p>Assume we have a large dataset. How will we perform regression data analysis now? In such cases, we <a id="id637" class="indexterm"/>can use R and Hadoop integration to <a id="id638" class="indexterm"/>perform parallel linear regression by implementing Mapper and Reducer. It will divide the dataset into chunks among the available nodes and then they will process the distributed data in parallel. It will not fire memory issues when we run with an R and Hadoop cluster because the large dataset is going to be distributed and processed with R among Hadoop computation nodes. Also, keep in mind that this implemented method does not provide higher prediction accuracy than the <a id="id639" class="indexterm"/>
<code class="literal">lm()</code> model.</p><p>RHadoop is used here for integration of R and Hadoop, which is a trusted open source distribution of <a id="id640" class="indexterm"/>
<strong>Revolution Analytics</strong>. For more information on <a id="id641" class="indexterm"/>RHadoop, visit <a class="ulink" href="https://github.com/RevolutionAnalytics/RHadoop/wiki">https://github.com/RevolutionAnalytics/RHadoop/wiki</a>. Among the packages of RHadoop, here we are using only the <code class="literal">rmr</code> and <code class="literal">rhdfs</code> libraries.</p><p>Let's see how to perform regression analysis with R and Hadoop data technologies.</p><div><pre class="programlisting"># Defining the datasets with Big Data matrix X
X = matrix(rnorm(20000), ncol = 10)
X.index = to.dfs(cbind(1:nrow(X), X))
y = as.matrix(rnorm(2000))</pre></div><p>Here, the <code class="literal">Sum()</code> function<a id="id642" class="indexterm"/> is re-usable as shown in the following code:</p><div><pre class="programlisting"># Function defined to be used as reducers 
Sum = 
  function(., YY) 
    keyval(1, list(Reduce('+', YY)))</pre></div><p>The <a id="id643" class="indexterm"/>outline of the linear regression algorithm is as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Calculating the <code class="literal">Xtx</code> value with MapReduce job1.</li><li class="listitem">Calculating <a id="id644" class="indexterm"/>the <code class="literal">Xty</code> value with MapReduce job2.</li><li class="listitem">Deriving the coefficient values with <code class="literal">Solve (Xtx, Xty)</code>.</li></ol></div><p>Let's understand these steps one by one.</p><p>The first step is to calculate the <code class="literal">Xtx</code> value with MapReduce job 1.</p><div><ol class="orderedlist arabic"><li class="listitem">The big matrix is passed to the Mapper in chunks of complete rows. Smaller cross-products are computed for these submatrices and passed on to a single Reducer, which sums them together. Since we have a single key, a Combiner is mandatory and since the matrix sum is associative and commutative, we certainly can use it here.<div><pre class="programlisting"># XtX = 
  values(

# For loading hdfs data in to R 
    from.dfs(

# MapReduce Job to produce XT*X
      mapreduce(
        input = X.index,

# Mapper – To calculate and emitting XT*X
        map = 
          function(., Xi) {
            yi = y[Xi[,1],]
            Xi = Xi[,-1]
            keyval(1, list(t(Xi) %*% Xi))},

# Reducer – To reduce the Mapper output by performing sum operation over them
        reduce = Sum,
        combine = TRUE)))[[1]]</pre></div></li><li class="listitem">When we have a <a id="id645" class="indexterm"/>large amount of data stored in <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>), we need to pass its path value to the <a id="id646" class="indexterm"/>input parameters in the <code class="literal">MapReduce</code> method.</li><li class="listitem">In the preceding code, we saw that <code class="literal">X</code> is the design matrix, which has been created with the following function:<div><pre class="programlisting">X = matrix(rnorm(2000), ncol = 10)</pre></div></li><li class="listitem">Its output will look as shown in the following screenshot:<div><img src="img/3282OS_06_10.jpg" alt="Linear regression with R and Hadoop"/></div></li></ol></div><p>So, here all the <a id="id647" class="indexterm"/>columns will be considered as explanatory variables and their standard errors can be calculated in a similar manner to how we calculated them with normal linear regression.</p><p>To calculate the <code class="literal">Xty</code> value with MapReduce job 2 is pretty much the same as for the vector <code class="literal">y</code>, which is available to the nodes according to normal scope rules.</p><div><pre class="programlisting">Xty = values(

# For loading hdfs data
from.dfs(

# MapReduce job to produce XT * y
      mapreduce(
       input = X.index,

# Mapper – To calculate and emitting XT*y
        map = function(., Xi) {
          yi = y[Xi[,1],]
          Xi = Xi[,-1]
          keyval(1, list(t(Xi) %*% yi))},

# Reducer – To reducer the Mapper output by performing # sum operation over them
        reduce = Sum,
        combine = TRUE)))[[1]]</pre></div><p>To derive<a id="id648" class="indexterm"/> the coefficient values with <code class="literal">solve (Xtx, Xty)</code>, use the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Finally, we <a id="id649" class="indexterm"/>just need to call the following line of code to get the coefficient values.<div><pre class="programlisting">solve(XtX, Xty)</pre></div></li><li class="listitem">The output of the preceding command will be as shown in the following screenshot:<div><img src="img/3282OS_06_02.jpg" alt="Linear regression with R and Hadoop"/></div></li></ol></div></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec61"/>Logistic regression</h2></div></div></div><p>In statistics, logistic<a id="id650" class="indexterm"/> regression or logit regression is a type of<a id="id651" class="indexterm"/> probabilistic classification model. Logistic regression is used extensively in numerous disciplines, including the medical and social science fields. It can be binomial or multinomial.</p><p>Binary logistic regression deals with situations in which the outcome for a dependent variable can have two possible types. Multinomial logistic regression deals with situations <a id="id652" class="indexterm"/>where the outcome can have three or more possible <a id="id653" class="indexterm"/>types.</p><p>Logistic regression can be implemented using logistic functions, which are listed here.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">To predict the log odds ratios, use the following formula:<p>logit(p) = β0 + β1 × x1 + β2 × x2 + ... + βn × xn</p></li><li class="listitem" style="list-style-type: disc">The probability formula is as follows:<p>p = e<sup>logit(p)</sup> ⁄ 1 + e<sup>logit(p)</sup></p></li></ul></div><p><code class="literal">logit(p)</code> is a linear function of the explanatory variable, X (x1,x2,x3..xn), which is similar to linear regression. So, the output of this function will be in the range 0 to 1. Based on the probability score, we can set its probability range from 0 to 1. In a majority of the cases, if the score is greater than 0.5, it will be considered as 1, otherwise 0. Also, we can say it provides a classification boundary to classify the outcome variable.</p><div><img src="img/3282OS_06_04.jpg" alt="Logistic regression"/></div><p>The <a id="id654" class="indexterm"/>preceding figure is of a <a id="id655" class="indexterm"/>training dataset. Based on the training dataset plot, we<a id="id656" class="indexterm"/> can say there is one classification boundary generated by the <code class="literal">glm</code> model in R.</p><p>Applications of logistic regression include:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Predicting the likelihood of an online purchase</li><li class="listitem" style="list-style-type: disc">Detecting the presence of diabetes</li></ul></div><div><div><div><div><h3 class="title"><a id="ch06lvl3sec50"/>Logistic regression with R</h3></div></div></div><p>To perform <a id="id657" class="indexterm"/>logistic regression with R, we will use the <code class="literal">iris</code> dataset<a id="id658" class="indexterm"/> and the <code class="literal">glm</code> model.</p><div><pre class="programlisting">#loading iris dataset
data(iris)

# Setting up target variable
target &lt;- data.frame(isSetosa=(iris$Species == 'setosa'))

# Adding target to iris and creating new dataset
inputdata &lt;- cbind(target,iris)

# Defining the logistic regression formula
formula &lt;- isSetosa ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width

# running Logistic model via glm()
logisticModel &lt;- glm(formula, data=inputdata, family="binomial")</pre></div></div><div><div><div><div><h3 class="title"><a id="ch06lvl3sec51"/>Logistic regression with R and Hadoop</h3></div></div></div><p>To perform <a id="id659" class="indexterm"/>logistic regression with R and Hadoop, we<a id="id660" class="indexterm"/> will use RHadoop with <code class="literal">rmr2</code>.</p><p>The outline of the logistic regression algorithm is as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Defining the <a id="id661" class="indexterm"/><code class="literal">lr.map</code> Mapper function</li><li class="listitem" style="list-style-type: disc">Defining <a id="id662" class="indexterm"/>the <code class="literal">lr.reducer</code> Reducer function</li><li class="listitem" style="list-style-type: disc">Defining <a id="id663" class="indexterm"/>the <code class="literal">logistic.regression</code> MapReduce function</li></ul></div><p>Let's understand them one by one.</p><p>We will first define the logistic regression function with gradient decent. Multivariate regression <a id="id664" class="indexterm"/>can be performed by forming the <a id="id665" class="indexterm"/>nondependent variable into a matrix data format. For factorial variables, we can translate them to binary variables for fitting the model. This function will ask for <code class="literal">input</code>, <code class="literal">iterations</code>, <code class="literal">dims</code>, and <code class="literal">alpha</code> as input parameters.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">lr.map</code>: This <a id="id666" class="indexterm"/>stands for the logistic regression Mapper, which will compute the contribution of subset points to the gradient.<div><pre class="programlisting"># Mapper – computes the contribution of a subset of points to the gradient.

lr.map = 
    function(., M) {
      Y = M[,1] 
      X = M[,-1]
      keyval(
        1,
        Y * X * 
          g(-Y * as.numeric(X %*% t(plane))))}</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">lr.reducer</code>: This<a id="id667" class="indexterm"/> stands for the logistic regression Reducer, which is performing just a big sum of all the values of key 1.<div><pre class="programlisting"># Reducer – Perform sum operation over Mapper output.

lr.reduce =
    function(k, Z) 
      keyval(k, t(as.matrix(apply(Z,2,sum))))</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">logistic.regression</code>: This will mainly define the <code class="literal">logistic.regression</code> MapReduce function with the following input parameters. Calling<a id="id668" class="indexterm"/> this function will start executing logistic regression of the MapReduce function.<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">input</code>: This is an input dataset</li><li class="listitem" style="list-style-type: disc"><code class="literal">iterations</code>: This is the fixed number of iterations for calculating the gradient</li><li class="listitem" style="list-style-type: disc"><code class="literal">dims</code>: This is the dimension of input variables</li><li class="listitem" style="list-style-type: disc"><code class="literal">alpha</code>: This is the learning rate</li></ul></div></li></ul></div><p>Let's see how to develop the logistic regression function.</p><div><pre class="programlisting"># MapReduce job – Defining MapReduce function for executing logistic regression

logistic.regression = 
  function(input, iterations, dims, alpha){
  plane = t(rep(0, dims))
  g = function(z) 1/(1 + exp(-z))
  for (i in 1:iterations) {
    gradient = 
      values(
        from.dfs(
          mapreduce(
            input,
            map = lr.map,
            reduce = lr.reduce,
            combine = T)))
    plane = plane + alpha * gradient }
  plane }</pre></div><p>Let's <a id="id669" class="indexterm"/>run this logistic regression function <a id="id670" class="indexterm"/>as follows:</p><div><pre class="programlisting"># Loading dataset
data(foodstamp)

# Storing data to hdfs 
testdata &lt;-  to.dfs(as.matrix(foodstamp))

# Running logistic regression with R and Hadoop
print(logistic.regression(testdata,10,3,0.05))</pre></div><p>The output of the preceding command will be as follows:</p><div><img src="img/3282OS_06_09.jpg" alt="Logistic regression with R and Hadoop"/></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec41"/>Unsupervised machine learning algorithm</h1></div></div></div><p>In <a id="id671" class="indexterm"/>machine<a id="id672" class="indexterm"/> learning, unsupervised learning is used for finding the hidden structure from the unlabeled dataset. Since the datasets are not labeled, there will be no error while evaluating for potential solutions.</p><p>Unsupervised machine learning includes several algorithms, some of which are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Clustering</li><li class="listitem" style="list-style-type: disc">Artificial neural networks</li><li class="listitem" style="list-style-type: disc">Vector quantization</li></ul></div><p>We will consider popular clustering algorithms here.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec62"/>Clustering</h2></div></div></div><p>Clustering is<a id="id673" class="indexterm"/> the task of grouping<a id="id674" class="indexterm"/> a set of <a id="id675" class="indexterm"/>object in such a <a id="id676" class="indexterm"/>way that similar objects with similar characteristics<a id="id677" class="indexterm"/> are grouped in the same category, but other <a id="id678" class="indexterm"/>objects are grouped in other categories. In clustering, the input datasets are not labeled; they need to be labeled based on the similarity of their data structure.</p><p>In unsupervised machine learning, the classification technique performs the same procedure to map the data to a category with the help of the provided set of input training datasets. The corresponding procedure is known as clustering (or cluster analysis), and involves grouping data into categories based on some measure of inherent similarity; for example, the distance between data points.</p><p>From the following figure, we can identify clustering as grouping objects based on their similarity:</p><div><img src="img/3282OS_06_05.jpg" alt="Clustering"/></div><p>There are <a id="id679" class="indexterm"/>several clustering techniques available within R libraries, such<a id="id680" class="indexterm"/> as k-means, k-medoids, hierarchical, and density-based clustering. Among them, k-means is widely used as the clustering algorithm in data science. This algorithm asks for a number of clusters to be the input parameters from the user side.</p><p>Applications of clustering are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Market segmentation</li><li class="listitem" style="list-style-type: disc">Social network analysis</li><li class="listitem" style="list-style-type: disc">Organizing computer network</li><li class="listitem" style="list-style-type: disc">Astronomical data analysis</li></ul></div><div><div><div><div><h3 class="title"><a id="ch06lvl3sec52"/>Clustering with R</h3></div></div></div><p>We are <a id="id681" class="indexterm"/>considering the <code class="literal">k-means</code> method here for implementing<a id="id682" class="indexterm"/> the clustering model over the <code class="literal">iris</code> input dataset, which <a id="id683" class="indexterm"/>can be achieved by just calling its in-built R dataset – the <code class="literal">iris</code> data (for more information, visit <a class="ulink" href="http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html">http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html</a>). Here we will see how k-means clustering can be performed with R.</p><div><pre class="programlisting"># Loading iris flower dataset
data("iris")
# generating clusters for iris dataset
kmeans &lt;- kmeans(iris[, -5], 3, iter.max = 1000)

# comparing iris Species with generated cluster points
Comp &lt;- table(iris[, 5], kmeans$cluster)</pre></div><p>Deriving <a id="id684" class="indexterm"/>clusters for small datasets is quite simple, but <a id="id685" class="indexterm"/>deriving it for huge datasets requires the use of Hadoop for providing computation power.</p></div><div><div><div><div><h3 class="title"><a id="ch06lvl3sec53"/>Performing clustering with R and Hadoop</h3></div></div></div><p>Since the k-means clustering algorithm is already developed in RHadoop, we are going to use and<a id="id686" class="indexterm"/> understand it. You can make changes <a id="id687" class="indexterm"/>in their Mappers and Reducers as per the input dataset format. As we are dealing with Hadoop, we need to develop the Mappers and Reducers to be run on nodes in a parallel manner.</p><p>The outline of the clustering algorithm is as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Defining the <a id="id688" class="indexterm"/><code class="literal">dist.fun</code> distance function</li><li class="listitem" style="list-style-type: disc">Defining the <a id="id689" class="indexterm"/><code class="literal">k-means.map</code> k-means Mapper function</li><li class="listitem" style="list-style-type: disc">Defining the <a id="id690" class="indexterm"/><code class="literal">k-means.reduce</code> k-means Reducer function</li><li class="listitem" style="list-style-type: disc">Defining the <a id="id691" class="indexterm"/><code class="literal">k-means.mr</code> k-means MapReduce function</li><li class="listitem" style="list-style-type: disc">Defining input data points to be provided to the clustering algorithms</li></ul></div><p>Now we will run <code class="literal">k-means.mr</code> (the k-means MapReduce job) by providing the required parameters.</p><p>Let's understand them one by one.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">  <code class="literal">dist.fun</code>: First, we will see the <code class="literal">dist.fun</code> function for calculating the distance between a matrix of center <code class="literal">C</code> and a matrix of point <code class="literal">P</code>, which is tested. It can produce 10<sup>6</sup> points and 10<sup>2</sup>  centers in five dimensions in approximately 16 seconds.<div><pre class="programlisting"># distance calculation function
dist.fun = 
      function(C, P) {
        apply(
          C,
          1, 
          function(x) 
            colSums((t(P) - x)^2))}</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">k-means.map</code>: The Mapper of the k-means MapReduce algorithm will compute the distance between points and all the centers and return the closest center <a id="id692" class="indexterm"/>for each point. This Mapper <a id="id693" class="indexterm"/>will run in iterations based on the <a id="id694" class="indexterm"/>following code. With the first iteration, the cluster center will be assigned randomly and from the next iteration, it will calculate these cluster centers based on the minimum distance from all the points of the cluster.<div><pre class="programlisting"># k-Means Mapper
  kmeans.map = 
      function(., P) {
        nearest = {

# First interations- Assign random cluster centers 
          if(is.null(C)) 
            sample(
              1:num.clusters, 
              nrow(P), 
              replace = T)

# Rest of the iterations, where the clusters are assigned # based on the minimum distance from points
          else {
            D = dist.fun(C, P)
            nearest = max.col(-D)}}
 
       if(!(combine || in.memory.combine))
          keyval(nearest, P) 
        else 
          keyval(nearest, cbind(1, P))}</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">k-means.reduce</code>: The Reducer of the k-means MapReduce algorithm will compute the <a id="id695" class="indexterm"/>column average of matrix points as key.<div><pre class="programlisting"># k-Means Reducer
kmeans.reduce = {

# calculating the column average for both of the 
# conditions

      if (!(combine || in.memory.combine) ) 
        function(., P) 
          t(as.matrix(apply(P, 2, mean)))
      else 
        function(k, P) 
          keyval(
            k, 
            t(as.matrix(apply(P, 2, sum))))}</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">kmeans.mr</code>: Defining the k-means MapReduce function involves specifying several <a id="id696" class="indexterm"/>input parameters, which<a id="id697" class="indexterm"/> are as follows:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">P</code>: This <a id="id698" class="indexterm"/>denotes the input data points</li><li class="listitem" style="list-style-type: disc"><code class="literal">num.clusters</code>: This is the total number of clusters</li><li class="listitem" style="list-style-type: disc"><code class="literal">num.iter</code>: This is the total number of iterations to be processed with datasets</li><li class="listitem" style="list-style-type: disc"><code class="literal">combine</code>: This will decide whether the Combiner should be enabled or disabled (<code class="literal">TRUE</code> or <code class="literal">FALSE</code>)<div><pre class="programlisting"># k-Means MapReduce – for 
kmeans.mr = 
  function(
    P, 
    num.clusters, 
    num.iter, 
    combine, 
    in.memory.combine) {
    C = NULL
    for(i in 1:num.iter ) {
      C = 
        values(

# Loading hdfs dataset
          from.dfs(

# MapReduce job, with specification of input dataset,
# Mapper and Reducer
            mapreduce(
              P,
              map = kmeans.map,
              reduce = kmeans.reduce)))
      if(combine || in.memory.combine)
        C = C[, -1]/C[, 1]
      if(nrow(C) &lt; num.clusters) {
        C = 
          rbind(
            C,
            matrix(
              rnorm(
                (num.clusters - 
                   nrow(C)) * nrow(C)), 
              ncol = nrow(C)) %*% C) }}
        C}</pre></div></li></ul></div></li><li class="listitem" style="list-style-type: disc">Defining <a id="id699" class="indexterm"/>the input data points to be <a id="id700" class="indexterm"/>provided to the clustering algorithms:<div><pre class="programlisting"># Input data points
P = do.call(
      rbind, 
      rep(


        list(

# Generating Matrix of
          matrix(
# Generate random normalized data with sd = 10
            rnorm(10, sd = 10), 
            ncol=2)), 
        20)) + 
    matrix(rnorm(200), ncol =2)</pre></div></li><li class="listitem" style="list-style-type: disc">Running <code class="literal">kmeans.mr</code> (the k-means MapReduce job) by providing it with the required parameters.<div><pre class="programlisting"># Running kmeans.mr Hadoop MapReduce algorithms with providing the required input parameters

kmeans.mr(
      to.dfs(P),
      num.clusters = 12, 
      num.iter = 5,
      combine = FALSE,
      in.memory.combine = FALSE)</pre></div></li><li class="listitem" style="list-style-type: disc">The<a id="id701" class="indexterm"/> output of the preceding command <a id="id702" class="indexterm"/>is shown in the following screenshot:<div><img src="img/3282OS_06_06.jpg" alt="Performing clustering with R and Hadoop"/></div></li></ul></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec42"/>Recommendation algorithms</h1></div></div></div><p>Recommendation is a machine-learning technique to predict what new items a user would like based <a id="id703" class="indexterm"/>on associations with the user's previous<a id="id704" class="indexterm"/> items. Recommendations are widely used in the field of e-commerce applications. Through this flexible data and behavior-driven algorithms, businesses can increase conversions by helping to ensure that relevant choices are automatically suggested to the right customers at the right time with cross-selling or up-selling.</p><p>For example, when a customer is looking for a Samsung Galaxy S IV/S4 mobile phone on Amazon, the store will also suggest other mobile phones similar to this one, presented in the <strong>Customers Who Bought This Item Also Bought</strong> window.</p><p>There are two different types of recommendations:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>User-based recommendations</strong>: In this type, users (customers) similar to current <a id="id705" class="indexterm"/>user (customer) are determined. Based on this user similarity, their <a id="id706" class="indexterm"/>interested/used items can<a id="id707" class="indexterm"/> be recommended to other users. Let's learn it through an example.<div><img src="img/3282OS_06_07.jpg" alt="Recommendation algorithms"/></div><p>Assume <a id="id708" class="indexterm"/>there are two users named Wendell and James; both have a similar interest because both are using an iPhone. Wendell had used two items, iPad and iPhone, so James will be recommended to use iPad. This is user-based recommendation.</p></li><li class="listitem" style="list-style-type: disc"><strong>Item-based recommendations</strong>: In this type, items similar to the items that are<a id="id709" class="indexterm"/> being <a id="id710" class="indexterm"/>currently used by a user are determined. Based on the item-similarity score, the similar items will be presented to the users for cross-selling and up-selling type of recommendations. Let's learn it through an example.<div><img src="img/3282OS_06_08.jpg" alt="Recommendation algorithms"/></div></li></ul></div><p>For example, a user named Vaibhav likes and uses the following books:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>Apache Mahout Cookbook</em>, <em>Piero Giacomelli</em>, <em>Packt Publishing</em></li><li class="listitem" style="list-style-type: disc"><em>Hadoop MapReduce Cookbook</em>, <em>Thilina Gunarathne</em> and <em>Srinath Perera</em>, <em>Packt Publishing</em></li><li class="listitem" style="list-style-type: disc"><em>Hadoop Real-World Solutions Cookbook</em>, <em>Brian Femiano</em>, <em>Jon Lentz</em>, and <em>Jonathan R. Owens</em>, <em>Packt Publishing</em></li><li class="listitem" style="list-style-type: disc"><em>Big Data For Dummies</em>, <em>Dr. Fern Halper</em>, <em>Judith Hurwitz</em>, <em>Marcia Kaufman</em>, and <em>Alan Nugent</em>, <em>John Wiley &amp; Sons Publishers</em></li></ul></div><p>Based on the preceding information, the recommender system will predict which new books Vaibhav would like to read, as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>Big Data Analytics with R and Hadoop</em>, <em>Vignesh Prajapati</em>, <em>Packt Publishing</em></li></ul></div><p>Now we <a id="id711" class="indexterm"/>will see how to generate recommendations with R and Hadoop. But before going towards the R and Hadoop combination, let us first see how to generate it with R. This will clear the concepts to translate <a id="id712" class="indexterm"/>your generated recommender systems to MapReduce recommendation algorithms. In case of generating recommendations with R and Hadoop, we will use the RHadoop distribution of <a id="id713" class="indexterm"/>
<strong>Revolution Analytics</strong>.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec63"/>Steps to generate recommendations in R</h2></div></div></div><p>To generate <a id="id714" class="indexterm"/>recommendations for users, we <a id="id715" class="indexterm"/>need to have datasets in a special format that can be read by the algorithm. Here, we will use the collaborative filtering algorithm for generating the recommendations rather than content-based algorithms. Hence, we will need the user's rating information for the available item sets. So, the <code class="literal">small.csv</code> dataset is given in the format <code class="literal">user ID, item ID, item's ratings</code>.</p><div><pre class="programlisting"># user ID, item ID, item's rating
1,         101,     5.0
1,         102,     3.0
1,         103,     2.5
2,         101,     2.0
2,         102,     2.5
2,         103,     5.0
2,         104,     2.0
3,         101,     2.0
3,         104,     4.0
3,         105,     4.5
3,         107,     5.0
4,         101,     5.0
4,         103,     3.0
4,         104,     4.5
4,         106,     4.0
5,         101,     4.0
5,         102,     3.0
5,         103,     2.0
5,         104,     4.0
5,         105,     3.5
5,         106,     4.0</pre></div><p>  The preceding code and datasets are reproduced from the book <em>Mahout in Action, Robin
    Anil, Ellen Friedman, Ted Dunning,</em> and <em>Sean Owen, Manning Publications</em> and the website is
    <a class="ulink" href="http://www.fens.me/">http://www.fens.me/</a>.</p><p>Recommendations can be derived from the matrix-factorization technique as follows:</p><div><pre class="programlisting">Co-occurrence matrix * scoring matrix = Recommended Results</pre></div><p>To <a id="id716" class="indexterm"/>generate the recommenders, we will<a id="id717" class="indexterm"/> follow the given steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Computing the co-occurrence matrix.</li><li class="listitem">Establishing the user-scoring matrix.</li><li class="listitem">Generating recommendations.</li></ol></div><p>From the next section, we will see technical details for performing the preceding steps.</p><div><ol class="orderedlist arabic"><li class="listitem">In the first section, computing the co-occurrence matrix, we will be able to identify the co-occurred item sets given in the dataset. In simple words, we can call it counting the pair of items from the given dataset.<div><pre class="programlisting"># Quote plyr package
library (plyr)

# Read dataset
train &lt;-read.csv (file = "small.csv", header = FALSE)
names (train) &lt;-c ("user", "item", "pref") 

# Calculated User Lists
usersUnique &lt;-function () {
  users &lt;-unique (train $ user)
  users [order (users)]
}

# Calculation Method Product List
itemsUnique &lt;-function () {
  items &lt;-unique (train $ item)
  items [order (items)]
}

# Derive unique User Lists
users &lt;-usersUnique () 

# Product List
items &lt;-itemsUnique () 


# Establish Product List Index
index &lt;-function (x) which (items %in% x)
data&lt;-ddply(train,.(user,item,pref),summarize,idx=index(item)) 

# Co-occurrence matrix
Co-occurrence &lt;-function (data) {
  n &lt;-length (items)
  co &lt;-matrix (rep (0, n * n), nrow = n)
  for (u in users) {
    idx &lt;-index (data $ item [which(data$user == u)])
    m &lt;-merge (idx, idx)
    for (i in 1: nrow (m)) {
      co [m$x[i], m$y[i]] = co[m$x[i], m$y[i]]+1
    }
  }
  return (co)
}

# Generate co-occurrence matrix
co &lt;-co-occurrence (data) </pre></div></li><li class="listitem">To <a id="id718" class="indexterm"/>establish the user-scoring <a id="id719" class="indexterm"/>matrix based on the user's rating information, the user-item rating matrix can be generated for users.<div><pre class="programlisting"># Recommendation algorithm
recommend &lt;-function (udata = udata, co = coMatrix, num = 0) {
  n &lt;- length(items)
  
  # All of pref
  pref &lt;- rep (0, n)
  pref[udata$idx] &lt;-udata$pref
  
  # User Rating Matrix
  userx &lt;- matrix(pref, nrow = n)
  
  # Scoring matrix co-occurrence matrix *
  r &lt;- co %*% userx
  
  # Recommended Sort
  r[udata$idx] &lt;-0
  idx &lt;-order(r, decreasing = TRUE)
  topn &lt;-data.frame (user = rep(udata$user[1], length(idx)), item = items[idx], val = r[idx])

  # Recommended results take months before the num
  if (num&gt; 0) {
    topn &lt;-head (topn, num)
  }

  # Recommended results take months before the num
  if (num&gt; 0) {
    topn &lt;-head (topn, num)
  }

  # Back to results 
  return (topn)
}</pre></div></li><li class="listitem">Finally, the <a id="id720" class="indexterm"/>recommendations as output can be generated by the product operations of both <a id="id721" class="indexterm"/>matrix items: co-occurrence matrix and user's scoring matrix.<div><pre class="programlisting"># initializing dataframe for recommendations storage
recommendation&lt;-data.frame()

# Generating recommendations for all of the users
for(i in 1:length(users)){
  udata&lt;-data[which(data$user==users[i]),]
  recommendation&lt;-rbind(recommendation,recommend(udata,co,0)) 
}</pre></div></li></ol></div><div><div><h3 class="title"><a id="tip20"/>Tip</h3><p>Generating<a id="id722" class="indexterm"/> recommendations <a id="id723" class="indexterm"/>via <strong>Myrrix</strong> and R<a id="id724" class="indexterm"/> interface is quite easy. For more information, refer to <a class="ulink" href="https://github.com/jwijffels/Myrrix-R-interface">https://github.com/jwijffels/Myrrix-R-interface</a>.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec64"/>Generating recommendations with R and Hadoop</h2></div></div></div><p>To generate recommendations with R and Hadoop, we need to develop an algorithm that will be <a id="id725" class="indexterm"/>able to run and perform <a id="id726" class="indexterm"/>data processing in a parallel manner. This can be implemented using Mappers and Reducers. A very interesting part of this section is how we can use R and Hadoop together to generate recommendations from big datasets.</p><p>So, here are the steps that are similar to generating recommendations with R, but translating them to the Mapper and Reducer paradigms is a little tricky:</p><div><ol class="orderedlist arabic"><li class="listitem">Establishing the co-occurrence matrix items.</li><li class="listitem">Establishing the user scoring matrix to articles.</li><li class="listitem">Generating recommendations.</li></ol></div><p>We will use the same concepts as our previous operation with R to generate recommendations with R and Hadoop. But in this case, we need to use a key-value paradigm as it's the base of parallel operations. Therefore, every function will be implemented by considering the key-value paradigm.</p><div><ol class="orderedlist arabic"><li class="listitem">In the first section, establishment of the co-occurrence matrix items, we will establish co-occurrence items in steps: grouped by user, locate each user-selected items appearing alone counting, and counting in pairs.<div><pre class="programlisting"># Load rmr2 package
library (rmr2)

# Input Data File
train &lt;-read.csv (file = "small.csv", header = FALSE)
names (train) &lt;-c ("user", "item", "pref")

# Use the hadoop rmr format, hadoop is the default setting.
rmr.options (backend = 'hadoop')

# The data set into HDFS
train.hdfs = to.dfs (keyval (train$user, train))

# see the data from hdfs
from.dfs (train.hdfs)</pre></div><p>The key points to note are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">train.mr</code>: This is the MapReduce job's key-value paradigm information</li><li class="listitem" style="list-style-type: disc"><strong>key</strong>: This is the list of items vector</li><li class="listitem" style="list-style-type: disc"><strong>value</strong>: This is the item combination vector</li></ul></div><div><pre class="programlisting"># MapReduce job 1 for co-occurrence matrix items
train.mr &lt;-mapreduce (
  train.hdfs, 
  map = function (k, v) {
    keyval (k, v$item)
  }

# for identification of co-occurrence items
  , Reduce = function (k, v) {
    m &lt;-merge (v, v)
    keyval (m$x, m$y)
  }
)</pre></div><p>The <a id="id727" class="indexterm"/>co-occurrence matrix items will be combined to count them.</p><p>To <a id="id728" class="indexterm"/>define a MapReduce job, <code class="literal">step2.mr</code> is used for calculating the frequency of the combinations of items.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Step2.mr</code>: This is the MapReduce job's key value paradigm information</li><li class="listitem" style="list-style-type: disc"><strong>key</strong>: This is the list of items vector</li><li class="listitem" style="list-style-type: disc"><strong>value</strong>: This is the co-occurrence matrix dataframe value (<code class="literal">item</code>, <code class="literal">item</code>, <code class="literal">Freq</code>)</li></ul></div><div><pre class="programlisting"># MapReduce function for calculating the frequency of the combinations of the items.
step2.mr &lt;-mapreduce (
  train.mr,

  map = function (k, v) {
    d &lt;-data.frame (k, v)
    d2 &lt;-ddply (d,. (k, v), count)

    key &lt;- d2$k
    val &lt;- d2
    keyval(key, val)
  }
)

# loading data from HDFS
from.dfs(step2.mr)</pre></div></li><li class="listitem">To<a id="id729" class="indexterm"/> establish the <a id="id730" class="indexterm"/>user-scoring matrix to articles, let us define the <code class="literal">Train2.mr</code> MapReduce job.<div><pre class="programlisting"># MapReduce job for establish user scoring matrix to articles

train2.mr &lt;-mapreduce (
  train.hdfs, 
  map = function(k, v) {
      df &lt;- v

# key as item
    key &lt;-df $ item

# value as [item, user pref]
    val &lt;-data.frame (item = df$item, user = df$user, pref = df$pref)

# emitting (key, value)pairs
    keyval(key, val)
  }
)

# loading data from HDFS
from.dfs(train2.mr)</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Train2.mr</code>: This is the MapReduce job's key value paradigm information</li><li class="listitem" style="list-style-type: disc"><strong>key</strong>: This is the list of items</li><li class="listitem" style="list-style-type: disc"><strong>value</strong>: This is the value of the user goods scoring matrix</li></ul></div><p>The following is the consolidation and co-occurrence scoring matrix:</p><div><pre class="programlisting"># Running equi joining two data – step2.mr and train2.mr
eq.hdfs &lt;-equijoin (
  left.input = step2.mr, 
  right.input = train2.mr,
  map.left = function (k, v) {
    keyval (k, v)
  },
  map.right = function (k, v) {
    keyval (k, v)
  },
  outer = c ("left")
)

# loading data from HDFS
from.dfs (eq.hdfs)</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">eq.hdfs</code>: This is the MapReduce job's key value paradigm information</li><li class="listitem" style="list-style-type: disc"><strong>key</strong>: The key here is null</li><li class="listitem" style="list-style-type: disc"><strong>value</strong>: This is the merged dataframe value</li></ul></div></li><li class="listitem">In<a id="id731" class="indexterm"/> the section of <a id="id732" class="indexterm"/>generating recommendations, we will obtain the recommended list of results.<div><pre class="programlisting"># MapReduce job to obtain recommended list of result from equijoined data
cal.mr &lt;-mapreduce (
  input = eq.hdfs,

  map = function (k, v) {
    val &lt;-v
    na &lt;-is.na (v$user.r)
    if (length (which(na))&gt; 0) val &lt;-v [-which (is.na (v $ user.r)),]
    keyval (val$kl, val)
  }
  , Reduce = function (k, v) {
    val &lt;-ddply (v,. (kl, vl, user.r), summarize, v = freq.l * pref.r)
    keyval (val $ kl, val)
  }
)

# loading data from HDFS
from.dfs (cal.mr)</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Cal.mr</code>: This is the MapReduce job's key value paradigm information</li><li class="listitem" style="list-style-type: disc"><strong>key</strong>: This is the list of items</li><li class="listitem" style="list-style-type: disc"><strong>value</strong>: This is the recommended result dataframe value</li></ul></div><p>By<a id="id733" class="indexterm"/> defining <a id="id734" class="indexterm"/>the result for getting the list of recommended items with preference value, the sorting process will be applied on the recommendation result.</p><div><pre class="programlisting"># MapReduce job for sorting the recommendation output
result.mr &lt;-mapreduce (
  input = cal.mr,
  map = function (k, v) {
    keyval (v $ user.r, v)
  }
  , Reduce = function (k, v) {
    val &lt;-ddply (v,. (user.r, vl), summarize, v = sum (v))
    val2 &lt;-val [order (val$v, decreasing = TRUE),]
    names (val2) &lt;-c ("user", "item", "pref")
    keyval (val2$user, val2)
  }
)
# loading data from HDFS
from.dfs (result.mr)</pre></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">result.mr</code>: This is the MapReduce job's key value paradigm information</li><li class="listitem" style="list-style-type: disc"><strong>key</strong>: This is the user ID</li><li class="listitem" style="list-style-type: disc"><strong>value</strong>: This is the recommended outcome dataframe value</li></ul></div></li></ol></div><p>Here, we <a id="id735" class="indexterm"/>have designed the collaborative algorithms for generating item-based recommendation. Since we have tried to <a id="id736" class="indexterm"/>make it run on parallel nodes, we have focused on the Mapper and Reducer. They may not be optimal in some cases, but you can make them optimal by using the available code.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec43"/>Summary</h1></div></div></div><p>In this chapter, we learned how we can perform Big Data analytics with machine learning with the help of R and Hadoop technologies. In the next chapter, we will learn how to enrich datasets in R by integrating R to various external data sources.</p></div></body></html>