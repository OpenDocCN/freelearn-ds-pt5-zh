["```py\n    import pandas as pd\n    import xgboost as xgb from sklearn.metrics import accuracy_score\n    ```", "```py\n    data = pd.read_csv(\"data/wholesale-data.csv\")\n    ```", "```py\n    data.head()\n    ```", "```py\n    X = data.copy()X.drop(\"Channel\", inplace = True, axis = 1)Y = data.Channel\n    ```", "```py\n    X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values\n    ```", "```py\n    train = xgb.DMatrix(X_train, label=Y_train)test = xgb.DMatrix(X_test, label=Y_test)\n    ```", "```py\n    param = {'max_depth':6, 'eta':0.1, 'silent':1, 'objective':'multi:softmax', 'num_class': 3}num_round = 5 model = xgb.train(param, train, num_round)\n    ```", "```py\n    preds = model.predict(test)\n    ```", "```py\n    acc = accuracy_score(Y_test, preds)print(\"Accuracy: %.2f%%\" % (acc * 100.0))\n    ```", "```py\ndata['target_variable'].value_counts()\n```", "```py\npositive = sum(Y == 1)\nnegative = sum(Y == 0)\nscale_pos_weight = negative/positive\n```", "```py\n    data = pd.read_csv(\"../data/adult-data.csv\", names=['age', 'workclass','education-num', 'occupation', 'capital-gain', 'capital-loss', 'hoursper-week', 'income'])\n    ```", "```py\ntrain = xgb.DMatrix('data/wholesale-data.dat.train#train.cache')\n```", "```py\nfrom sklearn.datasets import dump_svmlight_file\ndump_svmlight_file(X_train, Y_train, 'data/wholesale-data.dat.train', zero_based=True, multilabel=False)\n```", "```py\n    import pandas as pd\n    import numpy as np\n    data = pd.read_csv(\"../data/adult-data.csv\", names=['age', 'workclass', 'fnlwgt', 'education-num', 'occupation', 'capital-gain', 'capital-loss', 'hours-per-week', 'income'])\n    ```", "```py\n    from sklearn.preprocessing import LabelEncoder\n    data['workclass'] = LabelEncoder().fit_transform(data['workclass'])\n    data['occupation'] = LabelEncoder().fit_transform(data['occupation'])\n    data['income'] = LabelEncoder().fit_transform(data['income'])\n    ```", "```py\n    import xgboost as xgb\n    X = data.copy()\n    X.drop(\"income\", inplace = True, axis = 1)\n    Y = data.income\n    X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values\n    Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values\n    train = xgb.DMatrix(X_train, label=Y_train)\n    test = xgb.DMatrix(X_test, label=Y_test)\n    ```", "```py\n    test_error = {}\n    for i in range(20):\n        param = {'max_depth':i, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'}\n        num_round = 50\n        model_metrics = xgb.cv(param, train, num_round, nfold = 10)\n        test_error[i] = model_metrics.iloc[-1]['test-error-mean']\n    ```", "```py\n    import matplotlib.pyplot as plt\n    plt.scatter(test_error.keys(),test_error.values())\n    plt.xlabel('Max Depth')\n    plt.ylabel('Test Error')\n    plt.show()\n    ```", "```py\n    for i in range(1,100,5):\n        param = {'max_depth':9, 'eta':0.001*i, 'silent':1, 'objective':'binary:hinge'}\n        num_round = 500\n        model_metrics = xgb.cv(param, train, num_round, nfold = 10)\n        test_error[i] = model_metrics.iloc[-1]['test-error-mean']\n    ```", "```py\n    lr = [0.001*(i) for i in test_error.keys()]\n    plt.scatter(temp,test_error.values())\n    plt.xlabel('Learning Rate')\n    plt.ylabel('Error')\n    plt.show()\n    ```", "```py\n    param = {'max_depth':9, 'eta':0.01, 'silent':1, 'objective':'binary:hinge'}\n    num_round = 500\n    model_metrics = xgb.cv(param, train, num_round, nfold = 10)\n    plt.scatter(range(500),model_metrics['test-error-mean'], s = 0.7, label = 'Test Error')\n    plt.scatter(range(500),model_metrics['train-error-mean'], s = 0.7, label = 'Train Error')\n    plt.legend()\n    plt.show()\n    ```", "```py\n    list(model_metrics['test-error-mean']).index(min(model_metrics['test-error-mean']))\n    ```", "```py\n    model.save_model('wholesale-model.model')\n    ```", "```py\n    loaded_model = xgb.Booster({'nthread': 2})\n    loaded_model.load_model('wholesale-model.model')\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    data = pd.read_csv(\"../data/adult-data.csv\", names=['age', 'workclass', 'education-num', 'occupation', 'capital-gain', 'capital-loss', 'hours-per-week', 'income'])\n    ```", "```py\n    data[['workclass', 'occupation', 'income']] = data[['workclass', 'occupation', 'income']].apply(lambda x: x.str.strip())\n    ```", "```py\n    from sklearn.preprocessing import LabelEncoder\n    from collections import defaultdict\n    label_dict = defaultdict(LabelEncoder)\n    data[['workclass', 'occupation', 'income']] = data[['workclass', 'occupation', 'income']].apply(lambda x: label_dict[x.name].fit_transform(x))\n    ```", "```py\n    import pickle\n    with open( 'income_labels.pkl', 'wb') as f:\n            pickle.dump(label_dict, f, pickle.HIGHEST_PROTOCOL)\n    ```", "```py\n    model.save_model('income-model.model')\n    ```", "```py\n    import xgboost as xgb\n    loaded_model = xgb.Booster({'nthread': 8})\n    loaded_model.load_model('income-model.model')\n    def load_obj(file):\n          with open(file + '.pkl', 'rb') as f:\n                return pickle.load(f)\n    label_dict = load_obj('income_labels')\n    ```", "```py\n    age = input(\"Please enter age: \")\n    workclass = input(\"Please enter workclass: \")\n    education_num = input(\"Please enter education_num: \")\n    occupation = input(\"Please enter occupation: \")\n    capital_gain = input(\"Please enter capital_gain: \")\n    capital_loss = input(\"Please enter capital_loss: \")\n    hours_per_week = input(\"Please enter hours_per_week: \")\n    ```", "```py\n    data_list = [age, workclass, education_num, occupation, capital_gain, capital_loss, hours_per_week]\n    data = pd.DataFrame([data_list])\n    data.columns = ['age', 'workclass', 'education-num', 'occupation', 'capital-gain', 'capital-loss', 'hours-per-week']\n    ```", "```py\n    data[['workclass', 'occupation']] = data[['workclass', 'occupation']].apply(lambda x: label_dict[x.name].transform(x))\n    ```", "```py\n    data = data.astype(int)\n    data_xgb = xgb.DMatrix(data)\n    pred = loaded_model.predict(data_xgb)\n    ```", "```py\n    income = label_dict['income'].inverse_transform([int(pred[0])])\n    ```", "```py\n    pip3 install keras\n    ```", "```py\n    import pandas as pd\n    from keras.models import Sequential\n    from keras.layers import Dense\n    import numpy as np\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    data = pd.read_csv(\"data/wholesale-data.csv\")\n    data.head()\n    ```", "```py\n    X = data.copy()X.drop(\"Channel\", inplace = True, axis = 1)Y = data.Channel\n    ```", "```py\n    X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values\n    ```", "```py\n    model = Sequential()\n    model.add(Dense(units=8, activation='relu', input_dim=7))\n    model.add(Dense(units=16, activation='relu'))\n    model.add(Dense(units=1, activation='sigmoid'))\n    ```", "```py\n    model.compile(loss='binary_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n    model.fit(X_train, Y_train, epochs=5, batch_size=8)\n    ```", "```py\n    preds = model.predict(X_test, batch_size=128)\n    ```", "```py\n    accuracy = accuracy_score(Y_test, preds.astype(int))\n    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n    ```", "```py\nmodel = Sequential()model.add(Dense(128, input_dim=784))model.add(Activation('relu'))\nmodel.add(Dense(10))model.add(Activation('softmax'))\n```", "```py\ninputs = Input(shape=(784,))\nx = Dense(128, activation='relu')(inputs)prediction = Dense(10, activation='softmax')(x)model = Model(inputs=inputs, outputs=prediction)\n```", "```py\nfilepath=\"model-weights-{epoch:02d}-{val_loss:.2f}.hdf5\"\nmodel_ckpt = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\ncallbacks = [model_ckpt]\n```", "```py\nmodel.save('Path to model')\n```", "```py\nkeras.models.load_model('Path to model')\n```", "```py\nEarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1, mode='auto')\n```", "```py\n    import pandas as pd\n    import numpy as np\n    data = pd.read_csv('data/avocado.csv')\n    data.T\n    ```", "```py\n    data['Day'], data['Month'] = data.Date.str[:2], data.Date.str[3:5] \n    data = data.drop(['Unnamed: 0', 'Date'], axis = 1)\n    ```", "```py\n    from sklearn.preprocessing import LabelEncoder\n    from collections import defaultdict\n    label_dict = defaultdict(LabelEncoder)\n    data[['region', 'type', 'Day', 'Month', 'year']] = data[['region', 'type', 'Day', 'Month', 'year']].apply(lambda x: label_dict[x.name].fit_transform(x))\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X = data\n    y = X.pop('AveragePrice')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=9)\n    ```", "```py\n    from keras.callbacks import ModelCheckpoint, EarlyStopping\n    filepath=\"avocado-{epoch:02d}-{val_loss:.2f}.hdf5\"\n    model_ckpt = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n    es = EarlyStopping(monitor='val_loss', min_delta=1, patience=5, verbose=1)\n    callbacks = [model_ckpt, es]\n    ```", "```py\n    from keras.models import Sequential\n    from keras.layers import Dense\n    model = Sequential()\n    model.add(Dense(units=16, activation='relu', input_dim=13))\n    model.add(Dense(units=8, activation='relu'))\n    model.add(Dense(units=1, activation='linear'))\n    model.compile(loss='mse', optimizer='adam')\n    ```", "```py\n    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=40, batch_size=32)\n    model.evaluate(X_test, y_test) \n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    data = pd.read_csv('data/avocado.csv')\n    data['Day'], data['Month'] = data.Date.str[:2], data.Date.str[3:5]\n    data = data.drop(['Unnamed: 0', 'Date'], axis = 1)\n    data = data.dropna()\n    ```", "```py\n    from sklearn.preprocessing import LabelEncoder\n    from collections import defaultdict\n    label_dict = defaultdict(LabelEncoder)\n    data[['region', 'type', 'Day', 'Month', 'year']] = data[['region', 'type', 'Day', 'Month', 'year']].apply(lambda x: label_dict[x.name].fit_transform(x))\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X = data\n    y = X.pop('AveragePrice')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=9)\n    ```", "```py\n    cat_cols_dict = {col: list(data[col].unique()) for col in ['region', 'type', 'Day', 'Month', 'year'] }\n    ```", "```py\n    train_input_list = []\n    test_input_list = []\n    for col in cat_cols_dict.keys():\n        raw_values = np.unique(data[col])\n        value_map = {}\n        for i in range(len(raw_values)):\n            value_map[raw_values[i]] = i       \n        train_input_list.append(X_train[col].map(value_map).values)\n        test_input_list.append(X_test[col].map(value_map).fillna(0).values)\n    other_cols = [col for col in data.columns if (not col in cat_cols_dict.keys())]\n    train_input_list.append(X_train[other_cols].values)\n    test_input_list.append(X_test[other_cols].values)\n    ```", "```py\n    cols_out_dict = {\n        'region': 12, \n        'type': 1, \n        'Day': 10, \n        'Month': 3, \n        'year': 1\n    }\n    ```", "```py\n    from keras.models import Model\n    from keras.layers import Input, Dense, Concatenate, Reshape, Dropout\n    from keras.layers.embeddings import Embedding\n    inputs = []\n    embeddings = []\n    for col in cat_cols_dict.keys():\n\n        inp = Input(shape=(1,), name = 'input_' + col)\n        embedding = Embedding(cat_cols_dict[col], cols_out_dict[col], input_length=1, name = 'embedding_' + col)(inp)\n        embedding = Reshape(target_shape=(cols_out_dict[col],))(embedding)\n        inputs.append(inp)\n        embeddings.append(embedding)\n    ```", "```py\n    input_numeric = Input(shape=(8,))\n    embedding_numeric = Dense(16)(input_numeric) \n    inputs.append(input_numeric)\n    embeddings.append(embedding_numeric)\n    x = Concatenate()(embeddings)\n    x = Dense(16, activation='relu')(x)\n    x = Dense(4, activation='relu')(x)\n    output = Dense(1, activation='linear')(x)\n    model = Model(inputs, output)\n    model.compile(loss='mse', optimizer='adam')\n    ```", "```py\n    model.fit(train_input_list, y_train, validation_data = (test_input_list, y_test), epochs=50, batch_size=32)\n    ```", "```py\n    embedding_region = model.get_layer('embedding_region').get_weights()[0]\n    ```", "```py\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=2)\n    Y = pca.fit_transform(embedding_region[:25])\n    plt.figure(figsize=(8,8))\n    plt.scatter(-Y[:, 0], -Y[:, 1])\n    for i, txt in enumerate((label_dict['region'].inverse_transform(cat_cols_dict['region']))[:25]):\n        plt.annotate(txt, (-Y[i, 0],-Y[i, 1]), xytext = (-20, 8), textcoords = 'offset points')\n    plt.show()\n    ```"]