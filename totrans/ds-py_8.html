<html><head></head><body><div><div><h1 id="_idParaDest-205"><em class="italics"><a id="_idTextAnchor225"/>Appendix</em></h1>
		</div>
		<div><div></div>
		</div>
		<div><h2>About</h2>
			<p>This section is included to assist you in performing the activities present in the book. It includes detailed steps that are to be performed by the students to complete and achieve the objectives of the book.</p>
		</div>
		<div><h2 id="_idParaDest-206"><a id="_idTextAnchor226"/>Chapter 1: Introduction to Data Science and Data Preprocessing</h2>
			<h3 id="_idParaDest-207"><a id="_idTextAnchor227"/>Activity 1: Pre-Processing Using the Bank Marketing Subscription Dataset</h3>
			<p>Solution</p>
			<p>Let's perform various pre-processing tasks on the<code>Bank Marketing Subscription </code>dataset. We'll also be splitting the dataset into training and testing data. Follow these steps to complete this activity:</p>
			<ol>
				<li>Open a Jupyter notebook and add a new cell to import the pandas library and load the dataset into a pandas dataframe. To do so, you first need to import the library, and then use the <code>pd.read_csv()</code> function, as shown here:<pre>import pandas as pd
Link = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv'
#reading the data into the dataframe into the object data
df = pd.read_csv(Link, header=0)</pre></li>
				<li>To find the number of rows and columns in the dataset, add the following code:<pre>#Finding number of rows and columns
print("Number of rows and columns : ",df.shape)</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_61.jpg" alt="Figure 1.60: Number of rows and columns in the dataset" width="1144" height="51"/></div><h6>Figure 1.60: Number of rows and columns in the dataset</h6></li>
				<li>To print the list of all columns, add the following code:<pre>#Printing all the columns
print(list(df.columns))</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_62.jpg" alt="Figure 1.61: List of columns present in the dataset" width="1259" height="99"/></div><h6>Figure 1.61: List of columns present in the dataset</h6></li>
				<li>To overview the basic statistics of each column, such as the count, mean, median, standard deviation, minimum value, maximum value, and so on, add the following code:<pre>#Basic Statistics of each column
df.describe().transpose()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_63.jpg" alt="Figure 1.62: Basic statistics of each column" width="1576" height="790"/></div><h6>Figure 1.62: Basic statistics of each column</h6></li>
				<li>To print the basic information of each column, add the following code:<pre>#Basic Information of each column
print(df.info())</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_64.jpg" alt="Figure 1.63: Basic information of each column&#13;&#10;" width="1574" height="1098"/></div><h6>Figure 1.63: Basic information of each column</h6><p>In the preceding figure, you can see that none of the columns contains any null values. Also, the type of each column is provided.</p></li>
				<li>Now let's check for missing values and the type of each feature. Add the following code to do this:<pre>#finding the data types of each column and checking for null
null_ = df.isna().any()
dtypes = df.dtypes
sum_na_ = df.isna().sum()
info = pd.concat([null_,sum_na_,dtypes],axis = 1,keys = ['isNullExist','NullSum','type'])
info</pre><p>Have a look at the output for this in the following figure:</p><div><img src="img/C13322_01_65.jpg" alt="Figure 1.64: Information of each column stating the number of null values and the data types&#13;&#10;" width="1405" height="1230"/></div><h6>Figure 1.64: Information of each column stating the number of null values and the data types</h6></li>
				<li>Since we have loaded the dataset into the <code>data</code> object, we will remove the null values from the dataset. To remove the null values from the dataset, add the following code: <pre>#removing Null values
df = df.dropna()
#Total number of null in each column
print(df.isna().sum())# No NA</pre><p>Have a look at the output for this in the following figure:</p><div><img src="img/C13322_01_66.jpg" alt="Figure 1.65: Features of dataset with no null values&#13;&#10;" width="1095" height="641"/></div><h6>Figure 1.65: Features of dataset with no null values</h6></li>
				<li>Now we check the frequency distribution of the <code>education</code> column in the dataset. Use the <code>value_counts()</code> function to implement this:<pre>df.education.value_counts()</pre><p>Have a look at the output for this in the following figure:</p><div><img src="img/C13322_01_67.jpg" alt="Figure 1.66: Frequency distribution of the education column&#13;&#10;" width="1314" height="330"/></div><h6>Figure 1.66: Frequency distribution of the education column</h6></li>
				<li>In the preceding figure, we can see that the <code>education</code> column of the dataset has many categories. We need to reduce the categories for better modeling. To check the various categories in the <code>education</code> column, we use the <code>unique()</code> function. Type the following code to implement this:<pre>df.education.unique()  </pre><p>The output is as follows:</p><div><img src="img/C13322_01_68.jpg" alt="Figure 1.67: Various categories of the education column&#13;&#10;" width="1280" height="134"/></div><h6>Figure 1.67: Various categories of the education column</h6></li>
				<li>Now let's group the <code>basic.4y</code>, <code>basic.9y</code>, and <code>basic.6y</code> categories together and call them <code>basic</code>. To do this, we can use the <code>replace</code> function from pandas:<pre>df.education.replace({"basic.9y":"Basic","basic.6y":"Basic","basic.4y":"Basic"},inplace=True)</pre></li>
				<li>To check the list of categories after grouping, add the following code:<pre>df.education.unique()  </pre><div><img src="img/C13322_01_69.jpg" alt="Figure 1.68: Various categories of the education column&#13;&#10;" width="1168" height="88"/></div><h6>Figure 1.68: Various categories of the education column</h6><p>In the preceding figure, you can see that <code>basic.9y</code>, <code>basic.6y</code>, and <code>basic.4y</code> are grouped together as <code>Basic</code>.</p></li>
				<li>Now we select and perform a suitable encoding method for the data. Add the following code to implement this: <pre>#Select all the non numeric data using select_dtypes function
data_column_category = df.select_dtypes(exclude=[np.number]).columns</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_70.jpg" alt="Figure 1.69: Various columns of the dataset&#13;&#10;" width="1456" height="122"/></div><h6>Figure 1.69: Various columns of the dataset</h6></li>
				<li>Now we define a list with all the names of the categorical features in the data. Also, we loop through every variable in the list, getting dummy variable encoded output. Add the following code to do this:<pre>cat_vars=data_column_category
for var in cat_vars:
    cat_list='var'+'_'+var
    cat_list = pd.get_dummies(df[var], prefix=var)
    data1=df.join(cat_list)
    df=data1
 df.columns</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_71.jpg" alt="Figure 1.70: List of categorical features in the data&#13;&#10;" width="1426" height="730"/></div><h6>Figure 1.70: List of categorical features in the data</h6></li>
				<li>Now we neglect the categorical column for which we have done encoding. We'll select only the numerical and encoded categorical columns. Add the code to do this:<pre>#Categorical features
cat_vars=data_column_category
#All features
data_vars=df.columns.values.tolist()
#neglecting the categorical column for which we have done encoding
to_keep = []
for i in data_vars:
    if i not in cat_vars:
        to_keep.append(i)
        
#selecting only the numerical and encoded catergorical column
data_final=df[to_keep]
data_final.columns</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_72.jpg" alt="Figure 1.71: List of numerical and encoded categorical columns&#13;&#10;" width="1415" height="675"/></div><h6>Figure 1.71: List of numerical and encoded categorical columns</h6></li>
				<li>Finally, we split the data into train and test sets. Add the following code to implement this:<pre>#Segregating Independent and Target variable
X=data_final.drop(columns='y')
y=data_final['y']
from sklearn. model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
print("FULL Dateset X Shape: ", X.shape )
print("Train Dateset X Shape: ", X_train.shape )
print("Test Dateset X Shape: ", X_test.shape )</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C13322_01_73.jpg" alt="Figure 1.72: Shape of the full, train, and test datasets&#13;&#10;" width="1222" height="113"/>
				</div>
			</div>
			<h6>Figure 1.72: Shape of the full, train, and test datasets</h6>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor228"/>Chapter 2: Data Visualization</h2>
			<h3 id="_idParaDest-209"><a id="_idTextAnchor229"/>Activity 2: Line Plot</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Create a list of 6 strings for each month, January through June, and save it as x using:<pre>x = ['January','February','March','April','May','June']</pre></li>
				<li>Create a list of 6 values for '<code>Items Sold</code>' that starts at 1000 and increases by 200, so the final value is 2000 and save it as y as follows:<pre>y = [1000, 1200, 1400, 1600, 1800, 2000]</pre></li>
				<li>Plot y ('<code>Items Sold</code>') by x ('<code>Month</code>') with a dotted blue line and star markers using the following:<pre>plt.plot(x, y, '*:b')</pre></li>
				<li>Set the x-axis to '<code>Month</code>' using the following code:<pre>plt.xlabel('Month')</pre></li>
				<li>Set the y-axis to '<code>Items Sold</code>' as follows:<pre>plt.ylabel('Items Sold')</pre></li>
				<li>To set the title to read '<code>Items Sold has been Increasing Linearly</code>', refer to the following code:<pre>plt.title('Items Sold has been Increasing Linearly')</pre><p>Check out the following screenshot for the resultant output:</p></li>
			</ol>
			<div><div><img src="img/C13322_02_33.jpg" alt="Figure 2.33: Line plot of items sold by month&#13;&#10;" width="509" height="271"/>
				</div>
			</div>
			<h6>Figure 2.33: Line plot of items sold by month</h6>
			<h3 id="_idParaDest-210"><a id="_idTextAnchor230"/>Activity 3: Bar Plot</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Create a list of five strings for <code>x</code> containing the names of NBA franchises with the most titles using the following code:<pre>x = ['Boston Celtics','Los Angeles Lakers', 'Chicago Bulls', 'Golden State Warriors', 'San Antonio Spurs']</pre></li>
				<li>Create a list of five values for <code>y</code> containing values for '<code>Titles Won</code>' that correspond with the strings in <code>x</code> using the following code:<pre>y = [17, 16, 6, 6, 5]</pre></li>
				<li>Place <code>x</code> and <code>y</code> into a data frame with the column names '<code>Team</code>' and '<code>Titles</code>', respectively, as follows:<pre>import pandas as pd
 
df = pd.DataFrame({'Team': x,
                   'Titles': y})</pre></li>
				<li>To sort the data frame descending by 'Titles' and save it as df_sorted, refer to the following code:<pre>df_sorted = df.sort_values(by=('Titles'), ascending=False)</pre><h4>Note </h4><p class="callout">If we sort with <code>ascending=True</code>, the plot will have larger values to the right. Since we want the larger values on the left, we will be using <code>ascending=False</code>.</p></li>
				<li>Make a programmatic title and save it as title by first finding the team with the most titles and saving it as the <code>team_with_most_titles</code> object using the following code:<pre>team_with_most_titles = df_sorted['Team'][0]</pre></li>
				<li>Then, retrieve the number of titles for the team with the most titles using the following code:<pre>most_titles = df_sorted['Titles'][0]</pre></li>
				<li>Lastly, create a string that reads '<code>The Boston Celtics have the most titles with 17</code>' using the following code:<pre>title = 'The {} have the most titles with {}'.format(team_with_most_titles, most_titles)</pre></li>
				<li>Use a bar graph to plot the number of titles by team using the following code: <pre>import matplotlib.pyplot as plt
 
plt.bar(df_sorted['Team'], df_sorted['Titles'], color='red')</pre></li>
				<li>Set the x-axis label to '<code>Team</code>' using the following:<pre>plt.xlabel('Team')</pre></li>
				<li>Set the y-axis label to '<code>Number of Championships</code>' using the following:<pre>plt.ylabel('Number of Championships')</pre></li>
				<li>To prevent the x tick labels from overlapping by rotating them 45 degrees, refer to the following code:<pre>plt.xticks(rotation=45)</pre></li>
				<li>Set the title of the plot to the programmatic <code>title</code> object we created as follows: <pre>plt.title(title)</pre></li>
				<li>Save the plot to our current working directory as '<code>Titles_by_Team.png</code>' using the following code: <pre>plt.savefig('Titles_by_Team)</pre></li>
				<li>Print the plot using <code>plt.show()</code>. To understand this better, check out the following output screenshot:<div><img src="img/C13322_02_34.jpg" alt="Figure 2.34: The bar plot of the number of titles held by an NBA team&#13;&#10;" width="674" height="346"/></div><h6>Figure 2.34: The bar plot of the number of titles held by an NBA team</h6><h4>Note</h4><p class="callout">When we print the plot to the console using <code>plt.show()</code>, it appears as intended; however, when we open the file we created titled '<code>Titles_by_Team.png</code>', we see that it crops the x tick labels.</p><p>The following figure displays the bar plot with the cropped x tick labels.</p><div><img src="img/C13322_02_35.jpg" alt="Figure 2.35: 'Titles_by_Team.png' with x tick labels cropped&#13;&#10;" width="537" height="302"/></div><h6>Figure 2.35: 'Titles_by_Team.png' with x tick labels cropped</h6></li>
				<li>To fix the cropping issue, add <code>bbox_inches='tight'</code> as an argument inside of <code>plt.savefig()</code> as follows:<pre>plt.savefig('Titles_by_Team', bbox_inches='tight')</pre></li>
				<li>Now, when we open the saved '<code>Titles_by_Team.png</code>' file from our working directory, we see that the x tick labels are not cropped.<p>Check out the following output for the final result:</p></li>
			</ol>
			<div><div><img src="img/C13322_02_36.jpg" alt="Figure 2.36: 'Titles_by_Team.png' without cropped x tick labels&#13;&#10;" width="610" height="349"/>
				</div>
			</div>
			<h6>Figure 2.36: 'Titles_by_Team.png' without cropped x tick labels</h6>
			<h3 id="_idParaDest-211"><a id="_idTextAnchor231"/>Activity 4: Multiple Plot Types Using Subplots</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Import the '<code>Items_Sold_by_Week.csv</code>' file and save it as the <code>Items_by_Week</code> data frame object using the following code:<pre>import pandas as pd
 
Items_by_Week = pd.read_csv('Items_Sold_by_Week.csv')</pre></li>
				<li>Import the '<code>Weight_by_Height.csv</code>' file and save it as the <code>Weight_by_Height</code> data frame object as follows:<pre>Weight_by_Height = pd.read_csv('Weight_by_Height.csv')</pre></li>
				<li>Generate an array of 100 normally distributed numbers to use as data for the histogram and box-and-whisker plots and save it as y using the following code:<pre>y = np.random.normal(loc=0, scale=0.1, size=100)</pre></li>
				<li>To generate a figure with six subplots organized in three rows and two columns that do not overlap refer to the following code:<pre>import matplotlib.pyplot as plt
 
fig, axes = plt.subplots(nrows=3, ncols=2)
plt.tight_layout()</pre></li>
				<li>Set the respective axes' titles to match those in Figure 2.32 using the following code:<pre>axes[0,0].set_title('Line') 
axes[0,1].set_title('Bar') 
axes[1,0].set_title('Horizontal Bar') 
axes[1,1].set_title('Histogram') 
axes[2,0].set_title('Scatter') 
axes[2,1].set_title('Box-and-Whisker') </pre><div><img src="img/C13322_02_37.jpg" alt="Figure 2.37: Titled, non-overlapping empty subplots&#13;&#10;" width="572" height="276"/></div><h6>Figure 2.37: Titled, non-overlapping empty subplots</h6></li>
				<li>On the '<code>Line</code>', '<code>Bar</code>', and '<code>Horizontal Bar</code>' axes, plot '<code>Items_Sold</code>' by '<code>Week</code>' from '<code>Items_by_Week</code>' using:<pre>axes[0,0].plot(Items_by_Week['Week'], Items_by_Week['Items_Sold'])
axes[0,1].bar(Items_by_Week['Week'], Items_by_Week['Items_Sold'])
axes[1,0].barh(Items_by_Week['Week'], Items_by_Week['Items_Sold'])</pre><p>See the resultant output in the following figure:</p><div><img src="img/C13322_02_38.jpg" alt="Figure 2.38: Line, bar, and horizontal bar plots added&#13; &#10;" width="608" height="277"/></div><h6>Figure 2.38: Line, bar, and horizontal bar plots added</h6></li>
				<li>On the '<code>Histogram</code>' and '<code>Box-and-Whisker</code>' axes, plot the array of 100 normally distributed numbers using the following code:<pre>axes[1,1].hist(y, bins=20)axes[2,1].boxplot(y)</pre><p>The resultant output is displayed here:</p><div><img src="img/C13322_02_39.jpg" alt="Figure 2.39: The histogram and box-and-whisker added&#13;&#10;" width="578" height="273"/></div><h6>Figure 2.39: The histogram and box-and-whisker added</h6></li>
				<li>Plot '<code>Weight</code>' by '<code>Height</code>' on the '<code>Scatterplot</code>' axes from the '<code>Weight_by_Height</code>' data frame using the following code:<pre>axes[2,0].scatter(Weight_by_Height['Height'], Weight_by_Height['Weight'])</pre><p>See the figure here for the resultant output:</p><div><img src="img/C13322_02_40.jpg" alt="Figure 2.40: Scatterplot added&#13;&#10;" width="623" height="284"/></div><h6>Figure 2.40: Scatterplot added</h6></li>
				<li>Label the x- and y-axis for each subplot using <code>axes[row, column].set_xlabel('X-Axis Label')</code> and <code>axes[row, column].set_ylabel('Y-Axis Label')</code>, respectively.<p>See the figure here for the resultant output:</p><div><img src="img/C13322_02_41.jpg" alt="Figure 2.41: X and y axes have been labeled&#13;&#10;" width="641" height="279"/></div><h6>Figure 2.41: X and y axes have been labeled</h6></li>
				<li>Increase the size of the figure with the <code>figsize</code> argument in the subplots function as follows:<pre>fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8,8))</pre></li>
				<li>Save the figure to the current working directory as '<code>Six_Subplots</code>' using the following code:<pre>fig.savefig('Six_Subplots')</pre><p>The following figure displays the '<code>Six_Subplots.png</code>' file:</p></li>
			</ol>
			<div><div><img src="img/C13322_02_42.jpg" alt="Figure 2.42: The Six_Subplots.png file&#13;&#10;" width="591" height="571"/>
				</div>
			</div>
			<h6>Figure 2.42: The Six_Subplots.png file</h6>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor232"/>Chapter 3: Introduction to Machine Learning via Scikit-Learn</h2>
			<h3 id="_idParaDest-213"><a id="_idTextAnchor233"/>Activity 5: Generating Predictions and Evaluating the Performance of a Multiple Linear Regression Model</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Generate predictions on the test data using the following:<pre>predictions = model.predict(X_test)
2.    Plot the predicted versus actual values on a scatterplot using the following code:
import matplotlib.pyplot as plt
from scipy.stats import pearsonr
 
plt.scatter(y_test, predictions)
plt.xlabel('Y Test (True Values)')
plt.ylabel('Predicted Values')
plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, predictions)[0], 2))
plt.show()</pre><p>Refer to the resultant output here:</p><div><img src="img/C13322_03_33.jpg" alt="Figure 3.33: A scatterplot of predicted versus actual values from a multiple linear regression model&#13;&#10;" width="576" height="272"/></div><h6>Figure 3.33: A scatterplot of predicted versus actual values from a multiple linear regression model</h6><h4>Note</h4><p class="callout">There is a much stronger linear correlation between the predicted and actual values in the multiple linear regression model (r = 0.93) relative to the simple linear regression model (r = 0.62).</p></li>
				<li>To plot the distribution of the residuals, refer to the code here:<pre>import seaborn as sns
from scipy.stats import shapiro
 
sns.distplot((y_test - predictions), bins = 50)
plt.xlabel('Residuals')
plt.ylabel('Density')
plt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - predictions)[1]))
plt.show()</pre><p>Refer to the resultant output here:</p><div><img src="img/C13322_03_34.jpg" alt="Figure 3.34: The distribution of the residuals from a multiple linear regression model&#13;&#10;" width="569" height="276"/></div><h6>Figure 3.34: The distribution of the residuals from a multiple linear regression model</h6><h4>Note</h4><p class="callout">Our residuals are negatively skewed and non-normal, but this is less skewed than in the simple linear model.</p></li>
				<li>Calculate the metrics for mean absolute error, mean squared error, root mean squared error, and R-squared, and put them into a DataFrame as follows:<pre>from sklearn import metrics
import numpy as np
 
metrics_df = pd.DataFrame({'Metric': ['MAE', 
                                      'MSE', 
                                      'RMSE', 
                                      'R-Squared'],
                          'Value': [metrics.mean_absolute_error(y_test, predictions),
                                      metrics.mean_squared_error(y_test, predictions),
                                    np.sqrt(metrics.mean_squared_error(y_test, predictions)),
                                    metrics.explained_variance_score(y_test, predictions)]}).round(3)
print(metrics_df)</pre><p>Please refer to the resultant output:</p><div><img src="img/C13322_03_35.jpg" alt="Figure 3.35: Model evaluation metrics from a multiple linear regression model&#13;&#10;" width="482" height="79"/></div></li>
			</ol>
			<h6> </h6>
			<h6>Figure 3.35: Model evaluation metrics from a multiple linear regression model</h6>
			<p>The multiple linear regression model performed better on every metric relative to the simple linear regression model.</p>
			<h3 id="_idParaDest-214"><a id="_idTextAnchor234"/>Activity 6: Generating Predictions and Evaluating Performance of a Tuned Logistic Regression Model</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Generate the predicted probabilities of rain using the following code:<pre>predicted_prob = model.predict_proba(X_test)[:,1]</pre></li>
				<li>Generate the predicted class of rain using <code>predicted_class = model.predict(X_test)</code>.</li>
				<li>Evaluate performance using a confusion matrix and save it as a DataFrame using the following code:<pre>from sklearn.metrics import confusion_matrix
import numpy as np
 
cm = pd.DataFrame(confusion_matrix(y_test, predicted_class))
cm['Total'] = np.sum(cm, axis=1)
cm = cm.append(np.sum(cm, axis=0), ignore_index=True)
cm.columns = ['Predicted No', 'Predicted Yes', 'Total']
cm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])
print(cm)</pre><h6> </h6><div><img src="img/C13322_03_36.jpg" alt="Figure 3.36: The confusion matrix from our logistic regression grid search model&#13;&#10;" width="503" height="64"/></div><h6>Figure 3.36: The confusion matrix from our logistic regression grid search model</h6><h4>Note</h4><p class="callout">Nice! We have decreased our number of false positives from 6 to 2. Additionally, our false negatives were lowered from 10 to 4 (see in <em class="italics">Exercise 26</em>). Be aware that results may vary slightly. </p></li>
				<li>For further evaluation, print a classification report as follows:<pre>from sklearn.metrics import classification_report
 
print(classification_report(y_test, predicted_class))</pre></li>
			</ol>
			<h6>            </h6>
			<div><div><img src="img/C13322_03_37.jpg" alt="Figure 3.37: The classification report from our logistic regression grid search model&#13;&#10;" width="567" height="122"/>
				</div>
			</div>
			<h6>Figure 3.37: The classification report from our logistic regression grid search model</h6>
			<p>By tuning the hyperparameters of the logistic regression model, we were able to improve upon a logistic regression model that was already performing very well.</p>
			<h3 id="_idParaDest-215"><a id="_idTextAnchor235"/>Activity 7: Generating Predictions and Evaluating the Performance of the SVC Grid Search Model</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Extract predicted classes of rain using the following code:<pre>predicted_class = model.predict(X_test)</pre></li>
				<li>Create and print a confusion matrix using the code here:<pre>from sklearn.metrics import confusion_matrix
import numpy as np
 
cm = pd.DataFrame(confusion_matrix(y_test, predicted_class))
cm['Total'] = np.sum(cm, axis=1)
cm = cm.append(np.sum(cm, axis=0), ignore_index=True)
cm.columns = ['Predicted No', 'Predicted Yes', 'Total']
cm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])
print(cm)</pre><p>See the resultant output here:</p><h6>                 </h6><div><img src="img/C13322_03_38.jpg" alt="Figure 3.38: The confusion matrix from our SVC grid search model&#13;&#10;" width="567" height="62"/></div><h6>Figure 3.38: The confusion matrix from our SVC grid search model</h6></li>
				<li>Generate and print a classification report as follows:<pre>from sklearn.metrics import classification_report
 
print(classification_report(y_test, predicted_class))</pre><p>See the resultant output here:</p></li>
			</ol>
			<h6> </h6>
			<div><div><img src="img/C13322_03_39.jpg" alt="Figure 3.39: The classification report from our SVC grid search model&#13;&#10;" width="538" height="122"/>
				</div>
			</div>
			<h6>Figure 3.39: The classification report from our SVC grid search model</h6>
			<p>Here, we demonstrated how to tune the hyperparameters of an SVC model using grid search.</p>
			<h3 id="_idParaDest-216"><a id="_idTextAnchor236"/>Activity 8: Preparing Data for a Decision Tree Classifier</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Import <code>weather.csv</code> and store it as a DataFrame using the following:<pre>import pandas as pd
 
df = pd.read_csv('weather.csv')</pre></li>
				<li>Dummy code the <code>Description</code> column as follows:<pre>import pandas as pd
 
df_dummies = pd.get_dummies(df, drop_first=True)</pre></li>
				<li>Shuffle <code>df_dummies</code> using the following code:<pre>from sklearn.utils import shuffle
 
df_shuffled = shuffle(df_dummies, random_state=42)</pre></li>
				<li>Split <code>df_shuffled</code> into X and y as follows:<pre>DV = 'Rain'
X = df_shuffled.drop(DV, axis=1)
y = df_shuffled[DV]</pre></li>
				<li>Split <code>X</code> and <code>y</code> into testing and training data:<pre>from sklearn.model_selection import train_test_split
 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</pre></li>
				<li>Scale <code>X_train</code> and <code>X_test</code> using the following code:<pre>from sklearn.preprocessing import StandardScaler
 
model = StandardScaler()
X_train_scaled = model.fit_transform(X_train)
X_test_scaled = model.transform(X_test)</pre></li>
			</ol>
			<h3 id="_idParaDest-217"><a id="_idTextAnchor237"/>Activity 9: Generating Predictions and Evaluating the Performance of a Decision Tree Classifier Model</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Generate the predicted probabilities of rain using the following:<pre>predicted_prob = model.predict_proba(X_test_scaled)[:,1]</pre></li>
				<li>Generate the predicted classes of rain using the following:<pre>predicted_class = model.predict(X_test)</pre></li>
				<li>Generate and print a confusion matrix with the code here:<pre>from sklearn.metrics import confusion_matrix
import numpy as np
 
cm = pd.DataFrame(confusion_matrix(y_test, predicted_class))
cm['Total'] = np.sum(cm, axis=1)
cm = cm.append(np.sum(cm, axis=0), ignore_index=True)
cm.columns = ['Predicted No', 'Predicted Yes', 'Total']
cm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])
print(cm)</pre><p>Refer to the resultant output here:</p><div><img src="img/C13322_03_40.jpg" alt="Figure 3.40: The confusion matrix from our tuned decision tree classifier model&#13;&#10;" width="501" height="61"/></div><h6>Figure 3.40: The confusion matrix from our tuned decision tree classifier model</h6></li>
				<li>Print a classification report as follows:<pre>from sklearn.metrics import classification_report
print(classification_report(y_test, predicted_class))</pre><p>Refer to the resultant output here:</p></li>
			</ol>
			<div><div><img src="img/C13322_03_41.jpg" alt="Figure 3.41: The classification report from our tuned decision tree classifier model&#13;&#10;" width="517" height="121"/>
				</div>
			</div>
			<h6>Figure 3.41: The classification report from our tuned decision tree classifier model</h6>
			<p>There was only one misclassified observation. Thus, by tuning a decision tree classifier model on our <code>weather.csv</code> dataset, we were able to predict rain (or snow) with great accuracy. We can see that the sole driving feature was temperature in Celsius. This makes sense due to the way in which decision trees use recursive partitioning to make predictions.</p>
			<h3 id="_idParaDest-218"><a id="_idTextAnchor238"/>Activity 10: Tuning a Random Forest Regressor</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Specify the hyperparameter space as follows:<pre>import numpy as np
 
grid = {'criterion': ['mse','mae'],
        'max_features': ['auto', 'sqrt', 'log2', None],
        'min_impurity_decrease': np.linspace(0.0, 1.0, 10),
        'bootstrap': [True, False],
        'warm_start': [True, False]}</pre></li>
				<li>Instantiate the <code>GridSearchCV</code> model, optimizing the explained variance using the following code:<pre>from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
 
model = GridSearchCV(RandomForestRegressor(), grid, scoring='explained_variance', cv=5)</pre></li>
				<li>Fit the grid search model to the training set using the following (note that this may take a while):<pre>model.fit(X_train_scaled, y_train)</pre><p>See the output here:</p><div><img src="img/C13322_03_42.jpg" alt="Figure 3.42: The output from our tuned random forest regressor grid search model&#13;&#10;" width="671" height="196"/></div><h6>Figure 3.42: The output from our tuned random forest regressor grid search model</h6></li>
				<li>Print the tuned parameters as follows:<pre>best_parameters = model.best_params_
print(best_parameters)</pre><p>See the resultant output below: </p></li>
			</ol>
			<div><div><img src="img/C13322_03_43.jpg" alt="Figure 3.43: The tuned hyperparameters from our random forest regressor grid search model&#13;&#10;" width="779" height="19"/>
				</div>
			</div>
			<h6>Figure 3.43: The tuned hyperparameters from our random forest regressor grid search model</h6>
			<h3 id="_idParaDest-219"><a id="_idTextAnchor239"/>Activity 11: Generating Predictions and Evaluating the Performance of a Tuned Random Forest Regressor Model</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Generate predictions on the test data using the following:<pre>predictions = model.predict(X_test_scaled)</pre></li>
				<li>Plot the correlation of predicted and actual values using the following code:<pre>import matplotlib.pyplot as plt
from scipy.stats import pearsonr
 
plt.scatter(y_test, predictions)
plt.xlabel('Y Test (True Values)')
plt.ylabel('Predicted Values')
plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, predictions)[0], 2))
plt.show()</pre><p>Refer to the resultant output here:</p><div><img src="img/C13322_03_44.jpg" alt="Figure 3.44: A scatterplot of predicted and actual values from our random forest regression model with tuned hyperparameters&#13;&#10;" width="545" height="271"/></div><p class="Normal" lang="en-US" xml:lang="en-US"> </p><h6>Figure 3.44: A scatterplot of predicted and actual values from our random forest regression model with tuned hyperparameters</h6></li>
				<li>Plot the distribution of residuals as follows:<pre>import seaborn as sns
from scipy.stats import shapiro
 
sns.distplot((y_test - predictions), bins = 50)
plt.xlabel('Residuals')
plt.ylabel('Density')
plt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - predictions)[1]))
plt.show()</pre><p>Refer to the resultant output here:</p><h6>                 </h6><div><img src="img/C13322_03_45.jpg" alt="Figure 3.45: A histogram of residuals from a random forest regression model with &#13;&#10;tuned hyperparameters&#13;&#10;" width="572" height="273"/></div><h6>Figure 3.45: A histogram of residuals from a random forest regression model with tuned hyperparameters</h6></li>
				<li>Compute metrics, place them in a DataFrame, and print it using the code here:<pre>from sklearn import metrics
import numpy as np
 
metrics_df = pd.DataFrame({'Metric': ['MAE', 
                                      'MSE', 
                                      'RMSE', 
                                      'R-Squared'],
                          'Value': [metrics.mean_absolute_error(y_test, predictions),
                                    metrics.mean_squared_error(y_test, predictions),
                                    np.sqrt(metrics.mean_squared_error(y_test, predictions)),
                                    metrics.explained_variance_score(y_test, predictions)]}).round(3)
print(metrics_df)</pre><p>Find the resultant output here:</p></li>
			</ol>
			<h6>                 </h6>
			<div><div><img src="img/C13322_03_46.jpg" alt="Figure 3.46: Model evaluation metrics from our random forest regression model with tuned hyperparameters&#13;&#10;" width="489" height="75"/>
				</div>
			</div>
			<h6>Figure 3.46: Model evaluation metrics from our random forest regression model with tuned hyperparameters</h6>
			<p>The random forest regressor model seems to underperform compared to the multiple linear regression, as evidenced by greater MAE, MSE, and RMSE values, as well as less explained variance. Additionally, there was a weaker correlation between the predicted and actual values, and the residuals were further from being normally distributed. Nevertheless, by leveraging ensemble methods using a random forest regressor, we constructed a model that explains 75.8% of the variance in temperature and predicts temperature in Celsius + 3.781 degrees.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor240"/>Chapter 4: Dimensionality Reduction and Unsupervised Learning</h2>
			<h3 id="_idParaDest-221"><a id="_idTextAnchor241"/>Activity 12: Ensemble k-means Clustering and Calculating Predictions</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<p>After the glass dataset has been imported, shuffled, and standardized (see Exercise 58):</p>
			<ol>
				<li value="1">Instantiate an empty data frame to append each model and save it as the new data frame object <code>labels_df</code> with the following code:<pre>import pandas as pd
labels_df = pd.DataFrame()</pre></li>
				<li>Import the <code>KMeans</code> function outside of the loop using the following:<pre>from sklearn.cluster import KMeans</pre></li>
				<li>Complete 100 iterations as follows:<pre>for i in range(0, 100):</pre></li>
				<li>Save a KMeans model object with two clusters (arbitrarily decided upon, a priori) using:<pre>model = KMeans(n_clusters=2)</pre></li>
				<li>Fit the model to <code>scaled_features</code> using the following:<pre>model.fit(scaled_features)</pre></li>
				<li>Generate the labels array and save it as the labels object, as follows:<pre>labels = model.labels_</pre></li>
				<li>Store labels as a column in <code>labels_df</code> named after the iteration using the code:<pre>labels_df['Model_{}_Labels'.format(i+1)] = labels</pre></li>
				<li>After labels have been generated for each of the 100 models (see Activity 21), calculate the mode for each row using the following code:<pre>row_mode = labels_df.mode(axis=1)</pre></li>
				<li>Assign <code>row_mode</code> to a new column in <code>labels_df</code>, as shown in the following code:<pre>labels_df['row_mode'] = row_mode</pre></li>
				<li>View the first five rows of labels_df <pre>print(labels_df.head(5))</pre></li>
			</ol>
			<div><div><img src="img/C13322_04_24.jpg" alt="Figure 4.24: First five rows of labels_df&#13;&#10;" width="836" height="124"/>
				</div>
			</div>
			<h6>Figure 4.24: First five rows of labels_df</h6>
			<p>We have drastically increased the confidence in our predictions by iterating through numerous models, saving the predictions at each iteration, and assigning the final predictions as the mode of these predictions. However, these predictions were generated by models using a predetermined number of clusters. Unless we know the number of clusters a priori, we will want to discover the optimal number of clusters to segment our observations.</p>
			<h3 id="_idParaDest-222"><a id="_idTextAnchor242"/>Activity 13: Evaluating Mean Inertia by Cluster after PCA Transformation</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Instantiate a PCA model with the value for the <code>n_components</code> argument equal to <code>best_n_components</code> (that is, remember, <code>best_n_components = 6</code>) as follows:<pre>from sklearn.decomposition import PCA
model = PCA(n_components=best_n_components)</pre></li>
				<li>Fit the model to <code>scaled_features</code> and transform them into the six components, as shown here:<pre>df_pca = model.fit_transform(scaled_features)</pre></li>
				<li>Import <code>numpy</code> and the <code>KMeans</code> function outside the loop using the following code:<pre>from sklearn.cluster import KMeans
import numpy as np</pre></li>
				<li>Instantiate an empty list, <code>inertia_list</code>, for which we will append inertia values after each iteration using the following code:<pre>inertia_list = []</pre></li>
				<li>In the inside for loop, we will iterate through 100 models as follows:<pre>for i in range(100):</pre></li>
				<li>Build our <code>KMeans</code> model with <code>n_clusters=x</code> using:<pre>model = KMeans(n_clusters=x)</pre><h4>Note</h4><p class="callout">The value for x will be dictated by the outer loop which is covered in detail here.</p></li>
				<li>Fit the model to <code>df_pca</code> as follows:<pre>model.fit(df_pca)</pre></li>
				<li>Get the inertia value and save it to the object inertia using the following code:<pre>inertia = model.inertia_</pre></li>
				<li>Append inertia to <code>inertia_list</code> using the following code:<pre>inertia_list.append(inertia)</pre></li>
				<li>Moving to the outside loop, instantiate another empty list to store the average inertia values using the following code:<pre>mean_inertia_list_PCA = []</pre></li>
				<li>Since we want to check the average inertia over 100 models for <code>n_clusters</code> 1 through 10, we will instantiate the outer loop as follows:<pre>for x in range(1, 11):</pre></li>
				<li>After the inside loop has run through its 100 iterations, and the inertia value for each of the 100 models have been appended to <code>inertia_list</code>, compute the mean of this list, and save the object as <code>mean_inertia</code> using the following code:<pre>mean_inertia = np.mean(inertia_list)</pre></li>
				<li>Append <code>mean_inertia</code> to <code>mean_inertia_list_PCA</code> using the following code:<pre>mean_inertia_list_PCA.append(mean_inertia)</pre></li>
				<li>	Print <code>mean_inertia_list_PCA</code> to the console using the following code:<pre>print(mean_inertia_list_PCA)</pre></li>
				<li> Notice the output in the following screenshot:</li>
			</ol>
			<div><div><img src="img/C13322_04_25.jpg" alt="Figure 4.25: mean_inertia_list_PCA&#13;&#10;" width="835" height="32"/>
				</div>
			</div>
			<h6>Figure 4.25: mean_inertia_list_PCA</h6>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor243"/>Chapter 5: Mastering Structured Data</h2>
			<h3 id="_idParaDest-224"><a id="_idTextAnchor244"/>Activity 14: Training and Predicting the Income of a Person</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Import the libraries and load the income dataset using pandas. First, import pandas and then read the data using <code>read_csv</code>.<pre>import pandas as pd
import xgboost as xgb
import numpy as np
from sklearn.metrics import accuracy_score
data = pd.read_csv("../data/adult-data.csv", names=['age', 'workclass', 'education-num', 'occupation', 'capital-gain', 'capital-loss', 'hours-per-week', 'income'])</pre><p>The reason we are passing the names of the columns is because the data doesn't contain them. We do this to make our lives easy.</p></li>
				<li>Use Label Encoder from sklearn to encode strings. First, import <code>Label Encoder</code>. Then, encode all string categorical columns one by one.<pre>from sklearn.preprocessing import LabelEncoder
data['workclass'] = LabelEncoder().fit_transform(data['workclass'])
data['occupation'] = LabelEncoder().fit_transform(data['occupation'])
data['income'] = LabelEncoder().fit_transform(data['income'])</pre><p>Here, we encode all the categorical string data that we have. There is another method we can use to prevent writing the same piece of code again and again. See if you can find it.</p></li>
				<li>We first separate the dependent and independent variables.<pre>X = data.copy()
X.drop("income", inplace = True, axis = 1)
Y = data.income</pre></li>
				<li>Then, we divide them into training and testing sets with an 80:20 split.<pre>X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values
Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values</pre></li>
				<li>Next, we convert them into DMatrix, a data structure that the library supports.<pre>train = xgb.DMatrix(X_train, label=Y_train)
test = xgb.DMatrix(X_test, label=Y_test)</pre></li>
				<li>Then, we use the following parameters to train the model using XGBoost.<pre>param = {'max_depth':7, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'} num_round = 50
model = xgb.train(param, train, num_round)</pre></li>
				<li>Check the accuracy of the model.<pre>preds = model.predict(test)
accuracy = accuracy_score(Y[int(Y.shape[0]*0.8):].values, preds)
print("Accuracy: %.2f%%" % (accuracy * 100.0))</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C13322_05_36.jpg" alt="Figure 5.36: Final model accuracy&#13;&#10;" width="602" height="20"/>
				</div>
			</div>
			<h6>Figure 5.36: Final model accuracy</h6>
			<h3 id="_idParaDest-225"><a id="_idTextAnchor245"/>Activity 15: Predicting the Loss of Customers</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Load the income dataset using pandas. First, import pandas, and then read the data using <code>read_csv</code>.<pre>import pandas as pd
import numpy as np
data = data = pd.read_csv("data/telco-churn.csv")</pre></li>
				<li>The <code>customerID</code> variable is not required because any future prediction will have a unique <code>customerID</code>, making this variable useless for prediction.<pre>data.drop('customerID', axis = 1, inplace = True)</pre></li>
				<li>Convert all categorical variables to integers using scikit. One example is given below.<pre>from sklearn.preprocessing import LabelEncoder
data['gender'] = LabelEncoder().fit_transform(data['gender'])</pre></li>
				<li>Check the data types of the variables in the dataset.<pre>data.dtypes</pre><p>The data types of the variables will be shown as follows:</p><div><img src="img/C13322_05_37.jpg" alt="Figure 5.37: Data types of variables&#13;&#10;" width="930" height="448"/></div><h6>Figure 5.37: Data types of variables</h6></li>
				<li>As you can see, <code>TotalCharges</code> is an object. So, convert the data type of <code>TotalCharges</code> from object to numeric. coerce will make the missing values null.<pre>data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')</pre></li>
				<li>Convert the data frame to an XGBoost variable and find the best parameters for the dataset using the previous exercises as reference.<pre>import xgboost as xgb
import matplotlib.pyplot as plt
X = data.copy()
X.drop("Churn", inplace = True, axis = 1)
Y = data.Churn
X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values
Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values
train = xgb.DMatrix(X_train, label=Y_train)
test = xgb.DMatrix(X_test, label=Y_test)
test_error = {}
for i in range(20):
    param = {'max_depth':i, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'}
    num_round = 50
    model_metrics = xgb.cv(param, train, num_round, nfold = 10)
    test_error[i] = model_metrics.iloc[-1]['test-error-mean']
 
plt.scatter(test_error.keys(),test_error.values())
plt.xlabel('Max Depth')
plt.ylabel('Test Error')
plt.show()</pre><p>Check out the output in the following screenshot:</p><div><img src="img/C13322_05_38.jpg" alt="Figure 5.38: Graph of max depth to test error for telecom churn dataset&#13;&#10;" width="507" height="255"/></div><h6>Figure 5.38: Graph of max depth to test error for telecom churn dataset</h6><p>From the graph, it is clear that a max depth of 4 gives the least error. So, we will be using that to train our model.</p></li>
				<li>Create the model using the <code>max_depth</code> parameter that we chose from the previous steps.<pre>param = {'max_depth':4, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'}
num_round = 100
model = xgb.train(param, train, num_round)
preds = model.predict(test)
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y[int(Y.shape[0]*0.8):].values, preds)
print("Accuracy: %.2f%%" % (accuracy * 100.0))</pre><p>The output is as follows:</p><div><img src="img/C13322_05_39.jpg" alt="Figure 5.39: Final accuracy &#13;&#10;" width="579" height="18"/></div><h6>Figure 5.39: Final accuracy </h6></li>
				<li>Save the model for future use using the following code:<pre>model.save_model('churn-model.model')</pre></li>
			</ol>
			<h3 id="_idParaDest-226"><a id="_idTextAnchor246"/>Activity 16: Predicting a Customer's Purchase Amount</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Load the <code>Black Friday</code> dataset using pandas. First, import <code>pandas</code>, and then, read the data using <code>read_csv</code>.<pre>import pandas as pd
import numpy as np
data = data = pd.read_csv("data/BlackFriday.csv")</pre></li>
				<li>The <code>User_ID</code> variable is not required to allow predictions on new user Ids, so we drop it. <pre>data.isnull().sum()
data.drop(['User_ID', 'Product_Category_2', 'Product_Category_3'], axis = 1, inplace = True)</pre><p>The product category variables have high null values, so we drop them as well.</p></li>
				<li>Convert all categorical variables to integers using scikit-learn.<pre>from collections import defaultdict
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
label_dict = defaultdict(LabelEncoder)
data[['Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1']] = data[['Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1']].apply(lambda x: label_dict[x.name].fit_transform(x)) </pre></li>
				<li>Split the data into training and testing sets and convert it into the form required by the embedding layers. <pre>from sklearn.model_selection import train_test_split
X = data
y = X.pop('Purchase')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=9)
 
cat_cols_dict = {col: list(data[col].unique()) for col in ['Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1']}
train_input_list = []
test_input_list = []
 
for col in cat_cols_dict.keys():
    raw_values = np.unique(data[col])
    value_map = {}
    for i in range(len(raw_values)):
        value_map[raw_values[i]] = i       
    train_input_list.append(X_train[col].map(value_map).values)
    test_input_list.append(X_test[col].map(value_map).fillna(0).values)</pre></li>
				<li>Create the network using the embedding and dense layers in Keras and perform hyperparameter tuning to get the best accuracy. <pre>from keras.models import Model
from keras.layers import Input, Dense, Concatenate, Reshape, Dropout
from keras.layers.embeddings import Embedding
cols_out_dict = {
    'Product_ID': 20,
    'Gender': 1,
    'Age': 2,
    'Occupation': 6,
    'City_Category': 1,
    'Stay_In_Current_City_Years': 2,
    'Marital_Status': 1,
    'Product_Category_1': 9
}
 
inputs = []
embeddings = []
 
for col in cat_cols_dict.keys():
 
    inp = Input(shape=(1,), name = 'input_' + col)
    embedding = Embedding(len(cat_cols_dict[col]), cols_out_dict[col], input_length=1, name = 'embedding_' + col)(inp)
    embedding = Reshape(target_shape=(cols_out_dict[col],))(embedding)
    inputs.append(inp)
    embeddings.append(embedding)</pre></li>
				<li>Now, we create a three-layer network after the embedding layers.<pre>x = Concatenate()(embeddings)
x = Dense(4, activation='relu')(x)
x = Dense(2, activation='relu')(x)
output = Dense(1, activation='relu')(x)
 
model = Model(inputs, output)
 
model.compile(loss='mae', optimizer='adam')
 
model.fit(train_input_list, y_train, validation_data = (test_input_list, y_test), epochs=20, batch_size=128)</pre></li>
				<li>Check the RMSE of the model on the test set.<pre>from sklearn.metrics import mean_squared_error
y_pred = model.predict(test_input_list)
np.sqrt(mean_squared_error(y_test, y_pred))</pre><p>The RMSE is:</p><div><img src="img/C13322_05_40.jpg" alt="Figure 5.40: RMSE model&#13;&#10;" width="525" height="15"/></div><h6>Figure 5.40: RMSE model</h6></li>
				<li>Visualize the product ID embedding.<pre>import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
embedding_Product_ID = model.get_layer('embedding_Product_ID').get_weights()[0]
pca = PCA(n_components=2) 
Y = pca.fit_transform(embedding_Product_ID[:40])
plt.figure(figsize=(8,8))
plt.scatter(-Y[:, 0], -Y[:, 1])
for i, txt in enumerate(label_dict['Product_ID'].inverse_transform(cat_cols_dict['Product_ID'])[:40]):
    plt.annotate(txt, (-Y[i, 0],-Y[i, 1]), xytext = (-20, 8), textcoords = 'offset points')
plt.show()</pre><p>The plot is as follows:</p><h6> </h6><div><img src="img/C13322_05_41.jpg" alt="" width="533" height="463"/></div><h6>Figure 5.41: Plot of clustered model</h6><p>From the plot, you can see that similar products have been clustered together by the model.</p></li>
				<li>Save the model for future use.<pre>model.save ('black-friday.model')</pre></li>
			</ol>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor247"/>Chapter 6: Decoding Images</h2>
			<h3 id="_idParaDest-228"><a id="_idTextAnchor248"/>Activity 17: Predict if an Image Is of a Cat or a Dog</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">If you look at the name of the images in the dataset, you will find that the images of dogs start with dog followed by '.' and then a number, for example – "dog.123.jpg". Similarly, the images of cats start with cat. So, let's create a function to get the label from the name of the file:<pre>def get_label(file):
    class_label = file.split('.')[0]
    if class_label == 'dog': label_vector = [1,0]
    elif class_label == 'cat': label_vector = [0,1]
    return label_vector</pre><p>Then, create a function to read, resize, and preprocess the images:</p><pre>import os
import numpy as np
from PIL import Image
from tqdm import tqdm
from random import shuffle
 
SIZE = 50
 
def get_data():
    data = []
    files = os.listdir(PATH)
 
    for image in tqdm(files): 
        label_vector = get_label(image) 
        img = Image.open(PATH + image).convert('L')
        img = img.resize((SIZE,SIZE)) 
        data.append([np.asarray(img),np.array(label_vector)])
        
    shuffle(data)
    return data</pre><p><code>SIZE</code> here refers to the dimension of the final square image we will input to the model. We resize the image to have the length and breadth equal to <code>SIZE</code>.</p><h4>Note</h4><p class="callout">When running <code>os.listdir(PATH)</code>, you will find that all the images of cats come first, followed by images of dogs. </p></li>
				<li>To have the same distribution of both the classes in the training and testing sets, we will shuffle the data.</li>
				<li>Define the size of the image and read the data. Split the loaded data into training and testing sets:<pre>data = get_data()
train = data[:7000]
test = data[7000:]
x_train = [data[0] for data in train]
y_train = [data[1] for data in train]
x_test = [data[0] for data in test]
y_test = [data[1] for data in test]</pre></li>
				<li>Transform the lists to numpy arrays and reshape the images to a format that Keras will accept:<pre>y_train = np.array(y_train)
y_test = np.array(y_test)
x_train = np.array(x_train).reshape(-1, SIZE, SIZE, 1)
x_test = np.array(x_test).reshape(-1, SIZE, SIZE, 1)</pre></li>
				<li>Create a CNN model that makes use of regularization to perform training:<pre>from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization
model = Sequential() </pre><p>Add the convolutional layers:</p><pre>model.add(Conv2D(48, (3, 3), activation='relu', padding='same', input_shape=(50,50,1)))    
model.add(Conv2D(48, (3, 3), activation='relu'))  </pre><p>Add the pooling layer:</p><pre>model.add(MaxPool2D(pool_size=(2, 2)))</pre></li>
				<li>Add the batch normalization layer along with a dropout layer using the following code:<pre>model.add(BatchNormalization())
model.add(Dropout(0.10))</pre></li>
				<li>Flatten the 2D matrices into 1D vectors:<pre>model.add(Flatten())</pre></li>
				<li>Use dense layers as the final layers for the model:<pre>model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(2, activation='softmax'))</pre></li>
				<li>Compile the model and then train it using the training data:<pre>model.compile(loss='categorical_crossentropy', 
              optimizer='adam',
              metrics = ['accuracy'])
Define the number of epochs you want to train the model for:
EPOCHS = 10
model_details = model.fit(x_train, y_train,
                    batch_size = 128, 
                    epochs = EPOCHS, 
                    validation_data= (x_test, y_test),
                    verbose=1)</pre></li>
				<li>Print the model's accuracy on the test set:<pre>score = model.evaluate(x_test, y_test)
print("Accuracy: {0:.2f}%".format(score[1]*100)) </pre><div><img src="img/C13322_06_39.jpg" alt="Figure 6.39: Model accuracy on the test set&#13;&#10;" width="572" height="22"/></div><h6>Figure 6.39: Model accuracy on the test set</h6></li>
				<li>Print the model's accuracy on the training set:<pre>score = model.evaluate(x_train, y_train)
print("Accuracy: {0:.2f}%".format(score[1]*100)) </pre></li>
			</ol>
			<div><div><img src="img/C13322_06_40.jpg" alt="Figure 6.40: Model accuracy on the train set&#13;&#10;" width="546" height="23"/>
				</div>
			</div>
			<h6>Figure 6.40: Model accuracy on the train set</h6>
			<p>The test set accuracy for this model is 70.4%. The training set accuracy is really high, at 96%. This means that the model has started to overfit. Improving the model to get the best possible accuracy is left for you as an exercise. You can plot the incorrectly predicted images using the code from previous exercises to get a sense of how well the model performs:</p>
			<pre>import matplotlib.pyplot as plt
y_pred = model.predict(x_test)
incorrect_indices = np.nonzero(np.argmax(y_pred,axis=1) != np.argmax(y_test,axis=1))[0]
labels = ['dog', 'cat']
image = 5
plt.imshow(x_test[incorrect_indices[image]].reshape(50,50),  cmap=plt.get_cmap('gray'))
plt.show()
print("Prediction: {0}".format(labels[np.argmax(y_pred[incorrect_indices[image]])]))</pre>
			<div><div><img src="img/C13322_06_41.jpg" alt="Figure 6.41: Incorrect prediction of a dog by the regularized CNN model&#13;&#10;" width="780" height="352"/>
				</div>
			</div>
			<h6>Figure 6.41: Incorrect prediction of a dog by the regularized CNN model</h6>
			<h3 id="_idParaDest-229"><a id="_idTextAnchor249"/>Activity 18: Identifying and Augmenting an Image</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Create functions to get the images and the labels of the dataset:<pre>from PIL import Image
def get_input(file):
    return Image.open(PATH+file)
 
def get_output(file):
    class_label = file.split('.')[0]
    if class_label == 'dog': label_vector = [1,0]
    elif class_label == 'cat': label_vector = [0,1]
    return label_vector</pre></li>
				<li>Create functions to preprocess and augment images:<pre>SIZE = 50
def preprocess_input(image):
    # Data preprocessing
    image = image.convert('L')
    image = image.resize((SIZE,SIZE))
    
    
    # Data augmentation
    random_vertical_shift(image, shift=0.2)
    random_horizontal_shift(image, shift=0.2)
    random_rotate(image, rot_range=45)
    random_horizontal_flip(image)
    
    return np.array(image).reshape(SIZE,SIZE,1)</pre></li>
				<li>Implement the augmentation functions to randomly execute the augmentation when passed an image and return the image with the result. <p>This is for horizontal flip:</p><pre>import random
def random_horizontal_flip(image):
    toss = random.randint(1, 2)
    if toss == 1:
        return image.transpose(Image.FLIP_LEFT_RIGHT)
    else:
        return image</pre><p>This is for rotation:</p><pre>def random_rotate(image, rot_range):
    value = random.randint(-rot_range,rot_range)
    return image.rotate(value)</pre><p>This is for image shift:</p><pre>import PIL
def random_horizontal_shift(image, shift):
    width, height = image.size
    rand_shift = random.randint(0,shift*width)
    image = PIL.ImageChops.offset(image, rand_shift, 0)
    image.paste((0), (0, 0, rand_shift, height))
    return image
 def random_vertical_shift(image, shift):
    width, height = image.size
    rand_shift = random.randint(0,shift*height)
    image = PIL.ImageChops.offset(image, 0, rand_shift)
    image.paste((0), (0, 0, width, rand_shift))
    return image</pre></li>
				<li>Finally, create the generator that will generate images batches to be used to train the model:<pre>import numpy as np
def custom_image_generator(images, batch_size = 128):
    while True:
        # Randomly select images for the batch
        batch_images = np.random.choice(images, size = batch_size)
        batch_input = []
        batch_output = [] 
        
        # Read image, perform preprocessing and get labels
        for file in batch_images:
            # Function that reads and returns the image
            input_image = get_input(file)
            # Function that gets the label of the image
            label = get_output(file)
            # Function that pre-processes and augments the image
            image = preprocess_input(input_image)
 
            batch_input.append(image)
            batch_output.append(label)
 
        batch_x = np.array(batch_input)
        batch_y = np.array(batch_output)
 
        # Return a tuple of (images,labels) to feed the network
        yield(batch_x, batch_y)</pre></li>
				<li>Create functions to load the test dataset's images and labels:<pre>def get_label(file):
    class_label = file.split('.')[0]
    if class_label == 'dog': label_vector = [1,0]
    elif class_label == 'cat': label_vector = [0,1]
    return label_vector</pre><p>This <code>get_data</code> function is similar to the one we used in <em class="italics">Activity 1</em>. The modification here is that we get the list of images to be read as an input parameter, and we return a tuple of images and their labels:</p><pre>def get_data(files):
    data_image = []
    labels = []
    for image in tqdm(files):
        
        label_vector = get_label(image)
        
 
        img = Image.open(PATH + image).convert('L')
        img = img.resize((SIZE,SIZE))
        
        
        labels.append(label_vector)
        data_image.append(np.asarray(img).reshape(SIZE,SIZE,1))
        
    data_x = np.array(data_image)
    data_y = np.array(labels)
        
    return (data_x, data_y)</pre></li>
				<li>Now, create the test train split and load the test dataset:<pre>import os
files = os.listdir(PATH)
random.shuffle(files)
train = files[:7000]
test = files[7000:]
validation_data = get_data(test)</pre></li>
				<li>Create the model and perform training:<pre>from keras.models import Sequential
model = Sequential()</pre><p>Add the convolutional layers</p><pre>from keras.layers import Input, Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization
model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(50,50,1)))    
model.add(Conv2D(32, (3, 3), activation='relu'))</pre><p>Add the pooling layer:</p><pre>model.add(MaxPool2D(pool_size=(2, 2)))</pre></li>
				<li>Add the batch normalization layer along with a dropout layer:<pre>model.add(BatchNormalization())
model.add(Dropout(0.10))</pre></li>
				<li>Flatten the 2D matrices into 1D vectors:<pre>model.add(Flatten())</pre></li>
				<li>Use dense layers as the final layers for the model:<pre>model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
 
model.add(Dense(2, activation='softmax'))</pre></li>
				<li>Compile the model and train it using the generator that you created:<pre>EPOCHS = 10
BATCH_SIZE = 128
model.compile(loss='categorical_crossentropy', 
              optimizer='adam',
              metrics = ['accuracy']) 
model_details = model.fit_generator(custom_image_generator(train, batch_size = BATCH_SIZE),
                    steps_per_epoch = len(train) // BATCH_SIZE, 
                    epochs = EPOCHS, 
                    validation_data= validation_data,
                    verbose=1) </pre></li>
			</ol>
			<p>The test set accuracy for this model is 72.6%, which is an improvement on the model in <em class="italics">Activity 21</em>. You will observe that the training accuracy is really high, at 98%. This means that this model has started to overfit, much like the one in <em class="italics">Activity 21</em>. This could be due to a lack of data augmentation. Try changing the data augmentation parameters to see if there is any change in accuracy. Alternatively, you can modify the architecture of the neural network to get better results. You can plot the incorrectly predicted images to get a sense of how well the model performs.</p>
			<pre>import matplotlib.pyplot as plt
y_pred = model.predict(validation_data[0])
incorrect_indices = np.nonzero(np.argmax(y_pred,axis=1) != np.argmax(validation_data[1],axis=1))[0]
labels = ['dog', 'cat']
image = 7
plt.imshow(validation_data[0][incorrect_indices[image]].reshape(50,50), cmap=plt.get_cmap('gray'))
plt.show()
print("Prediction: {0}".format(labels[np.argmax(y_pred[incorrect_indices[image]])]))</pre>
			<div><div><img src="img/C13322_06_42.jpg" alt="Figure 6.42: Incorrect prediction of a cat by the data augmentation CNN model &#13;&#10;" width="982" height="348"/>
				</div>
			</div>
			<h6>Figure 6.42: Incorrect prediction of a cat by the data augmentation CNN model</h6>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor250"/>Chapter 7: Processing Human Language</h2>
			<h3 id="_idParaDest-231"><a id="_idTextAnchor251"/>Activity 19: Predicting Sentiments of Movie Reviews</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Read the IMDB movie review dataset using pandas in Python:<pre>import pandas as pd
data = pd.read_csv('../../chapter 7/data/movie_reviews.csv', encoding='latin-1')</pre></li>
				<li>Convert the tweets to lowercase to reduce the number of unique words: <pre>data.text = data.text.str.lower()</pre><h4>Note</h4><p class="callout">Keep in mind that "<code>Hello</code>" and "<code>hellow</code>" are not the same to a computer.</p></li>
				<li>Clean the reviews using RegEx with the <code>clean_str</code> function:<pre>import re
def clean_str(string):
    
    string = re.sub(r"https?\://\S+", '', string)
    string = re.sub(r'\&lt;a href', ' ', string)
    string = re.sub(r'&amp;amp;', '', string) 
    string = re.sub(r'&lt;br /&gt;', ' ', string)
    string = re.sub(r'[_"\-;%()|+&amp;=*%.,!?:#$@\[\]/]', ' ', string)
    string = re.sub('\d','', string)
    string = re.sub(r"can\'t", "cannot", string)
    string = re.sub(r"it\'s", "it is", string)
    return string
data.SentimentText = data.SentimentText.apply(lambda x: clean_str(str(x)))</pre></li>
				<li>Next, remove stop words and other frequently occurring unnecessary words from the reviews: <h4>Note</h4><p class="callout">To see how we found these, words refer to <em class="italics">Exercise 51</em>. </p></li>
				<li>This step converts strings into toke<a id="_idTextAnchor252"/>ns (which will be helpful in the next step):<pre>from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,sent_tokenize
stop_words = stopwords.words('english') + ['movie', 'film', 'time']
stop_words = set(stop_words)
remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]
data['SentimentText'] = data['SentimentText'].apply(remove_stop_words)</pre></li>
				<li>Create the word embedding of the reviews with the tokens created in the previous step. Here, we will use genism Word2Vec to create these embedding vectors:<pre>from gensim.models import Word2Vec
model = Word2Vec(
        data['SentimentText'].apply(lambda x: x[0]),
        iter=10,
        size=16,
        window=5,
        min_count=5,
        workers=10)
model.wv.save_word2vec_format('movie_embedding.txt', binary=False)</pre></li>
				<li>Combine the tokens to get a string and then drop any review that does not have anything in it after stop word removal:<pre>def combine_text(text):    
    try:
        return ' '.join(text[0])
    except:
        return np.nan
 
data.SentimentText = data.SentimentText.apply(lambda x: combine_text(x))
data = data.dropna(how='any')</pre></li>
				<li>Tokenize the reviews using the Keras Tokenizer and convert them into numbers: <pre>from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(list(data['SentimentText']))
sequences = tokenizer.texts_to_sequences(data['SentimentText'])
word_index = tokenizer.word_index</pre></li>
				<li>Finally, pad the tweets to have a maximum of 100 words. This will remove any words after the 100-word limit and add 0s if the number of words is less than 100:<pre>from keras.preprocessing.sequence import pad_sequences
reviews = pad_sequences(sequences, maxlen=100)</pre></li>
				<li>Load the created embedding to get the embedding matrix using the <code>load_embedding</code> function discussed in the <em class="italics">Text Processing</em> section:<pre>import numpy as np
 
def load_embedding(filename, word_index , num_words, embedding_dim):
    embeddings_index = {}
    file = open(filename, encoding="utf-8")
    for line in file:
        values = line.split()
        word = values[0]
        coef = np.asarray(values[1:])
        embeddings_index[word] = coef
    file.close()
    
    embedding_matrix = np.zeros((num_words, embedding_dim))
    for word, pos in word_index.items():
        if pos &gt;= num_words:
            continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[pos] = embedding_vector
    return embedding_matrix
 
embedding_matrix = load_embedding('movie_embedding.txt', word_index, len(word_index), 16)</pre></li>
				<li>Convert the label into one-hot vector using pandas' <code>get_dummies</code> function and split the dataset into testing and training sets with an 80:20 split:<pre>from sklearn.model_selection import train_test_split
labels = pd.get_dummies(data.Sentiment)
X_train, X_test, y_train, y_test = train_test_split(reviews,labels, test_size=0.2, random_state=9)</pre></li>
				<li>Create the neural network model starting with the input and embedding layers. This layer converts the input words into their embedding vectors:<pre>from keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten
from keras.models import Model
inp = Input((100,))
embedding_layer = Embedding(len(word_index),
                    16,
                    weights=[embedding_matrix],
                    input_length=100,
                    trainable=False)(inp)</pre></li>
				<li>Create the rest of the fully connected neural network using Keras:<pre>model = Flatten()(embedding_layer)
model = BatchNormalization()(model)
model = Dropout(0.10)(model)
model = Dense(units=1024, activation='relu')(model)
model = Dense(units=256, activation='relu')(model)
model = Dropout(0.5)(model)
predictions = Dense(units=2, activation='softmax')(model)
model = Model(inputs = inp, outputs = predictions)</pre></li>
				<li>Compile and train the model for 10 epochs. You can modify the model and the hyperparameters to try and get a better accuracy:<pre>model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = ['acc'])
model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=256)</pre></li>
				<li>Calculate the accuracy of the model on the test set to see how well our model performs on previously unseen data by using the following:<pre>from sklearn.metrics import accuracy_score
preds = model.predict(X_test)
accuracy_score(np.argmax(preds, 1), np.argmax(y_test.values, 1))</pre><p>The accuracy of the model is:</p><div><img src="img/C13322_07_39.jpg" alt="Figure 7.39: Model accuracy&#13;&#10;" width="537" height="24"/></div><h6>Figure 7.39: Model accuracy</h6></li>
				<li>Plot the confusion matrix of the model to get a proper sense of the model's prediction:<pre>y_actual = pd.Series(np.argmax(y_test.values, axis=1), name='Actual')
y_pred = pd.Series(np.argmax(preds, axis=1), name='Predicted')
pd.crosstab(y_actual, y_pred, margins=True)</pre><p>Check the following </p><h6> </h6><div><img src="img/C13322_07_40.jpg" alt="Figure 7.40: Confusion matrix of the model (0 = negative sentiment, 1 = positive sentiment)&#13;&#10;" width="642" height="147"/></div><h6>Figure 7.40: Confusion matrix of the model (0 = negative sentiment, 1 = positive sentiment)</h6></li>
				<li>Check the performance of the model by seeing the sentiment predictions on random reviews using the following code:<pre>review_num = 111
print("Review: \n"+tokenizer.sequences_to_texts([X_test[review_num]])[0])
sentiment = "Positive" if np.argmax(preds[review_num]) else "Negative"
print("\nPredicted sentiment = "+ sentiment)
sentiment = "Positive" if np.argmax(y_test.values[review_num]) else "Negative"
print("\nActual sentiment = "+ sentiment)</pre><p>Check that you receive the following output:</p></li>
			</ol>
			<h6>                </h6>
			<div><div><img src="img/C13322_07_41.jpg" alt="Figure 7.41: A review from the IMDB dataset&#13;&#10;" width="781" height="143"/>
				</div>
			</div>
			<h6>Figure 7.41: A review from the IMDB dataset</h6>
			<h3 id="_idParaDest-232"><a id="_idTextAnchor253"/>Activity 20: Predicting Sentiments from Tweets</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Read the tweet dataset using pandas and rename the columns with those given in the following code:<pre>import pandas as pd
data = pd.read_csv('tweet-data.csv', encoding='latin-1', header=None)
data.columns = ['sentiment', 'id', 'date', 'q', 'user', 'text']</pre></li>
				<li>Drop the following columns as we won't be using them. You can analyze and use them if you want when trying to improve the accuracy:<pre>data = data.drop(['id', 'date', 'q', 'user'], axis=1)</pre></li>
				<li>We perform this activity only on a subset (400,000 tweets) of the data to save time. If you want, you can work on the whole dataset:<pre>data = data.sample(400000).reset_index(drop=True)</pre></li>
				<li>Convert the tweets to lowercase to reduce the number of unique words. Keep in mind that "<code>Hello</code>" and "<code>hellow</code>" are not the same to a computer:<pre>data.text = data.text.str.lower()</pre></li>
				<li>Clean the tweets using the <code>clean_str</code> function: <pre>import re
def clean_str(string):
    string = re.sub(r"https?\://\S+", '', string)
    string = re.sub(r"@\w*\s", '', string)
    string = re.sub(r'\&lt;a href', ' ', string)
    string = re.sub(r'&amp;amp;', '', string) 
    string = re.sub(r'&lt;br /&gt;', ' ', string)
    string = re.sub(r'[_"\-;%()|+&amp;=*%.,!?:#$@\[\]/]', ' ', string)
    string = re.sub('\d','', string)
    return string
 
data.text = data.text.apply(lambda x: clean_str(str(x)))</pre></li>
				<li>Remove all the stop words from the tweets, as was done in the <code>Text Preprocessing</code> section:<pre>from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,sent_tokenize
stop_words = stopwords.words('english')
stop_words = set(stop_words)
remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]
data['text'] = data['text'].apply(remove_stop_words)
 
def combine_text(text):    
    try:
        return ' '.join(text[0])
    except:
        return np.nan
 
data.text = data.text.apply(lambda x: combine_text(x))
 
data = data.dropna(how='any')</pre></li>
				<li>Tokenize the tweets and convert them to numbers using the Keras Tokenizer:<pre>from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(list(data['text']))
sequences = tokenizer.texts_to_sequences(data['text'])
word_index = tokenizer.word_index</pre></li>
				<li>Finally, pad the tweets to have a maximum of 50 words. This will remove any words after the 50-word limit and add 0s if the number of words is less than 50:<pre>from keras.preprocessing.sequence import pad_sequences
tweets = pad_sequences(sequences, maxlen=50)</pre></li>
				<li>Create the embedding matrix from the GloVe embedding file that we downloaded using the <code>load_embedding</code> function:<pre>import numpy as np
def load_embedding(filename, word_index , num_words, embedding_dim):
    embeddings_index = {}
    file = open(filename, encoding="utf-8")
    for line in file:
        values = line.split()
        word = values[0]
        coef = np.asarray(values[1:])
        embeddings_index[word] = coef
    file.close()
    
    embedding_matrix = np.zeros((num_words, embedding_dim))
    for word, pos in word_index.items():
        if pos &gt;= num_words:
            continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[pos] = embedding_vector
    return embedding_matrix
 
embedding_matrix = load_embedding('../../embedding/glove.twitter.27B.50d.txt', word_index, len(word_index), 50)</pre></li>
				<li>Split the dataset into training and testing sets with an 80:20 spilt. You can experiment with different splits: <pre>from sklearn.model_selection import train_test_split  
X_train, X_test, y_train, y_test = train_test_split(tweets, pd.get_dummies(data.sentiment), test_size=0.2, random_state=9)</pre></li>
				<li>Create the LSTM model that will predict the sentiment. You can modify this to create your own neural network:<pre>from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization, Embedding, Flatten, LSTM
embedding_layer = Embedding(len(word_index),
                           50,
                           weights=[embedding_matrix],
                           input_length=50,
                            trainable=False)
model = Sequential()
model.add(embedding_layer)
model.add(Dropout(0.5))
model.add(LSTM(100, dropout=0.2))
model.add(Dense(2, activation='softmax'))
 
model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = ['acc'])</pre></li>
				<li>Train the model. Here, we train it only for 10 epochs. You can increase the number of epochs to try and get a better accuracy:<pre>model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=256)</pre></li>
				<li>Check how well the model is performing by predicting the sentiment of a few tweets in the test set:<pre>preds = model.predict(X_test)
review_num = 1
print("Tweet: \n"+tokenizer.sequences_to_texts([X_test[review_num]])[0])
sentiment = "Positive" if np.argmax(preds[review_num]) else "Negative"
print("\nPredicted sentiment = "+ sentiment)
sentiment = "Positive" if np.argmax(y_test.values[review_num]) else "Negative"
print("\nActual sentiment = "+ sentiment)</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C13322_07_42.jpg" alt="Figure 7.42: Positive (left) and negative (right) tweets and their predictions&#13;&#10;" width="570" height="112"/>
				</div>
			</div>
			<h6>Figure 7.42: Positive (left) and negative (right) tweets and their predictions</h6>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor254"/>Chapter 8: Tips and Tricks of the Trade</h2>
			<h3 id="_idParaDest-234"><a id="_idTextAnchor255"/>Activity 21: Classifying Images using InceptionV3</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">Create functions to get images and labels. Here <code>PATH</code> variable contains the path to the training dataset.<pre>from PIL import Image
def get_input(file):
    return Image.open(PATH+file)
def get_output(file):
    class_label = file.split('.')[0]
    if class_label == 'dog': label_vector = [1,0]
    elif class_label == 'cat': label_vector = [0,1]
    return label_vector</pre></li>
				<li>Set <code>SIZE</code> and <code>CHANNELS. SIZE</code> is the dimension of the square image input. <code>CHANNELS</code> is the number of channels in the training data images. There are 3 channels in a RGB image.<pre>SIZE = 200
CHANNELS = 3</pre></li>
				<li>Create a function to preprocess and augment images:<pre>def preprocess_input(image):
    
    # Data preprocessing
    image = image.resize((SIZE,SIZE))
    image = np.array(image).reshape(SIZE,SIZE,CHANNELS)
    
    # Normalize image 
    image = image/255.0
    
    return image</pre></li>
				<li>Finally, develop the generator that will generate the batches:<pre>import numpy as np
def custom_image_generator(images, batch_size = 128):
    
    while True:
        # Randomly select images for the batch
        batch_images = np.random.choice(images, size = batch_size)
        batch_input = []
        batch_output = [] 
        
        # Read image, perform preprocessing and get labels
        for file in batch_images:
            # Function that reads and returns the image
            input_image = get_input(file)
            # Function that gets the label of the image
            label = get_output(file)
            # Function that pre-processes and augments the image
            image = preprocess_input(input_image)
 
            batch_input.append(image)
            batch_output.append(label)
 
        batch_x = np.array(batch_input)
        batch_y = np.array(batch_output)
 
        # Return a tuple of (images,labels) to feed the network
        yield(batch_x, batch_y)</pre></li>
				<li>Next, we will read the validation data. Create a function to read the images and their labels:<pre>from tqdm import tqdm
def get_data(files):
    data_image = []
    labels = []
    for image in tqdm(files):
        label_vector = get_output(image)
        
        img = Image.open(PATH + image)
        img = img.resize((SIZE,SIZE))
        
        labels.append(label_vector)
        img = np.asarray(img).reshape(SIZE,SIZE,CHANNELS)
        img = img/255.0
        data_image.append(img)
        
    data_x = np.array(data_image)
    data_y = np.array(labels)
        
    return (data_x, data_y)</pre></li>
				<li>Read the validation files:<pre>import os
files = os.listdir(PATH)
random.shuffle(files)
train = files[:7000]
test = files[7000:]
validation_data = get_data(test)
7.    Plot a few images from the dataset to see whether you loaded the files correctly:
import matplotlib.pyplot as plt
plt.figure(figsize=(20,10))
columns = 5
for i in range(columns):
    plt.subplot(5 / columns + 1, columns, i + 1)
    plt.imshow(validation_data[0][i])</pre><p>A random sample of the images is shown here:</p><div><img src="img/C13322_08_16.jpg" alt="Figure 8.16: Sample images from the loaded dataset&#13;&#10;" width="1301" height="262"/></div><h6>Figure 8.16: Sample images from the loaded dataset</h6></li>
				<li>Load the Inception model and pass the shape of the input images:<pre>from keras.applications.inception_v3 import InceptionV3
base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(SIZE,SIZE,CHANNELS))</pre></li>
				<li>Add the output dense layer according to our problem:<pre>from keras.layers import GlobalAveragePooling2D, Dense, Dropout
from keras.models import Model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(2, activation='softmax')(x)
 
model = Model(inputs=base_model.input, outputs=predictions)</pre></li>
				<li>Next, compile the model to make it ready for training:<pre>model.compile(loss='categorical_crossentropy', 
              optimizer='adam',
              metrics = ['accuracy'])
And then perform the training of the model:
EPOCHS = 50
BATCH_SIZE = 128
 
model_details = model.fit_generator(custom_image_generator(train, batch_size = BATCH_SIZE),
                    steps_per_epoch = len(train) // BATCH_SIZE, 
                    epochs = EPOCHS, 
                    validation_data= validation_data,
                    verbose=1)</pre></li>
				<li>Evaluate the model and get the accuracy:<pre>score = model.evaluate(validation_data[0], validation_data[1])
print("Accuracy: {0:.2f}%".format(score[1]*100))</pre><p>The accuracy is as follows:</p></li>
			</ol>
			<div><div><img src="img/C13322_08_17.jpg" alt="Figure 8.17: Model accuracy&#13;&#10;" width="539" height="19"/>
				</div>
			</div>
			<h6>Figure 8.17: Model accuracy</h6>
			<h3 id="_idParaDest-235"><a id="_idTextAnchor256"/>Activity 22: Using Transfer Learning to Predict Images</h3>
			<p><strong class="bold">Solution</strong>:</p>
			<ol>
				<li value="1">First, set the random number seed so that the results are reproducible:<pre>from numpy.random import seed
seed(1)
from tensorflow import set_random_seed
set_random_seed(1)</pre></li>
				<li>Set <code>SIZE</code> and <code>CHANNELS</code><p><code>SIZE</code> is the dimension of the square image input. <code>CHANNELS</code> is the number of channels in the training data images. There are 3 channels in a RGB image.</p><pre>SIZE = 200
CHANNELS = 3</pre></li>
				<li>Create functions to get images and labels. Here <code>PATH</code> variable contains the path to the training dataset.<pre>from PIL import Image
def get_input(file):
    return Image.open(PATH+file)
def get_output(file):
    class_label = file.split('.')[0]
    if class_label == 'dog': label_vector = [1,0]
    elif class_label == 'cat': label_vector = [0,1]
    return label_vector</pre></li>
				<li>Create a function to preprocess and augment images:<pre>def preprocess_input(image):
    
    # Data preprocessing
    image = image.resize((SIZE,SIZE))
    image = np.array(image).reshape(SIZE,SIZE,CHANNELS)
    
    # Normalize image 
    image = image/255.0
    
    return image</pre></li>
				<li>Finally, create the generator that will generate the batches:<pre>import numpy as np
def custom_image_generator(images, batch_size = 128):
    
    while True:
        # Randomly select images for the batch
        batch_images = np.random.choice(images, size = batch_size)
        batch_input = []
        batch_output = [] 
        
        # Read image, perform preprocessing and get labels
        for file in batch_images:
            # Function that reads and returns the image
            input_image = get_input(file)
            # Function that gets the label of the image
            label = get_output(file)
            # Function that pre-processes and augments the image
            image = preprocess_input(input_image)
 
            batch_input.append(image)
            batch_output.append(label)
 
        batch_x = np.array(batch_input)
        batch_y = np.array(batch_output)
 
        # Return a tuple of (images,labels) to feed the network
        yield(batch_x, batch_y)</pre></li>
				<li>Next, we will read the development and test data. Create a function to read the images and their labels:<pre>from tqdm import tqdm
def get_data(files):
    data_image = []
    labels = []
    for image in tqdm(files):
        
        label_vector = get_output(image)
        
 
        img = Image.open(PATH + image)
        img = img.resize((SIZE,SIZE))
        
       
        labels.append(label_vector)
        img = np.asarray(img).reshape(SIZE,SIZE,CHANNELS)
        img = img/255.0
        data_image.append(img)
        
    data_x = np.array(data_image)
    data_y = np.array(labels)
        
    return (data_x, data_y)</pre></li>
				<li>Now read the development and test files. The split for the train/dev/test set is <code>70%/15%/15%</code>.<pre>import random
random.shuffle(files)
train = files[:7000]
development = files[7000:8500]
test = files[8500:]
development_data = get_data(development)
test_data = get_data(test)</pre></li>
				<li>Plot a few images from the dataset to see whether you loaded the files correctly:<pre>import matplotlib.pyplot as plt
plt.figure(figsize=(20,10))
columns = 5
for i in range(columns):
    plt.subplot(5 / columns + 1, columns, i + 1)
    plt.imshow(validation_data[0][i])</pre><p>Check the output in the following screenshot:</p><div><img src="img/C13322_08_18.jpg" alt="Figure 8.18: Sample images from the loaded dataset&#13;&#10;" width="1235" height="244"/></div><h6>Figure 8.18: Sample images from the loaded dataset</h6></li>
				<li>Load the Inception model and pass the shape of the input images:<pre>from keras.applications.inception_v3 import InceptionV3
base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(200,200,3))
10.  Add the output dense layer according to our problem:
from keras.models import Model
from keras.layers import GlobalAveragePooling2D, Dense, Dropout
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
keep_prob = 0.5
x = Dropout(rate = 1 - keep_prob)(x)
predictions = Dense(2, activation='softmax')(x)
 
model = Model(inputs=base_model.input, outputs=predictions)</pre></li>
				<li>This time around, we will freeze the first five layers of the model to help with the training time:<pre>for layer in base_model.layers[:5]:
    layer.trainable = False</pre></li>
				<li>Compile the model to make it ready for training:<pre>model.compile(loss='categorical_crossentropy', 
              optimizer='adam',
              metrics = ['accuracy'])</pre></li>
				<li>Create callbacks for Keras:<pre>from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard
callbacks = [
    TensorBoard(log_dir='./logs',
                update_freq='epoch'),
    EarlyStopping(monitor = "val_loss",
                 patience = 18,
                 verbose = 1,
                 min_delta = 0.001,
                 mode = "min"),
    ReduceLROnPlateau(monitor = "val_loss",
                     factor = 0.2,
                     patience = 8,
                     verbose = 1,
                     mode = "min"),
    ModelCheckpoint(monitor = "val_loss",
                   filepath = "Dogs-vs-Cats-InceptionV3-{epoch:02d}-{val_loss:.2f}.hdf5", 
                   save_best_only=True,
                   period = 1)]</pre><h4>Note</h4><pre>EPOCHS = 50
BATCH_SIZE = 128
model_details = model.fit_generator(custom_image_generator(train, batch_size = BATCH_SIZE),
                   steps_per_epoch = len(train) // BATCH_SIZE, 
                   epochs = EPOCHS, 
                   callbacks = callbacks,
                   validation_data= development_data,
                   verbose=1)</pre><p>The training logs on TensorBoard are shown here:</p><div><img src="img/C13322_08_19.jpg" alt="Figure 8.19: Training set logs from TensorBoard&#13;&#10;" width="652" height="593"/></div><h6>Figure 8.19: Training set logs from TensorBoard</h6></li>
				<li>You can now fine-tune the hyperparameters taking accuracy of the development set as the metric.<p>The logs of the development set from the TensorBoard tool are shown here:</p><div><img src="img/C13322_08_20.jpg" alt="Figure 8.20: Validation set logs from TensorBoard" width="610" height="596"/></div><h6>Figure 8.20: Validation set logs from TensorBoard</h6><p>The learning rate decrease can be observed from the following plot:</p><div><img src="img/C13322_08_21.jpg" alt="Figure 8.21: Learning rate log from TensorBoard&#13;&#10;" width="602" height="257"/></div><h6>Figure 8.21: Learning rate log from TensorBoard</h6></li>
				<li>Evaluate the model on the test set and get the accuracy:<pre>score = model.evaluate(test_data[0], test_data[1])
print("Accuracy: {0:.2f}%".format(score[1]*100))</pre><p>To understand fully, refer to the following output screenshot:</p></li>
			</ol>
			<div><div><img src="img/C13322_08_22.jpg" alt="Figure 8.22: The final accuracy of the model on the test set&#13;&#10;" width="635" height="30"/>
				</div>
			</div>
			<h6>Figure 8.22: The final accuracy of the model on the test set</h6>
			<p>As you can see, the model gets an accuracy of 93.6% on the test set, which is different from the accuracy of the development set (93.3% from the TensorBoard training logs). The early stopping callback stopped training when there wasn't a significant improvement in the loss of the development set; this helped us save some time. The learning rate was reduced after nine epochs, which helped training, as can be seen here:</p>
			<div><div><img src="img/C13322_08_23.jpg" alt="Figure 8.23: A snippet of the training logs of the model" width="1244" height="171"/>
				</div>
			</div>
			<h6>Figure 8.23: A snippet of the training logs of the model</h6>
		</div>
	</div>
<div><nav id="toc" epub:type="toc">
		<h2>Contents</h2>
			<ol>
				<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-1">Preface</a>
					<ol>
						<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-2">About the Book</a>
							<ol>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-3">About the Authors </a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-4">Learning Objectives</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-5">Audience</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-6">Approach</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-7">Minimum Hardware Requirements</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-8">Software Requirements </a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-9">Installation and Setup</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-10">Using Kaggle for Faster Experimentation</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-11">Conventions</a></li>
								<li><a href="C13322_Preface_Epub_Final_SW.xhtml#_idParaDest-12">Installing the Code Bundle</a></li>
							</ol>
						</li>
					</ol>
				</li>
				<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-13">Chapter 1</a></li>
				<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-14">Introduction to Data Science and Data Pre-Processing</a>
					<ol>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-15">Introduction</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-16">Python Libraries</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-17">Roadmap for Building Machine Learning Models</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-18">Data Representation</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-19">Independent and Target Variables</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-20">Exercise 1: Loading a Sample Dataset and Creating the Feature Matrix and Target Matrix</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-21">Data Cleaning </a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-22">Exercise 2: Removing Missing Data</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-23">Exercise 3: Imputing Missing Data</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-24">Exercise 4: Finding and Removing Outliers in Data</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-25">Data Integration</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-26">Exercise 5: Integrating Data</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-27">Data Transformation</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-28">Handling Categorical Data</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-29">Exercise 6: Simple Replacement of Categorical Data with a Number</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-30">Exercise 7: Converting Categorical Data to Numerical Data Using Label Encoding</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-31">Exercise 8: Converting Categorical Data to Numerical Data Using One-Hot Encoding</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-32">Data in Different Scales</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-33">Exercise 9: Implementing Scaling Using the Standard Scaler Method</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-34">Exercise 10: Implementing Scaling Using the MinMax Scaler Method</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-35">Data Discretization</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-36">Exercise 11: Discretization of Continuous Data </a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-37">Train and Test Data</a>
							<ol>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-38">Exercise 12: Splitting Data into Train and Test Sets</a></li>
								<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-39">Activity 1: Pre-Processing Using the Bank Marketing Subscription Dataset</a></li>
							</ol>
						</li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-40">Supervised Learning</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-41">Unsupervised Learning </a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-42">Reinforcement Learning</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-43">Performance Metrics</a></li>
						<li><a href="C13322_01_Epub_Final_SW.xhtml#_idParaDest-44">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-45">Chapter 2</a></li>
				<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-46">Data Visualization</a>
					<ol>
						<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-47">Introduction</a></li>
						<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-48">Functional Approach</a>
							<ol>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-49">Exercise 13: Functional Approach – Line Plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-50">Exercise 14: Functional Approach – Add a Second Line to the Line Plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-51">Activity 2: Line Plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-52">Exercise 15: Creating a Bar Plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-53">Activity 3: Bar Plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-54">Exercise 16: Functional Approach – Histogram</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-55">Exercise 17: Functional Approach – Box-and-Whisker plot</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-56">Exercise 18: Scatterplot</a></li>
							</ol>
						</li>
						<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-57">Object-Oriented Approach Using Subplots</a>
							<ol>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-58">Exercise 19: Single Line Plot using Subplots</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-59">Exercise 20: Multiple Line Plots Using Subplots</a></li>
								<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-60">Activity 4: Multiple Plot Types Using Subplots</a></li>
							</ol>
						</li>
						<li><a href="C13322_02_Epub_Final_SW.xhtml#_idParaDest-61">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-62">Chapter 3</a></li>
				<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-63">Introduction to Machine Learning via Scikit-Learn</a>
					<ol>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-64">Introduction</a></li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-65">Introduction to Linear and Logistic Regression</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-66">Simple Linear Regression</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-67">Exercise 21: Preparing Data for a Linear Regression Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-68">Exercise 22: Fitting a Simple Linear Regression Model and Determining the Intercept and Coefficient</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-69">Exercise 23: Generating Predictions and Evaluating the Performance of a Simple Linear Regression Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-70">Multiple Linear Regression</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-71">Exercise 24: Fitting a Multiple Linear Regression Model and Determining the Intercept and Coefficients</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-72">Activity 5: Generating Predictions and Evaluating the Performance of a Multiple Linear Regression Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-73">Logistic Regression</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-74">Exercise 25: Fitting a Logistic Regression Model and Determining the Intercept and Coefficients</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-75">Exercise 26: Generating Predictions and Evaluating the Performance of a Logistic Regression Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-76">Exercise 27: Tuning the Hyperparameters of a Multiple Logistic Regression Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-77">Activity 6: Generating Predictions and Evaluating Performance of a Tuned Logistic Regression Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-78">Max Margin Classification Using SVMs</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-79">Exercise 28: Preparing Data for the Support Vector Classifier (SVC) Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-80">Exercise 29: Tuning the SVC Model Using Grid Search</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-81">Activity 7: Generating Predictions and Evaluating the Performance of the SVC Grid Search Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-82">Decision Trees</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-83">Activity 8: Preparing Data for a Decision Tree Classifier</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-84">Exercise 30: Tuning a Decision Tree Classifier Using Grid Search</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-85">Exercise 31: Programmatically Extracting Tuned Hyperparameters from a Decision Tree Classifier Grid Search Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-86">Activity 9: Generating Predictions and Evaluating the Performance of a Decision Tree Classifier Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-87">Random Forests</a>
							<ol>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-88">Exercise 32: Preparing Data for a Random Forest Regressor</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-89">Activity 10: Tuning a Random Forest Regressor</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-90">Exercise 33: Programmatically Extracting Tuned Hyperparameters and Determining Feature Importance from a Random Forest Regressor Grid Search Model</a></li>
								<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-91">Activity 11: Generating Predictions and Evaluating the Performance of a Tuned Random Forest Regressor Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_03_Epub_Final_SW.xhtml#_idParaDest-92">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-93">Chapter 4</a></li>
				<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-94">Dimensionality Reduction and Unsupervised Learning</a>
					<ol>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-95">Introduction</a></li>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-96">Hierarchical Cluster Analysis (HCA)</a>
							<ol>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-97">Exercise 34: Building an HCA Model</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-98">Exercise 35: Plotting an HCA Model and Assigning Predictions</a></li>
							</ol>
						</li>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-99">K-means Clustering</a>
							<ol>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-100">Exercise 36: Fitting k-means Model and Assigning Predictions</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-101">Activity 12: Ensemble k-means Clustering and Calculating Predictions</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-102">Exercise 37: Calculating Mean Inertia by n_clusters</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-103">Exercise 38: Plotting Mean Inertia by n_clusters</a></li>
							</ol>
						</li>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-104">Principal Component Analysis (PCA)</a>
							<ol>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-105">Exercise 39: Fitting a PCA Model</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-106">Exercise 40: Choosing n_components using Threshold of Explained Variance</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-107">Activity 13: Evaluating Mean Inertia by Cluster after PCA Transformation</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-108">Exercise 41: Visual Comparison of Inertia by n_clusters</a></li>
							</ol>
						</li>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-109">Supervised Data Compression using Linear Discriminant Analysis (LDA)</a>
							<ol>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-110">Exercise 42: Fitting LDA Model</a></li>
								<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-111">Exercise 43: Using LDA Transformed Components in Classification Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_04_Epub_Final_SW.xhtml#_idParaDest-112">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-113">Chapter 5</a></li>
				<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-114">Mastering Structured Data</a>
					<ol>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-115">Introduction</a></li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-116">Boosting Algorithms</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-117">Gradient Boosting Machine (GBM)</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-118">XGBoost (Extreme Gradient Boosting)</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-119">Exercise 44: Using the XGBoost library to Perform Classification</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-120">XGBoost Library</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-121">Controlling Model Overfitting</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-122">Handling Imbalanced Datasets</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-123">Activity 14: Training and Predicting the Income of a Person</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-124">External Memory Usage</a></li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-125">Cross-validation</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-126">Exercise 45: Using Cross-validation to Find the Best Hyperparameters</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-127">Saving and Loading a Model</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-128">Exercise 46: Creating a Python Pcript that Predicts Based on Real-time Input</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-129">Activity 15: Predicting the Loss of Customers</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-130">Neural Networks</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-131">What Is a Neural Network?</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-132">Optimization Algorithms</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-133">Hyperparameters</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-134">Keras</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-135">Exercise 47: Installing the Keras library for Python and Using it to Perform Classification</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-136">Keras Library</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-137">Exercise 48: Predicting Avocado Price Using Neural Networks</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-138">Categorical Variables</a>
							<ol>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-139">One-hot Encoding</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-140">Entity Embedding</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-141">Exercise 49: Predicting Avocado Price Using Entity Embedding</a></li>
								<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-142">Activity 16: Predicting a Customer's Purchase Amount</a></li>
							</ol>
						</li>
						<li><a href="C13322_05_Epub_Final_SW.xhtml#_idParaDest-143">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-144">Chapter 6</a></li>
				<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-145">Decoding Images</a>
					<ol>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-146">Introduction</a></li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-147">Images</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-148">Exercise 50: Classify MNIST Using a Fully Connected Neural Network</a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-149">Convolutional Neural Networks</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-150">Convolutional Layer</a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-151">Pooling Layer</a></li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-152">Adam Optimizer</a></li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-153">Cross-entropy Loss</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-154">Exercise 51: Classify MNIST Using a CNN</a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-155">Regularization</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-156">Dropout Layer</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-157">L1 and L2 Regularization</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-158">Batch Normalization</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-159">Exercise 52: Improving Image Classification Using Regularization Using CIFAR-10 images </a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-160">Image Data Preprocessing</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-161">Normalization</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-162">Converting to Grayscale</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-163">Getting All Images to the Same Size</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-164">Other Useful Image Operations</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-165">Activity 17: Predict if an Image Is of a Cat or a Dog</a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-166">Data Augmentation</a></li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-167">Generators</a>
							<ol>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-168">Exercise 53: Classify CIFAR-10 Images with Image Augmentation</a></li>
								<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-169">Activity 18: Identifying and Augmenting an Image</a></li>
							</ol>
						</li>
						<li><a href="C13322_06_Epub_Final_SW.xhtml#_idParaDest-170">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-171">Chapter 7</a></li>
				<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-172">Processing Human Language</a>
					<ol>
						<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-173">Introduction</a></li>
						<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-174">Text Data Processing </a>
							<ol>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-175">Regular Expressions</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-176">Exercise 54: Using RegEx for String Cleaning </a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-177">Basic Feature Extraction</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-178">Text Preprocessing</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-179">Exercise 55: Preprocessing the IMDB Movie Review Dataset</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-180">Text Processing</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-181">Exercise 56: Creating Word Embeddings Using Gensim</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-182">Activity 19: Predicting Sentiments of Movie Reviews</a></li>
							</ol>
						</li>
						<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-183">Recurrent Neural Networks (RNNs)</a>
							<ol>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-184">LSTMs</a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-185">Exercise 57: Performing Sentiment Analysis Using LSTM </a></li>
								<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-186">Activity 20: Predicting Sentiments from Tweets</a></li>
							</ol>
						</li>
						<li><a href="C13322_07_Epub_Final_SW.xhtml#_idParaDest-187">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-188">Chapter 8</a></li>
				<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-189">Tips and Tricks of the Trade</a>
					<ol>
						<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-190">Introduction</a></li>
						<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-191">Transfer Learning</a>
							<ol>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-192">Transfer Learning for Image Data</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-193">Exercise 58: Using InceptionV3 to Compare and Classify Images</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-194">Activity 21: Classifying Images using InceptionV3</a></li>
							</ol>
						</li>
						<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-195">Useful Tools and Tips</a>
							<ol>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-196">Train, Development, and Test Datasets</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-197">Working with Unprocessed Datasets</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-198">pandas Profiling</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-199">TensorBoard</a></li>
							</ol>
						</li>
						<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-200">AutoML</a>
							<ol>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-201">Exercise 59: Get a Well-Performing Network Using Auto-Keras</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-202">Model Visualization Using Keras</a></li>
								<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-203">Activity 22: Using Transfer Learning to Predict Images</a></li>
							</ol>
						</li>
						<li><a href="C13322_08_Epub_Final_SW.xhtml#_idParaDest-204">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-205">Appendix</a>
					<ol>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-206">Chapter 1: Introduction to Data Science and Data Preprocessing</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-207">Activity 1: Pre-Processing Using the Bank Marketing Subscription Dataset</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-208">Chapter 2: Data Visualization</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-209">Activity 2: Line Plot</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-210">Activity 3: Bar Plot</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-211">Activity 4: Multiple Plot Types Using Subplots</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-212">Chapter 3: Introduction to Machine Learning via Scikit-Learn</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-213">Activity 5: Generating Predictions and Evaluating the Performance of a Multiple Linear Regression Model</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-214">Activity 6: Generating Predictions and Evaluating Performance of a Tuned Logistic Regression Model</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-215">Activity 7: Generating Predictions and Evaluating the Performance of the SVC Grid Search Model</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-216">Activity 8: Preparing Data for a Decision Tree Classifier</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-217">Activity 9: Generating Predictions and Evaluating the Performance of a Decision Tree Classifier Model</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-218">Activity 10: Tuning a Random Forest Regressor</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-219">Activity 11: Generating Predictions and Evaluating the Performance of a Tuned Random Forest Regressor Model</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-220">Chapter 4: Dimensionality Reduction and Unsupervised Learning</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-221">Activity 12: Ensemble k-means Clustering and Calculating Predictions</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-222">Activity 13: Evaluating Mean Inertia by Cluster after PCA Transformation</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-223">Chapter 5: Mastering Structured Data</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-224">Activity 14: Training and Predicting the Income of a Person</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-225">Activity 15: Predicting the Loss of Customers</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-226">Activity 16: Predicting a Customer's Purchase Amount</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-227">Chapter 6: Decoding Images</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-228">Activity 17: Predict if an Image Is of a Cat or a Dog</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-229">Activity 18: Identifying and Augmenting an Image</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-230">Chapter 7: Processing Human Language</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-231">Activity 19: Predicting Sentiments of Movie Reviews</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-232">Activity 20: Predicting Sentiments from Tweets</a></li>
							</ol>
						</li>
						<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-233">Chapter 8: Tips and Tricks of the Trade</a>
							<ol>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-234">Activity 21: Classifying Images using InceptionV3</a></li>
								<li><a href="C13322_Solution_Epub_Final_SW.xhtml#_idParaDest-235">Activity 22: Using Transfer Learning to Predict Images</a></li>
							</ol>
						</li>
					</ol>
				</li>
			</ol>
		</nav>
		<nav epub:type="landmarks">
		<h2>Landmarks</h2>
			<ol>
				<li><a epub:type="cover" href="Images/cover.xhtml">Cover</a></li>
				<li><a epub:type="toc" href="C13322_FM_Epub_Final_SW.xhtml#_idContainer004">Table of Contents</a></li>
			</ol>
		</nav>
	</div></body></html>