- en: Chapter 3. Data Analysis – Explore and Wrangle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing univariate data graphically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping the data and using dot plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using scatter plots for multivariate data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using heat maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing summary statistics and plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a box-and-whisker plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing random sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardizing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming the words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing word lemmatization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing the text as a bag of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating term frequencies and inverse document frequencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you venture into any data science application, it is always helpful in
    the long run to have a good understanding of the data that you are about to process.
    An understanding of the underlying data will help you choose the right algorithm
    to use for the problem at hand. Exploring the data at various levels of granularity
    is called **Exploratory Data Analysis** (**EDA**). In many cases, **EDA** can
    uncover patterns that are typically revealed by a data mining algorithm **EDA**
    helps us understand data characteristics and provides you with the proper guidance
    in order to choose the right algorithm for the given problem.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover **EDA** in detail. We will look into the practical
    techniques and tools that are used to perform **EDA** operations in an effective
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing and transformation are two other important processes that
    can improve the quality of data science models and increase the success rate of
    data science projects.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing is the process of making the data ready in order to be ingested
    either by a data mining method or machine learning algorithm. It encompasses many
    things such as data cleaning, attribute subset selection, data transformation,
    and others. We will cover both numerical data preprocessing and text data preprocessing
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Text data is a different beast than the numerical data. We need different transformation
    methods in order to make it suitable for ingestion in the machine learning algorithms.
    In this chapter, we will see how we can transform the text data. Typically, text
    transformation is a staged process with various components in the form of a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop word removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base form conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature derivation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, these components are applied to a given text in order to extract
    features. At the end of the pipeline, the text data is transformed in a way that
    it can be fed as input to the machine learning algorithms. In this chapter, we
    will see recipes for every component listed in the preceding pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Many times, a lot of errors may be introduced during the data collection phase.
    These may be due to human errors, limitations, or bugs in the data measuring or
    collective process/device. Data inconsistency is a big challenge. We will start
    our data preprocessing journey with data imputation is a way to handle errors
    in the incoming data and then proceed to other methods.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing univariate data graphically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Datasets with only one variable/column are called univariate data. Univariate
    is a general term in mathematics, which refers to any expression, equation, function,
    or polynomial with only one variable. In our case, we will restrict the univariate
    function to datasets. Let''s say that we will measure the heights of a group of
    people in meters; the data will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Our measurement is only about a single attribute of people, height. This is
    an example of univariate data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start our **EDA** recipe by looking at a sample univariate dataset through
    visualization. It is easy to analyze the data characteristics through the right
    visualization techniques. We will use `pyplot` to draw graphs in order to visualize
    the data. Pyplot is the state-machine interface to the matplotlib plotting library.
    Figures and axes are implicitly and automatically created to achieve the desired
    plot. The following link is a good reference for `pyplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://matplotlib.org/users/pyplot_tutorial.html](http://matplotlib.org/users/pyplot_tutorial.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will use a number of Presidential Requests of Congress
    in State of the Union Address. The following URL contains the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.presidency.ucsb.edu/data/sourequests.php](http://www.presidency.ucsb.edu/data/sourequests.php)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will visually look at this data and identify any outliers present in the
    data. We will follow a recursive approach with respect to the outliers. Once we
    have identified the outliers, we will remove them from the dataset and plot the
    remaining data in order to find any new outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recursively looking into the data after removing the perceived outlier in every
    iteration is a common approach in detection of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will load the data using NumPy's data loading utility. Then, we will address
    the data quality issues; in this case, we will address how to handle the null
    values. As you can see in the data, the years 1956 and 1958 have null entries.
    Let's replace the null values by `0` using the lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this, let''s plot the data to look for any trends:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s calculate the percentile values and plot them as references in the plot
    that has been generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s inspect the data visually for outliers and then remove them
    using the mask function. Let''s plot the data again without the outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the first step, we will put some data loading techniques that we learnt
    in the previous chapter to action. You will have noticed that the years `1956`
    and `1958` are left blank. We will replace them with `0` using an anonymous function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fill_data` lambda function will replace any null value in the dataset;
    in this case, line no 11 and 13 with 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will pass `fill_data` to the `genfromtxt` function''s `converters` parameter.
    Note that `converters` takes a dictionary as its input. The key in the dictionary
    dictates which column our function should be applied to. The value indicates the
    function. In this case, we specified `fill_data` as the function and set the key
    to 1 indicating that the `fill_data` function has to be applied to column 1\.
    Now let''s look at the data in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the years `1956` and `1958` have a `0` value added to them.
    For the ease of plotting, we will load the year data in x and the number of Presidential
    Requests to Congress in the State of Union Address to y:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, in the first column, the year is loaded in `x` and the next
    column in `y`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 2, we will plot the data with the x axis as the year and y axis representing
    the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We will first close any previous graphs that are open from the previous programs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will give a number to our plot. This is very useful when we have a lot of
    graphs in a program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will specify a title for our plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will plot x and y. The ''ro'' parameter tells plyplot to plot x
    and y as dots (0) in the color red (r):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the *x* and *y* axes labels are provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A casual look at this graph shows that the data is spread everywhere and no
    trends or patterns can be found in the first glance. However, with a keen eye,
    you can notice three points: one point at the top on the right-hand side and others
    to the immediate left of **1960** in the *x* axis. They are starkly different
    from all the other points in the sample, and hence, they are outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An outlier is an observation that lies outside the overall pattern of a distribution
    (Moore and McCabe 1999).
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand these points further, we will take the help of percentiles.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we have a vector V of length N, the qth percentile of V is the qth ranked
    value in a sorted copy of V. The values and distances of the two nearest neighbors
    as well as the *interpolation* parameter will determine the percentile if the
    normalized ranking does not match q exactly. This function is the same as the
    median if`q=50`, the same as the minimum if `q=0`, and the same as the maximum
    if `q=100`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to [http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html](http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Why don't we use averages? We will look into averages in the summary statistics
    section; however, looking at the percentiles has its own advantages. Average values
    are typically skewed by outliers; outliers such as the one at the top on the right-hand
    side can drag the average to a higher value and the outliers near 1960 can do
    the opposite. Percentiles give us a better clarity about the range of values in
    our dataset. We can calculate the percentiles using NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we will calculate the percentiles and print them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The percentile values calculated and printed for this dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Interpreting the percentiles:'
  prefs: []
  type: TYPE_NORMAL
- en: 25% of the points in the dataset are below 13.00 (25th percentile value).
  prefs: []
  type: TYPE_NORMAL
- en: 50% of the points in the dataset are below 18.50 (50th percentile value).
  prefs: []
  type: TYPE_NORMAL
- en: 75% of the points in the dataset are below 25.25 (75th percentile value).
  prefs: []
  type: TYPE_NORMAL
- en: A point to note is that the 50th percentile is the median. Percentiles give
    us a good idea of the range of our values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 4, we will plot these percentile values as horizontal lines in our
    graph in order to enhance our visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We used the `plt.axhline()` function to draw these horizontal lines. This function
    will draw a line at the given y value from the minimum of x to the maximum of
    x. Using the label parameter, we gave it a name and set the color of the line
    through the c parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A good way to understand any function is to pass the function name to `help()`
    in the Python console. In this case, help (plt.axhline) in the Python console
    will give you the details.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will place the legend using `plt.legend()`, and using the `loc`
    parameter, ask pyplot to determine the best location to put the legend so that
    it does not affect the plot readability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our graph is now as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In step 5, we will move on to remove the outliers using the mask function in
    NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Masking is a convenient way to hide some of the values without removing them
    from our array. We used the `ma.masked_where` function, where we passed a condition
    and an array. The function then masks the values in the array that meet the condition.
    Our first condition was to mask all the points in the `y` array, where the array
    value was `0`. We stored the new masked array as `y_masked`. Then, we applied
    another condition on `y_masked` to remove point 54.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in step 6, we will repeat the plotting steps. Our final plot looks
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Creating Anonymous functions* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pre-processing columns* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Acquiring data with Python* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Outliers* recipe in [Chapter 4](ch04.xhtml "Chapter 4. Data Analysis – Deep
    Dive"), *Analyzing Data - Deep Dive*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grouping the data and using dot plots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**EDA** is about zooming in and out of the data from multiple angles in order
    to get a better grasp of the data. Let''s now see the data from a different angle
    using dot plots. A dot plot is a simple plot where the data is grouped and plotted
    in a simple scale. It''s up to us to decide how we want to group the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dot plots are best used for small-sized to medium-sized datasets. For large-sized
    data, a histogram is usually used.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this exercise, we will use the same data as the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s load the necessary libraries. We will follow it up with the loading
    of our data and along the way, we will handle the missing values. Finally, we
    will group the data using a frequency counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We will proceed to group the data by the year range and plot it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will prepare the data for a simple dot plot and proceed with plotting
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In step 1, we will load the data. This is the same as the data loading discussed
    in the previous recipe. Before we start plotting the data, we want to group them
    in order to see the overall data characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: In steps 2 and 3, we will group the data using different criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at step 2.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will use a function called `Counter()` from the `collections` package.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a set of points, `Counter()` returns a dictionary where key is a data
    point and value is the frequency of the data points in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We will pass our dataset to `Counter()` and extract the keys from the actual
    data point and values, the respective frequency from this dictionary into numpy
    arrays `x_` and `y_` for ease of plotting. Thus, we have now grouped our data
    using frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to plot this, we will perform another grouping with this data
    in step 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the x axis is years. Our data is also sorted by the year in an
    ascending order. In this step, we will group our data in a range of years, five
    in this case; that is, let''s say that we will make a group from the first five
    years, our second group is the next five years, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `group` variable defines how many years we want in a single group; in this
    example, we have 5 groups and `keys` and `values` are two empty lists. We will
    proceed to fill them with values from `x` and `y` till `group_count` reaches `group`,
    that is, `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `x_group` is the name of the dictionary that now stores the group of values.
    We will need to preserve the order in which we will insert our records and so,
    we will use `OrderedDict` in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`OrderedDict` preserves the order in which the keys are inserted.'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's proceed to plot these values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to plot all our graphs in a single window; hence, we will use the `subplot`
    parameter to the subplot, which defines the number of rows (3, the number in the
    hundredth place), number of columns (1, the number in the tenth place), and finally
    the plot number (1 in the unit place). Our plot output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the top graph, the data is grouped by frequency. Here, our x axis is the
    count and y axis is the number of Presidential Requests. We can see that 30 or
    more Presidential Requests have occurred only once. As said before, the dot plot
    is good at analyzing the range of the data points under different groupings.
  prefs: []
  type: TYPE_NORMAL
- en: The middle graph can be viewed as a very simple histogram. As the title of the
    graph (`in plt.title()`) says, it's the simplest form of a dot plot, where the
    *x* axis is the actual values and y axis is the number of times this x value occurs
    in the dataset. In a histogram, the bin size has to be set carefully; if not,
    it can distort the complete picture about the data. However, this can be avoided
    in this simple dot plot.
  prefs: []
  type: TYPE_NORMAL
- en: In the bottom graph, we have grouped the data by years.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Creating Anonymous functions* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pre-processing columns* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Acquiring data with Python* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using Dictionary objects* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using scatter plots for multivariate data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From a single column, we will now move on to multiple columns. In multivariate
    data analysis, we are interested in seeing if there any relationships between
    the columns that we are analyzing. In two column/variable cases, the best place
    to start is a standard scatter plot. There can be four types of relationships,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: No relationship
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strong
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multivariate (not simple) relationship
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the Iris dataset. It's a multivariate dataset introduced by Sir
    Ronald Fisher. Refer to [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: The Iris dataset has 150 instances and four attributes/columns. The 150 instances
    are composed of 50 records from each of the three species of the Iris flower (Setosa,
    virginica, and versicolor). The four attributes are the sepal length in cm, sepal
    width in cm, petal length in cm, and petal width in cm. Thus, the Iris dataset
    also serves as a great classification dataset. A classification method can be
    written in such a way that, given a record, we can classify which species that
    record belongs to after appropriate training.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s load the necessary libraries and extract the Iris data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We will proceed with demonstrating with a scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The scikit library provides a convenient function to load the Iris dataset
    called `load_iris()`. We will use this to load the Iris data in the variable data
    in step 1\. The `data` is a dictionary object. Using the data and target keys,
    we will retrieve the records and class labels. We will look at the `x` and `y`
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, `x` is a matrix with `150` rows and four columns; `y` is a
    vector of length `150`. The `data` dictionary can also be queried to view the
    column names using the `feature_names` keyword, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then create a scatter plot of the iris variables in step 2\. As we
    did before, we will use subplot here to accommodate all the plots in a single
    figure. We will get two combinations of our column using `itertools.Combination`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can iterate `col_pairs` to get two combinations of our column and plot a
    scatter plot for each, as you can see in the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We will pass a `c` parameter in order to indicate the color of the points. In
    this case, we will pass our y variable (class label) so that the different species
    of iris are plotted in different colors in our scatter plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have plotted two combinations of our columns. We also have
    the class labels represented using three different colors. Let's look at the bottom
    left plot, petal length versus petal width. We see that different range of values
    belong to different class labels. Now, this gives us a great clue for classification;
    the petal width and length variables are good candidates if the problem in hand
    is classification.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the Iris dataset, the petal width and length can alone classify the records
    in their respective flower family.
  prefs: []
  type: TYPE_NORMAL
- en: These kinds of observations can be quickly made during the feature selection
    process with the help of bivariate scatter plots.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Using iterables* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python for Data
    Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Working with itertools* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using heat maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Heat maps are another interesting visualization technique. In a heat map, the
    data is represented as a matrix where the range of values taken by attributes
    are represented as color gradients. Look at the following Wikipedia reference
    for a general introduction to heat maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://en.wikipedia.org/wiki/Heat_map](http://en.wikipedia.org/wiki/Heat_map)'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will again resort to the Iris dataset in order to demonstrate how to build
    a heat map. We will also see the various ways that heat maps can be used on this
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how we can represent the whole data as a heat map
    and how the various interpretations of the data can be made from the heat map.
    Let's proceed to build a heat map of the Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s load the necessary libraries and import the Iris dataset. We will proceed
    with scaling the variables in the data by their mean value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot our heat map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In step 1, we will load the Iris dataset. Similar to the other recipes, we
    will take the data dictionary objects and store them as x and y for clarity. In
    step 2, we will scale the variables by their means:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: With the parameter standard set to false, the scale function will use only the
    mean of the columns in order to normalize the data.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for the scaling is to adjust the range of values that each column
    takes to a common scale, typically between 0 and 1\. Having them in the same scale
    is very important for the heat map visualization as the values decide the color
    gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Don't forget to scale your variables to bring them to the same range. Not having
    a proper scaling may lead to variables with a bigger range and scale, thus dominating
    others.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 3, we will perform the actual plotting. Before we plot, we will subset
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we selected only the first 25 records from the dataset. We
    did so in order to have the labels in the y axis to be readable. We will store
    the labels for the x and y axes in `col_names` and `y_labels`, respectively. Finally,
    we will use the `pcolor` function from pyplot to plot a heat map of the Iris data.
    We will do a little more tinkering with pcolor to make it look nice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The *x* and *y* axis ticks are set uniformly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The x axis ticks are displayed at the top of the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The y axis ticks are displayed to the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will pass on the label values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output plot is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another interesting way to use a heat map is to view the variables separated
    by their respective classes; for example, in the Iris dataset, we will plot three
    different heat maps for the three classes that are present. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/B04041_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first 50 records belong to the `setosa` class, the next 50 to `versicolor`,
    and the last 50 belong to `virginica`. We will make three heat maps for each of
    these classes.
  prefs: []
  type: TYPE_NORMAL
- en: The cells are filled with the actual values of the records. You can notice that,
    for `setosa`, the sepal width has a good variation but doesn't show any significance
    in the case of `versicolor` and `virginica`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Scaling the data* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data Analysis
    – Explore and Wrangle"), *Analyzing Data - Explore & Wrangle*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing summary statistics and plots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary purpose of using summary statistics is to get a good understanding
    of the location and dispersion of the data. By summary statistics, we refer to
    mean, median, and standard deviation. These quantities are quite easy to calculate.
    However, one should be careful when using them. If the underlying data is not
    unimodal, that is, it has multiple peaks, these quantities may not be of much
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the given data is unimodal, that is, having only one peak, the mean, which
    gives the location, and standard deviation, which gives the variance, are valuable
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's use our Iris dataset to explore some of these summary statistics. In this
    section, we don't have a wholesome program producing a single output; however,
    we will have different steps demonstrating different summary measures.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s begin by importing the necessary libraries. We will follow it up with
    the loading of the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now demonstrate how to calculate the mean, trimmed mean, and range values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will show the variance, standard deviation, mean absolute deviation,
    and median absolute deviation calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loading of the Iris dataset is not repeated in this recipe. It's assumed
    that the reader can look at the previous recipe to do the same. Further, we will
    assume that the x variable is loaded with all the instance of the Iris records
    with each record having four columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1 prints the mean value of each of the column in the Iris dataset. We
    used NumPy''s `mean` function for the same. The output of the print statement
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we have the mean value for each column. The code to calculate
    the mean is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We passed all the rows and columns in the loop. Thus, we get the mean value
    by columns.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting measure is what is called trimmed mean. It has its own advantages.
    The 10% trimmed mean of a given sample is computed by excluding the 10% largest
    and 10% smallest values from the sample and taking the arithmetic mean of the
    remaining 80% of the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to the regular mean, a trimmed mean is less sensitive to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'SciPy provides us with a trim mean function. We will demonstrate the trimmed
    mean calculation in step 2\. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With the Iris dataset, we don't see a lot of difference, but in real-world datasets,
    the trimmed mean is very handy as it gives a better picture of the location of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Till now, what we saw was the location of the data and that the mean and trimmed
    mean gives a good inference on the data location. Another important aspect to
    look at is the dispersion of the data. The simplest way to look at the data dispersion
    is range, which is defined as follows, given a set of values, x, the range is
    the maximum value of x – minimum value of x. In Step 3, we will calculate and
    print the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the data falls in a very narrow range, say, most of the values cluster around
    a single value and we have a few extreme values, then the range may be misleading.
  prefs: []
  type: TYPE_NORMAL
- en: When the data falls in a very narrow range and clusters around a single value,
    variance is used as a typical measure of the dispersion/spread of the data. Variance
    is the sum of the squared difference between the individual values and the mean
    value divided by the number of instances. In step 4, we will see the variance
    calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, in addition to variance, we can see std-dev, that is,
    standard deviation. As variance is the square of the difference, it''s not in
    the same measurement scale as the original data. We will use standard deviation,
    which is the square root of the variance, in order to get the data back into its
    original scale. Let''s look at the output of the print statement, where we listed
    both the variance and standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we mentioned earlier, the mean is very sensitive to outliers; variance also
    uses the mean, and hence, it''s prone to the same issues as the mean. We can use
    other measures for variance to avoid this trap. One such measure is absolute average
    deviation; instead of taking the square of the difference between the individual
    values and mean and dividing it by the number of instances, we will take the absolute
    of the difference between the mean and individual values and divide it by the
    number of instances. In step 5, we will define a function for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the function returns the absolute difference between the mean
    and individual values. The output of this step is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With the data having many outliers, there is another set of metrics that come
    in handy. They are the median and percentiles. We already saw percentiles in the
    previous section while plotting the univariate data. Traditionally, median is
    defined as a value from the dataset such that half of all the points in the dataset
    are smaller and the other half is larger than the median value.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Percentiles are a generalization of the concept of median. The 50th percentile
    is the traditional median value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw the 25th and 75th percentiles in the previous section. The 25th percentile
    is a value such that 25% of all the points in the dataset are smaller than this
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The median is the measure of the location of the data distribution. Using percentiles,
    we can get a metric for the dispersion of the data, the interquartile range. The
    interquartile range is the distance between the 75th percentile and 25th percentile.
    Similar to the mean absolute deviation as explained previously, we also have the
    median absolute deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 6, we will calculate and display both the interquartile range and median
    absolute deviation. We will define the following function in order to calculate
    the median absolute deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Grouping Data and Using Plots* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Analyzing Data - Explore & Wrangle*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a box-and-whisker plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A box-and-whisker plot is a good companion with the summary statistics to view
    the statistical summary of the data in hand. Box-and-whiskers can effectively
    represent quantiles in data and also outliers, if any, emphasizing the overall
    structure of the data. A box plot consists of the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: A horizontal line indicating the median that indicates the location of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A box spanning the interquartile range, measuring the dispersion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of whiskers that extends from the central box horizontally and vertically,
    which indicates the tail of the distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's use the box plot to look at the Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s load the necessary libraries to begin with. We will follow this with
    loading the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s demonstrate how to create a box-and-whisker plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code is very straightforward. We will load the Iris data in x and pass
    the x values to the box plot function from pyplot. As you know, our x has four
    columns. The box plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The box plot has captured both the location and variation of all the four columns
    in a single plot.
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal red line indicates the median, which is the location of the data.
    You can see that the sepal length has a higher median than the rest of the columns.
  prefs: []
  type: TYPE_NORMAL
- en: The box spanning the interquartile range measuring the dispersion can be seen
    for all the four variables.
  prefs: []
  type: TYPE_NORMAL
- en: You can see a set of whiskers that extends from the central box horizontally
    and vertically, which indicates the tail of the distribution. Whiskers help you
    to see the extreme values in the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It will also be interesting to see how the data is distributed across the various
    class labels. Similar to how we did in the scatter plots, let''s do the same with
    the box-and-whisker plot. The following code and chart explains how to plot a
    box plot across various class labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the following chart, we now have a box-and-whisker plot for
    each class label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more…](img/B04041_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Imputing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many real-world scenarios, we have the problem of incomplete or missing data.
    We need a strategy to handle the incomplete data. This strategy can be formulated
    either using the data alone or in conjunction with the class labels, if the labels
    are present.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's first look at the ways of imputing the data without using the class labels.
  prefs: []
  type: TYPE_NORMAL
- en: A simple technique is to ignore the missing value and hence, avoid the overhead
    of data imputation. However, this can be applied when the data is available in
    abundance, which is not always the case. If the dataset has very few missing values
    and the percentage of the missing values is minimal, we can ignore them. Typically,
    it's not about ignoring a single value of a variable, it's about ignoring a tuple
    that contains this variable. We have to be more careful when ignoring a whole
    tuple, as the other attributes in this tuple may be very critical for our task.
  prefs: []
  type: TYPE_NORMAL
- en: A better way to handle the missing data is to estimate it. Now, the estimation
    process can be carried out considering only the data or in conjunction with the
    class label. In the case of a continuous variable, the mean, median, or the most
    frequent value can be used to replace the missing value. Scikit-learn provides
    you with an `Imputer()` function in module preprocessing to handle the missing
    data. Let's see an example where we will perform data imputation. To better understand
    the imputation technique, we will artificially introduce some missing values in
    the Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s load the necessary libraries to begin with. We will load the Iris dataset
    as usual and introduce some arbitrary missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see some data imputation in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Step 1 is about loading the Iris data in memory. In step 2, we will introduce
    some missing values; in this case, we will set all the columns in the third row
    to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 3, we will use the Imputer object to handle the missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we will need two parameters, `missing_valu`es to specify the
    missing values, and strategy, which is a way to impute these missing values. The
    Imputer object provides the following three strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: median
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: most_frequent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the mean, any cell with the `0` value will be replaced by the mean value
    of the column that the cell belongs to. In the case of the median, the median
    value is used to replace `0`, and in `most_frequent`, as the name suggests, the
    most frequent value is used to replace `0`. Based on the context of our application,
    one of these strategies can be applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'The intial value of x[2,:] is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We will make it `0` in all the columns and use an imputer with the mean strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at the imputer output, let''s calculate the mean values for
    all the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the imputed output for row number 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the imputer has filled the missing values with the mean value
    of the respective columns.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed, we can also leverage the class labels and impute the missing
    values either using the mean or median:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of using the mean or median of the whole dataset, what we did was to
    subset the data by the class variable of the missing tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We introduced the missing value in the third record. We will take the class
    label associated with this record to the `missing_y` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will take all the tuples that have the same class label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We can now apply the mean or median strategy by replacing the missing tuple
    with the mean or median of all the tuples that belong to this class label.
  prefs: []
  type: TYPE_NORMAL
- en: We took the mean/median value of this subset for the data imputation process.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Performing Summary Statistics* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Analyzing Data - Explore & Wrangle*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing random sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this we will learn to how to perform a random sampling of data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically, in scenarios where it's very expensive to access the whole dataset,
    sampling can be effectively used to extract a portion of the dataset for analysis.
    Sampling can be effectively used in EDA as well. A sample should be a good representative
    of the underlying dataset. It should have approximately the same characteristics
    as the underlying dataset. For example, with respect to the mean, the sample mean
    should be as close to the original data's mean value as possible. There are several
    sampling techniques; we will cover one of them here.
  prefs: []
  type: TYPE_NORMAL
- en: In simple random sampling, there is an equal chance of selecting any tuple.
    For our example, we want to sample ten records randomly from the Iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will begin with loading the necessary libraries and importing the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s demonstrate how sampling is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In step 1, we will load the Iris dataset. In step 2, we will do a random selection
    using the `choice` function from `numpy.random`.
  prefs: []
  type: TYPE_NORMAL
- en: The two parameters that we will pass to the choice functions are a range variable
    for the total number of rows in the original dataset and the sample size that
    we require. From zero to the total number of rows in the original dataset, the
    choice function randomly picks n integers, where n is the size of the sample,
    which is dictated by `no_records` in our case.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect is that one of the parameters to the choice function
    is `replace` and it's set to True by default; it specifies whether we need to
    sample with replacement or without replacement. Sampling without replacement removes
    the sampled item from the original list so it will not be a candidate for future
    sampling. Sampling with replacement does the opposite; every element has an equal
    chance to be sampled in future sampling even though it's been sampled before.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stratified sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the underlying dataset consists of different groups, a simple random sampling
    may fail to capture adequate samples in order to be able to represent the data.
    For example, in a two-class classification problem, 10% of the data belongs to
    the positive class and 90% belongs to the negative class. This kind of problem
    is called class imbalance problem in machine learning. When we do sampling on
    such imbalanced datasets, the sample should also reflect the preceding percentages.
    This kind of sampling is called stratified sampling. We will look more into stratified
    sampling in future chapters on machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Progressive sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How do we determine the correct sample size that we need for a given problem?
    We discussed several sampling techniques before but we don't have a strategy to
    select the correct sample size. There is no simple answer for this. One way to
    do this is to use progressive sampling. Select a sample size and get the samples
    through any of the sampling techniques, apply the desired operation on the data,
    and record the results. Now, increase the sample size and repeat the steps. This
    iterative process is called progressive sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this we will learn to how to scale the data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scaling is an important type of data transformation. Typically, by doing scaling
    on a dataset, we can control the range of values that the data type can assume.
    In a dataset with multiple columns, the columns with a bigger range and scale
    tend to dominate other columns. We will perform scaling of the dataset in order
    to avoid these interferences.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we are comparing two software products based on the number of
    features and the number of lines of code. The difference in the number of lines
    of code will be very high compared to the difference in the number of features.
    In this case, our comparison will be dominated by the number of lines of code.
    If we use any similarity measure, the similarity or difference will be dominated
    by the number of lines of code. To avoid such a situation, we will adopt scaling.
    The simplest scaling is min-max scaling. Let's look at min-max scaling on a randomly
    generated dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s generate some random data in order to test our scaling functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will demonstrate scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In step 1, we will generate a list of random numbers between 10 and 25\. In
    step 2, we will define a function to perform min-max scaling on the given input.
    Min-max scaling is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: In step 2 we define a function to do the above task.
  prefs: []
  type: TYPE_NORMAL
- en: This transforms the range of the given value. After transformation, the values
    will fall in the [ 0,1 ] range.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 3, we will first print the original input list. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We will pass this list to our `min_max` function in order to get the scaled
    output, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: You can see the scaling in action; `10`, which is the smallest number, has been
    assigned a value of `0.0` and `23`, the highest number, is assigned a value of
    `1.0`. Thus, we scaled the data in the [0,1] range.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-learn provides a MinMaxScaler function for the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We saw examples where we scaled the data to a range (0,1); this can be extended
    to any range. Let''s say that our new range is `nr_min,nr_max`, then the min-max
    formula is modified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The following will be the Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'where, range_values is a tuple of two elements, where the 0th element is the
    new range''s lower end and the first element is the higher end. Let''s invoke
    this function on our input and see how the output is, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The lowest value, `10`, is now scaled to `100` and the highest value, `23`,
    is scaled to `200`.
  prefs: []
  type: TYPE_NORMAL
- en: Standardizing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standardization is the process of converting the input so that it has a mean
    of `0` and standard deviation of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are given a vector X, the mean of `0` and standard deviation of 1 for
    X can be achieved by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Standardized X = x– mean(value) / standard deviation (X)
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how this can be achieved in Python.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s import the necessary libraries to begin with. We will follow this with
    the generation of the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to demonstrate standardization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will generate some random data using np.random:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'We will perform standardization using the `scale` function from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: The `x_centered` is scaled using only the mean; you can see the `with_mean`
    parameter set to `True` and `with_std` set to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: The `x_standard` is standardized using both mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us look at the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Standardization can be generalized to any level and spread, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Standardized value = value – level / spread
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break the preceding equation in two parts: just the numerator part,
    which is called centering, and the whole equation, which is called standardization.
    Using the mean values, centering plays a critical role in regression. Consider
    a dataset that has two attributes, weight and height. We will center the data
    such that the predictor, weight, has a mean of `0`. This makes the interpretation
    of intercept easier. The intercept will be interpreted as what is the expected
    height when the predictor values are set to their mean.'
  prefs: []
  type: TYPE_NORMAL
- en: Performing tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you are given any text, the first job is to tokenize the text into a format
    that is based on the given problem requirements. Tokenization is a very broad
    term; we can tokenize the text at the following various levels of granularity:'
  prefs: []
  type: TYPE_NORMAL
- en: The paragraph level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sentence level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will see sentence level and word level tokenization. The
    methods are similar and can be easily applied to a paragraph level or any other
    level of granularity as required by the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will see how to perform sentence level and word level tokenization in a single
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start with the demonstration of sentence tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'A quick peek at how NLTK performs its sentence tokenization in the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In step 1, we will initialize a variable sentence with a paragraph. This is
    the same example that we used in the dictionary recipe. In step 2, we will use
    nltk's `sent_tokenize` function to extract sentences from the given text.You can
    look into the source of `sent_tokenize` in nltk in the documentation found at
    [http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize](http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, `sent_tokenize` loads a prebuilt tokenizer model, and using
    this model, it tokenizes the given text and returns the output. The tokenizer
    model is an instance of PunktSentenceTokenizer from the `nltk.tokenize.punkt`
    module. There are several pretrained instances of this tokenizer available in
    different languages. In our case, you can see that the language parameter is set
    to English.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the output of this step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the sentence tokenizer has split our input text into three
    sentences. Let''s proceed to step 3, where we will tokenize these sentences into
    words. Here, we will use the `word_tokenize` function in order to extract the
    words from each of the sentences and store them in a dictionary, where the key
    is the sentence number and the value is the list of words for that sentence. Let''s
    look at the output of the print statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: The `word_tokenize` uses a regular expression to split the sentences into words.
    It will be useful to look at the source of `word_tokenize` found at [http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize](http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize).
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For sentence tokenization, we saw a way of doing it in NLTK. There are other
    methods available. The `nltk.tokenize.simple` module has a `line_tokenize` method.
    Let''s take the same input sentence as before and run it using `line_tokenize`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we have only the sentence retrieved from the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now modify our input in order to include new line characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we have a new line character added. We will again apply `line_tokenize`
    to get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: You can see that it has tokenized our sentences at the new line and now we have
    three sentences.
  prefs: []
  type: TYPE_NORMAL
- en: See *Chapter 3* of the *NLTK* book; it has more references for sentence and
    word tokenization. It can be found at [http://www.nltk.org/book/ch03.html](http://www.nltk.org/book/ch03.html).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Using Dictionary object* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python
    for Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Writing list* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python for Data
    Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In text processing, we are interested in words or phrases that will help us
    differentiate the given text from the other text in the corpus. Let's call these
    words or phrases as key phrases. Every text mining application needs a way to
    find out the key phrases. An information retrieval application needs key phrases
    for the easy retrieval and ranking of search results. A text classification system
    needs key phrases as its features that are to be fed to a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: This is where stop words come into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: '*"Sometimes, some extremely common words which would appear to be of little
    value in helping select documents matching a user need are excluded from the vocabulary
    entirely. These words are called stop words."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Introduction to Information Retrieval By Christopher D. Manning, Prabhakar
    Raghavan, and Hinrich Schütze*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python NLTK library provides us with a default stop word corpus that we
    can leverage, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we have printed the list of stop words in English.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s load the necessary library and introduce our input text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now demonstrate the stop words removal process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In step 1, we will import the necessary libraries from nltk. We will need the
    list of English stop words, so we will import the stop word corpus. We will need
    to tokenize our input text into words. For this, we will import the `word_tokenize`
    function from the `nltk.tokenize` module.
  prefs: []
  type: TYPE_NORMAL
- en: For our input text, we took the introduction paragraph from Wikipedia on text
    mining, which can be found at [http://en.wikipedia.org/wiki/Text_mining](http://en.wikipedia.org/wiki/Text_mining).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will tokenize the input text into words using the word_tokenize
    function. The words is now a list of all the words tokenized from the input. Let''s
    look at the output of the print function, where we will print the length of the
    words list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: We have a total of 259 words in our list.
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, we will compile a list of the English stop words in a list called
    `stop_words`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 2, we will use a list comprehension to get a final list of the words;
    only those words that are not in the stop word list that we created in step 2\.
    This way, we can remove the stop words from our input. Let''s now look at the
    output of our print statement, where we will print the final list where the stop
    words have been removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we chopped off nearly 64 words from our input text, which were
    the stop words.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Stop words are not limited to proper English words. It''s contextual, depending
    on the application in hand and how you want to program your system. Ideally, if
    we are not interested in special characters, we can include them in our stop word
    list. Let''s look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will run another list comprehension in order to remove punctuations
    from our words. Now, the output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that stop word removal is contextual and based on the application.
    If you are working on a sentiment analysis application on mobile or chat room
    text, emoticons are highly useful. You don't remove them as they form a very good
    feature set for the downstream machine learning application.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, in a document, the frequency of stop words is very high. However,
    there may be other words in your corpus that may have a very high frequency. Based
    on your context, you can add them to your stop word list.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Performing Tokenization* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Analyzing Data - Explore & Wrangle*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*List generation* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python for Data
    Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming the words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this we will see how to stem the word.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Standardization of the text is a different beast and we need different tools
    to tame it. In this section, we will look into how we can convert words to their
    base forms in order to bring consistency to our processing. We will start with
    traditional ways that include stemming and lemmatization. English grammar dictates
    how certain words are used in sentences. For example, perform, performing, and
    performs indicate the same action; they appear in different sentences based on
    the grammar rules.
  prefs: []
  type: TYPE_NORMAL
- en: '*The goal of both stemming and lemmatization is to reduce inflectional forms
    and sometimes derivationally related forms of a word to a common base form.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Introduction to Information Retrieval By Christopher D. Manning, Prabhakar
    Raghavan & Hinrich Schütze*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let''s look into how we can perform word stemming using Python NLTK. NLTK provides
    us with a rich set of functions that can help us do the stemming pretty easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the list of functions in the module, and for our interest, we have
    the following stemmers:'
  prefs: []
  type: TYPE_NORMAL
- en: Porter – porter stemmer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lancaster – Lancaster stemmer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snowball – snowball stemmer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porter is the most commonly used stemmer. The algorithm is not very aggressive
    when moving words to their root form.
  prefs: []
  type: TYPE_NORMAL
- en: Snowball is an improvement over porter. It is also faster than porter in terms
    of the computational time.
  prefs: []
  type: TYPE_NORMAL
- en: Lancaster is the most aggressive stemmer. With porter and snowball, the final
    word tokens would still be readable by humans, but with Lancaster, it is not readable.
    It's the fastest of the trio.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use some of them to see how the stemming of words can
    be performed.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin with, let''s load the necessary libraries and declare the dataset
    against which we would want to demonstrate stemming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s jump into the different stemming algorithms, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In step 1, we will import the stem module from nltk. We will also create a list
    of words that we want to stem. If you observe carefully, the words have been chosen
    to have different suffixes, including s, ies, ed, ing, and so on. Additionally,
    there are some words in their root form already, such as throttle and fry. The
    idea is to see how the stemming algorithm treats them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Steps 2, 3, and 4 are very similar; we will invoke the porter, lancaster, and
    snowball stemmers on the input and print the output. We will use a list comprehension
    to apply these words to our input and finally, print the output. Let''s look at
    the print output to understand the effect of stemming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output from step 2\. Porter stemming was applied to our input words.
    We can see that the words with the suffixes ies, s, ed , and ing have been reduced
    to their root forms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: It's interesting to note that throttle is changed to throttle.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 3, we will print the output of lancaster, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: The word throttle has been left as it is. Note what has happened to movies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, let''s look at the output produced by the snowball stemmer in step
    4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: The output is pretty similar to the porter stemmer.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the three algorithms are pretty involved; going into the details of these
    algorithms is beyond the scope of this book. I will recommend you to look to the
    web for more details on these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'For details of the porter and snowball stemmers, refer to the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://snowball.tartarus.org/algorithms/porter/stemmer.html](http://snowball.tartarus.org/algorithms/porter/stemmer.html)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*List Comprehension* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python for
    Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing word lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this we will learn how to perform word lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stemming is a heuristic process, which goes about chopping the word suffixes
    in order to get to the root form of the word. In the previous recipe, we saw that
    it may end up chopping even the right words, that is, chopping the derivational
    affixes.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following Wikipedia link for the derivational patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://en.wikipedia.org/wiki/Morphological_derivation#Derivational_patterns](http://en.wikipedia.org/wiki/Morphological_derivation#Derivational_patterns)'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, lemmatization uses a morphological analysis and vocabulary
    to get the lemma of a word. It tries to change only the inflectional endings and
    give the base word from a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: See Wikipedia for more information on inflection at [http://en.wikipedia.org/wiki/Inflection](http://en.wikipedia.org/wiki/Inflection).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use NLTK's `WordNetLemmatizer`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin with, we will load the necessary libraries. Once again, as we did
    in the previous recipes, we will prepare a text input in order to demonstrate
    lemmatization. We will then proceed to implement lemmantization in the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Step 1 is very similar to our stemming recipe. We will provide the input. In
    step 2, we will do the lemmatization. This lemmatizer uses Wordnet's built-in
    morphy-function.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://wordnet.princeton.edu/man/morphy.7WN.html](https://wordnet.princeton.edu/man/morphy.7WN.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the output from the print statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: The first thing to strike is the word movie. You can see that it has got this
    right. Porter and the other algorithms had chopped the last letter e.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look into a small example using lemmatizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: The word running should ideally be run and our lemmatizer should have gotten
    it right. We can see that it has not made any changes to running. However, our
    heuristic-based stemmers have got it right! Then, what has gone wrong with our
    lemmatizer?
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By default, the lemmatizer assumes that the input is a noun; this can be rectified
    by passing the POS tag of the word to our lemmatizer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Performing Tokenization* recipe in [Chapter 3](ch03.xhtml "Chapter 3. Data
    Analysis – Explore and Wrangle"), *Analyzing Data - Explore & Wrangle*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing the text as a bag of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this we will learn how represent the text as a bag of words.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to do machine learning on text, we will need to convert the text to
    numerical feature vectors. In this section, we will look into the bag of words
    representation, where the text is converted to numerical vectors and the column
    names are the underlying words and values can be either of thw following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary, which indicates whether the word is present/absent in the given document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency, which indicates the count of the word in the given document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TFIDF, which is a score that we will cover subsequently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bag of words is the most frequent way of representing the text. As the name
    suggests, the order of words is ignored and only the presence/absence of words
    are key to this representation. It is a two-step process, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For every word in the document that is present in the training set, we will
    assign an integer and store this as a dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For every document, we will create a vector. The columns of the vectors are
    the actual words itself. They form the features. The values of the cell are binary,
    frequency, or TFIDF.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s load the necessary libraries and prepare the dataset for the demonstration
    of bag of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s jump into how to transform the text into a bag of words representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In step 1, we will define the input. This is the same input that we used for
    the stop word removal recipe. In step 2, we will import the sentence tokenizer
    and tokenize the given input into sentences. We will treat every sentence here
    as a document:'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending on your application, the notion of a document can change. In this
    case, our sentence is considered as a document. In some cases, we can also treat
    a paragraph as a document. In web page mining, a single web page can be treated
    as a document or parts of the web page separated by the <p> tags can also be treated
    as a document.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: If we print the length of the sentence list, we will get six, and so in our
    case, we have six documents.
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we will import `CountVectorizer` from the `scikitlearn.feature_extraction`
    text package. It converts a collection of documents—in this case, a list of sentences—to
    a matrix, where the rows are sentences and the columns are the words in these
    sentences. The count of these words are inserted in the value of these cells.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will transform the list of sentences into a term document matrix using `CountVectorizer`.
    Let''s dissect the output one by one. First, we will look into `count_v`, which
    is a `CountVectorizer` object. We had mentioned in the introduction that we need
    to build a dictionary of all the words in the given text. The `vocabulary_` of
    `count_v` attribute provides us with the list of words and their associated IDs
    or feature indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This dictionary can be retrieved using the `vocabulary_` attribute. This is
    a map of the terms in order to feature indices. We can also use the following
    function to get the list of words (features):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now move on to look at `tdm`, which is the object that we received after
    transforming the given input using CountVectorizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, tdm is a sparse matrix object. Refer to the following link
    to understand more about the sparse matrix representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can look into the shape of this object and also inspect some of the elements,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the shape of the matrix is 6 X 122\. We have six documents,
    that is, sentences in our context and 122 words that form the vocabulary. Note
    that this is a sparse matrix representation; as all the sentences will not have
    all the words, a lot of the cell values will have zero as an entry and hence,
    we will print only the indices that have non-zero entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'From `tdm.indptr`, we know that document 1''s entry starts from `0` and ends
    at 18 in the `tdm.data` and `tdm.indices` arrays, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify this in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: We can see that `107`, which corresponds to the word text, has occurred four
    times in the first sentence, and similarly, mining has occurred once. Thus, in
    this recipe, we converted a given text into a feature vector, where the features
    are words.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `CountVectorizer` class has a lot of other features to offer in order to
    transform the text into feature vectors. Let''s look at some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: The first one is binary, which is set to `False`; we can also have it set to
    `True`. Then, the final matrix would not have the count but will have one or zero,
    based on the presence or absence of the word in the document.
  prefs: []
  type: TYPE_NORMAL
- en: The lowercase is set to `True` by default; the input text is transformed into
    lowercase before the mapping of the words to feature indices is performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'While creating a mapping of the words to feature indices, we can ignore some
    words by providing a stop word list. Observe the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'If we print the size of the vocabulary that has been built, we can see the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we have 106 now as compared to 122 that we had before.
  prefs: []
  type: TYPE_NORMAL
- en: We can also give a fixed set of vocabulary to `CountVectorizer`. The final sparse
    matrix columns will be only from these fixed sets and anything that is not in
    this set will be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next interesting parameter is the ngram range. You can see that a tuple
    (1,1) has been passed. This ensures that only one grams or single words are used
    while creating a feature set. For example, this can be changed to (1,2), which
    tells `CountVectorizer` to create both unigrams and bigrams. Let''s look at the
    following code and the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Both the unigrams and bigrams are now a part of our feature set.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will leave you to explore the other parameters. The documentation for these
    parameters is available at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Using Dictionaries* recipe in [Chapter 1](ch01.xhtml "Chapter 1. Python for
    Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Removing Stop words, Stemming of words, Lemmatization of words* recipe in
    [Chapter 3](ch03.xhtml "Chapter 3. Data Analysis – Explore and Wrangle"), *Analyzing
    Data - Explore & Wrangle*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating term frequencies and inverse document frequencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this we will learn how to calculate term frequencies and inverse document
    frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Occurrences and counts are good as feature values, but they suffer from some
    problems. Let's say that we have four documents of unequal length. This will give
    a higher weightage to the terms in the longer documents than those in the shorter
    ones. So, instead of using the plain vanilla occurrence, we will normalize it;
    we will divide the number of occurrences of a word in a document by the total
    number of words in the document. This metric is called term frequencies. Term
    frequency is also not without problems. There are words that will occur in many
    documents. These words would dominate the feature vector but they are not informative
    enough to distinguish the documents in the corpus. Before we look into a new metric
    that can avoid this problem, let's define document frequency. Similar to word
    frequency, which is local with respect to a document, we can calculate a score
    called document frequency, which is the number of documents that the word occurs
    in the corpus divided by the total number of documents in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: The final metric that we will use for the words is the product of the term frequency
    and the inverse of the document frequency. This is called the TFIDF score.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Load the necessary libraries and declare the input data that will be used for
    the demonstration of term frequencies and inverse document frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how to find the term frequency and inverse document frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Steps 1, 2, and 3 are the same as the previous recipe. Let''s look at step
    4, where we will pass the output of step 3 in order to calculate the TFIDF score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Tdm is a sparse matrix. Now, let''s look at the values of these matrices, using
    indices, data, and index pointer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The data shows the values, we don't have the occurences, but the normalized
    TFIDF score for the words.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once again, we can delve deeper into the TFIDF transformer by looking into
    the parameters that can be passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: The documentation for this is available at [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html).
  prefs: []
  type: TYPE_NORMAL
