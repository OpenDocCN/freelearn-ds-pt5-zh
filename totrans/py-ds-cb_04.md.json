["```py\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import scale\nimport scipy\nimport matplotlib.pyplot as plt\n\n# Load Iris data\ndata = load_iris()\nx = data['data']\ny = data['target']\n\n# Since PCA is an unsupervised method, we will not be using the target variable y\n# scale the data such that mean = 0 and standard deviation = 1\nx_s = scale(x,with_mean=True,with_std=True,axis=0)\n\n# Calculate correlation matrix\nx_c = np.corrcoef(x_s.T)\n\n# Find eigen value and eigen vector from correlation matrix\neig_val,r_eig_vec = scipy.linalg.eig(x_c)\nprint 'Eigen values \\n%s'%(eig_val)\nprint '\\n Eigen vectors \\n%s'%(r_eig_vec)\n\n# Select the first two eigen vectors.\nw = r_eig_vec[:,0:2]\n\n# # Project the dataset in to the dimension\n# from 4 dimension to 2 using the right eignen vector\nx_rd = x_s.dot(w)\n\n# Scatter plot the new two dimensions\nplt.figure(1)\nplt.scatter(x_rd[:,0],x_rd[:,1],c=y)\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\n```", "```py\nx_s = scale(x,with_mean=True,with_std=True,axis=0)\nx_c = np.corrcoef(x_s.T)\n```", "```py\neig_val,r_eig_vec = scipy.linalg.eig(x_c)\nprint 'Eigen values \\n%s'%(eig_val)\nprint '\\n Eigen vectors \\n%s'%(r_eig_vec)\n# Select the first two eigen vectors.\nw = r_eig_vec[:,0:2]\n\n# # Project the dataset in to the dimension\n# from 4 dimension to 2 using the right eignen vector\nx_rd = x_s.dot(w)\n\n# Scatter plot the new two dimensions\nplt.figure(1)\nplt.scatter(x_rd[:,0],x_rd[:,1],c=y)\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\n```", "```py\n>>>x.shape\n(150, 4)\n>>>\n```", "```py\nx_s = scale(x,with_mean=True,with_std=True,axis=0)\n```", "```py\nprint Eigen values \\n%s%(eig_val)\nprint \\n Eigen vectors \\n%s%(r_eig_vec)\n```", "```py\n    print \"Component, Eigen Value, % of Variance, Cummulative %\"\n    cum_per = 0\n    per_var = 0\n    for i,e_val in enumerate(eig_val):\n        per_var = round((e_val / len(eig_val)),3)\n        cum_per+=per_var\n    print ('%d, %0.2f, %0.2f, %0.2f')%(i+1, e_val, per_var*100,cum_per*100)\n    ```", "```py\nfrom sklearn.datasets import make_circles\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import KernelPCA\n\n# Generate a dataset where the variations cannot be captured by a straight line.\nnp.random.seed(0)\nx,y = make_circles(n_samples=400, factor=.2,noise=0.02)\n\n# Plot the generated dataset\nplt.close('all')\nplt.figure(1)\nplt.title(\"Original Space\")\nplt.scatter(x[:,0],x[:,1],c=y)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\n\n# Try to fit the data using normal PCA\npca = PCA(n_components=2)\npca.fit(x)\nx_pca = pca.transform(x)\n```", "```py\nplt.figure(2)\nplt.title(\"PCA\")\nplt.scatter(x_pca[:,0],x_pca[:,1],c=y)\nplt.xlabel(\"$Component_1$\")\nplt.ylabel(\"$Component_2$\")\n\n# Plot using the first component from normal pca\nclass_1_indx = np.where(y==0)[0]\nclass_2_indx = np.where(y==1)[0]\n\nplt.figure(3)\nplt.title(\"PCA- One component\")\nplt.scatter(x_pca[class_1_indx,0],np.zeros(len(class_1_indx)),color='red')\nplt.scatter(x_pca[class_2_indx,0],np.zeros(len(class_2_indx)),color='blue')\n```", "```py\n# Create  KernelPCA object in Scikit learn, specifying a type of kernel as a parameter.\nkpca = KernelPCA(kernel=\"rbf\",gamma=10)\n# Perform KernelPCA\nkpca.fit(x)\nx_kpca = kpca.transform(x)\n\n# Plot the first two components.\nplt.figure(4)\nplt.title(\"Kernel PCA\")\nplt.scatter(x_kpca[:,0],x_kpca[:,1],c=y)\nplt.xlabel(\"$Component_1$\")\nplt.ylabel(\"$Component_2$\")\nplt.show()\n```", "```py\nKernelPCA(kernel=rbf,gamma=10) \n```", "```py\nfrom sklearn.datasets import make_moons\nx,y = make_moons(100)\nplt.figure(5)\nplt.title(\"Non Linear Data\")\nplt.scatter(x[:,0],x[:,1],c=y)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nplt.savefig('fig-7.png')\nplt.show()\n```", "```py\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import scale\nfrom scipy.linalg import svd\n\n# Load Iris dataset\ndata = load_iris()\nx = data['data']\ny = data['target']\n\n# Proceed by scaling the x variable w.r.t its mean,\nx_s = scale(x,with_mean=True,with_std=False,axis=0)\n\n# Decompose the matrix using SVD technique.We will use SVD implementation in scipy.\nU,S,V = svd(x_s,full_matrices=False)\n\n# Approximate the original matrix by selecting only the first two singular values.\nx_t = U[:,:2]\n\n# Finally we plot the datasets with the reduced components.\nplt.figure(1)\nplt.scatter(x_t[:,0],x_t[:,1],c=y)\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.show()\n```", "```py\n# Proceed by scaling the x variable w.r.t its mean,\nx_s = scale(x,with_mean=True,with_std=False,axis=0)\n# Decompose the matrix using SVD technique.We will use SVD implementation in scipy.\nU,S,V = svd(x_s,full_matrices=False)\n\n# Approximate the original matrix by selecting only the first two singular values.\nx_t = U[:,:2]\n\n# Finally we plot the datasets with the reduced components.\nplt.figure(1)\nplt.scatter(x_t[:,0],x_t[:,1],c=y)\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.show()\n```", "```py\n>>>x.shape\n(150, 4)\n>>>\n```", "```py\nx_s = scale(x,with_mean=True,with_std=False,axis=0)\n```", "```py\nfrom sklearn.datasets import fetch_20newsgroups\ndata = fetch_20newsgroups(categories=cat)\n```", "```py\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import euclidean_distances\nfrom sklearn.random_projection import GaussianRandomProjection\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load 20 newsgroup dataset\n# We select only sci.crypt category\n# Other categories include\n# 'sci.med', 'sci.space' ,'soc.religion.christian'\ncat =['sci.crypt']\ndata = fetch_20newsgroups(categories=cat)\n\n# Create a term document matrix, with term frequencies as the values\n# from the above dataset.\nvectorizer = TfidfVectorizer(use_idf=False)\nvector = vectorizer.fit_transform(data.data)\n\n# Perform the projection. In this case we reduce the dimension to 1000\ngauss_proj = GaussianRandomProjection(n_components=1000)\ngauss_proj.fit(vector)\n# Transform the original data to the new space\nvector_t = gauss_proj.transform(vector)\n\n# print transformed vector shape\nprint vector.shape\nprint vector_t.shape\n\n# To validate if the transformation has preserved the distance, we calculate the old and the new distance between the points\norg_dist = euclidean_distances(vector)\nred_dist = euclidean_distances(vector_t)\n\ndiff_dist = abs(org_dist - red_dist)\n\n# We take the difference between these points and plot them \n# as a heatmap (only the first 100 documents).\nplt.figure()\nplt.pcolor(diff_dist[0:100,0:100])\nplt.colorbar()\nplt.show()\n```", "```py\n(595, 16115)\n```", "```py\nimport numpy as np\nfrom collections import defaultdict\nfrom sklearn.decomposition import NMF\nimport matplotlib.pyplot as plt\n\n# load our ratings matrix in python.\nratings = [\n[5.0, 5.0, 4.5, 4.5, 5.0, 3.0, 2.0, 2.0, 0.0, 0.0],\n[4.2, 4.7, 5.0, 3.7, 3.5, 0.0, 2.7, 2.0, 1.9, 0.0],\n[2.5, 0.0, 3.3, 3.4, 2.2, 4.6, 4.0, 4.7, 4.2, 3.6],\n[3.8, 4.1, 4.6, 4.5, 4.7, 2.2, 3.5, 3.0, 2.2, 0.0],\n[2.1, 2.6, 0.0, 2.1, 0.0, 3.8, 4.8, 4.1, 4.3, 4.7],\n[4.7, 4.5, 0.0, 4.4, 4.1, 3.5, 3.1, 3.4, 3.1, 2.5],\n[2.8, 2.4, 2.1, 3.3, 3.4, 3.8, 4.4, 4.9, 4.0, 4.3],\n[4.5, 4.7, 4.7, 4.5, 4.9, 0.0, 2.9, 2.9, 2.5, 2.1],\n[0.0, 3.3, 2.9, 3.6, 3.1, 4.0, 4.2, 0.0, 4.5, 4.6],\n[4.1, 3.6, 3.7, 4.6, 4.0, 2.6, 1.9, 3.0, 3.6, 0.0]\n]\n\nmovie_dict = {\n1:\"Star Wars\",\n2:\"Matrix\",\n3:\"Inception\",\n4:\"Harry Potter\",\n5:\"The hobbit\",\n6:\"Guns of Navarone\",\n7:\"Saving Private Ryan\",\n8:\"Enemy at the gates\",\n9:\"Where eagles dare\",\n10:\"Great Escape\"\n}\n\nA = np.asmatrix(ratings,dtype=float)\n\n# perform non negative matrix transformation on the data.\nmax_components = 2\nreconstruction_error = []\nnmf = None\nnmf = NMF(n_components = max_components,random_state=1)\nA_dash = nmf.fit_transform(A)\n\n# Examine the reduced matrixfor i in range(A_dash.shape[0]):\nfor i in range(A_dash.shape[0]):\n    print \"User id = %d, comp1 score = %0.2f, comp 2 score = %0.2f\"%(i+1,A_dash[i][0],A_dash[i][1])\n\nplt.figure(1)\nplt.title(\"User Concept Mapping\")\nx = A_dash[:,0]\ny = A_dash[:,1]\nplt.scatter(x,y)\nplt.xlabel(\"Component 1 Score\")\nplt.ylabel(\"Component 2 Score\")\n\n# Let us examine our component matrix F.\nF = nmf.components_\nplt.figure(2)\nplt.title(\"Movie Concept Mapping\")\nx = F[0,:]\ny = F[1,:]\nplt.scatter(x,y)\nplt.xlabel(\"Component 1 score\")\nplt.ylabel(\"Component 2  score\")\nfor i in range(F[0,:].shape[0]):\n    plt.annotate(movie_dict[i+1],(F[0,:][i],F[1,:][i]))\nplt.show()\n```", "```py\n# perform non negative matrix transformation on the data.\nmax_components = 2\nreconstruction_error = []\nnmf = None\nnmf = NMF(n_components = max_components,random_state=1)\nA_dash = nmf.fit_transform(A)\n\n# Examine the reduced matrixfor i in range(A_dash.shape[0]):\nfor i in range(A_dash.shape[0]):\n    print \"User id = %d, comp1 score = %0.2f, comp 2 score = %0.2f\"%(i+1,A_dash[i][0],A_dash[i][1])\nplt.figure(1)\nplt.title(\"User Concept Mapping\")\nx = A_dash[:,0]\ny = A_dash[:,1]\nplt.scatter(x,y)\nplt.xlabel(\"Component 1 Score\")\nplt.ylabel(\"Component 2 Score\")\n\n# Let us examine our component matrix F.\nF = nmf.components_\nplt.figure(2)\nplt.title(\"Movie Concept Mapping\")\nx = F[0,:]\ny = F[1,:]\nplt.scatter(x,y)\nplt.xlabel(\"Component 1 score\")\nplt.ylabel(\"Component 2  score\")\nfor i in range(F[0,:].shape[0]):\n    plt.annotate(movie_dict[i+1],(F[0,:][i],F[1,:][i]))\nplt.show()\n```", "```py\nA_dash = nmf.fit_transform(A)\n```", "```py\n>>>A_dash.shape\n(10, 2)\n```", "```py\nfor i in range(A_dash.shape[0]):\nprint User id = %d, comp1 score = %0.2f, comp 2 score =%0.2f%(i+1,A_dash[i][0],A_dash[i][1])\n```", "```py\nF = nmf.components_\n```", "```py\nfor i in range(F[0,:].shape[0]):\nplt.annotate(movie_dict[i+1],(F[0,:][i],F[1,:][i]))\n```", "```py\nreconstructed_A = np.dot(W,H)\nnp.set_printoptions(precision=1)\nprint reconstructed_A\n```"]