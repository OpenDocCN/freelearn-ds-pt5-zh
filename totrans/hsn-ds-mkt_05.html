<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Drivers behind Marketing Engagement</h1>
                </header>
            
            <article>
                
<p>When you run marketing campaigns, one of the important measures that you will want to look at and analyze is customer engagement with your marketing efforts. For example, in email marketing, customer engagement can be measured by how many of your marketing emails were opened or ignored by your customers. Customer engagement can also be measured by the amount of website visits from individual customers. Successful marketing campaigns will draw a lot of engagement from your customers, while ineffective marketing campaigns will not only drive a lower amount of engagement from your customers, but will also negatively impact your business. Customers might mark emails from your business as spam or unsubscribe from your mailing list.</p>
<p>In order to understand what affects customer engagement, in this chapter, we will discuss how we can use explanatory analysis (more specifically, regression analysis). We will briefly cover the definition of explanatory analysis, what regression analysis is, and how to use a logistic regression model for explanatory analysis. Then, we will cover how to build and interpret regression analysis results in Python, using the <kbd>statsmodels</kbd> package. For R programmers, we will discuss how we can build and interpret regression analysis results with <kbd>glm</kbd>.</p>
<p>I<span>n this chapte</span><span>r, we will cover the following topics:</span></p>
<ul>
<li>Using regression analysis for explanatory analysis</li>
<li>Regression analysis with Python</li>
<li>Regression analysis with R</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using regression analysis for explanatory analysis</h1>
                </header>
            
            <article>
                
<p>In <a href="1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml" target="_blank">Chapter 2</a>, <em>Key Performance Indicators and Visualizations</em>, we discussed what <strong>descriptive analysis</strong> is and how it is used to better understand a dataset. We experimented using various visualization techniques and building different types of plots in Python and R.</p>
<p>In this chapter, we are going to expand our knowledge and start to discuss why, when, and how to use <strong>explanatory analysis</strong> for marketing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Explanatory analysis and regression analysis</h1>
                </header>
            
            <article>
                
<p>As we briefly discussed in <a href="c169428b-e0db-4624-896c-24316e9b29cc.xhtml" target="_blank">Chapter 1</a>, <em>Data Science and Marketing</em>, the purpose of explanatory analysis is to answer why we are using the data, whereas the purpose of descriptive analysis is to answer what we are using the data for, and how we are using it. When you run different marketing campaigns, often times, you will notice that some marketing campaigns perform much better than others; you might wonder why it is that some of your marketing campaigns work so well, while others do not. For example, you might want to understand what types and groups of customers typically open your marketing emails more often than others. As another example, you might want to analyze what attributes of the customer base are highly correlated with higher conversion rates and item purchases.</p>
<p>With explanatory analysis, you can analyze and understand the key factors that are highly and significantly correlated with the outcomes that you want. <strong>Regression analysis</strong> and regression models are frequently used to model the relationships between the attributes and the outcomes. Simply put, regression analysis estimates the values of output variables by finding a function of the attributes or features that best approximates the output values. One of the frequently used forms of regression analysis is <strong>linear regression</strong>. As the name suggests, in linear regression, we try to estimate the output variables via linear combinations of the features. If we use <em>Y</em> for the output variable and <em>X<sub>i</sub></em> for each of the features, where <em>i</em> is the <em>i</em>th feature, then the linear regression formula will look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4141c7bf-bbc5-4f2d-9b36-2c3877c5a77c.png" style="width:23.17em;height:1.25em;"/></p>
<p>As you can see from the preceding formula, the output variable <em>Y</em> is expressed as a linear combination of the features, <em>X<sub>i</sub></em>. The purpose of the linear regression models is to find the intercept, <em>a</em>, and the coefficients, <em>b<sub>i</sub></em>, that best estimate the output variable, using the given features. A fitted linear regression line will look something like the following <span>(image from <a href="https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2" target="_blank">https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2</a>)</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d60fa219-a286-4b2c-a92e-209e6f4d3109.png" style="width:36.17em;height:27.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"/>
<p>The blue dots in this diagram are the data points, and the red line is the fitted, or trained, linear regression line. As you can see in the graph, linear regression tries to estimate the target variable through a linear combination of the features.</p>
<p>In this chapter, we will discuss how we can use regression analysis, and, more specifically, <strong>logistic regression</strong> models, to understand what drives higher customer engagement.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p><span><strong>Logistic regression</strong> is a type of regression analysis that is used when the output variable is binary (one for a positive outcome versus zero for a negative outcome). Like any other linear regression models, logistic regression models estimate the output from linear combinations of the feature variables. The only difference is what the model estimates. Unlike other linear regression models, logistic regression models estimate the log odds of an event, or, in other words, the log ratios between the probabilities of positive and negative events. The equation looks as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1946fce3-7d03-485d-a545-bd55a8d47e10.png" style="width:28.75em;height:2.83em;"/></p>
<p>The ratio on the left is the odds of success, which represents the ratio between the probability of success and the probability of failure. The curve of the log odds, also called the <strong>logit curve</strong>, looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ef7f8d76-83b1-4317-8df0-623cc1512349.png" style="width:28.58em;height:19.25em;"/></p>
<p>The logistic regression model output is simply the inverse of logit, which ranges from zero to one. In this chapter, we are going to use regression analysis to understand what drives customer engagement, and the output variable will be whether a customer responded to marketing calls. Hence, logistic regression fits perfectly in this case, as the output is a binary variable that can take two values: responded versus did not respond. In the following sections, we will discuss how we can use and build logistic regression models in Python and R, and then we will cover how we can interpret regression analysis results in order to understand what attributes of customers are highly correlated with higher marketing engagement.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression analysis with Python</h1>
                </header>
            
            <article>
                
<p><span>In this section, you will learn how to use the <kbd>statsmodels</kbd> package in Python to conduct regression analysis. For those readers that would like to use R instead of Python, for this exercise, you can skip to the next section. We will start this section by looking at the data more closely, using the <kbd>pandas</kbd> and <kbd>matplotlib</kbd> packages, and then we will discuss how to build regression models and interpret the results by using the <kbd>statsmodels</kbd> library.</span></p>
<p><span>For this exercise, we will be using one of the publicly available datasets from IBM Watson, which can be found at <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/">https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/</a>. You can follow the link and download the data file in a CSV format. In order to load this data into your Jupyter Notebook, you can run the following code:</span></p>
<pre>import matplotlib.pyplot as plt<br/>import pandas as pd<br/><br/>df = pd.read_csv('../data/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')</pre>
<p><span>Similar to what we did in <a href="1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml" target="_blank">Chapter 2</a>, <em>Key Performance Indicators and Visualizations</em>, we are importing the <kbd>matplotlib</kbd> and <kbd>pandas</kbd> packages first; using the <kbd>read_csv</kbd> function in <kbd>pandas</kbd>, we can read the data into a <kbd>pandas</kbd></span> DataFrame<span>. We will use <kbd>matplotlib</kbd> later, for data analysis and visualizations.</span></p>
<p>The loaded DataFrame, <kbd>df</kbd>, looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-997 image-border" src="assets/333d7a69-95a2-4044-bcc1-5b6e92b72fc6.png" style="width:42.00em;height:20.33em;"/></p>
<p>As we discussed in <a href="1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml" target="_blank">Chapter 2</a>, <em>Key Performance Indicators and Visualizations</em>, a DataFrame <kbd>shape</kbd> attribute tells us the number of rows and columns in the DataFrame, and the <kbd>head</kbd> function will display the first five records of the dataset. Once you have successfully read the data into a <kbd>pandas</kbd> DataFrame, your data should look like it does in the screenshot.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data analysis and visualizations</h1>
                </header>
            
            <article>
                
<p><span>Before we dive into regression analysis, we will first take a more detailed look at the data, in order to have a better understanding of what data points we have and what patterns we can see in the data. If you look at the data, you will notice a column named <kbd>Response</kbd>. It contains information on whether a customer responded to marketing calls. We will use this field as a measure of customer engagement. For future computations, it will be better to encode this field with numerical values. Let's take a look at the following code:</span></p>
<pre><span class="n">df</span><span class="p">[</span><span class="s1">'Engaged'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'Response'</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s1">'No'</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span></pre>
<p>As you can see in this code, using the <kbd>apply</kbd> function of a <kbd>pandas</kbd> DataFrame, we are encoding those who did not respond to marketing calls (<kbd>No</kbd>) with a value of <kbd>0</kbd> and those who did respond (<kbd>Yes</kbd>) with a value of <kbd>1</kbd>. We are creating a new field named <kbd>Engaged</kbd> with these encoded values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Engagement rate</h1>
                </header>
            
            <article>
                
<p>The first thing that we are going to look at is the aggregate engagement rate. This engagement rate is simply the percentage of customers that responded to the marketing calls. Take a look at the following code:</p>
<pre>engagement_rate_df = pd.DataFrame(<br/>    df.groupby('Engaged').count()['Response'] / df.shape[0] * 100.0<br/>)</pre>
<p><span>As you can see from this code, we are grouping by the newly created field, <kbd>Engaged</kbd>, using the <kbd>groupby</kbd> function of a <kbd>pandas</kbd></span> DataFrame. Then<span>, we are counting the number of records (or customers) in each <kbd>Engaged</kbd> group with the <kbd>count</kbd> function. By dividing by the total number of customers in the</span> DataFrame a<span>nd multiplying by <kbd>100.0</kbd>, we get the engagement rate. The results are as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-996 image-border" src="assets/97193318-df6f-461f-9404-86cb8ed4b87b.png" style="width:24.67em;height:12.42em;"/></p>
<p><span>To make this easier to read, we can transpose the</span> DataFrame, m<span>eaning that we can flip the rows and columns in the</span> DataFrame. <span>You can transpose a <kbd>pandas</kbd></span> DataFrame <span>by using the <kbd>T</kbd> attribute of a</span> DataFrame. <span>It looks as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-992 image-border" src="assets/04dd8f15-66c7-4bdd-a2c3-1e04839e1447.png" style="width:25.08em;height:8.75em;"/></p>
<p><span>As you can see, about 14% of the customers have responded to marketing calls, and the remaining 86% of the customers have not responded.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sales channels</h1>
                </header>
            
            <article>
                
<p><span>Now, let's see whether we can find any noticeable patterns in the sales channel and engagement. We are going to analyze how the engaged and non-engaged customers are distributed among different sales channels. Let's first look at the following code:</span></p>
<pre>engagement_by_sales_channel_df = pd.pivot_table(<br/>    df, values='Response', index='Sales Channel', columns='Engaged', aggfunc=len<br/>).fillna(0.0)<br/><br/>engagement_by_sales_channel_df.columns = ['Not Engaged', 'Engaged']</pre>
<p><span>As you can see in this code snippet, we are using the <kbd>pivot_table</kbd> function in the <kbd>pandas</kbd> library to group by the <kbd>Sales Channel</kbd> and <kbd>Response</kbd> variables. Once you run this code, <kbd>engagement_by_sales_channel_df</kbd> will have the following data:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0cd77263-2dc4-4680-b552-5a1d92ed0bcb.png" style="width:18.08em;height:11.92em;"/></p>
<p>As you will have noticed in the previous section, there are significantly more customers that are not engaged with the marketing efforts, so it is quite difficult to look at the differences in the sales channel distributions between the engaged and non-engaged customers from raw numbers. To make the differences more visually identifiable, we can build pie charts using the following code:</p>
<pre>engagement_by_sales_channel_df.plot(<br/>    kind='pie',<br/>    figsize=(15, 7),<br/>    startangle=90,<br/>    subplots=True,<br/>    autopct=lambda x: '%0.1f%%' % x<br/>)<br/><br/>plt.show()</pre>
<p class="mce-root"/>
<p>Once you run this code, you will see the following pie charts, which show the distributions of engaged and non-engaged customers across different sales channels:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e5402ab-645b-41ea-92ef-4e00ba53ac02.png"/></p>
<p><span>Compared to the previous table that shows raw counts of engaged and non-engaged customers in each sales channel, these pie charts help us to visually spot the differences in the distributions more easily. As you can see from these charts, more than half of the engaged customers were from agents, whereas non-engaged customers are more evenly distributed across all four different channels. As you can see from these charts, analyzing and visualizing data can help us to notice interesting patterns in the data, which will further help when we run regression analysis in the later parts of this chapter.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Total claim amounts</h1>
                </header>
            
            <article>
                
<p><span>The last thing that we are going to look at before we dive into the regression analysis are the differences in the distributions of <kbd>Total Claim Amount</kbd> between the engaged and non-engaged groups. We are going to visualize this by using box plots. Let's first look at how we can build box plots in Python, as follows:</span></p>
<pre>ax = df[['Engaged', 'Total Claim Amount']].boxplot(<br/>    by='Engaged',<br/>    showfliers=False,<br/>    figsize=(7,5)<br/>)<br/><br/>ax.set_xlabel('Engaged')<br/>ax.set_ylabel('Total Claim Amount')<br/>ax.set_title('Total Claim Amount Distributions by Engagements')<br/><br/>plt.suptitle("")<br/>plt.show()</pre>
<p><span>As you can see in this code, it is quite straightforward to build box plots from a <kbd>pandas</kbd></span> DataFrame. You <span>can simply call the <kbd>boxplot</kbd> function. Box plots are a great way to visualize the distributions of continuous variables. They show the min, max, first quartile, median, and third quartile, all in one view. The following box plots show the distributions of the <kbd>Total Claim Amount</kbd> between the engaged and non-engaged groups:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-985 image-border" src="assets/c38b00b9-e2ca-4edd-a34a-cf67ba11ea74.png" style="width:30.92em;height:21.92em;"/></p>
<p><span>The central rectangle spans from the first quartile to the third quartile, and the green line shows the median. The lower and upper ends show the minimum and maximum of the distribution, respectively. One thing to note from the previous code is the <kbd>showfliers=False</kbd> argument. Let's see what happens when we set that argument to <kbd>True</kbd>, using the following code:</span></p>
<pre>ax = df[['Engaged', 'Total Claim Amount']].boxplot(<br/>    by='Engaged',<br/>    showfliers=True,<br/>    figsize=(7,5)<br/>)<br/><br/>ax.set_xlabel('Engaged')<br/>ax.set_ylabel('Total Claim Amount')<br/>ax.set_title('Total Claim Amount Distributions by Engagements')<br/><br/>plt.suptitle("")<br/>plt.show()</pre>
<p><span>Using this code and the <kbd>showfliers=True</kbd> flag, the resulting box plots now look as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-989 image-border" src="assets/5ff9f5b8-a67b-4e31-b2df-6f1c2661ac25.png" style="width:31.25em;height:22.17em;"/></p>
<p><span>As you notice in these box plots, they plot many dots above the upper boundary lines, which suggested maximum values in the previous box plots. The dots above the upper boundary line show the suspected outliers that are decided based on the <strong>Interquartile range</strong> (<strong>IQR</strong>). Th</span>e IQR is <span>simply the range between the first and third quartiles, and the points that fall <kbd>1.5*IQR</kbd> above the third quartile or <kbd>1.5*IQR</kbd> below the first quartile are suspected outliers and are shown with the dots.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression analysis</h1>
                </header>
            
            <article>
                
<p><span>So far, we have analyzed the types of fields that we have in the data and how the patterns differ between the engaged group and the non-engaged group. Now, we are going to discuss how to conduct and interpret regression analysis in Python by using the <kbd>statsmodels</kbd> package. We will first build a logistic regression model with continuous variables, and you'll learn how to interpret the results. Then, we are going to discuss different ways to handle categorical variables when fitting regression models, and what impact those categorical variables have on the fitted logistic regression model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous variables</h1>
                </header>
            
            <article>
                
<p><span>In linear regression, including logistic regression, it is straightforward to fit a regression model when the feature variables are continuous, as it just needs to find a linear combination of feature variables with numerical values for estimating the output variables. In order to fit a regression model with continuous variables, let's first take a look at how to get the data types of the columns in a <kbd>pandas</kbd></span> DataFrame. <span>Take a look at the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-983 image-border" src="assets/13cfa30a-b8b2-45db-8b1b-cc3b0b115cd3.png" style="width:26.83em;height:9.00em;"/></p>
<p><span>As you can see from this Jupyter Notebook screenshot, the <kbd>dtype</kbd> attribute of a <kbd>pandas</kbd> <kbd>Series</kbd> object tells you what type of data it contains. As you can see from this snapshot, the <kbd>Income</kbd> variable has integers and the <kbd>Customer Lifetime Value</kbd> feature has floating point numbers. In order to take a quick look at the distributions of variables with numerical values, you can also do the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-977 image-border" src="assets/58348fa5-b7e6-49c3-b9de-5175614c4b24.png" style="width:162.50em;height:51.75em;"/></p>
<p><span>As you can see in this Jupyter Notebook snapshot, the <kbd>describe</kbd> function of a <kbd>pandas</kbd></span> DataFrame show<span>s the distributions of all of the columns with numerical values. For example, you can see that there are a total of <kbd>9134</kbd> records in the <kbd>Customer Lifetime Value</kbd> column, with a mean of <kbd>8004.94</kbd> and ranges from <kbd>1898.01</kbd> to <kbd>83325.38</kbd>.</span></p>
<p>We are going to store this list of the names of continuous variables in a separate variable, named <kbd>continuous_vars</kbd>. Take a look at the following code:</p>
<pre>continuous_vars = [<br/>    'Customer Lifetime Value', 'Income', 'Monthly Premium Auto', <br/>    'Months Since Last Claim', 'Months Since Policy Inception', <br/>    'Number of Open Complaints', 'Number of Policies', <br/>    'Total Claim Amount'<br/>]</pre>
<p><span>Now that we know which columns are continuous variables, let's start to fit a logistic regression model. In order to do that, we need to first import the <kbd>statsmodels</kbd> package, as shown in the following code:</span></p>
<pre>import statsmodels.formula.api as sm</pre>
<p><span>With the <kbd>statsmodels</kbd> package imported, the code to initiate a logistic regression model is quite simple, and looks as follows:</span></p>
<pre>logit = sm.Logit(<br/>    df['Engaged'], <br/>    df[continuous_vars]<br/>)</pre>
<p><span>As you can see from this code, we are using the <kbd>Logit</kbd> function within the <kbd>statsmodels</kbd> package. We are supplying the <kbd>Engaged</kbd> column as the output variable, which the model will learn to estimate, and the <kbd>continuous_vars</kbd> that contain all of the continuous variables as the input variables. Once a logistic regression object is created with the output and input variables defined, we can train or fit this model by using the following code:</span></p>
<pre>logit_fit = logit.fit()</pre>
<p>As you can see in this code, we are using the <kbd>fit</kbd> function of the logistic regression object, <kbd>logit</kbd>, to train a logistic regression model. Once this code is run, the trained model, <kbd>logit_fit</kbd>, will have learned the optimal solution that best estimates the output variable, <kbd>Engaged</kbd>, by using the input variables. In order to get a detailed description of the trained model, you can use the following code:</p>
<pre>logit_fit.summary()</pre>
<p class="mce-root"/>
<p>When you run this code, the <kbd>summary</kbd> <span>function </span>will display the following output in the Jupyter Notebook:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/39146c4c-eacf-42ab-80f4-4801a8e3b76b.png" style="width:36.33em;height:31.75em;"/></p>
<p><span>Let's take a closer look at this model output. <kbd>coef</kbd> represents the coefficients for each of the input variables, and <kbd>z</kbd> represents the <em>z</em>-score, which is the number of standard deviations from the mean. The <kbd>P&gt;|z|</kbd> column represents the <em>p</em>-value, which means how likely it is to observe the relationship between the feature and the output variable by chance. So, the lower the value of <kbd>P&gt;|z|</kbd> is, the more likely it is that the relationship between the given feature and the output variable is strong and is not by chance. Typically, <kbd>0.05</kbd> is a good cut-off point for the <em>p</em>-value, and any value less than <kbd>0.05</kbd> signifies a strong relationship between the given feature and the output variable. </span></p>
<p><span>Looking at this model output, we can see that <kbd>Income</kbd>, <kbd>Monthly Premium Auto</kbd>, <kbd>Months Since Last Claim</kbd>, <kbd>Months Since Policy Inception</kbd>, and <kbd>Number of Policies</kbd> variables have significant relationships with the output variable, <kbd>Engaged</kbd>. For example, <kbd>Number of Policies</kbd> variable is significant and is negatively correlated with <kbd>Engaged</kbd>. This suggests that the more policies that the customers have, the less likely they are to respond to marketing calls. As another example, the <kbd>Months Since Last Claim</kbd> variable is significant and is negatively correlated with the output variable, <kbd>Engaged</kbd>. This means that the longer it has been since the last claim, the less likely that the customer is going to respond to marketing calls.</span></p>
<p>As you can see from these examples, you can interpret the regression analysis results quite easily by looking at the <em>p</em>-values and coefficients of the features from the model output. This is a good way to understand which attributes of customers are significantly and highly correlated with your outcomes of interest.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Categorical variables</h1>
                </header>
            
            <article>
                
<p><span>As you saw in the case of continuous variables in the previous section, it is quite straightforward to understand the relationships between the input and output variables from the coefficients and <em>p</em>-values. However, it becomes not so straightforward when we introduce <strong>categorical variables</strong>. Categorical variables often do not have any natural order, or they are encoded with non-numerical values, but in linear regression, we need the input variables to have numerical values that signify the order or magnitudes of the variables. For example, we cannot easily encode the </span><kbd>State</kbd><span> variable in our dataset with certain orderings or values. That is why we need to handle categorical variables differently from continuous variables when conducting regression analysis. In Python, there are multiple ways to handle categorical variables when using the <kbd>pandas</kbd> package. Let's first look at factorizing categorical variables, as shown in the following code:</span></p>
<pre>gender_values, gender_labels = df['Gender'].factorize()</pre>
<p><span>The <kbd>pandas</kbd> function, <kbd>factorize</kbd>, encodes categorical variables with numerical values by enumerating through the values. Let's take a look at the following output first:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-991 image-border" src="assets/6bd1e57a-4ea2-4d5d-b7a6-96c59a34773e.png" style="width:28.92em;height:8.92em;"/></p>
<p>As you can see from this output, the values of this <kbd>Gender</kbd> variable are encoded with zeros and ones, where <kbd>0</kbd> symbolizes female (<kbd>F</kbd>) and <kbd>1</kbd> symbolizes male (<kbd>M</kbd>). This is a quick way to encode categorical variables with numerical values. However, this function does not work when we want to embed natural orderings into the encoded values. For example, the <kbd>Education</kbd> variable in our dataset has five different categories: <kbd>High School or Below</kbd>, <kbd>Bachelor</kbd>, <kbd>College</kbd>, <kbd>Master</kbd>, and <kbd>Doctor</kbd>. We might want to embed the orderings when encoding different categories within this <kbd>Education</kbd> variable. </p>
<p>The following code shows another way to encode categorical variables with orderings when using <kbd>pandas</kbd>:</p>
<pre>categories = pd.Categorical(<br/>    df['Education'], <br/>    categories=['High School or Below', 'Bachelor', 'College', 'Master', 'Doctor']<br/>)</pre>
<p><span>As you can see in this code, we are using the <kbd>pd.Categorical</kbd> function to encode the values of <kbd>df['Education']</kbd>. We can define the orderings that we want with the argument, <kbd>categories</kbd>. In our example, we are giving values of <kbd>0</kbd>, <kbd>1</kbd>, <kbd>2</kbd>, <kbd>3</kbd>, and <kbd>4</kbd> for the <kbd>High School or Below</kbd>, <kbd>Bachelor</kbd>, <kbd>College</kbd>, <kbd>Master</kbd>, and <kbd>Doctor</kbd>, categories respectively. The output looks as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"/>
<p><span>We will now add these encoded variables to the</span> pandas DataFrame<span>, <kbd>df</kbd>, as shown in the following code:</span></p>
<pre>df['GenderFactorized'] = gender_values<br/>df['EducationFactorized'] = categories.codes</pre>
<p><span>With these encodings for the two categorical variables, <kbd>Gender</kbd> and <kbd>Education</kbd>, we can now fit a logistic regression model using the following code:</span></p>
<pre>logit = sm.Logit(<br/>    df['Engaged'], <br/>    df[[<br/>        'GenderFactorized',<br/>        'EducationFactorized'<br/>    ]]<br/>)<br/><br/>logit_fit = logit.fit()</pre>
<p><span>Similar to how we fit a logistic regression model with continuous variables previously, we can fit a logistic regression model with the encoded categorical variables, <kbd>GenderFactorized</kbd> and <kbd>EducationFactorized</kbd>, by using the <kbd>Logit</kbd> function in the <kbd>statsmodels</kbd> package. Using the <kbd>summary</kbd> function of the fitted logistic regression model object, we will get the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-982 image-border" src="assets/5ebbe76b-7881-455c-923a-8f3a1025b757.png" style="width:42.50em;height:28.25em;"/></p>
<p><span>As you can see in this output and by looking at the <em>p</em>-values in the <kbd>P&gt;|z|</kbd> column, both the <kbd>GenderFactorized</kbd> and <kbd>EducationFactorized</kbd> variables seem to have significant relationships with the output variable <kbd>Engaged</kbd>. If we look at the coefficients of these two variables, we can see that both are negatively correlated with the output. This suggests that male customers, encoded with <kbd>1</kbd> in the <kbd>GenderFactorized</kbd> variable, are less likely to be engaged with marketing calls, as compared to female customers, encoded with <kbd>0</kbd> in the <kbd>GenderFactorized</kbd> variable. Similarly, the higher the customers' education levels are, the less likely that they will be engaged with marketing calls.</span></p>
<p>We have discussed two ways of handling categorical variables in <kbd>pandas</kbd>, using the <kbd>factorize</kbd> and <kbd>Categorical</kbd> functions. With<span> these techniques, we can understand how different categories of categorical variables are correlated with the output variable.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining continuous and categorical variables</h1>
                </header>
            
            <article>
                
<p><span>The last Python exercise that we are going to do in this chapter involves combining continuous and categorical variables for our regression analysis. We can fit a logistic regression model by using both categorical and continuous variables, as shown in the following code</span><span>:</span></p>
<pre>logit = sm.Logit(<br/>    df['Engaged'], <br/>    df[['Customer Lifetime Value',<br/>        'Income',<br/>        'Monthly Premium Auto',<br/>        'Months Since Last Claim',<br/>        'Months Since Policy Inception',<br/>        'Number of Open Complaints',<br/>        'Number of Policies',<br/>        'Total Claim Amount',<br/>        'GenderFactorized',<br/>        'EducationFactorized'<br/>    ]]<br/>)<br/><br/>logit_fit = logit.fit()</pre>
<p><span>The only difference from the previous codes is the features that we selected to fit a logistic regression model. As you can see in this code, we are now fitting a logistic regression model with the continuous variables, as well as the two encoded categorical variables, <kbd>GenderFactorized</kbd> and <kbd>EducationFactorized</kbd>, that we created in the previous section. The results look as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-986 image-border" src="assets/9d55b21f-2e47-4da4-a572-f517ad414c2a.png" style="width:49.17em;height:43.17em;"/></p>
<p><span>Let's take a closer look at this output. The <kbd>Income</kbd>, <kbd>Monthly Premium Auto</kbd>, <kbd>Months Since Last Claim,</kbd> <kbd>Months Since Policy Inception</kbd>, <kbd>Number of Open Complaints</kbd>, <kbd>Number of Policies</kbd>, and <kbd>GenderFactorized</kbd> variable </span><span>are significant at a </span><kbd>0.05</kbd><span> significance level, and all of them have negative relationships with the output variable, <kbd>Engaged</kbd>. Hence, the higher the income is, the less likely that the customer will be engaged with marketing calls. Similarly, the more policies that the customer has, the less likely that he or she will be engaged with marketing calls.</span></p>
<p><span>Lastly, male customers are less likely to engage with marketing calls than female customers, which we can see from looking at the coefficient of <kbd>GenderFactorized</kbd>. </span><span>From looking at this regression analysis output, we can easily see the relationships between the input and output variables, and we can understand which attributes of customers are positively or negatively related to customer engagement with marketing calls.</span></p>
<div class="packt_infobox">The full code for the Python exercise in this chapter can be found at <a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.3/python/RegressionAnalysis.ipynb">https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.3/python/RegressionAnalysis.ipynb</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression analysis with R</h1>
                </header>
            
            <article>
                
<p><span>In this section, you are going to learn how to use the <kbd>glm</kbd> function in R to conduct regression analysis. For those readers that would like to use Python instead of R for this exercise, the step-by-step instructions for Python are in the previous section. We will start this section by analyzing the data more closely, using the <kbd>dplyr</kbd> package, and then we will discuss how to build regression models and interpret the results using the <kbd>glm</kbd> function.</span></p>
<p><span>For this exercise, we will be using one of the publicly available datasets from IBM Watson, which can be found at <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/">https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/</a>. You can follow this link and download the data file in a CSV format. In order to load this data into your RStudio, you can run the following code:</span></p>
<pre>library(dplyr)<br/>library(ggplot2)<br/><br/># Load data<br/>df &lt;- read.csv(<br/>  file="~/Documents/data-science-for-marketing/ch.3/data/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv", <br/>  header=TRUE, <br/>  sep=","<br/>)</pre>
<p><span>Similar to what we did in <a href="1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml" target="_blank">Chapter 2</a>, <em>Key Performance Indicators and Visualizations</em>, we will first import the <kbd>dplyr</kbd> and <kbd>ggplot2</kbd> packages for data analysis and plotting in the following sections. Using the <kbd>read.csv</kbd> function in R, we can read th</span>e data into a DataFrame<span>. Since this CSV file contains the header in the first row and the fields are separated by commas, we are using the <kbd>header=TRUE</kbd> and <kbd>sep=","</kbd> flags for the correct parsing.</span></p>
<p>The following screenshot shows how the raw data looks in the DataFrame:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-978 image-border" src="assets/bfe00dbe-78f4-45a4-a4aa-2376f7431410.png" style="width:162.50em;height:52.17em;"/></p>
<p>Now that we have loaded the data into a DataFrame, let's start to look at and analyze the data more closely, so that we can better understand the structure of the data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data analysis and visualization</h1>
                </header>
            
            <article>
                
<p><span>Before we dive into regression analysis, let's first take a more detailed look at the data, in order to have a better understanding of what data points we have and what patterns we can see in the data. If you look at the data, you will notice a column named </span><kbd>Response</kbd><span>. It contains information on whether a customer responded to their marketing calls. We will use this field as a measure of customer engagement. For future computations, it will be better to encode this field with numerical values. Let's take a look at the following code:</span></p>
<pre># Encode Response as 0s and 1s<br/>df$Engaged &lt;- as.integer(df$Response) - 1</pre>
<p>As you can see in this code, using the <kbd>as.integer</kbd> function, we are encoding those who did not respond to marketing calls (<kbd>No</kbd>) with a value of <kbd>0</kbd> and those who did respond (<kbd>Yes</kbd>) with a value of <kbd>1</kbd>. Because <kbd>as.integer function</kbd> encodes values to <kbd>1</kbd> and <kbd>2</kbd> by default, we are subtracting the values by <kbd>1</kbd> to encode the response values with zeros and ones. Then, we are creating a new field named <kbd>Engaged</kbd> with these encoded values.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Engagement rate</h1>
                </header>
            
            <article>
                
<p><span>The first thing that we are going to look at is the aggregate engagement rate. This engagement rate is simply the percentage of customers who responded to the marketing calls. Take a look at the following code:</span></p>
<pre>engagementRate &lt;- df %&gt;% <br/>  group_by(Engaged) %&gt;% <br/>  summarise(Count=n()) %&gt;%<br/>  mutate(Percentage=Count/nrow(df)*100.0)</pre>
<p class="mce-root"><span>As you can see in this code, we are grouping by the newly created field, </span><kbd>Engaged</kbd><span>, using the </span><kbd>group_by</kbd><span> function</span><span>. Then, we are counting the number of records or customers in each </span><kbd>Engaged</kbd><span> group </span>with<span> the <kbd>n()</kbd> function. By dividing by the total number of customers in </span>the DataFrame<span>, <kbd>df</kbd>, and multiplying by </span><kbd>100.0</kbd><span>, we get the engagement rate. The results look as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-980 image-border" src="assets/46c83142-7a95-4635-abf9-0bf6c55b1192.png" style="width:19.75em;height:4.92em;"/></p>
<p>To make it easier to read, we can transpose the DataFrame, meaning that we can flip the rows and columns in the DataFrame. You can transpose a DataFrame<span> by using the </span><kbd>t</kbd><span> function in R</span><span>. The code looks as follows:</span></p>
<pre># Transpose<br/>transposed &lt;- t(engagementRate)<br/><br/>colnames(transposed) &lt;- engagementRate$Engaged<br/>transposed &lt;- transposed[-1,]</pre>
<p>The transposed DataFrame appears as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-981 image-border" src="assets/61534cae-683e-4984-ae49-6a1f375555b3.png" style="width:16.58em;height:5.00em;"/></p>
<p><span>As you can see, it is easier to see the total number and percentage of engaged and non-engaged customers by transpos</span>ing the DataFrame<span>. From this data, we can see that about </span>14%<span> of the customers have responded to marketing calls, and the remaining 86% </span><span>of the customers have not responded.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sales channels</h1>
                </header>
            
            <article>
                
<p><span>Now, let's see if we can find any noticeable patterns in sales channels and engagement. We are going to analyze how the engaged and non-engaged customers are distributed among different sales channels. Let's first look at the following code:</span></p>
<pre>salesChannel &lt;- df %&gt;% <br/>  group_by(Engaged, Channel=Sales.Channel) %&gt;% <br/>  summarise(Count=n())</pre>
<p><span>As you can see in this code snippet, we are using the </span><kbd>group_by</kbd><span> function in R to group by the </span><kbd>Sales Channel</kbd><span> and </span><kbd>Engaged</kbd><span> variables. Then, using the <kbd>n()</kbd> function, we will count the number of customers in each group. Once you have run this code, the </span><kbd>salesChannel</kbd><span> </span>DataFrame w<span>ill look as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-984 image-border" src="assets/34a46f66-6a8f-4c2d-aa4c-b8ee50112ab4.png" style="width:18.67em;height:14.67em;"/></p>
<p><span>As you will have noticed from the previous section, there are significantly more customers that are not engaged with the marketing efforts, so it is quite difficult to compare and see the differences in the sales channel distributions between the engaged and non-engaged customers with the raw numbers. To make it easier to differentiate visually, we can build pie charts using the following code:</span></p>
<pre># pie chart<br/>ggplot(salesChannel, aes(x="", y=Count, fill=Channel)) + <br/>  geom_bar(width=1, stat = "identity", position=position_fill()) +<br/>  geom_text(aes(x=1.25, label=Count), position=position_fill(vjust = 0.5)) +<br/>  coord_polar("y") +<br/>  facet_wrap(~Engaged) +<br/>  ggtitle('Sales Channel (0: Not Engaged, 1: Engaged)') +<br/>  theme(<br/>    axis.title.x=element_blank(),<br/>    axis.title.y=element_blank(),<br/>    plot.title=element_text(hjust=0.5),<br/>    legend.position='bottom'<br/>  )</pre>
<p><span>Similar to what we did in <a href="1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml" target="_blank">Chapter 2</a>, <em>Key Performance Indicators and Visualizations</em>, we are using <kbd>ggplot</kbd> to build a chart in R. If you remember that chapter, we can build pie charts by using <kbd>geom_bar</kbd> with <kbd>coord_polar("y")</kbd>. By using <kbd>face_wrap(~Engaged)</kbd>, we can split the pie charts in two: one for non-engaged customers and another for engaged customers. Once you have run this code, you will see the following pie charts, which show the distributions of engaged and non-engaged customers across different sales channels:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-979 image-border" src="assets/8c162b53-d864-4865-93eb-13c38d1fc0a6.png" style="width:38.92em;height:26.42em;"/></p>
<p><span>Compared to the previous data table that shows raw counts of engaged and non-engaged customers in each sales channel, these pie charts can help us to visually see the differences in the distributions more easily. As you can see from these charts, more than half of the engaged customers were from agents, whereas non-engaged customers are more evenly distributed across all four different channels. As you can see from these charts, analyzing and visualizing data can help us to notice interesting patterns in the data, which will further help us when we run regression analysis in the later parts of this chapter.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Total claim amounts</h1>
                </header>
            
            <article>
                
<p><span>The last thing that we are going to look at before we dive into the regression analysis is are the differences in the distributions of</span> <kbd>Total Claim Amount</kbd><span> between the engaged and non-engaged groups. We are going to visualize this by using box plots. Let's first look at how we can build box plots in R:</span></p>
<pre>ggplot(df, aes(x="", y=Total.Claim.Amount)) + <br/>  geom_boxplot() +<br/>  facet_wrap(~Engaged) +<br/>  ylab("Total Claim Amount") +<br/>  xlab("0: Not Engaged, 1: Engaged") +<br/>  ggtitle("Engaged vs. Not Engaged: Total Claim Amount") +<br/>  theme(plot.title=element_text(hjust=0.5))</pre>
<p><span>As you can see in this code, it is quite straightforward to build box plots in R</span><span>. You can simply call the </span><kbd>ggplot</kbd><span> function with <kbd>geom_boxplot</kbd>. A box plot is a great way to visualize the distributions of continuous variables. It shows the min, max, first quartile, median, and third quartile, all in one view. The following box plot shows the distributions of </span><kbd>Total Claim Amount</kbd><span> between the engaged and non-engaged groups:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-995 image-border" src="assets/25b89a8f-f883-400c-812b-6866d25c84b7.png" style="width:36.17em;height:26.08em;"/></p>
<p><span>The central rectangle spans from the first quartile to the third quartile, and the line within the rectangle shows the median. The lower and upper ends of the lines from the rectangle show the minimum and maximum of the distribution, respectively. Another thing that you will notice from these box plots are the dots above the upper end of the line. </span></p>
<p><span>The dots beyond the end of the upper line show the suspected outliers, which are decided based on</span> the IQR<span>. The </span>IQR<span> is simply the range between the first and third quartiles, which is the same as the height of the rectangle in the box plot that spans from the first quartile to the third quartile. The data points that fall </span><kbd>1.5*IQR</kbd><span> above the third quartile or </span><kbd>1.5*IQR</kbd><span> below the first quartile are suspected outliers, and are shown with the dots.</span></p>
<p>Depending on your analysis goals, you might not care about (or you might not want to show) the outliers in box plots. Let's take a look at the following code to see how we can remove those outliers from the box plots:</p>
<pre># without outliers<br/>ggplot(df, aes(x="", y=Total.Claim.Amount)) + <br/>  geom_boxplot(outlier.shape = NA) +<br/>  scale_y_continuous(limits = quantile(df$Total.Claim.Amount, c(0.1, 0.9))) +<br/>  facet_wrap(~Engaged) +<br/>  ylab("Total Claim Amount") +<br/>  xlab("0: Not Engaged, 1: Engaged") +<br/>  ggtitle("Engaged vs. Not Engaged: Total Claim Amount") +<br/>  theme(plot.title=element_text(hjust=0.5))</pre>
<p><span>As you will notice in this code snippet, the only difference between this code and the previous one is <kbd>outlier.shape=NA</kbd> in the <kbd>geom_boxplot</kbd> function. Let's take a look at how the box plots look now:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-988 image-border" src="assets/427c69d6-87c2-4137-bd7d-f9d4d565c7c9.png" style="width:36.92em;height:26.25em;"/></p>
<p>In these plots, we can no longer see the dots beyond the end of the upper line. Depending on what you would like to show and analyze, having outliers in box plots may or may not help.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression analysis</h1>
                </header>
            
            <article>
                
<p><span>So far, we have analyzed the types of fields that we have in the data and how the patterns differ between the engaged group and the non-engaged group. Now, we are going to discuss how to conduct and interpret regression analysis in R, using the </span><kbd>glm</kbd><span> function. We will first build a logistic regression model with continuous variables, and you will learn how to interpret the results. Then, we are going to discuss how to handle categorical variables when fitting regression models in R, and what impact those categorical variables have on the fitted logistic regression model.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous variables</h1>
                </header>
            
            <article>
                
<p><span>In linear regression, including logistic regression, it is straightforward to fit a regression model when the feature variables are continuous, as it just needs to find a linear combination of feature variables with numerical values for estimating the output variable. In order to fit a regression model with continuous variables, let's first take a look at how to get the data types of the columns in an </span><kbd>R</kbd><span> </span>DataFrame. Tak<span>e a look at the following code:</span></p>
<pre># get data types of each column<br/>sapply(df, class)</pre>
<p><span>Using the <kbd>sapply</kbd> function in <kbd>R</kbd>, we can apply the <kbd>class</kbd> function across the columns in a</span> DataFrame, and <span>the <kbd>class</kbd> function tells us the types of data in each column. The results of this code are as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/99e6ea9c-822d-4e6b-8904-853a3b426c2b.png"/></p>
<p><span>Shown in the preceding screenshot, we can easily see which columns have numerical values and which do not. For example, the type of the <kbd>State</kbd> column is <kbd>"factor"</kbd>, which means that the variable is a categorical variable. On the other hand, the type of the <kbd>Customer.Lifetime.Value</kbd> column is <kbd>"numeric"</kbd>, and this means that this variable is a continuous variable with numeric values. Aside from this, we can also use an <kbd>R</kbd> function, <kbd>summary</kbd>, to get the summary statistics for each column of a</span> DataFrame, <span>so that we can see not only the types of each column, but can also take a look at a summary of what the distributions for each column look like. The code is as follows:</span></p>
<pre># summary statistics per column<br/>summary(df)</pre>
<p><span>When you run this code, you will get output that looks as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3fc841d4-179a-4089-b299-b0934b7fd763.png"/></p>
<p><span>In this output, we can easily see a snapshot of the distributions of each column in an <kbd>R</kbd></span> DataFrame. F<span>or example, for the <kbd>State</kbd> variable, we can easily see that there are <kbd>1703</kbd> records or customers from <kbd>Arizona</kbd> and <kbd>3150</kbd> customers from <kbd>California</kbd>. On the other hand, we can easily see that the minimum value for the<kbd>Customer.Lifetime.Value</kbd> variable is <kbd>1898</kbd>, whereas, the mean is <kbd>8005</kbd> and the maximum value is <kbd>83325</kbd>.</span></p>
<p>Given this information from the previous code, we can easily select only the columns with numerical values, by using the following code:</p>
<pre># get numeric columns<br/>continuousDF &lt;- select_if(df, is.numeric)<br/>colnames(continuousDF)</pre>
<p><span>As you can see in this code snippet, we are using the <kbd>select_if</kbd> function, and the arguments for this function are t</span>he DataFrame<span>, <kbd>df</kbd>, and a conditional statement, <kbd>is.numeric</kbd>, to define the type of column that we want to sub-select from the</span> DataFrame. Usin<span>g this function, only the numerical columns in</span> the DataFrame<span>, <kbd>df</kbd>, are selected and stored as a separate variable, named <kbd>continuousDF</kbd>. With the <kbd>colnames</kbd> function, we can see what columns are in the newly crea</span>ted DataFrame<span>, <kbd>continuousDF</kbd>. You should see an output that looks like the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-993 image-border" src="assets/fa4c358f-ccd2-4da8-9e75-a743a319ce02.png" style="width:153.25em;height:18.25em;"/></p>
<p>We are now ready to fit a logistic regression model with continuous variables. Let's take a look at the following code first:</p>
<pre># Fit regression model with continuous variables<br/>logit.fit &lt;- glm(Engaged ~ ., data = continuousDF, family = binomial)</pre>
<p>In R, you can fit regression models by using the <kbd>glm</kbd> function, which stands for <strong>generalized linear models</strong>. The R function <kbd>glm</kbd> can be used for various linear models. By default, the value of the family argument is <kbd>gaussian</kbd>, which tells the algorithm to fit a simple linear regression model. On the other hand, like in our case, if you use <kbd>binomial</kbd> for <kbd>family</kbd>, then it is going to fit a logistic regression model. For more detailed descriptions of the different values that you can use for the <kbd>family</kbd> argument, you can refer to <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html">https://stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html</a>.</p>
<p>The other two arguments that we passed on to the <kbd>glm</kbd> function are <kbd>formula</kbd> and <kbd>data</kbd>. The first argument, <kbd>formula</kbd>, is where you define how you want the model to be fit. The variable on the left side of <kbd>~</kbd> is the output variable, and the one on the right side of <kbd>~</kbd> is the input variable. In our case, we are telling the model to learn how to estimate the output variable, <kbd>Engaged</kbd>, by using all of the other variables as the input variables. If you want to use only a subset of the variables as the input variables, then you can use something like the following for the formula:</p>
<pre>Engaged ~ Income + Customer.Lifetime.Value</pre>
<p>In this formula, we are telling the model to learn how to estimate the output variable, <kbd>Engaged</kbd>, by only using <kbd>Income</kbd> and <kbd>Customer.Lifetime.Value</kbd> as the features. Lastly, the second argument in our <kbd>glm</kbd> function, <kbd>data</kbd>, defines which data to use to train a regression model.</p>
<p>Now that we have a trained logistic regression model, let's take a look at the following code, which shows how we can get the detailed regression analysis results from this model object:</p>
<pre>summary(logit.fit)</pre>
<p>The <kbd>summary</kbd> function in R provides a detailed description of the regression analysis results, which look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-990 image-border" src="assets/6c37619a-21ef-4ffa-b36e-9fc327da669a.png" style="width:33.08em;height:31.50em;"/></p>
<p><span>Let's take a more detailed look at this output. The <kbd>Estimate</kbd> column in the <kbd>Coefficients</kbd> section gives us the computed value for each of the feature coefficients. For example, the coefficient for the <kbd>Income</kbd> variable is <kbd>0.000002042</kbd>, and the coefficient for <kbd>Number.of.Policies</kbd> is <kbd>-0.02443</kbd>. We can also see that the estimated <kbd>Intercept</kbd> value is <kbd>-1.787</kbd>. The column <kbd>z value</kbd> gives us the <em>z</em>-score, which is the number of standard deviations from the mean of the population, and the column <kbd>Pr(&gt;|z|)</kbd> is the <em>p</em>-value, which means how likely it is to observe the relationship between the feature and the output variable by chance. So, the lower the value of <kbd>Pr(&gt;|z|)</kbd> is, the more likely it is that the relationship between the given</span> feature <span>and the</span> output vari<span>able is strong and is not by chance. Typically, <kbd>0.05</kbd> is a good cut-off point for the <em>p</em>-value, and any value less than <kbd>0.05</kbd> signifies a strong relationship between the given feature and the output variable. </span></p>
<p><span>As you can see from the <kbd>Signif. codes</kbd> section under the <kbd>Coefficients</kbd> section in the output, the <kbd>***</kbd></span> symbol<span>, next to the <em>p</em>-value in the <kbd>Coefficients</kbd> section, indicates the strongest relationship with the <em>p</em>-value at <kbd>0</kbd>; <kbd>**</kbd> means that the <em>p</em>-value is less than 0.001; <kbd>*</kbd> means that the <em>p</em>-value is less than <kbd>0.05</kbd>, and so forth. If you look at the regression analysis output again, only three variables, <kbd>Income</kbd>, <kbd>Number.of.Policies</kbd>, and <kbd>Total.Claim.Amount</kbd>, have significant relationships with the output variable, <kbd>Engaged</kbd>, at a <kbd>0.1</kbd> significance level. Also, we can see that <kbd>Income</kbd> and <kbd>Total.Claim.Amount</kbd> are positively correlated with <kbd>Engaged</kbd>, meaning that the higher the income is or the higher the total claim amount is, the more likely that a customer will be engaged with marketing calls. On the other hand, the variable <kbd>Number.of.Policies</kbd> is negatively correlated with <kbd>Engaged</kbd>, which suggests that the higher the number of policies that a customer has, the less likely that the given customer will be engaged with marketing calls.</span></p>
<p><span>As you can see in these examples, you can interpret the regression analysis results quite easily, by looking at the <em>p</em>-values and coefficients of the features from the model output. This is a good way to understand which attributes of customers are significantly and highly correlated with your outcomes of interest.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Categorical variables</h1>
                </header>
            
            <article>
                
<p>As you saw in the case with continuous variables in the previous section, it is quite straightforward to understand the relationships between the input and output variables from the coefficients and <em>p</em>-values. However, it becomes not so straightforward when we introduce categorical variables. Categorical variables often do not have any natural order but, in linear regression, we need the <kbd>input</kbd> variables to have numerical values that signify the orderings or magnitudes of the variables. For example, we cannot easily encode the <kbd>State</kbd> variable in our dataset with certain orders or values. That is why we need to handle categorical variables differently from continuous variables when conducting regression analysis. In R, the <kbd>factor</kbd> function helps you to handle these categorical variables easily when running regression analysis. Take a look at the following code:</p>
<pre># a. Education<br/># Fit regression model with Education factor variables<br/>logit.fit &lt;- glm(Engaged ~ factor(Education), data = df, family = binomial)<br/>summary(logit.fit)</pre>
<p><span>As you can see in this code, we are fitting a logistic regression model with <kbd>Engaged</kbd> as the output variable and the factorized <kbd>Education</kbd> as the input variable. Before we dive deeper into what this means, let's first look at the following regression analysis results:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-994 image-border" src="assets/31ab77a5-aeb4-4ed5-8288-144e0ea82443.png" style="width:38.42em;height:30.92em;"/></p>
<p><span>As you can see in this output, the <kbd>factor</kbd> function created four additional variables: <kbd>factor(Education)College</kbd>, <kbd>factor(Education)Doctor</kbd>, <kbd>factor(Education)High School or Below</kbd>, and <kbd>factor(Education)Master</kbd>. These variables are encoded with <kbd>0</kbd> if the given customer does not belong to the given category, or <kbd>1</kbd> if the given customer belongs to the given category. This way, we can understand the positive or negative relationship between each of the <kbd>Education</kbd> category and the <kbd>output</kbd> variable, <kbd>Engaged</kbd>. For example, the factor variable, <kbd>factor(Education)Doctor</kbd>, has a positive coefficient, which suggests that if a customer has a doctoral degree, then it is more likely that the given customer will be engaged with marketing calls. </span></p>
<p>If you look closely, you will notice that this output does not have a separate factor variable for the <kbd>Bachelor</kbd> category in the <kbd>Education</kbd> variable. This is because <kbd>(Intercept)</kbd> contains the information for the <kbd>Bachelor</kbd> category. If a customer has a bachelor's degree, then all of the other factor variables would have been encoded with <kbd>0</kbd>s. Hence, all of the coefficient values are cancelled out, and only the <kbd>(Intercept)</kbd> value stays. Since the estimated <kbd>(Intercept)</kbd> value is negative, if a customer has a <kbd>Bachelor</kbd> degree, then it is less likely that the given customer will be engaged with marketing calls.</p>
<p class="mce-root"/>
<p><span>Let's take a look at another example:</span></p>
<pre># b. Education + Gender<br/># Fit regression model with Education &amp; Gender variables<br/>logit.fit &lt;- glm(Engaged ~ factor(Education) + factor(Gender), data = df, family = binomial)<br/><br/>summary(logit.fit)</pre>
<p><span>As you can see in this code, we are now fitting a regression model with the <kbd>Education</kbd> and <kbd>Gender</kbd> variables, and the output looks as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/86d48265-182c-4c31-b7e1-755ea30c0488.png" style="width:32.17em;height:26.33em;"/></p>
<p><span>If you look closely at this output, you can only see one additional factor variable, <kbd>factor(Gender)M</kbd>, for male customers, where the data clearly has female customers. This is because the <kbd>Bachelor</kbd> category of the <kbd>Education</kbd> variable and the <kbd>F</kbd> (female) category of the <kbd>Gender</kbd> variable are lumped together as <kbd>(Intercept)</kbd> of this regression model. Thus, the base case, wherein the values of all of the factor variables are <kbd>0</kbd>, is for <kbd>female</kbd> customers with a <kbd>Bachelor</kbd> degree.</span></p>
<p class="mce-root"/>
<p><span>For male customers with a <kbd>Bachelor</kbd> degree, the factor variable <kbd>factor(Gender)M</kbd> will now have a value of <kbd>1</kbd>, and hence, the estimated value for the output variable, <kbd>Engaged</kbd>, will be the value of <kbd>(Intercept)</kbd> plus the coefficient value of <kbd>factor(Gender)M</kbd>.</span></p>
<p><span>As we have discussed so far, we can handle categorical variables by using the <kbd>factor</kbd> function in R. It is essentially the same as creating one separate input variable per category for each of the categorical variables. Using this technique, we can understand how different categories of categorical variables are correlated with the output variable.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Combining continuous and categorical variables</h1>
                </header>
            
            <article>
                
<p><span>The last exercise that we are going to do in this chapter involves combining continuous and categorical variables for our regression analysis. Let's first factorize the two categorical variables, <kbd>Gender</kbd> and <kbd>Education</kbd>, that we discussed in the previous section, and store them i</span>n a DataFrame<span> by using the following code:</span></p>
<pre>continuousDF$Gender &lt;- factor(df$Gender)<br/>continuousDF$Education &lt;- factor(df$Education)</pre>
<p><span>Th</span>e DataFrame<span>, <kbd>continuousDF</kbd>, now contains the following columns:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/51c0514f-23f0-474c-ae49-53136c894319.png" style="width:61.08em;height:7.50em;"/></p>
<p><span>Now, we are going to fit a logistic regression model with both the categorical and continuous variables, using the following code:</span></p>
<pre># Fit regression model with Education &amp; Gender variables<br/>logit.fit &lt;- glm(Engaged ~ ., data = continuousDF, family = binomial)<br/>summary(logit.fit)</pre>
<p><span>You should get an output that looks as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e3a76724-02ca-4dbb-b3db-9cbb77ee9554.png" style="width:34.58em;height:37.83em;"/></p>
<p><span>Let's take a closer look at this output. The <kbd>Total.Claim.Amount</kbd> variables and <kbd>EducationDoctor</kbd> variables are significant at a <kbd>0.05</kbd> significance level, and both of them have positive relationships with the output variable, <kbd>Engaged</kbd>. Hence, the higher the total claim amount is, the more likely that the customer is going to engage with the marketing calls. Also, customers with doctoral degrees are more likely to engage with marketing calls than those with other educational backgrounds. At a <kbd>0.1</kbd> significance level, we can see that <kbd>Income</kbd>, <kbd>Number.of.Policies</kbd>, and <kbd>EducationMaster</kbd> now have significant relationships with the output variable, <kbd>Engaged</kbd>. From looking at this regression analysis output, we can easily see the relationships between the input and output variables, and we can understand which attributes of customers are positively or negatively related to customer engagement with marketing calls.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_infobox">The full code for the R exercise can be found in the repository at <a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.3/R/RegressionAnalysis.R">https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.3/R/RegressionAnalysis.R</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed how to use explanatory analysis to draw insight on customer behavior. We discussed how regression analysis can be used to dive deeper into understanding customer behavior. More specifically, you learned how to use logistic regression to understand what attributes of customers drive higher engagement rates. In Python and R exercises, we employed the descriptive analysis that we covered in <a href="1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml" target="_blank">Chapter 2</a>, <em>Key Performance Indicators and Visualizations</em>, as well as regression analysis for explanatory analysis. We started the exercises by analyzing the data in order to better understand and identify noticeable patterns in the data. While analyzing the data, you learned one additional way to visualize the data, through box plots, using the <kbd>matplotlib</kbd> and <kbd>pandas</kbd> packages in Python and the <kbd>ggplot2</kbd> library in R.</p>
<p>While fitting regression models, we discussed the two different types of variables: continuous and categorical. You learned about the challenges in handling categorical variables when fitting logistic regression models, and how to handle such variables. For Python, we covered two ways of handling categorical variables: the <kbd>factorize</kbd> and <kbd>Categorical</kbd> functions from the <kbd>pandas</kbd> package. For R, we discussed how we can use the <kbd>factor</kbd> function to handle categorical variables when fitting a logistic regression model. With the regression analysis results, we showed how you can interpret the results and relationships between the input and output variables by looking at the coefficients and <em>p</em>-values. By looking at the regression analysis output, we can understand what attributes of customers show significant relationships with customer marketing engagement.</p>
<p>In the next chapter, we are going to expand your knowledge of explanatory analysis. We will analyze what drives conversions after customer engagements. You will also learn about another machine learning algorithm, decision trees, and how to use them for explanatory analysis.</p>


            </article>

            
        </section>
    </body></html>