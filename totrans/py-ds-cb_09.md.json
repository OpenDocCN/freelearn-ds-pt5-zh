["```py\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.grid_search import RandomizedSearchCV\nfrom operator import itemgetter\n\nimport numpy as np\n\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\n            n_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n    return x,y\n```", "```py\ndef build_forest(x,y,x_dev,y_dev):\n    \"\"\"\n    Build a random forest of fully grown trees\n    and evaluate peformance\n    \"\"\"\n    no_trees = 100\n    estimator = RandomForestClassifier(n_estimators=no_trees)\n    estimator.fit(x,y)\n\n    train_predcited = estimator.predict(x)\n    train_score = accuracy_score(y,train_predcited)\n    dev_predicted = estimator.predict(x_dev)\n    dev_score = accuracy_score(y_dev,dev_predicted)\n\n    print \"Training Accuracy = %0.2f Dev Accuracy = %0.2f\"%(train_score,dev_score)\n\ndef search_parameters(x,y,x_dev,y_dev):\n    \"\"\"\n    Search the parameters of random forest algorithm\n    \"\"\"\n    estimator = RandomForestClassifier()\n    no_features = x.shape[1]\n    no_iterations = 20\n    sqr_no_features = int(np.sqrt(no_features))\n\n    parameters = {\"n_estimators\"      : np.random.randint(75,200,no_iterations),\n                 \"criterion\"         : [\"gini\", \"entropy\"],\n                 \"max_features\"      : [sqr_no_features,sqr_no_features*2,sqr_no_features*3,sqr_no_features+10]\n                 }\n\n    grid = RandomizedSearchCV(estimator=estimator,param_distributions=parameters,\\\n    verbose=1, n_iter=no_iterations,random_state=77,n_jobs=-1,cv=5)\n    grid.fit(x,y)\n    print_model_worth(grid,x_dev,y_dev)\n\n    return grid.best_estimator_\n\ndef print_model_worth(grid,x_dev,y_dev):    \n    # Print the goodness of the models\n    # We take the top 5 models\n    scores = sorted(grid.grid_scores_, key=itemgetter(1), reverse=True) [0:5]\n\n    for model_no,score in enumerate(scores):\n        print \"Model %d, Score = %0.3f\"%(model_no+1,score.mean_validation_score)\n        print \"Parameters = {0}\".format(score.parameters)\n    print\n    dev_predicted = grid.predict(x_dev)\n\n    print classification_report(y_dev,dev_predicted)\n```", "```py\nif __name__ == \"__main__\":\n    x,y = get_data()    \n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    build_forest(x_train,y_train,x_dev,y_dev)\n    model = search_parameters(x,y,x_dev,y_dev)\n    get_feature_importance(model)\n```", "```py\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\n            n_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n    return x,y\n```", "```py\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n```", "```py\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n```", "```py\nbuild_forest(x_train,y_train,x_dev,y_dev)\n```", "```py\n    no_trees = 100\n    estimator = RandomForestClassifier(n_estimators=no_trees)\n    estimator.fit(x,y)\n\n    train_predcited = estimator.predict(x)\n    train_score = accuracy_score(y,train_predcited)\n    dev_predicted = estimator.predict(x_dev)\n    dev_score = accuracy_score(y_dev,dev_predicted)\n\n    print \"Training Accuracy = %0.2f Dev Accuracy = %0.2f\"%(train_score,dev_score)\n```", "```py\nparameters = {\"n_estimators\" : np.random.randint(75,200,no_iterations),\n\"criterion\" : [\"gini\", \"entropy\"],\n\"max_features\" : [sqr_no_features,sqr_no_features*2,sqr_no_features*3,sqr_no_features+10]\n}\n```", "```py\nno_iterations = 20\n```", "```py\nsqr_no_features = int(np.sqrt(no_features))\n```", "```py\ngrid = RandomizedSearchCV(estimator=estimator,param_distributions=parameters,\\\nverbose=1, n_iter=no_iterations,random_state=77,n_jobs=-1,cv=5)\n```", "```py\nestimator = RandomForestClassifier()\n```", "```py\ngrid.fit(x,y)\nprint_model_worth(grid,x_dev,y_dev)\n```", "```py\nscores = sorted(grid.grid_scores_, key=itemgetter(1), reverse=True) [0:5]\n```", "```py\nfor model_no,score in enumerate(scores):\nprint \"Model %d, Score = %0.3f\"%(model_no+1,score.mean_validation_score)\nprint \"Parameters = {0}\".format(score.parameters)\nprint\n```", "```py\ndev_predicted = grid.predict(x_dev)\nprint classification_report(y_dev,dev_predicted)\n```", "```py\ndef get_feature_importance(model):\n    feature_importance = model.feature_importances_\n    fm_with_id = [(i,importance) for i,importance in enumerate(feature_importance)]\n    fm_with_id = sorted(fm_with_id, key=itemgetter(1),reverse=True)[0:10]\n    print \"Top 10 Features\"\n    for importance in fm_with_id:\n        print \"Feature %d importance = %0.3f\"%(importance[0],importance[1])\n    print\n```", "```py\n    feature_importance = model.feature_importances_\n    fm_with_id = [(i,importance) for i,importance in enumerate(feature_importance)]\n```", "```py\n    fm_with_id = sorted(fm_with_id, key=itemgetter(1),reverse=True)[0:10]\n```", "```py\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.cross_validation import train_test_split, cross_val_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.grid_search import RandomizedSearchCV\nfrom operator import itemgetter\n\ndef get_data():\n \"\"\"\n Make a sample classification dataset\n Returns : Independent variable y, dependent variable x\n \"\"\"\n no_features = 30\n redundant_features = int(0.1*no_features)\n informative_features = int(0.6*no_features)\n repeated_features = int(0.1*no_features)\n x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\n n_informative = informative_features, n_redundant = redundant_features \\\n ,n_repeated = repeated_features,random_state=7)\n    return x,y\n```", "```py\ndef build_forest(x,y,x_dev,y_dev):\n    \"\"\"\n    Build a Extremely random tress\n    and evaluate peformance\n    \"\"\"\n    no_trees = 100\n    estimator = ExtraTreesClassifier(n_estimators=no_trees,random_state=51)\n    estimator.fit(x,y)\n\n    train_predcited = estimator.predict(x)\n    train_score = accuracy_score(y,train_predcited)\n    dev_predicted = estimator.predict(x_dev)\n    dev_score = accuracy_score(y_dev,dev_predicted)\n\n    print \"Training Accuracy = %0.2f Dev Accuracy = %0.2f\"%(train_score,dev_score)\n    print \"cross validated score\"\n    print cross_val_score(estimator,x_dev,y_dev,cv=5)\n\ndef search_parameters(x,y,x_dev,y_dev):\n    \"\"\"\n    Search the parameters \n    \"\"\"\n    estimator = ExtraTreesClassifier()\n    no_features = x.shape[1]\n    no_iterations = 20\n    sqr_no_features = int(np.sqrt(no_features))\n\n    parameters = {\"n_estimators\"      : np.random.randint(75,200,no_iterations),\n                 \"criterion\"         : [\"gini\", \"entropy\"],\n                 \"max_features\"      : [sqr_no_features,sqr_no_features*2,sqr_no_features*3,sqr_no_features+10]\n                 }\n\n    grid = RandomizedSearchCV(estimator=estimator,param_distributions=parameters,\\\n    verbose=1, n_iter=no_iterations,random_state=77,n_jobs=-1,cv=5)\n    grid.fit(x,y)\n    print_model_worth(grid,x_dev,y_dev)\n\n    return grid.best_estimator_\n```", "```py\nif __name__ == \"__main__\":\n    x,y = get_data()    \n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    build_forest(x_train,y_train,x_dev,y_dev)\n    model = search_parameters(x,y,x_dev,y_dev)\n```", "```py\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\n            n_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n    return x,y\n```", "```py\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n```", "```py\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n```", "```py\nbuild_forest(x_train,y_train,x_dev,y_dev)\n```", "```py\nno_trees = 100\n    estimator = ExtraTreesClassifier(n_estimators=no_trees,random_state=51)\n    estimator.fit(x,y)\n\n    train_predcited = estimator.predict(x)\n    train_score = accuracy_score(y,train_predcited)\n    dev_predicted = estimator.predict(x_dev)\n    dev_score = accuracy_score(y_dev,dev_predicted)\n\n    print \"Training Accuracy = %0.2f Dev Accuracy = %0.2f\"%(train_score,dev_score)\n    print \"cross validated score\"\n    print cross_val_score(estimator,x_dev,y_dev,cv=5)\n```", "```py\n    train_predcited = estimator.predict(x)\n```", "```py\n    train_score = accuracy_score(y,train_predcited)\n    dev_predicted = estimator.predict(x_dev)\n    dev_score = accuracy_score(y_dev,dev_predicted)\n```", "```py\n    print \"Training Accuracy = %0.2f Dev Accuracy = %0.2f\"%(train_score,dev_score)\n```", "```py\ndev_predicted = grid.predict(x_dev)\nprint classification_report(y_dev,dev_predicted)\n```", "```py\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import classification_report\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    no_features = 50\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\n            n_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n    return x,y\n\ndef get_random_subset(iterable,k):\n    subsets = []\n    iteration = 0\n    np.random.shuffle(iterable)\n    subset = 0\n    limit = len(iterable)/k\n    while iteration < limit:\n        if k <= len(iterable):\n            subset = k\n        else:\n            subset = len(iterable)\n        subsets.append(iterable[-subset:])\n        del iterable[-subset:]\n        iteration+=1\n    return subsets\n```", "```py\ndef build_rotationtree_model(x_train,y_train,d,k):\n    models = []\n    r_matrices = []\n    feature_subsets = []\n    for i in range(d):\n        x,_,_,_ = train_test_split(x_train,y_train,test_size=0.3,random_state=7)\n        # Features ids\n        feature_index = range(x.shape[1])\n        # Get subsets of features\n        random_k_subset = get_random_subset(feature_index,k)\n        feature_subsets.append(random_k_subset)\n        # Rotation matrix\n        R_matrix = np.zeros((x.shape[1],x.shape[1]),dtype=float)\n        for each_subset in random_k_subset:\n            pca = PCA()\n            x_subset = x[:,each_subset]\n            pca.fit(x_subset)\n            for ii in range(0,len(pca.components_)):\n                for jj in range(0,len(pca.components_)):\n                    R_matrix[each_subset[ii],each_subset[jj]] = pca.components_[ii,jj]\n\n        x_transformed = x_train.dot(R_matrix)\n\n        model = DecisionTreeClassifier()\n        model.fit(x_transformed,y_train)\n        models.append(model)\n        r_matrices.append(R_matrix)\n    return models,r_matrices,feature_subsets\n\ndef model_worth(models,r_matrices,x,y):\n\n    predicted_ys = []\n    for i,model in enumerate(models):\n        x_mod =  x.dot(r_matrices[i])\n        predicted_y = model.predict(x_mod)\n        predicted_ys.append(predicted_y)\n\n    predicted_matrix = np.asmatrix(predicted_ys)\n    final_prediction = []\n    for i in range(len(y)):\n        pred_from_all_models = np.ravel(predicted_matrix[:,i])\n        non_zero_pred = np.nonzero(pred_from_all_models)[0]  \n        is_one = len(non_zero_pred) > len(models)/2\n        final_prediction.append(is_one)\n\n    print classification_report(y, final_prediction)\n```", "```py\nif __name__ == \"__main__\":\n    x,y = get_data()    \n#    plot_data(x,y)\n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    # Build a bag of models\n    models,r_matrices,features = build_rotationtree_model(x_train,y_train,25,5)\n    model_worth(models,r_matrices,x_train,y_train)\n    model_worth(models,r_matrices,x_dev,y_dev)\n```", "```py\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\n            n_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n    return x,y\n```", "```py\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n```", "```py\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n```", "```py\n    models,r_matrices,features = build_rotationtree_model(x_train,y_train,25,5)\n```", "```py\n    models = []\n    r_matrices = []\n    feature_subsets = []\n```", "```py\n        x,_,_,_ = train_test_split(x_train,y_train,test_size=0.3,random_state=7)\n```", "```py\n        # Features ids\n        feature_index = range(x.shape[1])\n        # Get subsets of features\n        random_k_subset = get_random_subset(feature_index,k)\n        feature_subsets.append(random_k_subset)\n```", "```py\n    np.random.shuffle(iterable)\n```", "```py\n    limit = len(iterable)/k\n    while iteration < limit:\n        if k <= len(iterable):\n            subset = k\n        else:\n            subset = len(iterable)\n        iteration+=1\n```", "```py\n        subsets.append(iterable[-subset:])\n```", "```py\n        del iterable[-subset:]\n```", "```py\n        # Rotation matrix\n        R_matrix = np.zeros((x.shape[1],x.shape[1]),dtype=float)\n```", "```py\n        for each_subset in random_k_subset:\n            pca = PCA()\n            x_subset = x[:,each_subset]\n            pca.fit(x_subset)\n```", "```py\n            for ii in range(0,len(pca.components_)):\n                for jj in range(0,len(pca.components_)):\n                    R_matrix[each_subset[ii],each_subset[jj]] = pca.components_[ii,jj]\n```", "```py\n2,4,6 and 1,3,5\n```", "```py\n                    R_matrix[each_subset[ii],each_subset[jj]] = pca.components_[ii,jj]\n```", "```py\n        x_transformed = x_train.dot(R_matrix)\n```", "```py\nmodel = DecisionTreeClassifier()\nmodel.fit(x_transformed,y_train)\n```", "```py\nmodels.append(model)\nr_matrices.append(R_matrix)\n```", "```py\nmodel_worth(models,r_matrices,x_train,y_train)\nmodel_worth(models,r_matrices,x_dev,y_dev)\n```", "```py\n    for i, model in enumerate(models):\n        x_mod =  x.dot(r_matrices[i])\n        predicted_y = model.predict(x_mod)\n        predicted_ys.append(predicted_y)\n```", "```py\npredicted_matrix = np.asmatrix(predicted_ys)\n```", "```py\n    final_prediction = []\n    for i in range(len(y)):\n        pred_from_all_models = np.ravel(predicted_matrix[:,i])\n        non_zero_pred = np.nonzero(pred_from_all_models)[0]  \n        is_one = len(non_zero_pred) > len(models)/2\n        final_prediction.append(is_one)\n```", "```py\n    print classification_report(y, final_prediction)\n```"]