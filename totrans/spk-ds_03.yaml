- en: Chapter 3.  Introduction to DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To solve any real-world big data analytics problem, access to an efficient and
    scalable computing system is definitely mandatory. However, if the computing power
    is not accessible to the target users in a way that's easy and familiar to them,
    it will barely make any sense. Interactive data analysis gets easier with datasets
    that can be represented as named columns, which was not the case with plain RDDs.
    So, the need for a schema-based approach to represent data in a standardized way
    was the inspiration behind DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: The previous chapter outlined some design aspects of Spark. We learnt how Spark
    enabled distributed data processing on distributed collections of data (RDDs)
    through in-memory computation. It covered most of the points that revealed Spark
    as a fast, efficient, and scalable computing platform. In this chapter, we will
    see how Spark introduced the DataFrame API to make data scientists feel at home
    to carry out their usual data analysis activities with ease.
  prefs: []
  type: TYPE_NORMAL
- en: 'This topic is going to serve as a foundation for many upcoming chapters and
    we strongly recommend you to understand the concepts covered in here very well.
    As a prerequisite for this chapter, a basic understanding of SQL and Spark is
    needed. The topics covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Why DataFrames?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Catalyst optimizer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame basics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RDD versus DataFrame
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From RDDs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: From JSON
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: From JDBC sources
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: From other data sources
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why DataFrames?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from massive, scalable computing capability, big data applications also
    need a mix of a few more features, such as support for a relational system for
    interactive data analysis (simple SQL style), heterogeneous data sources, and
    different storage formats along with different processing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though Spark provided a functional programming API to manipulate distributed
    collections of data, it ended up with tuples (_1, _2, ...). Coding to operate
    on tuples was a little complicated and messy, and was slow at times. So, a standardized
    layer was needed, with the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Named columns with a schema (higher-level abstraction than tuples) so that manipulating
    and tracking them would be easy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functionality to consolidate data from various data sources such as Hive, Parquet,
    SQL Server, PostgreSQL, JSON, and also Spark's native RDDs, and unify them to
    a common format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to take advantage of built-in schemas in special file formats such as
    Avro, CSV, JSON, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for simple relational as well as complex logical operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elimination of the need to define column objects based on domain-specific tasks
    for the ML algorithms to work on, and to serve as a common data layer for all
    algorithms in MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A language-independent entity that can be passed between functions of different
    languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address the above requirements, the DataFrame API was built as one more level
    of abstraction on top of Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Executing SQL queries for basic business needs is very common and almost every
    business does it using some kind of database. So Spark SQL also supports the execution
    of SQL queries written using either a basic SQL syntax or HiveQL. Spark SQL can
    also be used to read data from an existing Hive installation. Apart from these
    plain SQL operations, Spark SQL also addresses some tough problems. Designing
    complex logic through relational queries was cumbersome and almost impossible
    at times. So, Spark SQL was designed to integrate the capabilities of relational
    processing and functional programming so that complex logics can be implemented,
    optimized, and scaled on a distributed computing setup. There are basically three
    ways to interact with Spark SQL, including SQL, the DataFrame API, and the Dataset
    API. The Dataset API is an experimental layer added in Spark 1.6 at the time of
    writing this book so we will limit our discussions to DataFrames only.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL exposes DataFrames as a higher-level API and takes care of all the
    complexities involved and also performs all the background tasks. Through the
    declarative syntax, users can focus on what the program should accomplish and
    not bother about the control flow, which will be taken care of by the Catalyst
    optimizer, built inside Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: The Catalyst optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Catalyst optimizer is the fulcrum of Spark SQL and DataFrame. It is built
    with the functional programming constructs of Scala and has the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Schema inference from various data formats:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark has built-in support for JSON schema inference. Users can just create
    a table out of any JSON file by registering it as a table and simply query it
    with SQL syntaxes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RDDs that are Scala objects; the type information is extracted from Scala's
    type system, that is, **case classes**, if they contain case classes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RDDs that are Python objects; the type information is extracted with a different
    approach. Since Python is not statically typed and follows a dynamic type system,
    the RDD can contain multiple types. So, Spark SQL samples the dataset and infers
    the schema using an algorithm similar to JSON schema inference.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In future, built-in support for CSV, XML, and other formats will be provided.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Built-in support for a wide range of data sources and query federation for
    efficient data import:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark has a built-in mechanism to fetch data from some external data sources
    (for example, JSON, JDBC, Parquet, MySQL, Hive, PostgreSQL, HDFS, S3, and so on)
    through query federation. It can accurately model the sourced data by using out-of-the-box
    SQL data types and other complex data types such as Struct, Union, Array, and
    so on.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It also allows users to source data using the **Data Source API** from the data
    sources that are not supported out of the box (for example, CSV, Avro HBase, Cassandra,
    and so on).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark uses predicate pushdown (pushes filtering or aggregation into external
    storage systems) to optimize data sourcing from external systems and combine them
    to form the data pipeline.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Control and optimization of code generation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization actually happens very late in the entire execution pipeline.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Catalyst is designed to optimize all phases of query execution: analysis, logical
    optimization, physical planning, and code generation to compile parts of queries
    to Java bytecode.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The DataFrame API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Excel spreadsheets like data representation, or output from a database projection
    (select statement's output), the data representation closest to human being had
    always been a set of uniform columns with multiple rows. Such a two-dimensional
    data structure that usually has labelled rows and columns is called a DataFrame
    in some realms, such as R DataFrames and Python's Pandas DataFrames. In a DataFrame,
    typically, a single column has the same kind of data, and rows describe data points
    about that column that mean something together, be it data about a person, a purchase,
    or a baseball game outcome. You can think of it as a matrix, or a spreadsheet,
    or an RDBMS table.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames in R and Pandas are very handy in slicing, reshaping, and analyzing
    data -essential operations in any data wrangling and data analysis workflow. This
    inspired the development of a similar concept on Spark, called DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame basics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DataFrame API was first introduced in Spark 1.3.0, released in March 2015\.
    It is a programming abstraction of Spark SQL for structured and semi-structured
    data processing. It enables developers to harness the power of the DataFrames,
    data structure through Python, Java, Scala, and R. Like RDDs, a Spark DataFrame
    is a distributed collection of records organized into named columns, similar to
    an RDBMS table or the DataFrames of R or Pandas. Unlike RDDs, however, they keep
    track of schemas and facilitate relational operations as well as procedural operations
    such as `map`. Internally, DataFrames store data in columnar format, but construct
    row objects on the fly when required by the procedural functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DataFrame API brings two features with it:'
  prefs: []
  type: TYPE_NORMAL
- en: Built-in support for a variety of data formats such as Parquet, Hive, and JSON.
    Nonetheless, through Spark SQL's external data sources API, DataFrames can access
    a wide array of third-party data sources such as databases and NoSQL stores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A more robust and feature-rich DSL with functions designed for common tasks
    such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational data processing - project, filter, aggregation, join
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: UDFs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The DataFrame API builds on the Spark SQL query optimizer to automatically execute
    code efficiently on a cluster of machines.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs versus DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RDDs and DataFrames are two different types of fault-tolerant and distributed
    data abstractions provided by Spark. They are similar to an extent but greatly
    differ when it comes to implementation. Developers need to have a clear understanding
    of their differences to be able to match their requirements to the right abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: Similarities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the similarities between RDDs and DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: Both are fault-tolerant, partitioned data abstractions in Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both can handle disparate data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both are lazily evaluated (execution happens when an output operation is performed
    on them), thereby having the ability to take the most optimized execution plan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both APIs are available in all four languages: Scala, Python, Java, and R'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the differences between RDDs and DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames are a higher-level abstraction than RDDs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The definition of RDD implies defining a **Directed Acyclic Graph** (**DAG**)
    whereas defining a DataFrame leads to the creation of an **Abstract Syntax Tree**
    (**AST**). An AST will be utilized and optimized by the Spark SQL catalyst engine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RDD is a general data structure abstraction whereas a DataFrame is a specialized
    data structure to deal with two-dimensional, table-like data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DataFrame API is actually SchemaRDD-renamed. The renaming was to signify
    that it is no longer inherited from RDD and to comfort data scientists with a
    familiar name and concept.
  prefs: []
  type: TYPE_NORMAL
- en: Creating DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark DataFrame creation is similar to RDD creation. To get access to the DataFrame
    API, you need SQLContext or HiveContext as an entry point. In this section, we
    are going to demonstrate how to create DataFrames from various data sources, starting
    from basic code examples with in-memory collections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating DataFrames](img/image_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Creating DataFrames from RDDs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following code creates an RDD from a list of colors followed by a collection
    of tuples containing the color name and its length. It creates a DataFrame using
    the `toDF` method to convert the RDD into a DataFrame. The `toDF` method takes
    a list of column labels as an optional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding example, the creation of a DataFrame is similar
    to that of an RDD from a developer's perspective. We created an RDD here and then
    transformed that to tuples which are then sent to the `toDF` method. Note that
    `toDF` takes a list of tuples instead of scalar elements. You need to pass tuples
    even to create single-column DataFrames. Each tuple is akin to a row. You can
    optionally label the columns; otherwise, Spark creates obscure names such as `_1`,
    `_2`. Type inference of the columns happens implicitly.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you already have the data as RDDs, Spark SQL supports two different methods
    for converting existing RDDs into DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: The first method uses reflection to infer the schema of an RDD that contains
    specific types of object, which means you are aware of the schema.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second method is through a programmatic interface that lets you construct
    a schema and then apply it to an existing RDD. While this method is more verbose,
    it allows you to construct DataFrames when the column types are not known until
    runtime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DataFrames from JSON
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JavaScript Object Notation, or JSON, is a language-independent, self-describing,
    lightweight data-exchange format. JSON has become a popular data exchange format
    and has become ubiquitous. In addition to JavaScript and RESTful interfaces, databases
    such as MySQL have accepted JSON as a data type and MongoDB stores all data as
    JSON documents in binary form. Conversion of data to and from JSON is essential
    for any modern data analysis workflow. The Spark DataFrame API lets developers
    convert JSON objects into DataFrames and vice versa. Let''s have a close look
    at the following examples for a better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Spark infers schemas automatically from the keys and creates a DataFrame accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Creating DataFrames from databases using JDBC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark allows developers to create DataFrames from other databases using JDBC,
    provided you ensure that the JDBC driver for the intended database is accessible.
    A JDBC driver is a software component that allows a Java application to interact
    with a database. Different databases require different drivers. Usually, database
    providers such as MySQL supply these driver components to access their databases.
    You have to ensure that you have the right driver for the database you want to
    work with.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example assumes that you already have a MySQL database running
    at the given URL, a table called `people` in the database called `test` with some
    data in it, and valid credentials to log in. There is an additional step of relaunching
    the REPL shell with the appropriate JAR file:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you do not already have the JAR file in your system, download it from the
    MySQL site at the following link: [https://dev.mysql.com/downloads/connector/j/](https://dev.mysql.com/downloads/connector/j/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Creating DataFrames from Apache Parquet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Parquet is an efficient, compressed columnar data representation available
    to any project in the Hadoop ecosystem. Columnar data representations store data
    by column, as opposed to the traditional approach of storing data row by row.
    Use cases that require frequent querying of two to three columns from several
    columns benefit greatly from such an arrangement because columns are stored contiguously
    on the disk and you do not have to read unwanted columns in row-oriented storage.
    Another advantage is in compression. Data in a single column belongs to a single
    type. The values tend to be similar, and sometimes identical. These qualities
    greatly enhance compression and encoding efficiency. Parquet allows compression
    schemes to be specified on a per-column level and allows adding more encodings
    as they are invented and implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark provides support for both reading and writing Parquet files that
    automatically preserves the schema of the original data. The following example
    writes the people data loaded into a DataFrame in the previous example into Parquet
    format and then re-reads it into an RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Creating DataFrames from other data sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark provides built-in support for multiple data sources such as JSON, JDBC,
    HDFS, Parquet, MYSQL, Amazon S3, and so on. In addition, it provides a Data Source
    API that provides a pluggable mechanism for accessing structured data through
    Spark SQL. There are several libraries built on top of this pluggable component,
    for example, CSV, Avro, Cassandra, and MongoDB, to name a few. These libraries
    are not part of the Spark code base. These are built for individual data sources
    and hosted on a community site, Spark packages.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section of this chapter, we learnt many different ways of creating
    DataFrames. In this section, we will focus on various operations that can be performed
    on DataFrames. Developers chain multiple operations to filter, transform, aggregate,
    and sort data in the DataFrames. The underlying Catalyst optimizer ensures efficient
    execution of these operations. These functions you find here are similar to those
    you commonly find in SQL operations on tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Under the hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You already know by now that the DataFrame API is empowered by Spark SQL and
    that the Spark SQL's Catalyst optimizer plays a crucial role in optimizing the
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Though the query is executed lazily, it uses the *catalog* component of Catalyst
    to identify whether the column names used in the program or expressions exist
    in the table being used and the data types are proper, and also takes many other
    such precautionary actions. The advantage to this approach is that, instead of
    waiting till program execution, an error pops up as soon as the user types an
    invalid expression.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explained the motivation behind the development of the DataFrame
    API in Spark and how development in Spark has become easier than ever. We briefly
    covered the design aspect of the DataFrame API and how it is built on top of Spark
    SQL. We discussed various ways of creating DataFrames from different data sources
    such as RDDs, JSON, Parquet, and JDBC. At the end of this chapter, we just gave
    you a heads-up on how to perform operations on DataFrames. We will discuss DataFrame
    operations in the context of data science and machine learning in more detail
    in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how Spark supports unified data access and
    discuss on Dataset and Structured Stream  components in details.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DataFrame reference on the SQL programming guide of Apache Spark official resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks: Introducing DataFrames in Apache Spark for Large Scale Data Science:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html](https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks: From Pandas to Apache Spark''s DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html](https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'API reference guide on Scala for Spark DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html](https://spark.apache.org/docs/1.5.1/api/java/org/apache/spark/sql/DataFrame.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Cloudera blogpost on Parquet - an efficient general-purpose columnar file
    format for Apache Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/](http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
