<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 9. Time Series"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch09" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 9. Time Series</h1></div></div></div><div class="calibre2"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote1" summary="Block quote"><tr class="calibre17"><td class="calibre18"> </td><td class="calibre18"><p class="calibre19"><span class="strong1"><em class="calibre13">"Again time elapsed."</em></span></p></td><td class="calibre18"> </td></tr><tr class="calibre17"><td class="calibre18"> </td><td colspan="2" class="calibre20">--<span class="strong1"><span class="strong1"><em class="calibre13">Carolyn Keene, The Secret of the Old Clock</em></span></span></td></tr></table></div><p class="calibre11">In several of the previous chapters, we saw how we can apply iterative algorithms to identify solutions to complex equations. We first encountered this with gradient descent—both batch and stochastic—but most recently we saw it in community detection in graphs using the graph-parallel model of computation.</p><p class="calibre11">This chapter is about time series data. A time series is any data series that consists of regular observations of a quantity arranged according to the time of their measurement. For many of the techniques in this chapter to work, we require that the intervals between successive observations are all equal. The period between measurements could be monthly in the case of sales figures, daily in the case of rainfall or stock market fluctuations, or by minute in the case of hits to a high-traffic website.</p><p class="calibre11">For us to be able to predict the future values of a time series, we require that the future values are, to some extent, based on the values that have come before. This chapter is therefore also about recursion: how we can build up a sequence where each new value is a function of the previous values. By modeling a real time series as a process where new values are generated in this way, we hope to be able to simulate the sequence forwards in time and produce a forecast.</p><p class="calibre11">Before we get to recursion though, we'll learn how we can adapt a technique we've already encountered—linear regression—to fit curves to time series data.</p><div class="calibre2" title="About the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch09lvl1sec138" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>About the data</h1></div></div></div><p class="calibre11">This chapter will make use of two datasets that come pre-installed with Incanter: the <span class="strong1"><strong class="calibre12">Longley dataset</strong></span>, which <a id="id1209" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>contains <a id="id1210" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data on seven economic variables measured in the United States between the years 1947 to 1962, and the <span class="strong1"><strong class="calibre12">Airline dataset</strong></span>, which contains the monthly total airline passengers from January 1949 to December 1960.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note84" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">You can download the source code for this chapter from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clojuredatascience/ch9-time-series">https://github.com/clojuredatascience/ch9-time-series</a>.</p></div></div><p class="calibre11">The Airline dataset is where we will spend most of our time in this chapter, but first let's look at the<a id="id1211" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> Longley dataset. It contains columns including the gross domestic product (GDP), the number of employed and unemployed people, the population, and the size of the armed forces. It's a classic dataset for analyzing multicollinearity since many of the predictors are themselves correlated. This won't affect the analysis we're performing since we'll only be using one of the predictors at a time.</p><div class="calibre2" title="Loading the Longley data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec191" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Loading the Longley data</h2></div></div></div><p class="calibre11">Since Incanter includes the Longley dataset as part of its sample datasets library, loading the data in is a <a id="id1212" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>simple matter of calling <code class="literal">incanter.datasets/get-dataset</code> with <code class="literal">:longley</code> as the only argument. Once loaded, we'll view the dataset with <code class="literal">incanter.core/view</code>:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-1 []
  (-&gt; (d/get-dataset :longley)
      (i/view)))</pre></div><p class="calibre11">The data should look something like this:</p><div class="mediaobject"><img src="Images/7180OS_09_100.jpg" alt="Loading the Longley data" class="calibre251"/></div><p class="calibre11">The data was <a id="id1213" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>originally published by the National Institute for Standards and Technology as a statistical reference dataset and the column descriptions are listed on their website at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.itl.nist.gov/div898/strd/lls/data/LINKS/i-Longley.shtml">http://www.itl.nist.gov/div898/strd/lls/data/LINKS/i-Longley.shtml</a>. We'll be considering the final three columns <span class="strong1"><strong class="calibre12">x4</strong></span>: the size of the armed forces, <span class="strong1"><strong class="calibre12">x5</strong></span>: the "non-institutional" population aged 14 and over, and <span class="strong1"><strong class="calibre12">x6</strong></span>: the year.</p><p class="calibre11">First, let's see how population changes with respect to time:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-2 []
  (let [data (d/get-dataset :longley)]
    (-&gt; (c/scatter-plot (i/$ :x6 data)
                        (i/$ :x5 data)
                        :x-label "Year"
                        :y-label "Population")
        (i/view))))</pre></div><p class="calibre11">The preceding code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_110.jpg" alt="Loading the Longley data" class="calibre45"/></div><p class="calibre11">The plot of population <a id="id1214" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>against year shows a very clear not-quite-linear relationship. The slight curve suggests that the population growth rate is increasing as the population increases.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note85" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Recall Gibrat's law from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <span class="strong1"><em class="calibre13">Correlation</em></span>, the growth rate of firms is proportional to their size. It's common to see growth curves similar to the preceding one when analyzing populations where Gibrat's law applies: the rate of growth will tend to increase over time.</p></div></div><p class="calibre11">We have seen how to fit a straight line through data with Incanter's linear model. Perhaps surprisingly, it's also possible to fit curves with the <code class="literal">linear-model</code> function.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Fitting curves with a linear model"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch09lvl1sec139" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Fitting curves with a linear model</h1></div></div></div><p class="calibre11">First, let's remind ourselves how we would fit a straight line using Incanter's <code class="literal">linear-model</code> <a id="id1215" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function. We want to extract the <code class="literal">x5</code> and <code class="literal">x6</code> columns from the dataset and apply them (in that order: <code class="literal">x6</code>, the year, is our predictor variable) to the <code class="literal">incanter.stats/linear-model</code> function.</p><div class="calibre2"><pre class="programlisting">(defn ex-9-3 []
  (let [data  (d/get-dataset :longley)
        model (s/linear-model (i/$ :x5 data)
                              (i/$ :x6 data))]
    (println "R-square" (:r-square model))
    (-&gt; (c/scatter-plot (i/$ :x6 data)
                        (i/$ :x5 data)
                        :x-label "Year"
                        :y-label "Population")
        (c/add-lines (i/$ :x6 data)
                     (:fitted model))
        (i/view))))

;; R-square 0.9879</pre></div><p class="calibre11">The preceding code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_120.jpg" alt="Fitting curves with a linear model" class="calibre45"/></div><p class="calibre11">While the straight line is a close fit to the data—generating an <span class="strong1"><em class="calibre13">R</em></span><sup class="calibre42">2</sup> of over 0.98—it doesn't capture the curve of the line. In particular, we can see that points diverge from the line at either end and in the middle of the chart. Our straightforward model has high bias and is systematically under- and over-predicting the population depending on the year. A plot of the residuals would clearly show that the errors are not normally distributed with equal variance.</p><p class="calibre11">The <code class="literal">linear-model</code> function is so-called because it generates models that have a linear relationship <a id="id1216" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with their parameters. However, and perhaps surprisingly, it's capable of generating non-linear predictions, provided we supply it with non-linear features. For example, we could add the year squared as a parameter, in addition to the year. In the following code, we do this using Incanter's <code class="literal">bind-columns</code> function to create a matrix of both of these features:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-4 []
  (let [data  (d/get-dataset :longley)
        x     (i/$ :x6 data)
        xs    (i/bind-columns x (i/sq x))
        model (s/linear-model (i/$ :x5 data) xs)]
    (println "R-square" (:r-square model))
    (-&gt; (c/scatter-plot (i/$ :x6 data)
                        (i/$ :x5 data)
                        :x-label "Year"
                        :y-label "Population")
        (c/add-lines (i/$ :x6 data)
                     (:fitted model))
        (i/view))))

;; 0.9983</pre></div><p class="calibre11">Our <span class="strong1"><em class="calibre13">R</em></span><sup class="calibre42">2</sup> has increased and we get the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_130.jpg" alt="Fitting curves with a linear model" class="calibre45"/></div><p class="calibre11">This appears to <a id="id1217" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>be a much better fit for the data. We can use our model for forecasting by creating a <code class="literal">forecast</code> function that takes the coefficients of the model and returns a function of <code class="literal">x</code>, the year, that multiplies them them by the features we've defined:</p><div class="calibre2"><pre class="programlisting">(defn forecast [coefs]
  (fn [x]
    (first
     (i/mmult (i/trans coefs)
              (i/matrix [1.0 x (i/sq x)])))))</pre></div><p class="calibre11">The coefficients includes a parameter for the bias term, so we're multiplying the coefficients by <span class="strong1"><em class="calibre13">1.0</em></span>, <span class="strong1"><em class="calibre13">x</em></span>, and <span class="strong1"><em class="calibre13">x</em></span><sup class="calibre42">2</sup>.</p><div class="calibre2"><pre class="programlisting">(defn ex-9-5 []
  (let [data  (d/get-dataset :longley)
        x     (i/$ :x6 data)
        xs    (i/bind-columns x (i/sq x))
        model (s/linear-model (i/$ :x5 data) xs)]
    (-&gt; (c/scatter-plot (i/$ :x6 data)
                        (i/$ :x5 data)
                        :x-label "Year"
                        :y-label "Population")
        (c/add-function (forecast (:coefs model))
                        1947 1970)
        (i/view))))</pre></div><p class="calibre11">Next, we extend <a id="id1218" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>our function plot all the way to 1970 to more clearly see the curve of the fitted model as follows:</p><div class="mediaobject"><img src="Images/7180OS_09_140.jpg" alt="Fitting curves with a linear model" class="calibre45"/></div><p class="calibre11">Of course, we are extrapolating beyond the bounds of our data. As discussed back in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <span class="strong1"><em class="calibre13">Correlation</em></span>, it is generally unwise to extrapolate very far. To illustrate why more clearly, let's turn our attention to another column in the Longley dataset, the size of the armed forces: <code class="literal">x6</code>.</p><p class="calibre11">We can plot this in the same way as before:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-6 []
  (let [data (d/get-dataset :longley)]
    (-&gt; (c/scatter-plot (i/$ :x6 data)
                        (i/$ :x4 data)
                        :x-label "Year"
                        :y-label "Size of Armed Forces")
        (i/view))))</pre></div><p class="calibre11">This generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_150.jpg" alt="Fitting curves with a linear model" class="calibre45"/></div><p class="calibre11">This is clearly a <a id="id1219" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>much more complicated series. We can see a sharp increase in the size of the armed forces between 1950 and 1952 followed by a gentle decline. On June 27<sup class="calibre42">th</sup> 1950, President Truman ordered air and naval forces to assist South Korea in what would become known as the Korean War.</p><p class="calibre11">To fit a curve to these data, we'll need to generate higher order polynomials. First, let's construct a <code class="literal">polynomial-forecast</code> function that will create the higher-order features for us automatically, based on a single <code class="literal">x</code> and the highest-degree polynomial to create:</p><div class="calibre2"><pre class="programlisting">(defn polynomial-forecast [coefs degree]
  (fn [x]
    (first
     (i/mmult (i/trans coefs)
              (for [i (range (inc degree))]
                (i/pow x i))))))</pre></div><p class="calibre11">For example, we could train a model all the way up to <span class="strong1"><em class="calibre13">x</em></span>11 using the following code:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-7 []
  (let [data (d/get-dataset :longley)
        degree 11
        x  (s/sweep (i/$ :x6 data))
        xs (reduce i/bind-columns
                   (for [i (range (inc degree))]
                     (i/pow x i)))
        model (s/linear-model (i/$ :x4 data) xs
                              :intercept false)]
    (println "R-square" (:r-square model))
    (-&gt; (c/scatter-plot (i/$ 1 xs) (i/$ :x4 data)
                        :x-label "Distance from Mean (Years)"
                        :y-label "Size of Armed Forces")
        (c/add-function (polynomial-forecast (:coefs model)
                                             degree)
                        -7.5 7.5)
        (i/view))))

;; R-square 0.9755</pre></div><p class="calibre11">The preceding<a id="id1220" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_160.jpg" alt="Fitting curves with a linear model" class="calibre45"/></div><p class="calibre11">The curve fits the data quite well, with an <span class="strong1"><em class="calibre13">R</em></span><sup class="calibre42">2</sup> of over 0.97. However, it should come as no surprise to you now to discover that we are overfitting the data. The model we have built is unlikely to have very much forecasting power. In fact, if we extend the range of the <a id="id1221" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chart to the right, as we do with <code class="literal">ex-9-8</code> to show predictions into the future, we obtain the following:</p><div class="mediaobject"><img src="Images/7180OS_09_170.jpg" alt="Fitting curves with a linear model" class="calibre45"/></div><p class="calibre11">Just two-and-a-half years after the last measured data point, our model is predicting that the military will grow more than 500 percent to over 175,000 people.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Time series decomposition"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch09lvl1sec140" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Time series decomposition</h1></div></div></div><p class="calibre11">One of the problems that <a id="id1222" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we have modeling the military time series is that there is simply not enough data to be able to produce a general model of the process that produced the series. A common way to model a time series is to decompose the series into a number of separate components:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><strong class="calibre12">Trend</strong></span>: Does the series generally increase or decrease over time? Is the trend an exponential curve as we saw with the population?</li><li class="listitem"><span class="strong1"><strong class="calibre12">Seasonality</strong></span>: Does the series exhibit periodic rises and falls at a set number of intervals? For monthly data it is common to observe a period cycle of 12 months.</li><li class="listitem"><span class="strong1"><strong class="calibre12">Cycles</strong></span>: Are there longer-term cycles in the dataset that span multiple seasons? For example, in financial data we might observe multi-year cycles corresponding to periods of expansion and recession.</li></ul></div><p class="calibre11">Another way of specifying the issue with the military data is that there is not enough information to determine whether or not there is a trend, and whether the observed peak is part of a seasonal or cyclic pattern. Although the data appears to trend upwards, it could be that we are looking closely at a cycle that will eventually decline back to where it started.</p><p class="calibre11">One of the datasets that we'll study in this chapter is a classic time series looking at monthly airline passenger numbers from 1949-1960. This dataset is larger and clearly exhibits trend and seasonal components.</p><div class="calibre2" title="Inspecting the airline data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec192" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Inspecting the airline data</h2></div></div></div><p class="calibre11">Like the Longley dataset, the <a id="id1223" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Airline dataset is part of Incanter's datasets library. We load the <code class="literal">incanter.datasets</code> library as <code class="literal">d</code> and <code class="literal">incanter.code</code> as <code class="literal">i</code>.</p><div class="calibre2"><pre class="programlisting">(defn ex-9-9 []
  (-&gt; (d/get-dataset :airline-passengers)
      (i/view)))</pre></div><p class="calibre11">The first few rows should look like this:</p><div class="mediaobject"><img src="Images/7180OS_09_180.jpg" alt="Inspecting the airline data" class="calibre347"/></div><p class="calibre11">When analyzing <a id="id1224" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>time series, it's important that the data is ordered sequentially in time. This data is ordered by year and by month. All the January data is followed by all the February data, and so on. To proceed further, we'll need to convert the year and month columns into <a id="id1225" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a single column we can sort by. For this, we'll use the <code class="literal">clj-time</code> library (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clj-time/clj-time">https://github.com/clj-time/clj-time</a>) once again.</p><div class="calibre2" title="Visualizing the airline data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch09lvl3sec44" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Visualizing the airline data</h3></div></div></div><p class="calibre11">When parsing<a id="id1226" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> times previously in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <span class="strong1"><em class="calibre13">Correlation</em></span>, we were able to take advantage of the fact that the string representation of the time was a default format that clj-time understood. Clj-time is not able to automatically infer all time representations of course. Particularly problematic is the difference between the <span class="strong1"><em class="calibre13">mm/dd/yyyy</em></span> American format and the <span class="strong1"><em class="calibre13">dd/mm/yyyy</em></span> favored by most of the rest of the world. The <code class="literal">clj-time.format </code>namespace provides a <code class="literal">parse</code> function that allows us to pass a format string instructing the library how it should interpret the string. We're including the <code class="literal">format</code> namespace as <code class="literal">tf</code> in the following code and specifying that our time will be expressed in the format <code class="literal">"MMM YYYY"</code>.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title8"><a id="note86" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">A list of formatter strings used by clj-time is available at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.joda.org/joda-time/key_format.html">http://www.joda.org/joda-time/key_format.html</a>.</p></div></div><p class="calibre11">In other words, three characters of "month" followed by four characters of "year".</p><div class="calibre2"><pre class="programlisting">(def time-format
  (tf/formatter "MMM YYYY"))

(defn to-time [month year]
  (tf/parse time-format (str month " " year)))</pre></div><p class="calibre11">With the earlier functions in place we can parse our year and month columns into a single time, order them sequentially, and extract the passenger numbers:</p><div class="calibre2"><pre class="programlisting">(defn airline-passengers []
  (-&gt;&gt; (d/get-dataset :airline-passengers)
       (i/add-derived-column :time [:month :year] to-time)
       (i/$order :time :asc)
       (i/$ :passengers)))</pre></div><p class="calibre11">The result is a <a id="id1227" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sequence of numbers representing the passenger count in order of ascending time. Let's visualize this as a line chart now:</p><div class="calibre2"><pre class="programlisting">(defn timeseries-plot [series]
  (-&gt; (c/xy-plot (range (count series)) series
               :x-label "Time"
               :y-label "Value")
      (i/view)))

(defn ex-9-10 []
  (-&gt; (airline-passengers)
      (timeseries-plot)))</pre></div><p class="calibre11">The preceding code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_190.jpg" alt="Visualizing the airline data" class="calibre45"/></div><p class="calibre11">You can see how the data has a pronounced seasonal pattern (repeating every 12 months), an upward trend, and a gentle growth curve.</p><p class="calibre11">The variance to<a id="id1228" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the right of the chart is greater than the variance to the left, so we say that the data is exhibiting some <a id="id1229" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">heteroscedasticity</strong></span>. We'll want to remove the increase in variance and also the upward trend from the dataset. This will yield a time series which is <a id="id1230" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">stationary</strong></span>.</p></div></div><div class="calibre2" title="Stationarity"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec193" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Stationarity</h2></div></div></div><p class="calibre11">A stationary time series is <a id="id1231" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>one whose statistical properties are constant in time. Most statistical forecasting <a id="id1232" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>methodologies assume the series has been transformed to be stationary. A prediction is made much easier with a stationary time series: we assume the statistical properties of the series will be the same in the future as they have been in the past. To remove both the increasing variance and the growth curve from the airline data, we can simply take the logarithm of the passenger numbers:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-11 []
  (-&gt; (airline-passengers)
      (i/log)
      (timeseries-plot)))</pre></div><p class="calibre11">This generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_200.jpg" alt="Stationarity" class="calibre45"/></div><p class="calibre11">The effect of taking the logarithm is twofold. Firstly, the heteroscedasticity evident in the initial chart has been removed. Secondly, the exponential growth curve has been reduced to a<a id="id1233" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> straight line.</p><p class="calibre11">This has made the data <a id="id1234" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>much easier to work with but we still have a trend, also known as <span class="strong1"><strong class="calibre12">drift</strong></span>, in the series. To get a truly stationary time series, we'll want to stabilize the mean as well. There are several ways to do this.</p></div><div class="calibre2" title="De-trending and differencing"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec194" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>De-trending and differencing</h2></div></div></div><p class="calibre11">The first method is de-trending the series. After taking the logarithm, the airline dataset contains a <a id="id1235" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>very strong<a id="id1236" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> linear trend. We could fit a linear model to this data and then plot the residuals:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-12 []
  (let [data (i/log (airline-passengers))
        xs   (range (count data))
        model (s/linear-model data xs)]
    (-&gt; (c/xy-plot xs (:residuals model)
                   :x-label "Time"
                   :y-label "Residual")
        (i/view))))</pre></div><p class="calibre11">This generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_210.jpg" alt="De-trending and differencing" class="calibre45"/></div><p class="calibre11">The residual plot shows a series whose mean is much more stable than the original series and the upward trend has been entirely removed. Unfortunately, though, the residuals don't appear to be quite normally distributed around the new mean. In particular there appears to be a "hump" in the middle of the chart. This suggests that our linear model is not performing ideally on the airline data. We could fit a curve to the data like we did at the beginning of the chapter, but let's instead look at another method of making<a id="id1237" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> time series stationary.</p><p class="calibre11">The second method is <a id="id1238" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>differencing. If we subtract the value of the directly preceding point from each point in the time series, we'll obtain a new time series (one data point shorter) that contains only the differences between successive points.</p><div class="calibre2"><pre class="programlisting">(defn difference [series]
  (map - (drop 1 series) series))

(defn ex-9-13 []
  (-&gt; (airline-passengers)
      (i/log)
      (difference)
      (timeseries-plot)))</pre></div><p class="calibre11">We can see the effect in the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_220.jpg" alt="De-trending and differencing" class="calibre45"/></div><p class="calibre11">Notice how the upward trend has been replaced with a series of fluctuations around a constant mean value. The mean is slightly above zero, corresponding to an increased propensity for differences to be positive and leading to the upward trend we observe.</p><p class="calibre11">Both techniques<a id="id1239" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> aim to result in a <a id="id1240" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>series whose mean is constant. In some cases, it may be necessary to difference the series more than once, or to apply differencing after de-trending to obtain a series which a truly a stable mean. Some drift is still evident in the series after de-trending, for example, so we'll use the differenced data for the rest of this chapter.</p><p class="calibre11">Before moving on to discuss how to model such time series for forecasting, let's take a detour to think about what a time series is, and how we might model a time series as a recursive process.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Discrete time models"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch09lvl1sec141" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Discrete time models</h1></div></div></div><p class="calibre11">Discrete time models, such as the ones we have been looking at so far, separate time into slices at<a id="id1241" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> regular intervals. For us to be able to predict future values of time slices, we assume that they are dependent on past slices.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note87" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Time series can also be analyzed with respect to frequency rather than time. We won't discuss frequency domain analysis in this chapter but the book's <a id="id1242" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>wiki at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://wiki.clojuredatascience.com">http://wiki.clojuredatascience.com</a> contains links to further resources.</p></div></div><p class="calibre11">In the following, let <span class="strong1"><em class="calibre13">y</em></span><sub class="calibre25">t</sub> denote the value of an observation at time <span class="strong1"><em class="calibre13">t</em></span>. The simplest time series possible would be one where the value of each time slice is the same as the one directly preceding it. The predictor for such a series would be:</p><div class="mediaobject"><img src="Images/7180OS_09_01.jpg" alt="Discrete time models" class="calibre348"/></div><p class="calibre11">This is to say that the prediction at time <span class="strong1"><em class="calibre13">t + 1</em></span> given <span class="strong1"><em class="calibre13">t</em></span> is equal to the observed value at time <span class="strong1"><em class="calibre13">t</em></span>. Notice that this definition is recursive: the value at time <span class="strong1"><em class="calibre13">t</em></span> depends on the value at <span class="strong1"><em class="calibre13">t - 1</em></span>. The value at <span class="strong1"><em class="calibre13">t - 1</em></span> depends on the value at <span class="strong1"><em class="calibre13">t - 2</em></span>, and so on.</p><p class="calibre11">We could model this "constant" time series as a lazy sequence in Clojure, where each value in the sequence is a constant value:</p><div class="calibre2"><pre class="programlisting">(defn constant-series [y]
  (cons y (lazy-seq (constant-series y))))

(defn ex-9-14 []
  (take 5 (constant-series 42)))

;; (42 42 42 42 42)</pre></div><p class="calibre11">Notice how the <a id="id1243" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>definition of <code class="literal">constant-series</code> contains a reference to itself. This is a recursive function definition that creates an infinite lazy sequence from which we can consume values.</p><p class="calibre11">The next time slice, at time <span class="strong1"><em class="calibre13">t + 1</em></span>, the actual value is observed to be <span class="strong1"><em class="calibre13">y</em></span><sub class="calibre25">t+1</sub>. If this value and our predicted value <span class="inlinemediaobject"><img src="Images/7180OS_09_02.jpg" alt="Discrete time models" class="calibre349"/></span> differ, then we can compute this difference as the error of our prediction:</p><div class="mediaobject"><img src="Images/7180OS_09_03.jpg" alt="Discrete time models" class="calibre350"/></div><p class="calibre11">By combining the two previous equations we obtain the stochastic model for a time series.</p><div class="mediaobject"><img src="Images/7180OS_09_04.jpg" alt="Discrete time models" class="calibre351"/></div><p class="calibre11">In other words, the value at the current time slice is the value at the previous time slice, plus some error.</p><div class="calibre2" title="Random walks"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec195" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Random walks</h2></div></div></div><p class="calibre11">One of the simplest <a id="id1244" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>stochastic processes is the random walk. Let's extend our <code class="literal">constant-series</code> into <a id="id1245" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a <code class="literal">random-walk</code> process. We'll want our errors to be normally distributed with a zero mean and constant variance. Let's simulate random noise with a call to Incanter's <code class="literal">stats/sample-normal</code> function.</p><div class="calibre2"><pre class="programlisting">(defn random-walk [y]
  (let [e (s/sample-normal 1)
        y (+ y e)]
    (cons y (lazy-seq (random-walk y)))))

(defn ex-9-15 []
  (-&gt;&gt; (random-walk 0)
       (take 50)
       (timeseries-plot)))</pre></div><p class="calibre11">You'll get a different result, of course, but it should look similar to the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_230.jpg" alt="Random walks" class="calibre45"/></div><p class="calibre11">The random walk <a id="id1246" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>model is <a id="id1247" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>very often seen in finance and econometrics.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note88" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The term <span class="strong1"><em class="calibre13">random walk</em></span> was first introduced by Karl Pearson in 1905. Many processes—from fluctuating stock prices to the path traced by a molecule as it travels in a gas—can be modeled as simple random walks. In 1973, the Princeton economist Burton Gordon Malkiel argued in his book <span class="strong1"><em class="calibre13">A Random Walk Down Wall Street</em></span> that stock prices evolve according to a random walk as well.</p></div></div><p class="calibre11">The random walk is not entirely unpredictable. Although the difference between each point and the next is governed by a random process, the variance of that difference is <a id="id1248" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>constant. This <a id="id1249" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>means that we can estimate confidence intervals for the magnitude of each step. However, since the mean is zero we cannot say with any confidence whether the difference will be positive or negative relative to the current value.</p></div><div class="calibre2" title="Autoregressive models"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec196" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Autoregressive models</h2></div></div></div><p class="calibre11">We've seen already in this chapter how to use a linear model to make a prediction based on a linear combination of predictors. In an autoregressive model we forecast the variable <a id="id1250" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of interest by using a linear combination of the past values of the variable.</p><p class="calibre11">The <a id="id1251" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>autoregressive model regresses the predictor against itself. In order to see how this works in practice, let's look at the following code:</p><div class="calibre2"><pre class="programlisting">(defn ar [ys coefs sigma]
  (let [e (s/sample-normal 1 :sd sigma)
        y (apply + e (map * ys coefs))]
    (cons y (lazy-seq
             (ar (cons y ys) coefs sigma)))))</pre></div><p class="calibre11">This shares much in common with the random walk recursive definition that we encountered a few pages previously. This time, however, we're generating each new <code class="literal">y</code> as a product of previous <code class="literal">ys</code> and the <code class="literal">coefs</code>.</p><p class="calibre11">We can generate an autoregressive series with a call to our new <code class="literal">ar</code> function, passing the previous <code class="literal">ys</code> and the coefficients of the autoregressive model:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-16 []
  (-&gt;&gt; (ar [1] [2] 0)
       (take 10)
       (timeseries-plot)))</pre></div><p class="calibre11">This generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_240.jpg" alt="Autoregressive models" class="calibre45"/></div><p class="calibre11">By taking an initial <a id="id1252" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>value of 1.0 and a coefficient of 2.0, with zero noise, we're creating an exponential <a id="id1253" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>growth curve. Each time step in the series is a power of two.</p><p class="calibre11">The autoregressive series is said to be <span class="strong1"><em class="calibre13">autocorrelated</em></span>. In other words, each point is linearly correlated to its preceding points. In the earlier case, this is simply twice the preceding value. The quantity of coefficients is said to be the order of the autocorrelation model and is often denoted by the letter <span class="strong1"><em class="calibre13">p</em></span>. The preceding example is therefore an autoregressive process with <span class="strong1"><em class="calibre13">p=1</em></span>, or an <span class="strong1"><em class="calibre13">AR(1)</em></span> process.</p><p class="calibre11">More intricate autoregressive series can be generated by increasing <span class="strong1"><em class="calibre13">p</em></span>.</p><div class="calibre2"><pre class="programlisting">(defn ex-9-17 []
  (let [init (s/sample-normal 5)]
    (-&gt;&gt; (ar init [0 0 0 0 1] 0)
         (take 30)
         (timeseries-plot))))</pre></div><p class="calibre11">For example, the previous code generates an autoregressive time series of order 5, or an <span class="strong1"><em class="calibre13">AR(5)</em></span> series. The <a id="id1254" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>effect is <a id="id1255" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>visible in the series as a regular cycle with a period of 5 points.</p><div class="mediaobject"><img src="Images/7180OS_09_250.jpg" alt="Autoregressive models" class="calibre45"/></div><p class="calibre11">We can combine the autoregressive model together with some noise to introduce a component of the random walk we saw previously. Let's increase sigma to 0.2:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-18 []
  (let [init (s/sample-normal 5)]
    (-&gt;&gt; (ar init [0 0 0 0 1] 0.2)
         (take 30)
         (timeseries-plot))))</pre></div><p class="calibre11">This generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_260.jpg" alt="Autoregressive models" class="calibre45"/></div><p class="calibre11">Notice how the <a id="id1256" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>characteristic "seasonal" cycle every five points is preserved, but has been combined with an<a id="id1257" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> element of noise too. Although this is simulated data, this simple model is beginning to approach the sort of series that regularly appears in time series analysis.</p><p class="calibre11">The general equation for an AR model of one lag is given by:</p><div class="mediaobject"><img src="Images/7180OS_09_05.jpg" alt="Autoregressive models" class="calibre352"/></div><p class="calibre11">where <span class="strong1"><em class="calibre13">c</em></span> is some constant, <span class="strong1"><em class="calibre13">ε</em></span><sub class="calibre25">t</sub> is the error, <span class="strong1"><em class="calibre13">y</em></span><sub class="calibre25">t-1</sub> is the value of the series at the previous time step, and <span class="strong1"><em class="calibre13">φ</em></span><sub class="calibre25">1</sub> is the coefficient denoted by the Greek symbol <span class="strong1"><em class="calibre13">phi</em></span>. More generally, the equation for an autoregressive model up to <span class="strong1"><em class="calibre13">p</em></span> lags is given by:</p><div class="mediaobject"><img src="Images/7180OS_09_06.jpg" alt="Autoregressive models" class="calibre353"/></div><p class="calibre11">Since our series are stationary, we have omitted the constant term <span class="strong1"><em class="calibre13">c</em></span> in the code.</p></div><div class="calibre2" title="Determining autocorrelation in AR models"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec197" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Determining autocorrelation in AR models</h2></div></div></div><p class="calibre11">Just as linear regression can establish a (linear) correlation between multiple independent variables, autoregression can establish a correlation between a variable and itself at <a id="id1258" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>different<a id="id1259" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> points in time.</p><p class="calibre11">Just as in linear regression we sought to establish correlation between the predictors and the response variable, so in time series analysis we seek to establish an autocorrelation with the time series and itself at a certain number of lags. Knowing the number of lags for which autocorrelation exists allows us to calculate the order of the autoregressive model.</p><p class="calibre11">It follows that we want to study the autocorrelation of the series at different lags. For example, a lag of zero will mean that we compare each point with itself (an autocorrelation of 1.0). A lag of 1 will mean that we compare each point with the directly preceding point. The <a id="id1260" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">autocorrelation function</strong></span> (<span class="strong1"><strong class="calibre12">ACF</strong></span>) is a linear dependence between a dataset and itself with a given lag. The ACF is therefore parameterized by the lag, <span class="strong1"><em class="calibre13">k</em></span>.</p><div class="mediaobject"><img src="Images/7180OS_09_07.jpg" alt="Determining autocorrelation in AR models" class="calibre354"/></div><p class="calibre11">Incanter contains an <code class="literal">auto-correlation</code> function that will return the autocorrelation for a given sequence and lag. However, we're defining our own <code class="literal">autocorrelation</code> function that will return the <code class="literal">autocorrelation</code> for a sequence of lags:</p><div class="calibre2"><pre class="programlisting">(defn autocorrelation* [series variance n k]
  (let [lag-product (-&gt;&gt; (drop k series)
                         (map * series)
                         (i/sum))]
    (cons (/ lag-product variance n)
          (lazy-seq
           (autocorrelation* series variance n (inc k))))))

(defn autocorrelation [series]
  (autocorrelation* (s/sweep series)
                    (s/variance series)
                    (dec (count series)) 0))</pre></div><p class="calibre11">Before calculating the autocorrelation, we use <code class="literal">sweep</code> function of <code class="literal">incanter.stats</code> to remove the mean from the series. This means that we can simply multiply the values of the series together with the values at lag <span class="strong1"><em class="calibre13">k</em></span> to determine whether they have a tendency to vary together. If they do, their products will be positive; if not, their products will be negative.</p><p class="calibre11">This function returns an infinite lazy sequence of autocorrelation values corresponding to the autocorrelation of lags <span class="strong1"><em class="calibre13">0...k</em></span>. Let's define a function for plotting these values as a <a id="id1261" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>bar chart. As with the <code class="literal">timeseries-plot</code>, this function will accept an ordered sequence of <a id="id1262" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>values:</p><div class="calibre2"><pre class="programlisting">(defn bar-plot [ys]
  (let [xs (range (count ys))]
    (-&gt; (c/bar-chart xs ys
                     :x-label "Lag"
                     :y-label "Value")
        (i/view))))

(defn ex-9-19 []
  (let [init (s/sample-normal 5)
        coefs [0 0 0 0 1]]
    (-&gt;&gt; (ar init coefs 0.2)
         (take 100)
         (autocorrelation)
         (take 15)
         (bar-plot))))</pre></div><p class="calibre11">This generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_270.jpg" alt="Determining autocorrelation in AR models" class="calibre45"/></div><p class="calibre11">The peaks every 5 lags are consistent with our <span class="strong1"><em class="calibre13">AR(5)</em></span> series generator. They diminish <a id="id1263" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>over time <a id="id1264" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>as noise interferes with the signal and decreases the measured autocorrelation.</p></div><div class="calibre2" title="Moving-average models"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec198" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Moving-average models</h2></div></div></div><p class="calibre11">An assumption of AR models is that noise is random with constant mean and variance. Our recursive AR function sampled values from the normal distribution to generate noise that satisfied these assumptions. In an AR process, therefore, the noise terms are <span class="strong1"><em class="calibre13">uncorrelated</em></span> with each other.</p><p class="calibre11">In some<a id="id1265" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> processes, though, the noise terms themselves are not uncorrelated. For an example of this <a id="id1266" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>consider a time series that reports the daily number of barbeques sold. We might observe peaks every 7 days corresponding to customers' increased likelihood of buying barbeques at the weekend. Occasionally, we might observe a period of several weeks where the total sales are down, and other periods of several weeks where the sales are correspondingly up. We might reason that this is due to the weather, with poor sales corresponding to a period of cold or rainy weather and good sales corresponding to a period of favorable weather. Whatever the cause, it will appear in our data as a pronounced shift in the mean value of the series. Series that exhibit this behavior are called <span class="strong1"><strong class="calibre12">moving-average</strong></span> (<span class="strong1"><strong class="calibre12">MA</strong></span>), models.</p><p class="calibre11">A first-order moving-average model, denoted by <span class="strong1"><em class="calibre13">MA(1)</em></span>, is:</p><div class="mediaobject"><img src="Images/7180OS_09_08.jpg" alt="Moving-average models" class="calibre355"/></div><p class="calibre11">where <span class="strong1"><em class="calibre13">μ</em></span> is the mean of the series, <span class="strong1"><em class="calibre13">ε<sub class="calibre25">t</sub></em></span> are the noise values, and <span class="strong1"><em class="calibre13">θ<sub class="calibre25">1</sub></em></span> is the parameter to the model. More generally for <span class="strong1"><em class="calibre13">q</em></span> terms the MA model is given by:</p><div class="mediaobject"><img src="Images/7180OS_09_09.jpg" alt="Moving-average models" class="calibre356"/></div><p class="calibre11">Thus, an MA model is conceptually a linear regression of the current value of the series against current and previous (unobserved) white noise error terms or random shocks. The error terms at each point are assumed to be mutually independent and come from the same (usually normal) distribution with zero mean and constant variance.</p><p class="calibre11">In MA models, we make the assumption that the noise values themselves are autocorrelated. We can model it like this:</p><div class="calibre2"><pre class="programlisting">(defn ma [es coefs sigma]
  (let [e (s/sample-normal 1 :sd sigma)
        y (apply + e (map * es coefs))]
    (cons y (lazy-seq
             (ma (cons e es) coefs sigma)))))</pre></div><p class="calibre11">Here, <code class="literal">es</code> are the<a id="id1267" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> previous <a id="id1268" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>errors, <code class="literal">coefs</code> are the parameters to the MA model, and <code class="literal">sigma</code> is the standard deviation of the errors.</p><p class="calibre11">Notice how the function differs from the <code class="literal">ar</code> function previously defined. Instead of retaining a sequence of the previous <span class="strong1"><em class="calibre13">ys</em></span>, we retain a sequence of the previous <span class="strong1"><em class="calibre13">es</em></span>. Let's see what sort of series an MA model generates:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-20 []
  (let [init (s/sample-normal 5)
        coefs [0 0 0 0 1]]
    (-&gt;&gt; (ma init coefs 0.5)
         (take 100)
         (timeseries-plot))))</pre></div><p class="calibre11">This generates a graph similar to the following:</p><div class="mediaobject"><img src="Images/7180OS_09_280.jpg" alt="Moving-average models" class="calibre45"/></div><p class="calibre11">You can see that the chart for an MA lacks the obvious repetition of the AR model. Viewed over a longer series of points, though, you can see how it reintroduces <span class="strong1"><em class="calibre13">drift</em></span> into the model as<a id="id1269" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the <a id="id1270" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>reverberations of one random shock are perpetuated in a new temporary mean.</p></div><div class="calibre2" title="Determining autocorrelation in MA models"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec199" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Determining autocorrelation in MA models</h2></div></div></div><p class="calibre11">You may <a id="id1271" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>wonder if an autocorrelation plot would help identify an MA process. Let's plot that now. An MA model can be harder to spot, so we'll generate more points before plotting the autocorrelation.</p><div class="calibre2"><pre class="programlisting">(defn ex-9-21 []
  (let [init (s/sample-normal 5)
        coefs [0 0 0 0 1]]
    (-&gt;&gt; (ma init coefs 0.2)
         (take 5000)
         (autocorrelation)
         (take 15)
         (bar-plot))))</pre></div><p class="calibre11">This generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_290.jpg" alt="Determining autocorrelation in MA models" class="calibre45"/></div><p class="calibre11">You can see on the preceding chart how it clearly shows the order of the MA process with a pronounced peak at lag 5. Notice though that, unlike the autoregressive process, there is no<a id="id1272" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> recurring peak every 5 lags. It's a feature of the MA process that, since the process introduces drift into the mean, autocorrelation for the other lags is greatly diminished.</p></div><div class="calibre2" title="Combining the AR and MA models"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec200" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Combining the AR and MA models</h2></div></div></div><p class="calibre11">The AR and MA models that we've been considering so far this chapter are two different but <a id="id1273" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>closely <a id="id1274" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>related ways of generating autocorrelated time series. They are not mutually exclusive, though, and when trying to model real time series you'll often encounter situations where the series appears to be a mixture of both.</p><div class="mediaobject"><img src="Images/7180OS_09_10.jpg" alt="Combining the AR and MA models" class="calibre357"/></div><p class="calibre11">We can combine both AR and MA processes into a single ARMA model, with two sets of coefficients: those of the autoregressive model and those of the moving-average model. The number of coefficients for each model need not be identical, and by convention the order of the AR model is identified by <span class="strong1"><em class="calibre13">p</em></span> and the order of the MA model identified by <span class="strong1"><em class="calibre13">q</em></span>.</p><div class="calibre2"><pre class="programlisting">(defn arma [ys es ps qs sigma]
  (let [e  (s/sample-normal 1 :sd sigma)
        ar (apply + (map * ys ps))
        ma (apply + (map * es qs))
        y  (+ ar ma e)]
    (cons y (lazy-seq
                (arma (cons y ys)
                      (cons e es)
                      ps qs sigma)))))</pre></div><p class="calibre11">Let's plot a longer series of points to see what sort of structure emerges:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-22 []
  (let [ys (s/sample-normal 10 :sd 1.0)
        es (s/sample-normal 10 :sd 0.2)
        ps [0 0 0 0.3 0.5]
        qs [0.2 0.8]]
    (-&gt;&gt; (arma ys es ps qs 0.2)
         (take 500)
         (timeseries-plot))))</pre></div><p class="calibre11">Notice how we're specifying a different number of parameters for the AR and MA portions of the model: 5 parameters for the AR and 2 parameters for the MA model. This is referred to as an <span class="strong1"><em class="calibre13">ARMA(5,2)</em></span> model.</p><div class="mediaobject"><img src="Images/7180OS_09_300.jpg" alt="Combining the AR and MA models" class="calibre45"/></div><p class="calibre11">The plot of<a id="id1275" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the earlier <a id="id1276" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>ARMA model over a longer series of points allows the effect of the MA terms to become visible. At this scale we can't see the effect of the AR component, so let's run the series though an autocorrelation plot as before:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-23 []
  (let [ys (s/sample-normal 10 :sd 1.0)
        es (s/sample-normal 10 :sd 0.2)
        ps [0 0 0 0.3 0.5]
        qs [0.2 0.8]]
    (-&gt;&gt; (arma ys es ps qs 0.2)
         (take 500)
         (autocorrelation)
         (take 15)
         (bar-plot))))</pre></div><p class="calibre11">You should see a chart similar to the following:</p><div class="mediaobject"><img src="Images/7180OS_09_310.jpg" alt="Combining the AR and MA models" class="calibre45"/></div><p class="calibre11">Far from<a id="id1277" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> making the<a id="id1278" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> order of the series clearer, with more data and both AR and MA components in the series the ACF plot is not very useful and quite unlike the strikingly clear autocorrelation plots that we have been looking at so far. The autocorrelation decays slowly to zero making it impossible to determine the order of the AR and MA processes, even though we've provided it with a large quantity of data.</p><p class="calibre11">The reason for this is that the MA portion of the model is overwhelming the AR portion of the model. We can't determine a cyclic pattern in the data because it is hidden behind a moving average that makes all points that are close to each other appear correlated. The best approach to fixing this is to plot the partial autocorrelation.</p></div><div class="calibre2" title="Calculating partial autocorrelation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec201" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating partial autocorrelation</h2></div></div></div><p class="calibre11">The <span class="strong1"><strong class="calibre12">partial autocorrelation function</strong></span> (<span class="strong1"><strong class="calibre12">PACF</strong></span>) aims to address the issue of spotting cyclic<a id="id1279" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> components in <a id="id1280" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a hybrid ARMA model. It's defined as the correlation coefficient between <span class="strong1"><em class="calibre13">y</em></span><sub class="calibre25">t</sub> and <span class="strong1"><em class="calibre13">y</em></span><sub class="calibre25">t+k</sub> given <a id="id1281" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>all the in-between observations. In other words, it's the autocorrelation at lag <span class="strong1"><em class="calibre13">k</em></span> that isn't already accounted for by lags 1 through <span class="strong1"><em class="calibre13">k-1</em></span>.</p><p class="calibre11">The first order, lag 1 partial autocorrelation is defined to equal the first order autocorrelation. The second order, lag 2 partial autocorrelation is equal to:</p><div class="mediaobject"><img src="Images/7180OS_09_11.jpg" alt="Calculating partial autocorrelation" class="calibre358"/></div><p class="calibre11">This is the <a id="id1282" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>correlation between <a id="id1283" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>values two time periods apart, <span class="strong1"><em class="calibre13">y</em></span><sub class="calibre25">t</sub> and <span class="strong1"><em class="calibre13">y</em></span><sub class="calibre25">t-2</sub>, conditional on knowledge of <span class="strong1"><em class="calibre13">y</em></span><sub class="calibre25">t-1</sub>. In a stationary time series, the two variances in the denominator will be equal.</p><p class="calibre11">The third order, lag 3 partial autocorrelation is equal to:</p><div class="mediaobject"><img src="Images/7180OS_09_12.jpg" alt="Calculating partial autocorrelation" class="calibre359"/></div><p class="calibre11">And so on, for any lag.</p><div class="calibre2" title="Autocovariance"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch09lvl3sec45" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Autocovariance</h3></div></div></div><p class="calibre11">The equations for partial autocorrelation require us to calculate the covariance of our data with itself at<a id="id1284" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> some lag. This is called the <span class="strong1"><strong class="calibre12">autocovariance</strong></span>. We have seen in previous chapters <a id="id1285" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>how to measure the covariance between two series of data, the tendency of two or more attributes to vary together. This function is very similar to the autocorrelation function we defined earlier in the chapter, and calculates the autocovariance for a range of lags beginning at zero:</p><div class="calibre2"><pre class="programlisting">(defn autocovariance* [series n k]
  (let [lag-product (-&gt;&gt; (drop k series)
                         (map * series)
                         (i/sum))]
    (cons (/ lag-product n)
          (lazy-seq
           (autocovariance* series n (inc k))))))

(defn autocovariance [series]
  (autocovariance* (s/sweep series) (count series) 0))</pre></div><p class="calibre11">As before, the return value will be a lazy sequence of lags, so we'll be sure to take only the values we need.</p></div><div class="calibre2" title="PACF with Durbin-Levinson recursion"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch09lvl3sec46" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>PACF with Durbin-Levinson recursion</h3></div></div></div><p class="calibre11">Because of the <a id="id1286" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>need to<a id="id1287" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> account for previously explained variation, calculating partial autocorrelation is a lot more involved than calculating autocorrelation. The Durbin-Levinson algorithm provides a way to calculate it recursively.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title8"><a id="note89" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Durbin-Levinson recursion, or simply Levinson Recursion, is a method <a id="id1288" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>for calculating the solution to equations involving matrices with constant values<a id="id1289" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> on the diagonals (called <span class="strong1"><strong class="calibre12">Toeplitz matrices</strong></span>). More <a id="id1290" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>information is available at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://en.wikipedia.org/wiki/Levinson_recursion">https://en.wikipedia.org/wiki/Levinson_recursion</a>.</p></div></div><p class="calibre11">An implementation of Levinson recursion is shown as follows. The math is beyond the scope of this book, but the general shape of the recursive function should be familiar to you now. At each iteration, we calculate the partial autocorrelation with a function of the previous partial autocorrelations and the autocovariance.</p><div class="calibre2"><pre class="programlisting">(defn pac* [pacs sigma prev next]
  (let [acv (first next)
        sum (i/sum (i/mult pacs (reverse prev)))
        pac (/ (- acv sum) sigma)]
    (cons pac
          (lazy-seq
           (pac* (-&gt;&gt; (i/mult pacs pac)
                      (reverse)
                      (i/minus pacs)
                      (cons pac))
                 (* (- 1 (i/pow pac 2)) sigma)
                 (cons acv prev)
                 (rest next))))))

(defn partial-autocorrelation [series]
  (let [acvs (autocovariance series)
        acv1 (first  acvs)
        acv2 (second acvs)
        pac  (/ acv2 acv1)]
    (concat [1.0 pac]
            (pac* (vector pac)
                  (- acv1 (* pac acv2))
                  (vector acv2)
                  (drop 2 acvs)))))</pre></div><p class="calibre11">As before, this function will create an infinite lazy sequence of partial autocorrelations, so we have to take only the numbers that we actually want from it.</p></div><div class="calibre2" title="Plotting partial autocorrelation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch09lvl3sec47" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Plotting partial autocorrelation</h3></div></div></div><p class="calibre11">Now that we've<a id="id1291" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> implemented a function to calculate the partial autocorrelations of a time series, let's plot them. We'll use the same ARMA coefficients as before so we can compare the difference.</p><div class="calibre2"><pre class="programlisting">(defn ex-9-24 []
  (let [ys (s/sample-normal 10 :sd 1.0)
        es (s/sample-normal 10 :sd 0.2)
        ps   [0 0 0 0.3 0.5]
        qs   [0.2 0.8]]
    (-&gt;&gt; (arma ys es ps qs 0.2)
         (take 500)
         (partial-autocorrelation)
         (take 15)
         (bar-plot))))</pre></div><p class="calibre11">This should generate a bar chart similar to the following:</p><div class="mediaobject"><img src="Images/7180OS_09_320.jpg" alt="Plotting partial autocorrelation" class="calibre45"/></div><p class="calibre11">Fortunately, this is rather different from the ACF plot that we created previously. There is a high partial autocorrelation at lags 1 and 2. This suggests that an <span class="strong1"><em class="calibre13">MA(2)</em></span> process is at work. Then, there is low partial autocorrelation until lag 5. This suggests that there is a an <span class="strong1"><em class="calibre13">AR(5)</em></span> model at work too.</p></div><div class="calibre2" title="Determining ARMA model order with ACF and PACF"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch09lvl3sec48" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Determining ARMA model order with ACF and PACF</h3></div></div></div><p class="calibre11">The differences between ACF and PACF plots are useful to help with selecting the most <a id="id1292" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>appropriate <a id="id1293" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>model for the time series. The following table describes the appearance of ACF and PACF plots <a id="id1294" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>for idealized AR and MA series.</p><div class="informaltable"><table border="1" class="calibre27"><colgroup class="calibre28"><col class="calibre29"/><col class="calibre29"/><col class="calibre29"/></colgroup><thead class="calibre30"><tr class="calibre31"><th valign="bottom" class="calibre32">
<p class="calibre19">Model</p>
</th><th valign="bottom" class="calibre32">
<p class="calibre19">ACF</p>
</th><th valign="bottom" class="calibre32">
<p class="calibre19">PACF</p>
</th></tr></thead><tbody class="calibre33"><tr class="calibre34"><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">AR(p)</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">Decays gradually</p>
</td><td class="calibre35">
<p class="calibre19">Cuts off after <span class="strong1"><em class="calibre13">p</em></span> lags</p>
</td></tr><tr class="calibre36"><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">MA(q)</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">Cuts off after <span class="strong1"><em class="calibre13">q</em></span> lags</p>
</td><td class="calibre35">
<p class="calibre19">Decays gradually</p>
</td></tr><tr class="calibre38"><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">ARMA(p,q)</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">Decays gradually</p>
</td><td class="calibre35">
<p class="calibre19">Decays gradually</p>
</td></tr></tbody></table></div><p class="calibre11">We are often not confronted with data that confirms to these ideals though. Given a real time series, particularly one without a significant number of points, it's not always obvious which would be the most appropriate model. The best course of action is often to pick the simplest model (the one with the lowest order) capable of describing your data.</p><div class="mediaobject"><img src="Images/7180OS_09_330.jpg" alt="Determining ARMA model order with ACF and PACF" class="calibre342"/></div><p class="calibre11">The preceding illustration shows sample ACF and PACF plots for an idealized <span class="strong1"><em class="calibre13">AR(1)</em></span> series. Next are<a id="id1295" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> sample <a id="id1296" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>ACF and <a id="id1297" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>PACF plots for an idealized <span class="strong1"><em class="calibre13">MA(1)</em></span> series.</p><div class="mediaobject"><img src="Images/7180OS_09_340.jpg" alt="Determining ARMA model order with ACF and PACF" class="calibre360"/></div><p class="calibre11">The dotted lines on the graphics indicate the threshold of significance. In general, we are not able to produce a model that perfectly captures all the autocorrelations in the time series and the significance threshold helps us prioritize the most important. A simple formula for determining significance threshold with an <span class="strong1"><em class="calibre13">α</em></span> of 5 percent is:</p><div class="mediaobject"><img src="Images/7180OS_09_13.jpg" alt="Determining ARMA model order with ACF and PACF" class="calibre361"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">n</em></span> is the number of points in the time series. If all points in the ACF and PACF are close to zero, the data are basically random.</p></div></div><div class="calibre2" title="ACF and PACF of airline data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec202" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>ACF and PACF of airline data</h2></div></div></div><p class="calibre11">Let's return<a id="id1298" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to the <a id="id1299" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>airline data that we started considering earlier and plot the ACF of the data for the first 25 lags.</p><div class="calibre2"><pre class="programlisting">(defn ex-9-25 []
  (-&gt;&gt; (airline-passengers)
       (difference)
       (autocorrelation)
       (take 25)
       (bar-plot)))</pre></div><p class="calibre11">This code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_350.jpg" alt="ACF and PACF of airline data" class="calibre45"/></div><p class="calibre11">You can see that there <a id="id1300" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>are regular <a id="id1301" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>peaks and troughs in the data. The first peak is at lag 12; the second is at lag 24. Since the data is monthly, these peaks correspond to an annual, seasonal, cycle. Since we have 144 points in our time series, the threshold for significance is about <span class="inlinemediaobject"><img src="Images/7180OS_09_14.jpg" alt="ACF and PACF of airline data" class="calibre362"/></span> or 0.17.</p><p class="calibre11">Next, let's look at the partial autocorrelation plot for the airline data:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-26 []
  (-&gt;&gt; (airline-passengers)
       (difference)
       (partial-autocorrelation)
       (take 25)
       (bar-plot)))</pre></div><p class="calibre11">This code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_360.jpg" alt="ACF and PACF of airline data" class="calibre45"/></div><p class="calibre11">The partial <a id="id1302" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>autocorrelation <a id="id1303" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>plot also has a peak at lag 12. Unlike the autocorrelation plot it doesn't have a peak at lag 24 because the periodic autocorrelation has already been accounted for at lag 12.</p><p class="calibre11">Although this appears to suggest an AR(12) model will be appropriate, that will create a large number of coefficients to learn, especially on a relatively small amount of data. Since the periodic cycle is seasonal, we ought to remove it with a second phase of differencing.</p></div><div class="calibre2" title="Removing seasonality with differencing"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec203" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Removing seasonality with differencing</h2></div></div></div><p class="calibre11">We have <a id="id1304" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>already differenced the data once, meaning that our model is referred to as an <span class="strong1"><strong class="calibre12">autoregressive integrated moving-average</strong></span> (<span class="strong1"><strong class="calibre12">ARIMA</strong></span>) model. The level of differencing is<a id="id1305" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> given the parameter <span class="strong1"><em class="calibre13">d</em></span>, and the full model order can therefore be specified as <span class="strong1"><em class="calibre13">ARIMA(p,d,q)</em></span>.</p><p class="calibre11">We can difference the data a second time to remove the strong seasonality in the data. Let's do this next:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-27 []
  (-&gt;&gt; (airline-passengers)
       (difference)
       (difference 12)
       (autocorrelation)
       (take 15)
       (bar-plot)))</pre></div><p class="calibre11">First, we plot the autocorrelation:</p><div class="mediaobject"><img src="Images/7180OS_09_362.jpg" alt="Removing seasonality with differencing" class="calibre45"/></div><p class="calibre11">Next, the <a id="id1306" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>partial autocorrelation:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-28 []
  (-&gt;&gt; (airline-passengers)
       (difference)
       (difference 12)
       (partial-autocorrelation)
       (take 15)
       (bar-plot)))</pre></div><p class="calibre11">This generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_364.jpg" alt="Removing seasonality with differencing" class="calibre45"/></div><p class="calibre11">The strong <a id="id1307" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>seasonal cycle accounted for most of the significance in the charts. We're left with negative autocorrelation at lag 1 on both charts, and a barely significant autocorrelation at lag 9 on the ACF. A general rule of thumb is that positive autocorrelation is best treated by adding an <span class="strong1"><em class="calibre13">AR</em></span> term to the model, while negative autocorrelation is usually best treated by adding an <span class="strong1"><em class="calibre13">MA</em></span> term to the model.</p><p class="calibre11">It appears based on the preceding charts that a justified model is an <span class="strong1"><em class="calibre13">MA(1)</em></span> model. This would probably be a good enough model for this case, but let's use this as an opportunity to demonstrate how to fit a large number of parameters to a model by trying to capture the <span class="strong1"><em class="calibre13">AR(9)</em></span> autocorrelation as well.</p><p class="calibre11">We'll consider an alternative to the cost function, the likelihood, which measures how closely the given model fits the data. The better the model fits, the greater the likelihood. Thus, we<a id="id1308" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> will want to maximize the likelihood, a goal also known as <span class="strong1"><strong class="calibre12">maximum likelihood estimation</strong></span>.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Maximum likelihood estimation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch09lvl1sec142" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Maximum likelihood estimation</h1></div></div></div><p class="calibre11">On several occasions throughout this book, we've expressed optimization problems in terms of a cost <a id="id1309" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function to be minimized. For example, in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Classification">Chapter 4</a>, <span class="strong1"><em class="calibre13">Classification</em></span>, we used Incanter to minimize <a id="id1310" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the logistic cost function whilst building a logistic regression classifier, and in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch05.xhtml" title="Chapter 5. Big Data">Chapter 5</a>, <span class="strong1"><em class="calibre13">Big Data</em></span>, we used gradient descent to minimize a least-squares cost function when performing batch and stochastic gradient descent.</p><p class="calibre11">Optimization can also be expressed as a benefit to maximize, and it's sometimes more natural to think in these terms. Maximum likelihood estimation aims to find the best parameters for a model by maximizing the likelihood function.</p><p class="calibre11">Let's say that the probability of an observation <span class="strong1"><em class="calibre13">x</em></span> given model parameters <span class="strong1"><em class="calibre13">β</em></span> is written as:</p><div class="mediaobject"><img src="Images/7180OS_09_15.jpg" alt="Maximum likelihood estimation" class="calibre363"/></div><p class="calibre11">Then, the likelihood can be expressed as:</p><div class="mediaobject"><img src="Images/7180OS_09_16.jpg" alt="Maximum likelihood estimation" class="calibre363"/></div><p class="calibre11">The likelihood is a measure of the <span class="strong1"><em class="calibre13">probability of the parameters</em></span>, given the data. The aim of maximum likelihood estimation is to find the parameter values that make the observed data most likely.</p><div class="calibre2" title="Calculating the likelihood"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec204" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating the likelihood</h2></div></div></div><p class="calibre11">Before calculating the<a id="id1311" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> likelihood for a time series, we'll illustrate the process by way of a simple example. Say we toss a coin 100 times and observe 56 heads, <span class="strong1"><em class="calibre13">h</em></span>, and 44 tails, <span class="strong1"><em class="calibre13">t</em></span>. Rather than assume that we have a fair coin with <span class="strong1"><em class="calibre13">P(h)=0.5</em></span> (and therefore that the slightly unequal totals are the result of chance variation), instead we could ask whether the observed values differ significantly from 0.5. We can do this by asking what value of <span class="strong1"><em class="calibre13">P(h)</em></span> makes the observed data most likely.</p><div class="calibre2"><pre class="programlisting">(defn ex-9-29 []
  (let [data 56
        f (fn [p]
            (s/pdf-binomial data :size 100 :prob p))]
    (-&gt; (c/function-plot f 0.3 0.8
                         :x-label "P"
                         :y-label "Likelihood")
        (i/view))))</pre></div><p class="calibre11">In the preceding code, we're using binomial distribution to model the sequence of coin tosses (recall from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Classification">Chapter 4</a>, <span class="strong1"><em class="calibre13">Classification</em></span>, that binomial distribution is used to model the number of times a binary outcome is expected to occur). The key point is that the data is fixed, and we're plotting the varying probabilities of observing that data given different<a id="id1312" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> parameters to the binomial distribution. The following plot shows the likelihood surface:</p><div class="mediaobject"><img src="Images/7180OS_09_366.jpg" alt="Calculating the likelihood" class="calibre45"/></div><p class="calibre11">As we might have expected, the most likely parameter to the binomial distribution is <span class="strong1"><em class="calibre13">p=0.56</em></span>. This contrived example could have more easily been calculated by hand, but the principle of maximum likelihood estimation is able to cope with much more complicated models.</p><p class="calibre11">In fact, our ARMA model is one such complicated model. The math for calculating the likelihood of time series parameters is beyond the scope of this book. We'll be making use of the <a id="id1313" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Clojure library Succession (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/henrygarner/succession">https://github.com/henrygarner/succession</a>) to calculate the likelihood for our time series.</p><p class="calibre11">It is often the case that we work with the log-likelihood rather than the likelihood. This is simply for mathematical convenience, since the log-likelihood:</p><div class="mediaobject"><img src="Images/7180OS_09_17.jpg" alt="Calculating the likelihood" class="calibre364"/></div><p class="calibre11">can be re-written as:</p><div class="mediaobject"><img src="Images/7180OS_09_18.jpg" alt="Calculating the likelihood" class="calibre365"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">k</em></span> is the <a id="id1314" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of parameters to the model. Taking the sum of a large number of parameters is more computationally convenient than taking the product, so the second formula is often preferred. Let's get a feel for how the likelihood function behaves on some test data by plotting the log-likelihood of different parameters against a simple <span class="strong1"><em class="calibre13">AR(2)</em></span> time series.</p><div class="calibre2"><pre class="programlisting">(defn ex-9-30 []
  (let [init  (s/sample-normal 2)
        coefs [0 0.5]
        data  (take 100 (ar init coefs 0.2))
        f     (fn [coef]
                (log-likelihood [0 coef] 2 0 data))]
    (-&gt; (c/function-plot f -1 1
                         :x-label "Coefficient"
                         :y-label "Log-Likelihood")
        (i/view))))</pre></div><p class="calibre11">The preceding code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_370.jpg" alt="Calculating the likelihood" class="calibre45"/></div><p class="calibre11">The peak of the <a id="id1315" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>curve corresponds to the best estimate for the parameters, given the data. Notice how the peak in the preceding plot is a little higher than 0.5: the noise we added to the model has meant that the best estimate is not exactly 0.5. </p></div><div class="calibre2" title="Estimating the maximum likelihood"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec205" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Estimating the maximum likelihood</h2></div></div></div><p class="calibre11">The number of <a id="id1316" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>parameters to our ARMA model is large, and so to determine the maximum likelihood we're going to use an optimization method that performs well in high-dimensional spaces. The method <a id="id1317" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>is called the <span class="strong1"><strong class="calibre12">Nelder-Mead</strong></span>, or <span class="strong1"><strong class="calibre12">simplex</strong></span>, method. In a space of <span class="strong1"><em class="calibre13">n</em></span> dimensions, a simplex is a <span class="strong1"><em class="calibre13">polytope</em></span> of <span class="strong1"><em class="calibre13">n+1</em></span> vertices.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note90" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">A <a id="id1318" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>polytope is a geometric object with flat sides that can exist in an arbitrary number of dimensions. A two-dimensional polygon is 2-polytope, and a three dimensional polyhedron is a 3-polytope.</p></div></div><p class="calibre11">The advantage of simplex optimization is that it doesn't need to calculate the gradient at each point in order to descend (or ascend) to a more optimal position. The Nelder-Mead method extrapolates the behavior of the objective function measured at each test point on the simplex. The worst point is replaced with a point created by reflecting through the centroid of the remaining points. If the new point is better than the current best point then we stretch the simplex out exponentially along this line. If the new point isn't much better than before we could be stepping across a valley, so we <a id="id1319" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>contract the simplex towards a possibly better point.</p><p class="calibre11">The following plot shows an example of how the simplex, represented as a triangle, reflects and contracts to find the optimal parameters.</p><div class="mediaobject"><img src="Images/7180OS_09_380.jpg" alt="Estimating the maximum likelihood" class="calibre366"/></div><p class="calibre11">The simplex is always represented as a shape whose number of vertices is one greater than the number of dimensions. The simplex for two-dimensional optimization, as in the preceding plot, is represented by a triangle. For an arbitrary <span class="strong1"><em class="calibre13">n</em></span>-dimensional space, the simplex will be represented as a polygon of <span class="strong1"><em class="calibre13">n+1</em></span> vertices.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note91" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The <a id="id1320" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>simplex method is also called the <span class="strong1"><strong class="calibre12">amoeba method</strong></span> due to the way it appears to crawl towards a more optimal position.</p></div></div><p class="calibre11">The simplex method of optimization isn't implemented in Incanter, but it's available in the <a id="id1321" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Apache <a id="id1322" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Commons Math library (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://commons.apache.org/proper/commons-math/">http://commons.apache.org/proper/commons-math/</a>). To use it, we'll need to wrap our objective function, the log-likelihood, in a representation that the library understands.</p><div class="calibre2" title="Nelder-Mead optimization with Apache Commons Math"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch09lvl3sec49" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Nelder-Mead optimization with Apache Commons Math</h3></div></div></div><p class="calibre11">Apache Commons Math is <a id="id1323" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a large and sophisticated library. We can't cover more than the barest <a id="id1324" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>essentials here. The next example is provided simply to illustrate how to integrate Clojure code <a id="id1325" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with the Java interfaces provided by the library.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title8"><a id="note92" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">An overview of <a id="id1326" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Apache Commons Math's extensive optimization capabilities is available at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://commons.apache.org/proper/commons-math/userguide/optimization.html">http://commons.apache.org/proper/commons-math/userguide/optimization.html</a>.</p></div></div><p class="calibre11">The Apache Commons <a id="id1327" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Math library expects that we'll provide an <code class="literal">ObjectiveFunction</code> to be optimized. Next, we create one by reifying a <code class="literal">MultivariateFunction</code>, since our objective function needs to be supplied with multiple parameters. Our response will be a single value: the log-likelihood.</p><div class="calibre2"><pre class="programlisting">(defn objective-function [f]
  (ObjectiveFunction. (reify MultivariateFunction
                        (value [_ v]
                          (f (vec v))))))</pre></div><p class="calibre11">The preceding code will return an <code class="literal">ObjectiveFunction</code> representation of an arbitrary function <code class="literal">f</code>. A <code class="literal">MultivariateFunction</code> expects to receive a parameter vector <code class="literal">v</code>, which we pass straight through to our <code class="literal">f</code>.</p><p class="calibre11">With this in place, we use some Java interop to call <code class="literal">optimize</code> on a <code class="literal">SimplexOptimizer</code> with some sensible default values. Our <code class="literal">InitialGuess</code> at the parameters is simply an array of zeros. The <code class="literal">NelderMeadSimplex</code> must be initialized with a default step size for each dimension, which can be any value except zero. We're picking a value of 0.2 for each parameter.</p><div class="calibre2"><pre class="programlisting">(defn arma-model [p q ys]
  (let [m (+ p q)
        f (fn [params]
            (sc/log-likelihood params p q ys))
        optimal (.optimize (SimplexOptimizer. 1e-10 1e-10)
                           (into-array
                            OptimizationData
                            [(MaxEval. 100000)
                             (objective-function f)
                             GoalType/MAXIMIZE
                             (-&gt;&gt; (repeat 0.0)
                                  (take m)
                                  (double-array)
                                  (InitialGuess.))
                             (-&gt;&gt; (repeat 0.1)
                                  (take m)
                                  (double-array)
                                  (NelderMeadSimplex.))]))
        point (-&gt; optimal .getPoint vec)
        value (-&gt; optimal .getValue)]
    {:ar (take p point)
     :ma (drop p point)
     :ll value}))

(defn ex-9-31 []
  (-&gt;&gt; (airline-passengers)
       (i/log)
       (difference)
       (difference 12)
       (arma-model 9 1)))</pre></div><p class="calibre11">Our model is a <a id="id1328" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>large one <a id="id1329" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with many parameters and so the optimization will take a while to converge. If you run <a id="id1330" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the preceding example you should eventually see returned parameters similar to those shown next:</p><div class="calibre2"><pre class="programlisting">;; {:ar (-0.23769808471685377 -0.012617164166298971 ...),
;;  :ma (-0.14754455658280236),
;;  :ll 232.97813750669314}</pre></div><p class="calibre11">These are the maximum likelihood estimates for our model. Also included in the response is the log-likelihood for the model with the maximum-likelihood parameters.</p></div></div><div class="calibre2" title="Identifying better models with Akaike Information Criterion"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec206" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Identifying better models with Akaike Information Criterion</h2></div></div></div><p class="calibre11">When evaluating <a id="id1331" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>multiple models, it might appear that the best model is the one with the greatest maximum<a id="id1332" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> likelihood estimate. After all, the estimate has determined that the model is the best candidate for generating the observed data. However, the maximum likelihood estimate takes no account of the complexity of the model and, in general, simpler models are to be preferred. Think back to the beginning of the chapter and our high-order polynomial model that had a high <span class="strong1"><em class="calibre13">R</em></span>2 but provided no predictive power.</p><p class="calibre11">The <span class="strong1"><strong class="calibre12">Akaike Information Criterion</strong></span> (<span class="strong1"><strong class="calibre12">AIC</strong></span>) is a method for comparing models that rewarded<a id="id1333" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> goodness of fit, as assessed by the likelihood function, but includes a penalty that is a function of the number of parameters. This penalty discourages overfitting, since increasing the number of parameters to the model almost always improves the goodness of fit.</p><p class="calibre11">The AIC can be calculated from the following formula:</p><div class="mediaobject"><img src="Images/7180OS_09_19.jpg" alt="Identifying better models with Akaike Information Criterion" class="calibre367"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">k</em></span> is the <a id="id1334" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of parameters to the model and <span class="strong1"><em class="calibre13">L</em></span> is the likelihood function. We can calculate the <a id="id1335" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>AIC in the following way in Clojure with the parameter counts <span class="strong1"><em class="calibre13">p</em></span> and <span class="strong1"><em class="calibre13">q</em></span>.</p><div class="calibre2"><pre class="programlisting">(defn aic [coefs p q ys]
  (- (* 2 (+ p q 1))
     (* 2 (log-likelihood coefs p q ys))))</pre></div><p class="calibre11">If we were to produce multiple models and pick the best one, we would want to pick the one with the lowest AIC.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Time series forecasting"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch09lvl1sec143" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Time series forecasting</h1></div></div></div><p class="calibre11">With the parameter <a id="id1336" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>estimates having been defined, we're finally in a position to use our model for forecasting. We've actually already written most of the code we need to do this: we have an <code class="literal">arma</code> function that's capable of generating an autoregressive moving-average series based on some seed data and the model parameters <span class="strong1"><em class="calibre13">p</em></span> and <span class="strong1"><em class="calibre13">q</em></span>. The seed data will be our measured values of <span class="strong1"><em class="calibre13">y</em></span> from the airline data, and the values of <span class="strong1"><em class="calibre13">p</em></span> and <span class="strong1"><em class="calibre13">q</em></span> will be the parameters that we calculated using the Nelder-Mead method.</p><p class="calibre11">Let's plug those numbers into our ARMA model and generate a sequence of predictions for <span class="strong1"><em class="calibre13">y</em></span>:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-32 []
  (let [data (i/log (airline-passengers))
        diff-1  (difference 1 data)
        diff-12 (difference 12 diff-1)
        forecast (-&gt;&gt; (arma (take 9 (reverse diff-12))
                       []
                       (:ar params)
                       (:ma params) 0)
                      (take 100)
                      (undifference 12 diff-1)
                      (undifference 1 data))]
    (-&gt;&gt; (concat data forecast)
         (i/exp)
         (timeseries-plot))))</pre></div><p class="calibre11">The preceding code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_390.jpg" alt="Time series forecasting" class="calibre45"/></div><p class="calibre11">The line up to time slice 144 is <a id="id1337" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the original series. The line subsequent to this point is our forecast series. The forecast looks a lot like we might have hoped: the exponentially increasing trend continues, as do the regular seasonal pattern of peaks and troughs.</p><p class="calibre11">In fact, the forecast is almost too regular. Unlike the series at points 1 to 144, our forecast contains no noise. Let's add some noise to make our forecast more realistic. To determine how much noise is justified, we could look to see what the error was in our past forecasting. To avoid our errors compounding, we should make predictions one time step ahead, and observe the difference between the prediction and the actual value.</p><p class="calibre11">Let's run our ARMA function with a sigma of 0.02:</p><div class="calibre2"><pre class="programlisting">(defn ex-9-33 []
  (let [data (i/log (airline-passengers))
        diff-1  (difference 1 data)
        diff-12 (difference 12 diff-1)
        forecast (-&gt;&gt; (arma (take 9 (reverse diff-12))
                       []
                       (:ar params)
                       (:ma params) 0.02)
                      (take 10)
                      (undifference 12 diff-1)
                      (undifference 1 data))]
    (-&gt;&gt; (concat data forecast)
         (i/exp)
         (timeseries-plot))))</pre></div><p class="calibre11">The preceding code may generate a chart like the following:</p><div class="mediaobject"><img src="Images/7180OS_09_395.jpg" alt="Time series forecasting" class="calibre45"/></div><p class="calibre11">Now we get a <a id="id1338" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sense of the volatility of the forecast. By running the simulation several times we can get a sense of the variety of different possible outcomes. What would be useful is if we could determine the confidence interval of our predictions: the upper and lower expectation of all future series, including noise.</p><div class="calibre2" title="Forecasting with Monte Carlo simulation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch09lvl2sec207" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Forecasting with Monte Carlo simulation</h2></div></div></div><p class="calibre11">Although analytic <a id="id1339" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>methods do <a id="id1340" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>exist for calculating the expected future value of a time series, together with confidence intervals, we'll use this final section to arrive at these values through simulation instead. By studying the variation amongst many forecasts we can arrive at confidence intervals for our model predictions.</p><p class="calibre11">For example, if we run a very large number of simulations we can calculate the 95 percent confidence intervals on our future predictions based on the range within which values fall 95 percent of the time. This is the essence of the Monte Carlo simulation, which is a commonly used statistical tool for problems that are analytically intractable.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note93" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The Monte Carlo method was developed and used systematically during the Manhattan Project, the American World War II effort to develop nuclear weapons. John Von Neumann and Stanislaw Ulam suggested it as a means to investigate properties of neutron travel through radiation shielding and named the method after the Monte Carlo Casino in Monaco.</p></div></div><p class="calibre11">We've already laid <a id="id1341" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>all the foundations <a id="id1342" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>for Monte Carlo simulations of the time series forecasts. We simply need to run the simulation many hundreds of times and collect the results. In the following code, we run 1,000 simulations and gather the mean and standard deviation across all forecasts at each future time slice. By creating two new series (an upper bound that adds the standard deviation multiplied by 1.96 and a lower bound that subtracts the standard deviation multiplied by 1.96), we're able to visualize the 95 percent confidence interval for the future values of the series.</p><div class="calibre2"><pre class="programlisting">(defn ex-9-34 []
  (let [data (difference (i/log (airline-passengers)))
        init (take 12 (reverse data))
        forecasts (for [n (range 1000)]
                    (take 20
                          (arma init [0.0028 0.0028]
                                (:ar params1)
                                (:ma params1)
                                0.0449)))
        forecast-mean (map s/mean (i/trans forecasts))
        forecast-sd (-&gt; (map s/sd (i/trans forecasts))
                        (i/div 2)
                        (i/mult 1.96))
        upper (-&gt;&gt; (map + forecast-mean forecast-sd)
                   (concat data)
                   (undifference 0)
                   (i/exp))
        lower (-&gt;&gt; (map - forecast-mean forecast-sd)
                   (concat data)
                   (undifference 0)
                   (i/exp))
        n (count upper)]
    (-&gt; (c/xy-plot   (range n) upper
                     :x-label "Time"
                     :y-label "Value"
                     :series-label "Upper Bound"
                     :legend true)
        (c/add-lines (range n) lower
                     :series-label "Lower Bound")
        (i/view))))</pre></div><p class="calibre11">This <a id="id1343" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>generates the<a id="id1344" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> following chart:</p><div class="mediaobject"><img src="Images/7180OS_09_400.jpg" alt="Forecasting with Monte Carlo simulation" class="calibre45"/></div><p class="calibre11">The upper and lower bounds provide the confidence intervals for our time series predictions into the future.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch09lvl1sec144" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we've considered the task of analyzing discrete time series: sequential observations taken at fixed intervals in time. We've seen how the challenge of modeling such a series can be made easier by decomposing it into a set of components: a trend component, a seasonal component, and a cyclic component.</p><p class="calibre11">We've seen how ARMA models decompose a series further into autoregressive and moving-average components, each of which is in some way determined by past values of the series. This conception of a series is inherently recursive, and we've seen how Clojure's natural capabilities for defining recursive functions and lazy sequences lend themselves to the algorithmic generation of such series. By determining each value of the series as a function of the previous values, we implemented a recursive ARMA generator that was capable of simulating a measured series and forecasting it forwards in time.</p><p class="calibre11">We've also learned about expectation maximization: a way of reframing solutions to optimization problems as those which generate the greatest likelihood, given the data. And we've also seen how the Apache Commons Math library can be used to estimate the maximum likelihood parameters using the Nelder-Mead method. Finally, we saw how forecasting could be accomplished by playing the sequence forward in time, and how Monte Carlo simulation could be used to estimate the future error of the series.</p><p class="calibre11">In the final chapter, we'll turn our attention away from data analysis towards data visualization. In some respects, the most important challenge for data scientists is communication, and we'll see how Clojure can support us in presenting our data in the most effective way.</p></div></div>



  </body></html>