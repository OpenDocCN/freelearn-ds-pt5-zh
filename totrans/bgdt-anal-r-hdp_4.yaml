- en: Chapter 4. Using Hadoop Streaming with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned how to integrate R and Hadoop with the
    help of RHIPE and RHadoop and also sample examples. In this chapter, we are going
    to discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics of Hadoop streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to run Hadoop streaming with R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the HadoopStreaming R package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the basics of Hadoop streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop streaming is a Hadoop utility for running the Hadoop MapReduce job with
    executable scripts such as Mapper and Reducer. This is similar to the pipe operation
    in Linux. With this, the text input file is printed on stream (`stdin`), which
    is provided as an input to Mapper and the output (`stdout`) of Mapper is provided
    as an input to Reducer; finally, Reducer writes the output to the HDFS directory.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of the Hadoop streaming utility is that it allows Java as
    well as non-Java programmed MapReduce jobs to be executed over Hadoop clusters.
    Also, it takes care of the progress of running MapReduce jobs. The Hadoop streaming
    supports the Perl, Python, PHP, R, and C++ programming languages. To run an application
    written in other programming languages, the developer just needs to translate
    the application logic into the Mapper and Reducer sections with the key and value
    output elements. We learned in [Chapter 2](ch02.html "Chapter 2. Writing Hadoop
    MapReduce Programs"), *Writing Hadoop MapReduce Programs*, that to create Hadoop
    MapReduce jobs we need Mapper, Reducer, and Driver as the three main components.
    Here, creating the driver file for running the MapReduce job is optional when
    we are implementing MapReduce with R and Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is written with the intention of integrating R and Hadoop. So we
    will see the example of R with Hadoop streaming. Now, we will see how we can use
    Hadoop streaming with the R script written with Mapper and Reducer. From the following
    diagrams, we can identify the various components of the Hadoop streaming MapReduce
    job.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the basics of Hadoop streaming](img/3282OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hadoop streaming components
  prefs: []
  type: TYPE_NORMAL
- en: Now, assume we have implemented our Mapper and Reducer as `code_mapper.R` and
    `code_reducer.R`. We will see how we can run them in an integrated environment
    of R and Hadoop. This can be run with the Hadoop streaming command with various
    generic and streaming options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the format of the Hadoop streaming command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The following diagram shows an example of the execution of Hadoop streaming,
    a MapReduce job with several streaming options.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the basics of Hadoop streaming](img/3282OS_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hadoop streaming command options
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding image, there are about six unique important components that
    are required for the entire Hadoop streaming MapReduce job. All of them are streaming
    options except jar.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a line-wise description of the preceding Hadoop streaming
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 1**: This is used to specify the Hadoop jar files (setting up the classpath
    for the Hadoop jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 2**: This is used for specifying the input directory of HDFS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 3**: This is used for specifying the output directory of HDFS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 4**: This is used for making a file available to a local machine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 5**: This is used to define the available R file as Mapper'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 6**: This is used for making a file available to a local machine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 7**: This is used to define the available R file as Reducer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main six Hadoop streaming components of the preceding command are listed
    and explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**jar:** This option is used to run a jar with coded classes that are designed
    for serving the streaming functionality with Java as well as other programmed
    Mappers and Reducers. It''s called the Hadoop streaming jar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input****:** This option is used for specifying the location of input dataset
    (stored on HDFS) to Hadoop streaming MapReduce job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output:** This option is used for telling the HDFS output directory (where
    the output of the MapReduce job will be written) to Hadoop streaming MapReduce
    job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**file:** This option is used for copying the MapReduce resources such as Mapper,
    Reducer, and Combiner to computer nodes (Tasktrackers) to make it local.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mapper:** This option is used for identification of the executable `Mapper`
    file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**reducer:** This option is used for identification of the executable `Reducer`
    file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are other Hadoop streaming command options too, but they are optional.
    Let''s have a look at them:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inputformat`: This is used to define the input data format by specifying the
    Java class name. By default, it''s `TextInputFormat`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`outputformat`: This is used to define the output data format by specifying
    the Java class name. By default, it''s `TextOutputFormat`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`partitioner`: This is used to include the class or file written with the code
    for partitioning the output as (key, value) pairs of the Mapper phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combiner`: This is used to include the class or file written with the code
    for reducing the Mapper output by aggregating the values of keys. Also, we can
    use the default combiner that will simply combine all the key attribute values
    before providing the Mapper''s output to the Reducer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cmdenv`: This option will pass the environment variable to the streaming command.
    For example, we can pass `R_LIBS = /your /path /to /R /libraries`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputreader`: This can be used instead of the `inputformat` class for specifying
    the record reader class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose`: This is used to verbose the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numReduceTasks`: This is used to specify the number of Reducers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mapdebug`: This is used to debug the script of the `Mapper` file when the
    Mapper task fails.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reducedebug`: This is used to debug the script of the `Reducer` file when
    the Reducer task fails.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, it's time to look at some generic options for the Hadoop streaming MapReduce
    job.
  prefs: []
  type: TYPE_NORMAL
- en: '`conf`: This is used to specify an application configuration file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`D`: This is used to define the value for a specific MapReduce or HDFS property.
    For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-D property = value or to specify the temporary HDFS directory`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'or to specify the total number of zero Reducers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The `-D` option only works when a tool is implemented.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`fs`: This is used to define the Hadoop NameNode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`jt`: This is used to define the Hadoop JobTracker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`files`: This is used to specify the large or multiple text files from HDFS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`libjars`: This is used to specify the multiple jar files to be included in
    the classpath.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`archives`: This is used to specify the jar files to be unarchived on the local
    machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Understanding how to run Hadoop streaming with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we understood what Hadoop streaming is and how it can be called with Hadoop
    generic as well as streaming options. Next, it's time to know how an R script
    can be developed and run with R. For this, we can consider a better example than
    a simple word count program.
  prefs: []
  type: TYPE_NORMAL
- en: 'The four different stages of MapReduce operations are explained here as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a MapReduce application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to code a MapReduce application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to run a MapReduce application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to explore the output of a MapReduce application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding a MapReduce application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Problem definition: The problem is to segment a page visit by the geolocation.
    In this problem, we are going to consider the website [http://www.gtuadmissionhelpline.com/](http://www.gtuadmissionhelpline.com/),
    which has been developed to provide guidance to students who are looking for admission
    in the Gujarat Technological University. This website contains the college details
    of various fields such as Engineering (diploma, degree, and masters), Medical,
    Hotel Management, Architecture, Pharmacy, MBA, and MCA. With this MapReduce application,
    we will identify the fields that visitors are interested in geographically.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, most of the online visitors from Valsad city visit the pages of
    MBA colleges more often. Based on this, we can identify the mindset of Valsad
    students; they are highly interested in getting admissions in the MBA field. So,
    with this website traffic dataset, we can identify the city-wise interest levels.
    Now, if there are no MBA colleges in Valsad, it will be a big issue for them.
    They will need to relocate to other cities; this may increase the cost of their
    education.
  prefs: []
  type: TYPE_NORMAL
- en: By using this type of data, the Gujarat Technological University can generate
    informative insights for students from different cities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input dataset source: To perform this type of analysis, we need to have the
    web traffic data for that website. Google Analytics is one of the popular and
    free services for tracking an online visitor''s metadata from the website. Google
    Analytics stores the web traffic data in terms of various dimensions ad metrics.
    We need to design a specific query to extract the dataset from Google Analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input dataset: The extracted Google Analytics dataset contains the following
    four data columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`date`: This is the date of visit and in the form of YYYY/MM/DD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`country`: This is the country of the visitor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`city`: This is the city of the visitor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pagePath`: This is the URL of a page of the website.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The head section of the input dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output format is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding a MapReduce application](img/3282OS_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is a sample output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding a MapReduce application](img/3282OS_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding how to code a MapReduce application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will learn about the following two units of a MapReduce
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: Mapper code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducer code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with the Mapper code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mapper code: This R script, named `ga-mapper.R`, will take care of the Map
    phase of a MapReduce job.'
  prefs: []
  type: TYPE_NORMAL
- en: The Mapper's job is to work on each line and extract a pair (key, value) and
    pass it to the Reducer to be grouped/aggregated. In this example, each line is
    an input to Mapper and the output `City:PagePath`. `City` is a key and `PagePath`
    is a value. Now Reducer can get all the page paths for a given city; hence, it
    can be grouped easily.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We extract the third and fourth element for the city and pagePath respectively.
    Then, they will be written to the stream as key-value pairs and fed to Reducer
    for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As soon as the output of the Mapper phase as (key, value) pairs is available
    to the standard output, Reducers will read the line-oriented output from `stdout`
    and convert it into final aggregated key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how the Mapper output format is and how the input data format of Reducer
    looks like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reducer code: This R script named `ga_reducer.R` will take care of the Reducer
    section of the MapReduce job.'
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed, the output of Mapper will be considered as the input for Reducer.
    Reducer will read these city and pagePath pairs, and combine all of the values
    with its respective key elements.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The Mapper output is written in two main fields with `\t` as the separator and
    the data line-by-line; hence, we have split the data by using `\t` to capture
    the two main attributes (key and values) from the stream input.
  prefs: []
  type: TYPE_NORMAL
- en: After collecting the key and value, the Reducer will compare it with the previously
    captured value. If not set previously, then set it; otherwise, combine it with
    the previous character value using the `combine` function in R and finally, print
    it to the HDFS output location.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Understanding how to run a MapReduce application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the development of the Mapper and Reducer script with the R language,
    it's time to run them in the Hadoop environment. Before we execute this script,
    it is recommended to test them on the sample dataset with simple pipe operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will run the developed Mapper and Reducer scripts over
    a local machine. But it will run similar to the Hadoop streaming job. We need
    to test this for any issue that might occur at runtime or for the identification
    of programming or logical mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have Mapper and Reducer tested and ready to be run with the Hadoop
    streaming command. This Hadoop streaming operation can be executed by calling
    the generic `jar` command followed with the streaming command options as we learned
    in the *Understanding the basics of Hadoop streaming* section of this chapter.
    We can execute the Hadoop streaming job in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: From a command prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R or the RStudio console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The execution command with the generic and streaming command options will be
    the same for both the ways.
  prefs: []
  type: TYPE_NORMAL
- en: Executing a Hadoop streaming job from the command prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we already learned in the section *Understanding the basics of Hadoop streaming*,
    the execution of Hadoop streaming MapReduce jobs developed with R can be run using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Executing the Hadoop streaming job from R or an RStudio console
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Being an R user, it will be more appropriate to run the Hadoop streaming job
    from an R console. This can be done with the `system` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This preceding command is similar to the one that you have already used in the
    command prompt to execute the Hadoop streaming job with the generic options as
    well as the streaming options.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how to explore the output of MapReduce application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After completing the execution successfully, it's time to explore the output
    to check whether the generated output is important or not. The output will be
    generated along with two directories, `_logs` and `_SUCCESS`. `_logs` will be
    used for tracking all the operations as well as errors; `_SUCCESS` will be generated
    only on the successful completion of the MapReduce job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, the commands can be fired in the following two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: From a command prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From an R console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring an output from the command prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To list the generated files in the output directory, the following command
    will be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The snapshot for checking the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring an output from the command prompt](img/3282OS_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Exploring an output from R or an RStudio console
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The same command can be used with the `system` method in the R (with RStudio)
    console.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'A screenshot of the preceding function is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring an output from R or an RStudio console](img/3282OS_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding basic R functions used in Hadoop MapReduce scripts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will see some basic utility functions used in Hadoop Mapper and Reducer
    for data processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '`file`: This function is used to create the connection to a file for the reading
    or writing operation. It is also used for reading and writing from/to `stdin`
    or `stdout`. This function will be used at the initiation of the Mapper and Reducer
    phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`write`: This function is used to write data to a file or standard input. It
    will be used after the key and value pair is set in the Mapper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`print`: This function is used to write data to a file or standard input. It
    will be used after the key and value pair is ready in the Mapper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`close`: This function can be used for closing the connection to the file after
    the reading or writing operation is completed. It can be used with Mapper and
    Reducer at the close (`conn`) end when all the processes are completed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stdin`: This is a standard connection corresponding to the input. The `stdin()`
    function is a text mode connection that returns the connection object. This function
    will be used in Mapper as well as Reducer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`stdout`: This is a standard connection corresponding to the output. The `stdout()`
    function is a text mode connection that also returns the object. This function
    will be used in Mapper as well as Reducer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`sink`: `sink` drives the R output to the connection. If there is a file or
    stream connection, the output will be returned to the file or stream. This will
    be used in Mapper and Reducer for tracking all the functional outputs as well
    as the errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Monitoring the Hadoop MapReduce job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A small syntax error in the Reducer phase leads to a failure of the MapReduce
    job. After the failure of a Hadoop MapReduce job, we can track the problem from
    the Hadoop MapReduce administration page, where we can get information about running
    jobs as well as completed jobs.
  prefs: []
  type: TYPE_NORMAL
- en: In case of a failed job, we can see the total number of completed/failed Map
    and Reduce jobs. Clicking on the failed jobs will provide the reason for the failing
    of those particular number of Mappers or Reducers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we can check the real-time progress of that running MapReduce job with
    the JobTracker console as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring the Hadoop MapReduce job](img/3282OS_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Monitoring Hadoop MapReduce job
  prefs: []
  type: TYPE_NORMAL
- en: 'Through the command, we can check the history of that particular MapReduce
    job by specifying its output directory with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The following command will print the details of the MapReduce job, failed and
    reasons for killed up jobs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will print about the successful task and the task attempts
    made for each task.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the HadoopStreaming R package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HadoopStreaming is an R package developed by *David S. Rosenberg*. We can say
    this is a simple framework for MapReduce scripting. This also runs without Hadoop
    for operating data in a streaming fashion. We can consider this R package as a
    Hadoop MapReduce initiator. For any analyst or developer who is not able to recall
    the Hadoop streaming command to be passed in the command prompt, this package
    will be helpful to quickly run the Hadoop MapReduce job.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three main features of this package are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunkwise data reading: The package allows chunkwise data reading and writing
    for Hadoop streaming. This feature will overcome memory issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supports various data formats: The package allows the reading and writing of
    data in three different data formats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robust utility for the Hadoop streaming command: The package also allows users
    to specify the command-line argument for Hadoop streaming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This package is mainly designed with three functions for reading the data efficiently:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hsTableReader`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hsKeyValReader`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hsLineReader`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's understand these functions and their use cases. After that we will
    understand these functions with the help of the word count MapReduce job.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the hsTableReader function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `hsTableReader` function is designed for reading data in the table format.
    This function assumes that there is an input connection established with the file,
    so it will retrieve the entire row. It assumes that all the rows with the same
    keys are stored consecutively in the input ﬁle.
  prefs: []
  type: TYPE_NORMAL
- en: As the Hadoop streaming job guarantees that the output rows of Mappers will
    be sorted before providing to the reducers, there is no need to use the `sort`
    function in a Hadoop streaming MapReduce job. When we are not running this over
    Hadoop, we explicitly need to call the `sort` function after the `Mapper` function
    gets execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining a function of `hsTableReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The terms in the preceding code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`file`: This is a connection object, stream, or string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chunkSize`: This indicates the maximum number of lines to be read at a time
    by the function. `-1` means all the lines at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cols`: This means a list of column names as "what" argument to scan.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip`: This is used to skip the first n data rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FUN`: This function will use the data entered by the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`carryMemLimit`: This indicates the maximum memory limit for the values of
    a single key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`carryMaxRows`: This indicates the maximum rows to be considered or read from
    the file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stringsAsFactors`: This defines whether the strings are converted to factors
    or not (`TRUE` or `FALSE`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, data in file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the preceding code is as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the hsTableReader function](img/3282OS_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The data read by `hsTableReader` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the preceding code is as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the hsTableReader function](img/3282OS_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the hsKeyValReader function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `hsKeyValReader` function is designed for reading the data available in
    the key-value pair format. This function also uses `chunkSize` for defining the
    number of lines to be read at a time, and each line consists of a key string and
    a value string.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The terms of this function are similar to `hsTablereader()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the preceding code is as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the hsKeyValReader function](img/3282OS_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding the hsLineReader function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `hsLineReader` function is designed for reading the entire line as a string
    without performing the data-parsing operation. It repeatedly reads the `chunkSize`
    lines of data from the file and passes a character vector of these strings to
    `FUN`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The terms of this function are similar to `hsTablereader()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for the preceding code is as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the hsLineReader function](img/3282OS_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can get more information on these methods as well as other existing methods
    at [http://cran.r-project.org/web/packages/HadoopStreaming/HadoopStreaming.pdf](http://cran.r-project.org/web/packages/HadoopStreaming/HadoopStreaming.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will implement the above data-reading methods with the Hadoop MapReduce
    program to be run over Hadoop. In some of the cases, the key-values pairs or data
    rows will not be fed in the machine memory; so reading that data chunk wise will
    be more appropriate than improving the machine configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem definition:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hadoop word count: As we already know what a word count application is, we
    will implement the above given methods with the concept of word count. This R
    script has been reproduced here from the HadoopStreaming R package, which can
    be downloaded along with the HadoopStreaming R library distribution as the sample
    code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input dataset: This has been taken from [Chapter 1](ch01.html "Chapter 1. Getting
    Ready to Use R and Hadoop") of *Anna Karenina* (novel) by the Russian writer *Leo
    Tolstoy*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'R script: This section contains the code of the Mapper, Reducer, and the rest
    of the configuration parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'File: `hsWordCnt.R`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Running a Hadoop streaming job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since this is a Hadoop streaming job, it will run same as the executed previous
    example of a Hadoop streaming job. For this example, we will use a shell script
    to execute the `runHadoop.sh` file to run Hadoop streaming.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up the system environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Setting up the MapReduce job parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Removing the existing output directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Designing the Hadoop MapReduce command with generic and streaming options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Extracting the output from HDFS to the local directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Executing the Hadoop streaming job
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can now execute the Hadoop streaming job by executing the command, `runHadoop.sh`.
    To execute this, we need to set the user permission.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Finally, it will execute the whole Hadoop streaming job and then copy the output
    to the local directory.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned most of the ways to integrate R and Hadoop for performing data
    operations. In the next chapter, we will learn about the data analytics cycle
    for solving real world data analytics problems with the help of R and Hadoop.
  prefs: []
  type: TYPE_NORMAL
