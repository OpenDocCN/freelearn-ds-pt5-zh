- en: Jupyter and Big Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data is the topic on everyone's mind. I thought it would be good to see
    what can be done with big data in Jupyter. One up-and-coming language for dealing
    with large datasets is Spark. Spark is an open source toolset. We can use Spark
    coding in Jupyter much like the other languages we have seen.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark for use in Jupyter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Spark's features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the tools we will be using is Apache Spark. Spark is an open source toolset
    for cluster computing. While we will not be using a cluster, typical usage for
    Spark is a larger set of machines or clusters that operate in parallel to analyze
    a big dataset. Installation instructions are available at [https://www.dataquest.io/blog/pyspark-installation-guide](https://www.dataquest.io/blog/pyspark-installation-guide).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark on macOS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up-to-date instructions for installing Spark are available at [https://medium.freecodecamp.org/installing-scala-and-apache-spark-on-mac-os-837ae57d283f](https://medium.freecodecamp.org/installing-scala-and-apache-spark-on-mac-os-837ae57d283f).
    The main steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Get Homebrew from [http://brew.sh](http://brew.sh). If you are doing software
    development on macOS, you will likely already have Homebrew.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install `xcode-select`: `xcode-select` is used for different languages. For
    Spark we use Java, Scala and, of course, Spark as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Again, it is likely that you will already have this for other software development
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Homebrew to install Java:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Use Homebrew to install Scala:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Use Homebrew to install Apache Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You should test whether this is working using the Spark shell, as in this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the familiar logo display:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The site continues on to talk about setting up exports and the like, but I did
    not need to do this.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can bring up a Python 3 Notebook and start using Spark in
    Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: You can type `quit()` to exit.
  prefs: []
  type: TYPE_NORMAL
- en: Now, when we run our Notebook while using a Python kernel, we can access Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Windows install
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I had run Spark in Python 2 in previous of Jupyter. I could not get the installs
    to work correctly for Windows.
  prefs: []
  type: TYPE_NORMAL
- en: First Spark script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our first script reads in a text file and sees how much the line lengths add
    up to, as shown next. Note that we are reading in the Notebook file we are running;
    the Notebook is named `Spark File Lengths`, and is stored in the `Spark File Lengths.ipynb` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the `print(totalLengths)` script, we first initialize Spark, but only if
    we have not done so already. Spark will complain if you try to initialize it more
    than once, so all Spark scripts should have this `if` statement prefix.
  prefs: []
  type: TYPE_NORMAL
- en: The script reads in a text file (the source of this script), takes every line
    and computes its length, and then adds all the lengths together.
  prefs: []
  type: TYPE_NORMAL
- en: A `lambda` function is an anonymous (not named) function that takes arguments
    and returns a value. In the first case, given a `s` string return its length.
  prefs: []
  type: TYPE_NORMAL
- en: A `reduce` function takes each value as an argument, applies the second argument
    to it, replaces the first value with the result, and then proceeds with the rest
    of the list. In our case, it walks through the line lengths and adds them all
    up.
  prefs: []
  type: TYPE_NORMAL
- en: Then, running this in a Notebook, we see the following screenshot. Note that
    the size of your file may be slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, the first time you begin the Spark engine (using the line `sc = pyspark.SparkContext()`),
    it may take a while and your script may not complete successfully. If that happens,
    just try it again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25b44c28-0313-4131-8024-e3d8b4e3a4f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Spark word count
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have seen some of the functionality, let''s explore further. We
    can use a script similar to the following to count the word occurrences in a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We have the same preamble to the coding. Then, we load the text file into memory.
  prefs: []
  type: TYPE_NORMAL
- en: Once the file is loaded, we split each line into words and use a `lambda` function
    to tick off each occurrence of a word. The code is truly creating a new record
    for each word occurrence, such as at appears one. The idea is that this process
    could be split over multiple processors, where each processor generates these
    low-level information bits. We are not concerned with optimizing this process
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have all of these records, we reduce/summarize the record set according
    to the word occurrences mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: The `counts` object is called a **Resilient Distributed Dataset** (**RDD**)
    in Spark. It is resilient since care is taken to persist the dataset. The RDD
    is distributed as it can be manipulated by all nodes in the operating cluster.
    And, of course, it is a dataset consisting of a variety of data items.
  prefs: []
  type: TYPE_NORMAL
- en: The last `for` loop runs `collect()` against the RDD. As mentioned, this RDD
    could be distributed among many nodes. The `collect()` function pulls all copies
    of the RDD into one location. Then, we loop through each record.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run this in Jupyter, we see something akin to this displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a7cdef0-9bef-45ad-a289-bcfdba125d25.png)'
  prefs: []
  type: TYPE_IMG
- en: The listing is abbreviated as the list of words continues for some time. What
    is curious is that the word splitting logic in Spark does not appear to work very
    well; some of the results are not words, such as the first entry an empty string.
  prefs: []
  type: TYPE_NORMAL
- en: Sorted word count
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the same script with a minor modification, we can make one more call
    and sort the results. The script now looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have added another function call to RDD creation, `sortByKey()`. So,
    after we have mapped/reduced, and arrived at a list of words and occurrences,
    we can then easily sort the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resultning output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93cdf7dd-0eff-4bd5-bfd0-2eb6b597200e.png)'
  prefs: []
  type: TYPE_IMG
- en: Estimate pi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use `map` or `reduce` to estimate pi if we have code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This code has the same preamble. We are using the Python `random` package. There
    is a constant for the number of samples to attempt.
  prefs: []
  type: TYPE_NORMAL
- en: We are building an RDD called `count`. We call the `parallelize` function to
    split this process between the nodes available. The code just maps the result
    of the `sample` function call. Finally, we reduce the generated map set by adding
    all the samples.
  prefs: []
  type: TYPE_NORMAL
- en: The `sample` function gets two random numbers and returns a one or a zero depending
    on where the two numbers end up in size. We are looking for random numbers in
    a small range and then checking whether they occur within a circle of the same
    diameter. With a large enough sample, we would end up with `PI(3.141...)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run this in Jupyter, we see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23444f8f-7695-43d8-b936-47962275bb0a.png)'
  prefs: []
  type: TYPE_IMG
- en: When I ran this with `NUM_SAMPLES = 100000`, I ended up with `PI = 3.126400`.
  prefs: []
  type: TYPE_NORMAL
- en: Log file examination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I downloaded one of the `access_log` files from `monitorware.com`. Like any
    other web access log, we have one line per entry, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The first part is the IP address of the caller, followed by a timestamp, the
    type of HTTP access, the URL referenced, the HTTP type, the resulting HTTP response
    code, and finally the number of bytes in the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use Spark to load in and parse out some statistics of the log entries,
    as in this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This script has the same preamble as the others. We read in the `access_log`
    file. Then, we print the `count` record.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we find out how many log entries were `GET` and `POST` operations.
    `GET` is assumed to be the most prevalent.
  prefs: []
  type: TYPE_NORMAL
- en: When I first did this, I really didn't expect anything else, so I removed `gets`
    and `posts` from the set and printed out the outliers to see what they were.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run this in Jupyter, we see the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fb013de-6368-4a87-8871-32ed6e0fc998.png)'
  prefs: []
  type: TYPE_IMG
- en: The text processing was not very fast (especially for so few records).
  prefs: []
  type: TYPE_NORMAL
- en: I liked being able to work with the data frames in such a way. There is something
    pleasing about being able to do basic algebra with sets in a programmatic way,
    without having to be concerned about edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, a `HEAD` request works just like  `GET`, but does not return the
    `HTTP` body. This allows a caller to determine what kind of response would have
    come back and respond appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Spark primes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can run a series of numbers through a filter to determine whether each number
    is prime or not. We can use this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The script generates numbers up to `100000`.
  prefs: []
  type: TYPE_NORMAL
- en: We then loop over each of the numbers and pass it to our filter. If the filter
    returns `True`, we get a record. Then, we just count how many results we found.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this in Jupyter, we see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17a3cee1-e1b9-4f85-90d9-12257698f786.png)'
  prefs: []
  type: TYPE_IMG
- en: This was very fast. I was waiting and didn't notice that it went so quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Spark text file analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will look through a news article to determine some basic
    information from the article.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the following script against the 2600 raid news article from
    [https://www.newsitem.com](https://www.newsitem.com):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The code reads in the article and splits it into `sentences`, as determined
    by the appearance of a period. From there, the code maps out the `bigrams` present.
    A bigram is a pair of words that appear next to each other. We then sort the list
    and print out the top ten most prevalent.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run this in a Notebook, we see these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/043463c5-8d7b-4b08-a66f-d04458eb481e.png)'
  prefs: []
  type: TYPE_IMG
- en: I really had no idea what to expect from the output. It's curious that you can
    glean some insight into the article as `the` and `mall` appear `15` times and
    `the` and `guards` appear `11` times—a raid must have occurred in a mall and included
    the security guards in some manner.
  prefs: []
  type: TYPE_NORMAL
- en: Spark evaluating history data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we combine the previous sections to look at some historical
    data and determine a number of useful attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The historical data we are using is the guest list for the Jon Stewart television
    show. A typical record from the data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This contains the year, the occupation of the guest, the date of appearance,
    a logical grouping of the occupations, and the name of the guest.
  prefs: []
  type: TYPE_NORMAL
- en: For our analysis, we will be looking at the number of appearances per year,
    the occupation that appears most frequently, and the personality who appears most
    frequently.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The script has a number of features:'
  prefs: []
  type: TYPE_NORMAL
- en: We are using several packages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has the familiar context preamble.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We start dictionaries for `years`, `occupations`, and `guests`. A dictionary
    contains `key` and `value`. For this use, the key will be the raw value from the
    CSV. The value will be the number of occurrences in the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We open the file and start reading line by line, using a `reader` object. We
    are using ignore errors as there are a couple of nulls in the file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On each line, we take the value of interest (`year`, `occupation`, `name`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We see if the value is present in the appropriate dictionary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is there, increment the value (counter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, initialize an entry in the dictionary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then sort each of the dictionaries in reverse order of the number of appearances
    of the item.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we display the top five values for each dictionary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we run this in a Notebook, we have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db77a6e1-123b-46a5-bf85-43732af2ed73.png)'
  prefs: []
  type: TYPE_IMG
- en: We show the tail of the script and the preceding output.
  prefs: []
  type: TYPE_NORMAL
- en: There may be a smarter way to do all of this, but I am not aware of it.
  prefs: []
  type: TYPE_NORMAL
- en: The build-up of the accumulators is pretty standard, regardless of what language
    you are using. I think there is an opportunity to use a `map()` function here.
  prefs: []
  type: TYPE_NORMAL
- en: I really liked just trimming off the lists/arrays so easily instead of having
    to call a function.
  prefs: []
  type: TYPE_NORMAL
- en: The number of guests per year is very consistent. Actors are prevalent—probably
    the group of people of most interest to the audience. The guest list was a little
    surprising. The guests are mostly actors, but I think all have strong political
    directions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used Spark functionality via Python coding for Jupyter.
    First, we installed the Spark additions to Jupyter. We wrote an initial script
    that just read lines from a text file. We went further and determined the word
    counts in that file. We added sorting to the results. We wrote was a script to
    estimate pi. We evaluated web log files for anomalies. We determined a set of
    prime numbers, and we evaluated a text stream for certain characteristics.
  prefs: []
  type: TYPE_NORMAL
