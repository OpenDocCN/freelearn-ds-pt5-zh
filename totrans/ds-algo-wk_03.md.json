["```py\nS={(none,cold,no),(small,cold,no),(good,cold,no),(none,warm,no),(small,warm,no),(good,warm,yes)}\n```", "```py\nIG(S,swimming suit)=0.3166890883\nIG(S,water temperature)=0.19087450461\n```", "```py\n# source_code/3/construct_decision_tree.py\n# Constructs a decision tree from data specified in a CSV file.\n# Format of a CSV file:\n# Each data item is written on one line, with its variables separated\n# by a comma. The last variable is used as a decision variable to\n# branch a node and construct the decision tree.\n\nimport math\n# anytree module is used to visualize the decision tree constructed by\n# this ID3 algorithm.\nfrom anytree import Node, RenderTree\nimport sys\nsys.path.append('../common')\nimport common\nimport decision_tree\n\n# Program start csv_file_name = sys.argv[1]\nverbose = int(sys.argv[2])  # verbosity level, 0 - only decision tree\n\n# Define the enquired column to be the last one.\n# I.e. a column defining the decision variable.\n(heading, complete_data, incomplete_data,\n enquired_column) = common.csv_file_to_ordered_data(csv_file_name)\n\ntree = decision_tree.constuct_decision_tree(\n    verbose, heading, complete_data, enquired_column)\ndecision_tree.display_tree(tree)\n```", "```py\n# source_code/common/decision_tree.py\n# ***Decision Tree library ***\n# Used to construct a decision tree and a random forest.\nimport math\nimport random\nimport common\nfrom anytree import Node, RenderTree\nfrom common import printfv\n\n# Node for the construction of a decision tree.\nclass TreeNode: \n    def __init__(self, var=None, val=None):\n        self.children = []\n        self.var = var\n        self.val = val\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def get_children(self):\n        return self.children\n\n    def get_var(self):\n        return self.var\n\n    def get_val(self):\n        return self.val\n\n    def is_root(self):\n        return self.var is None and self.val is None\n\n    def is_leaf(self):\n        return len(self.children) == 0\n\n    def name(self):\n        if self.is_root():\n            return \"[root]\"\n        return \"[\" + self.var + \"=\" + self.val + \"]\"\n\n# Constructs a decision tree where heading is the heading of the table\n# with the data, i.e. the names of the attributes.\n# complete_data are data samples with a known value for every attribute.\n# enquired_column is the index of the column (starting from zero) which\n# holds the classifying attribute.\ndef construct_decision_tree(verbose, heading, complete_data, enquired_column):\n    return construct_general_tree(verbose, heading, complete_data,\n                                  enquired_column, len(heading))\n\n# m is the number of the classifying variables that should be at most\n# considered at each node. *m needed only for a random forest.*\ndef construct_general_tree(verbose, heading, complete_data,\n                           enquired_column, m):\n    available_columns = []\n    for col in range(0, len(heading)):\n        if col != enquired_column:\n            available_columns.append(col)\n    tree = TreeNode()\n    printfv(2, verbose, \"We start the construction with the root node\" +\n                        \" to create the first node of the tree.\\n\")\n    add_children_to_node(verbose, tree, heading, complete_data,\n                         available_columns, enquired_column, m)\n    return tree\n\n# Splits the data samples into the groups with each having a different\n# value for the attribute at the column col.\ndef split_data_by_col(data, col):\n    data_groups = {}\n    for data_item in data:\n        if data_groups.get(data_item[col]) is None:\n            data_groups[data_item[col]] = []\n        data_groups[data_item[col]].append(data_item)\n    return data_groups\n\n# Adds a leaf node to node.\ndef add_leaf(verbose, node, heading, complete_data, enquired_column):\n    leaf_node = TreeNode(heading[enquired_column],\n                         complete_data[0][enquired_column])\n    printfv(2, verbose,\n            \"We add the leaf node \" + leaf_node.name() + \".\\n\")\n    node.add_child(leaf_node)\n\n# Adds all the descendants to the node.\ndef add_children_to_node(verbose, node, heading, complete_data,\n                         available_columns, enquired_column, m):\n    if len(available_columns) == 0:\n        printfv(2, verbose, \"We do not have any available variables \" +\n                \"on which we could split the node further, therefore \" +\n                \"we add a leaf node to the current branch of the tree. \")\n        add_leaf(verbose, node, heading, complete_data, enquired_column)\n        return -1\n\n    printfv(2, verbose, \"We would like to add children to the node \" +\n            node.name() + \".\\n\")\n\n    selected_col = select_col(\n        verbose, heading, complete_data, available_columns,\n        enquired_column, m)\n    for i in range(0, len(available_columns)):\n        if available_columns[i] == selected_col:\n            available_columns.pop(i)\n            break\n\n    data_groups = split_data_by_col(complete_data, selected_col)\n    if (len(data_groups.items()) == 1):\n        printfv(2, verbose, \"For the chosen variable \" +\n                heading[selected_col] +\n                \" all the remaining features have the same value \" +\n                complete_data[0][selected_col] + \". \" +\n                \"Thus we close the branch with a leaf node. \")\n        add_leaf(verbose, node, heading, complete_data, enquired_column)\n        return -1\n\n    if verbose >= 2:\n        printfv(2, verbose, \"Using the variable \" +\n                heading[selected_col] +\n                \" we partition the data in the current node, where\" +\n                \" each partition of the data will be for one of the \" +\n                \"new branches from the current node \" + node.name() +\n                \". \" + \"We have the following partitions:\\n\")\n        for child_group, child_data in data_groups.items():\n            printfv(2, verbose, \"Partition for \" +\n                    str(heading[selected_col]) + \"=\" +\n                    str(child_data[0][selected_col]) + \": \" +\n                    str(child_data) + \"\\n\")\n        printfv(\n            2, verbose, \"Now, given the partitions, let us form the \" +\n                        \"branches and the child nodes.\\n\")\n    for child_group, child_data in data_groups.items():\n        child = TreeNode(heading[selected_col], child_group)\n        printfv(2, verbose, \"\\nWe add a child node \" + child.name() +\n                \" to the node \" + node.name() + \". \" +\n                \"This branch classifies %d feature(s): \" +\n                str(child_data) + \"\\n\", len(child_data))\n        add_children_to_node(verbose, child, heading, child_data, list(\n            available_columns), enquired_column, m)\n        node.add_child(child)\n    printfv(2, verbose,\n            \"\\nNow, we have added all the children nodes for the \" +\n            \"node \" + node.name() + \".\\n\")\n\n# Selects an available column/attribute with the highest\n# information gain.\ndef select_col(verbose, heading, complete_data, available_columns,\n               enquired_column, m):\n    # Consider only a subset of the available columns of size m.\n    printfv(2, verbose,\n            \"The available variables that we have still left are \" +\n            str(numbers_to_strings(available_columns, heading)) + \". \")\n    if len(available_columns) < m:\n        printfv(\n            2, verbose, \"As there are fewer of them than the \" +\n                        \"parameter m=%d, we consider all of them. \", m)\n        sample_columns = available_columns\n    else:\n        sample_columns = random.sample(available_columns, m)\n        printfv(2, verbose,\n                \"We choose a subset of them of size m to be \" +\n                str(numbers_to_strings(available_columns, heading)) +\n                \".\")\n\n    selected_col = -1\n    selected_col_information_gain = -1\n    for col in sample_columns:\n        current_information_gain = col_information_gain(\n            complete_data, col, enquired_column)\n        # print len(complete_data),col,current_information_gain\n        if current_information_gain > selected_col_information_gain:\n            selected_col = col\n            selected_col_information_gain = current_information_gain\n    printfv(2, verbose,\n            \"Out of these variables, the variable with \" +\n            \"the highest information gain is the variable \" +\n            heading[selected_col] +\n            \". Thus we will branch the node further on this \" +\n            \"variable. \" +\n            \"We also remove this variable from the list of the \" +\n            \"available variables for the children of the current node. \")\n    return selected_col\n\n# Calculates the information gain when partitioning complete_data\n# according to the attribute at the column col and classifying by the\n# attribute at enquired_column.\ndef col_information_gain(complete_data, col, enquired_column):\n    data_groups = split_data_by_col(complete_data, col)\n    information_gain = entropy(complete_data, enquired_column)\n    for _, data_group in data_groups.items():\n        information_gain -= (float(len(data_group)) / len(complete_data)\n                             ) * entropy(data_group, enquired_column)\n    return information_gain\n\n# Calculates the entropy of the data classified by the attribute\n# at the enquired_column.\ndef entropy(data, enquired_column):\n    value_counts = {}\n    for data_item in data:\n        if value_counts.get(data_item[enquired_column]) is None:\n            value_counts[data_item[enquired_column]] = 0\n        value_counts[data_item[enquired_column]] += 1\n    entropy = 0\n    for _, count in value_counts.items():\n        probability = float(count) / len(data)\n        entropy -= probability * math.log(probability, 2)\n    return entropy\n```", "```py\n# source_code/3/swim.csv\nswimming_suit,water_temperature,swim\nNone,Cold,No\nNone,Warm,No\nSmall,Cold,No\nSmall,Warm,No\nGood,Cold,No\nGood,Warm,Yes \n```", "```py\n$ python construct_decision_tree.py swim.csv 0 Root\n├── [swimming_suit=Small]\n│ ├── [water_temperature=Cold]\n│ │ └── [swim=No]\n│ └── [water_temperature=Warm]\n│   └── [swim=No]\n├── [swimming_suit=None]\n│ ├── [water_temperature=Cold]\n│ │ └── [swim=No]\n│ └── [water_temperature=Warm]\n│   └── [swim=No]\n└── [swimming_suit=Good]\n    ├── [water_temperature=Cold]\n    │   └── [swim=No]\n    └── [water_temperature=Warm]\n        └── [swim=Yes]\n```", "```py\nS={(Cold,Strong,Cloudy,No),(Warm,Strong,Cloudy,No),(Warm,None,Sunny,Yes), (Hot,None,Sunny,No),(Hot,Breeze,Cloudy,Yes),(Warm,Breeze,Sunny,Yes),(Cold,Breeze,Cloudy,No),(Cold,None,Sunny,Yes),(Hot,Strong,Cloudy,Yes),(Warm,None,Cloudy,Yes)}\n```", "```py\nScold={(Cold,Strong,Cloudy,No),(Cold,Breeze,Cloudy,No),(Cold,None,Sunny,Yes)}\nSwarm={(Warm,Strong,Cloudy,No),(Warm,None,Sunny,Yes),(Warm,Breeze,Sunny,Yes),(Warm,None,Cloudy,Yes)}\nShot={(Hot,None,Sunny,No),(Hot,Breeze,Cloudy,Yes),(Hot,Strong,Cloudy,Yes)}\n```", "```py\nsource_code/3/chess.csv\nTemperature,Wind,Sunshine,Play\nCold,Strong,Cloudy,No\nWarm,Strong,Cloudy,No\nWarm,None,Sunny,Yes\nHot,None,Sunny,No\nHot,Breeze,Cloudy,Yes\nWarm,Breeze,Sunny,Yes\nCold,Breeze,Cloudy,No\nCold,None,Sunny,Yes\nHot,Strong,Cloudy,Yes\nWarm,None,Cloudy,Yes\n```", "```py\n$ python construct_decision_tree.py chess.csv 0\nRoot\n├── [Temperature=Cold]\n│ ├── [Wind=Breeze]\n│ │ └── [Play=No]\n│ ├── [Wind=Strong]\n│ │ └── [Play=No]\n│ └── [Wind=None]\n│   └── [Play=Yes]\n├── [Temperature=Warm]\n│ ├── [Wind=Breeze]\n│ │ └── [Play=Yes]\n│ ├── [Wind=None]\n│ │ ├── [Sunshine=Sunny]\n│ │ │ └── [Play=Yes]\n│ │ └── [Sunshine=Cloudy]\n│ │   └── [Play=Yes]\n│ └── [Wind=Strong]\n│   └── [Play=No]\n└── [Temperature=Hot]\n    ├── [Wind=Strong]\n    │ └── [Play=Yes]\n    ├── [Wind=None]\n    │ └── [Play=No]\n    └── [Wind=Breeze]\n        └── [Play=Yes]\n```", "```py\n    Root\n    ├── [Temperature=Cold]\n    │    ├──[Rain=None]\n    │    │    └──[Shopping=Yes]\n    │    └──[Rain=Strong]\n    │         └──[Shopping=Yes]\n    └── [Temperature=Warm]\n         ├──[Rain=None]\n         │    └──[Shopping=No]\n         └── [Rain=Strong]\n              └── [Shopping=No]\n\n```", "```py\n    Root\n    ├── [Season=Autumn]\n    │    ├──[Wind=Breeze]\n    │    │    └──[Play=Yes]\n    │    ├──[Wind=Strong]\n    │    │    └──[Play=No]\n    │    └──[Wind=None]\n    │         └──[Play=Yes]\n    ├── [Season=Summer]\n    │    ├──[Temperature=Hot]\n    │    │    └──[Play=Yes]\n    │    └──[Temperature=Warm]\n    │         └──[Play=Yes]\n    ├── [Season=Winter]\n    │    └──[Play=No]\n    └── [Season=Spring]\n         ├── [Temperature=Hot]\n         │    └──[Play=No]\n         ├── [Temperature=Warm]\n         │    └──[Play=Yes]\n         └── [Temperature=Cold]\n              └── [Play=Yes]\n\n```"]