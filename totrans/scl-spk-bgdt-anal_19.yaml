- en: PySpark and SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss two other popular APIs: PySpark and SparkR
    for writing Spark code in Python and R programming languages respectively. The
    first part of this chapter will cover some technical aspects while working with
    Spark using PySpark. Then we will move to SparkR and see how to use it with ease.
    The following topics will be discussed throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation and getting started with PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with DataFrame APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UDFs with PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analytics using PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to SparkR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why SparkR?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation and getting started with SparkR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data processing and manipulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with RDD and DataFrame using SparkR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization using SparkR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is one of the most popular and general purpose programming languages
    with a number of exciting features for data processing and machine learning tasks.
    To use Spark from Python, PySpark was initially developed as a lightweight frontend
    of Python to Apache Spark and using Spark's distributed computation engine. In
    this chapter, we will discuss a few technical aspects of using Spark from Python
    IDE such as PyCharm.
  prefs: []
  type: TYPE_NORMAL
- en: Many data scientists use Python because it has a rich variety of numerical libraries
    with a statistical, machine learning, or optimization focus. However, processing
    large-scale datasets in Python is usually tedious as the runtime is single-threaded.
    As a result, data that fits in the main memory can only be processed. Considering
    this limitation and for getting the full flavor of Spark in Python, PySpark was
    initially developed as a lightweight frontend of Python to Apache Spark and using
    Spark's distributed computation engine. This way, Spark provides APIs in non-JVM
    languages like Python.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this PySpark section is to provide basic distributed algorithms
    using PySpark. Note that PySpark is an interactive shell for basic testing and
    debugging and is not supposed to be used for a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installation and configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many ways of installing and configuring PySpark on Python IDEs such
    as PyCharm, Spider, and so on. Alternatively, you can use PySpark if you have
    already installed Spark and configured the `SPARK_HOME`. Thirdly, you can also
    use PySpark from the Python shell. Below we will see how to configure PySpark
    for running standalone jobs.
  prefs: []
  type: TYPE_NORMAL
- en: By setting SPARK_HOME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At first, download and place the Spark distribution at your preferred place,
    say `/home/asif/Spark`. Now let''s set the `SPARK_HOME` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s set `PYTHONPATH` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to add the following two paths to the environmental path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s refresh the current terminal so that the newly modified `PATH`
    variable is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'PySpark depends on the `py4j` Python package. It helps the Python interpreter
    to dynamically access the Spark object from the JVM. This package can be installed
    on Ubuntu as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, the default `py4j`, which is already included in Spark (`$SPARK_HOME/python/lib`),
    can be used too.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like Scala interactive shell, an interactive shell is also available for Python.
    You can execute Python code from Spark root folder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If the command went fine, you should observer the following screen on Terminal
    (Ubuntu):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00375.jpeg)**Figure 1**: Getting started with PySpark shell'
  prefs: []
  type: TYPE_NORMAL
- en: Now you can enjoy Spark using the Python interactive shell. This shell might
    be sufficient for experimentations and developments. However, for production level,
    you should use a standalone application.
  prefs: []
  type: TYPE_NORMAL
- en: 'PySpark should be available in the system path by now. After writing the Python
    code, one can simply run the code using the Python command, then it runs in local
    Spark instance with default configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that the current distribution of Spark is only Python 2.7+ compatible.
    Hence, we will have been strict on this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, it is better to use the `spark-submit` script if you want to pass
    the configuration values at runtime. The command is pretty similar to the Scala
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The configuration values can be passed at runtime, or alternatively, they can
    be changed in the `conf/spark-defaults.conf` file. After configuring the Spark
    config file, the changes also get reflected while running PySpark applications
    using a simple Python command.
  prefs: []
  type: TYPE_NORMAL
- en: However, unfortunately, at the time of this writing, there's no pip install
    advantage for using PySpark. But it is expected to be available in the Spark 2.2.0
    release (for more, refer to [https://issues.apache.org/jira/browse/SPARK-1267](https://issues.apache.org/jira/browse/SPARK-1267)).
    The reason why there is no pip install for PySpark can be found in the JIRA ticket
    at [https://issues.apache.org/jira/browse/SPARK-1267](https://issues.apache.org/jira/browse/SPARK-1267).
  prefs: []
  type: TYPE_NORMAL
- en: By setting PySpark on Python IDEs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can also configure and run PySpark from Python IDEs such as PyCharm. In this
    section, we will show how to do it. If you're a student, you can get the free
    licensed copy of PyCharm once you register using your university/college/institute
    email address at [https://www.jetbrains.com/student/](https://www.jetbrains.com/student/).
    Moreover, there's also a community (that is, free) edition of PyCharm, so you
    don't need to be a student in order to use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently PySpark has been published with Spark 2.2.0 PyPI (see [https://pypi.python.org/pypi/pyspark](https://pypi.python.org/pypi/pyspark)/.
    This has been a long time coming (previous releases included pip installable artifacts
    that for a variety of reasons couldn''t be published to PyPI). So if you (or your
    friends) want to be able to work with PySpark locally on your laptop you''ve got
    an easier path getting started, just execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if you are using Windos 7, 8 or 10, you should install pyspark manually.
    For exmple using PyCharm, you can do it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00281.jpeg)**Figure 2:** Installing PySpark on Pycharm IDE on Windows
    10'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, you should create a Python script with Project interpreter as Python
    2.7+. Then you can import pyspark along with other required models as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that if you''re a Windows user, Python also needs to have the Hadoop runtime;
    you should put the `winutils.exe` file in the `SPARK_HOME/bin` folder. Then create
    a environmental variable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select your python file | Run | Edit configuration | Create an environmental
    variable whose key is `HADOOP_HOME` and the value is the `PYTHON_PATH` for example
    for my case it''s `C:\Users\admin-karim\Downloads\spark-2.1.0-bin-hadoop2.7`.
    Finally, press OK then you''re done:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00110.jpeg)**Figure 3:** Setting Hadoop runtime env on Pycharm IDE
    on Windows 10'
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s all you need. Now if you start writing Spark code, you should at first
    place the imports in the `try` block as follows (just for example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And the `catch` block can be placed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following figure that shows importing and placing Spark packages
    in the PySpark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00256.jpeg)**Figure 4**: Importing and placing Spark packages in PySpark
    shell'
  prefs: []
  type: TYPE_NORMAL
- en: 'If these blocks execute successfully, you should observe the following message
    on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00005.jpeg)**Figure 5**: PySpark package has been imported successfully'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before going deeper, at first, we need to see how to create the Spark session.
    It can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now under this code block, you should place your codes, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code demonstrates how to compute principal components on a RowMatrix
    and use them to project the vectors into a low-dimensional space. For a clearer
    picture, refer to the following code that shows how to use the PCA algorithm on
    PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00188.jpeg)**Figure 6**: PCA result after successful execution of the
    Python script'
  prefs: []
  type: TYPE_NORMAL
- en: Working with DataFrames and RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SparkDataFrame is a distributed collection of rows under named columns. Less
    technically, it can be considered as a table in a relational database with column
    headers. Furthermore, PySpark DataFrame is similar to Python pandas. However,
    it also shares some mutual characteristics with RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Immutable**: Just like an RDD, once a DataFrame is created, it can''t be
    changed. We can transform a DataFrame to an RDD and vice versa after applying
    transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lazy Evaluations:** Its nature is a lazy evaluation. In other words, a task
    is not executed until an action is performed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed:** Both the RDD and DataFrame are distributed in nature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like Java/Scala's DataFrames, PySpark DataFrames are designed for processing
    a large collection of structured data; you can even handle petabytes of data.
    The tabular structure helps us understand the schema of a DataFrame, which also
    helps optimize execution plans on SQL queries. Additionally, it has a wide range
    of data formats and sources.
  prefs: []
  type: TYPE_NORMAL
- en: You can create RDDs, datasets, and DataFrames in a number of ways using PySpark.
    In the following subsections, we will show some examples of doing that.
  prefs: []
  type: TYPE_NORMAL
- en: Reading a dataset in Libsvm format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how to read data in LIBSVM format using the read API and the `load()`
    method by specifying the format of the data (that is, `libsvm`) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding MNIST dataset can be downloaded from [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2).
    This will essentially return a DataFrame and the content can be seen by calling
    the `show()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00068.gif)**Figure 7**: A snap of the handwritten dataset in LIBSVM
    format'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also specify other options such as how many features of the raw dataset
    you want to give to your DataFrame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if you want to create an RDD from the same dataset, you can use the MLUtils
    API from `pyspark.mllib.util` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can save the RDD in your preferred location as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Reading a CSV file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with loading, parsing, and viewing simple flight data. At first,
    download the NYC flights dataset as a CSV from [https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv](https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv).
    Now let''s load and parse the dataset using `read.csv()` API of PySpark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This is pretty similar to reading the libsvm format. Now you can see the resulting
    DataFrame''s structure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00338.gif)**Figure 8**: Schema of the NYC flight dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see a snap of the dataset using the `show()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s view the sample of the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00370.gif)**Figure 9**: Sample of the NYC flight dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Reading and manipulating raw text files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can read a raw text data file using the `textFile()` method. Suppose you
    have the logs of some purchase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now reading and creating RDD is pretty straightforward using the `textFile()`
    method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the structure is not that readable. So we can think of giving
    a better structure by converting the texts as DataFrame. At first, we need to
    collect the header information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now filter out the header and make sure the rest looks correct as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We still have the RDD but with a bit better structure of the data. However,
    converting it into DataFrame will provide a better view of the transactional data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code creates a DataFrame by specifying the `header.split` is
    providing the names of the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00208.gif)**Figure 10**: Sample of the transactional data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you could save this DataFrame as a view and make a SQL query. Let''s do
    a query with this DataFrame now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00051.gif)**Figure 11**: Query result on the transactional data using
    Spark SQL'
  prefs: []
  type: TYPE_NORMAL
- en: Writing UDF on PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like Scala and Java, you can also work with **User Defined Functions** (aka.
    **UDF**) on PySpark. Let's see an example in the following. Suppose we want to
    see the grade distribution based on the score for some students who have taken
    courses at a university.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can store them in two separate arrays as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s declare an empty array for storing the data about courses and students
    so that later on both can be appended to this array as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that for the preceding code to work, please import the following at the
    beginning of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now let's create a DataFrame from these two objects toward converting corresponding
    grades against each one's score. For this, we need to define an explicit schema.
    Let's suppose that in your planned DataFrame, there would be three columns named
    `Student`, `Course`, and `Score`.
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, let''s import necessary modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the schema can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s create an RDD from the Raw Data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s convert the RDD into the DataFrame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00311.gif)**Figure 12**: Sample of the randomly generated score for
    students in subjects'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, now we have three columns. However, we need to convert the score into
    grades. Say you have the following grading schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '*90~100=> A*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*80~89 => B*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*60~79 => C*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*0~59 => D*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For this, we can create our own UDF such that this will convert the numeric
    score to grade. It can be done in several ways. Following is an example of doing
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can have our own UDF as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The second argument in the `udf()` method is the return type of the method
    (that is, `scoreToCategory`). Now you can call this UDF to convert the score into
    grade in a pretty straightforward way. Let''s see an example of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The preceding line will take score as input for all entries and convert the
    score to a grade. Additionally, a new DataFrame with a column named `Grade` will
    be added.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00354.gif)**Figure 13**: Assigned grades'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can use the UDF with the SQL statement as well. However, for that, we
    need to register this UDF as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line will register the UDF as a temporary function in the database
    by default. Now we need to create a team view to allow executing SQL queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s execute an SQL query on the view `score` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00153.gif)**Figure 14**: Query on the students score and corresponding
    grades'
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete source code for this example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: A more detailed discussion on using UDF can be found at [https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html.](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html)
  prefs: []
  type: TYPE_NORMAL
- en: Now let's do some analytics tasks on PySpark. In the next section, we will show
    an example using the k-means algorithm for a clustering task using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Let's do some analytics with k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anomalous data refers to data that is unusual from normal distributions. Thus,
    detecting anomalies is an important task for network security, anomalous packets
    or requests can be flagged as errors or potential attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will use the KDD-99 dataset (can be downloaded here: [http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html)
    ). A number of columns will be filtered out based on certain criteria of the data
    points. This will help us understand the example. Secondly, for the unsupervised
    task; we will have to remove the labeled data. Let''s load and parse the dataset
    as simple texts. Then let''s see how many rows there are in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This essentially returns an RDD. Let''s see how many rows in the dataset are
    using the `count()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the dataset is pretty big with lots of features. Since we have parsed the
    dataset as simple texts, we should not expect to see the better structure of the
    dataset. Thus, let''s work toward converting the RDD into DataFrame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then let''s see some selected columns in the DataFrame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00080.gif)**Figure 15**: Sample of the KKD cup 99 dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, this dataset is already labeled. This means that the types of malicious
    cyber behavior have been assigned to a row where the label is the last column
    (that is, `_42`). The first five rows off the DataFrame are labeled normal. This
    means that these data points are normal. Now this is the time that we need to
    determine the counts of the labels for the entire dataset for each type of labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00134.gif)**Figure 16**: Available labels (attack types) in the KDD
    cup dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that there are 23 distinct labels (behavior for data objects). The
    most data points belong to Smurf. This is an abnormal behavior also known as DoS
    packet floods. The Neptune is the second highest abnormal behavior. The *normal*
    events are the third most occurring types of events in the dataset. However, in
    a real network dataset, you will not see any such labels.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the normal traffic will be much higher than any anomalous traffic. As
    a result, identifying the anomalous attack or anomaly from the large-scale unlabeled
    data would be tedious. For simplicity, let's ignore the last column (that is,
    labels) and think that this dataset is unlabeled too. In that case, the only way
    to conceptualize the anomaly detection is using unsupervised learning algorithms
    such as k-means for clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s work toward clustering the data points for this. One important thing
    about K-means is that it only accepts numeric values for modeling. However, our
    dataset also contains some categorical features. Now we can assign the categorical
    features binary values of 1 or 0 based on whether they are *TCP* or not. This
    can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, our dataset is almost ready. Now we can prepare our training and test
    set to training the k-means model with ease:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: However, some standardization is also required since we converted some categorical
    features to numeric features. Standardization can improve the convergence rate
    during the optimization process and can also prevent features with very large
    variances exerting an influence during model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will use StandardScaler, which is a feature transformer. It helps us
    standardize features by scaling them to unit variance. It then sets the mean to
    zero using column summary statistics in the training set samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s compute the summary statistics by fitting the preceding transformer
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the problem is the data that we have for training the k-means does not
    have a normal distribution. Thus, we need to normalize each feature in the training
    set to have the unit standard deviation. To make this happen, we need to further
    transform the preceding standardizer model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Well done! Now the training set is finally ready to train the k-means model.
    As we discussed in the clustering chapter, the trickiest thing in the clustering
    algorithm is finding the optimal number of clusters by setting the value of K
    so that the data objects get clustered automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'One Naive approach considered a brute force is setting `K=2` and observing
    the results and trying until you get an optimal one. However, a much better approach
    is the Elbow approach, where we can keep increasing the value of `K` and compute
    the **Within Set Sum of Squared Errors** (**WSSSE**) as the clustering cost. In
    short, we will be looking for the optimal `K` values that also minimize the WSSSE.
    Whenever a sharp decrease is observed, we will get to know the optimal value for
    `K`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, 30 is the best value for k. Let''s check the cluster assignments
    for each data point when we have 30 clusters. The next test would be to run for
    `k` values of 30, 35, and 40\. Three values of k are not the most you would test
    in a single run, but only used for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00100.jpeg)**Figure 17**: Final cluster centers for each attack type
    (abridged)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s compute and print the total cost for the overall clustering as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the WSSSE of our k-means model can be computed and printed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Your results might be slightly different. This is due to the random placement
    of the centroids when we first begin the clustering algorithm. Performing this
    many times allows you to see how points in your data change their value of k or
    stay the same. The full source code for this solution is given in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: A more comprehensive discussion on this topic can be found at [https://github.com/jadianes/kdd-cup-99-spark](https://github.com/jadianes/kdd-cup-99-spark).
    Also, interested readers can refer to the main and latest documentation on PySpark
    APIs at [http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/).
  prefs: []
  type: TYPE_NORMAL
- en: Well, now it's time to move to SparkR, another Spark API to work with population
    statistical programming language called R.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R is one of the most popular statistical programming languages with a number
    of exciting features that support statistical computing, data processing, and
    machine learning tasks. However, processing large-scale datasets in R is usually
    tedious as the runtime is single-threaded. As a result, only datasets that fit
    in someone's machine memory can be processed. Considering this limitation and
    for getting the full flavor of Spark in R, SparkR was initially developed at the
    AMPLab as a lightweight frontend of R to Apache Spark and using Spark's distributed
    computation engine.
  prefs: []
  type: TYPE_NORMAL
- en: This way it enables the R programmer to use Spark from RStudio for large-scale
    data analysis from the R shell. In Spark 2.1.0, SparkR provides a distributed
    data frame implementation that supports operations such as selection, filtering,
    and aggregation. This is somewhat similar to R data frames like `dplyr` but can
    be scaled up for large-scale datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Why SparkR?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can write Spark codes using SparkR too that supports distributed machine
    learning using MLlib. In summary, SparkR inherits many benefits from being tightly
    integrated with Spark including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supports various data sources API**: SparkR can be used to read in data from
    a variety of sources including Hive tables, JSON files, RDBMS, and Parquet files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataFrame optimizations**: SparkR DataFrames also inherit all of the optimizations
    made to the computation engine in terms of code generation, memory management,
    and so on. From the following graph, it can be observed that the optimization
    engine of Spark enables SparkR competent with Scala and Python:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00108.jpeg)**Figure 18:** SparkR DataFrame versus Scala/Python DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability:** Operations executed on SparkR DataFrames get automatically
    distributed across all the cores and machines available on the Spark cluster.
    Thus, SparkR DataFrames can be used on terabytes of data and run on clusters with
    thousands of machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best way of using SparkR is from RStudio. Your R program can be connected
    to a Spark cluster from RStudio using R shell, Rescript, or other R IDEs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Option 1.** Set `SPARK_HOME` in the environment (you can check [https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html)),
    load the SparkR package, and call `sparkR.session` as follows. It will check for
    the Spark installation, and, if not found, it will be downloaded and cached automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '**Option 2.** You can also manually configure SparkR on RStudio. For doing
    so, create an R script and execute the following lines of R code on RStudio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now load the SparkR library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, like Scala/Java/PySpark, the entry point to your SparkR program is the
    SparkR session that can be created by calling `sparkR.session` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, if you want, you could also specify certain Spark driver properties.
    Normally, these application properties and runtime environment cannot be set programmatically,
    as the driver JVM process would have been started; in this case, SparkR takes
    care of this for you. To set them, pass them as you would pass other configuration
    properties in the `sparkConfig` argument to `sparkR.session()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, the following Spark driver properties can be set in `sparkConfig`
    with `sparkR.session` from RStudio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00146.gif)**Figure 19**: Spark driver properties can be set in `sparkConfig`
    with `sparkR.session` from RStudio'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start with loading, parsing, and viewing simple flight data. At first,
    download the NY flights dataset as a CSV from [https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv](https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv).
    Now let''s load and parse the dataset using `read.csv()` API of R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s view the structure of the dataset using `View()` method of R as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00217.jpeg)**Figure 20**: A snap of the NYC flight dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s create the Spark DataFrame from the R DataFrame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the structure by exploring the schema of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00197.gif)**Figure 21**: The schema of the NYC flight dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see the first 10 rows of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00222.gif)**Figure 22**: The first 10 rows of the NYC flight dataset'
  prefs: []
  type: TYPE_NORMAL
- en: So, you can see the same structure. However, this is not scalable since we loaded
    the CSV file using standard R API. To make it faster and scalable, like in Scala,
    we can use external data source APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Using external data source APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned earlier, we can create DataFrame using external data source APIs
    as well. For the following example, we used `com.databricks.spark.csv` API as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the structure by exploring the schema of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00132.gif)**Figure 23**: The same schema of the NYC flight dataset
    using external data source API'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see the first 10 rows of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00003.jpeg)**Figure 24**: Same sample data from NYC flight dataset
    using external data source API'
  prefs: []
  type: TYPE_NORMAL
- en: So, you can see the same structure. Well done! Now it's time to explore something
    more, such as data manipulation using SparkR.
  prefs: []
  type: TYPE_NORMAL
- en: Data manipulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Show the column names in the SparkDataFrame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Show the number of rows in the SparkDataFrame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter flights data whose destination is only Miami and show the first six
    entries as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00285.jpeg)**Figure 25**: Flights with destination Miami only'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select specific columns. For example, let''s select all the flights that are
    going to Iowa that are delayed. Also, include the origin airport names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00269.gif)**Figure 26**: All the flights that are going to Iowa that
    are delayed'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can even use it to chain data frame operations. To show an example, at first,
    group the flights by date and then find the average daily delay. Then, finally,
    write the result into a SparkDataFrame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now print the computed DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00300.gif)**Figure 27**: Group the flights by date and then find the
    average daily delay'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see another example that aggregates average arrival delay for the entire
    destination airport:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Even more complex aggregation can be performed. For example, the following
    code aggregates the average, maximum, and minimum delay per each destination airport.
    It also shows the number of flights that land in those airports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00290.gif)**Figure 28**: Maximum and minimum delay per each destination
    airport'
  prefs: []
  type: TYPE_NORMAL
- en: Querying SparkR DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to Scala, we can perform a SQL query on the DataFrame once it is saved
    as `TempView` using the `createOrReplaceTempView()` method. Let''s see an example
    of that. At first, let''s save the fight DataFrame (that is, `flightDF`) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s select destination and destinations of all the flights with their
    associated carrier information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00293.jpeg)**Figure 29**: All the flights with their associated carrier
    information'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s make the SQL a bit more complex, such as finding the destination''s
    airport of all the flights that are at least 120 minutes delayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment queries and shows the name of the airports of all
    the flights that are delayed by at least 2 hours:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00302.jpeg)**Figure 30**: Destination airports of all the flights that
    are delayed by at least 2 hours'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s do a more complex query. Let''s find the origins of all the flights
    to Iowa that are delayed by at least 2 hours. Finally, sort them by arrival delay
    and limit the count up to 20 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment queries and shows the name of the airports of all
    the flights that are delayed by at least 2 hours to Iowa:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00308.jpeg)**Figure 31**: Origins of all the flights that are delayed
    by at least 2 hours where the destination is Iowa'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing your data on RStudio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we have seen how to load, parse, manipulate, and query
    the DataFrame. Now it would be great if we could show the data for better visibility.
    For example, what could be done for the airline carriers? I mean, is it possible
    to find the most frequent carriers from the plot? Let''s give `ggplot2` a try.
    At first, load the library for the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Now we already have the SparkDataFrame. What if we directly try to use our SparkSQL
    DataFrame class in `ggplot2`?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Obviously, it doesn''t work that way because the `ggplot2` function doesn''t
    know how to deal with those types of distributed data frames (the Spark ones).
    Instead, we need to collect the data locally and convert it back to a traditional
    R data frame as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s have a look at what we got using the `str()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'This is good because when we collect results from a SparkSQL DataFrame, we
    get a regular R `data.frame`. It is also very convenient since we can manipulate
    it as needed. And now we are ready to create the `ggplot2` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s give the plot a proper representation as a bar diagram as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00314.jpeg)**Figure 32**: Most frequent carriers are UA, B6, EV, and
    DL'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the graph, it is clear that the most frequent carriers are UA, B6, EV,
    and DL. This gets clearer from the following line of code in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00317.gif)**Figure 33:** Most most frequent carriers are UA, B6, EV,
    and DL'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code of the preceding analysis is given in the following to
    understand the flow of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we showed some examples of how to write your Spark code in
    Python and R. These are the most popular programming languages in the data scientist
    community.
  prefs: []
  type: TYPE_NORMAL
- en: We covered the motivation of using PySpark and SparkR for big data analytics
    with almost similar ease with Java and Scala. We discussed how to install these
    APIs on their popular IDEs such as PyCharm for PySpark and RStudio for SparkR.
    We also showed how to work with DataFrames and RDDs from these IDEs. Furthermore,
    we discussed how to execute Spark SQL queries from PySpark and SparkR. Then we
    also discussed how to perform some analytics with visualization of the dataset.
    Finally, we saw how to use UDFs with PySpark with examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we have discussed several aspects for two Spark''s APIs; PySpark and
    SparkR. There are much more to explore. Interested readers should refer to their
    websites for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PySpark: [http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "SparkR: [https://spark.apache.org/docs/latest/sparkr.html\uFEFF](https://spark.apache.org/docs/latest/sparkr.html)"
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
