- en: Chapter 8. Creating Ensemble Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A group of people have the ability to take better decisions than a single individual,
    especially when each group member comes in with their own biases. The ideology
    is also true for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: When single algorithms are not capable to generate the true prediction function,
    then ensemble machine-learning methods are used. When there is more focus on the
    performance of the model rather than the training time and the complexity of the
    model, then ensemble methods are preferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: What is ensemble learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing ensembles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combination strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting, bagging, and injecting randomness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is ensemble learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ensemble learning is a machine learning method where various models are prepared
    to work on the same problem. It is a process where multiple models are generated
    and the results obtained from them are combined to produce the final result. Moreover,
    ensemble models are inherently parallel; therefore, they are much more efficient
    at training and testing if we have access to multiple processors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ordinary methods of machine learning**: These use training data for learning
    a specific hypothesis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble learning**: This uses training data to build a set of hypothesis.
    These hypotheses are combined to build the final model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, it can be said that ensemble learning is the way toward preparing
    different individual learners for an objective function that utilizes different
    strategies, and in the long run, combines these learnings.
  prefs: []
  type: TYPE_NORMAL
- en: '![What is ensemble learning?](img/B05321_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding ensemble learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensemble learning, as we discussed, combines the learning of the various individual
    learners. It is the aggregation of multiple learned models that aim to improve
    accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Base learners: Each individual learner is called a base learner. Base learners
    may be suited for a specific situation, but are not good in generalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the weak generalization capability of the base learner, they are not
    suited for every scenario.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble learning uses these base (weak) learners to construct a strong learner
    which results in a comparatively much more accurate model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, decision tree algorithms are used as the base learning algorithm.
    Using the same kind of learning algorithm results in homogeneous learners. However,
    different algorithms can also be used, which will result in heterogeneous learners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to construct an ensemble
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is recommended that the base learners should be as diverse as possible. This
    enables the ensemble to handle and predict most of the situation with better accuracy.
    This diversity can be produced using different subsets of the dataset, manipulating
    or transforming the inputs, and using different techniques simultaneously for
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Also, when the individual base learners have a high accuracy then there is a
    good chance of having good accuracy with the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, construction of an ensemble is a two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create the base learners. They are generally constructed
    in parallel, but if base learners are influenced by the previously formed base
    learners then they are constructed in a sequential manner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second step is to combine these base learners and create an ensemble which
    is best suited to the use case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By using different types of base learners and combination techniques, we can
    altogether produce different ensemble learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different ways to implement the ensemble model:'
  prefs: []
  type: TYPE_NORMAL
- en: Sub-sample the training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulate the input features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulate the output features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inject randomness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning parameters of the classifier can be modified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combination strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Combination strategies can be classified into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Static combiners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive combiners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Static combiners:** The combiner choice standard is autonomous of the component
    vector. Static methodologies can be comprehensively partitioned into trainable
    and non-trainable.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trainable**: The combiner goes through a different training stage to enhance
    the performance of the ensemble. Here are two approaches that are widely used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weighted averaging**: The yield of every classifier is weighted by its very
    own performance measure:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the accuracy of prediction on a different validation set
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacked generalization**: The yield of the ensemble is treated as the feature
    vector to a meta-classifier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-trainable**: Performance of the individual classifier does not have an
    affect on the voting. Different combiners might be utilized. This depends upon
    the sort of yield delivered by the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Voting**: This is used when a single class label is generated by every classifier.
    Every classifier votes for a specific class. The class that receives the larger
    part vote on the ensemble wins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Averaging**: When a confidence estimate is created by every classifier then
    averaging is utilized. The class that has the highest number of posterior in the
    ensemble wins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Borda counts**: This is used when a rank is produced by every classifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive combiners**: This is a type of combiner function that is dependent
    on the feature vector given as input:'
  prefs: []
  type: TYPE_NORMAL
- en: A function that is local to every region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divide and conquer methodology creates modular ensembles and simple classifiers
    specializing in different regions of I/O space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The individual specialists are required to perform well in their region of ability
    and not for all inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subsampling training dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The learner is considered to be unstable if the output classifier has to undergo
    radical changes when there are some small variations in the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unstable learners**: Decision trees, neural networks, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stable learners**: Nearest neighbor, linear regression, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This particular technique is more suited for the unstable learners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two very common techniques used in subsampling are:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bagging is also known as Bootstrap Aggregation. It generates the additional
    data that is used for training by using sub-sampling on the same dataset with
    replacement. It creates multiple combinations with repetitions to generate the
    training dataset of the same size.
  prefs: []
  type: TYPE_NORMAL
- en: As sampling with replacement is done, on an average, each classifier is trained
    on 63.2% of the training example.
  prefs: []
  type: TYPE_NORMAL
- en: After training on these multiple datasets, bagging combines the result by majority
    voting. The class that received the most number of votes wins. By using these
    multiple datasets, bagging aims to reduce the variance. Accuracy is improved if
    the induced classifiers are uncorrelated.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest, a type of ensemble learning, uses bagging and is one of the most
    powerful methods.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through the bagging algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For iteration, *t=1 to T*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample randomly from the training dataset with replacement *N* samples
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Base learner is trained (for example, decision tree or neural network) on this
    sample
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For test sample, *t=1 to T*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start all the models that were trained
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prediction is done on the basis of the following:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: Averaging'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: Majority vote'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Bagging](img/B05321_08_02-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When does bagging work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Bagging works in scenarios where there would have been over-fitting if it wasn''t
    used. Let''s go through these scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Under-fitting**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High bias**: Models are not good enough and don''t fit the training data
    well'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small variance**: Whenever there is a change in the dataset, there is a very
    small change that needs to be done in the classifier'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Over-fitting**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small bias**: Models fit too well to the training data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Large variance**: Whenever there is a small change in the dataset, there
    is a large or drastic change that needs to be done in the classifier'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging aims to reduce the variance without affecting the bias. Therefore, the
    dependency of the model on the training dataset is reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Boosting is different to the bagging approach. It is based on the PAC framework,
    which is the Probably Approximately Correct framework.
  prefs: []
  type: TYPE_NORMAL
- en: '**PAC learning**: The probability of having larger confidence and smaller accuracy
    than the misclassification error:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: This is the percentage of correctly classifying the samples in
    the test dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confidence**: This is the probability of achieving the accuracy in the given
    experiment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The boosting approach is based on the concept of the "weak learner". When an
    algorithm performs somewhat better than 50% in binary classification tasks, then
    it is called a weak learner. The approach is to combine multiple weak learners
    together, and the aim is to:'
  prefs: []
  type: TYPE_NORMAL
- en: Improve confidence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is done by training these different weak learners on different datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Training:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samples are taken randomly from the dataset.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: First, learner *h1* is trained on the sample.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy of this learner, *h1*, is evaluated on the dataset.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A similar process is followed using different samples for multiple learners.
    They are divided such that they classify differently.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Test:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the test dataset, the learning is applied using the majority vote of all
    the learners
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence can also be boosted in a similar way, as boosting the accuracy with
    some trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting is less of an algorithm and rather it is a "framework". The main aim
    of this framework is to take a weak learning algorithm W and turn it into a strong
    learning algorithm. We will now discuss AdaBoost, which is short for "adaptive
    boosting algorithm". AdaBoost became famous because it was one of the first successful
    and practical boosting algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: It does not require a large number of hyper-parameters to be defined and executes
    in a polynomial time. The benefit of this algorithm is that it has the ability
    to automatically adapt to the data given to it.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost – boosting by sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After *n* iterations, a weak learner having distribution *D* over the training
    set is provided by the boosting:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All examples have the equal probability of being selected as the first component
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sub-sampling of the training set is done in accordance with distribution *Dn*
    and a model is trained by the weak learner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weights of misclassified instances are adjusted in a way that subsequent
    classifiers work on comparatively difficult cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A distribution *D(n+1)* is generated with the probability of misclassified samples
    increasing and correctly classified samples decreasing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After *t* iterations, according to the performance of the models, the votes
    of individual hypotheses are weighted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strength of AdaBoost derives from the adaptive resampling of examples, not
    from the final weighted combination.
  prefs: []
  type: TYPE_NORMAL
- en: What is boosting doing?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Every classifier has a specialization on the specific subset of dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An algorithm concentrates on examples with increasing levels of difficulty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting is able to reduce variance (similar to bagging)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also able to eliminate the consequences of high bias of the weak learner
    (not present in bagging)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Train versus test errors performance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can reduce the train errors to nearly 0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting is not present and is evident by the test errors
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The bias and variance decomposition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s discuss how bagging and boosting affect the bias-variance decomposition
    of the classification error:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Features of errors that can be expected from a learning algorithm:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bias term is a measure of performance of the classifier with respect to the
    target function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Variance terms measure the robustness of the classifier; if there is a change
    in the training dataset, then how is the model affected by it?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging and boosting are capable to reduce the variance term, and therefore
    reducing the errors in the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also proved that boosting attempts to reduce the bias term since it focuses
    on misclassified samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating the input features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One other technique with which we can generate multiple classifiers is by manipulating
    the set of input features which we feed to the learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can select different subsets of features and networks of different sizes.
    The input features subsets may be manually selected rather than in an automated
    fashion. This technique is widely used in image processing: one of the very famous
    examples is Principle Component Analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: The ensemble classifier generated in many experiments was able to perform like
    an actual human.
  prefs: []
  type: TYPE_NORMAL
- en: It was also found that when we delete even a few of the features that we gave
    as the input, it affects the performance of the classifier. This affects the overall
    voting, and the ensemble thus generated is not able to perform up to the expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Injecting randomness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is another universally useful technique for producing an ensemble of classifiers.
    In this method, we inject randomness into the learner algorithm. Neural networks
    with back-propagation are also created using the same technique for hidden weights.
    On the off chance that the calculation is connected to the same preparing illustrations,
    but with various starting weights, the subsequent classifier can be very diverse.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most computationally costly parts of outfits of decision trees involves
    preparing the decision tree. This is quick for decision stumps; however, for more
    profound trees, it can be restrictively costly.
  prefs: []
  type: TYPE_NORMAL
- en: The costly part is picking the tree structure. Once the tree structure is picked,
    it is extremely cheap to fill in the leaves (which is the predictions of the trees)
    utilizing the training data. A shockingly productive and successful option is
    to utilize trees with altered structures and random features. Accumulations of
    trees are called forests, thus classifiers fabricated like this are called random
    forests.
  prefs: []
  type: TYPE_NORMAL
- en: 'It takes three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The depth of the decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number form
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The calculation produces each of the K trees freely which makes it simple to
    parallelize. For every tree, it develops a full binary tree of a given depth.
    The elements utilized at the branches of this tree are chosen randomly, regularly
    with substitution, implying that the same element can seem numerous at times,
    even in one branch. Based on the training data, the leaves that will perform the
    actual predictions are filled. This last step is the main time when the training
    data is utilized. The subsequent classifier then only involves the voting of the
    K-numerous random trees.
  prefs: []
  type: TYPE_NORMAL
- en: The most astonishing thing about this methodology is that it works strikingly
    well. It tends to work best when the greater part of the components are not significant,
    since the quantity of features chosen for any given tree is minimal. A portion
    of the trees will query the useless features.
  prefs: []
  type: TYPE_NORMAL
- en: These trees will basically make random forecasts. Be that as it may, a portion
    of the trees will query good features and will make good forecasts (on the grounds
    that the leaves are assessed in light of the training data). In the event that
    you have enough trees, the arbitrary ones will wash out as noise, and just the
    good trees will affect the final classification.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Random forests were developed by Leo Breiman and Adele Cutler. Their strength
    in the field of machine learning has been shown nicely in a blog entry at Strata
    2012: "Ensembles of decision trees (often known as random forests) have been the
    most successful general-purpose algorithm in modern times", as they "automatically
    identify the structure, interactions, and relationships in the data".'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it has been noticed that "most Kaggle solutions have no less than
    one top entry that vigorously utilizes this methodology". Random forests additionally
    have been the preferred algorithm for recognizing the body part in Microsoft's
    Kinect, which is a movement detecting information gadgets for Xbox consoles and
    Windows PCs.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests comprises of a group of decision trees. We will consequently
    begin to analyze decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree, as discussed previously, is a tree-like chart where on each
    node there is a choice, in view of one single feature. Given an arrangement of
    features, the tree is navigated from node to node, as indicated by these decisions,
    until you come to a leaf. The name of this leaf is the expectation for the given
    list of features. A straightforward decision tree could be utilized to choose
    what you have to bring with you when going out.
  prefs: []
  type: TYPE_NORMAL
- en: '![Random forests](img/B05321_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Each tree is constructed in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly take an *n* sample case where n is the number of cases in the training
    set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacement is used to select these n cases. This particular set of data is
    used to construct the tree.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The node is split by *x<X*, where *X* is the number of input variables. The
    *X* doesn't change with the growth of the forest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning is not done as trees are allowed to go the maximum depth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The error rate in a random forest is dependent on the following (as given in
    the original paper):'
  prefs: []
  type: TYPE_NORMAL
- en: When correlation among the trees increases, the error rate in the random forest
    increases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When individual trees are weak, the error rate increases and it decreases when
    the individual trees are strengthened
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is found that *x*, which was mentioned previously, has an effect on correlation
    and strength. Increasing *x* increases both strength and correlation, and decreasing
    *x* decreases both. We try to find the particular value or the range where the
    *x* should lie to have the minimal error.
  prefs: []
  type: TYPE_NORMAL
- en: We use the **oob** (**out of bag**) error to find the best value or the range
    of the value of *x*.
  prefs: []
  type: TYPE_NORMAL
- en: This tree does not classify every one of the points effectively. We could transform
    this by expanding the tree depth. Thus, the tree can foresee the specimen data
    100% effectively, just by taking in the noise in the examples. In the most extreme
    case, the calculation is proportional to a word reference containing each specimen.
    This is known as over-fitting and prompts awful results when utilizing it for
    out of test forecasts. Keeping in mind the end goal to overcome over-fitting,
    we can prepare numerous decision trees by presenting weights for the examples
    and just considering an irregular subset of the features for every split. A definite
    conclusion of the random forest will be controlled by a lion's share vote on the
    trees' forecasts. This method is otherwise called bagging. It diminishes the variance
    (errors from noise in the training set) without expanding the bias (errors because
    of the insufficient adaptability of the model).
  prefs: []
  type: TYPE_NORMAL
- en: Features of random forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A random forest is highly accurate as compared to the existing algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used effectively and efficiently on big data. They are fast and don't
    require expensive hardware to run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the key features of a random forest is its ability to deal several numbers
    of input variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A random forest can show the generalization error estimate during the process
    of construction of the forest. It can also give the important variables for the
    classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the data is sparse, a random forest is an effective algorithm with good
    accuracy. It can also work on prediction of missing data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The models generated can be utilized later on data that we might receive in
    the future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In unbalanced datasets, it provides the feature to balance the error present
    in the class population.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these features can be used for unsupervised clustering too and also
    as an effective method for outlier detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One other key feature of a random forest is that it doesn't overfit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do random forests work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To comprehend and utilize the different choices, additional data about how they
    are figured is valuable. The vast majority of the alternatives rely upon two information
    objects produced by random forests.
  prefs: []
  type: TYPE_NORMAL
- en: At the point when the training set for the present tree is drawn by sampling
    with replacement, around 33% of the cases are left well enough alone for the example.
  prefs: []
  type: TYPE_NORMAL
- en: As more and more trees are added to the random forest, the information from
    the oob helps to generate the estimate of the classification error. After the
    construction of every tree, the greater part of the information keeps running
    down the tree, and vicinities are computed for every pair of cases.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if the same terminal node is shared by two cases then we
    increase their proximity by 1\. After the complete analysis, these proximities
    are normalized. Proximities are utilized as a part of supplanting missing information,
    finding outliers, and delivering in-depth perspectives of the information.
  prefs: []
  type: TYPE_NORMAL
- en: The out-of-bag (oob) error estimate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Random forests eliminate the requirement to cross-validate to achieve the unbiased
    estimate of the test set error. During the construction, it is estimated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Every tree is constructed utilizing an alternate bootstrap sample from the original
    information. Around 33% of the cases are left alone for the bootstrap test and
    not utilized as a part of the development of the kth tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There may be some cases that were not considered during the construction of
    the trees. We put these cases down the kth to achieve a classification. It produces
    a classification test set in around 33% of the trees. Towards the end of the run,
    take *j* to be the class that received the vast majority of the votes each time
    the case *n* was oob. The number of times that *j* is not equivalent to the true
    class of *n* at the midpoint of all the cases is the oob error estimate. This
    follows being unbiased in the various tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gini importance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The impurity measure is frequently used for decision trees. Misclassification
    is measured and is called gini impurity which applies in the context where there
    are multiple classifiers present.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a gini coefficient. This is applicable to binary classification.
    It needs a classifier that is able to rank the sample according to the probability
    of being in the right class.
  prefs: []
  type: TYPE_NORMAL
- en: Proximities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned before, we don't prune the trees while constructing the random
    forest. Therefore, the terminal nodes don't have many numbers of instances.
  prefs: []
  type: TYPE_NORMAL
- en: To find the proximity measure, we run all the cases from the training set down
    the tree. Let's say case x and case y arrive at the same terminal node, we then
    increase the proximity by 1.
  prefs: []
  type: TYPE_NORMAL
- en: After the run, we take twice the number of trees and divide the proximity of
    the case which was increased by 1 by this number.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in Julia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Random forests are available in the Julia-registered packages from Kenta Sato:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a CART-based random forest implementation in Julia. This package supports:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out-of-bag (OOB) errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature importances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various configurable parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two separate models available in this package:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each model has its own constructor that is trained by applying the fit method.
    We can configure these constructors with some keyword arguments listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This one is for the classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This one is for the regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: This is the number of weak estimators'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: This is the number of candidate features at each split'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If Integer is given, the fixed number of features are used
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If FloatingPoint is given, the proportions of the given value (0.0, 1.0] are
    used
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If Symbol is given, the number of candidate features is decided by a strategy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:sqrt: ifloor(sqrt(n_features))`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:third: div(n_features, 3)`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: The maximum depth of each tree'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default argument nothing means there is no limitation of the maximum depth
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: The minimum number of sub-samples to try to split a node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`criterion`: The criterion of the impurity measure (classification only)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:`gini: Gini index'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:entropy`: Cross entropy'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomForestRegressor` always uses the mean squared error for its impurity
    measure. Currently, there are no configurable criteria for the regression model.'
  prefs: []
  type: TYPE_NORMAL
- en: Learning and prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our example, we will be using the amazing "DecisionTree" package provided
    by Ben Sadeghi.
  prefs: []
  type: TYPE_NORMAL
- en: 'The package supports the following available models:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DecisionTreeClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DecisionTreeRegressor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomForestClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomForestRegressor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AdaBoostStumpClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Installation is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us start with the classification example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We now take the famous iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates a pruned tree classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Learning and prediction](img/B05321_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It generates such a tree given in the previous image. We now apply this learned
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'It generates the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s train the random forest classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'It generates the random forest classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will apply this learned model and check the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s train a regression tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'It generates the following tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now training a regression forest is made straightforward by the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Why is ensemble learning superior?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To comprehend the generalization power of ensemble learning being superior to
    an individual learner, Dietterich provided three reasons.
  prefs: []
  type: TYPE_NORMAL
- en: 'These three reasons help us understand the reason for the superiority of ensemble
    learning leading to a better hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: The training information won't give adequate data to picking a single best learner.
    For instance, there might be numerous learners performing similarly well on the
    training information set. In this way, joining these learners might be a superior
    decision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second reason is that, the search procedures of the learning algorithms
    may be defective. For instance, regardless of the possibility that there exists
    a best hypothesis, the learning algorithms may not be able to achieve that due
    to various reasons including generation of an above average hypothesis. Ensemble
    learning can improve on that part by increasing the possibility to achieve the
    best hypothesis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third reason is that one target function may not be present in the hypothesis
    space that we are searching in. This target function may lie in a combination
    of various hypothesis spaces, which is similar to combining various decision trees
    to generate the random forest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are numerous hypothetical studies on acclaimed ensemble techniques. For
    example, boosting and bagging are the methods to achieve these three points discussed.
  prefs: []
  type: TYPE_NORMAL
- en: It is also observed that boosting does not experience the ill effects of over-fitting
    even after countless, and now and then it is even ready to diminish the generalization
    error after the training error has achieved zero. Although numerous scientists
    have considered this marvel, hypothetical clarifications are still in belligerence.
  prefs: []
  type: TYPE_NORMAL
- en: The bias-variance decomposition is frequently utilized as a part of studying
    the execution of ensemble techniques. It is observed that bagging is able to nearly
    eliminate the variance, and by doing so becomes ideal to attach to learners that
    experience huge variance, such as unstable learners, decision trees, or neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting is able to minimize the bias, notwithstanding diminishing the variance,
    and by doing so becomes more viable to weak learners such as decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of ensemble learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensemble learning is used widely in applications, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Optical character recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text categorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computer-aided medical diagnosis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble learning can be used in nearly all scenarios where machine learning
    techniques are used.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble learning is a method for generating highly accurate classifiers by
    combining weak or less accurate ones. In this chapter, we discussed some of the
    methods for constructing ensembles and went through the three fundamental reasons
    why ensemble methods are able to outperform any single classifier within the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed bagging and boosting in detail. Bagging, also known as Bootstrap
    Aggregation, generates the additional data that is used for training by using
    sub-sampling on the same dataset with replacement. We also learned why AdaBoost
    performs so well and understood in detail about random forests. Random forests
    are highly accurate and efficient algorithms that don't overfit. We also studied
    how and why they are considered as one of the best ensemble models. We implemented
    a random forest model in Julia using the "DecisionTree" package.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf](http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.machine-learning.martinsewell.com/ensembles/ensemble-learning.pdf](http://www.machine-learning.martinsewell.com/ensembles/ensemble-learning.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://web.cs.wpi.edu/~xkong/publications/papers/ecml11.pdf](http://web.cs.wpi.edu/~xkong/publications/papers/ecml11.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
