<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Predictive Data Analytics – Modeling and Validation</h1>
                </header>
            
            <article>
                
<p>Our utmost objective in conducting various data analyses is trying to find patterns in order to predict what might happen in the future. For the stock market, researchers and professionals are conducting various tests to understand market mechanisms. In this case, many questions could be asked. What will the market index level be in the next five years? What will IBM's price range be next year? Will the market volatility increase or decrease in the future? What might be the impact if governments change their tax policies? What is the potential gain and loss if one country launches a trade war with another one? How do we predict a consumer's behavior by analyzing some related variables? Could we predict the probability that an undergraduate student will successfully graduate? Could we find an association between certain behaviors of one specific disease?</p>
<p>In this chapter, the following topics will be covered:</p>
<ul>
<li>Understanding predictive data analytics</li>
<li>Useful dataset</li>
<li>Predicting future events</li>
<li>Model selection</li>
<li>Granger causality test</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding predictive data analytics</h1>
                </header>
            
            <article>
                
<p>In terms of future events, people could have many questions. For an investor, if he/she could predict the future movement of a stock price, he/she could make more profit. For a company, if they could forecast the trend of their products, they could increase their stock price and products' market shares. For governments, if they could predict the impact of an aging population on society and the economy, they would have more incentive to design a better policy in terms of government budget and other related strategic decisions.</p>
<p>For universities, if they could have a good grasp of the market demand in terms of quality and skill sets for their graduates, they could design a set of better programs or launch new programs to satisfy the future needs in terms of a labor force.</p>
<p>For a better prediction or forecast, researchers have to consider many issues. For example, is the sample too small? How do we remove missing variables? Is this dataset biased in terms of data collection procedures? How do we treat extreme values or outliers? What is the seasonality and how do we deal with it? What kinds of models should we apply? In this chapter, some of these issues will be touched upon. We start with the useful dataset.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Useful datasets</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">One of the best data sources is the <strong>UCI Machine Learning Repository</strong>. When we go to the web page at <a href="https://archive.ics.uci.edu/ml/datasets.html"><span class="URLPACKT">https://archive.ics.uci.edu/ml/datasets.html</span></a>, we see the following list:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/a9587907-a45b-4cbe-96dd-1311c15b3231.png" style="width:36.25em;height:32.17em;" width="696" height="617"/></div>
<p>For example, if we click the first dataset (<span class="packt_screen">Abalone</span>), we see the following. To save space, only the top part is shown:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/acbbd129-c545-45ce-ab26-912bfaf16444.png" style="width:42.33em;height:21.17em;" width="586" height="293"/></div>
<p>From the web page, users can download the dataset and find definitions of variables and even citations. The code that follows can be used to download a related R dataset:</p>
<pre>dataSet&lt;-"UCIdatasets" 
path&lt;-"http://canisius.edu/~yany/RData/" 
con&lt;-paste(path,dataSet,".RData",sep='') 
load(url(con)) 
dim(.UCIdatasets) 
head(.UCIdatasets) </pre>
<p>The related output is shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/b6b6bf66-c2dd-43c9-b1a9-9ecc2ec02dab.png" style="width:50.25em;height:8.58em;" width="971" height="165"/></div>
<p>From the preceding output, we know that the dataset has <kbd>427</kbd> observations (dataset). For each dataset, we have <kbd>7</kbd> related features, such as <kbd>Name</kbd>, <kbd>Data_Types</kbd>, <kbd>Default_Task</kbd>, <kbd>Attribute_Types</kbd>, <kbd>N_Instances</kbd> (number of instances), <kbd>N_Attributes</kbd> (number of attributes), and <kbd>Year</kbd>. The variable called <kbd>Default_Task</kbd> could be interpreted as the basic usage of each dataset. For example, the first dataset, called <kbd>Abalone</kbd>, could be used for <kbd>Classification</kbd> problems. The <kbd>unique()</kbd> function could be used to find out all possible <kbd>Default_Task</kbd>, shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/422f53a0-2c9e-49ec-a8e1-3dd33534925d.png" style="width:46.83em;height:10.92em;" width="768" height="178"/></div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The AppliedPredictiveModeling R package</h1>
                </header>
            
            <article>
                
<p>This package includes many useful datasets that can be used for this chapter and others. The easiest way to find those datasets is with the <kbd>help()</kbd> function, shown here:</p>
<pre>&gt;library(AppliedPredictiveModeling) 
&gt;help(package=AppliedPredictiveModeling) </pre>
<p>The following table shows the datasets included in this package:</p>
<table style="border-collapse: collapse;width: 100%" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td style="width: 223px">
<div class="CDPAlignCenter CDPAlign"><strong>Data (name)</strong></div>
</td>
<td style="width: 193px">
<div class="CDPAlignCenter CDPAlign"><strong>Names of dataset(s)</strong></div>
</td>
<td style="width: 303px">
<div class="CDPAlignCenter CDPAlign"><strong>Explanation</strong></div>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>abalone</kbd></p>
</td>
<td>
<p><kbd>abalone</kbd></p>
</td>
<td>
<p>Abalone data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>bio</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>bio</kbd></p>
</td>
<td style="width: 303px">
<p>Hepatic injury data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>bookTheme</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>bookTheme</kbd></p>
</td>
<td style="width: 303px">
<p>Lattice themes</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>cars2010</kbd>,<kbd>cars2011</kbd>, and <kbd>cars2012</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>cars2010</kbd>,<kbd>cars2011</kbd>, <kbd>cars2012</kbd></p>
</td>
<td style="width: 303px">
<p>Fuel economy data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>chem</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>chem</kbd></p>
</td>
<td style="width: 303px">
<p>Hepatic injury data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>ChemicalManufacturingProcess</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>ChemicalManufacturingProcess</kbd></p>
</td>
<td style="width: 303px">
<p>Chemical manufacturing process data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>classes</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>classes</kbd></p>
</td>
<td style="width: 303px">
<p>Two class example data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>concrete</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>concrete</kbd></p>
</td>
<td style="width: 303px">
<p>Compressive strength of concrete</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>diagnosis</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>diagnosis</kbd></p>
</td>
<td style="width: 303px">
<p>Alzheimer's disease CSF data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>easyBoundaryFunc</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>easyBoundaryFunc</kbd></p>
</td>
<td style="width: 303px">
<p>Functions for simulating data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>fingerprints</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>fingerprints</kbd></p>
</td>
<td style="width: 303px">
<p>Permeability data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>getPackages</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>getPackages</kbd></p>
</td>
<td style="width: 303px">
<p>Install packages for each chapter</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>injury</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>injury</kbd></p>
</td>
<td style="width: 303px">
<p>Hepatic injury data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>logisticCreditPredictions</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>logisticCreditPredictions</kbd></p>
</td>
<td style="width: 303px">
<p>Credit data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>mixtures</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>mixtures</kbd></p>
</td>
<td style="width: 303px">
<p>Compressive strength of concrete</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>permeability</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>permeability</kbd></p>
</td>
<td style="width: 303px">
<p>Permeability data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>permuteRelief</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>permuteRelief</kbd></p>
</td>
<td style="width: 303px">
<p>Permutation statistics for the relief algorithm</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>predictors</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>predictors</kbd></p>
</td>
<td style="width: 303px">
<p>Alzheimer's disease CSF data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>quadBoundaryFunc</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>quadBoundaryFunc</kbd></p>
</td>
<td style="width: 303px">
<p>Functions for simulating data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>schedulingData</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>schedulingData</kbd></p>
</td>
<td style="width: 303px">
<p>HPC job scheduling data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>scriptLocation</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>scriptLocation</kbd></p>
</td>
<td style="width: 303px">
<p>Find chapter script files</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>segmentationOriginal</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>segmentationOriginal</kbd></p>
</td>
<td style="width: 303px">
<p>Cell body segmentation</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>solubility</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>solTestX</kbd>,<kbd>solTestXtrans</kbd>,<kbd>solTestY</kbd>,<kbd>solTrainX</kbd>,</p>
<p><kbd>solTrainXtrans</kbd>,<kbd>solTrainY</kbd>, and <kbd>trainX</kbd></p>
</td>
<td style="width: 303px">
<p>Solubility data</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>transparentTheme</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>transparentTheme</kbd></p>
</td>
<td style="width: 303px">
<p>Lattice themes</p>
</td>
</tr>
<tr>
<td style="width: 223px">
<p><kbd>twoClassData</kbd></p>
</td>
<td style="width: 193px">
<p><kbd>twoClassData</kbd></p>
</td>
<td style="width: 303px">
<p>Two class example data</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Table 10.1 datasets embedded in the R package AppliedPredictiveModeling</div>
<p>Here, we show you a few examples of how to load these datasets. To load one set of data, we use the <kbd>data()</kbd> function. For the first dataset, called <kbd>abalone</kbd>, we have the following code:</p>
<pre>&gt; library(AppliedPredictiveModeling) 
&gt; data(abalone) 
&gt; dim(abalone) 
&gt; head(abalone)</pre>
<p>The output is as follows:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/bbc1cb31-46bf-404a-8870-e8b7a73b3548.png" style="width:42.75em;height:10.08em;" width="754" height="177"/></p>
<p>For some, the big dataset includes a few sub-datasets:</p>
<pre>&gt; library(AppliedPredictiveModeling) 
&gt; data(solubility) 
&gt; ls(pattern="sol") 
<strong>[1] "solTestX"       "solTestXtrans"  "solTestY"
[4] "solTrainX"      "solTrainXtrans" "solTrainY"</strong></pre>
<p>To load each dataset, we could use the <kbd>dim()</kbd>, <kbd>head()</kbd>, <kbd>tail()</kbd>, and <kbd>summary()</kbd> functions.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Time series analytics</h1>
                </header>
            
            <article>
                
<p>Time series can be defined as a quantity's set of values obtained at successive times, often with equal intervals between them. There are different frequencies such as annual, quarterly, monthly, weekly, and daily. For the GDP (Gross Domestic Product) time series, we usually have quarterly or annual ones. For stock data, we usually have annual, monthly, and daily frequencies. Using the following code, we could upload the US GDP data for both quarterly and annual frequencies:</p>
<pre>&gt; path&lt;-"http://canisius.edu/~yany/RData/" 
&gt; dataSet&lt;-"usGDPannual" 
&gt; con&lt;-paste(path,dataSet,".RData",sep='') 
&gt; load(url(con)) 
&gt; head(.usGDPannual) 
<strong>  YEAR  GDP 
1 1930 92.2 
2 1931 77.4 
3 1932 59.5 
4 1933 57.2 
5 1934 66.8 
6 1935 74.3</strong> 
&gt; dataSet&lt;-"usGDPquarterly" 
&gt; con&lt;-paste(path,dataSet,".RData",sep='') 
&gt; load(url(con)) 
&gt; head(.usGDPquarterly) 
<strong>    DATE GDP_CURRENT GDP2009DOLLAR 
1 1947Q1       243.1        1934.5 
2 1947Q2       246.3        1932.3 
3 1947Q3       250.1        1930.3 
4 1947Q4       260.3        1960.7 
5 1948Q1       266.2        1989.5 
6 1948Q2       272.9        2021.9</strong> </pre>
<p>However, we have many issues for time series analysis. For example, from the point of view of macroeconomics, we have business or economic cycles which could be viewed when the economy is expanding and in recession. Industries or companies could have seasonality. Using the agriculture industry as an example, farmers would spend more during spring and fall seasons and less on winter. For the retail industry, they would have a huge cash inflow during the end-of-year holiday season.</p>
<p>To manipulate time series, we could use many useful functions included in an R package called <kbd>timeSeries</kbd>. For the following program, we average daily data with a weekly frequency:</p>
<pre>&gt; library(timeSeries)<br/>&gt; data(MSFT)<br/>&gt; x &lt;- MSFT<br/>&gt; by &lt;- timeSequence(from = start(x), to = end(x), by = "week")<br/>&gt; y&lt;-aggregate(x,by,mean)</pre>
<p>We could also use the <kbd>head()</kbd> function to see a few observations:</p>
<pre>&gt; head(x)<br/><strong>GMT</strong><br/><strong>              Open High Low Close Volume</strong><br/><strong>2000-09-27 63.4375 63.5625 59.8125 60.6250 53077800</strong><br/><strong>2000-09-28 60.8125 61.8750 60.6250 61.3125 26180200</strong><br/><strong>2000-09-29 61.0000 61.3125 58.6250 60.3125 37026800</strong><br/><strong>2000-10-02 60.5000 60.8125 58.2500 59.1250 29281200</strong><br/><strong>2000-10-03 59.5625 59.8125 56.5000 56.5625 42687000</strong><br/><strong>2000-10-04 56.3750 56.5625 54.5000 55.4375 68226700</strong><br/>&gt; head(y)<br/><strong>GMT</strong><br/><strong>              Open High Low Close Volume</strong><br/><strong>2000-09-27 63.4375 63.5625 59.8125 60.6250 53077800</strong><br/><strong>2000-10-04 59.6500 60.0750 57.7000 58.5500 40680380</strong><br/><strong>2000-10-11 54.9750 56.4500 54.1625 55.0875 36448900</strong><br/><strong>2000-10-18 53.0375 54.2500 50.8375 52.1375 50631280</strong><br/><strong>2000-10-25 61.7875 64.1875 60.0875 62.3875 86457340</strong><br/><strong>2000-11-01 66.1375 68.7875 65.8500 67.9375 53496000</strong></pre>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Predicting future events</h1>
                </header>
            
            <article>
                
<p>There are many techniques we could employ when trying to predict the future, such as <strong>moving average</strong> (<strong>MA</strong>), regression, auto-regression, and the like. First, let's start with the simplest one for a moving average:</p>
<pre>movingAverageFunction&lt;- function(data,n=10){<br/>  out= data<br/>  for(i in n:length(data)){<br/>    out[i] = mean(data[(i-n+1):i])<br/>  }<br/>  return(out)<br/>}</pre>
<p>In the preceding program, the default value for the number of periods is <kbd>10</kbd>. We could use the dataset called <kbd>MSFT</kbd> included in the R package called <kbd>timeSeries</kbd> (see the code that follows):</p>
<pre>&gt; library(timeSeries)<br/>&gt; data(MSFT)<br/>&gt; p&lt;-MSFT$Close<br/>&gt; #<br/>&gt; ma&lt;-movingAverageFunction(p,3)<br/>&gt; head(p)<br/><strong>[1] 60.6250 61.3125 60.3125 59.1250 56.5625 55.4375</strong><br/>&gt; head(ma)<br/><strong>[1] 60.62500 61.31250 60.75000 60.25000 58.66667 57.04167</strong><br/>&gt; mean(p[1:3])<br/><strong>[1] 60.75</strong><br/>&gt; mean(p[2:4])<br/><strong>[1] 60.25</strong></pre>
<p>Manually, we find that the average of the first three values of <em>x</em> is the same as the third value of <em>y</em>. In a sense, we could use the moving average to predict the future.</p>
<p>In the next example, we will show you how to estimate the expected next year's market return. Here, we use the S&amp;P500 index and the historical annual average as our expected values. First, we could go to Yahoo!Finance to download the data. The symbol for the S&amp;P500 index is <kbd>^GSPC</kbd>:</p>
<pre>library(plyr)<br/>year&lt;-c(2000,2000,2001,2001,2004)<br/>values&lt;-c(2, 3, 3, 5, 6)<br/>df &lt;- data.frame(DATE=year,B =values )<br/>dfsum &lt;- ddply(df, c("DATE"),summarize,B=sum(B))</pre>
<p>The output is shown here:</p>
<pre>&gt; df<br/><strong>  DATE B</strong><br/><strong>1 2000 2</strong><br/><strong>2 2000 3</strong><br/><strong>3 2001 3</strong><br/><strong>4 2001 5</strong><br/><strong>5 2004 6</strong><br/>&gt; dfsum<br/><strong>  DATE B</strong><br/><strong>1 2000 5</strong><br/><strong>2 2001 8</strong><br/><strong>3 2004 6</strong></pre>
<p>Now, we download the S&amp;P500 index's historical monthly price data from Yahoo!Finance. Alternatively, we could use the following code to download the R dataset. The first several commands are used to download the related dataset called .<kbd>sp500monthly</kbd>. The objective of the program is to estimate annual mean and its 90% confidence range:</p>
<pre>&gt; library(data.table)<br/>&gt; path&lt;-'http://canisius.edu/~yany/RData/'<br/>&gt; dataSet&lt;-'sp500monthly.RData'<br/>&gt; link&lt;-paste(path,dataSet,sep='')<br/>&gt; load(url(link))<br/>&gt; #head(.sp500monthly,2)<br/>&gt; p&lt;-.sp500monthly$ADJ.CLOSE<br/>&gt; n&lt;-length(p)<br/>&gt; logRet&lt;-log(p[2:n]/p[1:(n-1)])<br/>&gt; years&lt;-format(.sp500monthly$DATE[2:n],"%Y")<br/>&gt; y&lt;-data.frame(.sp500monthly$DATE[2:n],years,logRet)<br/>&gt; colnames(y)&lt;-c("DATE","YEAR","LOGRET")<br/>&gt; y2&lt;- data.table(y)<br/>&gt; z&lt;-y2[,sum(LOGRET),by=YEAR]<br/>&gt; z2&lt;-na.omit(z)<br/>&gt; annualRet&lt;-data.frame(z2$YEAR,exp(z2[,2])-1)<br/>&gt; n&lt;-nrow(annualRet)<br/>&gt; std&lt;-sd(annualRet[,2])<br/>&gt; stdErr&lt;-std/sqrt(n)<br/>&gt; ourMean&lt;-mean(annualRet[,2])<br/>&gt; min2&lt;-ourMean-2*stdErr<br/>&gt; max2&lt;-ourMean+2*stdErr<br/>&gt; cat("[min mean max ]\n")<br/><strong>[min mean max ]</strong><br/>&gt; cat(min2,ourMean,max2,"\n")<br/><strong>0.05032956 0.09022369 0.1301178</strong></pre>
<p>From the result, the historical mean annual returns for S&amp;P500 is 9%. If we claimed that next year's index return would be 9% as well, it could be between 5% and 13%, a potential huge swing.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Seasonality</h1>
                </header>
            
            <article>
                
<p>In the next example, we will show you auto-correlation. First, we upload an R package called <kbd>astsa</kbd>, which stands for <strong>Applied Statistical Time Series Analysis</strong>. Then, we upload the US GDP with quarterly frequency:</p>
<pre>library(astsa)<br/>path&lt;-"http://canisius.edu/~yany/RData/"<br/>dataSet&lt;-"usGDPquarterly"<br/>con&lt;-paste(path,dataSet,".RData",sep='')<br/>load(url(con))<br/>x&lt;-.usGDPquarterly$DATE<br/>y&lt;-.usGDPquarterly$GDP_CURRENT<br/>plot(x,y)<br/>diff4 = diff(y,4)<br/>acf2(diff4,24)</pre>
<p>In the preceding program, the <kbd>diff()</kbd> function takes a difference, such as the current value minus the previous value. The second input value indicates the lag. The function called <kbd>acf2()</kbd> is used to plot and print the ACF and PACF of a time series. ACF stands for <strong>auto-covariance function</strong>, while PACF stands for the <strong>partial auto-correlation function</strong>. The related graphs are shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/5e6f8677-2f47-4131-b91e-e55150949837.png" style="width:26.50em;height:27.33em;" width="548" height="563"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Visualizing components</h1>
                </header>
            
            <article>
                
<p>It is understandable that the concepts and datasets would be much more clear if we could use graphs. The first example shows the fluctuation of the US GDP over the last five decades:</p>
<pre>&gt; path&lt;-"http://canisius.edu/~yany/RData/"<br/>&gt; dataSet&lt;-"usGDPannual"<br/>&gt; con&lt;-paste(path,dataSet,".RData",sep='')<br/>&gt; load(url(con))<br/>&gt; title&lt;-"US GDP"<br/>&gt; xTitle&lt;-"Year"<br/>&gt; yTitle&lt;-"US annual GDP"<br/>&gt; x&lt;-.usGDPannual$YEAR<br/>&gt; y&lt;-.usGDPannual$GDP<br/>&gt; plot(x,y,main=title,xlab=xTitle,ylab=yTitle)</pre>
<p>The related graph is shown here:<br/></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/f5b79b72-a3ec-413b-ad67-27474b4d0220.png" style="width:22.83em;height:21.92em;" width="428" height="411"/></div>
<p>If we used log scale for the GDP, we would have the following code and graph:</p>
<pre>&gt; yTitle&lt;-"Log US annual GDP" <br/>&gt; plot(x,log(y),main=title,xlab=xTitle,ylab=yTitle)</pre>
<p>The following graph is close to a straight line:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/5bb7b753-57a8-42a5-b6f6-b57aec1c920e.png" style="width:21.33em;height:20.67em;" width="433" height="419"/></div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">R package – LiblineaR</h1>
                </header>
            
            <article>
                
<p>This package is the linear predictive models based on the <kbd>LIBLINEAR C/C++ Library</kbd>. Here is one example using the <kbd>iris</kbd> dataset. The program tries to predict which category a plant belongs to by using training data:</p>
<pre>library(LiblineaR)<br/>data(iris)<br/>attach(iris)<br/>x=iris[,1:4]<br/>y=factor(iris[,5])<br/>train=sample(1:dim(iris)[1],100)<br/>xTrain=x[train,];xTest=x[-train,]<br/>yTrain=y[train]; yTest=y[-train]<br/>s=scale(xTrain,center=TRUE,scale=TRUE)<br/>#<br/>tryTypes=c(0:7)<br/>tryCosts=c(1000,1,0.001)<br/>bestCost=NA<br/>bestAcc=0<br/>bestType=NA<br/>#<br/>for(ty in tryTypes){<br/>   for(co in tryCosts){<br/>     acc=LiblineaR(data=s,target=yTrain,type=ty,cost=co,bias=1,cross=5,verbose=FALSE)<br/>     cat("Results for C=",co,": ",acc," accuracy.\n",sep="")<br/>     if(acc&gt;bestAcc){<br/>         bestCost=co<br/>         bestAcc=acc<br/>         bestType=ty<br/>     }<br/>   }<br/>}<br/>cat("Best model type is:",bestType,"\n")<br/>cat("Best cost is:",bestCost,"\n")<br/>cat("Best accuracy is:",bestAcc,"\n")<br/># Re-train best model with best cost value.<br/>m=LiblineaR(data=s,target=yTrain,type=bestType,cost=bestCost,bias=1,verbose=FALSE)<br/># Scale the test data<br/>s2=scale(xTest,attr(s,"scaled:center"),attr(s,"scaled:scale"))<br/>pr=FALSE; # Make prediction<br/>if(bestType==0 || bestType==7) pr=TRUE<br/>p=predict(m,s2,proba=pr,decisionValues=TRUE)<br/>res=table(p$predictions,yTest) # Display confusion matrix<br/>print(res)<br/># Compute Balanced Classification Rate<br/>BCR=mean(c(res[1,1]/sum(res[,1]),res[2,2]/sum(res[,2]),res[3,3]/sum(res[,3])))<br/>print(BCR)</pre>
<p>The output is as follows. The <kbd>BCR</kbd> is the <strong>Balanced Classification Rate</strong>. For this rate, the higher, the better:</p>
<pre>&gt; cat("Best model type is:",bestType,"\n")<br/><strong>Best model type is: 4</strong> <br/>&gt; cat("Best cost is:",bestCost,"\n")<br/><strong>Best cost is: 1</strong> <br/>&gt; cat("Best accuracy is:",bestAcc,"\n")<br/><strong>Best accuracy is: 0.98</strong> <br/>&gt; print(res)<br/>     <strong>       yTest</strong><br/><strong>             setosa versicolor virginica</strong><br/><strong>  setosa 16 0 0</strong><br/><strong>  versicolor 0 17 0</strong><br/><strong>  virginica 0 3 14</strong><br/>&gt; print(BCR)<br/><strong>[1] 0.95</strong></pre>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">R package – datarobot</h1>
                </header>
            
            <article>
                
<p>The name DataRobot refers to three things: a Boston-based software company, the massively parallel modelling engine developed by the DataRobot company, and an open source R package that allows interactive R users to connect to this modelling engine. This vignette provides a brief introduction to the <kbd>datarobot</kbd> R package, highlighting the following key details of its use:</p>
<ul>
<li>Connecting to the DataRobot modeling engine from an interactive R session</li>
<li>Creating a new modeling project in the DataRobot modeling engine</li>
<li>Retrieving the results from a DataRobot modeling project</li>
<li>Generating predictions from any DataRobot model</li>
</ul>
<p>To launch the package, we use the <kbd>library()</kbd> function:</p>
<pre>&gt; library(datarobot)</pre>
<p>After the previous code, there is a good chance a new user will get an error message, something like the following:</p>
<pre>&gt; library(datarobot)<br/><strong>Did not connect to DataRobot on package startup. Use `ConnectToDataRobot`.</strong><br/><strong>To connect by default on startup, you can put a config file at: C:\Users\yany\Documents/.config/datarobot/drconfig.yaml</strong></pre>
<p>This means that we have to register with the company to get a verified token key. The final one will have the following format:</p>
<pre>loc&lt;- "YOUR-ENDPOINT-HERE"<br/>myToken&lt;-"YOUR-API_TOKEN-HERE"<br/>ConnectToDataRobot(endpoint=loc,token=myToken)</pre>
<p>We could also use the <kbd>help()</kbd> function to find users of the package:</p>
<pre>&gt; help(package=datarobot)</pre>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">R package – eclust</h1>
                </header>
            
            <article>
                
<p>This package is the <strong>Environment-Based Clustering for Interpretable Predictive Models in High Dimensional Data</strong>. First, let's look at a dataset called <kbd>simdata</kbd>, which contains simulated data for the package:</p>
<pre>&gt; library(eclust)<br/>&gt; data("simdata")<br/>&gt; dim(simdata)<br/><strong>[1] 100 502</strong><br/>&gt; simdata[1:5, 1:6]<br/><strong>              Y E Gene1 Gene2 Gene3 Gene4</strong><br/><strong>[1,] -94.131497 0 -0.4821629 0.1298527 0.4228393 0.36643188</strong><br/><strong>[2,] 7.134990 0 -1.5216289 -0.3304428 -0.4384459 1.57602830</strong><br/><strong>[3,] 1.974194 0 0.7590055 -0.3600983 1.9006443 -1.47250061</strong><br/><strong>[4,] -44.855010 0 0.6833635 1.8051352 0.1527713 -0.06442029</strong><br/><strong>[5,] 23.547378 0 0.4587626 -0.3996984 -0.5727255 -1.75716775</strong><br/>&gt; table(simdata[,"E"])<br/><strong> 0 1 </strong><br/><strong>50 50</strong> <br/>&gt;</pre>
<p>The preceding output shows that the dimension of the data is <kbd>100</kbd> by <kbd>502</kbd>. <kbd>Y</kbd> is a continuous response vector, and <kbd>E</kbd> is a binary environment variable for the ECLUST method. <em>E = 0</em> for unexposed <em>(n=50)</em> and <em>E = 1</em> for exposed <em>(n=50)</em>. The following R program estimates the Fisher z-transformation of correlations. The definition of Fisher's Z transformation is shown in the following code:</p>
<pre>&gt; library(eclust)<br/>&gt; data("simdata")<br/>&gt; X = simdata[,c(-1,-2)]<br/>&gt; firstCorr&lt;-cor(X[1:50,])<br/>&gt; secondCorr&lt;-cor(X[51:100,])<br/>&gt; score&lt;-u_fisherZ(n0=100,cor0=firstCorr,n1=100,cor1=secondCorr)<br/>&gt; dim(score)<br/><strong>[1] 500 500</strong><br/>&gt; score[1:5,1:5]<br/><strong>          Gene1 Gene2 Gene3 Gene4 Gene5</strong><br/><strong>Gene1 1.000000 -8.062020 6.260050 -8.133437 -7.825391</strong><br/><strong>Gene2 -8.062020 1.000000 9.162208 -7.431822 -7.814067</strong><br/><strong>Gene3 6.260050 9.162208 1.000000 8.072412 6.529433</strong><br/><strong>Gene4 -8.133437 -7.431822 8.072412 1.000000 -5.099261</strong><br/><strong>Gene5 -7.825391 -7.814067 6.529433 -5.099261 1.000000</strong><br/>&gt;</pre>
<p>The Fisher's Z-transformation is defined here. Assuming that we have a set of <em>n</em> pairs of x<sub>i</sub> and y<sub>i</sub>, we could estimate their correlation by applying the following formula:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/620cdfd9-7840-43c2-a907-d09d61ae4861.png" style="width:27.50em;height:3.33em;" width="5450" height="660"/></div>
<p class="mce-root CDPAlignCenter CDPAlign CDPAlignLeft">Here, ρ is the correlation between two variables, while <img class="fm-editor-equation" src="Images/53fe2075-f3dc-4564-b3a1-d5a6ab640992.png" style="width:0.92em;height:1.17em;" width="110" height="140"/> and <img class="fm-editor-equation" src="Images/7a3b0972-d6a4-4152-b5bf-9ef439ea5467.png" style="width:0.83em;height:1.42em;" width="100" height="170"/> are the means of <em>x</em> and <em>y.</em> The Fisher's z value is defined as <em>:</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7026ef21-8c4b-44b1-bb17-37482070aa91.png" style="width:19.92em;height:2.25em;" width="3900" height="440"/></p>
<div class="mce-root CDPAlignCenter CDPAlign CDPAlignLeft"><br/>
In is the natural logarithm function and <kbd>arctanh()</kbd> is the inverse hyperbolic tangent function.</div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Model selection</h1>
                </header>
            
            <article>
                
<p>When finding a good model, sometimes we face under fitting and over fitting. The first example is borrowed; you can download the program at <a href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py"><span class="URLPACKT">http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py</span></a>. It demonstrates the problems of under fitting and over fitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The true function is given here:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/42ddbca0-48d7-4ea0-b0d4-3fba96443781.png" style="width:23.42em;height:1.83em;" width="2810" height="220"/></div>
<p>In the following program, we try to use linear and polynomial models to approximate the equation. The slightly modified code is shown here. The program tries to show the impact of different models in terms of under-fitting and over-fitting:</p>
<pre>import sklearn<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.preprocessing import PolynomialFeatures <br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.model_selection import cross_val_score <br/>#<br/>np.random.seed(123)<br/>n= 30 # number of samples <br/>degrees = [1, 4, 15]<br/>def true_fun(x):<br/>    return np.cos(1.5*np.pi*x)<br/>x = np.sort(np.random.rand(n))<br/>y = true_fun(x) + np.random.randn(n) * 0.1<br/>plt.figure(figsize=(14, 5))<br/>title="Degree {}\nMSE = {:.2e}(+/- {:.2e})"<br/>name1="polynomial_features"<br/>name2="linear_regression"<br/>name3="neg_mean_squared_error"<br/>#<br/>for i in range(len(degrees)):<br/>    ax=plt.subplot(1,len(degrees),i+1)<br/>    plt.setp(ax, xticks=(), yticks=())<br/>    pFeatures=PolynomialFeatures(degree=degrees[i],include_bias=False)<br/>    linear_regression = LinearRegression()<br/>    pipeline=Pipeline([(name1,pFeatures),(name2,linear_regression)])<br/>    pipeline.fit(x[:,np.newaxis],y)<br/>    scores=cross_val_score(pipeline,x[:,np.newaxis],y,scoring=name3,cv=10)<br/>    xTest = np.linspace(0, 1, 100)<br/>    plt.plot(xTest,pipeline.predict(xTest[:,np.newaxis]),label="Model")<br/>    plt.plot(xTest,true_fun(xTest),label="True function")<br/>    plt.scatter(x,y,edgecolor='b',s=20,label="Samples")<br/>    plt.xlabel("x")<br/>    plt.ylabel("y")<br/>    plt.xlim((0,1))<br/>    plt.ylim((-2,2))<br/>    plt.legend(loc="best")<br/>    plt.title(title.format(degrees[i],-scores.mean(),scores.std()))<br/>plt.show()</pre>
<p>The related graphs are shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/05828301-7b09-4455-ad01-0185668c1de2.png" style="width:56.33em;height:22.83em;" width="814" height="330"/></p>
<p>Note that, on the top of each graph, <strong>MSE</strong> stands for the <strong>Mean Squared Error</strong>. For the left-hand graph, the program tries to use a line to fit the true model based on the input dataset. Since it is linear, the <strong>Degree</strong> is <strong>1</strong>. Compared with the second model with a <strong>Degree</strong> of <strong>4</strong>, the <strong>MSE</strong> of this learned model is larger, 0.54 versus 0.0879. This indicates that the linear model might under-fit the model while the second model might over-fit the model.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Python package – model-catwalk</h1>
                </header>
            
            <article>
                
<p>One example can be found at <a href="https://pypi.python.org/pypi/model-catwalk/0.2.1">https://pypi.python.org/pypi/model-catwalk/0.2.1</a> . Its first several lines of code are shown here:</p>
<pre>import datetime<br/>import pandas<br/>from sqlalchemy import create_engine<br/>from metta import metta_io as metta<br/>from catwalk.storage import FSModelStorageEngine, CSVMatrixStore<br/>from catwalk.model_trainers import ModelTrainer<br/>from catwalk.predictors import Predictor<br/>from catwalk.evaluation import ModelEvaluator<br/>from catwalk.utils import save_experiment_and_get_hash<br/>help(FSModelStorageEngine)</pre>
<p>The related output is shown here. To save space, only the top part is presented:</p>
<pre> Help on class FSModelStorageEngine in module catwalk.storage:<br/><br/>class FSModelStorageEngine(ModelStorageEngine)<br/> | Method resolution order:<br/> | FSModelStorageEngine<br/> | ModelStorageEngine<br/> | builtins.object<br/> | <br/> | Methods defined here:<br/> | <br/> | __init__(self, *args, **kwargs)<br/> | Initialize self. See help(type(self)) for accurate signature.<br/> | <br/> | get_store(self, model_hash)<br/> | <br/> | ----------------------------------------------------------------------</pre>
<pre> | Data descriptors inherited from ModelStorageEngine:<br/> | <br/> | __dict__<br/> | dictionary for instance variables (if defined)<br/> | <br/> | __weakref__<br/> | list of weak references to the object (if defined)</pre>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Python package – sklearn</h1>
                </header>
            
            <article>
                
<p>Since Python <kbd>sklearn</kbd> is a very useful package, it is worthwhile to show you more examples of using this package. The example cited here is how to use the package to classify documents by topics using a bag-of-words approach.</p>
<p>This example uses a <kbd>scipy.sparse</kbd> matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices. The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached. The ZIP file contains the input files and can be downloaded at <a href="http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz">http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz</a>. It has a size of about 14 MB. The code is available at the following web link: <a href="http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="URLPACKT">http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py</span></a>. To save space, only the first several lines are shown here:</p>
<pre>import logging<br/>import numpy as np<br/>from optparse import OptionParser<br/>import sys<br/>from time import time<br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets import fetch_20newsgroups<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.feature_extraction.text import HashingVectorizer<br/>from sklearn.feature_selection import SelectFromModel</pre>
<p>The related output is shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/ec6151e8-5131-4094-9450-3d875bb7860e.jpg" style="width:49.58em;height:37.92em;" width="739" height="565"/></p>
<p>For each method, there are three measures: <strong>score</strong>, <strong>training time</strong>, and <strong>testing time</strong>. For example, for the <kbd>RandomForestClassier</kbd> method, it uses lots of time for training and testing; see the longest three bars. It is understandable since this method uses lots of simulations.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Julia package – QuantEcon</h1>
                </header>
            
            <article>
                
<p>For predicting a future possible event, one of the tools is the Monte Carlo simulation. For this purpose, we could use a Julia package called <kbd>QuantEcon</kbd>. This package is <strong>Quantitative Economics with Julia</strong>. The first example is the Markov simulation:</p>
<pre>using QuantEcon<br/>P = [0.4 0.6; 0.2 0.8];<br/>mc = MarkovChain(P)<br/>x = simulate(mc, 100000);<br/>mean(x .== 1)<br/>#<br/>mc2 = MarkovChain(P, ["employed", "unemployed"])<br/>simulate(mc2, 4)</pre>
<p>The first part of the program simulates 100,000 times of P matrix while the second part simulates two statuses: <kbd>employed</kbd> and <kbd>unemployed</kbd>. See the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/3fda3b1a-27f1-4191-b659-126d13c59ba9.png" style="width:31.58em;height:14.17em;" width="453" height="203"/></div>
<p>The next example is also borrowed from the manual. The objective is to see how a person from one economic status transforms to another one in the future. First, let's see the following graph:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/152cb150-81fe-41ec-a543-8809645ad64f.png" style="width:24.33em;height:7.58em;" width="559" height="174"/></div>
<p>Let's look at the leftmost oval with <strong>poor</strong> inside. It means for a <strong>poor</strong> person he/she has 90% chance to remain <strong>poor</strong> and 10% moves to the <strong>middle class</strong>. It could be represented by the following matrix, putting zeros where there's no edge between nodes:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/747ea972-9704-472b-92c1-c2255dfd3f64.png" style="width:26.67em;height:6.08em;" width="3200" height="730"/></div>
<p class="mce-root"> Two states, <em>x</em> and <em>y</em>, are said to communicate with each other if there exists positive integers <em>j</em> and <em>k</em>, such as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f2000473-f69d-4a6f-a007-a40c7398b014.png" style="width:44.83em;height:3.17em;" width="7480" height="530"/></div>
<p class="mce-root">The stochastic matrix P is called irreducible if all states communicate; that is, if <em>x</em> and <em>y</em> communicate for every (<em>x, y</em>). It's clear from the graph that this stochastic matrix is irreducible: we can reach any state from any other state eventually. The following code would confirm this:</p>
<pre class="mce-root">using QuantEcon<br/>P = [0.9 0.1 0.0; 0.4 0.4 0.2; 0.1 0.1 0.8];<br/>mc = MarkovChain(P)<br/>is_irreducible(mc) </pre>
<p class="mce-root">The following graph would represent an extreme case for irreducible since the future status for a <strong>poor</strong> person will be 100% <strong>poor</strong>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/a26a6f7b-a22d-43d4-bfc7-3f9b19a746ac.png" style="width:14.67em;height:10.50em;" width="372" height="265"/></div>
<p>The following code would confirm this as well, as the result will be <kbd>false</kbd>:</p>
<pre>using QuantEcon<br/>P2 = [1.0 0.0 0.0; 0.1 0.8 0.1; 0.0 0.2 0.8];<br/>mc2 = MarkovChain(P2)<br/>is_irreducible(mc2)</pre>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Octave package – ltfat</h1>
                </header>
            
            <article>
                
<p>The <kbd>ltfat</kbd> package stands for <strong>Large Time/Frequency Analysis Toolbox</strong> and is a Matlab/Octave toolbox for working with time frequency analysis, wavelets, and signal processing. It is intended both as an educational and a computational tool. The toolbox provides a large number of linear transforms including Gabor and wavelet transforms, along with routines for constructing windows (filter prototypes) and manipulating coefficients. The following example is borrowed from their manual, which is available at <a href="http://ltfat.github.io/doc/ltfat.pdf"><span class="URLPACKT">http://ltfat.github.io/doc/ltfat.pdf</span></a>. The following example shows how the function <kbd>franalasso()</kbd> produces a sparse representation of a test signal <kbd>greasy</kbd>:</p>
<pre>pkg load ltfat<br/>f = greasy;<br/>name1="sparsified coefficients"<br/>name2="dual system coefficients"<br/>F = frame('dgtreal','gauss',64,512);<br/>lambda = 0.1;<br/>% Solve the basis pursuit problem<br/>[c,~,~,frec,cd] = franalasso(F,f,lambda);<br/>figure(1); % Plot sparse coefficients<br/>plotframe(F,c,’dynrange’,50);<br/>figure(2); % Plot coefficients <br/>plotframe(F,cd,’dynrange’,50);<br/>norm(f-frec)<br/>figure(3);<br/>semilogx([sort(abs(c),'descend')/max(abs(c)),...<br/>sort(abs(cd),’descend’)/max(abs(cd))]);<br/>legend({name1,name2});</pre>
<p>The program outputs three graphs, and the last one is shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/7d94b9e0-425d-4b52-9082-53b83bc25ce0.png" style="width:26.17em;height:23.58em;" width="409" height="369"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Granger causality test</h1>
                </header>
            
            <article>
                
<p>When saying that A causes B, this means that A is the reason that B happens. This is the common definition of causality: which one causes the next one. The Granger causality test is used to determine whether one time series is a factor and offers useful information in forecasting the second one. In the following code, a dataset called <kbd>ChickEgg</kbd> is used as an illustration. The dataset has two columns, number of chicks and number of eggs, with a timestamp:</p>
<pre>&gt; library(lmtest)<br/>&gt; data(ChickEgg)<br/>&gt; dim(ChickEgg)<br/><strong>[1] 54 2</strong><br/>&gt; ChickEgg[1:5,]<br/><strong>     chicken egg</strong><br/><strong>[1,] 468491 3581</strong><br/><strong>[2,] 449743 3532</strong><br/><strong>[3,] 436815 3327</strong><br/><strong>[4,] 444523 3255</strong><br/><strong>[5,] 433937 3156</strong></pre>
<p>The question is: could we use this year's egg numbers to predict the next year's chicken numbers? If this is true, our statement will be <em>the number of chicks Granger causes the number of eggs</em>. If this is not true, we say <em>the number of chicks does not Granger cause the number of eggs</em>. Here is the related code:</p>
<pre>&gt; library(lmtest)<br/>&gt; data(ChickEgg)<br/>&gt; grangertest(chicken~egg, order = 3, data = ChickEgg)<br/><strong>Granger causality test</strong><br/><strong>Model 1: chicken ~ Lags(chicken, 1:3) + Lags(egg, 1:3)</strong><br/><strong>Model 2: chicken ~ Lags(chicken, 1:3)</strong><br/><strong>  Res.Df Df F Pr(&gt;F) </strong><br/><strong>1 44 </strong><br/><strong>2 47 -3 5.405 0.002966 **</strong><br/><strong>---</strong><br/><strong>Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</strong></pre>
<p>In model 1, we try to use the lags of chicken plus the lags of the lags of egg to explain the chicken numbers. Since the P-value is quite small, it is significant at <kbd>0.01</kbd>. Because of this, we say that <em>egg Granger causes chicken</em>. The following test shows that chicken could not be used to forecast the next period's error:</p>
<pre>&gt; grangertest(egg~chicken, order = 3, data = ChickEgg)<br/><strong>Granger causality test</strong><br/><br/><strong>Model 1: egg ~ Lags(egg, 1:3) + Lags(chicken, 1:3)</strong><br/><strong>Model 2: egg ~ Lags(egg, 1:3)</strong><br/><strong>  Res.Df Df F Pr(&gt;F)</strong><br/><strong>1 44 </strong><br/><strong>2 47 -3 0.5916 0.6238</strong></pre>
<p>In the next example, we test which one (IBM's return or S&amp;P500 return) could Granger cause the other. First, we define a return function:</p>
<pre>ret_f&lt;-function(x,ticker=""){<br/>     n&lt;-nrow(x)<br/>     p&lt;-x[,6]<br/>     ret&lt;-p[2:n]/p[1:(n-1)]-1<br/>     output&lt;-data.frame(x[2:n,1],ret)<br/>     name&lt;-paste("RET_",toupper(ticker),sep='')<br/>     colnames(output)&lt;-c("DATE",name)<br/>     return(output)<br/>}<br/><br/></pre>
<p>Note that we use the return plus the ticker as the column name:</p>
<pre>&gt; x&lt;-read.csv("http://canisius.edu/~yany/data/ibmDaily.csv",header=T)<br/>&gt; ibmRet&lt;-ret_f(x,"ibm")<br/>&gt; x&lt;-read.csv("http://canisius.edu/~yany/data/^gspcDaily.csv",header=T)<br/>&gt; mktRet&lt;-ret_f(x,"mkt")<br/>&gt; final&lt;-merge(ibmRet,mktRet)<br/>&gt; head(final)<br/><strong>        DATE RET_IBM RET_MKT</strong><br/><strong>1 1962-01-03 0.008742545 0.0023956877</strong><br/><strong>2 1962-01-04 -0.009965497 -0.0068887673</strong><br/><strong>3 1962-01-05 -0.019694350 -0.0138730891</strong><br/><strong>4 1962-01-08 -0.018750380 -0.0077519519</strong><br/><strong>5 1962-01-09 0.011829467 0.0004340133</strong><br/><strong>6 1962-01-10 0.001798526 -0.0027476933</strong></pre>
<p>Now, the function could be called with the input values. The objective of the program is to test if we could use the lagged market returns to explain IBM's returns. Similarly, we test whether the lagged IBM's returns explain the market's returns:</p>
<pre>&gt; library(lmtest)<br/>&gt; grangertest(RET_IBM ~ RET_MKT, order = 1, data =final)<br/><strong>Granger causality test</strong><br/><strong>Model 1: RET_IBM ~ Lags(RET_IBM, 1:1) + Lags(RET_MKT, 1:1)</strong><br/><strong>Model 2: RET_IBM ~ Lags(RET_IBM, 1:1)</strong><br/><strong>  Res.Df Df F Pr(&gt;F) </strong><br/><strong>1 14149 </strong><br/><strong>2 14150 -1 24.002 9.729e-07 ***</strong><br/><strong>---</strong><br/><strong>Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</strong></pre>
<p>The preceding results suggest that the S&amp;P500 index could be used to explain IBM's next period's return, since it is statistically significant at 0.1%. The next line will test if the IBM lags of returns would explain S&amp;P500 index returns:</p>
<pre>&gt; grangertest(RET_MKT ~ RET_IBM, order = 1, data =final)<br/><strong>Granger causality test</strong><br/><strong>Model 1: RET_MKT ~ Lags(RET_MKT, 1:1) + Lags(RET_IBM, 1:1)</strong><br/><strong>Model 2: RET_MKT ~ Lags(RET_MKT, 1:1)</strong><br/><strong>  Res.Df Df F Pr(&gt;F) </strong><br/><strong>1 14149 </strong><br/><strong>2 14150 -1 7.5378 0.006049 **</strong><br/><strong>---</strong><br/><strong>Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</strong><br/>&gt; </pre>
<p>The result suggests that, over this period, IMB's returns could be used to explain the S&amp;P500 index in the next period's returns as well.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p style="font-weight: 400">In this chapter, we have discussed predictive data analytics, modeling and validation, some useful datasets, time series analytics, how to predict future events, seasonality, and how to visualize our data. For Python packages, we have mentioned <kbd>prsklearn</kbd> and <kbd>catwalk</kbd>. For R packages, we have discussed <kbd>datarobot</kbd>, <kbd>LiblineaR</kbd>, <kbd>andeclust</kbd>. For Julia packages, we explained <kbd>EQuantEcon</kbd>. For Octave, we have explained <kbd>ltfat</kbd>.</p>
<p style="font-weight: 400">In the next chapter, we will discuss Anaconda Cloud. Some topics include the Jupyter Notebook in depth, different formats of the Jupyter Notebooks, how to share notebooks with your partners, how to share different projects over different platforms, how to share your working environments, and how to replicate others' environments locally.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Review questions and exercises</h1>
                </header>
            
            <article>
                
<ol>
<li>Why do we care about predicting the future?</li>
<li>What does <em>seasonality</em> mean? How could it impact our predictions?</li>
<li>How does one measure the impact of seasonality?</li>
<li>Write an R program to use the moving average of the last five years to predict the next year's expected return. The source of the data is <a href="http://fiannce.yahoo.com">http://fiannce.yahoo.com</a>. You can test a few stocks such as IBM, C, and WMT. In addition, apply the same method to the S&amp;P500 index. What is your conclusion?</li>
<li>Assume that we have the following true model:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/86816f62-37f9-4548-85e8-7676d31831d3.png" style="width:10.58em;height:1.25em;" width="1860" height="220"/></div>
<p style="padding-left: 60px">Write a Python program to use linear and polynomial models to approximate the previous function and show the related graphs.</p>
<ol start="6">
<li>Download a market index monthly data and estimate its next year's annual return. The S&amp;P500 could be used as the index and Yahoo!Finance at <a href="http://finance.yahoo.com">finance.yahoo.com</a> could be used as the source of data. Source of data: <a href="https://finance.yahoo.com">https://finance.yahoo.com</a></li>
</ol>
<ol start="7">
<li>Download several countries' index data, such as the US, Canada, Hong Kong, and China. Estimate the returns. Conduct the Granger causality test to see which country's stock market index is a dominant force.</li>
<li>From Professor French's Data Library, download the Fama-French three-factor time series. Estimate the next year's <strong>Small Minus Big</strong> (<strong>SMB</strong>) factor and <strong>High Minus Low</strong> (<strong>HML</strong>) factor. The web page is <a href="http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html">http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html</a>. Alternatively, a Python dataset can be downloaded at <a href="http://canisius.edu/~yany/python/data/ffMonthly.pkl">http://canisius.edu/~yany/python/data/ffMonthly.pkl</a>. A few lines of Python code are shown here:</li>
</ol>
<pre style="padding-left: 90px">import pandas as pd<br/>ff=pd.read_pickle("c:/temp/ffMonthly.pkl")<br/>print(ff.head())</pre>
<p style="padding-left: 60px">The related output is shown here:</p>
<pre style="padding-left: 90px">print(ff.head())<br/>      <strong>      MKT_RF SMB HML Rf</strong><br/><strong>1926-07-01 0.0296 -0.0230 -0.0287 0.0022</strong><br/><strong>1926-08-01 0.0264 -0.0140 0.0419 0.0025</strong><br/><strong>1926-09-01 0.0036 -0.0132 0.0001 0.0023</strong><br/><strong>1926-10-01 -0.0324 0.0004 0.0051 0.0032</strong><br/><strong>1926-11-01 0.0253 -0.0020 -0.0035 0.003</strong></pre>
<ol start="9">
<li>What is IBM's next expected annual return?</li>
<li>What is the next three years' S&amp;P500 index level?</li>
<li>Could we add the business cycle to our regression models? The dataset can be downloaded by using the following R code:</li>
</ol>
<pre style="padding-left: 90px">&gt; path&lt;-"http://canisius.edu/~yany/RData/"<br/>&gt; dataSet&lt;-"businesscycle"<br/>&gt; link&lt;-paste(path,dataSet,".RData",sep='')<br/>&gt; load(url(link))<br/>&gt; head(.businessCycle)<br/>      DATE CYCLE<br/><strong>1 19261001 1.000</strong><br/><strong>2 19261101 0.846</strong><br/><strong>3 19261201 0.692</strong><br/><strong>4 19270101 0.538</strong><br/><strong>5 19270201 0.385</strong><br/><strong>6 19270301 0.231</strong></pre>
<p style="padding-left: 60px">A peak is defined as 1 while a trough is defined as -1. Any months between a peak or trough or between a trough and peak are linearly interpolated.</p>
<p style="padding-left: 60px">Note that the following four questions are based on Octave machine learning examples. The dataset and Octave sample programs can be downloaded at <a href="https://github.com/partharamanujam/octave-ml">https://github.com/partharamanujam/octave-ml.</a></p>
<ol start="12">
<li>Run the sample Octave machine learning program called <kbd>linear_gd.m</kbd>, the dataset containing historical records on the change in the water level.</li>
<li>Run the sample Octave machine learning program called <kbd>svm.m</kbd>. The program reads <kbd>spam-vocab</kbd> list into a struct <kbd>words</kbd> which occur at least 100 times in the spam corpus. Three input files are <kbd>spam-vocab.txt</kbd>, <kbd>spamTrain.mat</kbd>, and <kbd>spamTest.mat</kbd>.</li>
<li>Run the sample Octave machine learning program called <kbd>logistic_gd.m</kbd>.</li>
<li>Run the sample Octave machine learning program called <kbd>pca.m</kbd>.</li>
<li>Install a Julia package called <kbd>EmpiricalRisks</kbd> and show a few examples by using this package.</li>
</ol>


            </article>

            
        </section>
    </div></body></html>