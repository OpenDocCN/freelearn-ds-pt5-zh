<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Supervised Learning in Anaconda</h1>
                </header>
            
            <article>
                
<p>Since most of us understand the format of the function <em>y=f(x)</em>, it is a good idea to use it to explain supervised learning. When having both <em>y</em> and <em>x</em>, we could run various regressions to identify the correct function forms. This is the spirit of supervised learning. For supervised learning, we have two datasets: the <strong>training data</strong> and <strong>test data</strong>. Usually, the training set has a set of input variables, such as <em>x</em>, and a related output value such as <em>y</em> (that is, the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function form. Then, we apply this inferred function to map our test dataset.</p>
<p>In this chapter, the following topics will be covered:</p>
<ul>
<li>A glance at supervised learning</li>
<li>Classification</li>
<li>Implementation of supervised learning via R, Python, Julia, and Octave</li>
<li>Task view for machine learning in R</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">A glance at supervised learning</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we discussed unsupervised learning where we have input data only. In terms of the function <em>y=f(x)</em>, for unsupervised learning we have only inputs <em>x</em>. Unlike unsupervised learning, we have both inputs <em>x</em> and the corresponding output <em>y</em> for supervised learning. Our task is to find the best function, linking <em>x</em> with <em>y</em>, based on our training dataset. In supervised learning, our training dataset consists of an input object, typically a vector, and a desired output value, where it could be either binary, categorical, discrete, or continuous. A supervised learning algorithm examines a given training dataset and produces an inferred best-fit function. To verify the accuracy of this inferred function, we use the second dataset, the test set.</p>
<p>In an ideal world, we would want to have a large sample size. However, for many occasions, this is not true. In those cases, we could apply the bootstrap method for estimating statistical quantities from samples. The <strong>Bootstrap Aggregation</strong> algorithm is used to create multiple different models from a single training dataset. Actually, random forest is a sort of nickname for <em>decision tree ensemble through bagging</em>. Because of this, it ends up with a very powerful classifier.</p>
<p>Let's use the famous Titanic tragedy as an example. In 1912, on its maiden voyage, the Titanic sank after striking an iceberg. Of its passengers and crew, more than 1,500 died. First, let's look at the simple dataset. The following R code can be downloaded from the author's website at <a href="http://canisius.edu/~yany/RData/titanic.RData">http://canisius.edu/~yany/RData/titanic.RData</a>:</p>
<pre>&gt; path&lt;-"http://canisius.edu/~yany/RData/" 
&gt; dataSet&lt;-"titanic" 
&gt; link&lt;-paste(path,dataSet,".RData",sep='') 
&gt; con&lt;-url(link) 
&gt; load(con)  
&gt; dim(.titanic) 
[1] 2201    4 
&gt; head(.titanic) 
<strong>  CLASS   AGE GENDER SURVIVED 
1 First Adult   Male      Yes 
2 First Adult   Male      Yes 
3 First Adult   Male      Yes 
4 First Adult   Male      Yes 
5 First Adult   Male      Yes 
6 First Adult   Male      Yes</strong> 
&gt; summary(.titanic) 
<strong>    CLASS        AGE          GENDER     SURVIVED   
 Crew  :885   Adult:2092   Female: 470   No :1490   
 First :325   Child: 109   Male  :1731   Yes: 711   
 Second:285                                         
 Third :706</strong>   </pre>
<p>From the preceding output, it can be seen that the dataset has <kbd>2,201</kbd> observations with just <kbd>4</kbd> variables. The <kbd>CLASS</kbd> is for cabin or economic status, <kbd>GENDER</kbd> is for gender, and <kbd>SURVIVED</kbd> indicates whether the passenger survived or not. The <kbd>unique()</kbd> function can be used to show their unique values, shown here:</p>
<pre>&gt; unique(.titanic$CLASS) 
<strong>[1] First  Second Third  Crew   
Levels: Crew First Second Third</strong></pre>
<pre>&gt; unique(.titanic$GENDER) 
<strong>[1] Male   Female 
Levels: Female Male</strong> 
&gt; unique(.titanic$SURVIVED) 
<strong>[1] Yes No  
Levels: No Yes</strong></pre>
<p>Our task is to use the decision tree to find the contributions of those three input variables to the survival rate. Let's use the R package called <kbd>Rattle</kbd> to run a simple decision tree model by using the embedded dataset. To launch the <kbd>Rattle</kbd> package, we have the following two lines:</p>
<pre>&gt;library(rattle) 
&gt;rattle() </pre>
<p>The next screenshot shows how to choose a library and a related dataset. Note that, depending on the version, the dataset might not be available. An alternative way is to download from the author's web page at <a href="http://canisius.edu/~yany/RData/titanic.RData">http://canisius.edu/~yany/RData/titanic.RData</a>. The default setting for the partition is 70% (training), 15% (verification), and 15% (testing):</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-841 image-border" src="Images/b1deab5c-fea1-4335-9897-2a648e7b756d.png" style="width:37.75em;height:19.75em;" width="592" height="310"/></div>
<p>If we choose <span class="packt_screen">Model </span>| <span class="packt_screen">Tree</span>, and then <span class="packt_screen">Execute</span> in the upper left corner, we will get the following result:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-842 image-border" src="Images/7dcd68d4-ee7d-4ae0-8f2a-f9767065bfc6.png" style="width:17.58em;height:20.17em;" width="442" height="509"/></div>
<p> The top ratio of <strong>68%</strong> is for the rate of those who did not survive while the <strong>32%</strong> is for the survival rate. The first question is whether the passenger is a male or not. If the answer is a yes, then the person would have a <strong>21%</strong> chance of survival, shown in the lower-left leaf. For a female passenger who booked a cabin of <strong>Class 3</strong>, she would have a <strong>44%</strong> chance of survival. For female passengers, when they occupy a first- or second-class cabin, she would have a <strong>91%</strong> chance of survival. Since the age does not appear in our final result, the model considers it useless.</p>
<p>Note that if you plan to use the loaded dataset called <kbd>.titanic</kbd>, you could use <kbd>x&lt;-.titanic</kbd>, and then choose it by selecting <span class="packt_screen">R Dataset</span> after Rattle is launched. For different associated datasets with more variables, it is not a surprise that users could reach different conclusions. You can save the log, that is, the code, for a later usage. The simplified code is given here. The log is on the Rattle menu bar:</p>
<pre>library(rpart, quietly=TRUE) 
con&lt;-url("http://canisius.edu/~yany/RData/titanic.RData") 
load(con) 
x&lt;-.titanic 
scoring  &lt;- FALSE 
set.seed(42) 
risk&lt;-ident&lt;-ignore&lt;-weights&lt;-numeric&lt;-NULL 
str(dataset) 
n&lt;- nrow(dataset) 
train  &lt;- sample &lt;- sample(n,0.7*n) 
validate&lt;- sample(setdiff(seq_len(n),train),0.15*n) 
test&lt;- setdiff(setdiff(seq_len(n), train), validate) 
inputVars&lt;-categoric&lt;-c("CLASS","AGE","GENDER") 
target&lt;-"SURVIVED"
output&lt;-rpart(SURVIVED~.,data=x[train, c(inputVars, target)], 
   method="class",parms=list(split="information"),control= 
   rpart.control(usesurrogate=0,maxsurrogate=0)) 
fancyRpartPlot(output, main="Decision Tree for Titanic") </pre>
<p>The second example is associated with the dataset called <kbd>iris</kbd>. This time, the language used is Python. First, let's look at the data itself. The code and related output is shown here:</p>
<pre>import sklearn as sk 
from sklearn import datasets 
iris = datasets.load_iris() 
print("data:n",iris.data[0:4,]) 
print("target",iris.target[0:2,]) 
mylist=list(iris.target) 
used = [] 
[x for x in mylist if x not in used and used.append(x)] 
print("unique values for targetsn",used) </pre>
<p>The output would look like follows:</p>
<pre><strong>data: 
 [[ 5.1  3.5  1.4  0.2] 
 [ 4.9  3.   1.4  0.2] 
 [ 4.7  3.2  1.3  0.2] 
 [ 4.6  3.1  1.5  0.2]] 
target [0 0] 
unique values for targets 
 [0, 1, 2]</strong> </pre>
<p>The dataset has five variables: sepal length, sepal width, petal length, petal width (all in cm), and class. The last one has three unique values for Setosa, Versicolour, or Virginica; see <a href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a> for more information. The first several lines of the original dataset are shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-843 image-border" src="Images/a76421af-aa95-4003-8ab6-b768c25ef469.png" style="width:14.00em;height:8.42em;" width="206" height="124"/></div>
<p>The dataset can be downloaded from the UCI Machine Learning Repository: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a>. First, let's cut the dataset into a training and testing set. Then we use supervised learning to figure out the mapping function, that is, an inferred function:</p>
<pre>import numpy as np<br/>from sklearn import datasets<br/>from sklearn.neighbors import KNeighborsClassifier as KNC<br/>iris = datasets.load_iris()<br/>x= iris.data<br/>y= iris.target<br/>np.unique(y)<br/>np.random.seed(123)<br/>indices = np.random.permutation(len(x))<br/>iris_x_train = x[indices[:-10]]<br/>iris_y_train = y[indices[:-10]]<br/>iris_x_test = x[indices[-10:]]<br/>iris_y_test = y[indices[-10:]]<br/>knn = KNC()<br/>knn.fit(iris_x_train, iris_y_train)<br/>KNC(algorithm='auto',leaf_size=30, metric='minkowski',<br/>metric_params=None,n_jobs=1,n_neighbors=5, p=2,weights='uniform')<br/>knn.predict(iris_x_test)<br/>out=knn.predict(iris_x_test)<br/>print("predicted:",out)<br/>print("True :",iris_y_test)</pre>
<p>The output is shown here:</p>
<pre><strong>predicted: [1 1 2 1 2 0 1 1 2 2] 
True     : [1 1 2 2 1 0 1 1 2 2]</strong> </pre>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classification</h1>
                </header>
            
            <article>
                
<p>Supervised learning problems can be further divided into two groups: <strong>classification</strong> and <strong>regression</strong>. For the classification problem, the output variable, such as <em>y</em>, could be a binary variable, that is, 0 or 1, or several categories. For a regression, variables or values could be discrete or continuous. In the Titanic example, we have 1 for survived and 0 for not survived. For a regression problem, the output could be a value, such as, 2.5 or 0.234. In the previous chapter, we discussed the concept of distance between group members within the same group and between groups.</p>
<p>The logic for classification is that the distance between (among) group members is shorter than the distance between different groups. Alternatively speaking, the similarity between (among) group members is higher than the similarity between (among) different groups or categories. Since categorical data cannot be ranked, we could use the following method to group them: </p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/2adc625e-b24f-4b4c-a061-01ed4fb3e47d.png" style="width:24.00em;height:2.58em;" width="3720" height="400"/></div>
<p><span>Here,</span> <em>n<sub>matched</sub></em> <span>is the number of matched treatments and</span> <em>n<sub>total</sub></em> <span>is the number of total treatments. When both categorical data and numeric data are available, we could estimate both types of distances first and then choose appropriate weights to combine them.</span></p>
<p>Assume <em>d<sub>num</sub></em> is for the distance based on the numerical data and <em>d<sub>cat</sub></em> is for the distance based on the categorical data. Then, we have the following combined distance:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7ae7f744-791d-44da-b3e0-b20c7835aa9f.png" style="width:27.42em;height:1.42em;" width="4260" height="220"/> </div>
<p>Here, <em>w<sub>num</sub></em> is the weight for the numerical value.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The k-nearest neighbors algorithm</h1>
                </header>
            
            <article>
                
<p>In pattern recognition or grouping, the <strong>k-nearest neighbors</strong> <span>(<strong>k-NN</strong>) </span>algorithm is a non-parametric method implemented for classification and regression. For those two cases, the input consists of the k-closest training examples in the feature space. The following four lines of R code tries to separate plants into k-groups by using a dataset called <kbd>iris</kbd>:</p>
<pre>library(ggvis) 
x&lt;-ggvis 
y&lt;-layer_points 
iris %&gt;% x(~Petal.Length,~Petal.Width,fill=~Species) %&gt;% y() </pre>
<p>The graph is shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-844 image-border" src="Images/d75fcb5b-c007-4b03-85cd-6df7336f2c57.png" style="width:36.75em;height:24.42em;" width="599" height="397"/></div>
<p>The following diagram shows the five nearest neighbors:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-845 image-border" src="Images/7650430a-6ca4-4ab8-90d7-009957e9c655.png" style="width:21.92em;height:22.33em;" width="441" height="449"/></div>
<p>The code generated by the five nearest neighbors is given here. The code is slightly modified from the code offered by others at <a href="https://stats.stackexchange.com/questions/21572/how-to-plot-decision-boundary-of-a-k-nearest-neighbor-classifier-from-elements-o/21602#21602">https://stats.stackexchange.com/questions/21572/how-to-plot-decision-boundary-of-a-k-nearest-neighbor-classifier-from-elements-o/21602#21602</a>:</p>
<pre>library(ElemStatLearn) 
require(class) 
x &lt;- mixture.example$x 
y &lt;- mixture.example$y 
xnew &lt;- mixture.example$xnew 
px1 &lt;- mixture.example$px1 
px2 &lt;- mixture.example$px2 
# 
color1&lt;-"blue" 
color2&lt;-"pink3" 
kNearest&lt;-5   
model&lt;- knn(x, xnew,y,k=kNearest,prob=TRUE) 
title&lt;-paste(kNearest,"-nearest neighbour") 
prob &lt;- attr(model,"prob") 
prob &lt;- ifelse(model=="1",prob,1-prob) 
prob15 &lt;- matrix(prob,length(px1),length(px2)) 
par(mar=rep(2,4)) 
contour(px1,px2,prob15,levels=0.5,main=title,axes=FALSE) 
points(x, col=ifelse(g==1,color1,color2)) 
gd &lt;- expand.grid(x=px1, y=px2) 
points(gd,pch=".",cex=1.5,col=ifelse(prob15&gt;0.5,color1,color2)) 
box() </pre>
<p>For more details about each function, readers can find the related manual regarding the <kbd>ElemStatLearn</kbd> package.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Bayes classifiers</h1>
                </header>
            
            <article>
                
<p>This is a classification technique based on Bayes theorem with an assumption of independence among predictors. In layman's terms, a naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. First, let's look at the <kbd>HouseVotes84</kbd> dataset:</p>
<pre>&gt; library(mlbench)<br/>&gt; data(HouseVotes84)<br/>&gt; head(HouseVotes84)</pre>
<p>The output is shown in the following screenshot:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-846 image-border" src="Images/221910c7-5292-48d0-88f0-3cc7cd0d0102.png" style="width:39.00em;height:9.08em;" width="620" height="144"/></p>
<p>The first variable, <kbd>Class</kbd>, is a binary one: <kbd>republican</kbd> or <kbd>democrat</kbd>. In addition, we have 16 traits associated with each individual. The following program calls the <kbd>naiveBayes()</kbd> function which computes the conditional a-posterior probabilities of a categorical class variable given independent predictor variables using the Bayes rule:</p>
<pre>library(e1071) 
data(HouseVotes84, package = "mlbench") 
model&lt;-naiveBayes(Class ~ ., data = HouseVotes84) 
# 
predict(model, HouseVotes84[1:10,]) 
predict(model, HouseVotes84[1:10,], type = "raw") 
pred &lt;- predict(model, HouseVotes84) 
table(pred, HouseVotes84$Class) </pre>
<p>The final output is given here:</p>
<pre><strong>pred         democrat   republican </strong><br/><strong>democrat        238       13 
republican      29        155</strong> </pre>
<p>The next example is to predict the survival rate by applying the naive Bayes methodology to the Titanic dataset:</p>
<pre>&gt; library(e1071) 
&gt; data(Titanic) 
&gt; m &lt;- naiveBayes(Survived ~ ., data = Titanic) 
&gt; output&lt;-predict(m, as.data.frame(Titanic)) </pre>
<p>The output is shown here:</p>
<pre>&gt; print(m) 
<strong>Naive Bayes Classifier for Discrete Predictors 
Call: 
naiveBayes.formula(formula = Survived ~ ., data = Titanic) 
A-priori probabilities: 
Survived 
      No      Yes  
0.676965 0.323035  
Conditional probabilities: 
        Class 
Survived        1st        2nd        3rd       Crew 
     No  0.08187919 0.11208054 0.35436242 0.45167785 
     Yes 0.28551336 0.16596343 0.25035162 0.29817159 
        Sex 
Survived       Male     Female 
     No  0.91543624 0.08456376 
     Yes 0.51617440 0.48382560 
        Age 
Survived      Child      Adult 
     No  0.03489933 0.96510067 
     Yes 0.08016878 0.91983122</strong> 
&gt; print(output) 
<strong>[1] Yes No  No  No  Yes Yes Yes Yes No  No  No  No  Yes Yes 
[15] Yes Yes Yes No  No  No  Yes Yes Yes Yes No  No  No  No  
[29] Yes Yes Yes Yes 
Levels: No Yes</strong> </pre>
<p>From the previous output, we could find the final prediction in terms of 32 individuals.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is a wonderful research area of machine learning. It has a root from behavioral psychology. The mechanism would maximize some notion of cumulative reward when certain actions were taken in a set of environments (that is, an agent tries to learn optimal behavior through trial-and-error interactions within a dynamic environment setting).</p>
<p>Let's use an R package called <kbd>ReinforcementLearning</kbd>. First, let's look at the dataset, shown here:</p>
<pre>&gt; library("ReinforcementLearning") 
&gt; set.seed(123) 
&gt; data &lt;- sampleGridSequence(1000) 
&gt; dim(data) 
<strong>[1] 1000    4</strong> 
&gt; head(data) 
 <strong> State Action Reward NextState 
1    s2   left     -1        s2 
2    s4  right     -1        s4 
3    s2   down     -1        s2 
4    s4     up     -1        s4 
5    s4     up     -1        s4 
6    s1   left     -1        s1</strong> 
&gt; unique(data$State) 
<strong>[1] "s2" "s4" "s1" "s3"</strong> 
&gt; unique(data$Action) 
<strong>[1] "left"  "right" "down"  "up" </strong>   
&gt; unique(data$NextState) 
<strong>[1] "s2" "s4" "s1" "s3"</strong> 
&gt; unique(data$Reward) 
<strong>[1] -1 10</strong>  </pre>
<p>The function called <kbd>sampleGridSequence()</kbd> is used to generate <em>n</em> observations. The <kbd>State</kbd> and <kbd>NextState</kbd> variables have four unique values: <kbd>s1</kbd>, <kbd>s2</kbd>, <kbd>s3</kbd>, and <kbd>s4</kbd>. The variable, <kbd>Action</kbd>, has four values: <kbd>left</kbd>, <kbd>right</kbd>, <kbd>down</kbd>, and <kbd>up</kbd>. The variable, <kbd>Award</kbd>, has two unique values of <kbd>-1</kbd> and <kbd>10</kbd>, and we could view <kbd>-1</kbd> as a punishment and <kbd>10</kbd> as a reward. For the first observation, if our current and next state is <kbd>s2</kbd> and our action is <kbd>left</kbd>, we would suffer a <kbd>-1</kbd> penalty. The following result shows that if the next status is the same as our current one, no matter what our action would take, we would always have a negative penalty:</p>
<pre>&gt; x&lt;-subset(data,data$State==data$NextState) 
&gt; head(x) 
<strong>  State Action Reward NextState 
1    s2   left     -1        s2 
2    s4  right     -1        s4 
3    s2   down     -1        s2 
4    s4     up     -1        s4 
5    s4     up     -1        s4 
6    s1   left     -1        s1</strong> 
&gt; unique(x$Reward) 
<strong>[1] -1</strong>  </pre>
<p>The question is, when at a different state, what is the optimal action we should take? For example, at the <kbd>s1</kbd> state, should we move <kbd>left</kbd>, <kbd>right</kbd>, <kbd>up</kbd>, or <kbd>down</kbd>? Note that the <kbd>set.seed()</kbd> function is used to guarantee that every user would get the same result if they use the same random seed of <kbd>123</kbd>:</p>
<pre>library(ReinforcementLearning) 
set.seed(123) 
data &lt;- sampleGridSequence(1000) 
control &lt;- list(alpha = 0.1, gamma = 0.1, epsilon = 0.1) 
model &lt;- ReinforcementLearning(data,s="State",a="Action",r="Reward",s_new="NextState",control=control) 
print(model) </pre>
<p>The output is shown here:</p>
<pre>&gt; print(model) 
<strong>State-Action function Q 
         right        up        down      left 
s1 -1.09768561 -1.098833 -1.00284548 -1.098910 
s2 -0.03076799 -1.099211 -1.00501562 -1.005837 
s3 -0.02826295  9.808764 -0.02869875 -1.003904 
s4 -1.10177002 -1.106688 -1.10776585 -1.106276 
 
Policy 
     s1      s2      s3      s4  
 "down" "right"    "up" "right"  
Reward (last iteration) 
[1] -505</strong> </pre>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation of supervised learning via R</h1>
                </header>
            
            <article>
                
<p>As we have discussed in the previous chapter, the best choice to conduct various tests for supervised learning is applying an R package called <kbd>Rattle</kbd>. Here, we show two more examples. Let's first look at the <kbd>iris</kbd> dataset:</p>
<pre>&gt; library(rattle) 
&gt; rattle() </pre>
<p>The next example is using the <kbd>diabetes</kbd> dataset, shown in the screenshot here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-847 image-border" src="Images/e3bc511f-0ba3-494d-82e6-28b475bc696f.png" style="width:33.67em;height:19.50em;" width="679" height="394"/></p>
<p>For example, we could choose the logistic model after clicking <span class="packt_screen">Model</span> on the menu bar. After clicking on <span class="packt_screen">Execute</span>, we would have the following output:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-848 image-border" src="Images/b268ab90-54a9-4cca-a904-755a84f47845.png" style="width:39.83em;height:29.92em;" width="692" height="520"/></p>
<p>Based on the significant level of p-values, we could see that in addition to the intercept, <kbd>x1</kbd><em>,</em> <kbd>x2</kbd><em>,</em> <kbd>x3</kbd>, and <kbd>x6</kbd> are statistically significant.</p>
<p>The next example is from the R package called <kbd>LogicReg</kbd>. The code is given here:</p>
<pre>library(LogicReg) 
data(logreg.testdat) 
y&lt;-logreg.testdat[,1] 
x&lt;-logreg.testdat[, 2:21] 
n=1000 
n2=25000 
set.seed(123) 
myanneal&lt;-logreg.anneal.control(start=-1,end=-4,iter=n2,update=n) 
output&lt;-logreg(resp=y,bin=x,type=2,select = 1,ntrees=2,anneal.control=myanneal) 
plot(output) </pre>
<p>The related graph is shown here. Again, the <kbd>set.seed()</kbd> function is used for an easy replication. If the user omits the function or chooses a different seed, they would get quite a different result:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="Images/4033da8d-3a63-4da6-a1c3-f4b6e0bdd52f.png" style="width:26.42em;height:25.75em;" width="522" height="509"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to RTextTools</h1>
                </header>
            
            <article>
                
<p>This R package is about <em>Automatic Text Classification via Supervised Learning</em>. It is a machine learning package for automatic text classification that makes it simple for novice users to get started with machine learning, while allowing experienced users to easily experiment with different settings and algorithm combinations. The package includes nine algorithms for ensemble classification (svm, slda, boosting, bagging, random forests, glmnet, decision trees, neural networks, and maximum entropy), comprehensive analytics, and thorough documentation. Here, we use the New York Times Times article as an example. First, let's look at the data:</p>
<pre>&gt;library(RTextTools)  
&gt;data(NYTimes) 
&gt;set.seed(123) # guarantees the same result  
&gt;data &lt;- NYTimes[sample(1:3100,size=100,replace=FALSE),] 
&gt;head(data) </pre>
<p>When running the program, users should move this line. The output is shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-850 image-border" src="Images/7d443ffe-6d78-497b-b684-adca1173d118.png" style="width:46.33em;height:15.00em;" width="733" height="238"/></p>
<p>Next, we could look at the types of existing algorithms by using the <kbd>print_algorithms()</kbd> function, shown in the code and output here:</p>
<pre>&gt;library(RTextTools) 
&gt;print_algorithms() 
<strong>[1] "BAGGING"  "BOOSTING" "GLMNET"   "MAXENT"   "NNET"     
[6] "RF"       "SLDA"     "SVM"      "TREE"</strong>     </pre>
<p>The <kbd>BAGGING</kbd> is for zbc, <kbd>BOOSTING</kbd> is the bbb, <kbd>GLMNET</kbd> is for the general linear model, <kbd>MAXENT</kbd> is for maximum entropy model, <kbd>NNET</kbd> is for neural network, <kbd>RF</kbd> is for random forest, <kbd>SLDA</kbd> is for supervised machine learning algorithms, <kbd>SVM</kbd> is for support vector machine, and <kbd>TREE</kbd> is for decision trees. The code is shown here:</p>
<pre>library(RTextTools) 
data(NYTimes) 
data &lt;- NYTimes[sample(1:3100,size=100,replace=FALSE),] 
matrix &lt;- create_matrix(cbind(data["Title"],data["Subject"]), language="english", 
removeNumbers=TRUE, stemWords=FALSE, weighting=tm::weightTfIdf) 
container &lt;- create_container(matrix,data$Topic.Code,trainSize=1:75, testSize=76:100, 
virgin=TRUE) 
models &lt;- train_models(container, algorithms=c("MAXENT","SVM")) 
results &lt;- classify_models(container, models) 
analytics &lt;- create_analytics(container, results) 
summary(analytics) </pre>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation via Python</h1>
                </header>
            
            <article>
                
<p>In the previous chapter related to unsupervised learning, we have learnt about several Python packages. Fortunately, these packages can be applied to supervised learning algorithms as well. The following example is for a linear regression by using a few Python datasets. The Python dataset can be downloaded from the author's website at <a href="http://www.canisius.edu/~yany/python/ffcMonthly.pkl">http://www.canisius.edu/~yany/python/ffcMonthly.pkl</a>. Assume that the data is saved under <kbd>c:/temp/</kbd>:</p>
<pre>import pandas as pd 
x=pd.read_pickle("c:/temp/ffcMonthly.pkl") 
print(x.head()) 
print(x.tail()) </pre>
<p>The output is shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-851 image-border" src="Images/94d8d082-d81d-4027-9c23-f028ee1ad464.png" style="width:23.42em;height:11.83em;" width="417" height="210"/></p>
<p>We plan to run a linear regression; see the formula here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/8440dd65-eba0-4a2c-9086-19b30ad726e5.png" style="width:29.92em;height:1.33em;" width="5160" height="230"/>   </p>
<p>Here, <em>R<sub>i</sub></em> is stock i's returns, <em>R<sub>mkt</sub></em> is the <em>market returns</em>, <em>R<sub>SMB</sub></em> is the <em>portfolio returns of small stocks</em> minus the <em>portfolio returns of big stocks</em>, <em>R<sub>HML</sub></em> is the <em>portfolio returns with high book-to-market ratio</em> (of equity) minus the <em>portfolio returns of stocks with low book-to-market ratio</em>. The Python program is given next. Note that the Python dataset called <kbd>ffDaily.pkl</kbd> is downloadable at <a href="http://canisius.edu/~yany/python/data/ffDaily.pkl">http://canisius.edu/~yany/python/data/ffDaily.pkl</a>:</p>
<pre>import  scipy as sp 
import pandas as pd 
import quandl as qd 
import statsmodels.api as sm 
#quandl.ApiConfig.api_key = 'YOUR_API_KEY' 
a=qd.get("WIKI/IBM")  
p=a['Adj. Close'] 
n=len(p) 
ret=[] 
# 
for i in range(n-1): 
    ret.append(p[i+1]/p[i]-1) 
# 
c=pd.DataFrame(ret,a.index[1:n],columns=['RET']) 
ff=pd.read_pickle('c:/temp/ffDaily.pkl') 
final=pd.merge(c,ff,left_index=True,right_index=True) 
y=final['RET'] 
x=final[['MKT_RF','SMB','HML']] 
#x=final[['MKT_RF']] 
x=sm.add_constant(x) 
results=sm.OLS(y,x).fit() 
print(results.summary()) </pre>
<p>The output is shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-852 image-border" src="Images/e060f888-3d56-4194-8fc3-18ddbe658dd5.png" style="width:39.58em;height:23.58em;" width="729" height="434"/></p>
<p>The next example predicts <kbd>iris</kbd> categories. The code is given first:</p>
<pre>from sklearn import metrics 
from sklearn import datasets 
from sklearn.tree import DecisionTreeClassifier 
x=datasets.load_iris() 
model=DecisionTreeClassifier() 
model.fit(x.data, x.target) 
print(model) 
true=x.target 
predicted=model.predict(x.data) 
print("------ output below --------- n") 
print(metrics.classification_report(true, predicted)) 
print(metrics.confusion_matrix(true, predicted)) </pre>
<p>The nice output is shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-853 image-border" src="Images/53ba7d27-ecb4-4f1e-a0a4-028bc4f77698.png" style="width:34.42em;height:18.50em;" width="613" height="329"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using the scikit-learn (sklearn) module</h1>
                </header>
            
            <article>
                
<p>The following example is borrowed from <a href="http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py">http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py</a>. The program uses the <kbd>scikit-learn</kbd> module to recognize images of handwritten digits. The slightly modified code, for an easy presentation, is shown here:</p>
<pre>import matplotlib.pyplot as plt 
from sklearn import datasets, svm, metrics 
from sklearn.metrics import classification_report as report 
# 
format1="Classification report for classifier %s:n%sn" 
format2="Confusion matrix:n%s" 
digits = datasets.load_digits() 
imageLabels = list(zip(digits.images, digits.target)) 
for index,(image,label) in enumerate(imageLabels[:4]): 
    plt.subplot(2, 4, index + 1) 
    plt.axis('off') 
    plt.imshow(image,cmap=plt.cm.gray_r,interpolation='nearest') 
    plt.title('Training: %i' % label) 
n=len(digits.images) 
data = digits.images.reshape((n,-1)) 
classifier = svm.SVC(gamma=0.001) 
classifier.fit(data[:n//2],digits.target[:n//2]) 
expected = digits.target[n//2:] 
predicted = classifier.predict(data[n//2:]) 
print(format1 % (classifier,report(expected, predicted))) 
print(format2 % metrics.confusion_matrix(expected,predicted)) 
imageAndPrediction=list(zip(digits.images[n//2:],predicted)) 
for index,(image,prediction) in enumerate(imageAndPrediction[:4]): 
    plt.subplot(2,4,index+5) 
    plt.axis('off') 
    plt.imshow(image,cmap=plt.cm.gray_r,interpolation='nearest') 
    plt.title('Prediction: %i' % prediction) 
plt.show() </pre>
<p>Part of the output is shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-854 image-border" src="Images/1bfeae7f-59b4-4887-8bf0-ab8655bc2bd6.png" style="width:45.92em;height:18.33em;" width="857" height="341"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation via Octave</h1>
                </header>
            
            <article>
                
<p>The next example of running a linear regression and the related datasets could be downloaded at <a href="http://canisius.edu/~yany/data/c9_input.csv">http://canisius.edu/~yany/data/c9_input.csv</a>. In the following program, the input data set is assumed to be under <kbd>c:/temp</kbd>:</p>
<pre>a=csvread("c:/temp/c9_input.csv");<br/>x=a(:,2);<br/>y=a(:,3);<br/>figure % open a window for graph<br/>plot(x, y, 'o');<br/>ylabel('Annual returns for S&amp;P500')<br/>xlabel('Annual returns for IBM')</pre>
<p>The first graph is shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-855 image-border" src="Images/42a08750-02e4-427a-8b3b-581c3e0f3ab9.png" style="width:33.58em;height:25.92em;" width="520" height="401"/></p>
<p>To save space, the long program will not be shown here. Interested readers can use the previous link. However, its output graph is shown instead:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-856 image-border" src="Images/68d39b45-54a0-4a1c-b57f-a9aeefb5561f.png" style="width:33.17em;height:24.83em;" width="537" height="400"/></p>
<p> </p>
<p>For the next example, we download an Octave machine library at <a href="https://github.com/partharamanujam/octave-ml"><span class="URLPACKT">https://github.com/partharamanujam/octave-ml</span></a>. Assume that the location of the directory is <kbd>C:\Octave\octave-ml-master</kbd> and the related <kbd>octavelib</kbd> is <kbd>C:\Octave\octave-ml-master\octavelib</kbd>. Then we add its path to our Octave program with the following one-liner: </p>
<pre>addpath(genpath('C:\Octave\octave-ml-master\octavelib'));</pre>
<p>There are many useful programs included under the subdirectory. Note that the <kbd>.m</kbd> extensions are all removed in the following table for brevity. Some useful programs included in the directory are presented in the table here:</p>
<table style="border-collapse: collapse;width: 100%" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td style="width: 345px">
<div class="CDPAlignLeft CDPAlign">addBiasTerm</div>
</td>
<td style="width: 348px">
<div class="CDPAlignLeft CDPAlign">kMeansClosestCentroids</div>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>choosePolynomialForLinearGradDesc</p>
</td>
<td style="width: 348px">
<p>kMeansComputeCentroids</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>chooseRBFParamsForSVM</p>
</td>
<td style="width: 348px">
<p>kMeansCostFunction</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>chooseRegularizationForLinearGradDesc</p>
</td>
<td style="width: 348px">
<p>kMeansInitCentroids</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>chooseRegularizationForLogisticGradDesc</p>
</td>
<td style="width: 348px">
<p>linearRegressionCostFunction</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>collaborativeFilteringCostFunction</p>
</td>
<td style="width: 348px">
<p>logisticRegressionCostFunction</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>computeCoFiParamsByGradDescFmincg</p>
</td>
<td style="width: 348px">
<p>logisticRegressionOneVsAllError</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>computeGaussianParams</p>
</td>
<td style="width: 348px">
<p>logisticRegressionOneVsAllTheta</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>computeMultivarGaussianDistribution</p>
</td>
<td style="width: 348px">
<p>normalizeRatings</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>computePCA</p>
</td>
<td style="width: 348px">
<p>porterStemmer</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>computeScalingParams</p>
</td>
<td style="width: 348px">
<p>predictByLinearGradDesc</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>computeThetaByLinearGradDescFminunc</p>
</td>
<td style="width: 348px">
<p>predictByLogisticGradDescOneVsAll</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>computeThetaByLogisticGradDescFminunc</p>
</td>
<td style="width: 348px">
<p>predictByNormalEquation</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>computeThetaByNormalEquation</p>
</td>
<td style="width: 348px">
<p>projectPCAData</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>computeThresholdForMultivarGaussian</p>
</td>
<td style="width: 348px">
<p>recoverPCAData</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>fmincg</p>
</td>
<td style="width: 348px">
<p>scaleFeatures</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>generateFeaturesPolynomial</p>
</td>
<td style="width: 348px">
<p>sigmoid</p>
</td>
</tr>
<tr>
<td style="width: 345px">
<p>generateKMeansClusters</p>
</td>
<td style="width: 348px"/>
</tr>
<tr>
<td style="width: 345px">
<p>generateKMeansClustersMinCost</p>
</td>
<td style="width: 348px"/>
</tr>
</tbody>
</table>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Table 9.1 Supporting Octave programs under the octavelib directory</div>
<p>In addition, the sample programs and their related input datasets are included under the subdirectory called <kbd>examples</kbd> (see the following table):</p>
<table style="border-collapse: collapse;width: 100%" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td style="width: 292px">
<div class="CDPAlignCenter CDPAlign"><strong>Programs</strong></div>
</td>
<td style="width: 371px">
<div class="CDPAlignCenter CDPAlign"><strong>Dataset(s)</strong></div>
</td>
</tr>
<tr>
<td style="width: 292px">
<p>cofi.m</p>
</td>
<td style="width: 371px">
<p>movieList.mat</p>
</td>
</tr>
<tr>
<td style="width: 292px">
<p>extractEmailFeatures.m</p>
</td>
<td style="width: 371px">
<p>email-sample-1.txt</p>
<p>email-sample-2.txt</p>
<p>email-sample-3.txt</p>
</td>
</tr>
<tr>
<td style="width: 292px">
<p>gaussian_m.m</p>
</td>
<td style="width: 371px">
<p>anomaly.dat</p>
</td>
</tr>
<tr>
<td style="width: 292px">
<p>initEnv.m</p>
</td>
<td style="width: 371px">
<p>Note: setup program</p>
</td>
</tr>
<tr>
<td style="width: 292px">
<p>k_means.m</p>
</td>
<td style="width: 371px">
<p>bird_small.png</p>
</td>
</tr>
<tr>
<td style="width: 292px">
<p>linear_gd.m</p>
</td>
<td style="width: 371px">
<p>damlevels.mat</p>
</td>
</tr>
<tr>
<td style="width: 292px">
<p>logistic_gd.m</p>
</td>
<td style="width: 371px">
<p>numbers.mat</p>
</td>
</tr>
<tr>
<td style="width: 292px">
<p>normal_eq.m</p>
</td>
<td style="width: 371px">
<p>prices.csv</p>
</td>
</tr>
<tr>
<td style="width: 292px">
<p>pca.m</p>
</td>
<td style="width: 371px">
<p>faces.mat</p>
</td>
</tr>
<tr>
<td style="width: 292px">
<p>svm.m</p>
</td>
<td style="width: 371px">
<p>spam-vocab.txt</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Table 9.2 Examples and their related datasets</div>
<p>Let's look at the following program for k-means. The original program is called <kbd>k_means.m</kbd> and its input photo is <kbd>bird_small.png</kbd>. The program tries to save space for a given picture by using the k-means method:</p>
<pre>A = double(imread('bird_small.png')); 
A = A / 255; % Divide by 255, values in the range 0 - 1 
imgSize = size(A); 
X = reshape(A, imgSize(1) * imgSize(2), 3); 
k = 16; % using 4-bit (16) colors,minimize cost 
[Centroids,idx,cost]=generateKMeansClustersMinCost(X,k,10,10); 
fprintf('Cost/Distortion of computed clusters:%.3fn', cost); 
% regenerate colors &amp; image 
NewX = Centroids(idx, :); 
NewA = reshape(NewX, imgSize(1), imgSize(2), 3); 
% compare both the images 
fprintf('Comparing original &amp; compressed imagesn'); 
subplot(1, 2, 1); 
imagesc(A); 
axis("square"); 
title('Original'); 
subplot(1, 2, 2); 
imagesc(NewA); 
axis("square"); 
title('Compressed'); </pre>
<p>The related two photos are shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-857 image-border" src="Images/4ba844d4-d7ef-42c9-be45-be91b422ac59.png" style="width:30.17em;height:15.33em;" width="480" height="244"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation via Julia</h1>
                </header>
            
            <article>
                
<p>The first example uses the familiar dataset called <kbd>iris</kbd> again. Using the <kbd>kmeans()</kbd> function, the program tries to group these plants:</p>
<pre>using Gadfly 
using RDatasets 
using Clustering 
iris = dataset("datasets", "iris") 
head(iris) 
features=permutedims(convert(Array, iris[:,1:4]),[2, 1]) 
result=kmeans(features,3)                           
nameX="PetalLength" 
nameY="PetalWidth" 
assignments=result.assignments   
plot(iris, x=nameX,y=nameY,color=assignments,Geom.point) </pre>
<p>The related output is shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-858 image-border" src="Images/0b5d7032-066f-46f9-b30c-f85d7123baa6.png" style="width:32.75em;height:21.58em;" width="520" height="343"/></p>
<p>For the next example, we try to sort a set of random numbers into <kbd>20</kbd> clusters. The code is shown here:</p>
<pre>using Clustering 
srand(1234) 
nRow=5 
nCol=1000 
x = rand(nRow,nCol) 
maxInter=200  #max interation  
nCluster=20 
R = kmeans(x,nCluster;maxiter=maxInter,display=:iter) 
@assert nclusters(R) ==nCluster 
c = counts(R) 
clusters=R.centers </pre>
<p>To show the value of <kbd>x</kbd> and <kbd>clusters</kbd>, we simply type them on the command line, shown here:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-859 image-border" src="Images/ef6bb11b-21c2-42a0-a54d-0142c1756d00.png" style="width:36.00em;height:11.08em;" width="610" height="187"/></p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Task view for machine learning in R</h1>
                </header>
            
            <article>
                
<p>From the previous chapters, we know that there are about three dozen task views for different topics. A task view is a set of R packages around a specific topic such as finance, econometrics, and the like. In the previous chapter, we briefly discussed the task view for clustering. There is no task view with a name of supervised learning. Instead, the closest one will be the machine learning task view; see the screenshot here for the top part:</p>
<p class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-860 image-border" src="Images/b14b3127-1f80-4d80-838c-34d07f1f0024.png" style="width:38.83em;height:21.17em;" width="670" height="365"/></p>
<p>The URL is given in the preceding screenshot. Again, we could issue just three lines of R code to install the R packages included in the task view:</p>
<pre>&gt;install.packages("ctv") 
&gt;library("ctv") 
&gt;install.views("MachineLearning") </pre>
<p>About 105 related R packages were installed as of April 3, 2018.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have discussed supervised learning, such as classification, the k-nearest neighbors algorithm, Bayes classifiers, reinforcement learning, and the <kbd>RTextTools</kbd> and <kbd>sklearn</kbd> modules in R. In addition, we discussed implementations of supervised learning via R, Python, Julia, and Octave.</p>
<p>For the next chapter, we will discuss predictive data analytics, modeling and validation, some useful datasets, time-series analytics, how to predict the future, seasonality, and how to visualize our data. For Python packages, we will mention<span> </span><kbd>predictives-models-building</kbd>,<span> </span><kbd>model-catwalk</kbd>, and<span> </span><kbd>easyML</kbd>. For R packages, we will discuss<span> </span><kbd>datarobot</kbd>,<span> </span><kbd>LiblineaR</kbd>,<span> </span><kbd>eclust</kbd>, and<span> </span><kbd>AppliedPredictiveModeling</kbd>. For Julia packages, we will explain<span> </span><kbd>EmpiricalRisks</kbd><span> </span>and<span> </span><kbd>ValidatedNumerics</kbd>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Review questions and exercises</h1>
                </header>
            
            <article>
                
<ol>
<li> What does<span> </span><em>unsupervised learning</em> mean?</li>
<li> What is the major difference between unsupervised learning and supervised learning?</li>
<li>How do we install the Python package<span> </span><kbd>sklearn</kbd>?</li>
<li>Discuss the relationship between distance and clustering classification.</li>
<li>What does Bayes classification mean?</li>
<li>Find out the related functions for Bayes classification in R, Python, Octave, and Julia.</li>
</ol>
<p> </p>
<ol start="7">
<li>How many R packages are installed after you run the following three lines of R code?</li>
</ol>
<pre style="padding-left: 90px">&gt;install.packages("ctv")<br/>&gt;library("ctv")<br/>&gt;install.views("MachineLearning")</pre>
<ol start="8">
<li>Download the IBM monthly data from Yahoo!Finance, <a href="https://finance.yahoo.com">https://finance.yahoo.com</a> . Then run a Fama-French-Carhart four factor model by using Python. The 4-factor model is given here:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c8c7fbb6-7041-4152-92a6-7229f522dc89.png" style="width:37.92em;height:1.42em;" width="6150" height="230"/></div>
<p style="padding-left: 60px">The Python dataset related to those 4-factors can be downloaded from the author's website at <a href="http://www.canisius.edu/~yany/python/ffcMonthly.pkl">http://www.canisius.edu/~yany/python/ffcMonthly.pkl</a>.</p>
<ol start="9">
<li>Download the famous iris data set from the UCI Machine Learning Repository. The first several lines are given here: </li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="Images/276edafe-1193-447d-adbf-ea0e95089b47.png" style="width:16.17em;height:9.75em;" width="206" height="124"/></div>
<p style="padding-left: 90px">Then generate data sets for both R and Python. </p>
<ol start="10">
<li>To implement machine learning using Octave, how is the usage of the<span> </span><kbd>octavelib</kbd><span> </span>library introduced and used in the chapter? How many programs are under it?</li>
<li>From the provided Octave programs discussed in this chapter, run the program called<span> </span><kbd>logistic_gd.m</kbd><span> </span>with an input Octave data set called<span> </span><kbd>numbers.mat</kbd>. For more details, see Table 9.1 and 9.2 and the related explanations.</li>
<li>Discuss the<span> </span><kbd>spa</kbd><span> </span>R package and show a few related examples.</li>
<li>Download Fama-French-Carhart 4-factor model, see here:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/226dc7f5-afa8-4050-8ba8-404450076341.png" style="width:27.58em;height:1.33em;" width="4760" height="230"/></div>
<div style="padding-left: 60px">
<p>where R<sub>i</sub><span> </span>is stock<span> </span><em>i</em>’s returns, R<sub>mkt</sub><span> </span>is the market returns, R<sub>SMB</sub><span> </span>is the portfolio returns of small stocks might the portfolio returns of big stocks, R<sub>HML</sub><span> </span>is the portfolio returns with high book-to-market ratio (of equity) minus the portfolio returns of stocks with low book-to-market ratio, R<sub>MOM</sub><span> </span>is the momentum factor. Write R/Python/Octave/Julia programs to run linear regressions for IBM stocks. Source of Data: 1) Prof. French’s Data Library, <a href="http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html">http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html</a>, and Yahoo!Finance,<span> </span><a href="https://finance.yahoo.com">https://finance.yahoo.com</a>.</p>
</div>
<ol start="14">
<li>
<p>Download the Bank Marketing Data Set from the UCI Machine Learning Repository, <a href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing">https://archive.ics.uci.edu/ml/datasets/Bank+Marketing</a>. The data is related to direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. Use supervised learning to identify the most influential factors.</p>
</li>
<li>Download a data set called<span> </span><em>Census Income</em>, <a href="https://archive.ics.uci.edu/ml/datasets/Census+Income">https://archive.ics.uci.edu/ml/datasets/Census+Income</a><em>. </em>Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions:<span> </span><kbd>((AAGE&gt;16) &amp;&amp; (AGI&gt;100) &amp;&amp; (AFNLWGT&gt;1) &amp;&amp; (HRSWK&gt;0))</kbd><span> </span>. Write both R and Python programs to predict whether a person makes over $50,000 a year.</li>
<li>The utility function for an investor is shown here:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/02120cb7-b0a4-4dfc-aaac-5cf4806349ff.png" style="width:9.17em;height:1.42em;" width="1680" height="260"/></p>
<p style="padding-left: 60px">where U is the utility function, E(R) is the expected portfolio return and we could use its mean to approximate, A is the risk-averse coefficient and σ<sup>2</sup><span> </span>is the variance of the portfolio. Go to Professor French’s Data Library to download 10 industry portfolio returns. Assume a risk-averse variable of A varies from 1 to 100.  Use the k-means method to group investors based on E(R), A, and risk. </p>
<ol start="17">
<li>Download a data set called Bike Sharing Data Set from the UCI Machine Learning Repository, <a href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset">https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset</a>.  The dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bike share system with the corresponding weather and seasonal information. Write an R program to group individual riders into k-groups.</li>
</ol>
<p> </p>
<ol start="18">
<li>Download a data set call Bag of Words, <a href="https://archive.ics.uci.edu/ml/datasets/Bag+of+Words">https://archive.ics.uci.edu/ml/datasets/Bag+of+Words</a>.  Write both R and Python to conduct a cluster analysis.</li>
<li>Download a data set called Sonar Dataset. The Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the strength of sonar returns at different angles.</li>
<li>Use the data set called Boston to run a regression (Python). The first several lines are shown here:</li>
</ol>
<pre style="padding-left: 90px">from sklearn import datasets<br/>boston = datasets.load_boston()<br/>print(boston.data.shape)<br/>(506, 13)</pre>
<p> </p>


            </article>

            
        </section>
    </div></body></html>