- en: Time to Put Some Order - Cluster Your Data with Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*"If you take a galaxy and try to make it bigger, it becomes a cluster of galaxies,
    not a galaxy. If you try to make it smaller than that, it seems to blow itself
    apart"*'
  prefs: []
  type: TYPE_NORMAL
- en: '- Jeremiah P. Ostriker'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will delve deeper into machine learning and find out how
    we can take advantage of it to cluster records belonging to a certain group or
    class for a dataset of unsupervised observations. In a nutshell, the following
    topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering (HC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centroid-based clustering (CC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribution-based clustering (DC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining number of clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comparative analysis between clustering algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Submitting jobs on computing clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will provide a brief introduction to unsupervised machine
    learning technique with appropriate examples. Let's start the discussion with
    a practical example. Suppose you have a large collection of not-pirated-totally-legal
    mp3s in a crowded and massive folder on your hard drive. Now, what if you can
    build a predictive model that helps automatically group together similar songs
    and organize them into your favorite categories such as country, rap, rock, and
    so on. This act of assigning an item to a group such that a mp3 to is added to
    the respective playlist in an unsupervised way. In the previous chapters, we assumed
    you're given a training dataset of correctly labeled data. Unfortunately, we don't
    always have that extravagance when we collect data in the real-world. For example,
    suppose we would like to divide up a large amount of music into interesting playlists.
    How could we possibly group together songs if we don't have direct access to their
    metadata? One possible approach could be a mixture of various machine learning
    techniques, but clustering is often at the heart of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: In short, iIn unsupervised machine learning problem, correct classes of the
    training dataset are not available or unknown. Thus, classes have to be deduced
    from the structured or unstructured datasets as shown in *Figure 1*. This essentially
    implies that the goal of this type of algorithm is to preprocess the data in some
    structured ways. In other words, the main objective of the unsupervised learning
    algorithms is to explore the unknown/hidden patterns in the input data that are
    unlabeled*.* Unsupervised learning, however, also comprehends other techniques
    to explain the key features of the data in an exploratory way toward finding the
    hidden patterns. To overcome this challenge, clustering techniques are used widely
    to group unlabeled data points based on certain similarity measures in an unsupervised
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an in-depth theoretical knowledge of how unsupervised algorithms work,
    please refer to the following three books: *Bousquet*, *O.; von Luxburg*, *U.;
    Raetsch*, *G., eds* (2004). *Advanced Lectures on Machine Learning*. *Springer-Verlag*.
    ISBN 978-3540231226\. Or *Duda*, *Richard O.*; *Hart*, *Peter E.*; *Stork*, *David
    G*. (2001). *Unsupervised Learning and Clustering*. *Pattern classification* (2nd
    Ed.). *Wiley*. ISBN 0-471-05669-3 and *Jordan*, *Michael I.*; *Bishop*, *Christopher
    M*. (2004) *Neural Networks*. In *Allen B. Tucker* *Computer Science Handbook,
    Second Edition* (Section VII: Intelligent Systems). *Boca Raton*, FL: Chapman
    and Hall/CRC Press LLC. ISBN 1-58488-360-X.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00263.jpeg)**Figure 1:** Unsupervised learning with Spark'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In clustering tasks, an algorithm groups related features into categories by
    analyzing similarities between input examples where similar features are clustered
    and marked using circles around. Clustering uses include but are not limited to
    the following: search result grouping such as grouping customers, anomaly detection
    for suspicious pattern finding, text categorization for finding useful pattern
    in tests, social network analysis for finding coherent groups, data center computing
    clusters for finding a way to put related computers together, astronomic data
    analysis for galaxy formation, and real estate data analysis to identify neighborhoods
    based on similar features. We will show a Spark MLlib-based solution for the last
    use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss clustering techniques along with challenges
    and suitable examples. A brief overview of hierarchical clustering, centroid-based
    clustering, and distribution-based clustering will be provided too.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning and the clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering analysis is about dividing data samples or data points and putting
    them into corresponding homogeneous classes or clusters. Thus a trivial definition
    of clustering can be thought as the process of organizing objects into groups
    whose members are similar in some way.
  prefs: []
  type: TYPE_NORMAL
- en: A *cluster* is, therefore, a collection of objects that are *similar* between
    them and are *dissimilar* to the objects belonging to other clusters. As shown
    in *Figure 2*, if a collection of objects is given, clustering algorithms put
    those objects into a group based on similarity. A clustering algorithm such as
    K-means has then located the centroid of the group of data points. However, to
    make the clustering accurate and effective, the algorithm evaluates the distance
    between each point from the centroid of the cluster. Eventually, the goal of clustering
    is to determine the intrinsic grouping in a set of unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00059.jpeg)**Figure 2:** Clustering raw data'
  prefs: []
  type: TYPE_NORMAL
- en: Spark supports many clustering algorithms such as **K-means**, **Gaussian mixture**,
    **power iteration clustering** (**PIC**), l**atent dirichlet allocation** (**LDA**),
    **bisecting K-means**, and **Streaming K-means**. LDA is used for document classification
    and clustering commonly used in text mining. PIC is used for clustering vertices
    of a graph consisting of pairwise similarities as edge properties. However, to
    keep the objective of this chapter clearer and focused, we will confine our discussion
    to the K-means, bisecting K-means, and Gaussian mixture algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The hierarchical clustering technique is based on the fundamental idea of objects
    or features that are more related to those nearby than others far away. Bisecting
    K-means is an example of such hierarchical clustering algorithm that connects
    data objects to form clusters based on their corresponding distance.
  prefs: []
  type: TYPE_NORMAL
- en: In the hierarchical clustering technique, a cluster can be described trivially
    by the maximum distance needed to connect parts of the cluster. As a result, different
    clusters will be formed at different distances. Graphically, these clusters can
    be represented using a dendrogram. Interestingly, the common name hierarchical
    clustering evolves from the concept of the dendrogram.
  prefs: []
  type: TYPE_NORMAL
- en: Centroid-based clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In centroid-based clustering technique, clusters are represented by a central
    vector. However, the vector itself may not necessarily be a member of the data
    points. In this type of learning, a number of the probable clusters must be provided
    prior to training the model. K-means is a very famous example of this learning
    type, where, if you set the number of clusters to a fixed integer to say K, the
    K-means algorithm provides a formal definition as an optimization problem, which
    is a separate problem to be resolved to find the K cluster centers and assign
    the data objects the nearest cluster center. In short, this is an optimization
    problem where the objective is to minimize the squared distances from the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution-based clustestering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distribution-based clustering algorithms are based on statistical distribution
    models that provide more convenient ways to cluster related data objects to the
    same distribution. Although the theoretical foundations of these algorithms are
    very robust, they mostly suffer from overfitting. However, this limitation can
    be overcome by putting constraints on the model complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Centroid-based clustering (CC)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we discuss the centroid-based clustering technique and its
    computational challenges. An example of using K-means with Spark MLlib will be
    shown for a better understanding of the centroid-based clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in CC algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed previously, in a centroid-based clustering algorithm like K-means,
    setting the optimal value of the number of clusters K is an optimization problem.
    This problem can be described as NP-hard (that is non-deterministic polynomial-time
    hard) featuring high algorithmic complexities, and thus the common approach is
    trying to achieve only an approximate solution. Consequently, solving these optimization
    problems imposes an extra burden and consequently nontrivial drawbacks. Furthermore,
    the K-means algorithm expects that each cluster has approximately similar size.
    In other words, data points in each cluster have to be uniform to get better clustering
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Another major drawback of this algorithm is that this algorithm tries to optimize
    the cluster centers but not cluster borders, and this often tends to inappropriately
    cut the borders in between the clusters. However, sometimes, we can have the advantage
    of visual inspection, which is often not available for data on hyperplanes or
    multidimensional data. Nonetheless, a complete section on how to find the optimal
    value of K will be discussed later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How does K-means algorithm work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose we have *n* data points *x[i]*, *i=1...n* that need to be partitioned
    into *k* clusters. Now that the target here is to assign a cluster to each data
    point. K-means then aims to find the positions *μ[i],i=1...k* of the clusters
    that minimize the distance from the data points to the cluster. Mathematically,
    the K-means algorithm tries to achieve the goal by solving the following equation,
    that is, an optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00320.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *c[i]* is the set of data points assigned to cluster
    *i*, and *d(x,μ[i]) =||x−μ[i]||²[2]* is the Euclidean distance to be calculated
    (we will explain why we should use this distance measurement shortly). Therefore,
    we can understand that the overall clustering operation using K-means is not a
    trivial one but an NP-hard optimization problem. This also means that the K-means
    algorithm not only tries to find the global minima but also often gets stuck in
    different solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how we could formulate the algorithm before we can feed the
    data to the K-means model. At first, we need to decide the number of tentative
    clusters, *k* priory. Then, typically, you need to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00367.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here *|c|* is the number of elements in *c*.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering using the K-means algorithm begins by initializing all the coordinates
    to centroids. With every pass of the algorithm, each point is assigned to its
    nearest centroid based on some distance metric, usually *Euclidean distance*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Distance calculation:** Note that there are other ways to calculate the distance
    too, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chebyshev distance* can be used to measure the distance by considering only
    the most notable dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: The *Hamming distance* algorithm can identify the difference between two strings.
    On the other hand, to make the distance metric scale-undeviating, *Mahalanobis
    distance* can be used to normalize the covariance matrix. The *Manhattan distance*
    is used to measure the distance by considering only axis-aligned directions. The
    *Minkowski distance* algorithm is used to make the Euclidean distance, Manhattan
    distance, and Chebyshev distance. The *Haversine distance* is used to measure
    the great-circle distances between two points on a sphere from the location, that
    is, longitudes and latitudes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering these distance-measuring algorithms, it is clear that the Euclidean
    distance algorithm would be the most appropriate to solve our purpose of distance
    calculation in the K-means algorithm. The centroids are then updated to be the
    centers of all the points assigned to it in that iteration. This repeats until
    there is a minimal change in the centers. In short, the K-means algorithm is an
    iterative algorithm and works in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster assignment step**: K-means goes through each of the m data points
    in the dataset which is assigned to a cluster that is represented by the closest
    of the k centroids. For each point, the distances to each centroid is then calculated
    and simply pick the least distant one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update step**: For each cluster, a new centroid is calculated as the mean
    of all points in the cluster. From the previous step, we have a set of points
    which are assigned to a cluster. Now, for each such set, we calculate a mean that
    we declare a new centroid of the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of clustering using K-means of Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To further demonstrate the clustering example, we will use the *Saratoga NY
    Homes* dataset downloaded from [http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html)
    as an unsupervised learning technique using Spark MLlib. The dataset contains
    several features of houses located in the suburb of the New York City. For example,
    price, lot size, waterfront, age, land value, new construct, central air, fuel
    type, heat type, sewer type, living area, pct.college, bedrooms, fireplaces, bathrooms,
    and the number of rooms. However, only a few features have been shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Price** | **Lot Size** | **Water Front** | **Age** | **Land Value** | **Rooms**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 132,500 | 0.09 | 0 | 42 | 5,000 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 181,115 | 0.92 | 0 | 0 | 22,300 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 109,000 | 0.19 | 0 | 133 | 7,300 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 155,000 | 0.41 | 0 | 13 | 18,700 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 86,060 | 0.11 | 0 | 0 | 15,000 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 120,000 | 0.68 | 0 | 31 | 14,000 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 153,000 | 0.4 | 0 | 33 | 23,300 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 170,000 | 1.21 | 0 | 23 | 146,000 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 90,000 | 0.83 | 0 | 36 | 222,000 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 122,900 | 1.94 | 0 | 4 | 212,000 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 325,000 | 2.29 | 0 | 123 | 126,000 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '**Table 1:** Sample data from the Saratoga NY Homes dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The target of this clustering technique here is to show an exploratory analysis
    based on the features of each house in the city for finding possible neighborhoods
    for the house located in the same area. Before performing feature extraction,
    we need to load and parse the Saratoga NY Homes dataset. This step also includes
    loading packages and related dependencies, reading the dataset as RDD, model training,
    prediction, collecting the local parsed data, and clustering comparing.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**. Import-related packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2\. Create a Spark session - the entry point** - Here we at first set
    the Spark configuration by setting the application name and master URL. For simplicity,
    it''s standalone with all the cores on your machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3\. Load and parse the dataset** - Read, parse, and create RDDs from
    the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, to make the preceding code work, you should import the following
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00339.jpeg)**Figure 3:** A snapshot of the Saratoga NY Homes dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `parseLand` method that is used to create a `Land` class
    from an array of `Double` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And the `Land` class that reads all the features as a double is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you already know, to train the K-means model, we need to ensure all the
    data points and features to be numeric. Therefore, we further need to convert
    all the data points to double as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4\. Preparing the training set** - At first, we need to convert the
    data frame (that is, `landDF`) to an RDD of doubles and cache the data to create
    a new data frame to link the cluster numbers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we need to convert the preceding RDD of doubles into an RDD of dense
    vectors as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5\. Train the K-means model** - Train the model by specifying 10 clusters,
    20 iterations, and 10 runs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The Spark-based implementation of K-means starts working by initializing a set
    of cluster centers using the K-means algorithm by *Bahmani et al.*, *Scalable
    K-Means++*, VLDB 2012\. This is a variant of K-means++ that tries to find dissimilar
    cluster centers by starting with a random center and then doing passes where more
    centers are chosen with a probability proportional to their squared distance to
    the current cluster set. It results in a provable approximation to an optimal
    clustering. The original paper can be found at [http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6\. Evaluate the model error rate** - The standard K-means algorithm
    aims at minimizing the sum of squares of the distance between the points of each
    set, that is, the squared Euclidean distance, which is the WSSSE''s objective.
    The K-means algorithm aims at minimizing the sum of squares of the distance between
    the points of each set (that is, the cluster center). However, if you really wanted
    to minimize the sum of squares of the distance between the points of each set,
    you would end up with a model where each cluster is its own cluster center; in
    that case, that measure would be 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, once you have trained your model by specifying the parameters, you
    can evaluate the result by using **Within Set Sum of Squared Errors** (**WSSE**).
    Technically, it is something like the sum of the distances of each observation
    in each K cluster that can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding model training set produces the value of WCSSS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7\. Compute and print the cluster centers** - At first, we get the prediction
    from the model with the ID so that we can link them back to other information
    related to each house. Note that we will use an RDD of rows that we prepared in
    step 4*:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'However, it should be provided when a prediction is requested about the price.
    This should be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For better visibility and an exploratory analysis, convert the RDD to a DataFrame
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce the output shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00044.gif)**Figure 4:** A snapshot of the clusters predicted'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since there''s no distinguishable ID in the dataset, we represented the `Price`
    field to make the linking. From the preceding figure, you can understand where
    does a house having a certain price falls, that is, in which cluster. Now for
    better visibility, let''s join the prediction DataFrame with the original DataFrame
    to know the individual cluster number for each house:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You should observe the output in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00138.jpeg)**Figure 5:** A snapshot of the clusters predicted across
    each house'
  prefs: []
  type: TYPE_NORMAL
- en: To make the analysis, we dumped the output in RStudio and generated the clusters
    shown in *Figure 6*. The R script can be found on my GitHub repositories at [https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics](https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics).
    Alternatively, you can write your own script and do the visualization accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00142.jpeg)**Figure 6:** Clusters of the neighborhoods'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for more extensive analysis and visibility, we can observe related statistics
    for each cluster. For example, below I printed thestatistics related to cluster
    3 and 4 in *Figure 8* and *Figure 9*, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now get the descriptive statistics for each cluster as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'At first, let''s observe the related statistics of cluster 3 in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00353.jpeg)**Figure 7:** Statistics on cluster 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s observe the related statistics of cluster 4 in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00377.jpeg)**Figure 8:** Statistics on cluster 4'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, since the original screenshot was too large to fit in this page,
    the original images were modified and the column containing other variables of
    the houses were removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the random nature of this algorithm, you might receive different results
    for each successful iteration. However, you can lock the random nature of this
    algorithm by setting the seed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 8\. Stop the Spark session** - Finally, stop the Spark session using
    the stop method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we dealt with a very small set of features; common-sense
    and visual inspection would also lead us to the same conclusions. From the above
    example using the K-means algorithm, we can understand that there are some limitations
    for this algorithm. For example, it's really difficult to predict the K-value,
    and with a global cluster it does not work well. Moreover, different initial partitions
    can result in different final clusters, and, finally, it does not work well with
    clusters of different sizes and densities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome these limitations, we have some more robust algorithms in this
    book like MCMC (Markov Chain Monte Carlo; see also at [https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo))
    presented in the book: *Tribble*, *Seth D.*, **Markov chain Monte Carlo** algorithms
    using completely uniformly distributed driving sequences, Diss. Stanford University,
    2007.'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering (HC)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we discuss the hierarchical clustering technique and its computational
    challenges. An example of using the bisecting K-means algorithm of hierarchical
    clustering with Spark MLlib will be shown too for a better understanding of hierarchical
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of HC algorithm and challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A hierarchical clustering technique is computationally different from the centroid-based
    clustering in the way the distances are computed. This is one of the most popular
    and widely used clustering analysis technique that looks to build a hierarchy
    of clusters. Since a cluster usually consists of multiple objects, there will
    be other candidates to compute the distance too. Therefore, with the exception
    of the usual choice of distance functions, you also need to decide on the linkage
    criterion to be used. In short, there are two types of strategies in hierarchical
    clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bottom-up approach**: In this approach, each observation starts within its
    own cluster. After that, the pairs of clusters are merged together and one moves
    up the hierarchy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-down approach**: In this approach, all observations start in one cluster,
    splits are performed recursively, and one moves down the hierarchy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These bottom-up or top-down approaches are based on the s**ingle-linkage clustering**
    (**SLINK**) technique, which considers the minimum object distances, the **complete
    linkage clustering** (**CLINK**), which considers the maximum of object distances,
    and the u**nweighted pair group method with arithmetic mean** (**UPGMA**). The
    latter is also known as **average-linkage clustering**. Technically, these methods
    will not produce unique partitions out of the dataset (that is, different clusters).
  prefs: []
  type: TYPE_NORMAL
- en: A comparative analysis on these three approaches can be found at [https://nlp.stanford.edu/IR-book/completelink.html.](https://nlp.stanford.edu/IR-book/completelink.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the user still needs to choose appropriate clusters from the hierarchy
    for better cluster prediction and assignment. Although algorithms of this class
    like bisecting K-means are computationally faster than the K-means algorithm,
    there are three disadvantages to this type of algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: First, these methods are not very robust toward outliers or datasets containing
    noise or missing values. This disadvantage either imposes additional clusters
    or even causes other clusters to merge. This problem is commonly referred to as
    the chaining phenomenon, especially for single-linkage clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, from the algorithmic analysis, the complexity is for agglomerative clustering
    and for divisive clustering, which makes them too slow for large data sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third, SLINK and CLINK were previously used widely in data mining tasks as theoretical
    foundations of cluster analysis, but nowadays they are considered obsolete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting K-means with Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bisecting K-means can often be much faster than regular K-means, but it will
    generally produce a different clustering. A bisecting K-means algorithm is based
    on the paper, *A comparison of document clustering* techniques by *Steinbach*,
    *Karypis*, and *Kumar*, with modification to fit with Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: Bisecting K-means is a kind of divisive algorithm that starts from a single
    cluster that contains all the data points. Iteratively, it then finds all the
    divisible clusters on the bottom level and bisects each of them using K-means
    until there are K leaf clusters in total or no leaf clusters divisible. After
    that, clusters on the same level are grouped together to increase the parallelism.
    In other words, bisecting K-means is computationally faster than the regular K-means
    algorithm. Note that if bisecting all the divisible clusters on the bottom level
    results in more than K leaf clusters, larger clusters will always get higher priority.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that if the bisecting of all the divisible clusters on the bottom level
    results in more than K leaf clusters, larger clusters will always get higher priority.
    The following parameters are used in the Spark MLlib implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**K**: This is the desired number of leaf clusters. However, the actual number
    could be smaller if there are no divisible leaf clusters left during the computation.
    The default value is 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MaxIterations**: This is the max number of K-means iterations to split the
    clusters. The default value is 20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MinDivisibleClusterSize**: This is the minimum number of points. The default
    value is set as 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seed**: This is a random seed that disallows random clustering and tries
    to provide almost similar result in each iteration. However, it is recommended
    to use a long seed value like 12345 and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting K-means clustering of the neighborhood using Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how to cluster similar houses together to determine
    the neighborhood. The bisecting K-means is also similar to regular K-means except
    that the model training that takes different training parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You should refer to the previous example and just reuse the previous steps
    to get the trained data. Now let''s evaluate clustering by computing WSSSE as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You should observe the following output: `Within-Cluster Sum of Squares = 2.096980212594632E11`.
    Now for more analysis, please refer to step 5 in the previous section.'
  prefs: []
  type: TYPE_NORMAL
- en: Distribution-based clustering (DC)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the distribution-based clustering technique
    and its computational challenges. An example of using **Gaussian mixture models**
    (**GMMs**) with Spark MLlib will be shown for a better understanding of distribution-based
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in DC algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A distribution-based clustering algorithm like GMM is an expectation-maximization
    algorithm. To avoid the overfitting problem, GMM usually models the dataset with
    a fixed number of Gaussian distributions. The distributions are initialized randomly,
    and the related parameters are iteratively optimized too to fit the model better
    to the training dataset. This is the most robust feature of GMM and helps the
    model to be converged toward the local optimum. However, multiple runs of this
    algorithm may produce different results.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, unlike the bisecting K-means algorithm and soft clustering,
    GMM is optimized for hard clustering, and in order to obtain of that type, objects
    are often assigned to the Gaussian distribution. Another advantageous feature
    of GMM is that it produces complex models of clusters by capturing all the required
    correlations and dependence between data points and attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the down-side, GMM has some assumptions about the format and shape of the
    data, and this puts an extra burden on us (that is, users). More specifically,
    if the following two criteria do not meet, performance decreases drastically:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-Gaussian dataset: The GMM algorithm assumes that the dataset has an underlying
    Gaussian, which is generative distribution. However, many practical datasets do
    not satisfy this assumption that is subject to provide low clustering performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the clusters do not have even sizes, there is a high chance that small clusters
    will be dominated by larger ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does a Gaussian mixture model work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using GMM is a popular technique of soft clustering. GMM tries to model all
    the data points as a finite mixture of Gaussian distributions; the probability
    that each point belongs to each cluster is computed along with the cluster related
    statistics and represents an amalgamate distribution: where all the points are
    derived from one of *K* Gaussian subdistributions having own probability. In short,
    the functionality of GMM can be described in a three-steps pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Objective function:** Compute and maximize the log-likelihood using expectation–maximization
    (EM) as a framework'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**EM algorithm:**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**E step:** Compute the posterior probability of membership -i.e. nearer data
    points'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**M step:** Optimize the parameters.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assignment:** Perform soft assignment during step E.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Technically, when a statistical model is given, parameters of that model (that
    is, when applied to a data set) are estimated using the **maximum-likelihood estimation**
    (**MLE**). On the other hand, **EM** algorithm is an iterative process of finding
    maximum likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: Since the GMM is an unsupervised algorithm, GMM model depends on the inferred
    variables. Then EM iteration rotates toward performing the expectation (E) and
    maximization (M) step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark MLlib implementation uses the expectation-maximization algorithm
    to induce the maximum-likelihood model from a given a set of data points. The
    current implementation uses the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**K** is the number of desired clusters to cluster your data points'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ConvergenceTol** is the maximum change in log-likelihood at which we consider
    convergence achieved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MaxIterations** is the maximum number of iterations to perform without reaching
    the convergence point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**InitialModel** is an optional starting point from which to start the EM algorithm.
    If this parameter is omitted, a random starting point will be constructed from
    the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of clustering using GMM with Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, we saw how to cluster the similar houses together
    to determine the neighborhood. Using GMM, it is also possible to cluster the houses
    toward finding the neighborhood except the model training that takes different
    training parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You should refer to the previous example and just reuse the previous steps
    of getting the trained data. Now to evaluate the model''s performance, GMM does
    not provide any performance metrics like WCSS as a cost function. However, GMM
    provides some performance metrics like mu, sigma, and weight. These parameters
    signify the maximum likelihood among different clusters (five clusters in our
    case). This can be demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You should observe the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00154.jpeg)**Figure 9:** Cluster 1![](img/00017.jpeg)**Figure 10:**
    Cluster 2![](img/00240.jpeg)**Figure 11:** Cluster 3![](img/00139.jpeg)**Figure
    12:** Cluster 4![](img/00002.jpeg)**Figure 13:** Cluster 5'
  prefs: []
  type: TYPE_NORMAL
- en: The weight of clusters 1 to 4 signifies that these clusters are homogeneous
    and significantly different compared with cluster 5.
  prefs: []
  type: TYPE_NORMAL
- en: Determining number of clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The beauty of clustering algorithms like K-means algorithm is that it does the
    clustering on the data with an unlimited number of features. It is a great tool
    to use when you have a raw data and would like to know the patterns in that data.
    However, deciding the number of clusters prior to doing the experiment might not
    be successful but may sometimes lead to an overfitting or underfitting problem.
    On the other hand, one common thing to all three algorithms (that is, K-means,
    bisecting K-means, and Gaussian mixture) is that the number of clusters must be
    determined in advance and supplied to the algorithm as a parameter. Hence, informally,
    determining the number of clusters is a separate optimization problem to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use a heuristic approach based on the Elbow method.
    We start from K = 2 clusters, and then we ran the K-means algorithm for the same
    data set by increasing K and observing the value of cost function **Within-Cluster
    Sum of Squares** (**WCSS**). At some point, a big drop in cost function can be
    observed, but then the improvement became marginal with the increasing value of
    K. As suggested in cluster analysis literature, we can pick the K after the last
    big drop of WCSS as an optimal one.
  prefs: []
  type: TYPE_NORMAL
- en: 'By analysing below parameters, you can find out the performance of K-means:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Betweenness:** This is the between sum of squares also called as *intracluster
    similarity.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Withiness:** This is the within sum of square also called *intercluster similarity.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Totwithinss:** This is the sum of all the withiness of all the clusters also
    called *total intracluster similarity.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is to be noted that a robust and accurate clustering model will have a lower
    value of withiness and a higher value of betweenness. However, these values depend
    on the number of clusters, that is, K that is chosen before building the model.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us discuss how to take advantage of the Elbow method to determine the
    number of clusters. As shown in the following, we calculated the cost function
    WCSS as a function of a number of clusters for the K-means algorithm applied to
    home data based on all the features. It can be observed that a big drop occurs
    when K = 5\. Therefore, we chose the number of clusters as 5, as shown in *Figure
    10*. Basically, this is the one after the last big drop.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00303.jpeg)**Figure 14:** Number of clusters as a function of WCSS'
  prefs: []
  type: TYPE_NORMAL
- en: A comparative analysis between clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gaussian mixture is used mainly for expectation minimization, which is an example
    of an optimization algorithm. Bisecting K-means, which is faster than regular
    K-means, also produces slightly different clustering results. Below we try to
    compare these three algorithms. We will show a performance comparison in terms
    of model building time and the computional cost for each algorithm. As shown in
    the following code, we can compute the cost in terms of WCSS. The following lines
    of code can be used to compute the WCSS for the K-means and **b**isecting algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'For the dataset we used throughout this chapter, we got the following values
    of WCSS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that K-means shows slightly better performance in terms of the compute
    cost. Unfortunately, we don''t have any metrics like WCSS for the GMM algorithm.
    Now let''s observe the model building time for these three algorithms. We can
    start the system clock before starting model training and stop it immediately
    after the training has been finished as follows (for K-means):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For the training set we used throughout this chapter, we got the following
    values of model building time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In different research articles, it has been found that the bisecting K-means
    algorithm has been shown to result in better cluster assignment for data points.
    Moreover, compared to K-means, bisecting K-means, alos converges well towards
    global minima. K-means on the other hand, gets stuck in local minima. In other
    words, using bisecting K-means algorithm, we can avoid the local minima that K-means
    can suffer from.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you might observe different values of the preceding parameters depending
    upon your machine's hardware configuration and the random nature of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: More details analysis is up to the readers from the theoretical views. Interested
    readers should also refer to Spark MLlib-based clustering techniques at [https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)
    to get more insights.
  prefs: []
  type: TYPE_NORMAL
- en: Submitting Spark job for cluster analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The examples shown in this chapter can be made scalable for the even larger
    dataset to serve different purposes. You can package all three clustering algorithms
    with all the required dependencies and submit them as a Spark job in the cluster.
    Now use the following lines of code to submit your Spark job of K-means clustering,
    for example (use similar syntax for other classes), for the Saratoga NY Homes
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved even deeper into machine learning and found out how
    we can take advantage of machine learning to cluster records belonging to a dataset
    of unsupervised observations. Consequently, you learnt the practical know-how
    needed to quickly and powerfully apply supervised and unsupervised techniques
    on available data to new problems through some widely used examples based on the
    understandings from the previous chapters. The examples we are talking about will
    be demonstrated from the Spark perspective. For any of the K-means, bisecting
    K-means, and Gaussian mixture algorithms, it is not guaranteed that the algorithm
    will produce the same clusters if run multiple times. For example, we observed
    that running the K-means algorithm multiple times with the same parameters generated
    slightly different results at each run.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a performance comparison between K-means and Gaussian mixture, see *Jung.
    et. al and cluster analysis* lecture notes. In addition to K-means, bisecting
    K-means, and Gaussian mixture, MLlib provides implementations of three other clustering
    algorithms, namely, PIC, LDA, and streaming K-means. One thing is also worth mentioning
    is that to fine tune clustering analysis, often we need to remove unwanted data
    objects called outlier or anomaly. But using distance based clustering it''s really
    difficult to identify such data pints. Therefore, other distance metrics other
    than Euclidean can be used. Nevertheless, these links would be a good resource
    to start with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/keiraqz/anomaly-detection](https://github.com/keiraqz/anomaly-detection)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://www.dcc.fc.up.pt/~ltorgo/Papers/ODCM.pdf](https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next chapter, we will dig even deeper into tuning Spark applications
    for better performance. We will see some best practice to optimize the performance
    of Spark applications.
  prefs: []
  type: TYPE_NORMAL
