<html><head></head><body><div><h1 class="header-title">Declaring the Objectives</h1>
                
            
            
                
<p class="calibre4">This chapter introduces and explains (yet again, from a developer's perspective) the basic objectives behind statistics for data science and introduces the reader to the important terms and key concepts (with explanations and examples) that are used throughout the book.</p>
<p class="calibre4">In this chapter, we've broken things down into the following topics:</p>
<ul class="calibre18">
<li class="calibre19">A primer on the key objectives of data science</li>
<li class="calibre19">Bringing statistics into data science</li>
<li class="calibre19">Common terminologies used with statistics and data science</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Key objectives of data science</h1>
                
            
            
                
<p class="calibre4">As mentioned in <a href="7f3dc6b3-d483-4ffc-b330-22b36da9bdc7.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">Chapter 1</a>, <em class="calibre21">Transitioning from Data Developer to Data Scientist</em>, the idea of how data science is defined is a matter of opinion.</p>
<p class="calibre4">I personally like the explanation that data science is a progression or, even better, an evolution of thought or steps, as shown in the following figure:</p>
<div><img class="image-border7" src="img/f97c563f-aee0-496d-ae4b-68c4eee48a35.png"/></div>
<p class="calibre4">This data science evolution (depicted in the preceding figure) consists of a series of steps or phases that a data scientist tracks, comprising the following:</p>
<ul class="calibre18">
<li class="calibre19">Collecting data</li>
<li class="calibre19">Processing data</li>
<li class="calibre19">Exploring and visualizing data</li>
<li class="calibre19">Analyzing (data) and/or applying machine learning (to data)</li>
<li class="calibre19">Deciding (or planning) based on acquired insight</li>
</ul>
<p class="calibre4">Although a progression or evolution implies a sequential journey, in practice, this is an extremely fluid process; each of the phases may inspire the data scientist to reverse and repeat one or more of the phases until they are satisfied. In other words, all or some phases of the process may be repeated until the data scientist determines that the desired outcome is reached.</p>
<p class="calibre4">For example, after a careful review of a generated visualization (during the <em class="calibre21">Exploring and visualizing data</em> phase), one may determine that additional processing of the data is required or that additional data needs to be collected before any reasonable analysis or learning could be of value.</p>
<p>You might loosely compare the data science process to the agile software development mythology where a developer performs various tasks, the results are analyzed, more work is done, the work is again reviewed, and the process is repeated until the desired results or outcomes are obtained.</p>
<p class="calibre4">Let's explain each of the phases of the data science evolution.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Collecting data</h1>
                
            
            
                
<p class="calibre4">This should be somewhat obvious—without (at least some) data, we cannot perform any of the subsequent steps (although one might argue the point of inference, that would be inappropriate. There is no magic in data science. We, as data scientists, don't make something from anything. Inference (which we'll define later in this chapter) requires at least some data to begin with.</p>
<p class="calibre4">Some new concepts for collecting data include the fact that data can be collected from ample of sources, and the number and types of data sources continue to grow daily. In addition, how data is collected might require a perspective new to a data developer; data for data science isn't always sourced from a relational database, rather from machine-generated logging files, online surveys, performance statistics, and so on; again, the list is ever evolving.</p>
<p class="calibre4">Another point to ponder—collecting data also involves supplementation. For example, a data scientist might determine that he or she needs to be adding additional demographics to a particular pool of application data previously collected, processed, and reviewed.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Processing data</h1>
                
            
            
                
<p class="calibre4">The processing (or transformation) of data is where the data scientist's programming skills will come in to play (although you can often find a data scientist performing some sort of processing in other steps, like collecting, visualizing, or learning).</p>
<p class="calibre4">Keep in mind that there are many aspects of processing that occur within data science. The most common are formatting (and reformatting), which involves activities such as mechanically setting data types, aggregating values, reordering or dropping columns, and so on, cleansing (or addressing the quality of the data), which is solving for such things as default or missing values, incomplete or inapposite values, and so on, and profiling, which adds context to the data by creating a statistical understanding of the data.</p>
<p class="calibre4">The processing to be completed on the data can be simple (for example, it can be a very simple and manual event requiring repetitious updates to data in an MS Excel worksheet), or complex (as with the use of programming languages such as R or Python), or even more sophisticated (as when processing logic is coded into routines that can then be scheduled and rerun automatically on new populations of data).</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Exploring and visualizing data</h1>
                
            
            
                
<p class="calibre4">During this phase or step in the overall data science pipeline process, the data scientist will use various methods to dig deeper into the data. Typically, several graphical representations are created (again, either manually or through a programming script or tool) emphasizing or validating a data scientist's observation, a particular point, or belief. This is a significant step in the overall data science process as the data scientist may come to understand that additional processing should be done on the data, or additional data needs to be collected, or perhaps the original theories appear to be validated. These findings will be cause for a pause, reflecting on the next steps that need to be taken. Should the data scientist proceed with the formal analysis process, perhaps creating a predictive model for automated learning? Or, should the scientist revisit a previous step, collecting additional (or different) data for processing?</p>
<div><strong class="calibre3">Data visualization</strong> is a key technique permitting data scientists to perform analyses, identify key trends or events, and make more confident decisions much more quickly.</div>


            

            
        
    </div>



  
<div><h1 class="header-title">Analyzing the data and/or applying machine learning to the data</h1>
                
            
            
                
<p class="calibre4">In this phase, quite a bit of analysis takes place as the data scientist (driven by a high level of scientific curiosity and experience) attempts to shape a story based upon an observation or the interpretation of their understanding of the data (up to this point). The data scientist continues to slice and dice the data, using analytics or BI packages—such as Tableau or Pentaho or an open source solution such as R or Python—to create a concrete data storyline. Once again, based on these analysis results, the data scientist may elect to again go back to a prior phase, pulling new data, processing and reprocessing, and creating additional visualizations. At some point, when appropriate progress has been made, the data scientist may decide that the data is at such point where data analysis can begin. Machine learning (defined further later in this chapter) has evolved over time from being more of an exercise in pattern recognition to now being defined as utilizing a selected statistical method to dig deeper, using the data and results of the analysis of this phase to learn and make a prediction, on the project data.</p>
<p class="calibre4">The ability of a data scientist to extract a quantitative result from data through machine learning and express it as something that everyone (not just other data scientists) can understand immediately is an invaluable skill, and we will talk more about this throughout this book.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Deciding (or planning) based upon acquired insight</h1>
                
            
            
                
<p class="calibre4">In this step, the data scientist hopes to obtain value from their efforts in the form of an insight. The insight is gained by performing the preceding described phases, aimed at gaining an understanding of a particular situation or phenomena. The idea is that this insight can then be used as input to make better decisions.</p>
<p class="calibre4">A fun example that illustrates a creative use of insights mined from data is the (as of this writing, experimental) Roztayger personality match process powered by IBM Watson. Using either your Facebook or Twitter feeds (or you can enter a short bio), Watson will, on-the-fly, perform an analysis of your personality. The results are interesting and pretty spot on, and these insights are then used to suggest designer labels that may best suit you and your personal style.</p>
<p>You can find this feature at <a href="http://roztayger.com/match" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">http://roztayger.com/match</a>. The Personality Insights service extracts personality characteristics based on how a person writes. You can use the service to match individuals to other individuals, opportunities, and products, or tailor their experience with personalized messaging and recommendations. Characteristics include the Big 5 Personality Traits, Values, and Needs. At least 1,200 words of input text are recommended when using this service.</p>
<p class="calibre4">Once the (real-time) data science analysis is complete, the aforementioned website not only provides its recommendations but also shares the data behind its insights, showing an easy-to-understand, well-organized tabular view of the results, and an eye-catching visualization as well, as shown in the following figure:</p>
<div><img class="image-border8" src="img/91dbe2fb-1ece-4522-88c4-0dd8a8d7096b.png"/></div>
<p class="calibre4">This illustrates another key aspect of this phase of the data science progression, that is, once the data scientist identifies an insight, he must clearly present and communicate those data insights/findings.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Thinking like a data scientist</h1>
                
            
            
                
<p class="calibre4">As we've already stressed, agreement on the concepts of what a data scientist is and does are still just emerging. The entire field of data science is at best roughly defined. Transitioning to data science is perhaps as much about finding an organization or group whose needs match your skills as it is about understanding what skills and concepts are involved in data science and then working towards developing those skills.</p>
<p>Just as a data developer stays up to date and knowledgeable on the trends and tools in and around the manipulation of and access to data, so should the would-be data scientist.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Bringing statistics into data science</h1>
                
            
            
                
<p class="calibre4">Depending on your sources and individual beliefs, you may say the following:</p>
<p class="calibre4"><em class="calibre21">Statistics is data science, and data science is statistics</em>.</p>
<p class="calibre4">To clarify this, note that there is a popular opinion that statistics might be thought of as a study or process that covers the collection, analysis, interpretation, presentation, and organization of data. As you can see, that definition is pretty similar to the data science process we described in the previous section of this chapter.</p>
<p class="calibre4">Digging deeper into this topic, one will find that statistics always involves (or a collection of) techniques or approaches used to help analyze and present data (again, this understanding could also be used to describe data science).</p>
<p>It is commonly accepted that the terms data science and statistics have the same meaning, at least within some circles. Again, alignment of terms and concepts is still evolving among data scientists.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Common terminology</h1>
                
            
            
                
<p class="calibre4">Based upon personal experience, research, and various industry experts' advice, someone delving into the art of data science should take every opportunity to understand and gain experience as well as proficiency with the following list of common data science terms:</p>
<ul class="calibre18">
<li class="calibre19">Statistical population</li>
<li class="calibre19">Probability</li>
<li class="calibre19">False positives</li>
<li class="calibre19">Statistical inference</li>
<li class="calibre19">Regression</li>
<li class="calibre19">Fitting</li>
<li class="calibre19">Categorical data</li>
<li class="calibre19">Classification</li>
<li class="calibre19">Clustering</li>
<li class="calibre19">Statistical comparison</li>
<li class="calibre19">Coding</li>
<li class="calibre19">Distributions</li>
<li class="calibre19">Data mining</li>
<li class="calibre19">Decision trees</li>
<li class="calibre19">Machine learning</li>
<li class="calibre19">Munging and wrangling</li>
<li class="calibre19">Visualization</li>
<li class="calibre19">D3</li>
<li class="calibre19">Regularization</li>
<li class="calibre19">Assessment</li>
<li class="calibre19">Cross-validation</li>
<li class="calibre19">Neural networks</li>
<li class="calibre19">Boosting</li>
<li class="calibre19">Lift</li>
<li class="calibre19">Mode</li>
<li class="calibre19">Outlier</li>
<li class="calibre19">Predictive modeling</li>
<li class="calibre19">Big data</li>
<li class="calibre19">Confidence interval</li>
<li class="calibre19">Writing</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Statistical population</h1>
                
            
            
                
<p class="calibre4">You can perhaps think of a statistical population as a recordset (or a set of records). This set or group of records will be of similar items or events that are of interest to the data scientist for some experiment.</p>
<p class="calibre4">For a data developer, a population of data may be a recordset of all sales transactions for a month, and the interest might be reporting to the senior management of an organization which products are the fastest sellers and at which time of the year.</p>
<p class="calibre4">For a data scientist, a population may be a recordset of all emergency room admissions during a month, and the area of interest might be to determine the statistical demographics for emergency room use.</p>
<p>Typically, the terms <strong class="calibre3">statistical population</strong> and <strong class="calibre3">statistical model</strong> are or can be used interchangeably. Once again, data scientists continue to evolve with their alignment on their use of common terms.</p>
<p class="calibre4">Another key point concerning statistical populations is that the recordset may be a group of (actually) existing objects or a hypothetical group of objects. Using the preceding example, you might draw a comparison of actual objects as those actual sales transactions recorded for the month while the hypothetical objects as sales transactions are expected, forecast, or presumed (based upon observations or experienced assumptions or other logic) to occur during a month.</p>
<p class="calibre4">Finally, through the use of statistical inference (explained later in this chapter), the data scientist can select a portion or subset of the recordset (or population) with the intention that it will represent the total population for a particular area of interest. This subset is known as a <strong class="calibre7">statistical sample</strong>.</p>
<p class="calibre4">If a sample of a population is chosen accurately, characteristics of the entire population (that the sample is drawn from) can be estimated from the corresponding characteristics of the sample.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Probability</h1>
                
            
            
                
<p>Probability is concerned with the laws governing random events.<br class="calibre2"/>
                                                                                                 -www.britannica.com</p>
<p class="calibre4">When thinking of probability, you think of possible upcoming events and the likelihood of them actually occurring. This compares to a statistical thought process that involves analyzing the frequency of past events in an attempt to explain or make sense of the observations. In addition, the data scientist will associate various individual events, studying the relationship of these events. How these different events relate to each other governs the methods and rules that will need to be followed when we're studying their probabilities.</p>
<p>A probability distribution is a table that is used to show the probabilities of various outcomes in a sample population or recordset.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">False positives</h1>
                
            
            
                
<p class="calibre4">The idea of false positives is a very important statistical (data science) concept. A false positive is a mistake or an errored result. That is, it is a scenario where the results of a process or experiment indicate a fulfilled or true condition when, in fact, the condition is not true (not fulfilled). This situation is also referred to by some data scientists as a false alarm and is most easily understood by considering the idea of a recordset or statistical population (which we discussed earlier in this section) that is determined not only by the accuracy of the processing but by the characteristics of the sampled population. In other words, the data scientist has made errors during the statistical process, or the recordset is a population that does not have an appropriate sample (or characteristics) for what is being investigated.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Statistical inference</h1>
                
            
            
                
<p class="calibre4">What developer at some point in his or her career, had to create a sample or test data? For example, I've often created a simple script to generate a random number (based upon the number of possible options or choices) and then used that number as the selected option (in my test recordset). This might work well for data development, but with statistics and data science, this is not sufficient.</p>
<p class="calibre4">To create sample data (or a sample population), the data scientist will use a process called <strong class="calibre7">statistical inference</strong>, which is the process of deducing options of an underlying distribution through analysis of the data you have or are trying to generate for. The process is sometimes called <strong class="calibre7">inferential statistical analysis</strong> and includes testing various hypotheses and deriving estimates.</p>
<p class="calibre4">When the data scientist determines that a recordset (or population) should be larger than it actually is, it is assumed that the recordset is a sample from a larger population, and the data scientist will then utilize statistical inference to make up the difference.</p>
<p>The data or recordset in use is referred to by the data scientist as the observed data. Inferential statistics can be contrasted with descriptive statistics, which is only concerned with the properties of the observed data and does not assume that the recordset came from a larger population.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Regression</h1>
                
            
            
                
<p class="calibre4">Regression is a process or method (selected by the data scientist as the best fit technique for the experiment at hand) used for determining the relationships among variables. If you're a programmer, you have a certain understanding of what a variable is, but in statistics, we use the term differently. Variables are determined to be either dependent or independent.</p>
<p class="calibre4">An independent variable (also known as a <strong class="calibre7">predictor</strong>) is the one that is manipulated by the data scientist in an effort to determine its relationship with a dependent variable. A dependent variable is a variable that the data scientist is measuring.</p>
<p>It is not uncommon to have more than one independent variable in a data science progression or experiment.</p>
<p class="calibre4">More precisely, regression is the process that helps the data scientist comprehend how the typical value of the dependent variable (or criterion variable) changes when any one or more of the independent variables is varied while the other independent variables are held fixed.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Fitting</h1>
                
            
            
                
<p class="calibre4">Fitting is the process of measuring how well a statistical model or process describes a data scientist's observations pertaining to a recordset or experiment. These measures will attempt to point out the discrepancy between observed values and probable values. The probable values of a model or process are known as a distribution or a probability distribution.</p>
<p class="calibre4">Therefore, a probability distribution fitting (or distribution fitting) is when the data scientist fits a probability distribution to a series of data concerning the repeated measurement of a variable phenomenon.</p>
<p class="calibre4">The object of a data scientist performing a distribution fitting is to predict the probability or to forecast the frequency of, the occurrence of the phenomenon at a certain interval.</p>
<p>One of the most common uses of fitting is to test whether two samples are drawn from identical distributions.</p>
<p class="calibre4">There are numerous probability distributions a data scientist can select from. Some will fit better to the observed frequency of the data than others will. The distribution giving a close fit is supposed to lead to good predictions; therefore, the data scientist needs to select a distribution that suits the data well.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Categorical data</h1>
                
            
            
                
<p class="calibre4">Earlier, we explained how variables in your data can be either independent or dependent. Another type of variable definition is a categorical variable. This type of variable is one that can take on one of a limited, and typically fixed, number of possible values, thus assigning each individual to a particular category.</p>
<p class="calibre4">Often, the collected data's meaning is unclear. Categorical data is a method that a data scientist can use to put meaning to the data.</p>
<p class="calibre4">For example, if a numeric variable is collected (let's say the values found are 4, 10, and 12), the meaning of the variable becomes clear if the values are categorized. Let's suppose that based upon an analysis of how the data was collected, we can group (or categorize) the data by indicating that this data describes university students, and there is the following number of players:</p>
<ul class="calibre18">
<li class="calibre19">4 tennis players</li>
<li class="calibre19">10 soccer players</li>
<li class="calibre19">12 football players</li>
</ul>
<p class="calibre4">Now, because we grouped the data into categories, the meaning becomes clear.</p>
<p class="calibre4">Some other examples of categorized data might be individual pet preferences (grouped by the type of pet), or vehicle ownership (grouped by the style of a car owned), and so on.</p>
<p class="calibre4">So, categorical data, as the name suggests, is data grouped into some sort of category or multiple categories. Some data scientists refer to categories as sub-populations of data.</p>
<p>Categorical data can also be data that is collected as a yes or no answer. For example, hospital admittance data may indicate that patients either smoke or do not smoke.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Classification</h1>
                
            
            
                
<p class="calibre4">Statistical classification of data is the process of identifying which category (discussed in the previous section) a data point, observation, or variable should be grouped into. The data science process that carries out a classification process is known as a <strong class="calibre7">classifier</strong>.</p>
<p>Determining whether a book is fiction or non-fiction is a simple example classification. An analysis of data about restaurants might lead to the classification of them among several genres.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Clustering</h1>
                
            
            
                
<p class="calibre4">Clustering is the process of dividing up the data occurrences into groups or homogeneous subsets of the dataset, not a predetermined set of groups as in classification (described in the preceding section) but groups identified by the execution of the data science process based upon similarities that it found among the occurrences.</p>
<p class="calibre4">Objects in the same group (a group is also referred to as a cluster) are found to be more analogous (in some sense or another) to each other than to those objects found in other groups (or found in other clusters). The process of clustering is found to be very common in exploratory data mining and is also a common technique for statistical data analysis.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Statistical comparison</h1>
                
            
            
                
<p class="calibre4">Simply put, when you hear the term statistical comparison, one is usually referring to the act of a data scientist performing a process of analysis to view the similarities or variances of two or more groups or populations (or recordsets).</p>
<p class="calibre4">As a data developer, one might be familiar with various utilities such as FC Compare, UltraCompare, or WinDiff, which aim to provide the developer with a line-by-line comparison of the contents of two or more (even binary) files.</p>
<p class="calibre4">In statistics (data science), this process of comparing is a statistical technique to compare populations or recordsets. In this method, a data scientist will conduct what is called an <strong class="calibre7">Analysis of Variance</strong> (<strong class="calibre7">ANOVA</strong>), compare categorical variables (within the recordsets), and so on.</p>
<p>ANOVA is an assortment of statistical methods that are used to analyze the differences among group means and their associated procedures (such as variations among and between groups, populations, or recordsets). This method eventually evolved into the Six Sigma dataset comparisons.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Coding</h1>
                
            
            
                
<p class="calibre4">Coding or statistical coding is again a process that a data scientist will use to prepare data for analysis. In this process, both quantitative data values (such as income or years of education) and qualitative data (such as race or gender) are categorized or coded in a consistent way.</p>
<p class="calibre4">Coding is performed by a data scientist for various reasons such as follows:</p>
<ul class="calibre18">
<li class="calibre19">More effective for running statistical models</li>
<li class="calibre19">Computers understand the variables</li>
<li class="calibre19">Accountability--so the data scientist can run models blind, or without knowing what variables stand for, to reduce programming/author bias</li>
</ul>
<p>You can imagine the process of coding as the means to transform data into a form required for a system or application.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Distributions</h1>
                
            
            
                
<p class="calibre4">The distribution of a statistical recordset (or of a population) is a visualization showing all the possible values (or sometimes referred to as intervals) of the data and how often they occur. When a distribution of categorical data (which we defined earlier in this chapter) is created by a data scientist, it attempts to show the number or percentage of individuals in each group or category.</p>
<p class="calibre4">Linking an earlier defined term with this one, a probability distribution, stated in simple terms, can be thought of as a visualization showing the probability of occurrence of different possible outcomes in an experiment.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Data mining</h1>
                
            
            
                
<p class="calibre4">In <a href="7f3dc6b3-d483-4ffc-b330-22b36da9bdc7.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">Chapter 1</a>, <em class="calibre21">Transitioning from Data Developer to Data Scientist</em>, we said, with data mining, one is usually more absorbed in the data relationships (or the potential relationships between points of data, sometimes referred to as variables) and cognitive analysis.</p>
<p class="calibre4">To further define this term, we can mention that data mining is sometimes more simply referred to as knowledge discovery or even just discovery, based upon processing through or analyzing data from new or different viewpoints and summarizing it into valuable insights that can be used to increase revenue, cuts costs, or both.</p>
<p class="calibre4">Using software dedicated to data mining is just one of several analytical approaches to data mining. Although there are tools dedicated to this purpose (such as IBM Cognos BI and Planning Analytics, Tableau, SAS, and so on.), data mining is all about the analysis process finding correlations or patterns among dozens of fields in the data and that can be effectively accomplished using tools such as MS Excel or any number of open source technologies.</p>
<p>A common technique to data mining is through the creation of custom scripts using tools such as R or Python. In this way, the data scientist has the ability to customize the logic and processing to their exact project needs.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Decision trees</h1>
                
            
            
                
<p class="calibre4">A statistical decision tree uses a diagram that looks like a tree. This structure attempts to represent optional decision paths and a predicted outcome for each path selected. A data scientist will use a decision tree to support, track, and model decision making and their possible consequences, including chance event outcomes, resource costs, and utility. It is a common way to display the logic of a data science process.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Machine learning</h1>
                
            
            
                
<p class="calibre4">Machine learning is one of the most intriguing and exciting areas of data science. It conjures all forms of images around artificial intelligence which includes Neural Networks, <strong class="calibre7">Support Vector Machines</strong> (<strong class="calibre7">SVMs</strong>), and so on.</p>
<p class="calibre4">Fundamentally, we can describe the term machine learning as a method of training a computer to make or improve predictions or behaviors based on data or, specifically, relationships within that data. Continuing, machine learning is a process by which predictions are made based upon recognized patterns identified within data, and additionally, it is the ability to continuously learn from the data's patterns, therefore continuingly making better predictions.</p>
<p class="calibre4">It is not uncommon for someone to mistake the process of machine learning for data mining, but data mining focuses more on exploratory data analysis and is known as <strong class="calibre7">unsupervised learning</strong>.</p>
<p class="calibre4">Machine learning can be used to learn and establish baseline behavioral profiles for various entities and then to find meaningful anomalies.</p>
<p class="calibre4">Here is the exciting part: the process of machine learning (using data relationships to make predictions) is known as <strong class="calibre7">predictive analytics</strong>.</p>
<p class="calibre4">Predictive analytics allow the data scientists to produce reliable, repeatable decisions and results and uncover hidden insights through learning from historical relationships and trends in the data.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Munging and wrangling</h1>
                
            
            
                
<p class="calibre4">The terms <strong class="calibre7">munging</strong> and <strong class="calibre7">wrangling</strong> are buzzwords or jargon meant to describe one's efforts to affect the format of data, recordset, or file in some way in an effort to prepare the data for continued or otherwise processing and/or evaluations.</p>
<p class="calibre4">With data development, you are most likely familiar with the idea of <strong class="calibre7">Extract</strong>, <strong class="calibre7">Transform</strong>, and<strong class="calibre7"> Load</strong> (<strong class="calibre7">ETL</strong>). In somewhat the same way, a data developer may mung or wrangle data during the transformation steps within an ETL process.</p>
<p class="calibre4">Common munging and wrangling may include removing punctuation or HTML tags, data parsing, filtering, all sorts of transforming, mapping, and tying together systems and interfaces that were not specifically designed to interoperate. Munging can also describe the processing or filtering of raw data into another form, allowing for more convenient consumption of the data elsewhere.</p>
<p class="calibre4">Munging and wrangling might be performed multiple times within a data science process and/or at different steps in the evolving process. Sometimes, data scientists use munging to include various data visualization, data aggregation, training a statistical model, as well as much other potential work. To this point, munging and wrangling may follow a flow beginning with extracting the data in a raw form, performing the munging using various logic, and lastly, placing the resulting content into a structure for use.</p>
<p class="calibre4">Although there are many valid options for munging and wrangling data, preprocessing and manipulation, a tool that is popular with many data scientists today is a product named <strong class="calibre7">Trifecta</strong>, which claims that it is the number one (data) wrangling solution in many industries.</p>
<p>Trifecta can be downloaded for your personal evaluation from <a href="https://www.trifacta.com/" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">https://www.trifacta.com/</a>. Check it out!</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Visualization</h1>
                
            
            
                
<p class="calibre4">The main point (although there are other goals and objectives) when leveraging a data visualization technique is to make something complex appear simple. You can think of visualization as any technique for creating a graphic (or similar) to communicate a message.</p>
<p class="calibre4">Other motives for using data visualization include the following:</p>
<ul class="calibre18">
<li class="calibre19">To explain the data or put the data in context (which is to highlight demographical statistics)</li>
<li class="calibre19">To solve a specific problem (for example, identifying problem areas within a particular business model)</li>
<li class="calibre19">To explore the data to reach a better understanding or add clarity (such as what periods of time do this data span?)</li>
<li class="calibre19">To highlight or illustrate otherwise invisible data (such as isolating outliers residing in the data)</li>
<li class="calibre19">To predict, such as potential sales volumes (perhaps based upon seasonality sales statistics)</li>
<li class="calibre19">And others</li>
</ul>
<p class="calibre4">Statistical visualization is used in almost every step in the data science process, within the obvious steps such as exploring and visualizing, analyzing and learning, but can also be leveraged during collecting, processing, and the end game of using the identified insights.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">D3</h1>
                
            
            
                
<p class="calibre4">D3 or <kbd class="calibre22">D3.js</kbd>, is essentially an open source JavaScript library designed with the intention of visualizing data using today's web standards. D3 helps put life into your data, utilizing <strong class="calibre7">Scalable Vector Graphics</strong> (<strong class="calibre7">SVG</strong>), Canvas, and standard HTML.</p>
<p class="calibre4">D3 combines powerful visualization and interaction techniques with a data-driven approach to DOM manipulation, providing data scientists with the full capabilities of modern browsers and the freedom to design the right visual interface that best depicts the objective or assumption.</p>
<p class="calibre4">In contrast to many other libraries, <kbd class="calibre22">D3.js</kbd> allows inordinate control over the visualization of data. D3 is embedded within an HTML webpage and uses prebuilt JavaScript functions to select elements, create SVG objects, style them, or add transitions, dynamic effects, and so on.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Regularization</h1>
                
            
            
                
<p class="calibre4">Regularization is one possible approach that a data scientist may use for improving the results generated from a statistical model or data science process, such as when addressing a case of overfitting in statistics and data science.</p>
<p>We defined fitting earlier in this chapter (fitting describes how well a statistical model or process describes a data scientist's observations). Overfitting is a scenario where a statistical model or process seems to fit too well or appears to be too close to the actual data.</p>
<p class="calibre4">Overfitting usually occurs with an overly simple model. This means that you may have only two variables and are drawing conclusions based on the two. For example, using our previously mentioned example of <em class="calibre21">daffodil sales</em>, one might generate a model with temperature as an independent variable and sales as a dependent one. You may see the model fail since it is not as simple as concluding that warmer temperatures will always generate more sales.</p>
<p class="calibre4">In this example, there is a tendency to add more data to the process or model in hopes of achieving a better result. The idea sounds reasonable. For example, you have information such as average rainfall, pollen count, fertilizer sales, and so on; could these data points be added as explanatory variables?</p>
<p>An explanatory variable is a type of independent variable with a subtle difference. When a variable is independent, it is not affected at all by any other variables. When a variable isn't independent for certain, it's an explanatory variable.</p>
<p class="calibre4">Continuing to add more and more data to your model will have an effect but will probably cause overfitting, resulting in poor predictions since it will closely resemble the data, which is mostly just background noise.</p>
<p class="calibre4">To overcome this situation, a data scientist can use regularization, introducing a tuning parameter (additional factors such as a data points mean value or a minimum or maximum limitation, which gives you the ability to change the complexity or smoothness of your model) into the data science process to solve an ill-posed problem or to prevent overfitting.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Assessment</h1>
                
            
            
                
<p class="calibre4">When a data scientist evaluates a model or data science process for performance, this is referred to as assessment. Performance can be defined in several ways, including the model's growth of learning or the model's ability to improve (with) learning (to obtain a better score) with additional experience (for example, more rounds of training with additional samples of data) or accuracy of its results.</p>
<p class="calibre4">One popular method of assessing a model or processes performance is called <strong class="calibre7">bootstrap sampling</strong>. This method examines performance on certain subsets of data, repeatedly generating results that can be used to calculate an estimate of accuracy (performance).</p>
<p class="calibre4">The bootstrap sampling method takes a random sample of data, splits it into three files--a training file, a testing file, and a validation file. The model or process logic is developed based on the data in the training file and then evaluated (or tested) using the testing file. This tune and then test process is repeated until the data scientist is comfortable with the results of the tests. At that point, the model or process is again tested, this time using the validation file, and the results should provide a true indication of how it will perform.</p>
<p>You can imagine using the bootstrap <kbd class="calibre22">sampling</kbd> method to develop program logic by analyzing test data to determine logic flows and then running (or testing) your logic against the test data file. Once you are satisfied that your logic handles all of the conditions and exceptions found in your testing data, you can run a final test on a new, never-before-seen data file for a final validation test.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Cross-validation</h1>
                
            
            
                
<p class="calibre4">Cross-validation is a method for assessing a data science process performance. Mainly used with predictive modeling to estimate how accurately a model might perform in practice, one might see cross-validation used to check how a model will potentially generalize, in other words, how the model can apply what it infers from samples to an entire population (or recordset).</p>
<p class="calibre4">With cross-validation, you identify a (known) dataset as your validation dataset on which training is run along with a dataset of unknown data (or first seen data) against which the model will be tested (this is known as your <strong class="calibre7">testing dataset</strong>). The objective is to ensure that problems such as overfitting (allowing non-inclusive information to influence results) are controlled and also provide an insight into how the model will generalize a real problem or on a real data file.</p>
<p class="calibre4">The cross-validation process will consist of separating data into samples of similar subsets, performing the analysis on one subset (called the <strong class="calibre7">training set</strong>) and validating the analysis on the other subset (called the <strong class="calibre7">validation set</strong> or <strong class="calibre7">testing set</strong>). To reduce variability, multiple iterations (also called <strong class="calibre7">folds</strong> or <strong class="calibre7">rounds</strong>) of cross-validation are performed using different partitions, and the validation results are averaged over the rounds. Typically, a data scientist will use a models stability to determine the actual number of rounds of cross-validation that should be performed.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Neural networks</h1>
                
            
            
                
<p class="calibre4">Neural networks are also called <strong class="calibre7">artificial neural networks</strong> (<strong class="calibre7">ANNs</strong>), and the objective is to solve problems in the same way that the human brain would.</p>
<p class="calibre4">Google will provide the following explanation of ANN as stated in <em class="calibre21">Neural Network Primer: Part I, by Maureen Caudill, AI Expert, Feb. 1989</em>:</p>
<p>A computing system made up of several simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs.</p>
<p class="calibre4">To oversimplify the idea of neural networks, recall the concept of software encapsulation, and consider a computer program with an input layer, a processing layer, and an output layer. With this thought in mind, understand that neural networks are also organized in a network of these layers, usually with more than a single processing layer.</p>
<p class="calibre4">Patterns are presented to the network by way of the input layer, which then communicates to one (or more) of the processing layers (where the actual processing is done). The processing layers then link to an output layer where the result is presented.</p>
<p class="calibre4">Most neural networks will also contain some form of learning rule that modifies the weights of the connections (in other words, the network learns which processing nodes perform better and gives them a heavier weight) per the input patterns that it is presented with. In this way (in a sense), neural networks learn by example as a child learns to recognize a cat from being exposed to examples of cats.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Boosting</h1>
                
            
            
                
<p class="calibre4">In a manner of speaking, boosting is a process generally accepted in data science for improving the accuracy of a weak learning data science process.</p>
<p>Data science processes defined as weak learners are those that produce results that are only slightly better than if you would randomly guess the outcome. Weak learners are basically thresholds or a 1-level decision tree.</p>
<p class="calibre4">Specifically, boosting is aimed at reducing bias and variance in supervised learning.</p>
<p class="calibre4">What do we mean by bias and variance? Before going on further about boosting, let's take note of what we mean by bias and variance.</p>
<p class="calibre4">Data scientists describe bias as a level of favoritism that is present in the data collection process, resulting in uneven, disingenuous results and can occur in a variety of different ways. A <kbd class="calibre22">sampling</kbd> method is called <strong class="calibre7">biased</strong> if it systematically favors some outcomes over others.</p>
<p class="calibre4">A variance may be defined (by a data scientist) simply as the distance from a variable mean (or how far from the average a result is).</p>
<p class="calibre4">The boosting method can be described as a data scientist repeatedly running through a data science process (that has been identified as a weak learning process), with each iteration running on different and random examples of data sampled from the original population recordset. All the results (or classifiers or residue) produced by each run are then combined into a single merged result (that is a gradient).</p>
<p class="calibre4">This concept of using a random subset of the original recordset for each iteration originates from bootstrap sampling in bagging and has a similar variance-reducing effect on the combined model.</p>
<p class="calibre4">In addition, some data scientists consider boosting a means to convert weak learners into strong ones; in fact, to some, the process of boosting simply means turning a weak learner into a strong learner.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Lift</h1>
                
            
            
                
<p class="calibre4">In data science, the term lift compares the frequency of an observed pattern within a recordset or population with how frequently you might expect to see that same pattern occur within the data by chance or randomly.</p>
<p class="calibre4">If the lift is very low, then typically, a data scientist will expect that there is a very good probability that the pattern identified is occurring just by chance. The larger the lift, the more likely it is that the pattern is real.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Mode</h1>
                
            
            
                
<p class="calibre4">In statistics and data science, when a data scientist uses the term mode, he or she refers to the value that occurs most often within a sample of data. Mode is not calculated but is determined manually or through processing of the data.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Outlier</h1>
                
            
            
                
<p class="calibre4">Outliers can be defined as follows:</p>
<ul class="calibre18">
<li class="calibre19">A data point that is way out of keeping with the others</li>
<li class="calibre19">That piece of data that doesn't fit</li>
<li class="calibre19">Either a very high value or a very low value</li>
<li class="calibre19">Unusual observations within the data</li>
<li class="calibre19">An observation point that is distant from all others</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Predictive modeling</h1>
                
            
            
                
<p class="calibre4">The development of statistical models and/or data science processes to predict future events is called <strong class="calibre7">predictive modeling</strong>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Big Data</h1>
                
            
            
                
<p class="calibre4">Again, we have some variation of the definition of big data. A large assemblage of data, data sets that are so large or complex that traditional data processing applications are inadequate, and data about every aspect of our lives have all been used to define or refer to big data. In 2001, then Gartner analyst Doug Laney introduced the 3V's concept.</p>
<p>You can refer to the link: <a href="http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf&lt;/span&gt;</a></p>
<p class="calibre4">The 3V's, as per Laney, are volume, variety, and velocity. The V's make up the dimensionality of big data: volume (or the measurable amount of data), variety (meaning the number of types of data), and velocity (referring to the speed of processing or dealing with that data).</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Confidence interval</h1>
                
            
            
                
<p class="calibre4">The confidence interval is a range of values that a data scientist will specify around an estimate to indicate their margin of error, combined with a probability that a value will fall in that range. In other words, confidence intervals are good estimates of the unknown population parameter.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Writing</h1>
                
            
            
                
<p class="calibre4">Although visualizations grab much more of the limelight when it comes to presenting the output or results of a data science process or predictive model, writing skills are still not only an important part of how a data scientist communicates but still considered an essential skill for all data scientists to be successful.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p class="calibre4">In this chapter, we said that, currently, how data science is defined is a matter of opinion. A practical explanation is that data science is a progression or, even better, an evolution of thought, consisting of collecting, processing, exploring, and visualizing data, analyzing (data) and/or applying machine learning (to the data), and then deciding (or planning) based upon acquired insight(s).</p>
<p class="calibre4">Then, with the goal of thinking like a data scientist, we introduced and defined a number of common terms and concepts a data scientist should be comfortable with.</p>
<p class="calibre4">In the next chapter, we will present and explain how a data developer might understand and approach the topic of data cleaning using several common statistical methods.</p>
<p class="calibre4"/>
<p class="calibre4"/>
<p class="calibre4"/>


            

            
        
    </div>



  </body></html>