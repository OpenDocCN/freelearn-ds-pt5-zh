["```py\nre.sub(r\"https?\\://\\S+\\s\", '', \"https://www.asfd.com hello world\")\n```", "```py\n    string = \"first think another Disney movie, might good, it's kids \tmovie. watch it, can't help enjoy it. ages love movie. first saw movie 10 8 years later still love it! Danny Glover superb could play part better. Christopher Lloyd hilarious perfect part. Tony Danza believable Mel Clark. can't help, enjoy movie! give 10/10!<br /><br />- review Jamie Robert Ward (http://www.invocus.net)\"\n    ```", "```py\n    len(string)\n    ```", "```py\n    import re\n    string = re.sub(r\"https?\\://\\S+\", '', string)\n    string\n    ```", "```py\n    string = re.sub(r'<br />', ' ', string)\n    string\n    ```", "```py\n    string = re.sub('\\d','', string)\n    string\n    ```", "```py\n    string = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', '', string)\n    string\n    ```", "```py\n    string = re.sub(r\"can\\'t\", \"cannot\", string)\n    string = re.sub(r\"it\\'s\", \"it is\", string)\n    string\n    ```", "```py\n    len(string)\n    ```", "```py\n    re.findall(r\"[A-Z][a-z]*\", string)\n    ```", "```py\n    re.findall(r\"\\b[A-z]{1,2}\\b\", string)\n    ```", "```py\ndata = pd.read_csv('movie_reviews.csv', encoding='latin-1')\n```", "```py\ndata.iloc[0]\n```", "```py\ndata.SentimentText[0]\n```", "```py\ndata['word_count'] = data['SentimentText'].apply(lambda x: len(str(x).split(\" \")))\n```", "```py\ndata.loc[data.Sentiment == 0, 'word_count'].mean()\n```", "```py\ndata.loc[data.Sentiment == 1, 'word_count'].mean()\n```", "```py\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n```", "```py\ndata['stop_count'] = data['SentimentText'].apply(lambda x: len([x for x in x.split() if x in stop]))\n```", "```py\ndata.loc[data.Sentiment == 0, 'stop_count'].mean()\n```", "```py\ndata.loc[data.Sentiment == 1, 'stop_count'].mean()\n```", "```py\ndata['special_count'] = data['SentimentText'].apply(lambda x: len(re.sub('[^\\^&*$@#]+' ,'', x)))\n```", "```py\ndata['SentimentText'] = data['SentimentText'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n```", "```py\ndata['SentimentText'] = data['SentimentText'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n```", "```py\nword_freq = pd.Series(' '.join(data['SentimentText']).split()).value_counts()\nword_freq.head()\n```", "```py\ndata['SentimentText'] = data['SentimentText'].str.replace(r'<br />','')\ndata['SentimentText'] = data['SentimentText'].apply(lambda x: \" \".join(x for x in x.split() if x not in ['movie', 'film']))\n```", "```py\npunc_special = r\"[^A-Za-z0-9\\s]+\"\ndata['SentimentText'] = data['SentimentText'].str.replace(punc_special,'')\n```", "```py\nfrom autocorrect import spell\ndata['SentimentText'] = [' '.join([spell(i) for i in x.split()]) for x in data['SentimentText']]\n```", "```py\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\ndata['SentimentText'] = data['SentimentText'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\n```", "```py\nlemmatizer = nltk.stem.WordNetLemmatizer()\ndata['SentimentText'][:5].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()])) \n```", "```py\nimport nltk\nnltk.word_tokenize(\"Hello Dr. Ajay. It's nice to meet you.\")\n```", "```py\n    import pandas as pd\n    data = pd.read_csv('../../chapter 7/data/movie_reviews.csv', \tencoding='latin-1')\n    ```", "```py\n    data.SentimentText = data.SentimentText.str.lower()\n    ```", "```py\n    import re\n    def clean_str(string):\n         string = re.sub(r\"https?\\://\\S+\", '', string)\n         string = re.sub(r'\\<a href', ' ', string)\n         string = re.sub(r'&amp;', 'and', string) \n         string = re.sub(r'<br />', ' ', string)\n         string = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', string)\n         string = re.sub('\\d','', string)\n         string = re.sub(r\"can\\'t\", \"cannot\", string)\n         string = re.sub(r\"it\\'s\", \"it is\", string)\n         return string\n    ```", "```py\n    data.SentimentText = data.SentimentText.apply(lambda x: clean_str(str(x)))\n    ```", "```py\n    pd.Series(' '.join(data['SentimentText']).split()).value_counts().head(10)\n    ```", "```py\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize,sent_tokenize\n    stop_words = stopwords.words('english') + ['movie', 'film', 'time']\n    stop_words = set(stop_words)\n    remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]\n    data['SentimentText'] = data['SentimentText'].apply(remove_stop_words)\n    ```", "```py\n    def combine_text(text):    \n        try:\n            return ' '.join(text[0])\n        except:\n            return np.nan\n    data.SentimentText = data.SentimentText.apply(lambda x: combine_text(x))\n    data = data.dropna(how='any')\n    ```", "```py\n    from keras.preprocessing.text import Tokenizer\n    tokenizer = Tokenizer(num_words=250)\n    tokenizer.fit_on_texts(list(data['SentimentText']))\n    sequences = tokenizer.texts_to_sequences(data['SentimentText'])\n    ```", "```py\n    word_index = tokenizer.word_index\n    print('Found %s unique tokens.' % len(word_index))\n    ```", "```py\n    from keras.preprocessing.sequence import pad_sequences\n    reviews = pad_sequences(sequences, maxlen=200)\n    ```", "```py\n    import pickle\n    with open('tokenizer.pkl', 'wb') as handle:\n                pickle.dump(tokenizer, \n                            handle, \n                            protocol=pickle.HIGHEST_PROTOCOL)\n    ```", "```py\n    data.SentimentText[124]\n    ```", "```py\nreviews[124]\n```", "```py\nmodel = gensim.models.Word2Vec(\n        tokens,\n        iter=5\n        size=100,\n        window=5,\n        min_count=5,\n        workers=10,\n        sg=0)\n```", "```py\nvocab = list(model.wv.vocab)\nlen(vocab)\n```", "```py\nmodel.wv.most_similar('fun')\n```", "```py\nmodel.wv.save_word2vec_format('movie_embedding.txt', binary=False)\n```", "```py\ndef load_embedding(filename, word_index , num_words, embedding_dim):\n    embeddings_index = {}\n    file = open(filename, encoding=\"utf-8\")\n    for line in file:\n        values = line.split()\n        word = values[0]\n        coef = np.asarray(values[1:])\n        embeddings_index[word] = coef\n    file.close()\n\n    embedding_matrix = np.zeros((num_words, embedding_dim))\n    for word, pos in word_index.items():\n        if pos >= num_words:\n            continue\n        print(num_words)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[pos] = embedding_vector\n    return embedding_matrix\n```", "```py\n    data['SentimentText'] [0]\n    ```", "```py\n    data['SentimentText'] = data['SentimentText'].apply(lambda x: x[0])\n    ```", "```py\n    from gensim.models import Word2Vec\n    model = Word2Vec(\n            data['SentimentText'],\n            iter=50,\n            size=100,\n            window=5,\n            min_count=5,\n            workers=10)\n    ```", "```py\n    model.wv.most_similar('insight')\n    ```", "```py\n    model.wv.similarity(w1='violent', w2='brutal')\n    ```", "```py\n    from sklearn.decomposition import PCA\n    word_limit = 200\n    X = model[model.wv.vocab][: word_limit]\n    pca = PCA(n_components=2)\n    result = pca.fit_transform(X)\n    ```", "```py\n    import matplotlib.pyplot as plt\n    plt.scatter(result[:, 0], result[:, 1])\n    words = list(model.wv.vocab)[: word_limit]\n    for i, word in enumerate(words):\n        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n    plt.show()\n    ```", "```py\n    model.wv.save_word2vec_format('movie_embedding.txt', binary=False)\n    ```", "```py\nmodel.add(keras.layers.LSTM(units, activation='tanh', dropout=0.0, recurrent_dropout=0.0, return_sequences=False))\n```", "```py\n    import pandas as pd\n    data = pd.read_csv('../../chapter 7/data/movie_reviews.csv', encoding='latin-1')\n    ```", "```py\n    data.text = data.text.str.lower()\n    ```", "```py\n    import re\n    def clean_str(string):\n\n        string = re.sub(r\"https?\\://\\S+\", '', string)\n        string = re.sub(r'\\<a href', ' ', string)\n        string = re.sub(r'&amp;', '', string) \n        string = re.sub(r'<br />', ' ', string)\n        string = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', string)\n        string = re.sub('\\d','', string)\n        string = re.sub(r\"can\\'t\", \"cannot\", string)\n        string = re.sub(r\"it\\'s\", \"it is\", string)\n        return string\n    data.SentimentText = data.SentimentText.apply(lambda x: clean_str(str(x)))\n    ```", "```py\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize,sent_tokenize\n    stop_words = stopwords.words('english') + ['movie', 'film', 'time']\n    stop_words = set(stop_words)\n    remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]\n    data['SentimentText'] = data['SentimentText'].apply(remove_stop_words)\n    ```", "```py\n    def combine_text(text):    \n        try:\n            return ' '.join(text[0])\n        except:\n            return np.nan\n    data.SentimentText = data.SentimentText.apply(lambda x: combine_text(x))\n    data = data.dropna(how='any')\n    ```", "```py\n    from keras.preprocessing.text import Tokenizer\n    tokenizer = Tokenizer(num_words=5000)\n    tokenizer.fit_on_texts(list(data['SentimentText']))\n    sequences = tokenizer.texts_to_sequences(data['SentimentText'])\n    word_index = tokenizer.word_index\n    ```", "```py\n    from keras.preprocessing.sequence import pad_sequences\n    reviews = pad_sequences(sequences, maxlen=100)\n    ```", "```py\n    import numpy as np\n    def load_embedding(filename, word_index , num_words, embedding_dim):\n        embeddings_index = {}\n        file = open(filename, encoding=\"utf-8\")\n        for line in file:\n            values = line.split()\n            word = values[0]\n            coef = np.asarray(values[1:])\n            embeddings_index[word] = coef\n        file.close()\n\n        embedding_matrix = np.zeros((num_words, embedding_dim))\n        for word, pos in word_index.items():\n            if pos >= num_words:\n                continue\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[pos] = embedding_vector\n        return embedding_matrix\n    embedding_matrix = load_embedding('movie_embedding.txt', word_index, len(word_index), 16)\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(reviews, pd.get_dummies(data.Sentiment), test_size=0.2, random_state=9)\n    ```", "```py\n    from keras.models import Model\n    from keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten, LSTM\n    inp = Input((100,))\n    embedding_layer = Embedding(len(word_index),\n                        16,\n                        weights=[embedding_matrix],\n                        input_length=100,\n                        trainable=False)(inp)\n    model = Dropout(0.10)(embedding_layer)\n    model = LSTM(128, dropout=0.2)(model)\n    model = Dense(units=256, activation='relu')(model)\n    model = Dense(units=64, activation='relu')(model)\n    model = Dropout(0.3)(model)\n    predictions = Dense(units=2, activation='softmax')(model)\n    model = Model(inputs = inp, outputs = predictions)\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = ['acc'])\n    ```", "```py\n    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=256)\n    ```", "```py\n    from sklearn.metrics import accuracy_score\n    preds = model.predict(X_test)\n    accuracy_score(np.argmax(preds, 1), np.argmax(y_test.values, 1))\n    ```", "```py\n    y_actual = pd.Series(np.argmax(y_test.values, axis=1), name='Actual')\n    y_pred = pd.Series(np.argmax(preds, axis=1), name='Predicted')\n    pd.crosstab(y_actual, y_pred, margins=True)\n    ```", "```py\n    review_num = 110\n    print(\"Review: \\n\"+tokenizer.sequences_to_texts([X_test[review_num]])[0])\n    sentiment = \"Positive\" if np.argmax(preds[review_num]) else \"Negative\"\n    print(\"\\nPredicted sentiment = \"+ sentiment)\n    sentiment = \"Positive\" if np.argmax(y_test.values[review_num]) else \"Negative\"\n    print(\"\\nActual sentiment = \"+ sentiment)\n    ```"]