- en: Machine Learning Deep Dive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The prior chapter on machine learning provided a preliminary overview of the
    subject, including the different classes and core concepts in the subject area.
    This chapter will delve deeper into the theoretical aspects of machine learning
    such as the limits of algorithms and how different algorithms work.
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine learning** is a vast and complex subject, and to that end, this chapter
    focuses on the breadth of different topics, rather than the depth. The concepts
    are introduced at a high level and the reader may refer to other sources to further
    their understanding of the topics.'
  prefs: []
  type: TYPE_NORMAL
- en: We will start out by discussing a few fundamental theories in machine learning,
    such as Gradient Descent and VC Dimension. Next, we will look at Bias and Variance,
    two of the most important factors in any modelling process and the concept of
    bias-variance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: We'll then discuss the various machine learning algorithms, their strengths
    and areas of applications.
  prefs: []
  type: TYPE_NORMAL
- en: We'll conclude with exercises that leverage real-world datasets to perform machine
    learning operations using R.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The bias, variance, and regularization properties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient descent and VC dimension theories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tutorial: Machine learning with R'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bias, variance, and regularization properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bias, variance, and the closely related topic of regularization hold very special
    and fundamental positions in the field of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Bias happens when a machine learning model is too 'simple', leading to results
    that are consistently off from the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: Variance happens when a model is too 'complex', leading to results that are
    very accurate on test datasets, but do not perform well on unseen/new datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Once users become familiar with the process of creating machine learning models,
    it would seem that the process is quite simplistic - get the data, create a training
    set and a test set, create a model, apply the model on the test dataset, and the
    exercise is complete. Creating models is easy; creating a *good* model is a much
    more challenging topic. But how can one test the quality of a model? And, perhaps
    more importantly, how does one go about building a 'good' model?
  prefs: []
  type: TYPE_NORMAL
- en: The answer lies in a term called regularization. It's arguably a fancy word,
    but all it means is that during the process of creating a model, one benefits
    from penalizing an overly impressive performance on a training dataset and relaxing
    the same on a poorly performing model.
  prefs: []
  type: TYPE_NORMAL
- en: To understand regularization, it would help to know the concepts of overfitting
    and underfitting. For this, let us look at a simple but familiar example of drawing
    lines of best fit. For those who have used Microsoft Excel, you may have noticed
    the option to draw the *line of best fit* - in essence, given a set of points,
    you can draw a line that represents the data and approximates the function that
    the points represent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the prices vs square footage of a few properties.
    In order to determine the relationship between house prices and the size of the
    house, we can draw a line of best fit, or a trend line, as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sq. ft.** | **Price ($)** |'
  prefs: []
  type: TYPE_TB
- en: '| 862 | 170,982 |'
  prefs: []
  type: TYPE_TB
- en: '| 1235 | 227,932 |'
  prefs: []
  type: TYPE_TB
- en: '| 932 | 183,280 |'
  prefs: []
  type: TYPE_TB
- en: '| 1624 | 237,945 |'
  prefs: []
  type: TYPE_TB
- en: '| 1757 | 275,921 |'
  prefs: []
  type: TYPE_TB
- en: '| **1630** | 274,713 |'
  prefs: []
  type: TYPE_TB
- en: '| **1236** | 201,428 |'
  prefs: []
  type: TYPE_TB
- en: '| **1002** | 193,128 |'
  prefs: []
  type: TYPE_TB
- en: '| **1118** | 187,073 |'
  prefs: []
  type: TYPE_TB
- en: '| **1339** | 202,422 |'
  prefs: []
  type: TYPE_TB
- en: '| **1753** | 283,989 |'
  prefs: []
  type: TYPE_TB
- en: '| **1239** | 228,170 |'
  prefs: []
  type: TYPE_TB
- en: '| **1364** | 230,662 |'
  prefs: []
  type: TYPE_TB
- en: '| **995** | 169,369 |'
  prefs: []
  type: TYPE_TB
- en: '| **1000** | 157,305 |'
  prefs: []
  type: TYPE_TB
- en: 'If we were to draw a *line of best* *fit* using a linear trend line, the chart
    would look somewhat like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c352bb42-90f4-43a0-ab7a-05d53e2916ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Excel provides an useful additional feature that allows users to draw an extension
    of the trend line which can provide an estimate, or a *prediction*, of unknown
    variables. In this case, extending the trendline will show us, based on the function,
    what the prices for houses in the 1,800-2,000 sq. ft. range are likely to be.
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear function that describes the data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y=126.13x + 54,466.81*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following chart with an extended trend line shows that the price is most
    likely between `$275,000` and `$300,000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70b446ec-c433-4589-8aea-731806451a1c.png)'
  prefs: []
  type: TYPE_IMG
- en: However, one may argue that the line is not the best approximation and that
    it may be possible to increase the value of R2, which in this case is 0.87\. In
    general, the higher the R^2, the better the model that describes the data. There
    are various different types of *R²* values, but for the purpose of this section,
    we'll assume that the higher the *R²*, the better the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will draw a new trend line that has a much higher R^2,
    but using a polynomial function. This function has a higher R^2 (0.91 vs 0.87)
    and visually appears to be closer to the points on average.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function in this case is a 6^(th)-order polynomial:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = -0.00x⁶ + 0.00x⁵ - 0.00x⁴ + 2.50x³ - 2,313.40x² + 1,125,401.77x - 224,923,813.17*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98b77763-7482-4c42-9e49-62c7c75e2021.png)'
  prefs: []
  type: TYPE_IMG
- en: But, even though the line has a higher R^2, if we extend the trend line, intending
    to find what the prices of houses in the 1,800-2,000 sq. ft. range are likely
    to be, we get the following result.
  prefs: []
  type: TYPE_NORMAL
- en: Houses in the 1,800-2,000 sq. ft. range go from approx. $280,000 to negative
    $2 million at the 2,000^(th) sq. ft. In other words, people purchasing houses
    with 1800 sq. ft. are expected to spend $ 280,000 and those purchasing houses
    with 2,000 sq. ft. should, according to this function, with a 'higher R^2', receive
    $2 million! This, of course, is not accurate, but what we have just witnessed
    is what is known as **over-fitting**. The image below illustrates this phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f39c591-a89e-499e-921c-900f60f31753.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At the other end of the spectrum is **under-fitting**. This happens when the
    model built does not describe the data. In the following chart, the function y
    = 0.25x - 200 is one such example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60b80a9d-cace-4bbb-ac95-ed3eac84763b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In brief, this section can be abbreviated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A function that fits the data too well, such that the function can approximate
    nearly all of the points in the training dataset is considered overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that does not fit the data at all, or in other words is far from
    the actual points in the training dataset, is considered underfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning is the process of balancing between overfitting and underfitting
    the data. This is arguably not an easy exercise, which is why even though building
    a model may be trivial, building a model that is reasonably good is a much more
    difficult challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underfitting is when your function is *not thinking at all* - it has a high
    bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting is when your function is *thinking too hard* - it has a high variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another example for underfitting and overfitting is given in coming example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Say we are tasked with determining if a bunch of fruit are oranges or apples,
    and have been given their location in a fruit basket (left-side or right-side),
    size and weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![](img/be16688e-a356-4b63-923b-d9e25b01abb1.png) | ![](img/f6035a45-cd01-4de4-be71-f428126da2b1.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Basket 1 (Training Dataset)** | **Basket 2 (Test Dataset)** |'
  prefs: []
  type: TYPE_TB
- en: An example of overfitting could be that, based on the training dataset, with
    regard to Basket 1 we could conclude that the only fruits located on the right
    hand side of the basket are oranges and those on the left are all apples.
  prefs: []
  type: TYPE_NORMAL
- en: An example of underfitting could be that I conclude that the basket has only
    oranges.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model 1**: In the first case - for overfitting - I have, in essence, memorized
    the locations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model 2**: In the second case - for underfitting - I could not remember anything
    precisely at all.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, given a second basket - the test dataset where the positions of the apples
    and oranges are switched - if I were to use Model 1, I would incorrectly conclude
    that all the fruits on the right hand side are oranges and those on the left hand
    side are apples (since I memorized the training data).
  prefs: []
  type: TYPE_NORMAL
- en: If I were to use Model 2, I would, again, incorrectly conclude that all the
    fruits are oranges.
  prefs: []
  type: TYPE_NORMAL
- en: There are, however, ways to manage the balance between underfitting and overfitting
    - or in other words, between high bias and high variance.
  prefs: []
  type: TYPE_NORMAL
- en: One of the methods commonly used for bias-variance trade-off is known as regularization.
    This refers to the process of penalizing the model (for example, the model's coefficients
    in a regression) in order to produce an output that generalizes well across a
    range of data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table on the next page illustrates some of the key concepts of bias and
    variance and illustrates options for remedial steps when a model has high bias
    or high variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fd11d61-930f-4e67-a737-7ba72e754400.png)'
  prefs: []
  type: TYPE_IMG
- en: In terms of the modeling process, a high bias is generally indicated by the
    fact that both the training set error as well as the test set error remain consistently
    high. For high variance (overfitting), the training set error decreases rapidly,
    but the test set error remains unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient descent and VC Dimension theories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient descent and VC Dimension are two fundamental theories in machine learning.
    In general, **gradient descent** gives a structured approach to finding the optimal
    co-efficients of a function. The hypothesis space of a function can be large and
    with gradient descent, the algorithm tries to find a minimum (*a minima*) where
    the cost function (for example, the squared sum of errors) is the lowest.
  prefs: []
  type: TYPE_NORMAL
- en: '**VC Dimension** provides an upper bound on the maximum number of points that
    can be classified in a system. It is in essence the measure of the richness of
    a function and provides an assessment of what the limits of a hypothesis are in
    a structured way. The number of points that can be exactly classified by a function
    or hypothesis is known as the VC Dimension of the hypothesis. For example, a linear
    boundary can accurately classify 2 or 3 points but not 4\. Hence, the VC Dimension
    of this 2-dimensional space would be 3.'
  prefs: []
  type: TYPE_NORMAL
- en: VC Dimension, like many other topics in computational learning theory, is both
    complex and interesting. It is a lesser known (and discussed) topic, but one that
    has a profound implication as it attempts to answer questions about what the limits
    of learning can be.
  prefs: []
  type: TYPE_NORMAL
- en: Popular machine learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are various different classes of machine learning algorithms. As such,
    since algorithms can belong to multiple 'classes' or categories at the same time
    at a conceptual level, it is hard to specifically state that an algorithm belongs
    exclusively to a single class. In this section, we will briefly discuss a few
    of the most commonly used and well-known algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'These include:'
  prefs: []
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that in the examples, we have shown the basic use of the R functions using
    the entire dataset. In practice, we'd split the data into a training and test
    set, and once we have built a satisfactory model apply the same on the test dataset
    to evaluate the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression models range from commonly used linear, logistic, and multiple regression
    algorithms used in statistics to Ridge and Lasso regression, which penalizes co-efficients
    to improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: In our earlier examples, we saw the application of **linear regression** when
    we created trend-lines. **Multiple linear regression** refers to the fact that
    the process of creating the model requires multiple independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Total Advertising Cost = x* Print Ads**, would be a simple linear regression;
    whereas'
  prefs: []
  type: TYPE_NORMAL
- en: '**Total Advertising Cost = X + Print Ads + Radio Ads + TV Ads**, due to the
    presence of more than one independent variable (Print, Radio, and TV), would be
    a multiple linear regression.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logistic regression** is another commonly used statistical regression modelling
    technique that predicts the outcome of a discrete categorical value, mainly for
    cases where the outcome variable is dichotomous (for example, 0 or 1, Yes or No,
    and so on). There can, however, be more than 2 discrete outcomes (for example,
    State NY, NJ, CT) and this type of logistic regression is known as **multinomial
    logistic regression**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ridge and Lasso Regressions** include a regularization term (λ) in addition
    to the other aspects of Linear Regression. The regularization term, Ridge Regression,
    has the effect of reducing the β coefficients (thus ''penalizing'' the co-efficients).
    In Lasso, the regularization term tends to reduce some of the co-efficients to
    0, thus eliminating the effect of the variable on the final model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0734ce4-09ff-4e4f-9ae6-150d20324b53.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Association rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Association rules mining, or **apriori**, attempts to find relationships between
    variables in a dataset. Association rules are frequently used for various practical
    real-world use cases. Given a set of variables, apriori can indicate the patterns
    inherent in a transactional dataset. One of our tutorials will be based on implementing
    an R Shiny Application for apriori and hence, more emphasis is being provided
    for the same in this section.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's say a supermarket chain is deciding the order for placing
    items on the shelves. An apriori algorithm run against a database containing sales
    transactions would identify the items that, say, are most often bought together.
    This permits the supermarket to determine which items, when placed strategically
    in close proximity to one another, can yield the most sales. This is also often
    referred to as *market basket analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple example that reflects this could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In all these cases, the act of purchasing something on the left-hand side led
    to the purchase of the item mentioned on the right-hand side of the expression.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to derive association rules from databases that do not necessarily
    contain *transactions*, but instead use a sliding window to go through events
    along a temporal attribute, such as with the WINEPI algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 3 primary measures in apriori. To illustrate them, let us use a sample
    dataset containing items purchased in 4 separate transactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Transaction** | **Item 1** | **Item 2** | **Item 3** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Milk | Bread | Butter |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Milk | Egg | Butter |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Bread | Egg | Cheese |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Butter | Bread | Egg |'
  prefs: []
  type: TYPE_TB
- en: Confidence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Confidence refers to how often the right-hand side of the apriori expression
    is valid when the left-hand side is valid. For instance, given an expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We would like to know how often Bread was purchased *when Milk was also purchased*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transaction 1**: Milk and Bread are both present'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transaction 2**: Milk is present, but not Bread'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transactions 3 and 4**: Milk is not present'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, based on the what we saw, there were 2 transactions where Milk was present
    and of them, Bread was present in 1 transaction. Hence, the confidence for the
    rule {Milk} à {Bread} would be ½ = 50%
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking another expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We would like to know, when Bread was purchased, how often was Butter also
    purchased?:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transaction 1**: Bread and Butter are both present'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transaction 2**: There is no Bread (Butter is present, but our point of reference
    is Bread and hence this does not count)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transaction 3**: Bread is present but no Butter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transaction 4**: Bread and Butter are both present'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, we have Bread in 3 of the transactions, and Bread & Butter in 2 of the
    3 transactions. Hence, in this case, the 'confidence' of the rule `{Bread} à {Butter}`
    is *2/3 = 66.7*.
  prefs: []
  type: TYPE_NORMAL
- en: Support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Support refers to the number of times the rule is satisfied relative to the
    total number of transactions in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '{Milk} --> {Bread}, occurs in 1 out of 4 Transactions (in Transaction 1). Hence,
    the support for this rule is ¼ = 0.25 (or 25%).'
  prefs: []
  type: TYPE_NORMAL
- en: '{Bread} --> {Butter}, occurs in 2 out of 4 Transactions (in Transaction 1 and
    4). Hence, the support for this rule is ½ = 0.50 (or 50%).'
  prefs: []
  type: TYPE_NORMAL
- en: Lift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lift is arguably the most important of the 3 measures; it measures the support
    of the rule relative to the support of the individual sides of the expression;
    put differently, it measures how strong the rule is with respect to a random occurrence
    of the LHS and RHS of the expression. It is formally defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Lift = Support (Rule)/(Support(LHS) * Support (RHS))*'
  prefs: []
  type: TYPE_NORMAL
- en: A low value for lift (say, less than or equal to 1) indicates that the LHS and
    RHS occurrence are independent of one another, whereas a higher lift measure indicates
    that the co-occurrence is significant.
  prefs: []
  type: TYPE_NORMAL
- en: In our prior example,
  prefs: []
  type: TYPE_NORMAL
- en: '{Bread} --> {Butter} has a lift of:'
  prefs: []
  type: TYPE_NORMAL
- en: Support ({Bread} --> {Butter})
  prefs: []
  type: TYPE_NORMAL
- en: Support {Bread} * Support {Butter}
  prefs: []
  type: TYPE_NORMAL
- en: = 0.50/((3/4) * (3/4)) = 0.50/(0.75 * 0.75) = 0.89.
  prefs: []
  type: TYPE_NORMAL
- en: This indicates that although the Confidence of the rule was high, the rule in
    and of itself is not significant relative to other rules that may be higher than
    1.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a rule with a Lift higher than 1 would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '{Item 1: Bread} --> {Item 3: Cheese}'
  prefs: []
  type: TYPE_NORMAL
- en: 'This has a Lift of:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Support {Item 1: Bread --> Item 3: Cheese}/(Support {Item 1: Cheese} * Support
    {Item 3: Cheese})'
  prefs: []
  type: TYPE_NORMAL
- en: = (1/4)/((1/4)*(1/4) = 4.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision Trees are a predictive modeling technique that generates rules that
    derive the likelihood of a certain outcome based on the likelihood of the preceding
    outcomes. In general, decision trees are typically constructed similar to a **flowchart**,
    with a series of nodes and leaves that denote a parent-child relationship. Nodes
    that do not link to other nodes are known as leaves.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees belong to a class of algorithms that are often known as **CART**
    (**Classification and Regression Trees**). If the outcome of interest is a categorical
    variable, it falls under a classification exercise, whereas if the outcome is
    a number, it is known as a regression tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example will help to make this concept clearer. Take a look at the chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ac42b3c-4ec1-4a83-8c52-0e51853d030b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The chart shows a hypothetical scenario: if school is closed/not closed. The
    rectangular boxes (in blue) represent the nodes. The first rectangle (School Closed)
    represent the *root* node, whereas the inner rectangles represent the *internal*
    nodes. The rectangular boxes with angled edges (in green and italic letters) represent
    the ''*leaves*'' (or *terminal* nodes).'
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees are simple to understand and one of the few algorithms that are
    not a 'black box'. Algorithms such as those used to create Neural Networks are
    often considered black boxes, as it is very hard - if not impossible - to intuitively
    determine the exact path by which a final outcome was reached due to the complexity
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In R, there are various facilities for creating Decision Trees. A commonly used
    library for creating them in R is `rpart`. We'll revisit our `PimaIndiansDiabetes`
    dataset to see how a decision tree can be created using the package.
  prefs: []
  type: TYPE_NORMAL
- en: We would like to create a model to determine how glucose, insulin, (body) mass,
    and age are related to diabetes. Note that in the dataset, diabetes is a categorical
    variable with a yes/no response.
  prefs: []
  type: TYPE_NORMAL
- en: 'For visualizing the decision tree, we will use the `rpart.plot` package. The
    code for the same is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/27b2b5d4-4128-4690-8f2c-328712a01cfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Reading from the top, the graph shows that that there are 500 cases of `diabetes=neg`
    in the dataset (out of a total of 768 records).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Of the total number of records in the dataset (768) with value of glucose <
    128, there were 485 records marked as negative. Of these, the model correctly
    predicted 391 cases as negative (Node Number 2, the first one on the left from
    the bottom).
  prefs: []
  type: TYPE_NORMAL
- en: For the records which had a glucose reading of > 128, there were 283 records
    marked as positive (Node Number 3, the node immediately below the topmost/root
    node). The model correctly classified 174 of these cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another, more recent package for intuitive decision trees with comprehensive
    visual information is **FFTrees** (**Fast and Frugal Decision Trees**). The following
    example has been provided for informational purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c984b3e7-7aa5-4ff3-a53e-d2b4d2eb1943.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision Trees work by splitting the data recursively until a stopping criterion,
    such as when a certain depth has been reached, or the number of cases, is below
    a specified value. Each split is done based on the variable that will lead to
    a 'purer subset'.
  prefs: []
  type: TYPE_NORMAL
- en: In principle, we can grow an endless number of trees from a given set of variables,
    which makes it a particularly hard and intractable problem. Numerous algorithms
    exist which provide an efficient method for splitting and creating decision trees.
    One such method is Hunt's Algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further details about the algorithm can be found at: [https://www-users.cs.umn.edu/~kumar/dmbook/ch4.pdf](https://www-users.cs.umn.edu/~kumar/dmbook/ch4.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: The Random forest extension
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forest is an extension of the decision tree model that we just discussed.
    In practice, Decision Trees are simple to understand, simple to interpret, fast
    to create using available algorithms, and overall, intuitive. However, Decision
    Trees are sensitive to small changes in the data, permit splits only along an
    axis (linear splits) and can lead to overfitting. To mitigate some of the drawbacks
    of decision trees, whilst still getting the benefit of their elegance, algorithms
    such as Random Forest create multiple decision trees and sample random features
    to leverage and build an aggregate model.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest works on the principle of **bootstrap aggregating** or **bagging**.
    Bootstrap is a statistical term indicating random sampling with replacement. Bootstrapping
    a given set of records means taking a random number of records and possibly including
    the same record multiple times in a sample. Thereafter, the user would measure
    their metric of interest on the sample and then repeat the process. In this manner,
    the distribution of the values of the metric calculated from random samples multiple
    times is expected to represent the distribution of the population, and so the
    entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of Bagging a set of 3 numbers, such as (1,2,3,4), would be:'
  prefs: []
  type: TYPE_NORMAL
- en: (1,2,3), (1,1,3), (1,3,3), (2,2,1), and others.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrap Aggregating, or *bagging*, implies leveraging a voting method using
    *multiple bootstrap samples* at a time, building a model on each individual sample
    (set of n records) and then finally aggregating the results.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests also implement another level of operation beyond simple bagging.
    It also randomly selects the variables to be included in the model building process
    at each split. For instance, if we were to create a random forest model using
    the `PimaIndiansDiabetes` dataset with the variables pregnant, glucose, pressure,
    triceps, insulin, mass, pedigree, age, and diabetes, in each bootstrap sample
    (draw of n records), we would select a random subset of features with which to
    build the model--for instance, glucose, pressure, and insulin; insulin, age, and
    pedigree; triceps, mass, and insulin; and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, the package commonly used for RandomForest is called by its namesake,
    RandomForest. We can use it via the package as is or via caret. Both methods are
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Random Forest using the RandomForest package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Using Random Forest via caret using the `method="rf"` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to see the splits and other related information in each
    tree of the original Random Forest model (which did not use caret). This can be
    done using the `getTree` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Boosting algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting is a technique that uses weights and a set of *weak learners*, such
    as decision trees, in order to improve model performance. Boosting assigns weights
    to data based on model misclassification and future learner's (created during
    the boosting machine learning process) focus on the misclassified examples. Examples
    that were correctly classified will be reassigned new weights which will generally
    be lower than those that were not correctly classified. The weight can be based
    on a cost function, such as a majority vote, using subsets of the data.
  prefs: []
  type: TYPE_NORMAL
- en: In simple and non-technical terms, boosting uses *a series of weak learners,
    and each learner 'learns' from the mistakes of the prior learners*.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting is generally more popular compared to bagging as it assigns weights
    relative to model performance rather than assigning equal weights to all data
    points as in bagging. This is conceptually similar to the difference between a
    weighted average versus an average function with no weighting criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several packages in R for boosting algorithms and some of the commonly
    used ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Adaboost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GBM** (**Stochastic Gradient Boosting**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of these, XGBoost is a widely popular machine learning package that has been
    used very successfully in competitive machine learning platforms such as Kaggle.
    XGBoost has a very elegant and computationally efficient way to creating ensemble
    models. Because it is both accurate and extremely fast, users have often used
    XGBoost for compute-intensive ML challenges. You can learn more about Kaggle at
    [http://www.kaggle.com](http://www.kaggle.com).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Support vector machines, commonly known as **SVMs**, are another class of machine
    learning algorithm that are used to classify data into one or another category
    using a concept called **hyperplane**, which is used to demarcate a linear boundary
    between points.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, given a set of black and white points on an x-y axis, we can
    find multiple lines that will separate them. The line, in this case, represents
    the function that delineates the category that each point belongs to. In the following
    image, lines H1 and H2 both separate the points accurately. In this case, how
    can we determine which one of H1 and H2 would be the optimal line?:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/086d5f41-630d-4146-b5bc-34957e49f501.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively, we can say the line that is closest to the points - for instance,
    the vertical line H1 - might *not* be the optimal line to separate the points.
    Since the line is too close to the points, and so too specific to the points on
    the given dataset, a new point may be misclassified if it is even slightly off
    to the right or the left side of the line. In other words, the line is too sensitive
    to small changes in the data (which could be due to stochastic/deterministic noise,
    such as imperfections in the data).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the line H2 manages to separate the data whilst maintaining
    the maximum possible distance from the points closest to the line. Slight imperfections
    in the data are unlikely to affect the classification of the points to the extent
    line H1 may have done. This, in essence, describes the principle of the maximum
    margin of separation as shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/6f17c69e-6d57-4497-a95a-c8b36d1cdb19.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: The points close to the line, also known as the hyperplane, are known as the
    'support vectors' (hence the name). In the image, the points that lie on the dashed
    line are therefore the support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the real world, however, not all points may be ''linearly separable''. SVMs
    leverage a concept known as the ''kernel trick''. In essence, points that might
    not be linearly separable can be projected or mapped onto a higher dimensional
    surface. For example, given a set of points on a 2D x-y space that are not linearly
    separable, it may be possible to separate them if we were to project the points
    on a 3-dimensional space as shown in the following image. The points colored in
    red were not separable by a 2D line, but when mapped to a 3-dimensional surface,
    they can be separated by a hyperplane as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10a74760-3d89-464f-b9c2-685247b84ba6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are several packages in R that let users leverage SVM, such as `kernlab`,
    `e1071`, `klaR`, and others. Here, we illustrate the use of SVM from the `e1071`
    package, as shown as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/469a697c-7c5c-457e-a232-e330b01b3e66.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The K-Means machine learning technique
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-Means is one of the most popular unsupervised machine learning techniques
    that is used to create clusters, and so categorizes data.
  prefs: []
  type: TYPE_NORMAL
- en: 'An intuitive example could be posed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Say a university was offering a new course on American History and Asian History.
    The university maintains a 15:1 student-teacher ratio, so there is 1 teacher per
    15 students. It has conducted a survey which contains a 10-point numeric score
    that was assigned by each student to their preference of studying American History
    or Asian History.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the in-built K-Means algorithm in R to create 2 clusters and presumably,
    by the number of points in each cluster, it may be possible to get an estimate
    of the number of students who may sign up for each course. The code for the same
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image could provide an intuitive estimate of the number of students
    who may sign up for each course (and thereby determine how many teachers may be
    required):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33e06eed-9895-473c-a410-34899e68a800.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are several variations of the K-Means algorithm, but the standard and
    the most commonly used one is Lloyd''s Algorithm. The algorithm steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a set of n points (say in an x-y axis), in order to find k clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Select k points at random from the dataset to represent the mid-points for k
    clusters (say, the *initial centroids*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The distance from each of the other points to the selected k points (representing
    k clusters) is measured and assigned to the cluster that has the lowest distance
    from the point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster centers are recalculated as the mean of the points in the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The distance between the centroids and all the other points are again calculated
    as in Step 2 and new centroids are calculated as in Step 3\. In this manner, Steps
    2 and 3 are repeated until no new data is re-assigned.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Various *distance and similarity measures* exist for clustering, such as **Euclidean
    Distance** (straight-line distance), **Cosine Similarity** (Cosine of angles between
    vectors), **Hamming Distance** (generally used for categorical variables), **Mahalanobis
    Distance** (named after P.C. Mahalanobis; this measures the distance between a
    point and the mean of a distribution), and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the optimal number of clusters cannot always be unambiguously identified,
    there are various methods that attempt to find an estimate. In general, clusters
    can be measured by how close points within a cluster are to one another (within
    cluster variance, such as the sum of squares--WSS) and how far apart the clusters
    are (so higher distances between clusters would make the clusters more readily
    distinguishable). One such method that is used to determine the optimal number
    is known as the **elbow method**. The following chart illustrates the concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0a29271-8d0e-4b07-9d74-bd3c2dc982c5.png)'
  prefs: []
  type: TYPE_IMG
- en: The chart shows a plot of the WSS (within the cluster sum of squares that we're
    seeking to minimize) versus the number of clusters. As is evident, increasing
    the number of clusters from 1 to 2 decreases the WSS value substantially. The
    value for WSS decreases rapidly up until the 4^(th) or 5^(th) cluster, when adding
    more clusters does not lead to a significant improvement in WSS. By visual assessment,
    the machine learning practitioner can conclude that the ideal number of clusters
    that can be created is between 3-5, based on the image.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a low WSS score is not enough to determine the optimal number of clusters.
    It has to be done by inspecting the improvement in the metric. The WSS will eventually
    reduce to 0 when each point becomes an independent cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The neural networks related algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural Network related algorithms have existed for many decades. The first computational
    model was described by Warren McCulloch and Walter Pitts in 1943 in the Bulletin
    of Mathematical Biophysics.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about these concepts at [https://pdfs.semanticscholar.org/5272/8a99829792c3272043842455f3a110e841b1.pdf](https://pdfs.semanticscholar.org/5272/8a99829792c3272043842455f3a110e841b1.pdf)
    and [https://en.wikipedia.org/wiki/Artificial_neural_network](https://en.wikipedia.org/wiki/Artificial_neural_network).
  prefs: []
  type: TYPE_NORMAL
- en: Various man-made objects in the physical world, such as aeroplanes, have drawn
    inspiration from nature. A neural network is in essence a representation of the
    phenomenon of data exchange between the axons and dendrons (also known as dendrites)
    of neurons in the *human nervous system*. Just as data passes between one neuron
    to multiple other neurons to make complex decisions, an artificial neural network
    in similar ways creates a network of neurons that receive input from other neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, an artificial neural network consists of 4 main components:'
  prefs: []
  type: TYPE_NORMAL
- en: Input Layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden Layer(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output Layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nodes and Weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is depicted in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1664ecf7-b0e7-4a0b-bcca-457fa1f07629.png)'
  prefs: []
  type: TYPE_IMG
- en: Each node in the diagram produces an output based on the input from the preceding
    layer. The output is produced using an **activation function**. There are various
    types of activation functions and the output produced depends on the type of function
    used. Examples include binary step (0 or 1), tanh (between -1 and +1), sigmoid,
    and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a84f0aef-960d-4e7a-b280-d68dd85b9e11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The values x1 and x2 are the inputs, w1 and w2 represent the weights, and the
    node represents the point at which the inputs and their weights are evaluated
    and a specific output is produced by the activation function. The output f can
    thus be represented by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/889c62d8-9180-48a5-bfe3-06c3cdcd33ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, f represents the activation function, and b represents the bias term.
    The bias term is independent of the weights and the input values and allows the
    user to shift the output to achieve a better model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks with multiple hidden layers (generally 2 or more) are computationally
    intensive, and in recent days, neural networks with multiple hidden layers, also
    known as deep neural networks or more generally deep learning, have become immensely
    popular.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of the developments in the industry, driven by machine learning and artificial
    intelligence, have been the direct result of the implementation of such multi-layer
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, the package `nnet` provides a readily usable interface to neural networks.
    Although in practice, neural networks generally require sophisticated hardware,
    GPU cards, and so on for illustration purposes, we have leveraged the `nnet` package
    to run the earlier classification exercise on the `PimaIndiansDiabetes` dataset.
    In the example, we will leverage caret in order to execute the `nnet` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c6baa443-294b-4514-8dcb-38bb030d0fbf.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Tutorial - associative rules mining with CMS data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This tutorial will implement an interface for accessing rules created using
    the Apriori Package in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be downloading data from the CMS OpenPayments website. The site hosts
    data on payments made to physicians and hospitals by companies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f21aafc2-9e34-4665-865d-54eda8ee3aad.png)'
  prefs: []
  type: TYPE_IMG
- en: The site provides various ways of downloading data. Users can select the dataset
    of interest and download it manually. In our case, we will download the data using
    one of the Web-based APIs that is available to all users.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset can be downloaded either at the Unix terminal (in the virtual machine)
    or by accessing the site directly from the browser. If you are downloading the
    dataset in the Virtual Machine, run the following command in the terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, if you are downloading the data from a browser, enter the following
    URL in the browser window and hit *Enter*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://openpaymentsdata.cms.gov/resource/vq63-hu5i.csv?$query=select Physician_First_Name
    as firstName,Physician_Last_Name as lastName,Recipient_City as city,Recipient_State
    as state,Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name as company,Total_Amount_of_Payment_USDollars
    as payment,Nature_of_Payment_or_Transfer_of_Value as paymentNature,Product_Category_or_Therapeutic_Area_1
    as category,Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_1 as product
    where covered_recipient_type like "Covered Recipient Physician" and Recipient_State
    like "NY"](https://openpaymentsdata.cms.gov/resource/vq63-hu5i.csv?%24query=select%20Physician_First_Name%20as%20firstName,Physician_Last_Name%20as%20lastName,Recipient_City%20as%20city,Recipient_State%20as%20state,Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name%20as%20company,Total_Amount_of_Payment_USDollars%20as%20payment,Nature_of_Payment_or_Transfer_of_Value%20as%20paymentNature,Product_Category_or_Therapeutic_Area_1%20as%20category,Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_1%20as%20product%20where%20covered_recipient_type%20like%20%22Covered%20Recipient%20Physician%22%20and%20Recipient_State%20like%20%22NY%22)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f67c791-65e6-4e7c-aca8-8011b4e0eb33.png)'
  prefs: []
  type: TYPE_IMG
- en: Writing the R code for Apriori
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Apriori algorithm, as explained earlier, allows users to find relationships
    or patterns inherent in a dataset. For this, we will use the arules package in
    R/RStudio. The code will read the dataset downloaded (called `cms2016_2.csv` in
    the example) and run the apriori algorithm in order to find associative rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new R file in RStudio and enter the following code. Make sure that
    you change the location of the csv file that you downloaded to the appropriate
    directory where the file has been stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Shiny (R Code)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In RStudio, select File | New File | Shiny Web App:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0044242-690a-47a1-92bd-7c09da18d9bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Enter the following code in `app.R`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The following image shows the code being copied and saved in a file called `app.R`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd2894d0-c621-4d9e-b169-450f2408f273.png)'
  prefs: []
  type: TYPE_IMG
- en: Using custom CSS and fonts for the application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our application, we will use a custom CSS File. We will also use custom
    fonts in order to give the application a nice look-and-feel.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the custom CSS File from the software repository for this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CSS, Fonts, and other related files should be stored in a folder called
    `www` in the directory where you created the R Shiny Application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/429dc22c-98bc-473a-89dc-7961b7811cc9.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If all goes well, you should be now able to run the application by clicking
    on the Run App option on the top of the page, as shown in the following images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebc58a2c-7459-439d-9364-d22e13b55c10.png)'
  prefs: []
  type: TYPE_IMG
- en: Upon clicking the "Run" button, the user will see a popup window similar to
    the one shown below. Note that popups should be enabled in the browser for this
    to function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa672f9b-88b0-4b6b-8da1-970452c79e7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The app has multiple controls, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Search LHS/RHS**: Enter any test that you want to filter for, in the Left-Hand
    Side or the Right-Hand Side of the rule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support**: Indicates the prevalence of the rule in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confidence**: Of the rules, how many were exact matches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lift**: Variable defining the importance of a rule. Numbers above 1 are considered
    significant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use this app for any other rules file as long as they are processed
    in a way similar to the one outlined before in the R Script section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning practitioners are often of the opinion that creating models
    is easy, but creating a good one is much more difficult. Indeed, not only is creating
    a *good* model important, but perhaps more importantly, knowing how to identify
    a *good* model is what distinguishes successful versus less successful Machine
    Learning endeavors.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we read up on some of the deeper theoretical concepts in Machine
    Learning. Bias, Variance, Regularization, and other common concepts were explained
    with examples as and where needed. With accompanying R code, we also learnt about
    some of the common machine learning algorithms such as Random Forest, Support
    Vector Machines, and others. We concluded with a tutorial on how to create an
    exhaustive web-based application for Association Rules Mining against CMS OpenPayments
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will read about some of the technologies that are being
    used in enterprises for both big data as well as machine learning. We will also
    discuss the merits of cloud computing and how they are influencing the selection
    of enterprise software and hardware stacks.
  prefs: []
  type: TYPE_NORMAL
