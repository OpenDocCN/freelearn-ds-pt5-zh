<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Start Working with Spark â€“ REPL and RDDs</h1>
                
            
            
                
<p>"All this modern technology just makes people try to do everything at once."</p>
<p class="cdpalignright">- Bill Watterson</p>
<p class="mce-root">In this chapter, you will learn how Spark works; then, you will be introduced to RDDs, the basic abstractions behind Apache Spark, and you'll learn that they are simply distributed collections exposing Scala-like APIs. You will then see how to download Spark and how to make it run locally via the Spark shell.</p>
<p class="mce-root">In a nutshell, the following topics will be covered in this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Dig deeper into Apache Spark</li>
<li class="mce-root1">Apache Spark installation</li>
<li class="mce-root1">Introduction to RDDs</li>
<li class="mce-root1">Using the Spark shell</li>
<li class="mce-root1">Actions and Transformations</li>
<li class="mce-root1">Caching</li>
<li class="mce-root1">Loading and Saving data</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Dig deeper into Apache Spark</h1>
                
            
            
                
<p class="mce-root">Apache Spark is a fast in-memory data processing engine with elegant and expressive development APIs to allow data workers to efficiently execute streaming machine learning or SQL workloads that require fast interactive access to datasets. Apache Spark consists of Spark core and a set of libraries. The core is the distributed execution engine and the Java, Scala, and Python APIs offer a platform for distributed application development.</p>
<p class="mce-root">Additional libraries built on top of the core allow the workloads for streaming, SQL, Graph processing, and machine learning. SparkML, for instance, is designed for Data science and its abstraction makes Data science easier.</p>
<p class="mce-root">In order to plan and carry out the distributed computations, Spark uses the concept of a job, which is executed across the worker nodes using Stages and Tasks. Spark consists of a driver, which orchestrates the execution across a cluster of worker nodes. The driver is also responsible for tracking all the worker nodes as well as the work currently being performed by each of the worker nodes.</p>
<p class="mce-root">Let's look into the various components a little more. The key components are the Driver and the Executors which are all JVM processes (Java processes):</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Driver</strong>: The Driver program contains the applications, main program. If you are using the Spark shell, that becomes the Driver program and the Driver launches the executors across the cluster and also controls the task executions.</li>
<li class="mce-root1"><strong class="calibre1">Executor</strong>: Next are the executors which are processes running on the worker nodes in your cluster. Inside the executor, the individual tasks or computations are run. There could be one or more executors in each worker node and, similarly, there could be multiple tasks inside each executor. When Driver connects to the cluster manager, the cluster manager assigns resources to run executors.</li>
</ul>
<p>The cluster manager could be a standalone cluster manager, YARN, or Mesos.</p>
<p class="mce-root">The <strong class="calibre1">Cluster Manager</strong> is responsible for the scheduling and allocation of resources across the compute nodes forming the cluster. Typically, this is done by having a manager process which knows and manages a cluster of resources and allocates the resources to a requesting process such as Spark. We will look at the three different cluster managers: standalone, YARN, and Mesos further down in the next sections.</p>
<p class="mce-root">The following is how Spark works at a high level:</p>
<div><img class="image-border47" src="img/00292.jpeg"/></div>
<p class="mce-root">The main entry point to a Spark program is called the <kbd class="calibre11">SparkContext</kbd>. The <kbd class="calibre11">SparkContext</kbd> is inside the <strong class="calibre1">Driver</strong> component and represents the connection to the cluster along with the code to run the scheduler and task distribution and orchestration.</p>
<p>In Spark 2.x, a new variable called <kbd class="calibre22">SparkSession</kbd> has been introduced. <kbd class="calibre22">SparkContext</kbd>, <kbd class="calibre22">SQLContext</kbd>, and <kbd class="calibre22">HiveContext</kbd> are now member variables of the <kbd class="calibre22">SparkSession</kbd>.</p>
<p class="mce-root">When you start the <strong class="calibre1">Driver</strong> program, the commands are issued to the cluster using the <kbd class="calibre11">SparkContext</kbd>, and then the <strong class="calibre1">executors</strong> will execute the instructions. Once the execution is completed, the <strong class="calibre1">Driver</strong> program completes the job. You can, at this point, issue more commands and execute more Jobs.</p>
<p>The ability to maintain and reuse the <kbd class="calibre22">SparkContext</kbd> is a key advantage of the Apache Spark architecture, unlike the Hadoop framework where every <kbd class="calibre22">MapReduce</kbd> job or Hive query or Pig Script starts entire processing from scratch for each task we want to execute that too using expensive disk instead of memory.</p>
<p class="mce-root">The <kbd class="calibre11">SparkContext</kbd> can be used to create RDDs, accumulators, and broadcast variables on the cluster. Only one <kbd class="calibre11">SparkContext</kbd> may be active per JVM/Java process. You must <kbd class="calibre11">stop()</kbd> the active <kbd class="calibre11">SparkContext</kbd> before creating a new one.</p>
<p class="mce-root">The <strong class="calibre1">Driver</strong> parses the code, and serializes the byte level code across to the executors to be executed. When we perform any computations, the computations will actually be done at the local level by each node, using in-memory processing.</p>
<p class="mce-root">The process of parsing the code and planning the execution is the key aspect implemented by the <strong class="calibre1">Driver</strong> process.</p>
<p class="mce-root">The following is how Spark <strong class="calibre1">Driver</strong> coordinates the computations across the cluster:</p>
<div><img class="image-border48" src="img/00298.jpeg"/></div>
<p class="mce-root">The <strong class="calibre1">Directed Acyclic Graph</strong> (<strong class="calibre1">DAG</strong>) is the secret sauce of Spark framework. The <strong class="calibre1">Driver</strong> process creates a DAG of tasks for a piece of code you try to run using the distributed processing framework. Then, the DAG is actually executed in stages and tasks by the task scheduler by communicating with the <strong class="calibre1">Cluster Manager</strong> for resources to run the executors. A DAG represents a job, and a job is split into subsets, also called stages, and each stage is executed as tasks using one core per task.</p>
<p class="mce-root">An illustration of a simple job and how the DAG is split into stages and tasks is shown in the following two illustrations; the first one shows the job itself, and the second diagram shows the stages in the job and the tasks:</p>
<div><img class="image-border49" src="img/00301.jpeg"/></div>
<p class="mce-root">The following diagram now breaks down the job/DAG into stages and tasks:</p>
<div><img class="image-border50" src="img/00304.jpeg"/></div>
<p class="mce-root">The number of stages and what the stages consist of is determined by the kind of operations. Usually, any transformation comes into the same stage as the one before, but every operation such as reduce or shuffle always creates a new stage of execution. Tasks are part of a stage and are directly related to the cores executing the operations on the executors.</p>
<p>If you use YARN or Mesos as the cluster manager, you can use dynamic YARN scheduler to increase the number of executors when more work needs to be done, as well as killing idle executors.</p>
<p class="mce-root">The driver, hence, manages the fault tolerance of the entire execution process. Once the job is completed by the Driver, the output can be written to a file, database, or simply to the console.</p>
<p>Remember that the code in the Driver program itself has to be completely serializable including all the variables and objects.<br class="calibre23"/>
The often seen exception is a not a serializable exception, which is a result of including global variables from outside the block.</p>
<p class="mce-root">Hence, the Driver process takes care of the entire execution process while monitoring and managing the resources used, such as executors, stages, and tasks, making sure everything is working as planned and recovering from failures such as task failures on executor nodes or entire executor nodes as a whole.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Apache Spark installation</h1>
                
            
            
                
<p class="mce-root">Apache Spark is a cross-platform framework, which can be deployed on Linux, Windows, and a Mac Machine as long as we have Java installed on the machine. In this section, we will look at how to install Apache Spark.</p>
<p>Apache Spark can be downloaded from <a href="http://spark.apache.org/downloads.html" class="calibre21">http://spark.apache.org/downloads.html</a></p>
<p class="cdpalignleft1">First, let's look at the pre-requisites that must be available on the machine:</p>
<ul class="calibre9">
<li class="mce-root1">Java 8+ (mandatory as all Spark software runs as JVM processes)</li>
<li class="mce-root1">Python 3.4+ (optional and used only when you want to use PySpark)</li>
<li class="mce-root1">R 3.1+ (optional and used only when you want to use SparkR)</li>
<li class="mce-root1">Scala 2.11+ (optional and used only to write programs for Spark)</li>
</ul>
<p class="cdpalignleft1">Spark can be deployed in three primary deployment modes, which we will look at:</p>
<ul class="calibre9">
<li class="mce-root1">Spark standalone</li>
<li class="mce-root1">Spark on YARN</li>
<li class="mce-root1">Spark on Mesos</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Spark standalone</h1>
                
            
            
                
<p class="mce-root">Spark standalone uses a built-in scheduler without depending on any external scheduler such as YARN or Mesos. To install Spark in standalone mode, you have to copy the spark binary install package onto all the machines in the cluster.</p>
<p class="mce-root">In standalone mode, the client can interact with the cluster, either through spark-submit or Spark shell. In either case, the Driver communicates with the Spark master Node to get the worker nodes, where executors can be started for this application.</p>
<p>Multiple clients interacting with the cluster create their own executors on the Worker Nodes. Also, each client will have its own Driver component.</p>
<p class="cdpalignleft1">The following is the standalone deployment of Spark using Master node and worker nodes:</p>
<div><img class="image-border51" src="img/00307.jpeg"/></div>
<p class="cdpalignleft1">Let's now download and install Spark in standalone mode using a Linux/Mac:</p>
<ol class="calibre14">
<li value="1" class="mce-root1">Download Apache Spark from the link <a href="http://spark.apache.org/downloads.html" class="calibre10">http://spark.apache.org/downloads.html</a>:</li>
</ol>
<div><img src="img/00313.jpeg" class="calibre31"/></div>
<ol start="2" class="calibre14">
<li value="2" class="mce-root1">Extract the package in your local directory:</li>
</ol>
<pre class="calibre19">
<strong class="calibre1">      tar -xvzf spark-2.2.0-bin-hadoop2.7.tgz</strong>
</pre>
<ol start="3" class="calibre14">
<li value="3" class="mce-root1">Change directory to the newly created directory:</li>
</ol>
<pre class="calibre19">
<strong class="calibre1">      cd spark-2.2.0-bin-hadoop2.7</strong>
</pre>
<ol start="4" class="calibre14">
<li value="4" class="mce-root1">Set environment variables for <kbd class="calibre11">JAVA_HOME</kbd> and <kbd class="calibre11">SPARK_HOME</kbd> by implementing the following steps:
<ol class="calibre16">
<li value="1" class="mce-root1"><kbd class="calibre11">JAVA_HOME</kbd> should be where you have Java installed. On my Mac terminal, this is set as:</li>
</ol>
</li>
</ol>
<pre class="calibre19">
<strong class="calibre1">            export JAVA_HOME=/Library/Java/JavaVirtualMachines/<br class="title-page-name"/>                             jdk1.8.0_65.jdk/Contents/Home/</strong>
</pre>
<ol start="4" class="calibre14">
<li value="4" class="calibre15">
<ol start="2" class="calibre16">
<li value="2" class="mce-root1"><kbd class="calibre11">SPARK_HOME</kbd> should be the newly extracted folder. On my Mac terminal, this is set as:</li>
</ol>
</li>
</ol>
<pre class="calibre19">
<strong class="calibre1">            export SPARK_HOME= /Users/myuser/spark-2.2.0-bin-<br class="title-page-name"/>                               hadoop2.7</strong>
</pre>
<ol start="5" class="calibre14">
<li value="5" class="mce-root1">Run Spark shell to see if this works. If it does not work, check the <kbd class="calibre11">JAVA_HOME</kbd> and <kbd class="calibre11">SPARK_HOME</kbd> environment variable: <kbd class="calibre11">./bin/spark-shell</kbd></li>
<li value="6" class="mce-root1">You will now see the shell as shown in the following:
<p class="calibre32"><img class="image-border52" src="img/00316.jpeg"/></p>
</li>
<li value="7" class="mce-root1">You will see the Scala/ Spark shell at the end and now you are ready to interact with the Spark cluster:</li>
</ol>
<pre class="calibre19">
<strong class="calibre1">      scala&gt;</strong>
</pre>
<p class="mce-root">Now, we have a Spark-shell connected to an automatically setup local cluster running Spark. This is the quickest way to launch Spark on a local machine. However, you can still control the workers/executors as well as connect to any cluster (standalone/YARN/Mesos). This is the power of Spark, enabling you to quickly move from interactive testing to testing on a cluster and subsequently deploying your jobs on a large cluster. The seamless integration offers a lot of benefits, which you cannot realize using Hadoop and other technologies.</p>
<p>You can refer to the official documentation in case you want to understand all the settings <a href="http://spark.apache.org/docs/latest/" class="calibre21">http://spark.apache.org/docs/latest/</a>.</p>
<p class="mce-root">There are several ways to start the Spark shell as in the following snippet. We will see more options in a later section, showing Spark shell in more detail.:</p>
<ul class="calibre9">
<li class="mce-root1">Default shell on local machine automatically picks local machine as master:</li>
</ul>
<pre class="calibre19">
<strong class="calibre1">    ./bin/spark-shell</strong>
</pre>
<ul class="calibre9">
<li class="mce-root1">Default shell on local machine specifying local machine as master with <kbd class="calibre11">n</kbd> threads:</li>
</ul>
<pre class="calibre19">
<strong class="calibre1">    ./bin/spark-shell --master local[n]</strong>
</pre>
<ul class="calibre9">
<li class="mce-root1">Default shell on local machine connecting to a specified spark master:</li>
</ul>
<pre class="calibre19">
<strong class="calibre1">    ./bin/spark-shell --master spark://&lt;IP&gt;:&lt;Port&gt;</strong>
</pre>
<ul class="calibre9">
<li class="mce-root1">Default shell on local machine connecting to a YARN cluster using client mode:</li>
</ul>
<pre class="calibre19">
<strong class="calibre1">    ./bin/spark-shell --master yarn --deploy-mode client</strong>
</pre>
<ul class="calibre9">
<li class="mce-root1">Default shell on local machine connecting to a YARN cluster using cluster mode:</li>
</ul>
<pre class="calibre19">
<strong class="calibre1">    ./bin/spark-shell --master yarn --deploy-mode cluster</strong>
</pre>
<p class="mce-root">Spark Driver also has a Web UI, which helps you to understand everything about the Spark cluster, the executors running, the jobs and tasks, environment variables, and cache. The most important use, of course, is to monitor the jobs.</p>
<p>Launch the Web UI for the local Spark cluster at <kbd class="calibre22">http://127.0.0.1:4040/jobs/</kbd></p>
<p class="mce-root">The following is the Jobs tab in the Web UI:</p>
<div><img class="image-border53" src="img/00322.jpeg"/></div>
<p class="mce-root">The following is the tab showing all the executors of the cluster:</p>
<div><img class="image-border54" src="img/00200.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Spark on YARN</h1>
                
            
            
                
<p class="mce-root">In YARN mode, the client communicates with YARN resource manager and gets containers to run the Spark execution. You can regard it as something like a mini Spark-cluster deployed just for you.</p>
<p>Multiple clients interacting with the cluster create their own executors on the cluster nodes (node managers). Also, each client will have its own Driver component.</p>
<p class="mce-root">When running using YARN, Spark can run either in YARN-client mode or YARN-cluster mode.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">YARN client mode</h1>
                
            
            
                
<p class="mce-root">In YARN client mode, the Driver runs on a node outside the cluster (typically where the client is). Driver first contacts the resource manager requesting resources to run the Spark job. The resource manager allocates a container (container zero) and responds to the Driver. The Driver then launches the Spark application master in the container zero. The Spark application master then creates the executors on the containers allocated by the resource manager. The YARN containers can be on any node in the cluster controlled by node manager. So, all allocations are managed by resource manager.</p>
<div><p class="mce-root">Even the Spark application master needs to talk to resource manager to get subsequent containers to launch executors.</p>
<p class="mce-root">The following is the YARN-client mode deployment of Spark:</p>
</div>
<div><img class="image-border55" src="img/00203.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">YARN cluster mode</h1>
                
            
            
                
<div><p class="mce-root">In the YARN cluster mode, the Driver runs on a node inside the cluster (typically where the application master is). Client first contacts the resource manager requesting resources to run the Spark job. The resource manager allocates a container (container zero) and responds to the client. The client then submits the code to the cluster and then launches the Driver and Spark application master in the container zero. The Driver runs along with the application master and the Spark application master, and then creates the executors on the containers allocated by the resource manager. The YARN containers can be on any node in the cluster controlled by the node manager. So, all allocations are managed by the resource manager.</p>
<div><p class="mce-root">Even the Spark application master needs to talk to the resource manager to get subsequent containers to launch executors.</p>
</div>
</div>
<div><p class="mce-root">The following is the Yarn-cluster mode deployment of Spark:</p>
</div>
<div><img class="image-border56" src="img/00206.jpeg"/></div>
<p>There is no shell mode in YARN cluster mode, since the Driver itself is running inside YARN.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Spark on Mesos</h1>
                
            
            
                
<p class="cdpalignleft1">Mesos deployment is similar to Spark standalone mode and the Driver communicates with the Mesos Master, which then allocates the resources needed to run the executors. As seen in standalone mode, the Driver then communicates with the executors to run the job. Thus, the Driver in Mesos deployment first talks to the master and then secures the container's request on all the Mesos slave nodes.</p>
<p class="cdpalignleft1">When the containers are allocated to the Spark job, the Driver then gets the executors started up and then runs the code in the executors. When the Spark job is completed and Driver exits, the Mesos master is notified, and all the resources in the form of containers on the Mesos slave nodes are reclaimed.</p>
<p>Multiple clients interacting with the cluster create their own executors on the slave nodes. Also, each client will have its own Driver component. Both client and cluster mode are possible just like YARN mode</p>
<div><p class="cdpalignleft1">The following is the mesos-based deployment of Spark depicting the <strong class="calibre1">Driver</strong> connecting to <strong class="calibre1">Mesos Master Node</strong>, which also has the cluster manager of all the resources on all the Mesos slaves:</p>
</div>
<div><img class="image-border57" src="img/00209.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Introduction to RDDs</h1>
                
            
            
                
<p class="mce-root">A <strong class="calibre1">Resilient Distributed Dataset</strong> (<strong class="calibre1">RDD</strong>) is an immutable, distributed collection of objects. Spark RDDs are resilient or fault tolerant, which enables Spark to recover the RDD in the face of failures. Immutability makes the RDDs read-only once created. Transformations allow operations on the RDD to create a new RDD but the original RDD is never modified once created. This makes RDDs immune to race conditions and other synchronization problems.</p>
<p class="mce-root">The distributed nature of the RDDs works because an RDD only contains a reference to the data, whereas the actual data is contained within partitions across the nodes in the cluster.</p>
<p>Conceptually, a RDD is a distributed collection of elements spread out across multiple nodes in the cluster. We can simplify a RDD to better understand by thinking of a RDD as a large array of integers distributed across machines.</p>
<p class="mce-root">A RDD is actually a dataset that has been partitioned across the cluster and the partitioned data could be from <strong class="calibre1">HDFS</strong> (<strong class="calibre1">Hadoop Distributed File System</strong>), HBase table, Cassandra table, Amazon S3.</p>
<p class="mce-root">Internally, each RDD is characterized by five main properties:</p>
<ul class="calibre9">
<li class="mce-root1">A list of partitions</li>
<li class="mce-root1">A function for computing each split</li>
<li class="mce-root1">A list of dependencies on other RDDs</li>
<li class="mce-root1">Optionally, a partitioner for key-value RDDs (for example, to say that the RDD is hash partitioned)</li>
<li class="mce-root1">Optionally, a list of preferred locations to compute each split on (for example, block locations for an HDFS file)</li>
</ul>
<p class="mce-root">Take a look at the following diagram:</p>
<div><img class="image-border58" src="img/00212.jpeg"/></div>
<p class="mce-root">Within your program, the driver treats the RDD object as a handle to the distributed data. It is analogous to a pointer to the data, rather than the actual data used, to reach the actual data when it is required.</p>
<p class="mce-root">The RDD by default uses the hash partitioner to partition the data across the cluster. The number of partitions is independent of the number of nodes in the cluster. It could very well happen that a single node in the cluster has several partitions of data. The number of partitions of data that exist is entirely dependent on how many nodes your cluster has and the size of the data. If you look at the execution of tasks on the nodes, then a task running on an executor on the worker node could be processing the data which is available on the same local node or a remote node. This is called the locality of the data, and the executing task chooses the most local data possible.</p>
<p>The locality affects the performance of your job significantly. The order of preference of locality by default can be shown as<br class="calibre23"/>
<kbd class="calibre22">PROCESS_LOCAL &gt; NODE_LOCAL &gt; NO_PREF &gt; RACK_LOCAL &gt; ANY</kbd></p>
<p class="mce-root">There is no guarantee of how many partitions a node might get. This affects the processing efficiency of any executor, because if you have too many partitions on a single node processing multiple partitions, then the time taken to process all the partitions also grows, overloading the cores on the executor, and thus slowing down the entire stage of processing, which directly slows down the entire job. In fact, partitioning is one of the main tuning factors to improve the performance of a Spark job. Refer to the following command:</p>
<pre class="calibre19">
<strong class="calibre1">class RDD[T: ClassTag]</strong>
</pre>
<p class="mce-root">Let's look further into what an RDD will look like when we load data. The following is an example of how Spark uses different workers to load different partitions or splits of the data:</p>
<div><img class="image-border59" src="img/00218.jpeg"/></div>
<p class="mce-root">No matter how the RDD is created, the initial RDD is typically called the base RDD and any subsequent RDDs created by the various operations are part of the lineage of the RDDs. This is another very important aspect to remember, as the secret to fault tolerance and recovery is that the <strong class="calibre1">Driver</strong> maintains the lineage of the RDDs and can execute the lineage to recover any lost blocks of the RDDs.</p>
<p class="mce-root">The following is an example showing multiple RDDs created as a result of operations. We start with the <strong class="calibre1">Base RDD,</strong> which has 24 items and derive another RDD <strong class="calibre1">carsRDD</strong> that contains only items (3) which match cars:</p>
<div><img class="image-border60" src="img/00227.jpeg"/></div>
<p>The number of partitions does not change during such operations, as each executor applies the filter transformation in-memory, generating a new RDD partition corresponding to the original RDD partition.</p>
<p class="mce-root">Next, we will see how to create RDDs</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">RDD Creation</h1>
                
            
            
                
<p class="mce-root">An RDD is the fundamental object used in Apache Spark. They are immutable collections representing datasets and have the inbuilt capability of reliability and failure recovery. By nature, RDDs create new RDDs upon any operation such as transformation or action. RDDs also store the lineage which is used to recover from failures. We have also seen in the previous chapter some details about how RDDs can be created and what kind of operations can be applied to RDDs.</p>
<p class="mce-root">An RDD can be created in several ways:</p>
<ul class="calibre9">
<li class="mce-root1">Parallelizing a collection</li>
<li class="mce-root1">Reading data from an external source</li>
<li class="mce-root1">Transformation of an existing RDD</li>
<li class="mce-root1">Streaming API</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Parallelizing a collection</h1>
                
            
            
                
<p class="mce-root">Parallelizing a collection can be done by calling <kbd class="calibre11">parallelize()</kbd> on the collection inside the driver program. The driver, when it tries to parallelize a collection, splits the collection into partitions and distributes the data partitions across the cluster.</p>
<p class="mce-root">The following is an RDD to create an RDD from a sequence of numbers using the SparkContext and the <kbd class="calibre11">parallelize()</kbd> function. The <kbd class="calibre11">parallelize()</kbd> function essentially splits the Sequence of numbers into a distributed collection otherwise known as an RDD.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.take(10)</strong><br class="title-page-name"/>res0: Array[Int] = Array(1, 2, 3)
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Reading data from an external source</h1>
                
            
            
                
<p class="mce-root">A second method for creating an RDD is by reading data from an external distributed source such as Amazon S3, Cassandra, HDFS, and so on. For example, if you are creating an RDD from HDFS, then the distributed blocks in HDFS are all read by the individual nodes in the Spark cluster.</p>
<p class="mce-root">Each of the nodes in the Spark cluster is essentially doing its own input-output operations and each node is independently reading one or more blocks from the HDFS blocks. In general, Spark makes the best effort to put as much RDD as possible into memory. There is the capability to <kbd class="calibre11">cache</kbd> the data to reduce the input-output operations by enabling nodes in the spark cluster to avoid repeated reading operations, say from the HDFS blocks, which might be remote to the Spark cluster. There are a whole bunch of caching strategies that can be used within your Spark program, which we will examine later in a section for caching.</p>
<p class="mce-root">The following is an RDD of text lines loading from a text file using the Spark Context and the <kbd class="calibre11">textFile()</kbd> function. The <kbd class="calibre11">textFile</kbd> function loads the input data as a text file (each newline <kbd class="calibre11">\n</kbd> terminated portion becomes an element in the RDD). The function call also automatically uses HadoopRDD (shown in next chapter) to detect and load the data in the form of several partitions as needed, distributed across the cluster.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_two = sc.textFile("wiki1.txt")</strong><br class="title-page-name"/>rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.count</strong><br class="title-page-name"/>res6: Long = 9<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.first</strong><br class="title-page-name"/>res7: String = Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Transformation of an existing RDD</h1>
                
            
            
                
<p class="mce-root">RDDs, by nature, are immutable; hence, your RDDs could be created by applying transformations on any existing RDD. Filter is one typical example of a transformation.</p>
<p class="mce-root">The following is a simple <kbd class="calibre11">rdd</kbd> of integers and transformation by multiplying each integer by <kbd class="calibre11">2</kbd>. Again, we use the <kbd class="calibre11">SparkContext</kbd> and parallelize function to create a sequence of integers into an RDD by distributing the Sequence in the form of partitions. Then, we use the <kbd class="calibre11">map()</kbd> function to transform the RDD into another RDD by multiplying each number by <kbd class="calibre11">2</kbd>.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.take(10)</strong><br class="title-page-name"/>res0: Array[Int] = Array(1, 2, 3)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rdd_one_x2 = rdd_one.map(i =&gt; i * 2)</strong><br class="title-page-name"/>rdd_one_x2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at map at &lt;console&gt;:26<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one_x2.take(10)</strong><br class="title-page-name"/>res9: Array[Int] = Array(2, 4, 6)
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Streaming API</h1>
                
            
            
                
<p class="mce-root">RDDs can also be created via spark streaming. These RDDs are called Discretized Stream RDDs (DStream RDDs).</p>
<p class="mce-root">We will look at this further in <a href="part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 9</a>, <em class="calibre8">Stream Me Up, Scotty - Spark Streaming</em>.</p>
<p class="mce-root">In the next section, we will create RDDs and explore some of the operations using Spark-Shell.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Using the Spark shell</h1>
                
            
            
                
<p class="mce-root">Spark shell provides a simple way to perform interactive analysis of data. It also enables you to learn the Spark APIs by quickly trying out various APIs. In addition, the similarity to Scala shell and support for Scala APIs also lets you also adapt quickly to Scala language constructs and make better use of Spark APIs.</p>
<p>Spark shell implements the concept of <strong class="calibre27">read-evaluate-print-loop</strong> (<strong class="calibre27">REPL</strong>), which allows you to interact with the shell by typing in code which is evaluated. The result is then printed on the console, without needing to be compiled, so building executable code.</p>
<p class="mce-root">Start it by running the following in the directory where you installed Spark:</p>
<pre class="calibre19">
<strong class="calibre1">./bin/spark-shell</strong>
</pre>
<p class="mce-root">Spark shell launches and the Spark shell automatically creates the <kbd class="calibre11">SparkSession</kbd> and <kbd class="calibre11">SparkContext</kbd> objects. The <kbd class="calibre11">SparkSession</kbd> is available as a Spark and the <kbd class="calibre11">SparkContext</kbd> is available as sc.</p>
<p class="mce-root"><kbd class="calibre11">spark-shell</kbd> can be launched with several options as shown in the following snippet (the most important ones are in bold):</p>
<pre class="cdpalignleft3">
<strong class="calibre1">./bin/spark-shell --help</strong><br class="title-page-name"/>Usage: ./bin/spark-shell [options]<br class="title-page-name"/><br class="title-page-name"/>Options:<br class="title-page-name"/> --<strong class="calibre1">master</strong> MASTER_URL spark://host:port, mesos://host:port, yarn, or local.<br class="title-page-name"/> --<strong class="calibre1">deploy-mode</strong> DEPLOY_MODE Whether to launch the driver program locally ("client") or<br class="title-page-name"/> on one of the worker machines inside the cluster ("cluster")<br class="title-page-name"/> (Default: client).<br class="title-page-name"/> --<strong class="calibre1">class</strong> CLASS_NAME Your application's main class (for Java / Scala apps).<br class="title-page-name"/> --name NAME A name of your application.<br class="title-page-name"/> --<strong class="calibre1">jars</strong> JARS Comma-separated list of local jars to include on the driver<br class="title-page-name"/> and executor classpaths.<br class="title-page-name"/> --packages Comma-separated list of maven coordinates of jars to include<br class="title-page-name"/> on the driver and executor classpaths. Will search the local<br class="title-page-name"/> maven repo, then maven central and any additional remote<br class="title-page-name"/> repositories given by --repositories. The format for the<br class="title-page-name"/> coordinates should be groupId:artifactId:version.<br class="title-page-name"/> --exclude-packages Comma-separated list of groupId:artifactId, to exclude while<br class="title-page-name"/> resolving the dependencies provided in --packages to avoid<br class="title-page-name"/> dependency conflicts.<br class="title-page-name"/> --repositories Comma-separated list of additional remote repositories to<br class="title-page-name"/> search for the maven coordinates given with --packages.<br class="title-page-name"/> --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place<br class="title-page-name"/> on the PYTHONPATH for Python apps.<br class="title-page-name"/> --files FILES Comma-separated list of files to be placed in the working<br class="title-page-name"/> directory of each executor.<br class="title-page-name"/><br class="title-page-name"/> --conf PROP=VALUE Arbitrary Spark configuration property.<br class="title-page-name"/> --properties-file FILE Path to a file from which to load extra properties. If not<br class="title-page-name"/> specified, this will look for conf/spark-defaults.conf.<br class="title-page-name"/><br class="title-page-name"/> --<strong class="calibre1">driver-memory</strong> MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M).<br class="title-page-name"/> --driver-Java-options Extra Java options to pass to the driver.<br class="title-page-name"/> --driver-library-path Extra library path entries to pass to the driver.<br class="title-page-name"/> --driver-class-path Extra class path entries to pass to the driver. Note that<br class="title-page-name"/> jars added with --jars are automatically included in the<br class="title-page-name"/> classpath.<br class="title-page-name"/><br class="title-page-name"/> --<strong class="calibre1">executor-memory</strong> MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G).<br class="title-page-name"/><br class="title-page-name"/> --proxy-user NAME User to impersonate when submitting the application.<br class="title-page-name"/> This argument does not work with --principal / --keytab.<br class="title-page-name"/><br class="title-page-name"/> --help, -h Show this help message and exit.<br class="title-page-name"/> --verbose, -v Print additional debug output.<br class="title-page-name"/> --version, Print the version of current Spark.<br class="title-page-name"/><br class="title-page-name"/> Spark standalone with cluster deploy mode only:<br class="title-page-name"/> --driver-cores NUM Cores for driver (Default: 1).<br class="title-page-name"/><br class="title-page-name"/> Spark standalone or Mesos with cluster deploy mode only:<br class="title-page-name"/> --supervise If given, restarts the driver on failure.<br class="title-page-name"/> --kill SUBMISSION_ID If given, kills the driver specified.<br class="title-page-name"/> --status SUBMISSION_ID If given, requests the status of the driver specified.<br class="title-page-name"/><br class="title-page-name"/> Spark standalone and Mesos only:<br class="title-page-name"/> --<strong class="calibre1">total-executor-cores</strong> NUM Total cores for all executors.<br class="title-page-name"/><br class="title-page-name"/> Spark standalone and YARN only:<br class="title-page-name"/> --<strong class="calibre1">executor-cores</strong> NUM Number of cores per executor. (Default: 1 in YARN mode,<br class="title-page-name"/> or all available cores on the worker in standalone mode)<br class="title-page-name"/><br class="title-page-name"/> YARN-only:<br class="title-page-name"/> --driver-cores NUM Number of cores used by the driver, only in cluster mode<br class="title-page-name"/> (Default: 1).<br class="title-page-name"/> --queue QUEUE_NAME The YARN queue to submit to (Default: "default").<br class="title-page-name"/> --<strong class="calibre1">num-executors</strong> NUM Number of executors to launch (Default: 2).<br class="title-page-name"/> If dynamic allocation is enabled, the initial number of<br class="title-page-name"/> executors will be at least NUM.<br class="title-page-name"/> --archives ARCHIVES Comma separated list of archives to be extracted into the<br class="title-page-name"/> working directory of each executor.<br class="title-page-name"/> --principal PRINCIPAL Principal to be used to login to KDC, while running on<br class="title-page-name"/> secure HDFS.<br class="title-page-name"/> --keytab KEYTAB The full path to the file that contains the keytab for the<br class="title-page-name"/> principal specified above. This keytab will be copied to<br class="title-page-name"/> the node running the Application Master via the Secure<br class="title-page-name"/> Distributed Cache, for renewing the login tickets and the<br class="title-page-name"/> delegation tokens periodically.
</pre>
<p class="mce-root">You can also submit Spark code in the form of executable Java jars so that the job is executed in a cluster. Usually, you do this once you have reached a workable solution using the shell.</p>
<p>Use <kbd class="calibre22">./bin/spark-submit</kbd> when submitting a Spark job to a cluster (local, YARN, and Mesos).</p>
<p class="mce-root">The following are Shell Commands (the most important ones are in bold):</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; :help</strong><br class="title-page-name"/>All commands can be abbreviated, e.g., :he instead of :help.<br class="title-page-name"/>:edit &lt;id&gt;|&lt;line&gt; edit history<br class="title-page-name"/>:<strong class="calibre1">help</strong> [command] print this summary or command-specific help<br class="title-page-name"/>:<strong class="calibre1">history</strong> [num] show the history (optional num is commands to show)<br class="title-page-name"/>:h? &lt;string&gt; search the history<br class="title-page-name"/>:imports [name name ...] show import history, identifying sources of names<br class="title-page-name"/>:implicits [-v] show the implicits in scope<br class="title-page-name"/>:javap &lt;path|class&gt; disassemble a file or class name<br class="title-page-name"/>:line &lt;id&gt;|&lt;line&gt; place line(s) at the end of history<br class="title-page-name"/>:load &lt;path&gt; interpret lines in a file<br class="title-page-name"/>:<strong class="calibre1">paste</strong> [-raw] [path] enter paste mode or paste a file<br class="title-page-name"/>:power enable power user mode<br class="title-page-name"/>:quit exit the interpreter<br class="title-page-name"/>:replay [options] reset the repl and replay all previous commands<br class="title-page-name"/>:require &lt;path&gt; add a jar to the classpath<br class="title-page-name"/>:reset [options] reset the repl to its initial state, forgetting all session entries<br class="title-page-name"/>:save &lt;path&gt; save replayable session to a file<br class="title-page-name"/>:sh &lt;command line&gt; run a shell command (result is implicitly =&gt; List[String])<br class="title-page-name"/>:settings &lt;options&gt; update compiler options, if possible; see reset<br class="title-page-name"/>:silent disable/enable automatic printing of results<br class="title-page-name"/>:type [-v] &lt;expr&gt; display the type of an expression without evaluating it<br class="title-page-name"/>:kind [-v] &lt;expr&gt; display the kind of expression's type<br class="title-page-name"/>:warnings show the suppressed warnings from the most recent line which had any
</pre>
<p class="mce-root">Using the spark-shell, we will now load some data as an RDD:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.take(10)</strong><br class="title-page-name"/>res0: Array[Int] = Array(1, 2, 3)
</pre>
<p class="mce-root">As you see, we are running the commands one by one. Alternately, we can also paste the commands:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; :paste</strong><br class="title-page-name"/><strong class="calibre1">// Entering paste mode (ctrl-D to finish)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/><strong class="calibre1">rdd_one.take(10)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">// Exiting paste mode, now interpreting.</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:26<br class="title-page-name"/>res10: Array[Int] = Array(1, 2, 3)
</pre>
<p class="mce-root">In the next section, we will go deeper into the operations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Actions and Transformations</h1>
                
            
            
                
<p class="mce-root">RDDs are immutable and every operation creates a new RDD. Now, the two main operations that you can perform on an RDD are <strong class="calibre1">Transformations</strong> and <strong class="calibre1">Actions</strong>.</p>
<p class="mce-root"><strong class="calibre1">Transformations</strong> change the elements in the RDD such as splitting the input element, filtering out elements, and performing calculations of some sort. Several transformations can be performed in a sequence; however no execution takes place during the planning.</p>
<p>For transformations, Spark adds them to a DAG of computation and, only when driver requests some data, does this DAG actually gets executed. This is called <em class="calibre25">lazy</em> evaluation.</p>
<p class="mce-root">The reasoning behind the lazy evaluation is that Spark can look at all the transformations and plan the execution, making use of the understanding the Driver has of all the operations. For instance, if a filter transformation is applied immediately after some other transformation, Spark will optimize the execution so that each Executor performs the transformations on each partition of data efficiently. Now, this is possible only when Spark is waiting until something needs to be executed.</p>
<p class="mce-root"><strong class="calibre1">Actions</strong> are operations, which actually trigger the computations. Until an action operation is encountered, the execution plan within the spark program is created in the form of a DAG and does nothing. Clearly, there could be several transformations of all sorts within the execution plan, but nothing happens until you perform an action.</p>
<p class="mce-root">The following is a depiction of the various operations on some arbitrary data where we just wanted to remove all pens and bikes and just count cars<strong class="calibre1">.</strong> Each print statement is an action which triggers the execution of all the transformation steps in the DAG based execution plan until that point as shown in the following diagram:</p>
<div><img class="image-border61" src="img/00230.jpeg"/></div>
<p class="mce-root">For example, an action count on a directed acyclic graph of transformations triggers the execution of the transformation all the way up to the base RDD. If there is another action performed, then there is a new chain of executions that could take place. This is a clear case of why any caching that could be done at different stages in the directed acyclic graph will greatly speed up the next execution of the program. Another way that the execution is optimized is through the reuse of the shuffle files from the previous execution.</p>
<p class="mce-root">Another example is the collect action that collects or pulls all the data from all the nodes to the driver. You could use a partial function when invoking collect to selectively pull the data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Transformations</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre1">Transformations</strong> creates a new RDD from an existing RDD by applying transformation logic to each of the elements in the existing RDD. Some of the transformation functions involve splitting the element, filtering out elements, and performing calculations of some sort. Several transformations can be performed in a sequence. However, no execution takes place during the planning.</p>
<p class="mce-root">Transformations can be divided into four categories, as follows.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">General transformations</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre1">General transformations</strong> are transformation functions that handle most of the general purpose use cases, applying the transformational logic to existing RDDs and generating a new RDD. The common operations of aggregation, filters and so on are all known as general transformations.</p>
<p class="mce-root">Examples of general transformation functions are:</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">map</kbd></li>
<li class="mce-root1"><kbd class="calibre11">filter</kbd></li>
<li class="mce-root1"><kbd class="calibre11">flatMap</kbd></li>
<li class="mce-root1"><kbd class="calibre11">groupByKey</kbd></li>
<li class="mce-root1"><kbd class="calibre11">sortByKey</kbd></li>
<li class="mce-root1"><kbd class="calibre11">combineByKey</kbd></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Math/Statistical transformations</h1>
                
            
            
                
<p class="mce-root">Mathematical or statistical transformations are transformation functions which handle some statistical functionality, and which usually apply some mathematical or statistical operation on existing RDDs, generating a new RDD. Sampling is a great example of this and is used often in Spark programs.</p>
<p class="mce-root">Examples of such transformations are:</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">sampleByKey</kbd></li>
<li class="mce-root1"><kbd class="calibre11"><kbd class="calibre33">randomSplit</kbd></kbd></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Set theory/relational transformations</h1>
                
            
            
                
<p class="mce-root">Set theory/relational transformations are transformation functions, which handle transformations like Joins of datasets and other relational algebraic functionality such as <kbd class="calibre11">cogroup</kbd>. These functions work by applying the transformational logic to existing RDDs and generating a new RDD.</p>
<p class="mce-root">Examples of such transformations are:</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">cogroup</kbd></li>
<li class="mce-root1"><kbd class="calibre11">join</kbd></li>
<li class="mce-root1"><kbd class="calibre11">subtractByKey</kbd></li>
<li class="mce-root1"><kbd class="calibre11">fullOuterJoin</kbd></li>
<li class="mce-root1"><kbd class="calibre11">leftOuterJoin</kbd></li>
<li class="mce-root1"><kbd class="calibre11">rightOuterJoin</kbd></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Data structure-based transformations</h1>
                
            
            
                
<p class="mce-root">Data structure-based transformations are transformation functions which operate on the underlying data structures of the RDD, the partitions in the RDD. In these functions, you can directly work on partitions without directly touching the elements/data inside the RDD. These are essential in any Spark program beyond the simple programs where you need more control of the partitions and distribution of partitions in the cluster. Typically, performance improvements can be realized by redistributing the data partitions according to the cluster state and the size of the data, and the exact use case requirements.</p>
<p class="mce-root">Examples of such transformations are:</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">partitionBy</kbd></li>
<li class="mce-root1"><kbd class="calibre11">repartition</kbd></li>
<li class="mce-root1"><kbd class="calibre11">zipwithIndex</kbd></li>
<li class="mce-root1"><kbd class="calibre11">coalesce</kbd></li>
</ul>
<p class="mce-root">The following is the list of transformation functions as available in the latest Spark 2.1.1:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<th class="calibre30">Transformation</th>
<th class="calibre30">Meaning</th>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">map(func)</kbd></td>
<td class="calibre7">Return a new distributed dataset formed by passing each element of the source through a function <kbd class="calibre34">func</kbd>.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">filter(func)</kbd></td>
<td class="calibre7">Return a new dataset formed by selecting those elements of the source on which func returns true.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">flatMap(func)</kbd></td>
<td class="calibre7">Similar to map, but each input item can be mapped to 0 or more output items (so <kbd class="calibre34">func</kbd> should return a <kbd class="calibre34">Seq</kbd> rather than a single item).</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">mapPartitions(func)</kbd></td>
<td class="calibre7">Similar to map, but runs separately on each partition (block) of the RDD, so <kbd class="calibre34">func</kbd> must be of type <kbd class="calibre34">Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</kbd> when running on an RDD of type <kbd class="calibre34">T</kbd>.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">mapPartitionsWithIndex(func)</kbd></td>
<td class="calibre7">Similar to <kbd class="calibre34">mapPartitions</kbd>, but also provides <kbd class="calibre34">func</kbd> with an integer value representing the index of the partition, so <kbd class="calibre34">func</kbd> must be of type <kbd class="calibre34">(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</kbd> when running on an RDD of type <kbd class="calibre34">T</kbd>.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">sample(withReplacement, fraction, seed)</kbd></td>
<td class="calibre7">Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">union(otherDataset)</kbd></td>
<td class="calibre7">Return a new dataset that contains the union of the elements in the source dataset and the argument.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">intersection(otherDataset)</kbd></td>
<td class="calibre7">Return a new RDD that contains the intersection of elements in the source dataset and the argument.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">distinct([numTasks]))</kbd></td>
<td class="calibre7">Return a new dataset that contains the distinct elements of the source dataset.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">groupByKey([numTasks])</kbd></td>
<td class="calibre7">When called on a dataset of <kbd class="calibre34">(K, V)</kbd> pairs, returns a dataset of <kbd class="calibre34">(K, Iterable&lt;V&gt;)</kbd> pairs.<br class="title-page-name"/>
Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using <kbd class="calibre34">reduceByKey</kbd> or <kbd class="calibre34">aggregateByKey</kbd> will yield much better performance.<br class="title-page-name"/>
Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional <kbd class="calibre34">numTasks</kbd> argument to set a different number of tasks.</td>
</tr>
<tr class="calibre6">
<td class="calibre7">reduceByKey(func, [numTasks])</td>
<td class="calibre7">When called on a dataset of <kbd class="calibre34">(K, V)</kbd> pairs, returns a dataset of <kbd class="calibre34">(K, V)</kbd> pairs where the values for each key are aggregated using the given <kbd class="calibre34">reduce</kbd> function <kbd class="calibre34">func</kbd>, which must be of type <kbd class="calibre34">(V,V) =&gt; V</kbd>. As in <kbd class="calibre34">groupByKey</kbd>, the number of reduce tasks is configurable through an optional second argument.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</kbd></td>
<td class="calibre7">When called on a dataset of <kbd class="calibre34">(K, V)</kbd> pairs, returns a dataset of <kbd class="calibre34">(K, U)</kbd> pairs where the values for each key are aggregated using the given combine functions and a neutral <em class="calibre8">zero</em> value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. As in <kbd class="calibre34">groupByKey</kbd>, the number of reduce tasks is configurable through an optional second argument.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">sortByKey([ascending], [numTasks])</kbd></td>
<td class="calibre7">When called on a dataset of <kbd class="calibre34">(K, V)</kbd> pairs where <kbd class="calibre34">K</kbd> implements ordered, returns a dataset of <kbd class="calibre34">(K, V)</kbd> pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">join(otherDataset, [numTasks])</kbd></td>
<td class="calibre7">When called on datasets of type <kbd class="calibre34">(K, V)</kbd> and <kbd class="calibre34">(K, W)</kbd>, returns a dataset of <kbd class="calibre34">(K, (V, W))</kbd> pairs with all pairs of elements for each key. Outer joins are supported through <kbd class="calibre34">leftOuterJoin</kbd>, <kbd class="calibre34">rightOuterJoin</kbd>, and <kbd class="calibre34">fullOuterJoin</kbd>.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">cogroup(otherDataset, [numTasks])</kbd></td>
<td class="calibre7">When called on datasets of type <kbd class="calibre34">(K, V)</kbd> and <kbd class="calibre34">(K, W)</kbd>, returns a dataset of <kbd class="calibre34">(K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;))</kbd> tuples. This operation is also called <kbd class="calibre34">groupWith</kbd>.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">cartesian(otherDataset)</kbd></td>
<td class="calibre7">When called on datasets of types <kbd class="calibre34">T</kbd> and <kbd class="calibre34">U</kbd>, returns a dataset of <kbd class="calibre34">(T, U)</kbd> pairs (all pairs of elements).</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">pipe(command, [envVars])</kbd></td>
<td class="calibre7">Pipe each partition of the RDD through a shell command, for example, a Perl or bash script. RDD elements are written to the process's <kbd class="calibre34">stdin</kbd>, and lines output to its <kbd class="calibre34">stdout</kbd> are returned as an RDD of strings.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">coalesce(numPartitions)</kbd></td>
<td class="calibre7">Decrease the number of partitions in the RDD to <kbd class="calibre34">numPartitions</kbd>. Useful for running operations more efficiently after filtering down a large dataset.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">repartition(numPartitions)</kbd></td>
<td class="calibre7">Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">repartitionAndSortWithinPartitions(partitioner)</kbd></td>
<td class="calibre7">Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling <kbd class="calibre34">repartition</kbd> and then sorting within each partition because it can push the sorting down into the shuffle machinery.</td>
</tr>
</tbody>
</table>
<p class="mce-root">Â </p>
<p class="mce-root">We will illustrate the most common transformations:</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">map function</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">map</kbd> applies transformation function to input partitions to generate output partitions in the output RDD.</p>
<p class="mce-root">As shown in the following snippet, this is how we can map an RDD of a text file to an RDD with lengths of the lines of text:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_two = sc.textFile("wiki1.txt")</strong><br class="title-page-name"/>rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.count</strong><br class="title-page-name"/>res6: Long = 9<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.first</strong><br class="title-page-name"/>res7: String = Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rdd_three = rdd_two.map(line =&gt; line.length)</strong><br class="title-page-name"/>res12: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[11] at map at &lt;console&gt;:2<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_three.take(10)</strong><br class="title-page-name"/>res13: Array[Int] = Array(271, 165, 146, 138, 231, 159, 159, 410, 281)
</pre>
<p class="mce-root">The following diagram explains of how <kbd class="calibre11">map()</kbd> works. You can see that each partition of the RDD results in a new partition in a new RDD essentially applying the transformation to all elements of the RDD:</p>
<div><img class="image-border62" src="img/00236.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">flatMap function</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">flatMap()</kbd> applies transformation function to input partitions to generate output partitions in the output RDD just like <kbd class="calibre11">map()</kbd> function. However, <kbd class="calibre11">flatMap()</kbd> also flattens any collection in the input RDD elements.</p>
<pre>flatMap()</kbd> on a RDD of a text file to convert the lines in the text to a RDD containing the individual words. We also show <kbd class="calibre11">map()</kbd> called on the same RDD before <kbd class="calibre11">flatMap()</kbd> is called just to show the difference in behavior:</pre>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_two = sc.textFile("wiki1.txt")</strong><br class="title-page-name"/>rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.count</strong><br class="title-page-name"/>res6: Long = 9<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.first</strong><br class="title-page-name"/>res7: String = Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rdd_three = rdd_two.map(line =&gt; line.split(" "))</strong><br class="title-page-name"/>rdd_three: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[16] at map at &lt;console&gt;:26<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_three.take(1)</strong><br class="title-page-name"/>res18: Array[Array[String]] = Array(Array(Apache, Spark, provides, programmers, with, an, application, programming, interface, centered, on, a, data, structure, called, the, resilient, distributed, dataset, (RDD),, a, read-only, multiset, of, data, items, distributed, over, a, cluster, of, machines,, that, is, maintained, in, a, fault-tolerant, way.)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rdd_three = rdd_two.flatMap(line =&gt; line.split(" "))</strong><br class="title-page-name"/>rdd_three: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at flatMap at &lt;console&gt;:26<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_three.take(10)</strong><br class="title-page-name"/>res19: Array[String] = Array(Apache, Spark, provides, programmers, with, an, application, programming, interface, centered)
</pre>
<p class="mce-root">The following diagram explains how <kbd class="calibre11">flatMap()</kbd> works. You can see that each partition of the RDD results in a new partition in a new RDD, essentially applying the transformation to all elements of the RDD:</p>
<div><img class="image-border63" src="img/00239.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">filter function</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">filter</kbd> applies transformation function to input partitions to generate filtered output partitions in the output RDD.</p>
<pre>Spark</kbd>:</pre>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_two = sc.textFile("wiki1.txt")</strong><br class="title-page-name"/>rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.count</strong><br class="title-page-name"/>res6: Long = 9<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.first</strong><br class="title-page-name"/>res7: String = Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rdd_three = rdd_two.filter(line =&gt; line.contains("Spark"))</strong><br class="title-page-name"/>rdd_three: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[20] at filter at &lt;console&gt;:26<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt;rdd_three.count</strong><br class="title-page-name"/>res20: Long = 5
</pre>
<p class="mce-root">The following diagram explains how <kbd class="calibre11">filter</kbd> works. You can see that each partition of the RDD results in a new partition in a new RDD, essentially applying the filter transformation on all elements of the RDD.</p>
<p>Note that the partitions do not change, and some partitions could be empty too, when applying filter</p>
<div><img class="image-border64" src="img/00242.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">coalesce</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">coalesce</kbd> applies a <kbd class="calibre11">transformation</kbd> function to input partitions to combine the input partitions into fewer partitions in the output RDD.</p>
<p class="mce-root">As shown in the following code snippet, this is how we can combine all partitions to a single partition:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_two = sc.textFile("wiki1.txt")</strong><br class="title-page-name"/>rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.partitions.length</strong><br class="title-page-name"/>res21: Int = 2<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rdd_three = rdd_two.coalesce(1)</strong><br class="title-page-name"/>rdd_three: org.apache.spark.rdd.RDD[String] = CoalescedRDD[21] at coalesce at &lt;console&gt;:26<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_three.partitions.length</strong><br class="title-page-name"/>res22: Int = 1
</pre>
<p class="mce-root">The following diagram explains how <kbd class="calibre11">coalesce</kbd> works. You can see that a new RDD is created from the original RDD essentially reducing the number of partitions by combining them as needed:</p>
<div><img class="image-border65" src="img/00248.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">repartition</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">repartition</kbd> applies a <kbd class="calibre11">transformation</kbd> function to input partitions to <kbd class="calibre11">repartition</kbd> the input into fewer or more output partitions in the output RDD.</p>
<p class="mce-root">As shown in the following code snippet, this is how we can map an RDD of a text file to an RDD with more partitions:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_two = sc.textFile("wiki1.txt")</strong><br class="title-page-name"/>rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.partitions.length</strong><br class="title-page-name"/>res21: Int = 2<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rdd_three = rdd_two.repartition(5)</strong><br class="title-page-name"/>rdd_three: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[25] at repartition at &lt;console&gt;:26<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_three.partitions.length</strong><br class="title-page-name"/>res23: Int = 5
</pre>
<p class="mce-root">The following diagram explains how <kbd class="calibre11">repartition</kbd> works. You can see that a new RDD is created from the original RDD, essentially redistributing the partitions by combining/splitting them as needed:</p>
<div><img class="image-border66" src="img/00254.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Actions</h1>
                
            
            
                
<p class="mce-root">Action triggers the entire <strong class="calibre1">DAG</strong> (<strong class="calibre1">Directed Acyclic Graph</strong>) of transformations built so far to be materialized by running the code blocks and functions. All operations are now executed as the DAG specifies.</p>
<p class="mce-root">There are two kinds of action operations:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Driver</strong>: One kind of action is the driver action such as collect count, count by key, and so on. Each such action performs some calculations on the remote executor and pulls the data back into the driver.</li>
</ul>
<p>Driver-based action has the problem that actions on large datasets can easily overwhelm the memory available on the driver taking down the application, so you should use the driver involved actions judiciously</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Distributed</strong>: Another kind of action is a distributed action, which is executed on the nodes in the cluster. An example of such a distributed action is <kbd class="calibre11">saveAsTextfile</kbd>. This is the most common action operation due to the desirable distributed nature of the operation.</li>
</ul>
<p class="mce-root">The following is the list of action functions as available in the latest Spark 2.1.1:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<th class="calibre30">Action</th>
<th class="calibre30">Meaning</th>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">reduce(func)</kbd></td>
<td class="calibre7">Aggregate the elements of the dataset using a function <kbd class="calibre34">func</kbd> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">collect()</kbd></td>
<td class="calibre7">Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">count()</kbd></td>
<td class="calibre7">Return the number of elements in the dataset.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">first()</kbd></td>
<td class="calibre7">Return the first element of the dataset (similar to <kbd class="calibre34">take(1)</kbd>).</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">take(n)</kbd></td>
<td class="calibre7">Return an array with the first <kbd class="calibre34">n</kbd> elements of the dataset.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">takeSample(withReplacement, num, [seed])</kbd></td>
<td class="calibre7">Return an array with a random sample of <kbd class="calibre34">num</kbd> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">takeOrdered(n, [ordering])</kbd></td>
<td class="calibre7">Return the first <kbd class="calibre34">n</kbd> elements of the RDD using either their natural order or a custom comparator.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">saveAsTextFile(path)</kbd></td>
<td class="calibre7">Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call <kbd class="calibre34">toString</kbd> on each element to convert it to a line of text in the file.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">saveAsSequenceFile(path)</kbd><br class="title-page-name"/>
(Java and Scala)</td>
<td class="calibre7">Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS, or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's <kbd class="calibre34">Writable</kbd> interface. In Scala, it is also available on types that are implicitly convertible to <kbd class="calibre34">Writable</kbd> (Spark includes conversions for basic types like <kbd class="calibre34">Int</kbd>, <kbd class="calibre34">Double</kbd>, <kbd class="calibre34">String</kbd>, and so on).</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">saveAsObjectFile(path)</kbd><br class="title-page-name"/>
(Java and Scala)</td>
<td class="calibre7">Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using <kbd class="calibre34">SparkContext.objectFile()</kbd>.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">countByKey()</kbd></td>
<td class="calibre7">Only available on RDDs of type <kbd class="calibre34">(K, V)</kbd>. Returns a hashmap of <kbd class="calibre34">(K, Int)</kbd> pairs with the count of each key.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">foreach(func)</kbd></td>
<td class="calibre7">Run a function <kbd class="calibre34">func</kbd> on each element of the dataset. This is usually done for side effects such as updating an accumulator (<a href="http://spark.apache.org/docs/latest/programming-guide.html#accumulators" class="calibre3">http://spark.apache.org/docs/latest/programming-guide.html#accumulators</a>) or interacting with external storage systems.<br class="title-page-name"/>
Note: modifying variables other than accumulators outside of the <kbd class="calibre34">foreach()</kbd> may result in undefined behavior. See understanding closures (<a href="http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka" class="calibre3">http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka</a>) <a href="http://spark.apache.org/docs/latest/programming-guide.html#understanding-closures-a-nameclosureslinka" class="calibre3">for more details.</a></td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">reduce</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">reduce()</kbd> applies the reduce function to all the elements in the RDD and sends it to the Driver.</p>
<p class="mce-root">The following is example code to illustrate this. You can use <kbd class="calibre11">SparkContext</kbd> and the parallelize function to create an RDD from a sequence of integers. Then you can add up all the numbers of the RDD using the <kbd class="calibre11">reduce</kbd> function on the RDD.</p>
<p>Since this is an action, the results are printed as soon as you run the <kbd class="calibre22">reduce</kbd> function.</p>
<p class="mce-root">Shown below is the code to build a simple RDD from a small array of numbers and then perform a reduce operation on the RDD:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3,4,5,6))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.take(10)</strong><br class="title-page-name"/>res28: Array[Int] = Array(1, 2, 3, 4, 5, 6)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.reduce((a,b) =&gt; a +b)</strong><br class="title-page-name"/>res29: Int = 21
</pre>
<p class="mce-root">The following diagram is an illustration of <kbd class="calibre11">reduce()</kbd>. Driver runs the reduce function on the executors and collects the results in the end.</p>
<div><img class="image-border67" src="img/00257.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">count</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">count()</kbd> simply counts the number of elements in the RDD and sends it to the Driver.</p>
<p class="mce-root">The following is an example of this function. We created an RDD from a Sequence of integers using SparkContext and parallelize function and then called count on the RDD to print the number of elements in the RDD.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3,4,5,6))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.count</strong><br class="title-page-name"/>res24: Long = 6
</pre>
<p class="mce-root">The following is an illustration of <kbd class="calibre11">count()</kbd>. The Driver asks each of the executor/task to count the number of elements in the partition being handled by the task and then adds up the counts from all the tasks together at the Driver level.</p>
<div><img class="image-border68" src="img/00260.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">collect</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">collect()</kbd> simply collects all elements in the RDD and sends it to the Driver.</p>
<p class="mce-root">Shown here is an example showing what collect function essentially does. When you call collect on an RDD, the Driver collects all the elements of the RDD by pulling them into the Driver.</p>
<p>Calling collect on large RDDs will cause out-of-memory issues on the Driver.</p>
<p class="mce-root">Shown below is the code to collect the content of the RDD and display it:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; rdd_two.collect</strong><br class="title-page-name"/>res25: Array[String] = Array(Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way., It was developed in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs., "MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. ", Spark's RDDs function as a working set for distributed programs that offers a (deliberately) restricted form of distributed shared memory., The availability of RDDs facilitates t...
</pre>
<p class="mce-root">The following is an illustration of <kbd class="calibre11">collect()</kbd>. Using collect, the Driver is pulling all the elements of the RDD from all partitions.</p>
<div><img class="image-border69" src="img/00027.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Caching</h1>
                
            
            
                
<p class="mce-root">Caching enables Spark to persist data across computations and operations. In fact, this is one of the most important technique in Spark to speed up computations, particularly when dealing with iterative computations.</p>
<p class="mce-root">Caching works by storing the RDD as much as possible in the memory. If there is not enough memory then the current data in storage is evicted, as per LRU policy. If the data being asked to cache is larger than the memory available, the performance will come down because Disk will be used instead of memory.</p>
<p class="mce-root">You can mark an RDD as cached using either <kbd class="calibre11">persist()</kbd> or <kbd class="calibre11">cache()</kbd></p>
<div><kbd class="calibre22">cache()</kbd> is simply a synonym for persist<kbd class="calibre22">(MEMORY_ONLY)</kbd></div>
<p class="mce-root"><kbd class="calibre11">persist</kbd> can use memory or disk or both:</p>
<pre class="calibre19">
<strong class="calibre1">persist(newLevel: StorageLevel) </strong>
</pre>
<p class="mce-root">The following are the possible values for Storage level:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<th class="calibre30">Storage Level</th>
<th class="calibre30">Meaning</th>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">MEMORY_ONLY</kbd></td>
<td class="calibre7">Stores RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">MEMORY_AND_DISK</kbd></td>
<td class="calibre7">Stores RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">MEMORY_ONLY_SER</kbd><br class="title-page-name"/>
(Java and Scala)</td>
<td class="calibre7">Stores RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">MEMORY_AND_DISK_SER</kbd><br class="title-page-name"/>
(Java and Scala)</td>
<td class="calibre7">Similar to <kbd class="calibre34">MEMORY_ONLY_SER</kbd>, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">DISK_ONLY</kbd></td>
<td class="calibre7">Store the RDD partitions only on disk.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">MEMORY_ONLY_2</kbd>, <kbd class="calibre34">MEMORY_AND_DISK_2</kbd>, and so on.</td>
<td class="calibre7">Same as the preceding levels, but replicate each partition on two cluster nodes.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">OFF_HEAP</kbd> (experimental)</td>
<td class="calibre7">Similar to <kbd class="calibre34">MEMORY_ONLY_SER</kbd>, but store the data in off-heap memory. This requires off-heap memory to be enabled.</td>
</tr>
</tbody>
</table>
<p class="mce-root">Storage level to choose depends on the situation</p>
<ul class="calibre9">
<li class="mce-root1">If RDDs fit into memory, use <kbd class="calibre11">MEMORY_ONLY</kbd> as that's the fastest option for execution performance</li>
<li class="mce-root1">Try <kbd class="calibre11">MEMORY_ONLY_SER</kbd> is there are serializable objects being used in order to make the objects smaller</li>
<li class="mce-root1"><kbd class="calibre11">DISK</kbd> should not be used unless your computations are expensive.</li>
<li class="mce-root1">Use replicated storage for best fault tolerance if you can spare the additional memory needed. This will prevent recomputation of lost partitions for best availability.</li>
</ul>
<div><kbd class="calibre22">unpersist()</kbd> simply frees up the cached content.</div>
<p class="mce-root">The following are examples of how to call <kbd class="calibre11">persist()</kbd> function using different types of storage (memory or disk):</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; import org.apache.spark.storage.StorageLevel</strong><br class="title-page-name"/>import org.apache.spark.storage.StorageLevel<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.persist(StorageLevel.MEMORY_ONLY)</strong><br class="title-page-name"/>res37: rdd_one.type = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.unpersist()</strong><br class="title-page-name"/>res39: rdd_one.type = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.persist(StorageLevel.DISK_ONLY)</strong><br class="title-page-name"/>res40: rdd_one.type = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.unpersist()</strong><br class="title-page-name"/>res41: rdd_one.type = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:24
</pre>
<p class="mce-root">The following is an illustration of the performance improvement we get by caching.</p>
<p class="mce-root">First, we will run the code:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3,4,5,6))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.count</strong><br class="title-page-name"/>res0: Long = 6<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.cache</strong><br class="title-page-name"/>res1: rdd_one.type = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.count</strong><br class="title-page-name"/>res2: Long = 6
</pre>
<p class="mce-root">You can use the WebUI to look at the improvement achieved as shown in the following screenshots:</p>
<div><img class="image-border70" src="img/00052.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Loading and saving data</h1>
                
            
            
                
<p class="mce-root">Loading data into an RDD and Saving an RDD onto an output system both support several different methods. We will cover the most common ones in this section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Loading data</h1>
                
            
            
                
<p class="mce-root">Loading data into an RDD can be done by using <kbd class="calibre11">SparkContext</kbd>. Some of the most common methods are:.</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">textFile</kbd></li>
<li class="mce-root1"><kbd class="calibre11">wholeTextFiles</kbd></li>
<li class="mce-root1"><kbd class="calibre11">load</kbd> from a JDBC datasource</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">textFile</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">textFile()</kbd> can be used to load textFiles into an RDD and each line becomes an element in the RDD.</p>
<pre class="calibre19">
<strong class="calibre1">sc.textFile(name, minPartitions=None, use_unicode=True)</strong>
</pre>
<p class="mce-root">The following is an example of loading a <kbd class="calibre11">textfile</kbd> into an RDD using <kbd class="calibre11">textFile()</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_two = sc.textFile("wiki1.txt")</strong><br class="title-page-name"/>rdd_two: org.apache.spark.rdd.RDD[String] = wiki1.txt MapPartitionsRDD[8] at textFile at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.count</strong><br class="title-page-name"/>res6: Long = 9
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">wholeTextFiles</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">wholeTextFiles()</kbd> can be used to load multiple text files into a paired RDD containing pairs <kbd class="calibre11">&lt;filename, textOfFile&gt;</kbd> representing the filename and the entire content of the file. This is useful when loading multiple small text files and is different from <kbd class="calibre11">textFile</kbd> API because when whole <kbd class="calibre11">TextFiles()</kbd> is used, the entire content of the file is loaded as a single record:</p>
<pre class="calibre19">
<strong class="calibre1">sc.wholeTextFiles(path, minPartitions=None, use_unicode=True)</strong>
</pre>
<p class="mce-root">The following is an example of loading a <kbd class="calibre11">textfile</kbd> into an RDD using <kbd class="calibre11">wholeTextFiles()</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_whole = sc.wholeTextFiles("wiki1.txt")</strong><br class="title-page-name"/>rdd_whole: org.apache.spark.rdd.RDD[(String, String)] = wiki1.txt MapPartitionsRDD[37] at wholeTextFiles at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_whole.take(10)</strong><br class="title-page-name"/>res56: Array[(String, String)] =<br class="title-page-name"/>Array((file:/Users/salla/spark-2.1.1-bin-hadoop2.7/wiki1.txt,Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data 
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Load from a JDBC Datasource</h1>
                
            
            
                
<p class="mce-root">You can load data from an external data source which supports <strong class="calibre1">Java Database Connectivity</strong> (<strong class="calibre1">JDBC</strong>). Using a JDBC driver, you can connect to a relational database such as Mysql and load the content of a table into Spark as shown in in the following code snippet:</p>
<pre class="calibre19">
 <strong class="calibre1">sqlContext.load(path=None, source=None, schema=None, **options)</strong>
</pre>
<p class="mce-root">The following is an example of loading from a JDBC datasource:</p>
<pre class="calibre19">
<strong class="calibre1">val dbContent = sqlContext.load(source="jdbc",  url="jdbc:mysql://localhost:3306/test",  dbtable="test",  partitionColumn="id")</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Saving RDD</h1>
                
            
            
                
<p class="mce-root">Saving data from an RDD into a file system can be done by either:</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">saveAsTextFile</kbd></li>
<li class="mce-root1"><kbd class="calibre11">saveAsObjectFile</kbd></li>
</ul>
<p class="mce-root">The following is an example of saving an RDD to a text file</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; rdd_one.saveAsTextFile("out.txt")</strong>
</pre>
<p class="mce-root">There are many more ways of loading and saving data, particularly when integrating with HBase, Cassandra and so on.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, we discussed the internals of Apache Spark, what RDDs are, DAGs and lineages of RDDs, Transformations, and Actions. We also looked at various deployment modes of Apache Spark using standalone, YARN, and Mesos deployments. We also did a local install on our local machine and then looked at Spark shell and how it can be used to interact with Spark.</p>
<p class="mce-root">In addition, we also looked at loading data into RDDs and saving RDDs to external systems as well as the secret sauce of Spark's phenomenal performance, the caching functionality, and how we can use memory and/or disk to optimize the performance.</p>
<p class="mce-root">In the next chapter, we will dig deeper into RDD API and how it all works in <a href="part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 7</a>, <em class="calibre8">Special RDD Operations</em>.</p>


            

            
        
    </body></html>