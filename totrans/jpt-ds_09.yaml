- en: Machine Learning Using Jupyter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use several algorithms for machine learning under Jupyter.
    We have coding in both R and Python to portray the breadth of options available
    to the Jupyter developer.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naive Bayes is an algorithm that uses probability to classify the data according
    to Bayes theorem for strong independence of the features. Bayes theorem estimates
    the probability of an event based on prior conditions. So, overall, we use a set
    of feature values to estimate a value assuming the same conditions hold true when
    those features have similar values.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes using R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first implementation of naive Bayes uses the R programming language. The
    R implementation of the algorithm is encoded in the `e1071` library. `e1071` appears
    to have been the department identifier at the school where the package was developed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first install the package, and load the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Some notes on these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The `install.packages` call is commented out as we don't want to run this every
    time we run the script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`e1071` is the naive Bayes algorithm package.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `caret` package contains a method to partition a dataset randomly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We set the `seed` so as to be able to reproduce the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are using the `iris` dataset for this example. Specifically, using the other
    `iris` factors to predict the species.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Invocations of the package look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Where the parameters to `naiveBayes` are:'
  prefs: []
  type: TYPE_NORMAL
- en: Formula of the form *y ~ x1 + x2 ....*-attempt to predict *y* based on *x1,
    x2, ...*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data frame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optional Laplace smoothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optional subset of the data based on a Boolean filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optional function for handling `na` values (`na.action`)—default is to pass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once we have our model established we can then attempt a prediction using the
    `predict()` function with parameters for:'
  prefs: []
  type: TYPE_NORMAL
- en: Model (from the preceding call)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data frame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type whether the data is class or raw (conditionals)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we continue with the `iris` example with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Where we split the data into 75% training and 25% for testing, as you can see
    by the number of rows in each data frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we construct out model—we are trying to predict `Species` from the other
    features/columns of the data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/51a11daa-cfed-4f6c-8573-14f8c4a9d173.png)'
  prefs: []
  type: TYPE_IMG
- en: It is interesting that the Apriori assumption is an even split between the possibilities.
    Sepal length, width, and petal length have strong influences on species.
  prefs: []
  type: TYPE_NORMAL
- en: 'We make our prediction based on the model against the testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to measure the accuracy of the model. Normally we could use a
    scatter diagram using `x` from actual and `y` from predicted, but we have categorical
    data. We could build a vector of actual versus predicted and compare the two in
    a new results data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We end up with a model providing 97% (`35`/`36`) accuracy. This is a very good
    performance level, almost within the statistical boundary of excellent (+/- 2%).
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Python implementation of the algorithm is in the `sklearn` library. The
    whole process is much simpler. First, load the `iris` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Call upon the built-in Gaussian naive Bayes estimator for a model and prediction
    in one step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Determine the accuracy of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We end up with very similar results for estimation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Nearest neighbor estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using nearest neighbor, we have an unclassified object and a set of objects
    that are classified. We then take the attributes of the unclassified object, compare
    against the known classifications in place, and select the class that is closest
    to our unknown. The comparison distances resolve to Euclidean geometry computing
    the distances between two points (where known attributes fall in comparison to
    the unknown's attributes).
  prefs: []
  type: TYPE_NORMAL
- en: Nearest neighbor using R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this example, we are using the housing data from `ics.edu`. First, we load
    the data and assign column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We reorder the data so the key (the housing price `MDEV`) is in ascending order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can split the data into a training set and a test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We build our nearest neighbor model using both sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us look at the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b83dd910-3b61-49bc-9a9f-c64dc16daa2f.png)'
  prefs: []
  type: TYPE_IMG
- en: There is a slight Poisson distribution with the higher points near the left
    side. I think this makes sense as *natural* data. The start and end tails are
    dramatically going off page.
  prefs: []
  type: TYPE_NORMAL
- en: 'What about the accuracy of this model? I did not find a clean way to translate
    the predicted factors in the `knnModel` to numeric values, so I extracted them
    to a flat file, and then loaded them in separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '| predicted |'
  prefs: []
  type: TYPE_TB
- en: '| 10.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 9.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 7.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Then we can build up a `results` data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And compute our accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '| **testing.MDEV** | **predicted** | **accuracy** |'
  prefs: []
  type: TYPE_TB
- en: '| `5.6` | `10.5` | `0.5333333` |'
  prefs: []
  type: TYPE_TB
- en: '| `7.2` | `9.7` | `0.7422680` |'
  prefs: []
  type: TYPE_TB
- en: '| `8.1` | `7.0` | `1.1571429` |'
  prefs: []
  type: TYPE_TB
- en: '| `8.5` | `6.3` | `1.3492063` |'
  prefs: []
  type: TYPE_TB
- en: '| `10.5` | `13.1` | `0.8015267` |'
  prefs: []
  type: TYPE_TB
- en: '| `10.8` | `16.3` | `0.6625767` |'
  prefs: []
  type: TYPE_TB
- en: So, we are estimating within 2% (`1.01`) of our testing data.
  prefs: []
  type: TYPE_NORMAL
- en: Nearest neighbor using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Python, we have very similar steps for producing nearest neighbor estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the packages to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Numpy and pandas are standards. Nearest neighbors is one of the `sklearn` features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we load in our housing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0e325f2b-f9ef-48b4-977e-43f90fb4f279.png)'
  prefs: []
  type: TYPE_IMG
- en: The same data that we saw previously in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see how big it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And break up the data into a `training` and `testing` set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Display their indices and distances. Indices are varying quite a lot. Distances
    seem to be in bands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a nearest neighbors model from the `training` set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: It is interesting that with Python we do not have to store models off separately.
    Methods are stateful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make our predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Determine how well we have predicted the housing price:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We have a mean difference of ¾ versus an average value of `22`. This should
    mean an average percent difference of about 3%, but the average percentage difference
    calculated is close to 10%. So, we are not estimating well under Python.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use decision trees to predict values. A decision tree
    has a logical flow where the user makes decisions based on attributes following
    the tree down to a root level where a classification is then provided.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we are using automobile characteristics, such as vehicle weight,
    to determine whether the vehicle will produce good mileage. The information is
    extracted from the page at [https://alliance.seas.upenn.edu/~cis520/wiki/index.php?n=Lectures.DecisionTrees](https://alliance.seas.upenn.edu/~cis520/wiki/index.php?n=Lectures.DecisionTrees).
    I copied the data out to Excel and then wrote it as a CSV for use in this example.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We load the libraries to use `rpart` and `caret`. `rpart` has the decision
    tree modeling package. `caret` has the data partition function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We load in our `mpg` dataset and split it into a training and testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We develop a model to predict `mpg` acceptability based on the other factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The display is a text display of the decision tree. You can see the decision
    tree graphically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/821221be-1044-4c3d-ab67-3df2a9a1ef5a.png)'
  prefs: []
  type: TYPE_IMG
- en: It appears to be a very simple model. There must have been a change to mileage
    for the 1980 year as that is the main driver for the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we predict values and compare them against our `testing` set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2cc05716-1435-4010-84ac-025c3cd37b73.png)'
  prefs: []
  type: TYPE_IMG
- en: It looks like the package has converted `Bad`, `OK`, and `Good` into a numerical
    equivalent where `1` is `Bad` and others are `OK` or `Good`. Overall, we are not
    sure if we have a good model. There is clearly not much data to work with. A larger
    test set would clear up the model.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can perform the same analysis in Python. Load a number of imports that are
    to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Read in the `mpg` data file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/aba23356-a720-4d52-9562-bf7b5f910d2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Break up the data into factors and results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Split up the data between training and testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a decision tree model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the model fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Graph out the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can model the housing data as a neural network where the different data elements
    are inputs into the system and the output of the network is the house price. With
    a neural net we end up with a graphical model that provides the factors to apply
    to each input in order to arrive at our housing price.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a neural network package available in R. We load that in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Load in the housing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Split up the housing data into training and test sets (we have seen this coding
    in prior examples):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate our `neuralnet` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The display information for the `neuralnet` model is quite extensive. The first
    sets of display are listed as follows. It is unclear if any of these points are
    useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ec97c3f6-3876-4ca4-9f8e-f7ac12d67f25.png)'
  prefs: []
  type: TYPE_IMG
- en: This is just the top half of the graph. As you can see, every factor is adjusted
    into the model to arrive at our housing price. This is not useful—every factor
    cannot be that important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Determine how accurate we are with this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Given the model appears to be very inaccurate I am not sure going through the
    same steps in Python would be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The random forests algorithm attempts a number of random decision trees and
    provides the tree that works best within the parameters used to drive the model.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With R we include the packages we are going to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Split it up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This is one of the more informative displays about a model—we see the model
    explains 87% of the variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make our prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This is one of the lowest sum of squares among the models we produced in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used several machine learning algorithms, some of them in
    R and Python to compare and contrast. We used naive Bayes to determine how the
    data might be used. We applied nearest neighbor in a couple of different ways
    to see our results. We used decision trees to come up with an algorithm for predicting.
    We tried to use neural network to explain housing prices. Finally, we used the
    random forest algorithm to do the same—with the best results!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at optimizing Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
