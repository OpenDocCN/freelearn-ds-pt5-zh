["```py\nscala> val statesDF = spark.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").option(\"sep\", \",\").csv(\"statesPopulation.csv\")\nstatesDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]\n\n```", "```py\nscala> statesDF.printSchema\nroot\n |-- State: string (nullable = true)\n |-- Year: integer (nullable = true)\n |-- Population: integer (nullable = true)\n\n```", "```py\nscala> statesDF.explain(true)\n== Parsed Logical Plan ==\nRelation[State#0,Year#1,Population#2] csv\n== Analyzed Logical Plan ==\nState: string, Year: int, Population: int\nRelation[State#0,Year#1,Population#2] csv\n== Optimized Logical Plan ==\nRelation[State#0,Year#1,Population#2] csv\n== Physical Plan ==\n*FileScan csv [State#0,Year#1,Population#2] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/states.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Year:int,Population:int>\n\n```", "```py\nscala> statesDF.createOrReplaceTempView(\"states\")\n\n```", "```py\nscala> statesDF.show(5)\nscala> spark.sql(\"select * from states limit 5\").show\n+----------+----+----------+\n| State|Year|Population|\n+----------+----+----------+\n| Alabama|2010| 4785492|\n| Alaska|2010| 714031|\n| Arizona|2010| 6408312|\n| Arkansas|2010| 2921995|\n|California|2010| 37332685|\n+----------+----+----------+\n\n```", "```py\nscala> statesDF.sort(col(\"Population\").desc).show(5)\nscala> spark.sql(\"select * from states order by Population desc limit 5\").show\n+----------+----+----------+\n| State|Year|Population|\n+----------+----+----------+\n|California|2016| 39250017|\n|California|2015| 38993940|\n|California|2014| 38680810|\n|California|2013| 38335203|\n|California|2012| 38011074|\n+----------+----+----------+\n\n```", "```py\nscala> statesDF.groupBy(\"State\").sum(\"Population\").show(5)\nscala> spark.sql(\"select State, sum(Population) from states group by State limit 5\").show\n+---------+---------------+\n| State|sum(Population)|\n+---------+---------------+\n| Utah| 20333580|\n| Hawaii| 9810173|\n|Minnesota| 37914011|\n| Ohio| 81020539|\n| Arkansas| 20703849|\n+---------+---------------+\n\n```", "```py\nscala> statesDF.groupBy(\"State\").agg(sum(\"Population\").alias(\"Total\")).show(5)\nscala> spark.sql(\"select State, sum(Population) as Total from states group by State limit 5\").show\n+---------+--------+\n| State| Total|\n+---------+--------+\n| Utah|20333580|\n| Hawaii| 9810173|\n|Minnesota|37914011|\n| Ohio|81020539|\n| Arkansas|20703849|\n+---------+--------+\n\n```", "```py\nscala> statesDF.groupBy(\"State\").agg(sum(\"Population\").alias(\"Total\")).explain(true)\n== Parsed Logical Plan ==\n'Aggregate [State#0], [State#0, sum('Population) AS Total#31886]\n+- Relation[State#0,Year#1,Population#2] csv\n\n== Analyzed Logical Plan ==\nState: string, Total: bigint\nAggregate [State#0], [State#0, sum(cast(Population#2 as bigint)) AS Total#31886L]\n+- Relation[State#0,Year#1,Population#2] csv\n\n== Optimized Logical Plan ==\nAggregate [State#0], [State#0, sum(cast(Population#2 as bigint)) AS Total#31886L]\n+- Project [State#0, Population#2]\n +- Relation[State#0,Year#1,Population#2] csv\n\n== Physical Plan ==\n*HashAggregate(keys=[State#0], functions=[sum(cast(Population#2 as bigint))], output=[State#0, Total#31886L])\n+- Exchange hashpartitioning(State#0, 200)\n +- *HashAggregate(keys=[State#0], functions=[partial_sum(cast(Population#2 as bigint))], output=[State#0, sum#31892L])\n +- *FileScan csv [State#0,Population#2] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/states.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<State:string,Population:int>\n\n```", "```py\nscala> statesDF.groupBy(\"State\").agg(sum(\"Population\").alias(\"Total\")).sort(col(\"Total\").desc).show(5)\nscala> spark.sql(\"select State, sum(Population) as Total from states group by State order by Total desc limit 5\").show\n+----------+---------+\n| State| Total|\n+----------+---------+\n|California|268280590|\n| Texas|185672865|\n| Florida|137618322|\n| New York|137409471|\n| Illinois| 89960023|\n+----------+---------+\n\n```", "```py\nscala> statesDF.groupBy(\"State\").agg(\n             min(\"Population\").alias(\"minTotal\"), \n             max(\"Population\").alias(\"maxTotal\"),        \n             avg(\"Population\").alias(\"avgTotal\"))\n           .sort(col(\"minTotal\").desc).show(5) \nscala> spark.sql(\"select State, min(Population) as minTotal, max(Population) as maxTotal, avg(Population) as avgTotal from states group by State order by minTotal desc limit 5\").show\n+----------+--------+--------+--------------------+\n| State|minTotal|maxTotal| avgTotal|\n+----------+--------+--------+--------------------+\n|California|37332685|39250017|3.8325798571428575E7|\n| Texas|25244310|27862596| 2.6524695E7|\n| New York|19402640|19747183| 1.962992442857143E7|\n| Florida|18849098|20612439|1.9659760285714287E7|\n| Illinois|12801539|12879505|1.2851431857142856E7|\n+----------+--------+--------+--------------------+\n\n```", "```py\nscala> statesDF.groupBy(\"State\").pivot(\"Year\").sum(\"Population\").show(5)\n+---------+--------+--------+--------+--------+--------+--------+--------+\n| State| 2010| 2011| 2012| 2013| 2014| 2015| 2016|\n+---------+--------+--------+--------+--------+--------+--------+--------+\n| Utah| 2775326| 2816124| 2855782| 2902663| 2941836| 2990632| 3051217|\n| Hawaii| 1363945| 1377864| 1391820| 1406481| 1416349| 1425157| 1428557|\n|Minnesota| 5311147| 5348562| 5380285| 5418521| 5453109| 5482435| 5519952|\n| Ohio|11540983|11544824|11550839|11570022|11594408|11605090|11614373|\n| Arkansas| 2921995| 2939493| 2950685| 2958663| 2966912| 2977853| 2988248|\n+---------+--------+--------+--------+--------+--------+--------+--------+\n\n```", "```py\nscala> statesDF.filter(\"State == 'California'\").explain(true)\n\n== Parsed Logical Plan ==\n'Filter ('State = California)\n+- Relation[State#0,Year#1,Population#2] csv\n\n== Analyzed Logical Plan ==\nState: string, Year: int, Population: int\nFilter (State#0 = California)\n+- Relation[State#0,Year#1,Population#2] csv\n\n== Optimized Logical Plan ==\nFilter (isnotnull(State#0) && (State#0 = California))\n+- Relation[State#0,Year#1,Population#2] csv\n\n== Physical Plan ==\n*Project [State#0, Year#1, Population#2]\n+- *Filter (isnotnull(State#0) && (State#0 = California))\n +- *FileScan csv [State#0,Year#1,Population#2] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/states.csv], PartitionFilters: [], PushedFilters: [IsNotNull(State), EqualTo(State,California)], ReadSchema: struct<State:string,Year:int,Population:int>\n\n```", "```py\nscala> statesDF.filter(\"State == 'California'\").show\n+----------+----+----------+\n| State|Year|Population|\n+----------+----+----------+\n|California|2010| 37332685|\n|California|2011| 37676861|\n|California|2012| 38011074|\n|California|2013| 38335203|\n|California|2014| 38680810|\n|California|2015| 38993940|\n|California|2016| 39250017|\n+----------+----+----------+\n\n```", "```py\nimport org.apache.spark.sql.functions._\n\nscala> val toUpper: String => String = _.toUpperCase\ntoUpper: String => String = <function1>\n\n```", "```py\nscala> val toUpperUDF = udf(toUpper)\ntoUpperUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\n\n```", "```py\nscala> statesDF.withColumn(\"StateUpperCase\", toUpperUDF(col(\"State\"))).show(5)\n+----------+----+----------+--------------+\n| State|Year|Population|StateUpperCase|\n+----------+----+----------+--------------+\n| Alabama|2010| 4785492| ALABAMA|\n| Alaska|2010| 714031| ALASKA|\n| Arizona|2010| 6408312| ARIZONA|\n| Arkansas|2010| 2921995| ARKANSAS|\n|California|2010| 37332685| CALIFORNIA|\n+----------+----+----------+--------------+\n\n```", "```py\nscala> val statesDF = spark.read.option(\"header\", \"true\")\n                                .option(\"inferschema\", \"true\")\n                                .option(\"sep\", \",\")\n                                .csv(\"statesPopulation.csv\")\nstatesDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]\n\nscala> statesDF.schema\nres92: org.apache.spark.sql.types.StructType = StructType(\n                                                  StructField(State,StringType,true),\n                                                  StructField(Year,IntegerType,true),\n                                                  StructField(Population,IntegerType,true))\nscala> statesDF.printSchema\nroot\n |-- State: string (nullable = true)\n |-- Year: integer (nullable = true)\n |-- Population: integer (nullable = true)\n\n```", "```py\nscala> import org.apache.spark.sql.types.{StructType, IntegerType, StringType}\nimport org.apache.spark.sql.types.{StructType, IntegerType, StringType}\n\n```", "```py\nscala> val schema = new StructType().add(\"i\", IntegerType).add(\"s\", StringType)\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(i,IntegerType,true), StructField(s,StringType,true))\n\n```", "```py\nscala> schema.printTreeString\nroot\n |-- i: integer (nullable = true)\n |-- s: string (nullable = true)\n\n```", "```py\nscala> schema.prettyJson\nres85: String =\n{\n \"type\" : \"struct\",\n \"fields\" : [ {\n \"name\" : \"i\",\n \"type\" : \"integer\",\n \"nullable\" : true,\n \"metadata\" : { }\n }, {\n \"name\" : \"s\",\n \"type\" : \"string\",\n \"nullable\" : true,\n \"metadata\" : { }\n } ]\n}\n\n```", "```py\nimport org.apache.spark.sql.types._\n\n```", "```py\nimport org.apache.spark.sql.Encoders \n\n```", "```py\n\nscala> Encoders.product[(Integer, String)].schema.printTreeString\nroot\n |-- _1: integer (nullable = true)\n |-- _2: string (nullable = true)\n\n```", "```py\nscala> case class Record(i: Integer, s: String)\ndefined class Record\n\n```", "```py\nscala> Encoders.product[Record].schema.printTreeString\nroot\n |-- i: integer (nullable = true)\n |-- s: string (nullable = true)\n\n```", "```py\nimport org.apache.spark.sql.types._\n\n```", "```py\nscala> import org.apache.spark.sql.types.DataTypes\nimport org.apache.spark.sql.types.DataTypes\n\nscala> val arrayType = DataTypes.createArrayType(IntegerType)\narrayType: org.apache.spark.sql.types.ArrayType = ArrayType(IntegerType,true)\n\n```", "```py\nscala> val statesPopulationDF = spark.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").option(\"sep\", \",\").csv(\"statesPopulation.csv\")\nstatesPopulationDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]\n\nscala> val statesTaxRatesDF = spark.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").option(\"sep\", \",\").csv(\"statesTaxRates.csv\")\nstatesTaxRatesDF: org.apache.spark.sql.DataFrame = [State: string, TaxRate: double]\n\n```", "```py\nscala> statesPopulationDF.write.option(\"header\", \"true\").csv(\"statesPopulation_dup.csv\")\n\nscala> statesTaxRatesDF.write.option(\"header\", \"true\").csv(\"statesTaxRates_dup.csv\")\n\n```", "```py\nval statesPopulationDF = spark.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").option(\"sep\", \",\").csv(\"statesPopulation.csv\")\n\n```", "```py\ndef count(columnName: String): TypedColumn[Any, Long]\nAggregate function: returns the number of items in a group.\n\ndef count(e: Column): Column\nAggregate function: returns the number of items in a group.\n\ndef countDistinct(columnName: String, columnNames: String*): Column\nAggregate function: returns the number of distinct items in a group.\n\ndef countDistinct(expr: Column, exprs: Column*): Column\nAggregate function: returns the number of distinct items in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(col(\"*\")).agg(count(\"State\")).show\nscala> statesPopulationDF.select(count(\"State\")).show\n+------------+\n|count(State)|\n+------------+\n| 350|\n+------------+\n\nscala> statesPopulationDF.select(col(\"*\")).agg(countDistinct(\"State\")).show\nscala> statesPopulationDF.select(countDistinct(\"State\")).show\n+---------------------+\n|count(DISTINCT State)|\n+---------------------+\n| 50|\n\n```", "```py\ndef first(columnName: String): Column\nAggregate function: returns the first value of a column in a group.\n\ndef first(e: Column): Column\nAggregate function: returns the first value in a group.\n\ndef first(columnName: String, ignoreNulls: Boolean): Column\nAggregate function: returns the first value of a column in a group.\n\ndef first(e: Column, ignoreNulls: Boolean): Column\nAggregate function: returns the first value in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(first(\"State\")).show\n+-------------------+\n|first(State, false)|\n+-------------------+\n| Alabama|\n+-------------------+\n\n```", "```py\ndef last(columnName: String): Column\nAggregate function: returns the last value of the column in a group.\n\ndef last(e: Column): Column\nAggregate function: returns the last value in a group.\n\ndef last(columnName: String, ignoreNulls: Boolean): Column\nAggregate function: returns the last value of the column in a group.\n\ndef last(e: Column, ignoreNulls: Boolean): Column\nAggregate function: returns the last value in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(last(\"State\")).show\n+------------------+\n|last(State, false)|\n+------------------+\n| Wyoming|\n+------------------+\n\n```", "```py\ndef approx_count_distinct(columnName: String, rsd: Double): Column\nAggregate function: returns the approximate number of distinct items in a group.\n\ndef approx_count_distinct(e: Column, rsd: Double): Column\nAggregate function: returns the approximate number of distinct items in a group.\n\ndef approx_count_distinct(columnName: String): Column\nAggregate function: returns the approximate number of distinct items in a group.\n\ndef approx_count_distinct(e: Column): Column\nAggregate function: returns the approximate number of distinct items in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(col(\"*\")).agg(approx_count_distinct(\"State\")).show\n+----------------------------+\n|approx_count_distinct(State)|\n+----------------------------+\n| 48|\n+----------------------------+\n\nscala> statesPopulationDF.select(approx_count_distinct(\"State\", 0.2)).show\n+----------------------------+\n|approx_count_distinct(State)|\n+----------------------------+\n| 49|\n+----------------------------+\n\n```", "```py\ndef min(columnName: String): Column\nAggregate function: returns the minimum value of the column in a group.\n\ndef min(e: Column): Column\nAggregate function: returns the minimum value of the expression in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(min(\"Population\")).show\n+---------------+\n|min(Population)|\n+---------------+\n| 564513|\n+---------------+\n\n```", "```py\ndef max(columnName: String): Column\nAggregate function: returns the maximum value of the column in a group.\n\ndef max(e: Column): Column\nAggregate function: returns the maximum value of the expression in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(max(\"Population\")).show\n+---------------+\n|max(Population)|\n+---------------+\n| 39250017|\n+---------------+\n\n```", "```py\ndef avg(columnName: String): Column\nAggregate function: returns the average of the values in a group.\n\ndef avg(e: Column): Column\nAggregate function: returns the average of the values in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(avg(\"Population\")).show\n+-----------------+\n| avg(Population)|\n+-----------------+\n|6253399.371428572|\n+-----------------+\n\n```", "```py\ndef sum(columnName: String): Column\nAggregate function: returns the sum of all values in the given column.\n\ndef sum(e: Column): Column\nAggregate function: returns the sum of all values in the expression.\n\ndef sumDistinct(columnName: String): Column\nAggregate function: returns the sum of distinct values in the expression\n\ndef sumDistinct(e: Column): Column\nAggregate function: returns the sum of distinct values in the expression.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(sum(\"Population\")).show\n+---------------+\n|sum(Population)|\n+---------------+\n| 2188689780|\n+---------------+\n\n```", "```py\ndef kurtosis(columnName: String): Column\nAggregate function: returns the kurtosis of the values in a group.\n\ndef kurtosis(e: Column): Column\nAggregate function: returns the kurtosis of the values in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(kurtosis(\"Population\")).show\n+--------------------+\n|kurtosis(Population)|\n+--------------------+\n| 7.727421920829375|\n+--------------------+\n\n```", "```py\ndef skewness(columnName: String): Column\nAggregate function: returns the skewness of the values in a group.\n\ndef skewness(e: Column): Column\nAggregate function: returns the skewness of the values in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(skewness(\"Population\")).show\n+--------------------+\n|skewness(Population)|\n+--------------------+\n| 2.5675329049100024|\n+--------------------+\n\n```", "```py\ndef var_pop(columnName: String): Column\nAggregate function: returns the population variance of the values in a group.\n\ndef var_pop(e: Column): Column\nAggregate function: returns the population variance of the values in a group.\n\ndef var_samp(columnName: String): Column\nAggregate function: returns the unbiased variance of the values in a group.\n\ndef var_samp(e: Column): Column\nAggregate function: returns the unbiased variance of the values in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(var_pop(\"Population\")).show\n+--------------------+\n| var_pop(Population)|\n+--------------------+\n|4.948359064356177E13|\n+--------------------+\n\n```", "```py\ndef stddev(columnName: String): Column\nAggregate function: alias for stddev_samp.\n\ndef stddev(e: Column): Column\nAggregate function: alias for stddev_samp.\n\ndef stddev_pop(columnName: String): Column\nAggregate function: returns the population standard deviation of the expression in a group.\n\ndef stddev_pop(e: Column): Column\nAggregate function: returns the population standard deviation of the expression in a group.\n\ndef stddev_samp(columnName: String): Column\nAggregate function: returns the sample standard deviation of the expression in a group.\n\ndef stddev_samp(e: Column): Column\nAggregate function: returns the sample standard deviation of the expression in a group.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(stddev(\"Population\")).show\n+-----------------------+\n|stddev_samp(Population)|\n+-----------------------+\n| 7044528.191173398|\n+-----------------------+\n\n```", "```py\ndef covar_pop(columnName1: String, columnName2: String): Column\nAggregate function: returns the population covariance for two columns.\n\ndef covar_pop(column1: Column, column2: Column): Column\nAggregate function: returns the population covariance for two columns.\n\ndef covar_samp(columnName1: String, columnName2: String): Column\nAggregate function: returns the sample covariance for two columns.\n\ndef covar_samp(column1: Column, column2: Column): Column\nAggregate function: returns the sample covariance for two columns.\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(covar_pop(\"Year\", \"Population\")).show\n+---------------------------+\n|covar_pop(Year, Population)|\n+---------------------------+\n| 183977.56000006935|\n+---------------------------+\n\n```", "```py\nscala> statesPopulationDF.groupBy(\"State\").count.show(5)\n+---------+-----+\n| State|count|\n+---------+-----+\n| Utah| 7|\n| Hawaii| 7|\n|Minnesota| 7|\n| Ohio| 7|\n| Arkansas| 7|\n+---------+-----+\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.groupBy(\"State\").agg(min(\"Population\"), avg(\"Population\")).show(5)\n+---------+---------------+--------------------+\n| State|min(Population)| avg(Population)|\n+---------+---------------+--------------------+\n| Utah| 2775326| 2904797.1428571427|\n| Hawaii| 1363945| 1401453.2857142857|\n|Minnesota| 5311147| 5416287.285714285|\n| Ohio| 11540983|1.1574362714285715E7|\n| Arkansas| 2921995| 2957692.714285714|\n+---------+---------------+--------------------+\n\n```", "```py\nscala> statesPopulationDF.rollup(\"State\", \"Year\").count.show(5)\n+------------+----+-----+\n| State|Year|count|\n+------------+----+-----+\n|South Dakota|2010| 1|\n| New York|2012| 1|\n| California|2014| 1|\n| Wyoming|2014| 1|\n| Hawaii|null| 7|\n+------------+----+-----+\n\n```", "```py\nscala> statesPopulationDF.cube(\"State\", \"Year\").count.show(5)\n+------------+----+-----+\n| State|Year|count|\n+------------+----+-----+\n|South Dakota|2010| 1|\n| New York|2012| 1|\n| null|2014| 50|\n| Wyoming|2014| 1|\n| Hawaii|null| 7|\n+------------+----+-----+\n\n```", "```py\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions.col import org.apache.spark.sql.functions.max\n\n```", "```py\n val windowSpec = Window\n .partitionBy(\"State\")\n .orderBy(col(\"Population\").desc)\n .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(col(\"State\"), col(\"Year\"), max(\"Population\").over(windowSpec), rank().over(windowSpec)).sort(\"State\", \"Year\").show(10)\n+-------+----+------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n| State|Year|max(Population) OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|RANK() OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|\n+-------+----+------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n|Alabama|2010| 4863300| 6|\n|Alabama|2011| 4863300| 7|\n|Alabama|2012| 4863300| 5|\n|Alabama|2013| 4863300| 4|\n|Alabama|2014| 4863300| 3|\n\n```", "```py\nimport org.apache.spark.sql.functions._\nscala> statesPopulationDF.select(col(\"State\"), col(\"Year\"), ntile(2).over(windowSpec), rank().over(windowSpec)).sort(\"State\", \"Year\").show(10)\n+-------+----+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n| State|Year|ntile(2) OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|RANK() OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|\n+-------+----+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+\n|Alabama|2010| 2| 6|\n|Alabama|2011| 2| 7|\n|Alabama|2012| 2| 5|\n|Alabama|2013| 1| 4|\n|Alabama|2014| 1| 3|\n|Alabama|2015| 1| 2|\n|Alabama|2016| 1| 1|\n| Alaska|2010| 2| 7|\n| Alaska|2011| 2| 6|\n| Alaska|2012| 2| 5|\n+-------+----+-----------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------\n\n```", "```py\njoin(right: dataset[_]): DataFrame\nCondition-less inner join\n\njoin(right: dataset[_], usingColumn: String): DataFrame\nInner join with a single column\n\njoin(right: dataset[_], usingColumns: Seq[String]): DataFrame \nInner join with multiple columns\n\njoin(right: dataset[_], usingColumns: Seq[String], joinType: String): DataFrame\nJoin with multiple columns and a join type (inner, outer,....)\n\njoin(right: dataset[_], joinExprs: Column): DataFrame\nInner Join using a join expression\n\njoin(right: dataset[_], joinExprs: Column, joinType: String): DataFrame \nJoin using a Join expression and a join type (inner, outer, ...)\n\n```", "```py\ndef   join(right: dataset[_], joinExprs: Column, joinType: String): DataFrame Join with another DataFrame using the given join expression\n\nright: Right side of the join.\njoinExprs: Join expression.\njoinType : Type of join to perform. Default is *inner* join\n\n// Scala:\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\ndf1.join(df2, $\"df1Key\" === $\"df2Key\", \"outer\") \n\n```", "```py\nscala> val statesPopulationDF = spark.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").option(\"sep\", \",\").csv(\"statesPopulation.csv\")\nstatesPopulationDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]\n\nscala> val statesTaxRatesDF = spark.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").option(\"sep\", \",\").csv(\"statesTaxRates.csv\")\nstatesTaxRatesDF: org.apache.spark.sql.DataFrame = [State: string, TaxRate: double]\n\nscala> statesPopulationDF.count\nres21: Long = 357\n\nscala> statesTaxRatesDF.count\nres32: Long = 47\n\n%sql\nstatesPopulationDF.createOrReplaceTempView(\"statesPopulationDF\")\nstatesTaxRatesDF.createOrReplaceTempView(\"statesTaxRatesDF\")\n\n```", "```py\nval joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF(\"State\") === statesTaxRatesDF(\"State\"), \"inner\")\n\n%sql\nval joinDF = spark.sql(\"SELECT * FROM statesPopulationDF INNER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State\")\n\nscala> joinDF.count\nres22: Long = 329\n\nscala> joinDF.show\n+--------------------+----+----------+--------------------+-------+\n| State|Year|Population| State|TaxRate|\n+--------------------+----+----------+--------------------+-------+\n| Alabama|2010| 4785492| Alabama| 4.0|\n| Arizona|2010| 6408312| Arizona| 5.6|\n| Arkansas|2010| 2921995| Arkansas| 6.5|\n| California|2010| 37332685| California| 7.5|\n| Colorado|2010| 5048644| Colorado| 2.9|\n| Connecticut|2010| 3579899| Connecticut| 6.35|\n\n```", "```py\nscala> joinDF.explain\n== Physical Plan ==\n*BroadcastHashJoin [State#570], [State#577], Inner, BuildRight\n:- *Project [State#570, Year#571, Population#572]\n: +- *Filter isnotnull(State#570)\n: +- *FileScan csv [State#570,Year#571,Population#572] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/spark-2.1.0-bin-hadoop2.7/statesPopulation.csv], PartitionFilters: [], PushedFilters: [IsNotNull(State)], ReadSchema: struct<State:string,Year:int,Population:int>\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n +- *Project [State#577, TaxRate#578]\n +- *Filter isnotnull(State#577)\n +- *FileScan csv [State#577,TaxRate#578] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/spark-2.1.0-bin-hadoop2.7/statesTaxRates.csv], PartitionFilters: [], PushedFilters: [IsNotNull(State)], ReadSchema: struct<State:string,TaxRate:double>\n\n```", "```py\nval joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF(\"State\") === statesTaxRatesDF(\"State\"), \"leftouter\")\n\n%sql\nval joinDF = spark.sql(\"SELECT * FROM statesPopulationDF LEFT OUTER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State\")\n\nscala> joinDF.count\nres22: Long = 357\n\nscala> joinDF.show(5)\n+----------+----+----------+----------+-------+\n| State|Year|Population| State|TaxRate|\n+----------+----+----------+----------+-------+\n| Alabama|2010| 4785492| Alabama| 4.0|\n| Alaska|2010| 714031| null| null|\n| Arizona|2010| 6408312| Arizona| 5.6|\n| Arkansas|2010| 2921995| Arkansas| 6.5|\n|California|2010| 37332685|California| 7.5|\n+----------+----+----------+----------+-------+\n\n```", "```py\nval joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF(\"State\") === statesTaxRatesDF(\"State\"), \"rightouter\")\n\n%sql\nval joinDF = spark.sql(\"SELECT * FROM statesPopulationDF RIGHT OUTER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State\")\n\nscala> joinDF.count\nres22: Long = 323\n\nscala> joinDF.show\n+--------------------+----+----------+--------------------+-------+\n| State|Year|Population| State|TaxRate|\n+--------------------+----+----------+--------------------+-------+\n| Colorado|2011| 5118360| Colorado| 2.9|\n| Colorado|2010| 5048644| Colorado| 2.9|\n| null|null| null|Connecticut| 6.35|\n| Florida|2016| 20612439| Florida| 6.0|\n| Florida|2015| 20244914| Florida| 6.0|\n| Florida|2014| 19888741| Florida| 6.0|\n\n```", "```py\nval joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF(\"State\") === statesTaxRatesDF(\"State\"), \"fullouter\")\n\n%sql\nval joinDF = spark.sql(\"SELECT * FROM statesPopulationDF FULL OUTER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State\")\n\nscala> joinDF.count\nres22: Long = 351\n\nscala> joinDF.show\n+--------------------+----+----------+--------------------+-------+\n| State|Year|Population| State|TaxRate|\n+--------------------+----+----------+--------------------+-------+\n| Delaware|2010| 899816| null| null|\n| Delaware|2011| 907924| null| null|\n| West Virginia|2010| 1854230| West Virginia| 6.0|\n| West Virginia|2011| 1854972| West Virginia| 6.0|\n| Missouri|2010| 5996118| Missouri| 4.225|\n| null|null| null|  Connecticut|   6.35|\n\n```", "```py\nval joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF(\"State\") === statesTaxRatesDF(\"State\"), \"leftanti\")\n\n%sql\nval joinDF = spark.sql(\"SELECT * FROM statesPopulationDF LEFT ANTI JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State\")\n\nscala> joinDF.count\nres22: Long = 28\n\nscala> joinDF.show(5)\n+--------+----+----------+\n| State|Year|Population|\n+--------+----+----------+\n| Alaska|2010| 714031|\n|Delaware|2010| 899816|\n| Montana|2010| 990641|\n| Oregon|2010| 3838048|\n| Alaska|2011| 722713|\n+--------+----+----------+\n\n```", "```py\nval joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF(\"State\") === statesTaxRatesDF(\"State\"), \"leftsemi\")\n\n%sql\nval joinDF = spark.sql(\"SELECT * FROM statesPopulationDF LEFT SEMI JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State\")\n\nscala> joinDF.count\nres22: Long = 322\n\nscala> joinDF.show(5)\n+----------+----+----------+\n| State|Year|Population|\n+----------+----+----------+\n| Alabama|2010| 4785492|\n| Arizona|2010| 6408312|\n| Arkansas|2010| 2921995|\n|California|2010| 37332685|\n| Colorado|2010| 5048644|\n+----------+----+----------+\n\n```", "```py\nscala> val joinDF=statesPopulationDF.crossJoin(statesTaxRatesDF)\njoinDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 3 more fields]\n\n%sql\nval joinDF = spark.sql(\"SELECT * FROM statesPopulationDF CROSS JOIN statesTaxRatesDF\")\n\nscala> joinDF.count\nres46: Long = 16450\n\nscala> joinDF.show(10)\n+-------+----+----------+-----------+-------+\n| State|Year|Population| State|TaxRate|\n+-------+----+----------+-----------+-------+\n|Alabama|2010| 4785492| Alabama| 4.0|\n|Alabama|2010| 4785492| Arizona| 5.6|\n|Alabama|2010| 4785492| Arkansas| 6.5|\n|Alabama|2010| 4785492| California| 7.5|\n|Alabama|2010| 4785492| Colorado| 2.9|\n|Alabama|2010| 4785492|Connecticut| 6.35|\n|Alabama|2010| 4785492| Florida| 6.0|\n|Alabama|2010| 4785492| Georgia| 4.0|\n|Alabama|2010| 4785492| Hawaii| 4.0|\n|Alabama|2010| 4785492| Idaho| 6.0|\n+-------+----+----------+-----------+-------+\n\n```"]