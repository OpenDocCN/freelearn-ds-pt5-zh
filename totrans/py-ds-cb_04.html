<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch04" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 4. Data Analysis – Deep Dive</h1></div></div></div><p class="calibre11">In this chapter, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem">Extracting the principal components</li><li class="listitem">Using Kernel PCA</li><li class="listitem">Extracting features using Singular Value Decomposition</li><li class="listitem">Reducing the data dimension with Random Projection</li><li class="listitem">Decomposing Feature matrices using <strong class="calibre12">NMF</strong> (<strong class="calibre12">Non-negative Matrix Factorization</strong>)</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec54" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Introduction</h1></div></div></div><p class="calibre11">In this chapter, we will look at recipes dealing with dimensionality reduction. In the previous chapter, we looked at how to surf through the data to understand its characteristics in order to put it to meaningful use. We restricted our discussions to only bivariate data. Imagine a dataset with hundreds of columns; how do we proceed with the analysis of the data characteristics of such a large dimensional dataset? We need efficient tools to handle this hurdle as we work our way through the data.</p><p class="calibre11">Today, high-dimensional data is everywhere. Consider building a product recommendation engine for a moderately-sized e-commerce website. Even with the range of thousands of products, the number of variables to consider very high. Bioinformatics is another area with very high-dimensional data. Gene expression are microarray datasets could contain tens of thousands of dimensions.</p><p class="calibre11">If your task at hand is to<a id="id281" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> either explore the data or prepare the data for an algorithm, the high dimensionality, popularly called the <em class="calibre15">curse of dimensionality</em>, is a big roadblock. We need efficient methods to handle this. Additionally, the complexity of many existing data mining algorithms increases exponentially with the increase in the number of dimensions. With increasing dimensions, the algorithms become computationally infeasible and thus inapplicable in many applications.</p><p class="calibre11">Dimensionality reduction techniques preserve the structure of the data as much as possible <a id="id282" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>while reducing the number of dimensions. Thus, in the reduced feature space, the execution time of the algorithms is reduced as we have lower dimensions. As the structure of data is preserved, the results obtained can be a reliable approximation of the original data space. By preserving the structure, we mean two things; the first is not tampering with the variations in the original dataset and the second is preserving the distance between the data vectors in the new projected space.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl3sec03" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Matrix Decomposition:</h2></div></div></div><p class="calibre11">Matrix decomposition yields several techniques for dimensionality reduction. Our data is typically a<a id="id283" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> matrix with the instances in rows and features in columns. In the previous recipes, we have been storing our data as NumPy matrices all the way. For example, in the Iris dataset, our tuples or data instances were represented as rows and the features, which included sepal and petal width and length, were the columns of the matrix.</p><p class="calibre11">Matrix decomposition is a way of expressing a matrix. Say that A is a product of two other matrices, B and C. The matrix B is supposed to contain vectors that can explain the direction of variation in the data. The matrix C is supposed to contain the magnitude of this variation. Thus, our original matrix A is now expressed as a linear combination of B and C.</p><p class="calibre11">The techniques that we will see in the coming sections exploit matrix decomposition in order to tackle the dimensionality reduction. There are methods that insist that the basic vectors have to be orthogonal to each other, such as the principal component analysis, and there are some that don't insist on this requirement, such as dictionary learning.</p><p class="calibre11">Let's buckle up and see some of these techniques in action in this chapter.</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec55" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Extracting the principal components</h1></div></div></div><p class="calibre11">The first technique that we will look at is the <strong class="calibre12">Principal Component Analysis </strong>(<strong class="calibre12">PCA</strong>). PCA is an<a id="id284" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> unsupervised method. In multivariate<a id="id285" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> problems, PCA is used to reduce the dimension of the data with minimal information loss, in other words, retaining the maximum variation in the data. By variation, we mean the direction in which the data is dispersed to the maximum. Let's look at the following plot:</p><div><img src="img/B04041_04_24.jpg" alt="Extracting the principal components" class="calibre57"/></div><p class="calibre11">We have a scatter plot of two variables, <em class="calibre15">x1</em> and <em class="calibre15">x2</em>. The diagonal line indicates the maximum variation. By <a id="id286" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>using PCA, our intent is to capture this direction of the variation. So, instead of using the direction of two variables,<em class="calibre15"> x1</em> and <em class="calibre15">x2</em>, to represent this data, the quest is to find a vector represented by the blue line and represent the data with only this vector. Essentially we want to reduce the dimension of the data from two to one.</p><p class="calibre11">We will leverage the mathematical tools Eigenvalues and Eigenvectors to find this blue line vector.</p><p class="calibre11">We saw in the previous chapter that the variance measures the amount of dispersion or spread in the data. What we saw was an example in one dimension. In case of more than one dimension it is easy to express correlation among the variables as a matrix, called as Covariance matrix.  When the values of the Covariance matrix are normalized by standard deviation we get a Correlation matrix. In our case, the covariance matrix is a 2 X 2 matrix for two variables, <em class="calibre15">x1</em> and <em class="calibre15">x2</em>, and it measures how much these two variables move in the same direction or generally vary together.</p><p class="calibre11">When we perform Eigenvalue decomposition, that is, get the Eigenvectors and Eigenvalues of the covariance matrix, the principal Eigenvector, which is the vector with the largest Eigenvalue, is in the direction of the maximum variance in the original data.</p><p class="calibre11">In our example, this should be the vector that is represented by the blue line in our graph. We will then proceed to project our input data in this blue line vector in order to get the reduced dimension.</p><div><div><h3 class="title4"><a id="note19" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">With a dataset (n x m) with n instances and m dimensions, PCA projects it onto a smaller subspace (n x d), where d &lt;&lt; m.</p><p class="calibre19">A point to note is that PCA is computationally very expensive.</p></div></div><p class="calibre11">PCA can be <a id="id287" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>performed on both the covariance and correlation matrix. Remember when a Covariance matrix of a dataset with unevenly scaled datasets is used in PCA, the results may not be very useful. Curious readers can refer to the Book A First Course in Multivariate Statistics by Bernard Flury on the topic of using either correlation or covariance matrix for PCA.</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.springer.com/us/book/9780387982069">http://www.springer.com/us/book/9780387982069</a>.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec186" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Let's use the Iris dataset to understand how to use PCA efficiently in reducing the dimension of the dataset. The Iris dataset contains measurements for 150 iris flowers from three different species.</p><p class="calibre11">The three classes in the Iris dataset are as follows:</p><div><ul class="itemizedlist"><li class="listitem">Iris Setosa</li><li class="listitem">Iris Versicolor</li><li class="listitem">Iris Virginica</li></ul></div><p class="calibre11">The following are the four features in the Iris dataset:</p><div><ul class="itemizedlist"><li class="listitem">The sepal length in cm</li><li class="listitem">The sepal width in cm</li><li class="listitem">The petal length in cm</li><li class="listitem">The petal width in cm</li></ul></div><p class="calibre11">Can we use, say, two columns instead of all the four to express most of the variations in the data? Our quest is to reduce the dimension of the data. In this case, our instances have four columns. Let's say that we are building a classifier to predict the type of flower with a new instance; can we do this task using instances in the reduced dimension space? Can we reduce the number of columns from four to two and still achieve a good accuracy for our classifier?</p><p class="calibre11">PCA is done<a id="id288" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> using the following steps:</p><div><ol class="orderedlist"><li class="listitem1">Standardize the dataset to have a zero mean value.</li><li class="listitem1">Find the correlation matrix for the dataset and unit standard deviation value.</li><li class="listitem1">Reduce the Correlation matrix matrix into its Eigenvectors and values.</li><li class="listitem1">Select the top nEigenvectors based on the Eigenvalues sorted in descending order.</li><li class="listitem1">Project the input Eigenvectors matrix into the new subspace.</li></ol></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec187" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the necessary libraries and call the utility function <code class="literal">load_iris</code> from scikit-learn to get the Iris dataset:</p><div><pre class="programlisting">import numpy as np
from sklearn.datasets import load_iris
from sklearn.preprocessing import scale
import scipy
import matplotlib.pyplot as plt

# Load Iris data
data = load_iris()
x = data['data']
y = data['target']

# Since PCA is an unsupervised method, we will not be using the target variable y
# scale the data such that mean = 0 and standard deviation = 1
x_s = scale(x,with_mean=True,with_std=True,axis=0)

# Calculate correlation matrix
x_c = np.corrcoef(x_s.T)

# Find eigen value and eigen vector from correlation matrix
eig_val,r_eig_vec = scipy.linalg.eig(x_c)
print 'Eigen values \n%s'%(eig_val)
print '\n Eigen vectors \n%s'%(r_eig_vec)


# Select the first two eigen vectors.
w = r_eig_vec[:,0:2]

# # Project the dataset in to the dimension
# from 4 dimension to 2 using the right eignen vector
x_rd = x_s.dot(w)

# Scatter plot the new two dimensions
plt.figure(1)
plt.scatter(x_rd[:,0],x_rd[:,1],c=y)
plt.xlabel("Component 1")
plt.ylabel("Component 2")</pre></div><p class="calibre11">Now, we will proceed to Standardize this data, with a zero mean and standard deviation of one, we will leverage the <code class="literal">numpyscorr_coef</code> function to find the correlation matrix:</p><div><pre class="programlisting">x_s = scale(x,with_mean=True,with_std=True,axis=0)
x_c = np.corrcoef(x_s.T)</pre></div><p class="calibre11">We will then do the Eigenvalue decomposition and project our Iris data on the first two principal Eigenvectors. Finally, we will plot the dataset in the reduced space:</p><div><pre class="programlisting">eig_val,r_eig_vec = scipy.linalg.eig(x_c)
print 'Eigen values \n%s'%(eig_val)
print '\n Eigen vectors \n%s'%(r_eig_vec)
# Select the first two eigen vectors.
w = r_eig_vec[:,0:2]

# # Project the dataset in to the dimension
# from 4 dimension to 2 using the right eignen vector
x_rd = x_s.dot(w)

# Scatter plot the new two dimensions
plt.figure(1)
plt.scatter(x_rd[:,0],x_rd[:,1],c=y)
plt.xlabel("Component 1")
plt.ylabel("Component 2")</pre></div><p class="calibre11">Using function scale. The scale function can perform centering, scaling and standardization. Centering is subtracting the mean value from individual values, Scaling is dividing each value by the variable's standard deviation and finally Standardization is performing centering followed by scaling. Using variables with_mean and with_std function scale can be used to perform all three normalization techniques.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec188" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">The Iris <a id="id289" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>dataset has four columns. Though there are not many columns, it will serve our purpose. We intend to reduce the dimensionality of the Iris dataset to two from four and still retain all the information about the data.</p><p class="calibre11">We will load the Iris data to the <code class="literal">x</code> and <code class="literal">y </code>variables using the convenient <code class="literal">load_iris </code>function from scikit-learn. The <code class="literal">x </code>variable is our data matrix and we can inspect its shape as follows:</p><div><pre class="programlisting">&gt;&gt;&gt;x.shape
(150, 4)
&gt;&gt;&gt;</pre></div><p class="calibre11">We will scale the data matrix <code class="literal">x</code> to have zero mean and unit standard deviation. The rule of thumb is that if all your columns are measured in the same scale in your data and have the same unit of measurement, you don't have to scale the data. This will allow PCA to capture these basic units with the maximum variation:</p><div><pre class="programlisting">x_s = scale(x,with_mean=True,with_std=True,axis=0)</pre></div><p class="calibre11">We will proceed to build the correlation matrix of our input data:</p><p class="calibre11">
<em class="calibre15">The correlation matrix of n random variables X1, ..., Xn is then × n matrix whosei, jentry is corr (Xi, Xj), Wikipedia.</em>
</p><p class="calibre11">We will then use the SciPy library to calculate the Eigenvalues and Eigenvectors of the matrix.Let's look at our Eigenvalues and Eigenvectors:</p><div><pre class="programlisting">print Eigen values \n%s%(eig_val)
print \n Eigen vectors \n%s%(r_eig_vec)</pre></div><p class="calibre11">The output looks as follows:</p><div><img src="img/B04041_04_26.jpg" alt="How it works…" class="calibre58"/></div><p class="calibre11">In our case, the Eigenvalues are printed in a descending order. A key question is how many components should we choose? In the next section, we will explain a few ways of choosing the number of components.</p><p class="calibre11">You can see that<a id="id290" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> we selected only the first two columns of our right-hand side Eigenvectors. The discrimination capability of the retained components on the <code class="literal">y</code> variable is a good test of how much information or variation is retained in the data.</p><p class="calibre11">We will project the data to the new reduced dimension.</p><p class="calibre11">Finally, we will plot the components in the <code class="literal">x</code> and <code class="literal">y</code> axes and color them by the target variable:</p><div><img src="img/B04041_04_01.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">You can see that components <code class="literal">1</code> and <code class="literal">2</code> are able to discriminate the three classes of the iris flowers. Thus we have effectively used PCA in reducing the dimension to two from four and still able to discriminate the instances belonging to different classes of Iris flower.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec189" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">In the previous<a id="id291" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> section, we said that we would outline a couple of ways to help us select how many components should we include. In our recipe, we included only two. The following are a list of ways to select the components more empirically:</p><div><ol class="orderedlist"><li class="listitem1">The Eigenvalue criterion:<p class="calibre59">An Eigenvalue of one would mean that the component would explain about one variable's worth of variability. So, according to this criterion, a component should at least explain one variable's worth of variability. We can say that we will include only those Eigenvalues whose value is greater than or equal to one. Based on your data set you can set the threshold. In  a very large dimensional dataset including components capable of explaining only one variable may not be very useful.</p></li><li class="listitem1">The proportion of the variance explained criterion:<p class="calibre59">Let's run the following code:</p><div><pre class="programlisting1">print "Component, Eigen Value, % of Variance, Cummulative %"
cum_per = 0
per_var = 0
for i,e_val in enumerate(eig_val):
    per_var = round((e_val / len(eig_val)),3)
    cum_per+=per_var
print ('%d, %0.2f, %0.2f, %0.2f')%(i+1, e_val, per_var*100,cum_per*100)</pre></div></li><li class="listitem1">The <a id="id292" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>output is as follows:<div><img src="img/B04041_04_25.jpg" alt="There's more…" class="calibre60"/></div></li></ol></div><p class="calibre11">For each component, we printed the Eigenvalue, percentage of the variance explained by that component, and cumulative percentage value of the variance explained. For example, component <code class="literal">1</code> has an Eigenvalue of <code class="literal">2.91</code>; <code class="literal">2.91/4</code> gives the percentage of the variance explained, which is 72.80%. Now, if we include the first two components, then we can explain 95.80% of the variance in the data.</p><p class="calibre11">The decomposition of a correlation matrix into its Eigenvectors and values is a general technique that can be applied to any matrix. In this case, we will apply it to a correlation matrix in order to understand the principal axes of data distribution, that is, axes through which the maximum variation in the data is observed.</p><p class="calibre11">PCA can be used either as an exploratory technique or as a data preparation technique for a downstream algorithm. Document classification dataset problems typically have very large dimensional feature vectors. PCA can be used to reduce the dimension of the dataset in order to include only the most relevant features before feeding the data to a classification<a id="id293" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> algorithm.</p><p class="calibre11">A drawback of PCA worth mentioning here is that it is computationally expensive operation. Finally a point about numpy's corrcoeff function. The corrcoeff function will standardize your data internally as a part of its calculation. But since we want to explicitly state the reason for scaling, we have included it in our recipe.</p><div><div><h3 class="title5"><a id="tip10" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre19">
<strong class="calibre12">When would PCA work?</strong>
</p><p class="calibre19">The input dataset should have correlated columns for PCA to work effectively. Without a correlation of the input variables, PCA cannot help us.</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec190" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Performing Singular Value Decomposition</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Data Analysis – Deep Dive">Chapter 4</a>, <em class="calibre15">Analyzing Data - Deep Dive</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec56" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using Kernel PCA</h1></div></div></div><p class="calibre11">PCA makes an<a id="id294" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> assumption that all the principal directions of variation in the data are straight lines. This is not true in a lot of real-world datasets.</p><div><div><h3 class="title4"><a id="note20" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">PCA is limited to only those variables where the variation in the data falls in a straight line. In other words, it works only with linearly separable data.</p></div></div><p class="calibre11">In this section, we will look at kernel PCA, which will help us reduce the dimension of datasets where the variations in them are not straight lines. We will explicitly create such a dataset and apply kernel PCA on it.</p><p class="calibre11">In kernel PCA, a kernel function is applied to all the data points. This transforms the input data into kernel space. A normal PCA is performed in the kernel space.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec191" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">We will not use the Iris dataset here, but will generate a dataset where variations are not straight lines. This way, we cannot apply a simple PCA on this dataset. Let's proceed to look at our recipe.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec192" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the necessary libraries. We will proceed to make a dataset using the <code class="literal">make_circles</code> function from the scikit-learn library. We will plot this data and do a normal PCA on this dataset:</p><div><pre class="programlisting">from sklearn.datasets import make_circles
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA
from sklearn.decomposition import KernelPCA


# Generate a dataset where the variations cannot be captured by a straight line.
np.random.seed(0)
x,y = make_circles(n_samples=400, factor=.2,noise=0.02)

# Plot the generated dataset
plt.close('all')
plt.figure(1)
plt.title("Original Space")
plt.scatter(x[:,0],x[:,1],c=y)
plt.xlabel("$x_1$")
plt.ylabel("$x_2$")

# Try to fit the data using normal PCA
pca = PCA(n_components=2)
pca.fit(x)
x_pca = pca.transform(x)</pre></div><p class="calibre11">We will then plot the first two principal components of this dataset. We will plot the dataset using only the first principal component:</p><div><pre class="programlisting">plt.figure(2)
plt.title("PCA")
plt.scatter(x_pca[:,0],x_pca[:,1],c=y)
plt.xlabel("$Component_1$")
plt.ylabel("$Component_2$")

# Plot using the first component from normal pca
class_1_indx = np.where(y==0)[0]
class_2_indx = np.where(y==1)[0]

plt.figure(3)
plt.title("PCA- One component")
plt.scatter(x_pca[class_1_indx,0],np.zeros(len(class_1_indx)),color='red')
plt.scatter(x_pca[class_2_indx,0],np.zeros(len(class_2_indx)),color='blue')</pre></div><p class="calibre11">Let's finish it up<a id="id295" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> by performing a kernal PCA and plotting the components:</p><div><pre class="programlisting"># Create  KernelPCA object in Scikit learn, specifying a type of kernel as a parameter.
kpca = KernelPCA(kernel="rbf",gamma=10)
# Perform KernelPCA
kpca.fit(x)
x_kpca = kpca.transform(x)


# Plot the first two components.
plt.figure(4)
plt.title("Kernel PCA")
plt.scatter(x_kpca[:,0],x_kpca[:,1],c=y)
plt.xlabel("$Component_1$")
plt.ylabel("$Component_2$")
plt.show()</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec193" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we generated a dataset using the scikit's data generation function. In this case, we used the <code class="literal">make_circles</code> function. We can create two concentric circles, a large one containing the smaller one, using this function. Each concentric circle belongs to a certain class. Thus, we created a two class problem with two concentric circles.</p><p class="calibre11">First, let's look at the data that we generated. The <code class="literal">make_circles </code>function generated a dataset of size 400 with two dimensions. A plot of the original data is as follows:</p><div><img src="img/B04041_04_02.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">This chart describes<a id="id296" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> how our data has been distributed. The outer circle belongs to class one and the inner circle belongs to class two. Is there a way we can take this data and use it with a linear classifier? We will not be able to do it. The variations in the data are not straight lines. We cannot use the normal PCA. Hence, we will resort to a kernel PCA in order to transform the data.</p><p class="calibre11">Before we venture into kernel PCA, let's see what happens if we apply a normal PCA on this dataset.</p><p class="calibre11">Let's look at the output plot of the first two components:</p><div><img src="img/B04041_04_03.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">As you can <a id="id297" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see, the components of PCA are unable to distinguish between the two classes in a linear fashion.</p><p class="calibre11">Let's plot the first component and see its class distinguishing ability. The following graph, where we have plotted only the first component, explains how PCA is unable to differentiate the data:</p><div><img src="img/B04041_04_04.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">The normal PCA<a id="id298" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> approach is a linear projection technique that works well if the data is linearly separable. In cases where the data is not linearly separable, a nonlinear technique is required for the dimensionality reduction of the dataset.</p><div><div><h3 class="title5"><a id="note21" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Kernel PCA is a nonlinear technique for data reduction.</p></div></div><p class="calibre11">Let's proceed to create a kernel PCA object using the scikit-learn library. Here is our object creation code:</p><div><pre class="programlisting">KernelPCA(kernel=rbf,gamma=10) </pre></div><p class="calibre11">We selected the <strong class="calibre12">Radial Basis Function</strong> (<strong class="calibre12">RBF</strong>) kernel with a gamma value of ten. Gamma is the <a id="id299" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>parameter of the kernel (to handle nonlinearity)—the kernel coefficient.</p><p class="calibre11">Before we go further, let's look at a little bit of theory about what kernels really are. As a simple definition, a kernel is a function that computes the dot product, that is, the similarity between two vectors, which are passed to it as input.</p><p class="calibre11">The RBFGaussian kernel is<a id="id300" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> defined as follows for two points, <em class="calibre15">x</em> and <em class="calibre15">x'</em> in some input space:</p><div><img src="img/B04041_04_15.jpg" alt="How it works…" class="calibre61"/></div><p class="calibre11">Where,</p><div><img src="img/B04041_04_16.jpg" alt="How it works…" class="calibre62"/></div><p class="calibre11">
<em class="calibre15">The RBF decreases with distance and takes values between 0 and 1. Hence it can be interpreted as a similarity measure. The feature space of the RBF kernel has infinite dimensions –Wikipedia</em>.</p><p class="calibre11">This<a id="id301" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> can be found at:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://en.wikipedia.org/wiki/Radial_basis_function_kernel">http://en.wikipedia.org/wiki/Radial_basis_function_kernel</a>.</p><p class="calibre11">Let's now transform the input from the feature space into the kernel space. We will perform a PCA in the kernel space.</p><p class="calibre11">Finally, we will plot the first two principal components as a scatter plot. The points are colored based on their class value:</p><div><img src="img/B04041_04_05.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">You can see<a id="id302" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> in this graph that the points are linearly separated in the kernel space.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec194" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">Scikit-learn's kernel PCA object also allows other types of kernels, as follows:</p><div><ul class="itemizedlist"><li class="listitem">Linear</li><li class="listitem">Polynomial</li><li class="listitem">Sigmoid</li><li class="listitem">Cosine</li><li class="listitem">Precomputed</li></ul></div><p class="calibre11">Scikit-learn also provides other types of nonlinear data that is generated. The following is another example:</p><div><pre class="programlisting">from sklearn.datasets import make_moons
x,y = make_moons(100)
plt.figure(5)
plt.title("Non Linear Data")
plt.scatter(x[:,0],x[:,1],c=y)
plt.xlabel("$x_1$")
plt.ylabel("$x_2$")
plt.savefig('fig-7.png')
plt.show()</pre></div><p class="calibre11">The data <a id="id303" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>plot looks as follows:</p><div><img src="img/B04041_04_06.jpg" alt="There's more…" class="calibre45"/></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec57" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Extracting features using singular value decomposition</h1></div></div></div><p class="calibre11">After our discussion on PCA and kernel PCA, we can explain dimensionality reduction in the following way:</p><div><ul class="itemizedlist"><li class="listitem">You can transform the correlated variables into a set of non-correlated variables. This way, we will have a less dimension explaining the relationship in the underlying data without any loss of information.</li><li class="listitem">You can find out the principal axes, which has the most data variation recorded.</li></ul></div><p class="calibre11">
<strong class="calibre12">Singular Value Decomposition</strong> (<strong class="calibre12">SVD</strong>) is yet another matrix decomposition technique that can be <a id="id304" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>used to tackle the curse of the dimensionality problem. It can be used to find the best approximation of the original data using fewer dimensions. Unlike PCA, SVD works on the original data matrix.</p><div><div><h3 class="title4"><a id="note22" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">SVD does not need a covariance or correlation matrix. It works on the original data matrix.</p></div></div><p class="calibre11">SVD factors an <code class="literal">m</code> x <code class="literal">n</code> matrix <code class="literal">A</code> into a product of three matrices:</p><div><img src="img/B04041_04_17.jpg" alt="Extracting features using singular value decomposition" class="calibre63"/></div><p class="calibre11">Here, U is an <code class="literal">m</code> x <code class="literal">k</code> matrix, V is an <code class="literal">n</code> x <code class="literal">k</code> matrix, and S is a <code class="literal">k</code> x <code class="literal">k</code> matrix. The columns of U are called left singular vectors and columns of V are called right singular vectors.</p><p class="calibre11">The values on the diagonal of the S matrix are called singular values.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec195" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">We will use the Iris dataset for this exercise. Our task in hand is to reduce the dimensionality of the dataset from four to two.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec196" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the necessary libraries and get the Iris dataset:</p><div><pre class="programlisting">from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import scale
from scipy.linalg import svd

# Load Iris dataset
data = load_iris()
x = data['data']
y = data['target']

# Proceed by scaling the x variable w.r.t its mean,
x_s = scale(x,with_mean=True,with_std=False,axis=0)


# Decompose the matrix using SVD technique.We will use SVD implementation in scipy.
U,S,V = svd(x_s,full_matrices=False)

# Approximate the original matrix by selecting only the first two singular values.
x_t = U[:,:2]


# Finally we plot the datasets with the reduced components.
plt.figure(1)
plt.scatter(x_t[:,0],x_t[:,1],c=y)
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.show()</pre></div><p class="calibre11">Now, we <a id="id305" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will demonstrate how to perform an SVD operation on the Iris dataset:</p><div><pre class="programlisting"># Proceed by scaling the x variable w.r.t its mean,
x_s = scale(x,with_mean=True,with_std=False,axis=0)
# Decompose the matrix using SVD technique.We will use SVD implementation in scipy.
U,S,V = svd(x_s,full_matrices=False)

# Approximate the original matrix by selecting only the first two singular values.
x_t = U[:,:2]


# Finally we plot the datasets with the reduced components.
plt.figure(1)
plt.scatter(x_t[:,0],x_t[:,1],c=y)
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.show()</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec197" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">The Iris dataset has four columns. Though there are not many columns, it will serve our purpose. We intend to reduce the dimensionality of the Iris dataset to two from four and still retain all the information about the data.</p><p class="calibre11">We will load the Iris data to the <code class="literal">x</code> and <code class="literal">y </code>variables using the convenient <code class="literal">load_iris </code>function from scikit-learn. The <code class="literal">x </code>variable is our data matrix; we can inspect its shape in the following manner:</p><div><pre class="programlisting">&gt;&gt;&gt;x.shape
(150, 4)
&gt;&gt;&gt;</pre></div><p class="calibre11">We center the data matrix <code class="literal">x</code> using its mean. The rule of thumb is that if all your columns are measured in the same scale and have the same unit of measurement in the data, you don't have to scale the data. This will allow PCA to capture these basis units with the maximum variation. Note that we used only the mean while invoking the function scale:</p><div><pre class="programlisting">x_s = scale(x,with_mean=True,with_std=False,axis=0)</pre></div><div><ol class="orderedlist"><li class="listitem1">Run the SVD method on our scaled input dataset.</li><li class="listitem1">Select the top two singular components. This matrix is a reduced approximation <a id="id306" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the original input data.</li><li class="listitem1">Finally, plot the columns and color it by the class value:<div><img src="img/B04041_04_07.jpg" alt="How it works…" class="calibre45"/></div></li></ol></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec198" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">SVD is a two-mode factor analysis, where we start with an arbitrary rectangular matrix with two types of entities. This is different from our previous recipe where we saw PCA that took a correlation matrix as an input. PCA is a one-mode factor analysis as the rows and columns in the input square matrix represent the same entity.</p><p class="calibre11">In text mining applications, the input is typically presented as a <strong class="calibre12">Term-document Matrix</strong> (<strong class="calibre12">TDM</strong>). In a TDM, the rows correspond to the words and columns are the documents. The cell entries are filled with either the term frequency or <strong class="calibre12">Term Frequency Inverse Document Frequency </strong>(<strong class="calibre12">TFIDF</strong>) score. It is a rectangular matrix with two entities: words and documents<a id="id307" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> that are present in the rows and columns of the matrix.</p><p class="calibre11">SVD is widely<a id="id308" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> used in text mining applications to uncover the hidden relationships (semantic relationship) between words and documents, documents and documents, and words and words.</p><p class="calibre11">By applying SVD on a term-document matrix, we transform it into a new semantic space, where the words that do not occur together in the same document can still be close in the new semantic space. The goal of SVD is to find a useful way to model the relationship between the words and documents. After applying SVD, each document and word can be represented as a vector of the factor values. We can choose to ignore the components with very low values and, hence, avoid noises in the underlying dataset. This leads to an approximate representation <a id="id309" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of our text corpus. This is called <strong class="calibre12">Latent Semantic Analysis</strong> (<strong class="calibre12">LSA</strong>).</p><p class="calibre11">The ramification of this idea has a very high applicability in document indexing for search and information retrieval. Instead of indexing the original words as an inverted index, we can now index the output of LSA. This helps avoid problems such as synonymy and polysemy. In synonymy, users may tend to use different words to represent the same entity. A normal indexing is vulnerable to such scenarios. As the underlying document is indexed by regular words, a search may not yield the results. For example, if we indexed some documents related to financial instruments, typically the words would be currency, money, and similar stuff. Currency and money are synonymous words. While a user searches for money, he should be shown the documents related to currency as well. However, with regular indexing, the search engine would be able to retrieve only the documents having money. With latent semantic indexing, the documents with currency will also be retrieved. In the latent semantic space, currency and money will be close to each other as their neighboring words would be similar in the documents.</p><p class="calibre11">Polysemy is<a id="id310" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> about words that have more than one meaning. For example, bank can refer to a financial institution or a river bank. Similar to synonymy, polysemy can also be handled in the latent semantic space.</p><p class="calibre11">For more information<a id="id311" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> on LSA <a id="id312" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and latent semantic indexing, refer to the paper by Deerwester et al at:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.8490">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.8490</a>. For a comparative study of Eigen Values and Singular Values refer to the book Numerical Computing with MATLAB by Cleve Moler. Though the examples are in MATLAB, with the help of our recipe you can redo them in Python:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://in.mathworks.com/moler/eigs.pdf">https://in.mathworks.com/moler/eigs.pdf</a>
</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec58" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Reducing the data dimension with random projection</h1></div></div></div><p class="calibre11">The methods that we saw previously for dimensionality reduction are computationally expensive and not the fastest ones. Random projection is another way<a id="id313" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to perform dimensionality<a id="id314" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> reduction faster than these methods.</p><p class="calibre11">Random projections are attributed to the Johnson-Lindenstrauss lemma. According to the lemma, a mapping from a high-dimensional to a low-dimensional Euclidean space exists; such that the distance between the points is preserved within an epsilon variance. The goal is to preserve the pairwise distances between any two points in your data, and still reduce the number of dimensions in the data.</p><p class="calibre11">Let's say that if we are given <code class="literal">n</code>-dimensional data in any Euclidean space, according to the lemma, we can map it an Euclidean space of dimension k, such that all the distances between the points are preserved up to a multiplicative factor of (1-epsilon) and (1+ epsilon).</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec199" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">For this exercise, we will use the 20 newsgroup data (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://qwone.com/~jason/20Newsgroups/">http://qwone.com/~jason/20Newsgroups/</a>).</p><p class="calibre11">It is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different categories of news. Scikit-learn provides a convenient function to load this dataset:</p><div><pre class="programlisting">from sklearn.datasets import fetch_20newsgroups
data = fetch_20newsgroups(categories=cat)</pre></div><p class="calibre11">You can load all libraries or a list of categories of interest by providing a list of strings of categories. In our case, we will use the <code class="literal">sci.crypt</code> category.</p><p class="calibre11">We will load the input text as a term-document matrix where the features are individual words. On this, we will apply random projection in order to reduce the number of dimensions. We will try to see if the distances between the documents are preserved in the reduced space and an instance is a document.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec200" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's start <a id="id315" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with loading the necessary<a id="id316" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> libraries. Using scikit's utility function <code class="literal">fetch20newsgroups</code>, we will load the data. We will select only the <code class="literal">sci.crypt</code> category out of all the data. We will then transform our text data into a vector representation:</p><div><pre class="programlisting">from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import euclidean_distances
from sklearn.random_projection import GaussianRandomProjection
import matplotlib.pyplot as plt
import numpy as np

# Load 20 newsgroup dataset
# We select only sci.crypt category
# Other categories include
# 'sci.med', 'sci.space' ,'soc.religion.christian'
cat =['sci.crypt']
data = fetch_20newsgroups(categories=cat)

# Create a term document matrix, with term frequencies as the values
# from the above dataset.
vectorizer = TfidfVectorizer(use_idf=False)
vector = vectorizer.fit_transform(data.data)

# Perform the projection. In this case we reduce the dimension to 1000
gauss_proj = GaussianRandomProjection(n_components=1000)
gauss_proj.fit(vector)
# Transform the original data to the new space
vector_t = gauss_proj.transform(vector)

# print transformed vector shape
print vector.shape
print vector_t.shape

# To validate if the transformation has preserved the distance, we calculate the old and the new distance between the points
org_dist = euclidean_distances(vector)
red_dist = euclidean_distances(vector_t)

diff_dist = abs(org_dist - red_dist)

# We take the difference between these points and plot them 
# as a heatmap (only the first 100 documents).
plt.figure()
plt.pcolor(diff_dist[0:100,0:100])
plt.colorbar()
plt.show()</pre></div><p class="calibre11">Let's now proceed to demonstrate the concept of random projection.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec201" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">After loading the newsgroup dataset, we will convert it to a matrix through <code class="literal">TfidfVectorizer(use_idf=False)</code>.</p><p class="calibre11">Notice that we have set <code class="literal">use_idf</code> to <code class="literal">False</code>. This creates our input matrix where the rows are documents, columns are individual words, and cell values are word counts.</p><p class="calibre11">If we print our vector using the <code class="literal">print vector.shape</code> command, we will get the following output:</p><div><pre class="programlisting">(595, 16115)</pre></div><p class="calibre11">We can see that our input matrix has 595 documents and 16115 words; each word is a feature and, hence, a dimension.</p><p class="calibre11">We will perform the projection of the data using a dense Gaussian matrix. The Gaussian random matrix is generated by sampling elements from a normal distribution <code class="literal">N</code>(0, 1/number of components). In our case, the number of components is 1000. Our intention is to reduce the dimension to 1000 from 16115. We will then print the original and the reduced dimension in order to verify the reduction in the dimension.</p><p class="calibre11">Finally, we <a id="id317" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>would like to validate<a id="id318" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> whether the data characteristics are maintained after the projection. We will calculate the Euclidean distances between the vectors. We will record the distances in the original space and in the projected space as well. We will take a difference between them as in step 7 and plot the difference as a heat map:</p><div><img src="img/B04041_04_08.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">As you can see, the gradient is in the range of 0.000 to 0.105 and indicates the difference in distance of vectors in the original and reduced space. The difference between the distance in the original space and projected space are pretty much in a very small range.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec202" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">There are a <a id="id319" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>lot of references for random<a id="id320" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> projections. It is a very active field of research. Interested readers can refer to the following papers:</p><p class="calibre11">Experiments <a id="id321" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with random projections:
</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://dl.acm.org/citation.cfm?id=719759">http://dl.acm.org/citation.cfm?id=719759</a>
</p><p class="calibre11">Experiments with random projections for machine learning:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.9205">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.9205</a>
</p><p class="calibre11">In our recipe, we used the Gaussian random projection where a Gaussian random matrix was generated by sampling from a normal distribution, N(0,1/1000), where 1000 is the required dimension of the reduced space.</p><p class="calibre11">However, having a dense matrix can create severe memory-related issues while processing. In order to avoid this, Achlioptas proposed sparse random projections. Instead of choosing from a standard normal distribution, the entries are picked from {-1.0,1} with a probability of <code class="literal">{1/6,2/3,1/6}</code>. As you can see, the probability of having 0 is two-thirds and, hence, the resultant matrix will be sparse. Users can refer to the seminal paper by Achlioptas at <em class="calibre15">Dimitris Achlioptas</em>, <em class="calibre15">Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System Sciences, 66(4):671–687, 2003.</em>
</p><p class="calibre11">The scikit implementation allows the users to choose the density of the resultant matrix. Let's say that if we specify the density as d and s as 1/d, then the elements of the matrix are picked from the following equation:</p><div><img src="img/B04041_04_18.jpg" alt="There's more…" class="calibre64"/></div><p class="calibre11">With <a id="id322" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>probability<a id="id323" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of the following:</p><div><img src="img/B04041_04_19.jpg" alt="There's more…" class="calibre65"/></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec203" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Using kernel PCA</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Data Analysis – Deep Dive">Chapter 4</a>, <em class="calibre15">Analyzing Data - Deep Dive</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec59" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Decomposing the feature matrices using non-negative matrix factorization</h1></div></div></div><p class="calibre11">We <a id="id324" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>discussed all the previous matrix decomposition recipes from a data dimensionality<a id="id325" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> reduction perspective. Let's discuss this recipe from a collaborative filtering perspective to make it more interesting. Though data dimensionality reduction is what we are after, <strong class="calibre12">Non-negative Matrix Factorization</strong> (<strong class="calibre12">NMF</strong>) is used extensively in recommendation systems using a collaborative filtering algorithm.</p><p class="calibre11">Let's say that our input matrix A is of a dimension <code class="literal">m x n</code>. NMF factorizes the input matrix into two matrices, <code class="literal">A_dash</code> and <code class="literal">H</code>:</p><div><img src="img/B04041_04_21.jpg" alt="Decomposing the feature matrices using non-negative matrix factorization" class="calibre66"/></div><p class="calibre11">Let's say <a id="id326" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that <a id="id327" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we want to reduce the dimension of the A matrix to d, that is, we want the original m x n matrix to be decomposed into m x d, where d &lt;&lt; n.</p><p class="calibre11">The <code class="literal">A_dash</code> matrix is of a size m x d and the <code class="literal">H</code> matrix is of a size d x m. NMF solves this as an optimization problem, that is, minimizing the function:</p><div><img src="img/B04041_04_22.jpg" alt="Decomposing the feature matrices using non-negative matrix factorization" class="calibre67"/></div><p class="calibre11">The famous<a id="id328" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> Netflix challenge was solved using NMF. Please refer to the following link:</p><p class="calibre11">
<em class="calibre15">Gábor Takács et al., (2008). Matrix factorization and neighbor based algorithms for the Netflix prize problem. In: Proceedings of the 2008 ACM Conference on Recommender Systems, Lausanne, Switzerland, October 23 - 25, 267-274:</em>
</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://dl.acm.org/citation.cfm?id=1454049">http://dl.acm.org/citation.cfm?id=1454049</a>
</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec204" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">In order to explain NMF, let's create a toy recommendation problem. In a typical recommendation system such as the one with MovieLens or Netflix, there is a group of users and group of items (movies). If each user has rated a few movies, we want to predict their ratings for the movies that they have not rated. We will assume that the users have not watched the movies that they have not rated. Our prediction algorithm output is the ratings for these movies. We can then recommend the movies that have a very high rating from our prediction engine to these users.</p><p class="calibre11">Our toy problem is set as follows; we have the following movies:</p><div><table border="1" class="calibre68"><colgroup class="calibre69"><col class="calibre70"/><col class="calibre70"/></colgroup><thead class="calibre71"><tr class="calibre72"><th valign="bottom" class="calibre73">
<p class="calibre74">Movie ID</p>
</th><th valign="bottom" class="calibre73">
<p class="calibre74">Movie Name</p>
</th></tr></thead><tbody class="calibre75"><tr class="calibre76"><td class="calibre77">
<p class="calibre74">1</p>
</td><td class="calibre77">
<p class="calibre74">Star Wars</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">2</p>
</td><td class="calibre77">
<p class="calibre74">The Matrix</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">3</p>
</td><td class="calibre77">
<p class="calibre74">Inception</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">4</p>
</td><td class="calibre77">
<p class="calibre74">Harry Potter</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">5</p>
</td><td class="calibre77">
<p class="calibre74">The Hobbit</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">6</p>
</td><td class="calibre77">
<p class="calibre74">Guns of Navarone</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">7</p>
</td><td class="calibre77">
<p class="calibre74">Saving Private Ryan</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">8</p>
</td><td class="calibre77">
<p class="calibre74">Enemy at the Gates</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">9</p>
</td><td class="calibre77">
<p class="calibre74">Where Eagles Dare</p>
</td></tr><tr class="calibre79"><td class="calibre77">
<p class="calibre74">10</p>
</td><td class="calibre77">
<p class="calibre74">The Great Escape</p>
</td></tr></tbody></table></div><p class="calibre11">We <a id="id329" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have<a id="id330" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> ten movies, each identified with a movie ID. We also have 10 users who have rated these movies as follows:</p><div><table border="1" class="calibre68"><colgroup class="calibre69"><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/></colgroup><thead class="calibre71"><tr class="calibre72"><th valign="bottom" class="calibre73"> </th><th colspan="10" valign="bottom" class="calibre80">
<p class="calibre74">Movie ID</p>
</th></tr></thead><tbody class="calibre75"><tr class="calibre76"><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">User ID</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">1</p>
</td><td class="calibre77">
<p class="calibre74">2</p>
</td><td class="calibre77">
<p class="calibre74">3</p>
</td><td class="calibre77">
<p class="calibre74">4</p>
</td><td class="calibre77">
<p class="calibre74">5</p>
</td><td class="calibre77">
<p class="calibre74">6</p>
</td><td class="calibre77">
<p class="calibre74">7</p>
</td><td class="calibre77">
<p class="calibre74">8</p>
</td><td class="calibre77">
<p class="calibre74">9</p>
</td><td class="calibre77">
<p class="calibre74">10</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">1</p>
</td><td class="calibre77">
<p class="calibre74">5.0</p>
</td><td class="calibre77">
<p class="calibre74">5.0</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">5.0</p>
</td><td class="calibre77">
<p class="calibre74">3.0</p>
</td><td class="calibre77">
<p class="calibre74">2.0</p>
</td><td class="calibre77">
<p class="calibre74">2.0</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">2</p>
</td><td class="calibre77">
<p class="calibre74">4.2</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">5.0</p>
</td><td class="calibre77">
<p class="calibre74">3.7</p>
</td><td class="calibre77">
<p class="calibre74">3.5</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">2.7</p>
</td><td class="calibre77">
<p class="calibre74">2.0</p>
</td><td class="calibre77">
<p class="calibre74">1.9</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">3</p>
</td><td class="calibre77">
<p class="calibre74">2.5</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">3.3</p>
</td><td class="calibre77">
<p class="calibre74">3.4</p>
</td><td class="calibre77">
<p class="calibre74">2.2</p>
</td><td class="calibre77">
<p class="calibre74">4.6</p>
</td><td class="calibre77">
<p class="calibre74">4.0</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">4.2</p>
</td><td class="calibre77">
<p class="calibre74">3.6</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">4</p>
</td><td class="calibre77">
<p class="calibre74">3.8</p>
</td><td class="calibre77">
<p class="calibre74">4.1</p>
</td><td class="calibre77">
<p class="calibre74">4.6</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">2.2</p>
</td><td class="calibre77">
<p class="calibre74">3.5</p>
</td><td class="calibre77">
<p class="calibre74">3.0</p>
</td><td class="calibre77">
<p class="calibre74">2.2</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">5</p>
</td><td class="calibre77">
<p class="calibre74">2.1</p>
</td><td class="calibre77">
<p class="calibre74">2.6</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">2.1</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">3.8</p>
</td><td class="calibre77">
<p class="calibre74">4.8</p>
</td><td class="calibre77">
<p class="calibre74">4.1</p>
</td><td class="calibre77">
<p class="calibre74">4.3</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">6</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">4.4</p>
</td><td class="calibre77">
<p class="calibre74">4.1</p>
</td><td class="calibre77">
<p class="calibre74">3.5</p>
</td><td class="calibre77">
<p class="calibre74">3.1</p>
</td><td class="calibre77">
<p class="calibre74">3.4</p>
</td><td class="calibre77">
<p class="calibre74">3.1</p>
</td><td class="calibre77">
<p class="calibre74">2.5</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">7</p>
</td><td class="calibre77">
<p class="calibre74">2.8</p>
</td><td class="calibre77">
<p class="calibre74">2.4</p>
</td><td class="calibre77">
<p class="calibre74">2.1</p>
</td><td class="calibre77">
<p class="calibre74">3.3</p>
</td><td class="calibre77">
<p class="calibre74">3.4</p>
</td><td class="calibre77">
<p class="calibre74">3.8</p>
</td><td class="calibre77">
<p class="calibre74">4.4</p>
</td><td class="calibre77">
<p class="calibre74">4.9</p>
</td><td class="calibre77">
<p class="calibre74">4.0</p>
</td><td class="calibre77">
<p class="calibre74">4.3</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">8</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">4.9</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">2.9</p>
</td><td class="calibre77">
<p class="calibre74">2.9</p>
</td><td class="calibre77">
<p class="calibre74">2.5</p>
</td><td class="calibre77">
<p class="calibre74">2.1</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">9</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">3.3</p>
</td><td class="calibre77">
<p class="calibre74">2.9</p>
</td><td class="calibre77">
<p class="calibre74">3.6</p>
</td><td class="calibre77">
<p class="calibre74">3.1</p>
</td><td class="calibre77">
<p class="calibre74">4.0</p>
</td><td class="calibre77">
<p class="calibre74">4.2</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">4.6</p>
</td></tr><tr class="calibre81"><td class="calibre77">
<p class="calibre74">10</p>
</td><td class="calibre77">
<p class="calibre74">4.1</p>
</td><td class="calibre77">
<p class="calibre74">3.6</p>
</td><td class="calibre77">
<p class="calibre74">3.7</p>
</td><td class="calibre77">
<p class="calibre74">4.6</p>
</td><td class="calibre77">
<p class="calibre74">4.0</p>
</td><td class="calibre77">
<p class="calibre74">2.6</p>
</td><td class="calibre77">
<p class="calibre74">1.9</p>
</td><td class="calibre77">
<p class="calibre74">3.0</p>
</td><td class="calibre77">
<p class="calibre74">3.6</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td></tr></tbody></table></div><p class="calibre11">For readability, we have kept it as a matrix where the rows correspond to the users and columns correspond to the movies. The cell values are the ratings from 1 to 5, where 5 signifies a high user affinity to the movie and 1 signifies the user's dislike. There are 0 values in the cells that indicate that the user has not rated those movies. In this recipe, we will decompose the <code class="literal">user_id</code> x <code class="literal">movie_id</code> matrix using NMF.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec205" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">We <a id="id331" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will <a id="id332" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>start with loading the necessary libraries and then creating our dataset. We will store our dataset as a matrix:</p><div><pre class="programlisting">import numpy as np
from collections import defaultdict
from sklearn.decomposition import NMF
import matplotlib.pyplot as plt

# load our ratings matrix in python.
ratings = [
[5.0, 5.0, 4.5, 4.5, 5.0, 3.0, 2.0, 2.0, 0.0, 0.0],
[4.2, 4.7, 5.0, 3.7, 3.5, 0.0, 2.7, 2.0, 1.9, 0.0],
[2.5, 0.0, 3.3, 3.4, 2.2, 4.6, 4.0, 4.7, 4.2, 3.6],
[3.8, 4.1, 4.6, 4.5, 4.7, 2.2, 3.5, 3.0, 2.2, 0.0],
[2.1, 2.6, 0.0, 2.1, 0.0, 3.8, 4.8, 4.1, 4.3, 4.7],
[4.7, 4.5, 0.0, 4.4, 4.1, 3.5, 3.1, 3.4, 3.1, 2.5],
[2.8, 2.4, 2.1, 3.3, 3.4, 3.8, 4.4, 4.9, 4.0, 4.3],
[4.5, 4.7, 4.7, 4.5, 4.9, 0.0, 2.9, 2.9, 2.5, 2.1],
[0.0, 3.3, 2.9, 3.6, 3.1, 4.0, 4.2, 0.0, 4.5, 4.6],
[4.1, 3.6, 3.7, 4.6, 4.0, 2.6, 1.9, 3.0, 3.6, 0.0]
]


movie_dict = {
1:"Star Wars",
2:"Matrix",
3:"Inception",
4:"Harry Potter",
5:"The hobbit",
6:"Guns of Navarone",
7:"Saving Private Ryan",
8:"Enemy at the gates",
9:"Where eagles dare",
10:"Great Escape"
}

A = np.asmatrix(ratings,dtype=float)

# perform non negative matrix transformation on the data.
max_components = 2
reconstruction_error = []
nmf = None
nmf = NMF(n_components = max_components,random_state=1)
A_dash = nmf.fit_transform(A)

# Examine the reduced matrixfor i in range(A_dash.shape[0]):
for i in range(A_dash.shape[0]):
    print "User id = %d, comp1 score = %0.2f, comp 2 score = %0.2f"%(i+1,A_dash[i][0],A_dash[i][1])

plt.figure(1)
plt.title("User Concept Mapping")
x = A_dash[:,0]
y = A_dash[:,1]
plt.scatter(x,y)
plt.xlabel("Component 1 Score")
plt.ylabel("Component 2 Score")

# Let us examine our component matrix F.
F = nmf.components_
plt.figure(2)
plt.title("Movie Concept Mapping")
x = F[0,:]
y = F[1,:]
plt.scatter(x,y)
plt.xlabel("Component 1 score")
plt.ylabel("Component 2  score")
for i in range(F[0,:].shape[0]):
    plt.annotate(movie_dict[i+1],(F[0,:][i],F[1,:][i]))
plt.show()</pre></div><p class="calibre11">Let's now proceed to demonstrate a non-negative matrix transformation:</p><div><pre class="programlisting"># perform non negative matrix transformation on the data.
max_components = 2
reconstruction_error = []
nmf = None
nmf = NMF(n_components = max_components,random_state=1)
A_dash = nmf.fit_transform(A)

# Examine the reduced matrixfor i in range(A_dash.shape[0]):
for i in range(A_dash.shape[0]):
    print "User id = %d, comp1 score = %0.2f, comp 2 score = %0.2f"%(i+1,A_dash[i][0],A_dash[i][1])
plt.figure(1)
plt.title("User Concept Mapping")
x = A_dash[:,0]
y = A_dash[:,1]
plt.scatter(x,y)
plt.xlabel("Component 1 Score")
plt.ylabel("Component 2 Score")

# Let us examine our component matrix F.
F = nmf.components_
plt.figure(2)
plt.title("Movie Concept Mapping")
x = F[0,:]
y = F[1,:]
plt.scatter(x,y)
plt.xlabel("Component 1 score")
plt.ylabel("Component 2  score")
for i in range(F[0,:].shape[0]):
    plt.annotate(movie_dict[i+1],(F[0,:][i],F[1,:][i]))
plt.show()</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec206" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">We will load the data to a NumPy matrix A from the list. We will choose to reduce the dimension to two as dictated by the <code class="literal">max_components</code> variable. We will initialize the NMF object with the number of components. Finally, we will apply the algorithm to get the reduced matrix, <code class="literal">A_dash</code>.</p><p class="calibre11">That's all we need to do. The scikit library hides a lot of details for us. Let's now look at what is happening in the background. Formally, NMF decomposes the original matrix into two matrices, which when multiplied together, give the approximation of our original matrix. Look at the following line in our code:</p><div><pre class="programlisting">A_dash = nmf.fit_transform(A)</pre></div><p class="calibre11">The input matrix <code class="literal">A</code> is transformed into a reduced matrix, A_dash. Let's look at the shape of the new matrix:</p><div><pre class="programlisting">&gt;&gt;&gt;A_dash.shape
(10, 2)</pre></div><p class="calibre11">The<a id="id333" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> original matrix is reduced to two columns as compared to the original ten columns. This is the reduced space. From this data perspective, we can say that our algorithm<a id="id334" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> has now grouped our original ten movies into two concepts. The cell value indicates the user affinity towards each of the concepts.</p><p class="calibre11">We will print and see how the affinity looks:</p><div><pre class="programlisting">for i in range(A_dash.shape[0]):
print User id = %d, comp1 score = %0.2f, comp 2 score =%0.2f%(i+1,A_dash[i][0],A_dash[i][1])</pre></div><p class="calibre11">The output looks as follows:</p><div><img src="img/B04041_04_20.jpg" alt="How it works…" class="calibre82"/></div><p class="calibre11">Let's look at user 1; the first line in the preceding image says that user 1 has a score of 2.14 for concept 1 and 0 for concept 2, indicating that user 1 has more affinity towards concept 1.</p><p class="calibre11">Look at user ID 3; this user has more affinity towards concept 1. Now we have reduced our input dataset to two dimensions, it would be nice to view this in a graph.</p><p class="calibre11">In our <em class="calibre15">x</em> axis, we have component 1 and our <em class="calibre15">y</em> axis is component 2. We will plot the various users as a scatter plot. Our graph looks as follows:</p><div><img src="img/B04041_04_09.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">You <a id="id335" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can<a id="id336" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> see that we have two groups of users; all those with a component 1 score of greater than 1.5 and others with less than 1.5. We are able to group our users into two clusters in the reduced feature space.</p><p class="calibre11">Let's look at the other matrix, <code class="literal">F</code>:</p><div><pre class="programlisting">F = nmf.components_</pre></div><p class="calibre11">The <code class="literal">F</code> matrix has two rows; each row corresponds to our component and ten columns, each corresponding to a movie ID. Another way to look at it is the affinity of movies towards these concepts. Let's plot the matrix.</p><p class="calibre11">You <a id="id337" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can<a id="id338" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> see that our <em class="calibre15">x</em> axis is the first row and the <em class="calibre15">y</em> axis is the second row. In step 1, we declared a dictionary. We want this dictionary to annotate each point with the movie name:</p><div><pre class="programlisting">for i in range(F[0,:].shape[0]):
plt.annotate(movie_dict[i+1],(F[0,:][i],F[1,:][i]))</pre></div><p class="calibre11">The annotate method takes the string (used to annotate) as the first parameter and the <code class="literal">x</code> and <code class="literal">y</code> coordinates as a tuple.</p><p class="calibre11">You can see the output graph as follows:</p><div><img src="img/B04041_04_10.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">You can see that we have two distinct groups. All the war movies have a very low component 1 score and a very high component 2 score. All the fantasy movies have a vice<a id="id339" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> versa<a id="id340" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> score. We can safely say that component 1 comprises of war movies and the users having high component 1 scores have very high affinity towards war movies. The same can be said for fantasy movies.</p><p class="calibre11">Thus, using NMF, we are able to unearth the hidden features in our input matrix with respect to the movies.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec207" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">We saw how the feature space was reduced from ten dimensions to two dimensions for the users. Now, let's see how this can be used for the recommendation engines. Let's reconstruct the <a id="id341" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>original<a id="id342" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> matrix from the two matrices:</p><div><pre class="programlisting">reconstructed_A = np.dot(W,H)
np.set_printoptions(precision=1)
print reconstructed_A</pre></div><p class="calibre11">The reconstructed matrix looks as follows:</p><div><img src="img/B04041_04_23.jpg" alt="There's more…" class="calibre83"/></div><p class="calibre11">How different is it from the original matrix? The original matrix is given here; look at the highlighted row:</p><div><table border="1" class="calibre68"><colgroup class="calibre69"><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/><col class="calibre70"/></colgroup><thead class="calibre71"><tr class="calibre72"><th valign="bottom" class="calibre73"> </th><th colspan="10" valign="bottom" class="calibre80">
<p class="calibre74">Movie ID</p>
</th></tr></thead><tbody class="calibre75"><tr class="calibre76"><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">User ID</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">1</p>
</td><td class="calibre77">
<p class="calibre74">2</p>
</td><td class="calibre77">
<p class="calibre74">3</p>
</td><td class="calibre77">
<p class="calibre74">4</p>
</td><td class="calibre77">
<p class="calibre74">5</p>
</td><td class="calibre77">
<p class="calibre74">6</p>
</td><td class="calibre77">
<p class="calibre74">7</p>
</td><td class="calibre77">
<p class="calibre74">8</p>
</td><td class="calibre77">
<p class="calibre74">9</p>
</td><td class="calibre77">
<p class="calibre74">10</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">1</p>
</td><td class="calibre77">
<p class="calibre74">5.0</p>
</td><td class="calibre77">
<p class="calibre74">5.0</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">5.0</p>
</td><td class="calibre77">
<p class="calibre74">3.0</p>
</td><td class="calibre77">
<p class="calibre74">2.0</p>
</td><td class="calibre77">
<p class="calibre74">2.0</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">2</p>
</td><td class="calibre77">
<p class="calibre74">4.2</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">5.0</p>
</td><td class="calibre77">
<p class="calibre74">3.7</p>
</td><td class="calibre77">
<p class="calibre74">3.5</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">2.7</p>
</td><td class="calibre77">
<p class="calibre74">2.0</p>
</td><td class="calibre77">
<p class="calibre74">1.9</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">3</p>
</td><td class="calibre77">
<p class="calibre74">2.5</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">3.3</p>
</td><td class="calibre77">
<p class="calibre74">3.4</p>
</td><td class="calibre77">
<p class="calibre74">2.2</p>
</td><td class="calibre77">
<p class="calibre74">4.6</p>
</td><td class="calibre77">
<p class="calibre74">4.0</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">4.2</p>
</td><td class="calibre77">
<p class="calibre74">3.6</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">4</p>
</td><td class="calibre77">
<p class="calibre74">3.8</p>
</td><td class="calibre77">
<p class="calibre74">4.1</p>
</td><td class="calibre77">
<p class="calibre74">4.6</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">2.2</p>
</td><td class="calibre77">
<p class="calibre74">3.5</p>
</td><td class="calibre77">
<p class="calibre74">3.0</p>
</td><td class="calibre77">
<p class="calibre74">2.2</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">5</p>
</td><td class="calibre77">
<p class="calibre74">2.1</p>
</td><td class="calibre77">
<p class="calibre74">2.6</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">2.1</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">3.8</p>
</td><td class="calibre77">
<p class="calibre74">4.8</p>
</td><td class="calibre77">
<p class="calibre74">4.1</p>
</td><td class="calibre77">
<p class="calibre74">4.3</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">6</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">4.7</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">4.5</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">0.0</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">4.4</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">4.1</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">3.5</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">3.1</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">3.4</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">3.1</strong>
</p>
</td><td class="calibre77">
<p class="calibre74">
<strong class="calibre12">2.5</strong>
</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">7</p>
</td><td class="calibre77">
<p class="calibre74">2.8</p>
</td><td class="calibre77">
<p class="calibre74">2.4</p>
</td><td class="calibre77">
<p class="calibre74">2.1</p>
</td><td class="calibre77">
<p class="calibre74">3.3</p>
</td><td class="calibre77">
<p class="calibre74">3.4</p>
</td><td class="calibre77">
<p class="calibre74">3.8</p>
</td><td class="calibre77">
<p class="calibre74">4.4</p>
</td><td class="calibre77">
<p class="calibre74">4.9</p>
</td><td class="calibre77">
<p class="calibre74">4.0</p>
</td><td class="calibre77">
<p class="calibre74">4.3</p>
</td></tr><tr class="calibre76"><td class="calibre77">
<p class="calibre74">8</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">4.7</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">4.9</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">2.9</p>
</td><td class="calibre77">
<p class="calibre74">2.9</p>
</td><td class="calibre77">
<p class="calibre74">2.5</p>
</td><td class="calibre77">
<p class="calibre74">2.1</p>
</td></tr><tr class="calibre78"><td class="calibre77">
<p class="calibre74">9</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">3.3</p>
</td><td class="calibre77">
<p class="calibre74">2.9</p>
</td><td class="calibre77">
<p class="calibre74">3.6</p>
</td><td class="calibre77">
<p class="calibre74">3.1</p>
</td><td class="calibre77">
<p class="calibre74">4.0</p>
</td><td class="calibre77">
<p class="calibre74">4.2</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td><td class="calibre77">
<p class="calibre74">4.5</p>
</td><td class="calibre77">
<p class="calibre74">4.6</p>
</td></tr><tr class="calibre81"><td class="calibre77">
<p class="calibre74">10</p>
</td><td class="calibre77">
<p class="calibre74">4.1</p>
</td><td class="calibre77">
<p class="calibre74">3.6</p>
</td><td class="calibre77">
<p class="calibre74">3.7</p>
</td><td class="calibre77">
<p class="calibre74">4.6</p>
</td><td class="calibre77">
<p class="calibre74">4.0</p>
</td><td class="calibre77">
<p class="calibre74">2.6</p>
</td><td class="calibre77">
<p class="calibre74">1.9</p>
</td><td class="calibre77">
<p class="calibre74">3.0</p>
</td><td class="calibre77">
<p class="calibre74">3.6</p>
</td><td class="calibre77">
<p class="calibre74">0.0</p>
</td></tr></tbody></table></div><p class="calibre11">For<a id="id343" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> user 6 <a id="id344" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and movie 3, we now have a rating. This will help us decide whether to recommend this movie to the user, as he has not watched it. Remember that this is a toy dataset; real-world scenarios have many movies and users.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec208" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Extracting Features Using Singular Value Decomposition</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Data Analysis – Deep Dive">Chapter 4</a>, <em class="calibre15">Analyzing Data - Deep Dive</em></li></ul></div></div></div></div>



  </body></html>