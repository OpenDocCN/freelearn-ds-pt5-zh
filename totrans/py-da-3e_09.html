<html><head></head><body>
        

                            
                    Cleaning Messy Data
                
            
            
                
<p>Data analysts and scientists spend most of their time cleaning data and pre-processing messy datasets. While this activity is less talked about, it is one of the most performed activities and one of the most important skills for any data professional. Mastering the skill of data cleaning is necessary for any aspiring data scientist. Data cleaning and pre-processing is the process of identifying, updating, and removing corrupt or incorrect data. Cleaning and pre-processing results in high-quality data for robust and error-free analysis. Quality data can beat complex algorithms and outperform simple and less complex algorithms. In this context, high quality means accurate, complete, and consistent data. Data cleaning is a set of activities such as handling missing values, removing outliers, feature encoding, scaling, transformation, and splitting.</p>
<p>This chapter focuses on data cleaning, manipulation, and wrangling. Data preparation, manipulation, wrangling, and munging are all terms for the same thing, and the main objective is to clean up the data in order to get valuable insights. We will start by exploring employee data and then start filtering the data and handling missing values and outliers. After cleaning, we will focus on performing data transformation activities such as encoding, scaling, and splitting. We will mostly be using <kbd>pandas</kbd> and <kbd>scikit-learn</kbd> in this chapter.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Exploring data</li>
<li>Filtering data to weed out the noise</li>
<li>Handling missing values</li>
<li>Handling outliers</li>
<li>Feature encoding techniques</li>
<li>Feature scaling</li>
<li>Feature transformation</li>
<li>Feature splitting</li>
</ul>
<p>Let's get started!</p>
<h1 id="uuid-af9560f4-277b-43c3-91f9-06c21e388a9d">Technical requirements</h1>
<p>The following are the technical requirements for this chapter:</p>
<ul>
<li>You can find the code and the datasets that will be used in this chapter in this book's GitHub repository at <a href="https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter07">https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter07</a>.</li>
<li>All the code is available in the <kbd>ch7.ipynb</kbd> file. </li>
<li>This chapter uses only one CSV file (<kbd>employee.csv</kbd>) for practice purposes.</li>
<li>In this chapter, we will use the <kbd>pandas</kbd> and <kbd>scikit-learn</kbd> Python libraries, so please ensure you have them installed.</li>
</ul>
<h1 id="uuid-aaff02ce-c97c-40e3-948a-876bd61c6a80">Exploring data</h1>
<p>In this section, we will explore data by performing <strong>Exploratory Data Analysis</strong> (<strong>EDA</strong>). EDA is the most critical and most important component of the data analysis process. EDA offers the following benefits:</p>
<ul>
<li>It provides an initial glimpse of data and its context.</li>
<li>It captures quick insights and identifies the potential drivers from the data for predictive analysis. It finds the queries and questions that can be answered for decision-making purposes.</li>
<li>It assesses the quality of the data and helps us build the road map for data cleaning and preprocessing.</li>
<li>It finds missing values, outliers, and the importance of features for analysis.</li>
<li>EDA uses descriptive statistics and visualization techniques to explore data.</li>
</ul>
<p>In EDA, the first step is to read the dataset. We can read the dataset using <kbd>pandas</kbd>. The <kbd>pandas</kbd> library offers various options for reading data. It can read files in various formats, such as CSV, Excel, JSON, parquet, HTML, and pickle. All these methods were covered in the previous chapter. After reading the data, we can explore the data. This initial exploration will help us understand the data and gain some domain insights. Let's start with the EDA process. </p>
<p>First, we will read the <kbd>employee.csv</kbd> file (you can find this file in the <kbd>Chapter-7</kbd> folder of this book's GitHub repository at <a href="https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter07/employee.csv">https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter07/employee.csv</a>):</p>
<pre># import pandas<br/>import pandas as pd<br/><br/># Read the data using csv<br/>data=pd.read_csv('employee.csv')</pre>
<p>Let's take a look at the first five records in the file using the <kbd>head()</kbd> method:</p>
<pre># See initial 5 records<br/>data.head()</pre>
<p>This results in the following output:</p>
<div><img src="img/3d02b414-5c80-4d3a-8499-ecaa7ec598c5.png"/></div>
<p>Similarly, let's look at the last five records using the <kbd>head()</kbd> method:</p>
<pre># See last 5 records<br/>data.tail()</pre>
<p>This results in the following output:</p>
<div><img src="img/51201083-8298-496b-9d0c-b5ace193c3b1.png"/></div>
<p>We can check the list of columns using the <kbd>columns</kbd> attribute:</p>
<pre># Print list of columns in the data<br/>print(data.columns)</pre>
<p>This results in the following output:</p>
<pre><strong>Index(['name', 'age', 'income', 'gender', 'department', 'grade',</strong><br/><strong>       'performance_score'], dtype='object')</strong></pre>
<p>Let's check out the list of columns by using the shape of the DataFrame by using the <kbd>shape</kbd> attribute:</p>
<pre># Print the shape of a DataFrame<br/>print(data.shape)</pre>
<p>This results in the following output:</p>
<pre><strong>(9, 7)</strong></pre>
<p>As we can see, the dataset has <kbd>9</kbd> rows and <kbd>7</kbd> columns.</p>
<p>We can check the table schema, its columns, rows, data types, and missing values in the DataFrame by using the following code:</p>
<pre># Check the information of DataFrame<br/>data.info()</pre>
<p>This results in the following output:</p>
<div><img style="" src="img/166eeb97-d7d1-43f2-ae99-8eeafbbcdcca.png"/></div>
<p>In the preceding output, you can see that there are 7 columns in the data. Out of these 7 columns, 3 columns (age, income, and gender) have missing values. Out of these 7 columns, 4 are objects, 2 are floats, and 1 is an integer.</p>
<p>Now, let's take a look at the descriptive statistics of the data by using the <kbd>describe</kbd> function. This function will describe numerical objects. In our example, the age, income, and performance scores will describe the count, mean, standard deviation, min-max, and the first, second, and third quartiles:</p>
<pre># Check the descriptive statistics<br/>data.describe()</pre>
<p>This results in the following output:</p>
<div><img src="img/f3817ae7-9822-4147-a125-27a142c0b676.png"/></div>
<p>In the preceding code block, we have checked the descriptive statistics values of the data using the <kbd>describe()</kbd> function. From these results, we can interpret that the employee's age is ranging from 23 to 54 years. Here, the mean age is 40 years and the median age is 45 years. Similarly, we can draw conclusions for income and performance scores. Now that we've described the data, let's learn how to filter noise from data.</p>
<h1 id="uuid-19102d85-a441-4967-9607-6e1f0e3d2337">Filtering data to weed out the noise</h1>
<p>In the last two decades, the data size of companies and government agencies has increased due to digitalization. This also caused an increase in consistency, errors, and missing values. Data filtering is responsible for handling such issues and optimizing them for management, reporting, and predictions. The filtering process boosts the accuracy, relevance, completeness, consistency, and quality of the data by processing dirty, messy, or coarse datasets. It is a very crucial step for any kind of data management because it can make or break a competitive edge of business. Data scientists need to master the skill of data filtering. Different kinds of data need different kinds of treatment. That's why a systematic approach to data filtering needs to be taken.</p>
<p>In the previous section, we learned about data exploration, while in this section, we will learn about data filtering. Data can be filtered either column-wise or row-wise. Let's explore them one by one.</p>
<h2 id="uuid-fda0cc4f-f6e5-4b07-8690-3966b84215c3">Column-wise filtration  </h2>
<p>In this subsection, we will learnhow to filter column-wise data. We can filter columns using the <kbd>filter()</kbd> method. The <kbd>slicing []. filter()</kbd> method selects the columns when they're passed as a list of columns. Take a look at the following example:</p>
<pre># Filter columns<br/>data.filter(['name', 'department'])</pre>
<p>This results in the following output:</p>
<div><img src="img/acc285e8-6ca1-49dd-84d5-445a40c8fd8f.png"/></div>
<p>Similarly, we can also filter columns using slicing. In slicing, a single column does not need a list, but when we are filtering multiple columns, then they should be on the list. The output of a single column is a pandas Series. If we want the output as a DataFrame, then we need to put the name of the single column into a list. Take a look at the following example:</p>
<pre># Filter column "name"<br/>data['name']<br/><br/><strong>0       Allen Smith</strong><br/><strong>1           S Kumar</strong><br/><strong>2       Jack Morgan</strong><br/><strong>3         Ying Chin</strong><br/><strong>4     Dheeraj Patel</strong><br/><strong>5     Satyam Sharma</strong><br/><strong>6      James Authur</strong><br/><strong>7        Josh Wills</strong><br/><strong>8          Leo Duck</strong><br/><strong>Name: name, dtype: object</strong></pre>
<p>In the preceding example, we have selected a single column without passing it into the list and the output is a pandas Series. </p>
<p>Now, let's select a single column using a Python list:</p>
<pre># Filter column "name"<br/>data[['name']]</pre>
<p>This results in the following output:</p>
<div><img src="img/f42be3d8-d872-45b4-b40c-c39aa268a364.png"/></div>
<p>As you can see, a single column can be selected using a Python list. The output of this filter is a pandas DataFrame with a single column.</p>
<p>Now, let's filter multiple columns from the pandas DataFrame:</p>
<pre># Filter two columns: name and department<br/>data[['name','department']]</pre>
<p>This results in the following output:</p>
<div><img src="img/ea038b30-cb77-48f8-bd72-7a063d4a0579.png"/></div>
<p>As you can see, we have filtered the two columns without using the <kbd>filter()</kbd> function. </p>
<h2 id="uuid-f05d0521-c3a1-4b89-8f4d-c2921eaa3062">Row-wise filtration  </h2>
<p>Now, let's filter row-wise data. We can filter data using indices, slices, and conditions. In indices, you have to pass the index of the record, while for slicing, we need to pass the slicing range. Take a look at the following example:</p>
<pre># Select rows for the specific index<br/>data.filter([0,1,2],axis=0)</pre>
<p>This results in the following output:</p>
<div><img src="img/9d16add6-cd38-4581-ab1c-96fc90668c35.png"/></div>
<p>In the preceding example, we have filtered the data based on indexes.</p>
<p>The following is an example of filtering data by slicing:</p>
<pre># Filter data using slicing<br/>data[2:5]</pre>
<p>This results in the following output:</p>
<div><img src="img/be28f328-311c-4eb3-b66b-68902c7adfff.png"/></div>
<p>In condition-based filtration, we have to pass some conditions in square brackets, <kbd>[ ]</kbd>,  or brackets, <kbd>( )</kbd>. For a single value, we use the <kbd>==</kbd> (double equal to) condition, while for multiple values, we use the <kbd>isin()</kbd> function and pass the list of values. Let's take a look at the following example:</p>
<pre># Filter data for specific value<br/>data[data.department=='Sales']</pre>
<p>This results in the following output:</p>
<div><img src="img/4f23ef39-6cf0-413a-8d85-64ff83f73846.png"/></div>
<p>In the preceding code, we filtered the department sales in the first line of code using <kbd>==</kbd> (double equal to) as a condition. Now, let's filter multiple columns using the <kbd>isin()</kbd> function:</p>
<pre># Select data for multiple values<br/>data[data.department.isin(['Sales','Finance'])]</pre>
<p>This results in the following output:</p>
<div><img src="img/9d775a08-dd7b-4a96-9775-284254f7d2a0.png" style=""/></div>
<p>In the preceding example, we filtered the department sales and finance department using the <kbd>isin()</kbd> function.</p>
<p>Now, let's look at the <kbd>&gt;=</kbd> and <kbd>&lt;=</kbd> conditions for continuous variables. We can have single or multiple conditions. Let's take a look at the following example:</p>
<pre># Filter employee who has more than 700 performance score<br/>data[(data.performance_score &gt;=700)]</pre>
<p>This results in the following output:</p>
<div><img src="img/eb7360aa-658f-4f1c-80a4-429e794ff40c.png"/></div>
<p>In the preceding example, we filtered employees on the basis of their performance score (performance_score &gt;=700). Now, let's filter data using multiple conditions:</p>
<pre># Filter employee who has more than 500 and less than 700 performance score<br/>data[(data.performance_score &gt;=500) &amp; (data.performance_score &lt; 700)]</pre>
<p>This results in the following output:</p>
<div><img src="img/ac661fab-a43c-4c91-b0e9-23a07e3b9c75.png"/></div>
<p>We can also try the <kbd>query()</kbd> method. This method queries the columns using a boolean expression. Let's look at an example:</p>
<pre># Filter employee who has performance score of less than 500<br/>data.query('performance_score&lt;500')</pre>
<p>This results in the following output:</p>
<div><img src="img/050ffa66-9c67-4c42-b988-b30fa16c3d76.png"/></div>
<p>In the preceding example, we filtered the employees who have performance scores less than 500. Now, let's learn how to handle missing values.</p>
<h1 id="uuid-f8ab87cc-4434-4ec0-9a71-c83eaede52e2">Handling missing values</h1>
<p>Missing values are the values that are absent from the data. Absent values can occur due to human error, privacy concerns, or the value not being filled in by the respondent filling in the survey. This is the most common problem in data science and the first step of data preprocessing. Missing values affect a machine learning model's performance. Missing values can be handled in the following ways:</p>
<ul>
<li>Drop the missing value records.</li>
<li>Fill in the missing value manually.</li>
<li>Fill in the missing values using the measures of central tendency, such as mean, median, and mode. The mean is used to impute the numeric feature, the median is used to impute the ordinal feature, and the mode or highest occurring value is used to impute the categorical feature.</li>
<li>Fill in the most probable value using machine learning models such as regression, decision trees, KNNs.</li>
</ul>
<p>It is important to understand that in some cases, missing values will not impact the data. For example, driving license numbers, social security numbers, or any other unique identification numbers will not impact the machine learning models because they can't be used as features in the model. </p>
<p>In the following subsections, we will look at how missing values can be handled in more detail. First, we'll learn how to drop missing values.</p>
<h2 id="uuid-73e8d2bd-149e-4f9a-a3b6-91b4739ec62f">Dropping missing values</h2>
<p>In Python, missing values can be dropped using the <kbd>dropna()</kbd> function. <kbd>dropna</kbd> takes one argument: <kbd>how</kbd>. <kbd>how</kbd> can take two values: <kbd>all</kbd> or <kbd>any</kbd>. <kbd>any</kbd> drops certain rows that contain NAN or missing values, while <kbd>all</kbd> drops all the rows contains NAN or missing values:</p>
<pre># Drop missing value rows using dropna() function<br/># Read the data<br/><br/>data=pd.read_csv('employee.csv')<br/>data=data.dropna()<br/>data</pre>
<p>This results in the following output:</p>
<div><img src="img/b7ec72c4-477c-4d59-9eb6-a5214c6913f0.png"/></div>
<p>This summarizes the dataset as a dataframe.</p>
<h3 id="uuid-14f2a3c4-5582-4cac-8b69-5a943495fc6a">Filling in a missing value</h3>
<p>In Python, missing values can be dropped using the <kbd>fillna()</kbd> function. The <kbd>fillna()</kbd> function takes one value that we want to fill at the missing place. We can fill in the missing values using the mean, median, and mode:</p>
<pre># Read the data<br/>data=pd.read_csv('employee.csv')<br/><br/># Fill all the missing values in the age column with mean of the age column<br/>data['age']=data.age.fillna(data.age.mean())<br/>data</pre>
<p>This results in the following output:</p>
<div><img src="img/9d5464ea-c441-4d9d-9b0d-20a15aed67ca.png"/></div>
<p>In the preceding example, the missing values in the age column have been filled in with the mean value of the age column. Let's learn how to fill in the missing values using the median:</p>
<pre># Fill all the missing values in the income column with a median of the income column<br/>data['income']=data.income.fillna(data.income.median())<br/>data</pre>
<p>This results in the following output:</p>
<div><img src="img/03216565-ff03-4564-a983-d8d3f062abc7.png"/></div>
<p>In the preceding example, the missing values in the income column have been filled in with the median value of the income column. Let's learn how to fill in missing values using the mode:</p>
<pre># Fill all the missing values in the gender column(category column) with the mode of the gender column<br/>data['gender']=data['gender'].fillna(data['gender'].mode()[0])<br/>data</pre>
<p>This results in the following output:</p>
<div><img src="img/07110f3e-266b-4b0a-ad09-b26324b99100.png"/></div>
<p>In the preceding code example, the missing values in the gender column have been filled in with the mode value of the gender column. As you have seen, the mean, median, and mode help us handle missing values in pandas DataFrames. In the next section, we will focus on how to handle outliers.</p>
<h1 id="uuid-6935e195-1ee8-48a3-a95d-2af066d1fe43">Handling outliers</h1>
<p>Outliers are those data points that are distant from most of the similar points – in other words, we can say the outliers are entities that are different from the crowd. Outliers cause problems when it comes to building predictive models, such as long model training times, poor accuracy, an increase in error variance, a decrease in normality, and a reduction in the power of statistical tests.</p>
<p>There are two types of outliers: univariate and multivariate. Univariate outliers can be found in single variable distributions, while multivariates can be found in n-dimensional spaces. We can detect and handle outliers in the following ways:</p>
<ul>
<li><strong>Box Plot</strong>: We can use a box plot to create a bunch of data points through quartiles. It groups the data points between the first and third quartile into a rectangular box. The box plot also displays the outliers as individual points using the interquartile range.</li>
<li><strong>Scatter Plot</strong>: A scatter plot displays the points (or two variables) on the two-dimensional chart. One variable is placed on the x-axis, while the other is placed on the y-axis.</li>
<li><strong>Z-Score</strong>: The Z-score is a kind of parametric approach to detecting outliers. It assumes a normal distribution of the data. The outlier lies in the tail of the normal curve distribution and is far from the mean:</li>
</ul>
<div><img src="img/94e69c69-f0e5-4091-b438-416b93b77c12.png" style=""/></div>
<ul>
<li><strong>Interquartile Range (IQR)</strong>: IQR is a robust statistical measure of data dispersion. It is the difference between the third and first quartile. These quartiles can be visualized in a box plot. This is also known as the midspread, the middle 50%, or H-spread:</li>
</ul>
<div><img src="img/60d1ecea-dfd3-41bd-a8ca-50b0eb2ce248.png" style=""/></div>
<ul>
<li><strong>Percentile</strong>: A percentile is a statistical measure that divides data into 100 groups of equal size. Its value indicates the percentage of the population below that value. For example, the 95th percentile means 95% of people fall under this category.</li>
</ul>
<p>Let's drop some outliers using standard deviation and the mean: </p>
<pre># Dropping the outliers using Standard Deviation<br/># Read the data<br/>data=pd.read_csv('employee.csv')<br/><br/># Dropping the outliers using Standard Deviation<br/>upper_limit= data['performance_score'].mean () + 3 * data['performance_score'].std ()<br/>lower_limit = data['performance_score'].mean () - 3 * data['performance_score'].std ()<br/>data = data[(data['performance_score'] &lt; upper_limit) &amp; (data['performance_score'] &gt; lower_limit)]<br/>data</pre>
<p>This results in the following output:</p>
<div><img src="img/1ae1c2b9-ca13-4b8f-af45-3d4e6c245f0a.png" style=""/></div>
<p>In the preceding example, we are handling the outliers using standard deviation and the mean. We are using <img class="fm-editor-equation" src="img/4a8e21b2-2dfc-4fd4-b71f-83efba392c62.png" style="width:15.92em;height:1.08em;"/> as the upper limit and <img class="fm-editor-equation" src="img/d88fce77-3079-49f8-9481-b6a4bd758751.png" style="width:15.92em;height:1.08em;"/> as the lower limit for filtering the outliers. We can also try the percentile values to remove the outliers. Let's take a look at the following example:</p>
<pre># Read the data<br/>data=pd.read_csv('employee.csv')<br/><br/># Drop the outlier observations using Percentiles<br/>upper_limit = data['performance_score'].quantile(.99)<br/>lower_limit = data['performance_score'].quantile(.01)<br/>data = data[(data['performance_score'] &lt; upper_limit) &amp; (data['performance_score'] &gt; lower_limit)]<br/>data</pre>
<p>This results in the following output:</p>
<div><img src="img/98b31a6c-6ed8-4cfd-9ef2-91ebf8e8f072.png" style=""/></div>
<p>In the preceding code example, we handled the outliers using percentiles. We removed the outliers by using a percentile of 1 for the lower limit and by using a percentile of 99 for the upper limit. This helps us handle outliers in pandas DataFrames. In the next section, we will focus on how to perform feature encoding.</p>
<h1 id="uuid-ca99cc6e-3334-4cb3-9fdb-88763fe2afbf">Feature encoding techniques</h1>
<p>Machine learning models are mathematical models that required numeric and integer values for computation. Such models can't work on categorical features. That's why we often need to convert categorical features into numerical ones. Machine learning model performance is affected by what encoding technique we use. Categorical values range from 0 to N-1 categories.</p>
<h2 id="uuid-580bc989-12ae-4bfd-8372-293d626d4c99">One-hot encoding</h2>
<p>One-hot encoding transforms the categorical column into labels and splits the column into multiple columns. The numbers are replaced by binary values such as 1s or 0s. For example, let's say that, in the <kbd>color</kbd> variable, there are three categories; that is, <kbd>red</kbd>, <kbd>green</kbd>, and <kbd>blue</kbd>. These three categories are labeled and encoded into binary columns, as shown in the following diagram:</p>
<div><img src="img/704e22f0-db8d-4d31-9459-d632d51797fb.png" style=""/></div>
<p class="mce-root"/>
<p>One-hot encoding can also be performed using the <kbd>get_dummies()</kbd> function. Let's use the <kbd>get_dummies()</kbd> function as an example:</p>
<pre># Read the data<br/>data=pd.read_csv('employee.csv')<br/># Dummy encoding<br/>encoded_data = pd.get_dummies(data['gender'])<br/><br/># Join the encoded _data with original dataframe<br/>data = data.join(encoded_data)<br/><br/># Check the top-5 records of the dataframe<br/>data.head()</pre>
<p>This results in the following output:</p>
<div><img src="img/51e5d4fd-f955-4cd3-96be-c331ebaeeba0.png" style=""/></div>
<p>Here, we can see two extra columns, F and M. Both columns are dummy columns that were added by the Boolean encoder. We can also perform the same task with <kbd>OneHotEncoder</kbd> from the <kbd>scikit-learn</kbd> module. Let's look at an example of using <kbd>OneHotEncoder</kbd>.</p>
<pre># Import one hot encoder<br/>from sklearn.preprocessing import OneHotEncoder<br/><br/># Initialize the one-hot encoder object<br/>onehotencoder = OneHotEncoder()<br/><br/># Fill all the missing values in income column(category column) with mode of age column<br/>data['gender']=data['gender'].fillna(data['gender'].mode()[0])<br/><br/># Fit and transforms the gender column<br/>onehotencoder.fit_transform(data[['gender']]).toarray()</pre>
<p>This results in the following output:</p>
<pre>array([[1., 0.],<br/>       [1., 0.],<br/>       [0., 1.],<br/>       [1., 0.],<br/>       [1., 0.],<br/>       [1., 0.],<br/>       [1., 0.],<br/>       [1., 0.],<br/>       [0., 1.]])</pre>
<p class="mce-root">In the preceding code example, we imported <kbd>OneHotEncoder</kbd>, initialized its object, and then fit and transformed the model on the gender column. We can see that the output array has two columns for female and male employees.</p>
<h2 id="uuid-c8c628a6-e864-4a1f-8f76-dac18113aa53">Label encoding</h2>
<p>Label encoding is also known as integer encoding. Integer encoding replaces categorical values with numeric values. Here, the unique values in variables are replaced with a sequence of integer values. For example, let's say there are three categories: red, green, and blue. These three categories were encoded with integer values; that is, <kbd>red</kbd> is 0, <kbd>green</kbd> is 1, and <kbd>blue</kbd> is 2.</p>
<p>Let's take a look at the following label encoding example:</p>
<pre># Import pandas<br/>import pandas as pd<br/><br/># Read the data<br/>data=pd.read_csv('employee.csv')<br/><br/># Import LabelEncoder<br/>from sklearn.preprocessing import LabelEncoder<br/><br/># Instantiate the Label Encoder Object<br/>label_encoder = LabelEncoder()<br/><br/># Fit and transform the column<br/>encoded_data = label_encoder.fit_transform(data['department'])<br/><br/># Print the encoded<br/>print(encoded_data)</pre>
<p>This results in the following output:</p>
<pre>[2 1 0 0 2 1 2 1 0 2]</pre>
<p>In the preceding example, we performed simple label encoding. </p>
<p>In the following example, we are encoding the department column using the <kbd>LabelEncoder</kbd> class. First, we must import and initialize the <kbd>LabelEncoder</kbd> object and then fit and transform the column that we want to encode.  Let's perform the inverse transformation on the encoded labels:</p>
<pre># Perform inverse encoding<br/>inverse_encode=label_encoder.inverse_transform([0, 0, 1, 2])<br/><br/># Print inverse encode<br/>print(inverse_encode)</pre>
<p>This results in the following output:</p>
<pre>['Finance' 'Finance' 'Operations' 'Sales']</pre>
<p>In the preceding example, we reversed the encoding of the encoded values using <kbd>inverse_transformation()</kbd>. We can also use one-hot encoding with numerical variables. Here, each unique numeric value is encoded into an equivalent binary variable.</p>
<h2 id="uuid-9db0475a-1992-4c4c-bf0a-f937e5585640">Ordinal encoder</h2>
<p>Ordinal encoding is similar to label encoding, except there's an order to the encoding. The output encoding will start from 0 and end at one less than the size of the categories. Let's look at an example containing employee grades such as G0, G1, G2, G3, and G4. These five grades have been encoded with ordinal integer values; that is, <kbd>G0</kbd> is 0, <kbd>G1</kbd> is 1, <kbd>G2</kbd> is 2, <kbd>G3</kbd> is 3, and <kbd>G4</kbd> is 4. We can define the order of the values as a list and pass it to the category parameter. The ordinal encoder uses the integer or numeric values to encode. Here, the integer and numeric values are ordinal in nature. This encoding helps machine learning algorithms take advantage of this ordinal relationship.</p>
<p>Let's take a look at the following <kbd>OrdinalEncoder</kbd> example:</p>
<pre># Import pandas and OrdinalEncoder<br/>import pandas as pd<br/>from sklearn.preprocessing import OrdinalEncoder<br/><br/># Load the data<br/>data=pd.read_csv('employee.csv')<br/><br/># Initialize OrdinalEncoder with order<br/>order_encoder=OrdinalEncoder(categories=['G0','G1','G2','G3','G4'])<br/><br/># fit and transform the grade<br/>data['grade_encoded'] = label_encoder.fit_transform(data['grade'])<br/><br/># Check top-5 records of the dataframe<br/>data.head()</pre>
<p>This results in the following output:</p>
<div><img src="img/0a85a5ae-87a2-47e7-9b6f-75b4814c48ec.png"/></div>
<p>The preceding example is similar to the <kbd>LabelEncoder</kbd> example, except for the order of the values that were passed when the <kbd>OrdinalEncoder</kbd> object was initialized. In this example, the <kbd>categories</kbd> parameters were passed alongside the <kbd>grade</kbd> order at the time of initialization.</p>
<h1 id="uuid-27919f1f-7f26-4c5a-865e-9b5e56ba720f">Feature scaling</h1>
<p>In real life, most features have different ranges, magnitudes, and units, such as age being between 0-200 and salary being between 0 to thousands or millions. From a data analyst or data scientist's point of view, how can we compare these features when they are on different scales? High-magnitude features will weigh more on machine learning models than lower magnitude features. Thankfully, feature scaling or feature normalization can solve such issues.</p>
<p>Feature scaling brings all the features to the same level of magnitude. This is not compulsory for all kinds of algorithms; some algorithms clearly need scaled data, such as those that rely on Euclidean distance measures such as K-nearest neighbor and the K-means clustering algorithm.</p>
<h2 id="uuid-bc58c319-e168-44a1-add0-41f7bf31f789">Methods for feature scaling</h2>
<p>Now, let's look at the various methods we can use for feature scaling:</p>
<ul>
<li><strong>Standard Scaling or Z-Score Normalization</strong>: This method computes the scaled values of a feature by using the mean and standard deviation of that feature. It is best suited for normally distributed data. Suppose <sub><img class="fm-editor-equation" src="img/c70a5dad-c951-4138-b04c-706796d00869.png" style="width:0.75em;height:1.00em;"/></sub> is the mean and <sub><img class="fm-editor-equation" src="img/b2a11c1a-d40a-41ce-a8cb-d762bf9132ee.png" style="width:0.92em;height:0.92em;"/></sub> is the standard deviation of the feature column. This results in the following formula:</li>
</ul>
<div><img src="img/1438fcd1-7e86-409c-8c14-e034bd0a83ea.png" style=""/></div>
<p style="padding-left: 60px">Let's take a look at the following standard scaling example:</p>
<pre style="padding-left: 60px"># Import StandardScaler(or z-score normalization)<br/>from sklearn.preprocessing import StandardScaler<br/><br/># Initialize the StandardScaler<br/>scaler = StandardScaler()<br/><br/># To scale data<br/>scaler.fit(data['performance_score'].values.reshape(-1,1))<br/>data['performance_std_scaler']=scaler.transform(data['performance_score'].values.reshape(-1,1))<br/>data.head()</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<div><img src="img/b4d91332-cd72-4946-9f97-2c0b19da5675.png" style=""/></div>
<p style="padding-left: 60px">Here, we need to import and initialize the <kbd>StandardScaler</kbd> object. After initialization, we must perform fit and transform operations on the column that we want to scale.</p>
<ul>
<li><strong>Min-Max Scaling</strong>: This method linearly transforms the original data into the given range. It preserves the relationships between the scaled data and the original data. If the distribution is not normally distributed and the value of the standard deviation is very small, then the min-max scaler works better since it is more sensitive to outliers. Let's say that <sub><img class="fm-editor-equation" src="img/88f1ca2e-0121-4f60-9263-574826d461ad.png" style="width:2.58em;height:1.08em;"/></sub> is the minimum value and <sub><img class="fm-editor-equation" src="img/7d5098d6-0da7-4477-9c34-f6f2646aaffa.png" style="width:2.75em;height:0.92em;"/></sub>is the maximum value of a feature column, while <img class="fm-editor-equation" src="img/1d4830e9-dcbc-4692-b7d8-423e060735e3.png" style="width:5.17em;height:1.08em;"/> and <img class="fm-editor-equation" src="img/a6fc4c91-4f05-4b5f-8486-1a1d98183a7e.png" style="width:5.92em;height:1.00em;"/> are the new minimum and new maximum. This results in the following formula:</li>
</ul>
<div><img src="img/a13a857a-ff6e-4795-9e10-df7d5865fd52.png" style=""/></div>
<p style="padding-left: 60px">Let's take a look at the following min-max scaling example:</p>
<pre style="padding-left: 60px"># Import MinMaxScaler<br/>from sklearn.preprocessing import MinMaxScaler<br/><br/># Initialise the MinMaxScaler<br/>scaler = MinMaxScaler()<br/><br/># To scale data<br/>scaler.fit(data['performance_score'].values.reshape(-1,1))<br/>data['performance_minmax_scaler']=scaler.transform(data['performance_score'].values.reshape(-1,1))<br/>data.head()</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<div><img src="img/77655e38-fc1e-40e7-b6cf-e018e3f3f644.png"/></div>
<p> </p>
<p style="padding-left: 60px">Here, we need to import and initialize the <kbd>MinMaxScaler</kbd> object. After initialization, we, must perform the fit and transform operations on the column that we want to scale.</p>
<ul>
<li><strong>Robust Scaling</strong>: This method is similar to the min-max scaler method. Instead of min-max, this method uses an interquartile range. That's why it is robust to outliers. Suppose <img class="fm-editor-equation" src="img/460af998-c191-4b5d-8e94-ce27f70f91dc.png" style="width:1.58em;height:0.92em;"/>and <img class="fm-editor-equation" src="img/47c520a2-9875-422b-9fa7-7d9926f6fb43.png" style="width:1.67em;height:0.92em;"/> are the first and third quartiles of column x. This results in the following formula:</li>
</ul>
<div><img src="img/bed79988-4f47-4f1d-a43e-bd34e33ff648.png" style=""/></div>
<p style="padding-left: 60px">Let's take a look at the following robust scaling example:</p>
<pre style="padding-left: 60px"># Import RobustScaler<br/>from sklearn.preprocessing import RobustScaler<br/><br/># Initialise the RobustScaler<br/>scaler = RobustScaler()<br/><br/># To scale data<br/>scaler.fit(data['performance_score'].values.reshape(-1,1))<br/>data['performance_robust_scaler']=scaler.transform(data['performance_score'].values.reshape(-1,1))<br/><br/># See initial 5 records<br/>data.head()</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<div><img src="img/098a721a-ae9c-4f03-8d07-f40c52064ae0.png"/></div>
<p>Here, we need to import and initialize the <kbd>RobustScaler</kbd> object. After initialization, we must fit and transform the column that we want to scale.</p>
<h1 id="uuid-9bad5c61-e8e0-4576-877f-f0155dbc1bd8">Feature transformation</h1>
<p>Feature transformation alters features so that they're in the required form. It also reduces the effect of outliers, handles skewed data, and makes the model more robust. The following list shows the different kinds of feature transformation:</p>
<ul>
<li>Log transformation is the most common mathematical transformation used to transform skewed data into a normal distribution. Before applying the log transform, ensure that all the data values ​​only contain positive values; otherwise, this will throw an exception or error message.</li>
<li>Square and cube transformation has a moderate effect on distribution shape. It can be used to reduce left skewness.</li>
<li>Square and cube root transformation has a fairly strong transformation effect on the distribution shape but it is weaker than logarithms. It can be applied to right-skewed data.</li>
<li>Discretization can also be used to transform a numeric column or attribute. For example, the age of a group of candidates can be grouped into intervals such as 0-10, 11-20, and so on. We can also use discretization to assign conceptual labels instead of intervals such as youth, adult, and senior.</li>
</ul>
<p>If the feature is right-skewed or positively skewed or grouped at lower values, then we can apply the square root, cube root, and logarithmic transformations, while if the feature is left-skewed or negative skewed or grouped at higher values, then we can apply the cube, square, and so on.</p>
<p>Let's take a look at an example of discretization transformation:</p>
<pre># Read the data<br/>data=pd.read_csv('employee.csv')<br/><br/># Create performance grade function<br/>def performance_grade(score):<br/>    if score&gt;=700:<br/>        return 'A'<br/>    elif score&lt;700 and score &gt;= 500:<br/>        return 'B'<br/>    else:<br/>        return 'C'<br/><br/># Apply performance grade function on whole DataFrame using apply() function.<br/>data['performance_grade']=data.performance_score.apply(performance_grade)<br/><br/># See initial 5 records<br/>data.head()</pre>
<p>This results in the following output:</p>
<div><img src="img/eb1d7b77-c332-45a4-ad61-c18d0418f707.png"/></div>
<p>In the preceding example, we loaded the dataset and created the <kbd>performance_grade()</kbd> function. The <kbd>performance_grade()</kbd> function takes the performance score and converts it into grades; that is, <kbd>A</kbd>, <kbd>B</kbd>, and <kbd>C</kbd>.</p>
<h1 id="uuid-2bf4c713-801c-4253-9f1b-65fe3e885332">Feature splitting</h1>
<p>Feature splitting helps data analysts and data scientists create more new features for modeling. It allows machine learning algorithms to comprehend features and uncover potential information for decision-making; for example, splitting name features into first, middle, and last name and splitting an address into house number, locality, landmark, area, city, country, and zip code.</p>
<p>Composite features such as string and date columns violate the tidy data principles. Feature splitting is a good option if you wish to generate more features from a composite feature. We can utilize the components of a column to do this. For example, from a date object, we can easily get the year, month, and weekday. These features may directly affect the prediction model. There is no rule of thumb when it comes to breaking the features into components; this depends on the characteristics of the feature:</p>
<pre># Split the name column in first and last name<br/>data['first_name']=data.name.str.split(" ").map(lambda var: var[0])<br/>data['last_name']=data.name.str.split(" ").map(lambda var: var[1])<br/><br/># Check top-5 records<br/>data.head()</pre>
<p>This results in the following output:</p>
<div><img src="img/9bf00656-9c59-40e1-b7d1-c2d6ec146ee1.png"/></div>
<p>In the preceding example, we split the name column using the <kbd>split()</kbd> and <kbd>map()</kbd> functions. The <kbd>split()</kbd> function splits the name column using a space, while the <kbd>map()</kbd> function assigns the first divided string to the first name and the second divided string to the last name.</p>
<h1 id="uuid-5178697b-1522-45ba-b995-4d3e8fbff8b4">Summary</h1>
<p>In this chapter, we explored data preprocessing and feature engineering with Python. This had helped you gain important skills for data analysis. The main focus of this chapter was on cleaning and filtering out dirty data. We started with EDA and discussed data filtering, handling missing values, and outliers. After this, we focused on feature engineering tasks such as transformation, feature encoding, feature scaling, and feature splitting. We then explored various methods and techniques we can use when it comes to feature engineering.</p>
<p>In the next chapter, Chapter 8, <em>Signal Processing and Time Series</em>, we will focus on the importance of signal processing and time series data in Python. We'll start this chapter by analyzing time series data and discussing moving averages, autocorrelations, autoregressive models, and ARMA models. Then, we will look at signal processing and discuss Fourier transform, spectral transform, and filtering on signals.</p>


            

            
        
    </body></html>