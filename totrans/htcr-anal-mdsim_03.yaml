- en: Machine Learning Foundations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter provides an introduction to the mathematical foundations behind
    healthcare analytics and machine learning. It is intended mainly for healthcare
    professionals with little background knowledge of the math required for doing
    healthcare analytics. By the end of the chapter, you will be familiar with the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Medical decision making paradigms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basic machine learning pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model frameworks for medical decision making
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a poorly publicized fact that, in addition to the basic science courses
    and clinical rotations that they must do during their training, physicians also
    take courses in biostatistics and medical decision making. In these courses, prospective
    physicians learn some math and statistics that will help them as they sort through
    different symptoms, findings, and test results to arrive at diagnoses and treatment
    plans for their patients. Many physicians, already bombarded with endless medical
    facts and knowledge, shrug these courses off. Nevertheless, whether they learned
    it from these courses or from their own experiences, much of the reasoning that
    physicians use in their daily practice resembles the math behind some common machine
    learning algorithms. Let's explore that assertion a bit more in this section as
    we look at some popular frameworks for medical decision making and compare them
    to machine learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Tree-like reasoning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are all familiar with tree-like reasoning; it involves branching into various
    possible actions as different decision points are met. Here we look at tree-like
    reasoning more closely and examine its machine learning counterparts: the decision
    tree and the random forest.'
  prefs: []
  type: TYPE_NORMAL
- en: Categorical reasoning with algorithms and trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In one medical decision making paradigm, the clinical problem can be approached
    as a **tree** or an **algorithm**. Here, an algorithm does not refer to a "machine
    learning algorithm" in the computer science sense; it can be thought of as a structured,
    ordered set of rules to reach a decision. In this type of reasoning, the root
    of the tree represents the initiation of the patient encounter. As the physician
    learns more information while asking questions, they come to various branch or
    decision points where the physician can proceed in more than one route. These
    routes represent different clinical tests or alternate lines of questioning. The
    physician will repeatedly make decisions and pick the next branch, reaching a
    terminal node at which there are no more branches. The terminal node represents
    a definitive diagnosis or a treatment plan.
  prefs: []
  type: TYPE_NORMAL
- en: Here we have an example of a clinical management algorithm for weight and obesity
    management (National Heart, Lung, and Blood Institute, 2010). Each decision point
    (most of which are binary) is a diamond, while management plans are rectangles.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we have a female patient with several clinical variables
    that are measured: BMI = 27, waist circumference = 90 cm, and the number of cardiac
    risk factors = 3\. Starting at node #1, we skip from Node #2 directly to Node
    #4, since the BMI > 25\. At Node #5, again the answer is "Yes." At Node #7, again
    the answer is "Yes," taking us to the management plan outlined in Node #8:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d58ad818-5ab4-4107-b0f8-bd9a7e551a91.png)'
  prefs: []
  type: TYPE_IMG
- en: A second example of an algorithm that combines both diagnosis and treatment
    is shown as follows (Haggstrom, 2014; Kirk et al., 2014). In this algorithm for
    the diagnosis/treatment of pregnancy of an unknown location, a hemodynamically
    stable patient with no pain (a patient with stable heart and blood vessel function)
    is routed to have serum hCG drawn at 0 and 48 hours after presenting to the physician.
    Depending on the results, several possible diagnoses are given, along with corresponding
    management plans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in the clinical world, it is perfectly possible for these trees to
    be wrong; those cases are referred to as predictive errors. The goal in constructing
    any tree is to choose the best variables/cutpoints that minimize the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/084c9ce1-2a8e-4e6c-be6b-802ca2c4f7de.png)'
  prefs: []
  type: TYPE_IMG
- en: Algorithms have a number of advantages. For one, they model human diagnostic
    reasoning as sequences of hierarchical decisions or determinations. Also, their
    goal is to eliminate uncertainty by forcing the caretaker to provide a binary
    answer at each decision point. Algorithms have been shown to improve standardization
    of care in medical practice and are in widespread use for many medical conditions
    today not only in outpatient/inpatient practice but also prior to hospital arrival
    by **emergency medical technicians** (**EMTs**).
  prefs: []
  type: TYPE_NORMAL
- en: However, algorithms are often overly simplistic and don't consider the fact
    that medical symptoms, findings, or test results may not indicate 100% certainty.
    They are insufficient when multiple pieces of evidence must be weighed for arriving
    at a decision.
  prefs: []
  type: TYPE_NORMAL
- en: Corresponding machine learning algorithms – decision tree and random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding diagram, you may have noticed that the example tree most likely
    uses *subjectively* determined cutpoints in deciding which route to follow. For
    example, Diamond #5 uses a BMI cutoff of 25, and Diamond #7 uses a BMI cutoff
    of 30\. Nice, round numbers! In the decision analysis field, trees are usually
    constructed based on human inference and discussion. What if we could *objectively*
    determine the best variables to cut (and the corresponding cutpoints at which
    to cut) in order to minimize the error of the algorithm?'
  prefs: []
  type: TYPE_NORMAL
- en: This is just what we do when we train a formal **decision tree** using a machine
    learning algorithm. Decision trees evolved in the 1990s and used principles of
    information theory to optimize the branching variables/points of the tree to maximize
    the classification accuracy. The most common and simple algorithm for training
    a decision tree proceeds using what is known as a **greedy** approach. Starting
    at the first node, we take the training set of our data and **split** it based
    on each **variable**, using a variety of **cutpoints** for each variable. After
    each split, we calculate the entropy or information gain from the resulting split.
    Don't worry about the formulas for calculating these quantities, just know that
    they measure how much information is gained from the split, which correlates with
    how even the split is. For example, using the PUL algorithm shown previously,
    a split that results in eight normal intrauterine pregnancies and seven ectopic
    pregnancies would be favored over a split that results in 15 normal intrauterine
    pregnancies and zero ectopic pregnancies. Once we have the variable and cutpoint
    for the best split, we proceed and then repeat the method, using the remaining
    variables. To prevent **overfitting** the model to the training data, we stop
    splitting the tree when certain criteria are reached, or alternatively, we could
    train a big tree with many nodes and then remove (**prune**) some of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees have some limitations. For one thing, decision trees must split
    the decision space linearly at each step based on a single variable. Another problem
    is that decision trees are prone to overfitting. Because of these issues, decision
    trees typically aren't competitive with most state-of-the-art machine learning
    algorithms in terms of minimizing errors. However, the **random forest,** which
    is basically an ensemble of de-correlated decision trees, is currently among the
    most popular and accurate machine learning methods in medicine. We will make decision
    trees and random forests in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare* of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic reasoning and Bayes theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A second, more mathematical way of approaching the patient involves initializing
    the baseline probability of a disease for a patient and updating the probability
    of the disease with every new clinical finding discovered about the patient. The
    probability is updated using Bayes theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Using Bayes theorem for calculating clinical probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Briefly, Bayes theorem allows for the calculation of the post-test probability
    of a disease, given a pretest probability of disease, a test result, and the 2
    x 2 contingency table of the test. In this context, a "test" result does not have
    to be a lab test; it can be the presence or absence of any clinical finding as
    ascertained during the history and physical examination. For example, the presence
    of chest pain, whether the chest pain is substernal, the result of an exercise
    stress test, and the troponin result all qualify as clinical findings upon which
    post-test probabilities can be calculated. Although Bayes theorem can be extended
    to include continuously valued results, it is most convenient to binarize the
    test result before calculating the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the use of Bayes theorem, let's pretend you are a primary care
    physician and that a 55-year-old patient approaches you and says, "I’m having
    chest pain." When you hear the words "chest pain," the first life-threatening
    condition you are concerned about is a myocardial infarction. You can ask the
    question, "What is the likelihood that this patient is having a myocardial infarction?"
    In this case, the presence or absence of chest pain is the test (which is positive
    in this patient), and the presence or absence of myocardial infarction is what
    we're trying to calculate.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the baseline MI probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To calculate the probability that the chest-pain patient is having a **myocardial
    infarction** (**MI**), we must know three things:'
  prefs: []
  type: TYPE_NORMAL
- en: The pretest probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 2 x 2 contingency table of the clinical finding for the disease in question
    (MI, in this case)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result of this test (in this case, the patient is positive for chest pain)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the presence or absence of other findings is not yet known in the patient,
    we can take the pretest probability to be the baseline prevalence of MI in the
    population. Let's pretend that in your clinic's region, the baseline prevalence
    of MI in any given year is 5% for a 55-year-old person. Therefore, the pretest
    probability of MI in this patient is 5%. We will see later that the post-test
    probability of disease in this patient is the pretest probability multiplied by
    the likelihood ratio for positive chest pain (LR+). To get LR+, we need the 2
    x 2 contingency table.
  prefs: []
  type: TYPE_NORMAL
- en: 2 x 2 contingency table for chest pain and myocardial infarction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose the following table is the breakdown of chest pain and myocardial infarction
    in 400 patients who visited your clinic:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Myocardial Infarction present (D+)** | **Myocardial Infarction absent
    (D-)** | **Total** |'
  prefs: []
  type: TYPE_TB
- en: '| **Chest pain present (T+)** | 15 (TP) | 100 (FP) | 115 |'
  prefs: []
  type: TYPE_TB
- en: '| **Chest pain absent (T-)** | 5 (FN) | 280 (TN) | 285 |'
  prefs: []
  type: TYPE_TB
- en: '| **Total** | 20 | 380 | 400 |'
  prefs: []
  type: TYPE_TB
- en: Interpreting the contingency table and calculating sensitivity and specificity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding table, there are four numerical cells, labeled **TP**, **FP**,
    **FN**, and **TN**. These abbreviations stand for **true positives**, **false
    positives**, **false negatives**, and **true negatives**, respectively. The first
    word (true/false) indicates whether or not the test result matched the presence
    of disease as measured by the gold standard. The second word (positive/negative)
    indicates what the test result was. True positives and true negatives are desirable;
    this means that the test result is correct and the higher these numbers, the better
    the test is. On the other hand, false positives and false negatives are undesirable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two important quantities that can be calculated from the true/false positives/negatives
    include the **sensitivity** and the **specificity**. The sensitivity is a measure
    of how powerful the test is in detecting disease. It is expressed as the ratio
    of positive test results over the number of total patients who had the disease:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f57c38f2-5d63-427c-bdfa-a4e23dcd1f97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, the specificity is a measure of how good the test is at
    identifying patients who do not have the disease. It is expressed as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a94bae4-db71-474c-b262-9b03ce14ca85.png)'
  prefs: []
  type: TYPE_IMG
- en: These concepts can be confusing initially, so it may take some time and iterations
    before you get used to them, but the sensitivity and specificity are important
    concepts in biostatistics and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating likelihood ratios for chest pain (+ and -)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **likelihood ratio** is a measure of how much a test changes the likelihood
    of having a condition. It is often split into two quantities: the likelihood ratio
    of a positive test (LR+), and the likelihood ratio of a negative test (LR-).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The likelihood ratio for MI given a positive chest pain result is given by
    the following formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d17412ae-1c9b-4868-81d8-271832e712c6.png)![](img/6f713f3f-8fd0-49d7-a536-d6ac8ad7ee18.png)![](img/51b1818a-f713-4af4-bada-fa2e065f26fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The likelihood ratio for MI given a negative chest pain result would be given
    by the following formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/057e9976-eeea-424a-96a1-051227e920de.png)![](img/89c5d985-f456-4233-ab6c-a0d71a0391b4.png)![](img/1ddd20d5-f5d5-4aa0-878e-c86a2c91bcf9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the patient is positive for the presence of chest pain, only LR+ applies
    in this case. To get LR+, we use the appropriate numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Calculating the post-test probability of MI given the presence of chest pain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have LR+, we multiply it by the pretest probability to get the
    post-test probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This approach for diagnosis and management of the patient seems very appealing;
    being able to calculate an exact probability of disease seemingly eliminates many
    issues in diagnosis! Unfortunately, Bayes theorem breaks down in clinical practice
    for many reasons. First, a large amount of data is required at every step to update
    the probability. No physician or database has access to all the contingency tables
    required to update the Bayes theorem with every historical element or lab test
    result discovered about the patient. Second, this method of probabilistic reasoning
    is unnatural for humans to perform. The other techniques discussed are much more
    conducive to a performance by the human brain. Third, while the model may work
    for single diseases, it doesn’t work well when there are multiple diseases and
    comorbidities. Finally, and most importantly, the assumptions of conditional independence
    and exhaustiveness and exclusiveness that are fundamental to the Bayes theorem
    don’t hold in the clinical world. The reality is that symptoms and findings are
    not completely independent of each other; the presence or absence of one finding
    can influence that of many others. Together, these facts render the probability
    calculated by the Bayes theorem to be inexact and even misleading in most cases,
    even when one succeeds in calculating it. Nevertheless, Bayes theorem is important
    in medicine for many subproblems when ample evidence is available (for example,
    using chest pain characteristics to calculate the probability of MI during the
    patient history).
  prefs: []
  type: TYPE_NORMAL
- en: Corresponding machine learning algorithm – the Naive Bayes Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding example, we showed you how to calculate a post-test probability
    given a pretest probability, a likelihood, and a test result. The machine learning
    algorithm known as the Naive Bayes Classifier does this for every feature sequentially
    for a given observation. For example, in the preceding example, the post-test
    probability was 14.3%. Let's pretend that the patient now has a troponin drawn
    and it is elevated. 14.3% now becomes the pretest probability, and a new post-test
    probability is calculated based on the contingency table for troponin and MI,
    where the contingency tables are obtained from the training data. This is continued
    until all the features are exhausted. Again, the key assumption is that each feature
    is independent of all others. For the classifier, the category (outcome) having
    the highest post-test probability is assigned to the observation.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes Classifier is popular for a select group of applications. Its
    advantages include high interpretability, robustness to missing data, and ease/speed
    for training and predicting. However, its assumptions make the model unable to
    compete with more state-of-the-art algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Criterion tables and the weighted sum approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The third medical decision making paradigm we will discuss is the criterion
    table and its similarity to linear and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Criterion tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The use of criterion tables is partially motivated by an additional shortcoming
    of Bayes theorem: its sequential nature of considering each finding one at a time.
    Sometimes, it is more convenient to consider many factors simultaneously while
    considering diseases. What if we imagined the diagnosis of a certain disease as
    an additive sum of select factors? That is, in the MI example, the patient receives
    a point for having positive chest pain, a point for having a history of a positive
    stress test, and so on. We could establish a threshold for a point total that
    gives a positive diagnosis of MI. Because some factors are more important than
    others, we could use a weighted sum, in which each factor is multiplied by an
    importance factor before adding. For example, the presence of chest pain may be
    worth three points, and a history of a positive stress test may be worth five
    points. This is how criterion tables work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we have given the modified wells criteria as an example.
    The modified wells criteria (derived from Clinical Prediction, 2017) are used
    to determine whether or not a patient may have a **pulmonary embolism** (**PE**):
    a blood clot in the lung that is life-threatening. Note that criterion tables
    not only provide point values for each relevant clinical finding but also give
    thresholds for interpreting the total score:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Clinical finding** | **Score** |'
  prefs: []
  type: TYPE_TB
- en: '| Clinical symptoms of deep vein thrombosis (leg swelling, pain with palpation)
    | 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Alternative diagnosis is less likely than pulmonary embolism | 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Heart rate > 100 beats per minute | 1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Immobilization for > 3 days or surgery in the previous 4 weeks | 1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Previous diagnosis of deep vein thrombosis/pulmonary embolism | 1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Hemoptysis | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Patient has cancer | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |'
  prefs: []
  type: TYPE_TB
- en: '| **Risk stratification** |  |'
  prefs: []
  type: TYPE_TB
- en: '| Low risk for PE | < 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Medium risk for PE | 2.0 - 6.0 |'
  prefs: []
  type: TYPE_TB
- en: '| High risk for PE | > 6.0 |'
  prefs: []
  type: TYPE_TB
- en: Corresponding machine learning algorithms – linear and logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Notice that a criterion table tends to use nice, whole numbers that are easy
    to add. Obviously, this is so the criteria are convenient for physicians to use
    while seeing patients. What would happen if we could somehow determine the optimal
    point values for each factor, as well as the optimal threshold? Remarkably, the
    machine learning method called logistic regression does just this.
  prefs: []
  type: TYPE_NORMAL
- en: '**Logistic regression** is a popular statistical machine learning algorithm
    that is commonly used for binary classification tasks. It is a type of model known
    as a generalized linear model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand logistic regression, we must first understand **linear regression**.
    In linear regression, the *i*^(th) output variable (*y-hat*) is modeled as a weighted
    sum of the *p* individual predictor variables, *x[i]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98c7a9b8-9862-47b0-b8af-3b36482b8687.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The weights (beta) (also known as **coefficients**) of the variables can be
    determined by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edd0183b-6d4d-44ad-9d3a-1a02b73cafe1.png)'
  prefs: []
  type: TYPE_IMG
- en: Logistic regression is like linear regression, except that it applies a transformation
    to the output variable that limits its range to be between 0 and 1\. Therefore,
    it is well-suited to model probabilities of a positive response in classification
    tasks, since probabilities must also be between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression has many practical advantages. First of all, it is an intuitively
    simple model that is easy to understand and explain. Understanding its mechanics
    does not require much advanced mathematics beyond high school statistics, and
    can easily be explained to both technical and nontechnical stakeholders on a project.
  prefs: []
  type: TYPE_NORMAL
- en: Second, logistic regression is not computationally intensive, in terms of time
    or memory. The coefficients are simply a collection of numbers that is as long
    as the list of predictors, and its determination only involves several matrix
    multiplications (see the preceding second equation for an example). One caveat
    to this is that the matrices may be quite large when dealing with very large datasets
    (for example, billions of data points), but this is true of most machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Third, logistic regression does not require much preprocessing (for example,
    centering or scaling) of the variables (although transformations that move predictors
    toward a normal distribution can increase performance). As long as the variables
    are in a numeric format, that is enough to get started with logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, logistic regression, especially when coupled with regularization techniques
    such as lasso regularization, can have reasonably strong performance in making
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in today’s era of fast and powerful computing, logistic regression
    has largely been superseded by other algorithms that are more powerful, and typically
    more accurate. This is because logistic regression makes many major assumptions
    about the data and the modeling task:'
  prefs: []
  type: TYPE_NORMAL
- en: It assumes that every predictor has a linear relationship with the outcome variable.
    This is obviously not the case in most datasets. In other words, logistic regression
    is not strong at modeling nonlinearities in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It assumes that all of the predictors are independent of one another. Again,
    this is usually not the case, for example, two or more variables may interact
    to affect the prediction in a way that is more than just the linear sum of each
    variable. This can be partially remedied by adding products of predictors as interaction
    terms in the model, but choosing which interactions to model is not an easy task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is highly and adversely sensitive to multiply correlated predictor variables.
    In the presence of such data, logistic regression may cause overfitting. To overcome
    this, there are variable selection methods, such as forward step-wise logistic
    regression, backward step-wise logistic regression, and best subset logistic regression,
    but these algorithms are imprecise and/or time-intensive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, logistic regression is not robust to missing data, like some classifiers
    are (for example, Naive Bayes).
  prefs: []
  type: TYPE_NORMAL
- en: Pattern association and neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last medical decision making framework strikes at the heart of our neurobiological
    understanding of how we process information and make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Complex clinical reasoning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine that an elderly patient complaining of chest pain sees a highly experienced
    physician. Slowly, the clinician asks the appropriate questions and gets a representation
    of the patient as determined by the features of that patient's signs and symptoms.
    The patient says they have a history of high blood pressure but no other cardiac
    risk factors. The chest pain varies in intensity with the heartbeat (also known
    as pleuritic chest pain). The patient also reports they just came back to the
    United States from Europe. They also complain of swelling in the calf muscle.
    Slowly, the physician combines these lower level pieces of information (the absence
    of cardiac risk factors, the pleuritic chest pain, the prolonged period of immobility,
    a positive Homan's sign) and integrates it with memories of previous patients
    and the physician's own extensive knowledge to build a higher level view of this
    patient and realizes that the patient is having a pulmonary embolism. The physician
    orders a V/Q scan and proceeds to save the patient's life.
  prefs: []
  type: TYPE_NORMAL
- en: Such stories happen every day across the globe in medical clinics, hospitals,
    and emergency departments. Physicians use information from the patient history,
    exam, and test results to compose higher level understandings of their patients.
    How do they do it? The answer may lie in neural networks and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Corresponding machine learning algorithm – neural networks and deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How humans think and attain consciousness is certainly one of the universe's
    open questions. There is scarce knowledge on how human beings achieve rational
    thought or on how physicians make complex clinical decisions. However, perhaps
    the closest we have come to mimicking human brain performance in common cognitive
    tasks, as of this writing, is through neural networks and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: A **neural network** is modeled after the nervous system of mammals, in which
    predictor variables are connected to sequential layers of artificial "neurons”
    that aggregate and sum weighted inputs before sending their nonlinearly transformed
    outputs to the next layer. In this fashion, the data may pass through several
    layers before ultimately producing an outcome variable that indicates the likelihood
    of the target value is positive. The weights are usually trained by using the
    **backpropagation** technique, in which the negative difference between the correct
    output and predicted output is added to the weights at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The neural network and the backpropagation technique was first reported in the
    1980s in a famous paper published by *Nature* journal, as was discussed in [Chapter
    1](b15b2b73-d2bb-410f-ab55-5f0f1e91730e.xhtml), *Introduction to Healthcare Analytics*
    (Rumelhart et al., 1986); in the 2010s, modern computing power along with vast
    amounts of data led to the rebranding of neural networks as "**deep learning**."
    Along with the increases in computing power and data availability, there have
    been state-of-the-art performance gains in machine learning tasks, such as speech
    recognition, image and object identification, and digit recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental advantage of neural networks is that they are built to handle
    nonlinearities and complex interactions between predictor variables in the data.
    This is because each layer in a neural network is essentially performing a linear
    regression on the output of the previous layer, not simply on the input data itself.
    The more layers one has in a network, the more complex functions the network can
    model. The presence of nonlinear transformations in the neurons also contributes
    to this ability.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks also easily lend themselves to **multiclass problems**, in which
    there are more than two possible outcomes. Recognizing digits 0 through 9 is just
    one example of this.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks also have disadvantages. First of all, they have low interpretability
    and can be difficult to explain to nontechnical stakeholders on a project. Understanding
    neural networks requires knowledge of college-level calculus and linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Second of all, neural networks can be difficult to tune. There are often many
    parameters involved (for example, how to initialize weights, the number, and size
    of hidden layers, what activation functions to use, connectivity patterns, regularization,
    and learning rates) and tuning all of them systematically is close to impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, neural networks are prone to overfitting. Overfitting is when the model
    has “memorized” the training data and cannot generalize well to previously unseen
    data. This can happen if there are too many parameters/layers and/or if the data
    is iterated over too many times.
  prefs: []
  type: TYPE_NORMAL
- en: We will work with neural networks in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we spent a lot of time discussing machine learning models
    and how they correspond to frameworks for medical decision making. But how does
    one actually train a machine learning model? In healthcare, machine learning usually
    consists of a pattern of stereotyped tasks. We can refer to the collection of
    these tasks as a **pipeline**. While no two pipelines are exactly the same for
    any two machine learning applications, pipelines allow us to describe the machine
    learning process. In this section, we describe a generalized pipeline that many
    simple machine learning projects tend to follow, particularly when dealing with
    **structured data**, or data that can be organized into rows and columns.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can make computations on the data, it must be **loaded** from a storage
    location (usually a database or a real-time data feed) into a computing workspace.
    Workspaces allow the user to manipulate the data and build models using popular
    languages including R, Python, Hadoop, and Spark. Many commercial databases have
    specialized functionality in order to facilitate loading into workspaces. The
    machine learning languages themselves also have functions that read from text
    files and connect to and read from databases. Sometimes the user may also prefer
    to perform data quality control and cleansing directly in the database. This typically
    includes steps such as building a patient index, data normalization, and data
    cleaning. In [Chapter 4](e1b89921-e75b-4b16-a567-8970a173db53.xhtml), *Computing
    Foundations – Databases,* we discuss the manipulation of databases using the **Structured
    Query Language** (**SQL**) and in [Chapter 5](12ee77f2-0655-4dc5-abb1-2868d6fcc386.xhtml),
    *Computing Foundations – Introduction to Python,* we discuss methods for loading
    the data into a Python workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and preprocessing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a popular saying in data science that goes along the lines of, "For
    every 10 hours of a data scientist's time, 7 hours are spent cleaning the data."
    There are several subtasks that can be classified under **data cleansing**, and
    we will look at them now.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is usually organized in a database as separate tables that may be bound
    together by common patient or encounter identifiers. Machine learning algorithms
    usually work on a single data structure at a time. Therefore, combining and merging
    the data from several tables into one final table is an important task. Along
    the way, you'll have to make some decisions as to which data to preserve (demographic
    data is usually indispensable) along with which data you can safely forget (the
    exact timestamps of anti-asthmatic medication administrations may not be important
    if you are trying to predict cancer onset, for example).
  prefs: []
  type: TYPE_NORMAL
- en: Parsing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are cases in which some or all of the data we need is in a condensed form.
    An example includes flat files of healthcare survey data in which each survey
    is encoded as an *N*-character string, with the characters at each position corresponding
    to specific survey responses. In these cases, the data we want must be broken
    down into its various components and converted into a useful format before we
    can use it. We refer to this activity as **parsing**. Even data that is expressed
    using particular medical coding systems may require some parsing.
  prefs: []
  type: TYPE_NORMAL
- en: Converting types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are familiar at all with programming, you know that data can be stored
    as different **variable types**, ranging from simple integers to complex decimals
    to string (character) types. These types differ in terms of the operations that
    can be performed on them. For example, if the numbers 3 and 5 are stored as integer
    types, we can easily calculate 3+5= 8 using code. However, if they are stored
    as string types, adding "3" to "5" may yield an error, or it may yield "35," and
    this would cause all sorts of problems with our data, as you can imagine. Part
    of cleaning and inspecting the data is making sure every variable is stored as
    its proper type. Numerical data should correspond to numerical types, and most
    other data should correspond to string or categorical types.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the variable type, in many modeling languages, decisions must
    be made as to how to store data using more complex **data containers**, such as
    lists, vectors, and dataframes in R and lists, dictionaries, tuples, and dataframes
    in Python. Various importing and modeling functions may assume different choices
    of data structures, so once again, interconversion between data structures is
    usually necessary in order to achieve the desired result, and this is a crucial
    part of data cleansing. We will cover Python-related data structures in [Chapter
    5](12ee77f2-0655-4dc5-abb1-2868d6fcc386.xhtml), *Computing Foundations – Introduction
    to Python*.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Part of the reason why machine learning is so uniquely difficult in healthcare
    is its propensity for **missing data**. Inpatient hospital-data collection is
    often dependent on the nurses and other clinical staff to be completed thoroughly,
    and given how busy nurses and other clinical staff are, it''s no wonder that many
    inpatient datasets have certain features, such as urinary intake and output or
    timestamps of medication administrations, inconsistently reported. Another example
    is diagnosis codes: a patient may be eligible for a dozen medical diagnoses but
    in the interest of time, only five are entered into the chart by the outpatient
    physician. When details such as these are left out of our data, our models will
    be that much less accurate when applied to real patients.'
  prefs: []
  type: TYPE_NORMAL
- en: Even more problematic than the lack of detail is the effect of the missing data
    on our algorithms. Even one missing value in a dataframe that consists of thousands
    of patients and hundreds of features can prevent a model from running successfully.
    A quick fix might be simply to type in or impute a zero where the missing value
    is. But if the variable is a hemoglobin lab value, surely a hemoglobin of 0.0
    is impossible. Should we impute the missing data with the mean hemoglobin lab
    value instead? Do we use the overall mean or the gender-specific mean? Questions
    such as these are the reasons why dealing with missing data is practically a data
    science field in itself. The importance of having the basic awareness of missing
    data in your dataset cannot be overemphasized. In particular, it is important
    to know the difference between zero-valued data and missing data. Also, gaining
    some familiarity with concepts such as **zero**, **NaN** ("not a number"), **NULL**
    ("missing"), or **NA** ("not applicable") and how they are expressed in your languages
    of choice, whether SQL, Python, R, or some other language, is important.
  prefs: []
  type: TYPE_NORMAL
- en: The final goal of the data-cleansing stage is usually a single **data frame**,
    which is a single data structure that organizes the data into a matrix-like object
    of rows and columns, where rows comprise the individual events or observations
    and columns reflect different features of the observations using various data
    types. In an ideal world, all of the variables will have been inspected and converted
    to the appropriate type, and there would be no missing data. It should be noted
    that there may be some back-and-forth iterations between data cleansing, exploring,
    visualizing, and feature selection before reaching this final milestone. Data
    exploration/visualization and feature selection are the two pipeline steps that
    we'll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and visualizing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be done in close conjunction with parsing and cleaning the data, data exploration
    and visualization is an important part of the model-building process. This part
    of the pipeline is hard to define concretely–what exactly is one looking for when
    exploring the data? The underlying theory is that humans can do certain things
    much better than computers can–things such as making connections and identifying
    patterns. The more one looks at and analyzes the data, the more one will discover
    about how the variables are related and how they can be used to predict the target
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: A popular exploratory activity in this step is to take a stock of all of the
    predictor variables; that is, their formats (for example, whether they are binary,
    categorical, or continuous) and how many missing values there are in each. For
    **binary variables**, it is helpful to count how many responses are positive and
    how many are negative; for **categorical variables**, it is helpful to count how
    many possible values each variable can take and the frequency histograms for each;
    and for **continuous variables**, calculating some measures of central tendency
    (for example, mean, median, mode) and dispersion (for example, standard deviation,
    percentiles) is a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: Additional exploratory and visualization activities can be done to elucidate
    the relationships between selected predictor variables and the target variable.
    Specific plots vary depending on the formats (binary, categorical, continuous).
    For example, when both the predictor variable and target variable are continuous,
    a **scatterplot** is a popular visualization; to make a scatterplot the values
    of each variable are plotted on separate axes. If the predictor variable is continuous
    and the target variable is binary or categorical, a **dual overlapping frequency
    histogram** is a good tool, as is a **box-and-whisker plot**.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, there are so many predictor variables that it becomes impossible
    to inspect manually and visualize each relationship. In these cases, automatic
    analyses, and calculating measures and statistics, such as correlation coefficients,
    become important.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building models, more features are not always better. From an implementation
    perspective, a predictive pipeline modeling real-time clinical settings that interacts
    with multiple devices, health informatics systems, and source databases is more
    likely to fail than a simplified version with a minimal number of features. Specifically,
    while cleaning and exploring your data, you will find that not all of the features
    will be significantly related to the outcome variable.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, many of the variables may be highly correlated with other variables
    and will offer little new information for making accurate predictions. Leaving
    these variables in your model could, in fact, reduce the accuracy of your model
    because they add random noise to the data. Therefore, a usual step in the machine
    learning pipeline is to perform **feature selection** and remove unwanted features
    from your data. The number and which variables to remove depends on many factors,
    including the choice of your machine learning algorithm and how interpretable
    you want the model to be.
  prefs: []
  type: TYPE_NORMAL
- en: There are many approaches to removing extraneous features from the final model.
    Iterative approaches, in which features are removed and the resulting model is
    built, evaluated, and compared to previous models, are popular because they allow
    one to measure how adjustments affect the performance of the model. Several algorithms
    for selecting features include **best subset selection** and forward and backward
    **step-wise regression**. There are also a variety of measures for feature importance,
    and these include the relative risk ratio, odds ratio, *p*-value significance,
    lasso regularization, correlation coefficient, and random forest out-of-bag error,
    and we will explore some of these measures in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have our final data frame, we can think of the machine learning problem
    as minimizing an error function. All we are trying to do is make the best predictions
    on unseen patients/encounters; we are trying to minimize the difference between
    the predicted value and the observed value. For example, if we are trying to predict
    cancer onset, we want the predicted likelihood of cancer occurrence to be high
    in patients that developed cancer and low in patients that have not developed
    cancer. In machine learning, the difference between the predicted values and the
    observed values is known as an **error function** or **cost function**. Cost functions
    can take various forms, and machine learning practitioners often tinker with them
    while performing modeling. When minimizing the cost function, we need to know
    what weights we assign to certain features. In most cases, features that are more
    highly correlated to the outcome variable should be given more mathematical importance
    than features that are less highly correlated to the outcome variable. In a simplistic
    sense, we can refer to these "importance variables" as weights, or parameters.
    One of the major goals of supervised machine learning is all about finding that
    unique set of parameters or weights that minimizes our cost function. Almost every
    machine learning algorithm has its own way of assigning weights to different features.
    We will study this part of the pipeline in greater detail for the logistic regression,
    random forest, and neural network algorithms in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, after building the model, it is important to evaluate its performance
    against the ground truth, so that we can adjust it if needed, compare different
    models, and report the results of our model to others. Methods for evaluating
    model performance depend on the structure of the target variable being predicted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, the first step in evaluating a model is making a 2 x 2 contingency table,
    an example of which is shown as follows (Preventive Medicine, 2016). In a 2 x
    2 contingency table, all of the observations are split into four categories, which
    are further discussed in the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d8cf94a-5970-48b9-be2f-ca1e51974339.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For binary-valued target variables (for example, classification problems),
    there will be four types of observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Those that had a positive outcome for which we predicted a positive outcome
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those that had a positive outcome for which we predicted a negative outcome
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those that had a negative outcome for which we predicted a positive outcome
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those that had a negative outcome for which we predicted a negative outcome
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These four classes of observations are referred to respectively as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives** (**TP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negatives** (**FN**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positives** (**FP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negatives** (**TN**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various performance measures can then be calculated from these four quantities.
    We will cover the popular ones in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity (Sn)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **sensitivity**, also known as the **recall**, answers the question, "How
    effective is my model at incorrectly detecting observations that are positive
    for disease?"
  prefs: []
  type: TYPE_NORMAL
- en: 'Its formula is given as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e474c83-a92f-4d77-8186-7780c26623a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Specificity (Sp)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **specificity** answers the question: "How effective is my model at incorrectly
    detecting observations that are negative for disease?"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Its formula is given as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af2e3b6f-7279-4f3d-bf77-a7324ba933d1.png)'
  prefs: []
  type: TYPE_IMG
- en: The sensitivity and specificity are complementary performance measures and are
    often reported together to measure the performance of a model.
  prefs: []
  type: TYPE_NORMAL
- en: Positive predictive value (PPV)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **positive predictive value** (**PPV**), also known as the **precision**,
    answers the question: "Given a positive prediction of my model, how likely is
    it to be correct?"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Its formula is given as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e922c36c-f6e9-42fe-8e65-487f69e26c41.png)'
  prefs: []
  type: TYPE_IMG
- en: Negative predictive value (NPV)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **negative predictive value** (**NPV**) answers the question: "Given a
    negative prediction of my model, how likely is it to be correct?"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Its formula is given as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be226f63-bfec-4049-ab9a-a8c90dc8fa8e.png)'
  prefs: []
  type: TYPE_IMG
- en: False-positive rate (FPR)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **false-positive rate** (**FPR**) answers the question: "Given a negative
    observation, what is the likelihood that my model will classify it as positive?"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Its formula is given as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdddf257-28aa-45c1-9a5b-b31df515e570.png)'
  prefs: []
  type: TYPE_IMG
- en: It is also equal to one minus the *specificity (1 - Sp)*.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy (Acc)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **accuracy** (**Acc**) answers the question, "Given any observation, what
    is the likelihood that my model will classify it correctly?" It can be used as
    a standalone measure of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its formula is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6dbbe2cb-7130-48ec-b7dd-feb3e67a6524.png)'
  prefs: []
  type: TYPE_IMG
- en: Receiver operating characteristic (ROC) curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the target variable is binary, many machine learning algorithms will return
    the prediction for the observation in the form of a score that ranges from 0 to
    1\. Therefore, the positive or negative value of the prediction depends on where
    we set the threshold in that range. For example, if we build a model to predict
    cancer malignancy and determine that a particular patient's malignancy likelihood
    is 0.65, choosing a positive threshold of 0.60 makes a positive prediction for
    that patient, while choosing a threshold of 0.70 makes a negative prediction for
    that patient. All of the performance scores vary according to where we set the
    threshold. Certain thresholds will lead to better performance than others, depending
    on our goal for detection. For example, if we are interested in cancer detection,
    setting a threshold to a low value such as 0.05 will increase the sensitivity
    of our model, at the expense of the specificity, but this may be desired because
    we may not mind the false positives as long as we can identify every patient who
    is possibly at risk for cancer.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most common performance measurement paradigm for binary-valued outcome
    variables is to construct a **receiver operating characteristic** (**ROC**) **curve**.
    In this curve, we plot the values of two measures, the false-positive rate, and
    the sensitivity, as we vary the threshold from 0 to 1\. The sensitivity is usually
    inversely related to the false-positive rate, yielding a lowercase-r-shaped curve
    in most cases. The stronger the model, the higher the sensitivity and the lower
    the false positive rate will generally be, and the **area under the curve** (**AUC**)
    will tend to approach 1\. The AUC can, therefore, be used to compare models (for
    the same use case) while removing the dependency on the threshold's value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example ROC plot (Example ROC Curves, 2016) shown as follows has two ROC
    curves, a dark one and a light one. Because the red (dark) curve has a greater
    area under the curve than the lighter curve, the model measured by the dark curve
    can be seen as being better performing than that reflected by the lighter curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d70319db-e8f3-4126-9056-fb88d1f540c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Precision-recall curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **precision-recall curve** is an alternative to the ROC curve when the target
    variable is imbalanced (for example, when the positive-negative ratio is very
    low or very high). In healthcare, many use cases have a low positive-negative
    ratio, so you may see this curve often. It plots the positive predictive value
    of the sensitivity as the threshold varies from 0 to 1\. In most cases, this yields
    an uppercase-L-shaped curve.
  prefs: []
  type: TYPE_NORMAL
- en: Continuously valued target variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For continuously valued target variables (for example, regression problems),
    there is no concept of true positives or false positives, so the previously discussed
    measures and curves cannot be calculated. Instead, the **residual sum of squares**
    (**RSS**) **error** is usually calculated: it is the sum of the squared distances
    between the actual values and the predicted values.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have toured some of the machine learning and mathematical
    foundations for performing healthcare analytics. In the next chapter, we'll continue
    exploring the foundational triumvirate of healthcare analytics by moving on to
    the computing leg.
  prefs: []
  type: TYPE_NORMAL
- en: References and further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clinical Prediction (2017). "Wells Clinical Prediction Rule for Pulmonary Embolism."[http://www.clinicalprediction.com/wells-score-for-pe/](http://www.clinicalprediction.com/wells-score-for-pe/).
    Accessed June 6, 2018.
  prefs: []
  type: TYPE_NORMAL
- en: '"File: Example ROC curves.png." Wikimedia Commons, the free media repository.
    26 Nov 2016, 05:26 UTC. 11 Jul 2018, 01:53 [https://commons.wikimedia.org/w/index.php?title=File:Example_ROC_curves.png&oldid=219960771](https://commons.wikimedia.org/w/index.php?title=File:Example_ROC_curves.png&oldid=219960771).'
  prefs: []
  type: TYPE_NORMAL
- en: '"File: Preventive Medicine Statistics Sensitivity TPR, Specificity TNR, PPV,
    NPV, FDR, FOR, ACCuracy, Likelihood Ratio, Diagnostic Odds Ratio 2 Final.png."
    Wikimedia Commons, the free media repository. 26 Nov 2016, 04:26 UTC. 11 Jul 2018,
    01:42 [https://commons.wikimedia.org/w/index.php?title=File:Preventive_Medicine_Statistics_Sensitivity_TPR,_Specificity_TNR,_PPV,_NPV,_FDR,_FOR,_ACCuracy,_Likelihood_Ratio,_Diagnostic_Odds_Ratio_2_Final.png&oldid=219913391](https://commons.wikimedia.org/w/index.php?title=File:Preventive_Medicine_Statistics_Sensitivity_TPR,_Specificity_TNR,_PPV,_NPV,_FDR,_FOR,_ACCuracy,_Likelihood_Ratio,_Diagnostic_Odds_Ratio_2_Final.png&oldid=219913391).'
  prefs: []
  type: TYPE_NORMAL
- en: Häggström, Mikael (2014). "[Medical gallery of Mikael Häggström 2014](https://en.wikiversity.org/wiki/WikiJournal_of_Medicine/Medical_gallery_of_Mikael_H%C3%A4ggstr%C3%B6m_2014)".
    WikiJournal of Medicine 1 (2). [DOI](https://en.wikipedia.org/wiki/Digital_object_identifier):[10.15347/wjm/2014.008](https://doi.org/10.15347/wjm/2014.008).
    [ISSN](https://en.wikipedia.org/wiki/International_Standard_Serial_Number) [2002-4436](https://www.worldcat.org/issn/2002-4436).
    [Public Domain](https://creativecommons.org/publicdomain/zero/1.0/deed.en).
  prefs: []
  type: TYPE_NORMAL
- en: 'James G, Witten D, Hastie T, Tibshirani R (2014). *An Introduction to Statistical
    Learning.* New York: Springer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kirk E, Bottomley C, Bourne T (2014). "Diagnosing ectopic pregnancy and current
    concepts in the management of pregnancy of unknown location". Hum. Reprod. Update
    20 (2): 250–61\. [DOI](https://en.wikipedia.org/wiki/Digital_object_identifier):[10.1093/humupd/dmt047](https://doi.org/10.1093/humupd/dmt047).
    [PMID](https://en.wikipedia.org/wiki/PMID)[24101604](https://www.ncbi.nlm.nih.gov/pubmed/24101604).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mark, DB (2005). "Decision-Making in Clinical Medicine." In Kasper DL, Braunwald
    E, Fauci AS, Hauser SL, Longo DL, Jameson JL. eds. *Harrison''s* *Principles of
    Internal Medicine*, 16e. New York, NY: McGraw-Hill.'
  prefs: []
  type: TYPE_NORMAL
- en: 'National Heart, Lung, and Blood Institute (2010). "Treatment Algorithm." *Guidelines
    on Overweight and Obesity: Electronic Textbook.* [https://www.nhlbi.nih.gov/health-pro/guidelines/current/obesity-guidelines/e_textbook/txgd/algorthm/algorthm.htm](https://www.nhlbi.nih.gov/health-pro/guidelines/current/obesity-guidelines/e_textbook/txgd/algorthm/algorthm.htm)
    . Accessed June 3, 2018.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rumelhart DE, Hinton GE, Williams RJ (1986). "Learning representations by backpropagating
    errors." *Nature *323(9): 533-536.'
  prefs: []
  type: TYPE_NORMAL
