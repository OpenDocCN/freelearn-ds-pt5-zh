<html><head></head><body><div><div><p class="hidden">3</p>
		</div>
		<div><h1 id="_idParaDest-53"><a id="_idTextAnchor054"/>Web Scraping and Interactive Visualizations</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Describe how HTTP requests work</li>
				<li class="bullets">Scrape tabular data from a web page</li>
				<li class="bullets">Build and transform Pandas DataFrames</li>
				<li class="bullets">Create interactive visualizations</li>
			</ul>
			<p>In this chapter, you will learn the fundamentals of HTTP requests, scrape web page data, and then create interactive visualizations using the Jupyter Notebook.</p>
		</div>
		<div><h2 id="_idParaDest-54"><a id="_idTextAnchor055"/>Introduction</h2>
			<p>So far in this book, we have focused on using Jupyter to build reproducible data analysis pipelines and predictive models. We'll continue to explore these topics in this chapter, but the main focus here is data acquisition. In particular, we will show you how data can be acquired from the web using HTTP requests. This will involve scraping web pages by requesting and parsing HTML. We will then wrap up this chapter by using interactive visualization techniques to explore the data we've collected.</p>
			<p>The amount of data available online is huge and relatively easy to acquire. It's also continuously growing and becoming increasingly important. Part of this continual growth is the result of an ongoing global shift from newspapers, magazines, and TV to online content. With customized newsfeeds available all the time on cell phones, and live-news sources such as Facebook, Reddit, Twitter, and YouTube, it's difficult to imagine the historical alternatives being relevant much longer. Amazingly, this accounts for only some of the increasingly massive amounts of data available online.</p>
			<p>With this global shift toward consuming content using HTTP services (blogs, news sites, Netflix, and so on), there are plenty of opportunities to use data-driven analytics. For example, Netflix looks at the movies a user watches and predicts what they will like. This prediction is used to determine the suggested movies that appear. In this chapter, however, we won't be looking at "business-facing" data as such, but instead we will see how the client can leverage the internet as a database. Never before has this amount and variety of data been so easily accessible. We'll use web-scraping techniques to collect data, and then we'll explore it with interactive visualizations in Jupyter.</p>
			<p>Interactive visualization is a visual form of data representation, which helps users understand the data using graphs or charts. Interactive visualization helps a developer or analyst present data in a simple form, which can be understood by non-technical personnel too.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor056"/>Scraping Web Page Data</h2>
			<p>In the spirit of leveraging the internet as a database, we can think about acquiring data from web pages either by scraping content or by interfacing with web APIs. Generally, scraping content means getting the computer to read data that was intended to be displayed in a human-readable format. This is in contradistinction to web APIs, where data is delivered in machine-readable formats—the most common being JSON.</p>
			<p>In this topic, we will focus on web scraping. The exact process for doing this will depend on the page and desired content. However, as we will see, it's quite easy to scrape anything we need from an HTML page so long as we have an understanding of the underlying concepts and tools. In this topic, we'll use Wikipedia as an example and scrape tabular content from an article. Then, we'll apply the same techniques to scrape </p>
			<p>data from a page on an entirely separate domain. But first, we'll take some time to introduce HTTP requests.</p>
			<h3 id="_idParaDest-56"><a id="_idTextAnchor057"/>Introduction to HTTP Requests</h3>
			<p>The Hypertext Transfer Protocol, or HTTP for short, is the foundation of data communication for the internet. It defines how a page should be requested and how the response should look. For example, a client can request an Amazon page of laptops for sale, a Google search of local restaurants, or their Facebook feed. Along with the URL, the request will contain the user agent and available browsing cookies among the contents of the request header. The user agent tells the server what browser and device the client is using, which is usually used to provide the most user-friendly version of the web page's response. Perhaps they have recently logged in to the web page; such information would be stored in a cookie that might be used to automatically log the user in.</p>
			<p>These details of HTTP requests and responses are taken care of under the hood thanks to web browsers. Luckily for us, today the same is true when making requests with high-level languages such as Python. For many purposes, the contents of request headers can be largely ignored. Unless otherwise specified, these are automatically generated in Python when requesting a URL. Still, for the purposes of troubleshooting and understanding the responses yielded by our requests, it's useful to have a foundational understanding of HTTP.</p>
			<p>There are many types of HTTP methods, such as GET, HEAD, POST, and PUT. The first two are used for requesting that data be sent from the server to the client, whereas the last two are used for sending data to the server.</p>
			<h4>Note</h4>
			<p class="callout">Take a look at this GET request example for the <code>User-Agent</code> is Mozilla/5.0, which corresponds to a standard desktop browser. Among other lines in the header, we note the <code>Accept</code> and <code>Accept-Language</code> fields, which specify the acceptable content types and language of the response.</p>
			<p>These HTTP methods are summarized below:</p>
			<ul>
				<li><strong class="bold">GET</strong>: Retrieves the information from the specified URL</li>
				<li><strong class="bold">HEAD</strong>: Retrieves the meta information from the HTTP header of the specified URL</li>
				<li><strong class="bold">POST</strong>: Sends the attached information for appending to the resource(s) at the specified URL</li>
				<li><strong class="bold">PUT</strong>: Sends the attached information for replacing the resource(s) at the specified URL</li>
			</ul>
			<p>A<strong class="bold"> </strong><strong class="bold">GET</strong> request is sent each time we type a web page address into our browser and press Enter. For web scraping, this is usually the only HTTP method we are interested in, and it's the only method we'll be using in this chapter.</p>
			<p>Once the request has been sent, a variety of response types can be returned from the server. These are labeled with 100-level to 500-level codes, where the first digit in the code represents the response class. These can be described as follows:</p>
			<ul>
				<li><strong class="bold">1xx</strong>: Informational response, for example, server is processing a request. It's uncommon to see this.</li>
				<li><strong class="bold">2xx</strong>: Success, for example, page has loaded properly.</li>
				<li><strong class="bold">3xx</strong>: Redirection, for example, the requested resource has been moved and we were redirected to a new URL.</li>
				<li><strong class="bold">4xx</strong>: Client error, for example, the requested resource does not exist.</li>
				<li><strong class="bold">5xx</strong>: Server error, for example, the website server is receiving too much traffic and could not fulfill the request.</li>
			</ul>
			<p>For the purposes of web scraping, we usually only care about the response class, that is, the first digit of the response code. However, there exist subcategories of responses within each class that offer more granularity on what's going on. For example, a 401 code indicates an unauthorized response, whereas a 404 code indicates a page not found response. This distinction is noteworthy because a 404 would indicate we've requested a page that does not exist, whereas 401 tells us we need to log in to view the particular resource.</p>
			<p>Let's see how HTTP requests can be done in Python and explore some of these topics using the Jupyter Notebook.</p>
			<h3 id="_idParaDest-57"><a id="_idTextAnchor058"/>Making HTTP Requests in the Jupyter Notebook</h3>
			<p>Now that we've talked about how HTTP requests work and what type of responses we should expect, let's see how this can be done in Python. We'll use a library called <code>urllib</code>, for making HTTP requests, but <code>urllib</code> in the official Python documentation.</p>
			<p><strong class="bold">Requests</strong> is a great choice for making simple and advanced web requests. It allows for all sorts of customization with respect to headers, cookies, and authorization. It tracks redirects and provides methods for returning specific page content such as JSON. Furthermore, there's an extensive suite of advanced features. However, it does not allow JavaScript to be rendered.</p>
			<p>Oftentimes, servers return HTML with JavaScript code snippets included, which are automatically run in the browser on load time. When requesting content with Python using Requests, this JavaScript code is visible, but it does not run. Therefore, any elements that would be altered or created by doing so are missing. Often, this does not affect the ability to get the desired information, but in some cases we may need to render the JavaScript in order to scrape the page properly. For doing this, we could use a library like Selenium.</p>
			<p>This has a similar API to the Requests library, but provides support for rendering JavaScript using web drivers. It can even run JavaScript commands on live pages, for example, to change the text color or scroll to the bottom of the page.</p>
			<h4>Note</h4>
			<p class="callout">For more information, refer to: <a href="http://docs.python-requests.org/en/master/user/advanced/">http://docs.python-requests.org/en/master/user/advanced/</a> and <a href="http://selenium-python.readthedocs.io/">http://selenium-python.readthedocs.io/.</a></p>
			<p>Let's dive into an exercise using the Requests library with Python in a Jupyter Notebook.</p>
			<h3 id="_idParaDest-58"><a id="_idTextAnchor059"/>Exercise 14: Handling HTTP Requests With Python in a Jupyter Notebook</h3>
			<ol>
				<li>Start the <code>NotebookApp</code> from the project directory by executing jupyter notebook. Navigate to the <code>lesson-3</code> directory and open up the l<code>esson- 3-workbook.ipynb</code> file. Find the cell near the top where the packages are loaded and run it. <p>We are going to request a web page and then examine the response object. There are many different libraries for making requests and many choices for exactly how to do so with each. We'll only use the Requests library, as it provides excellent documentation, advanced features, and a simple API.</p></li>
				<li>Scroll down to <code>Subtopic A: Introduction to HTTP requests</code> and run the first cell in that section to import the Requests library. Then, prepare a request by running the cell containing the following code:<pre>url = 'https://jupyter.org/'
req = requests.Request('GET', url) req.headers['User-Agent'] = 'Mozilla/5.0'
req = req.prepare()</pre><p>We use the Request class to prepare a GET request to the jupyter.org homepage. By specifying the user agent as Mozilla/5.0, we are asking for a response that would be suitable for a standard desktop browser. Finally, we prepare the request.</p></li>
				<li>Print the docstring for the "prepared request" req, by running the cell containing <code>req?</code>:<div><img src="img/C13018_03_01.jpg" alt=" Figure 3.1: Printing the docstring for req&#13;&#10;" width="908" height="413"/></div><h6> Figure 3.1: Printing the docstring for req</h6><p>Looking at its usage, we see how the request can be sent using a session. This is similar to opening a web browser (starting a session) and then requesting a URL.</p></li>
				<li>Make the request and store the response in a variable named page, by running the following code:<pre>with requests.Session() as sess: page = sess.send(req)</pre><p>This code returns the HTTP response, as referenced by the page variable. By using the <code>with</code> statement, we initialize a session whose scope is limited to the indented code block. This means we do not have to worry about explicitly closing the session, as it is done automatically.</p></li>
				<li>Run the next two cells in the notebook to investigate the response. The string representation of page should indicate a 200 status code response. This should agree with the <code>status_code</code> attribute.</li>
				<li>Save the response text to the <code>page_html</code> variable and take a look at the head of the string with <code>page_html[:1000]</code>:<div><img src="img/C13018_03_02.jpg" alt="Figure 3.2: The HTML response text" width="958" height="278"/></div><h6>Figure 3.2: The HTML response text</h6><p>As expected, the response is HTML. We can format this output better with the help of <code>BeautifulSoup</code>, a library which will be used extensively for HTML parsing later in this section.</p></li>
				<li>Print the head of the formatted HTML by running the following:<pre>from bs4 import BeautifulSoup
print(BeautifulSoup(page_html, 'html.parser').prettify() [:1000])</pre><p>We import <code>BeautifulSoup</code> and then print the output, where newlines are indented depending on their hierarchy in the HTML structure.</p></li>
				<li>We can take this a step further and actually display the HTML in Jupyter by using the IPython display module. Do this by running the following code:<pre>from IPython.display import HTML HTML(page_html)Here, we see the HTML rendered as well as possible, given that no JavaScript code has been run and no external resources have loaded. For example, the images that are hosted on the jupyter.org server are not rendered and we instead see the <code>alt</code> text: <strong class="bold">circle of programming icons</strong>, <strong class="bold">Jupyter logo</strong>, and so on. </pre><div><img src="img/C13018_03_03.jpg" alt="Figure 3.3: The output obtained when no images are loaded&#13;&#10;" width="903" height="350"/></div><h6>Figure 3.3: The output obtained when no images are loaded</h6></li>
				<li>Let's compare this to the live website, which can be opened in Jupyter using an IFrame. Do this by running the following code:<pre>from IPython.display import IFrame IFrame(src=url, height=800, width=800)</pre><div><img src="img/C13018_03_04.jpg" alt="Figure 3.4: Rendering of the entire Jupyter website&#13;&#10;" width="1088" height="612"/></div><h6>Figure 3.4: Rendering of the entire Jupyter website</h6><p>Here, we see the full site rendered, including JavaScript and external resources. In fact, we can even click on the hyperlinks and load those pages in the IFrame, just like a regular browsing session.</p></li>
				<li>It's good practice to close the IFrame after using it. This prevents it from eating up memory and processing power. It can be closed by selecting the cell and clicking <strong class="bold">Current Outputs</strong> | <strong class="bold">Clear </strong>from the Cell menu in the Jupyter Notebook.<p>Recall how we used a prepared request and session to request this content as a string in Python. This is often done using a shorthand method instead. The drawback is that we do not have as much customization of the request header, but that's usually fine.</p></li>
				<li>Make a request to <a href="http://www.python.org/">http://www.python.org/ </a>by running the following code:<pre>url = 'http://www.python.org/' page = requests.get(url)
page
&lt;Response [200]&gt;</pre><p>The string representation of the page (as displayed beneath the cell) should indicate a 200 status code, indicating a successful response.</p></li>
				<li>Run the next two cells. Here, we print the <code>url</code> and history attributes of our page.<p>The URL returned is not what we input; notice the difference? We were redirected from the input URL, <a href="http://www.python.org/">http://www.python.org/,</a> to the secured version of that page, <a href="http://www.python.org/">https://www.python.org/</a>. The difference is indicated by an additional s at the start of the URL, in the protocol. Any redirects are stored in the history attribute; in this case, we find one page in here with status code 301 (permanent redirect), corresponding to the original URL requested.</p></li>
			</ol>
			<p>Now that we're comfortable making requests, we'll turn our attention to parsing the HTML. This can be something of an art, as there are usually multiple ways to approach it, and the best method often depends on the details of the specific HTML in question.</p>
			<h3 id="_idParaDest-59"><a id="_idTextAnchor060"/>Parsing HTML in the Jupyter Notebook</h3>
			<p>When scraping data from a web page, after making the request, we must extract the data from the response content. If the content is HTML, then the easiest way to do this is with a high-level parsing library such as Beautiful Soup. This is not to say it's the only way; in principle, it would be possible to pick out the data using regular expressions or Python string methods such as split, but pursuing either of these options would be an inefficient use of time and could easily lead to errors. Therefore, it's generally frowned upon and instead, the use of a trustworthy parsing tool is recommended. </p>
			<p>In order to understand how content can be extracted from HTML, it's important to know the fundamentals of HTML. For starters, HTML stands for Hyper Text Markup Language. Like Markdown or XML (eXtensible Markup Language), it's simply a language for marking up text. In HTML, the display text is contained within the content section of HTML elements, where element attributes specify how that element should appear on the page.</p>
			<div><div><img src="img/C13018_03_05.jpg" alt="Figure 3.5: Fundamental blocks of HTML&#13;&#10;" width="1800" height="552"/>
				</div>
			</div>
			<h6>Figure 3.5: Fundamental blocks of HTML</h6>
			<p>Looking at the anatomy of an HTML element, as seen in the preceding picture, we see the content enclosed between start and end tags. In this example, the tags are <code>&lt;p&gt;</code> for paragraph; other common tag types are <code>&lt;div&gt;</code> (text block), <code>&lt;table&gt;</code> (data table),</p>
			<p><code>&lt;h1&gt;</code> (heading), <code>&lt;img&gt;</code> (image), and <code>&lt;a&gt;</code> (hyperlinks). Tags have attributes, which can hold important metadata. Most commonly, this metadata is used to specify how the element text should appear on the page. This is where CSS files come into play. The attributes can store other useful information, such as the hyperlink <code>href</code> in an <code>&lt;a&gt;</code> tag, which specifies a URL link, or the alternate alt label in an <code>&lt;img&gt;</code> tag, which specifies the text to display if the image resource cannot be loaded.</p>
			<p>Now, let's turn our attention back to the Jupyter Notebook and parse some HTML! Although not necessary when following along with this exercise, it's very helpful in real-world situations to use the developer tools in Chrome or Firefox to help identify the HTML elements of interest. We'll include instructions for doing this with Chrome in the following exercise.</p>
			<h3 id="_idParaDest-60"><a id="_idTextAnchor061"/>Exercise 15: Parsing HTML With Python in a Jupyter Notebook</h3>
			<ol>
				<li value="1">In <code>lesson-3-workbook.ipynb</code> file, scroll to the top of <code>Subtopic B: Parsing HTML</code> with Python.<p> In this exercise, we'll scrape the central bank interest rates for each country, as reported by Wikipedia. Before diving into the code, let's first open up the web page containing this data.</p></li>
				<li>Go to <a href="https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates">https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates</a> in a web browser. Use Chrome, if possible, as later in this exercise we'll show you how to view and search the HTML using Chrome's developer tools.<p>Looking at the page, we see very little content other than a big list of countries and their interest rates. This is the table we'll be scraping.</p></li>
				<li>Return to the Jupyter Notebook and load the HTML as a Beautiful Soup object so that it can be parsed. Do this by running the following code:<pre>from bs4 import BeautifulSoup
soup = BeautifulSoup(page.content, 'html.parser')</pre><p>We use Python's default html.parser as the parser, but third-party parsers such as <code>lxml</code> may be used instead, if desired. Usually, when working with a new object like this Beautiful Soup one, it's a good idea to pull up the docstring by doing <code>soup?</code>. However, in this case, the docstring is not particularly informative. Another tool for exploring Python objects is <code>pdir</code>, which lists all of an object's attributes and methods (this can be installed with pip install <code>pdir2</code>). It's basically a formatted version of Python's built-in <code>dir</code> function.</p></li>
				<li>Display the attributes and methods for the BeautifulSoup object by running the following code. This will run, regardless of whether or not the <code>pdir</code> external library is installed:<pre>try:
import pdir dir = pdir
except:
print('You can install pdir with:\npip install pdir2') dir(soup)</pre><p>Here, we see a list of methods and attributes that can be called on soup. The most commonly used function is probably <code>find_all</code>, which returns a list of elements that match the given criteria.</p></li>
				<li>Get the h1 heading for the page with the following code:<pre>h1 = soup.find_all('h1') h1
&gt;&gt; [&lt;h1 class="firstHeading" id="firstHeading" lang="en"&gt;List of countries by central bank interest rates&lt;/h1&gt;]</pre><p>Usually, pages only have one H1 (top-level heading) element, so it's no surprise that we only find one here.</p></li>
				<li>Run the next couple of cells. We redefine H1 to the first (and only) list element with <code>h1 = h1[0]</code>, and then print out the HTML element attributes with <code>h1.attrs</code>:<p>We see the class and ID of this element, which can both be referenced by CSS code to define the style of this element.</p></li>
				<li>Get the HTML element content (that is, the visible text) by printing <code>h1.text</code>.</li>
				<li>Get all the images on the page by running the following code:<pre>imgs = soup.find_all('img') len(imgs)
&gt;&gt; 91</pre><p>There are lots of images on the page. Most of these are for the country flags.</p></li>
				<li>Print the source of each image by running the following code:<pre>[element.attrs['src'] for element in imgs if 'src' in element.attrs.keys()]</pre><p>We use a list comprehension to iterate through the elements, selecting the <code>src</code> attribute of each (so long as that attribute is actually available).</p><p>Now, let's scrape the table. We'll use Chrome's developer tools to hunt down the element this is contained within.</p><div><img src="img/C13018_03_06.jpg" alt="Figure 3.6: Scraping the table on the target web page&#13;&#10;" width="948" height="239"/></div><h6>Figure 3.6: Scraping the table on the target web page</h6></li>
				<li>If not already done, open the Wikipedia page we're looking at in Chrome. Then, in the browser, select <strong class="bold">Developer Tools</strong> from the <strong class="bold">View</strong> menu. A sidebar will open. The HTML is available to look at from the <strong class="bold">Elements</strong> tab in Developer Tools.</li>
				<li>Select the little arrow in the top left of the tools sidebar. This allows us to hover over the page and see where the HTML element is located, in the <strong class="bold">Elements</strong> section of the sidebar:<div><img src="img/C13018_03_07.jpg" alt="Figure 3.7: Arrow Icon for locating the HTML element &#13;&#10;" width="596" height="202"/></div><h6>Figure 3.7: Arrow Icon for locating the HTML element </h6></li>
				<li>Hover over the body to see how the table is contained within the div that has <code>id="bodyContent"</code>:<div><img src="img/C13018_03_08.jpg" alt="Figure 3.8: HTML code for table on the target web page&#13;&#10;" width="1800" height="931"/></div><h6>Figure 3.8: HTML code for table on the target web page</h6></li>
				<li>Select that <code>div</code> by running the following code:<pre>body_content = soup.find('div', {'id': 'bodyContent'})</pre><p>We can now seek out the table within this subset of the full HTML. Usually, tables are organized into headers <code>&lt;th&gt;</code>, rows <code>&lt;tr&gt;</code>, and data entries <code>&lt;td&gt;</code>.</p></li>
				<li>Get the table headers by running the following code:<pre>table_headers = body_content.find_all('th')[:3] table_headers
&gt;&gt;&gt; [&lt;th&gt;Country or&lt;br/&gt;
currency union&lt;/th&gt;, &lt;th&gt;Central bank&lt;br/&gt; interest rate (%)&lt;/th&gt;, &lt;th&gt;Date of last&lt;br/&gt; change&lt;/th&gt;]</pre><p>Here, we see three headers. In the content of each is a break element <code>&lt;br/&gt;</code>, which will make the text a bit more difficult to cleanly parse.</p></li>
				<li>Get the text by running the following code:<pre>table_headers = [element.get_text().replace('\n', ' ')
for element in table_headers]
table_headers
&gt;&gt; ['Country or currency union', 'Central bank interest rate (%)', 'Date of last change']</pre><p>Here, we get the content with the <code>get_text</code> method, and then run the replace string method to remove the newline resulting from the <code>&lt;br/&gt;</code> element.</p><p>To get the data, we'll first perform some tests and then scrape all the data in a single cell.</p></li>
				<li>Get the data for each cell in the second <code>&lt;tr&gt;</code> (row) element by running the following code:<pre>row_number = 2
d1, d2, d3 = body_content.find_all('tr')[row_number]\
.find_all('td')</pre><p>We find all the row elements, pick out the third one, and then find the three data elements inside that.</p><p>Let's look at the resulting data and see how to parse the text from each row.</p></li>
				<li>Run the next couple of cells to print <code>d1</code> and its text attribute:<div><img src="img/C13018_03_09.jpg" alt="Figure 3.9: Printing d1 and its text attribute&#13;&#10;" width="823" height="239"/></div><h6>Figure 3.9: Printing d1 and its text attribute</h6><p>We're getting some undesirable characters at the front. This can be solved by searching for only the text of the <code>&lt;a&gt;</code> tag.</p></li>
				<li>Run <code>d1.find('a').text</code> to return the properly <em class="italics">cleaned</em> data for that cell.</li>
				<li>Run the next couple of cells to print <code>d2</code> and its text. This data appears to be clean enough to convert directly into a float.</li>
				<li>Run the next couple of cells to print <code>d3</code> and its text:<div><img src="img/C13018_03_10.jpg" alt="Figure 3.10: Printing d3 and its text attribute&#13;&#10;" width="826" height="173"/></div><h6>Figure 3.10: Printing <code>d3</code> and its text attribute</h6><p>Similar to <code>d1</code>, we see that it would be better to get only the span element's text.</p></li>
				<li>Properly parse the date for this table entry by running the following code:<pre>d3.find_all('span')[1].text
&gt;&gt; '30 June 2016'</pre></li>
				<li>Now, we're ready to perform the full scrape by iterating over the row elements <code>&lt;th&gt;</code>. Run the following code:<pre>data = []
for i, row in enumerate(body_content.find_all('tr')):
...
...
&gt;&gt; Ignoring row 101 because len(data) != 3
&gt;&gt; Ignoring row 102 because len(data) != 3</pre><h4>Note</h4><p class="callout">For the complete code, refer to the following: <a href="https://bit.ly/2EKMNbV">https://bit.ly/2EKMNbV</a>.</p><p>We iterate over the rows, ignoring any that contain more than three data elements. These rows will not correspond to data in the table we are interested in. Rows that do have three data elements are assumed to be in the table, and we parse the text from these as identified during the testing.</p><p>The text parsing is done inside a <code>try/except</code> statement, which will catch any errors and allow this row to be skipped without stopping the iteration. Any rows that raise errors due to this statement should be looked at. The data for these could be recorded manually or accounted for by altering the scraping loop and re-running it. In this case, we'll ignore any errors for the sake of time.</p></li>
				<li>Print the head of the scraped data list by running <code>print(data[:10])</code>:<pre>&gt;&gt; [['Albania', 1.25, '4 May 2016'],
['Angola', 16.0, '30 June 2016'],
['Argentina', 26.25, '11 April 2017'],
['Armenia', 6.0, '14 February 2017'],
['Australia', 1.5, '2 August 2016'],
['Azerbaijan', 15.0, '9 September 2016'],
['Bahamas', 4.0, '22 December 2016'],
['Bahrain', 1.5, '14 June 2017'],
['Bangladesh', 6.75, '14 January 2016'],
['Belarus', 12.0, '28 June 2017']]</pre></li>
				<li>We'll visualize this data later in the chapter. For now, save the data to a CSV file by running the following code:<pre>f_path = '../data/countries/interest-rates.csv' with open(f_path, 'w') as f:
f.write('{};{};{}\n'.format(*table_headers)) for d in data:
f.write('{};{};{}\n'.format(*d))</pre><p>Note that we are using semicolons to separate the fields.</p></li>
			</ol>
			<h3 id="_idParaDest-61">Activity 3: Web Scra<a id="_idTextAnchor062"/><a id="_idTextAnchor063"/>ping With Jupyter Notebooks</h3>
			<p>You should have completed the previous exercise in this chapter.</p>
			<p>In this activity, we are going to get the population of each country. Then, in the next topic, this will be visualized along with the interest rate data scraped in the previous exercise.</p>
			<p>The page we look at in this activity is available here: <a href="http://www.worldometers.info/world-population/population-by-country/">http://www.worldometers.info/world-population/population-by-country/</a>.</p>
			<p>Our aim is to apply the basic of web scrapping to a new web page and scrape some more data.</p>
			<h4>Note</h4>
			<p class="callout">This page may have changed since this document was created. If this URL no longer leads to a table of country populations, please use this Wikipedia page instead: <a href="https://en.wikipedia.org/wiki/List_of_countries_by_population(United_Nations)">https://en.wikipedia.org/wiki/List_of_countries_by_population(United_Nations)</a>.</p>
			<p>In order to do this, the following steps have to be executed:</p>
			<ol>
				<li value="1">Scrape the data from the web page.</li>
				<li>In the <code>lesson-3-workbook.ipynb</code> Jupyter Notebook, scroll to <code>Activity A: Web scraping with Python</code>.</li>
				<li>Set the <code>url</code> variable and load an IFrame of our page in the notebook. </li>
				<li>Close the IFrame by selecting the cell and clicking <strong class="bold">Current Outputs</strong> | Clear from the <strong class="bold">Cell</strong> menu in the Jupyter Notebook.</li>
				<li>Request the page and load it as a <code>BeautifulSoup</code> object. </li>
				<li>Print the H1 for the page.</li>
				<li>Get and print the table headings. </li>
				<li>Select first three columns and parse the text. </li>
				<li>Get the data for a sample row. </li>
				<li>How many columns of data do we have? Print the length of <code>row_data</code>. </li>
				<li>Print the first elements of the row. </li>
				<li>Select the data elements d1, d2, and d3.</li>
				<li>Looking at the <code>row_data</code> output, we can find out how to correctly parse the data. Select the content of the <code>&lt;a&gt;</code> element in the first data element, and then simply get the text from the others. </li>
				<li>Scrape and parse the table data. </li>
				<li>Print the head of the scraped data.</li>
				<li>Finally, save the data to a CSV file for later use.<h4>Note</h4><p class="callout">The detailed steps along with the solutions are presented in the <em class="italics">Appendix A</em> (pg. no. 160).</p></li>
			</ol>
			<p>To summarize, we've seen how Jupyter Notebooks can be used for web scraping. We started this chapter by learning about HTTP methods and status codes. Then, we used the Requests library to actually perform HTTP requests with Python and saw how the Beautiful Soup library can be used to parse the HTML responses.</p>
			<p>Our Jupyter Notebook turned out to be a great tool for this type of work. We were able to explore the results of our web requests and experiment with various HTML parsing techniques. We were also able to render the HTML and even load a live version of the web page inside the notebook!</p>
			<p>In the next topic of this chapter, we shift to a completely new topic: interactive visualizations. We'll see how to create and display interactive charts right inside the notebook, and use these charts as a way to explore the data we have just collected.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor064"/>Interactive Visualizations</h2>
			<p>Visualizations are quite useful as a means of extracting information from a dataset. For example, with a bar graph it's very easy to distinguish the value distribution, compared to looking at the values in a table. Of course, as we have seen earlier in this book, they can be used to study patterns in the dataset that would otherwise be quite difficult to identify. Furthermore, they can be used to help explain a dataset to an unfamiliar party. If included in a blog post, for example, they can boost reader interest levels and be used to break up blocks of text.</p>
			<p>When thinking about interactive visualizations, the benefits are similar to static visualizations, but enhanced because they allow for active exploration on the viewer's part. Not only do they allow the viewer to answer questions they may have about the data, they also think of new questions while exploring. This can benefit a separate party such as a blog reader or co-worker, but also a creator, as it allows for easy ad hoc exploration of the data in detail, without having to change any code.</p>
			<p>In this topic, we'll discuss and show how to use Bokeh to build interactive visualizations in Jupyter. Prior to this, however, we'll briefly revisit pandas DataFrames, which play an important role in doing data visualization with Python.</p>
			<h3 id="_idParaDest-63"><a id="_idTextAnchor065"/>Building a DataFrame to Store and Organize Data</h3>
			<p>As we've seen time and time again in this book, pandas is an integral part of doing data science with Python and Jupyter Notebooks. DataFrames offer a way to organize and store labeled data, but more importantly, pandas provides time saving methods for transforming data within a DataFrame. Examples we have seen in this book include dropping duplicates, mapping dictionaries to columns, applying functions over columns, and filling in missing values.</p>
			<p>With respect to visualizations, DataFrames offer methods for creating all sorts of matplotlib graphs, including <code>df.plot.barh()</code>, <code>df.plot.hist()</code>, and more. The interactive visualization library Bokeh previously relied on pandas DataFrames for their <em class="italics">high-level charts</em>. These worked similar to Seaborn, as we saw earlier in the previous chapter, where a DataFrame is passed to the plotting function along with the specific columns to plot. The most recent version of Bokeh, however, has dropped support for this behavior. Instead, plots are now created in much the same way as matplotlib, where the data can be stored in simple lists or NumPy arrays. The point of this discussion is that DataFrames are not entirely necessary, but still very helpful for organizing and manipulating the data prior to visualization.</p>
			<h3 id="_idParaDest-64"><a id="_idTextAnchor066"/>Exercise 16: Building and Merging Pandas DataFrames</h3>
			<p>Let's dive right into an exercise, where we'll continue working on the country data we scraped earlier. Recall that we extracted the central bank interest rates and populations of each country, and saved the results in CSV files. We'll load the data from these files and merge them into a DataFrame, which will then be used as the data source for the interactive visualizations to follow.</p>
			<ol>
				<li value="1">In the <code>lesson-3-workbook.ipynb</code> of the Jupyter Notebook, scroll to the <code>Subtopic A: Building a DataFrame to store and organize data</code> subsection in the <code>Topic B</code> section.<p>We are first going to load the data from the CSV files, so that it's back to the state it was in after scraping. This will allow us to practice building DataFrames from Python objects, as opposed to using the <code>pd.read_csv</code> function.</p><h4>Note</h4><p class="callout">When using <code>pd.read_csv</code>, the datatype for each column will be inferred from the string input. On the other hand, when using <code>pd.DataFrame</code> as we do here, the datatype is instead taken as the type of the input variables. In our case, as will be seen, we read the file and do not bother converting the variables to numeric or date-time until after instantiating the DataFrame.</p></li>
				<li>Load the CSV files into lists by running the following code:<pre>with open('../data/countries/interest-rates.csv', 'r') as f:
int_rates_col_names = next(f).split(',')
int_rates = [line.split(',') for line in f.read(). splitlines()]
with open('../data/countries/populations.csv', 'r') as f: populations_col_names = next(f).split(',')
populations = [line.split(',') for line in f.read(). splitlines()]</pre></li>
				<li>Check what the resulting lists look like by running the next two cells. We should see an output similar to the following:<pre>print(int_rates_col_names) int_rates[:5]
&gt;&gt; ['Country or currency union', 'Central bank interest ...
...
['Indonesia', '263', '991', '379', '1.10 %'],
['Brazil', '209', '288', '278', '0.79 %']]</pre><p>Now, the data is in a standard Python list structure, just as it was after scraping from the web pages in the previous sections. We're now going to create two DataFrames and merge them, so that all of the data is organized within one object.</p></li>
				<li>Use the standard DataFrame constructor to create the two DataFrames by running the following code: <pre>df_int_rates = pd.DataFrame(int_rates, columns=int_rates_ col_names)
df_populations = pd.DataFrame(populations, columns=populations_col_names)</pre><p>This isn't the first time we've used this function in this book. Here, we pass the lists of data (as seen previously) and the corresponding column names. The input data can also be of dictionary type, which can be useful when each column is contained in a separate list.</p><p>Next, we're going to clean up each DataFrame. Starting with the interest rates one, let's print the head and tail, and list the data types.</p></li>
				<li>When displaying the entire DataFrame, the default maximum number of rows is 60 (for version 0.18.1). Let's reduce this to 10 by running the following code:<pre>pd.options.display.max_rows = 10</pre></li>
				<li>Display the head and tail of the interest rates DataFrame by running the following code:<pre><code>df_int_rates</code></pre><div><img src="img/C13018_03_11.jpg" alt="Figure 3.11: Table for interest rates by country&#13;&#10;" width="836" height="369"/></div><h6>Figure 3.11: Table for interest rates by country</h6></li>
				<li>Print the data types by running:<pre>df_int_rates.dtypes 				
&gt;&gt; Country or currency union		object
&gt;&gt; Central bank interest rate (%)		object	
&gt;&gt; Date of last change			object
&gt;&gt; dtype: object</pre><p>Pandas has assigned each column as a string datatype, which makes sense because the input variables were all strings. We'll want to change these to string, float, and datetime, respectively.</p></li>
				<li>Convert to the proper datatypes by running the following code:<pre>df_int_rates['Central bank interest rate (%)'] = \ df_int_rates['Central bank interest rate (%)']\
.astype(float, copy=False)
df_int_rates['Date of last change'] = \ pd.to_datetime(df_int_rates['Date of last change'])</pre><p>We use <code>astype</code> to cast the Interest Rate values as floats, setting <code>copy=False</code> to save memory. Since the date values are given in such an easy-to-read format, these can be converted simply by using <code>pd.to_datetime</code>.</p></li>
				<li>Check the new datatypes of each column by running the following code:<pre>df_int_rates.dtypes
&gt;&gt; Country or currency union					object
&gt;&gt; Central bank interest rate (%)				float64
&gt;&gt; Date of last change					datetime64[ns]	
&gt;&gt; dtype: object</pre><p>As can be seen, everything is now in the proper format.</p></li>
				<li>Let's apply the same procedure to the other DataFrame. Run the next few cells to repeat the preceding steps for <code>df_populations</code>:<pre>df_population</pre><div><img src="img/C13018_03_12.jpg" alt="Figure 3.12: Table for population by country&#13;&#10;" width="801" height="337"/></div><h6>Figure 3.12: Table for population by country</h6><p>Then, run this code:</p><pre>df_populations['Population (2017)'] = df_populations['Population (2017)']\
.str.replace(',', '')\
.astype(float, copy=False)
df_populations['Yearly Change'] = df_populations['Yearly Change']\
.str.rstrip('%')\
.astype(float, copy=False)</pre><p>To cast the numeric columns as a float, we had to first apply some modifications to the strings in this case. We stripped away any commas from the populations and removed the percent sign from the Yearly Change column, using string methods.</p><p>Now, we're going to merge the DataFrames on the country name for each row. Keep in mind that these are still the raw country names as scraped from the web, so there might be some work involved with matching the strings.</p></li>
				<li>Merge the DataFrames by running the following code:<pre>df_merge = pd.merge(df_populations,
df_int_rates,
left_on='Country (or dependency)', right_on='Country or currency union', how='outer'
df_merge</pre><p>We pass the population data in the left DataFrame and the interest rates in the right one, performing an outer match on the country columns. This will result in <code>NaN</code> values where the two do not overlap.</p></li>
				<li>For the sake of time, let's just look at the most populated countries to see whether we missed matching any. Ideally, we would want to check everything. Look at the most populous countries by running the following code:<pre>df_merge.sort_values('Population (2017)', ascending=False)\
.head(10)</pre><div><img src="img/C13018_03_13.jpg" alt="Figure 3.13: The table for most populous countries&#13;&#10;" width="1800" height="727"/></div><h6>Figure 3.13: The table for most populous countries</h6><p>It looks like U.S. didn't match up. This is because it's listed as <em class="italics">United States</em> in the interest rates data. Let's remedy this.</p></li>
				<li>Fix the label for U.S. in the populations table by running the following code:<pre>col = 'Country (or dependency)'
df_populations.loc[df_populations[col] == 'U.S.'] = 'United States'</pre><p>We rename the country for the populations DataFrame with the use of the <code>loc</code> method to locate that row.</p><p>Now, let's merge the DataFrames properly.</p></li>
				<li>Re-merge the DataFrames on the country names, but this time use an inner merge to remove the <code>NaN</code> values:<pre>df_merge = pd.merge(df_populations,
df_int_rates,
left_on='Country (or dependency)', right_on='Country or currency union',
how='inner')</pre></li>
				<li>We are left with two identical columns in the merged DataFrame. Drop one of them by running the following code:<pre>del df_merge['Country or currency union']</pre></li>
				<li>Rename the columns by running the following code:<pre>name_map = {'Country (or dependency)': 'Country', 'Population (2017)': 'Population',
'Central bank interest rate (%)': 'Interest
rate'}
df_merge = df_merge.rename(columns=name_map)</pre><p>We are left with the following merged and cleaned DataFrame:</p><div><img src="img/C13018_03_14.jpg" alt="Figure 3.14: Ouput after cleaning and merging tables&#13;&#10;" width="874" height="440"/></div><h6>Figure 3.14: Ouput after cleaning and merging tables</h6></li>
				<li>Now that we have all the data in a nicely organized table, we can move on to the fun part: visualizing it. Let's save this table to a CSV file for later use, and then move on to discuss how visualizations can be created with Bokeh.<p>Write the merged data to a CSV file for later use with the following code:</p><pre>df_merge.to_csv('../data/countries/merged.csv', index=False)</pre></li>
			</ol>
			<h3 id="_idParaDest-65"><a id="_idTextAnchor067"/>Introduction to Bokeh</h3>
			<p>Bokeh is an interactive visualization library for Python. Its goal is to provide similar functionality to D3, the popular interactive visualization library for JavaScript. Bokeh functions very differently than D3, which is not surprising given the differences between Python and JavaScript. Overall, it's much simpler and it doesn't allow nearly as much customization as D3 does. This works to its advantage though, as it's much easier to use, and it still boasts an excellent suite of features that we'll explore in this section.</p>
			<p>Let's dive right into a quick exercise with the Jupyter Notebook and introduce Bokeh by example.</p>
			<h4>Note</h4>
			<p class="callout">There is good documentation online for Bokeh, but much of it is outdated. Searching something like Bokeh bar plot in Google still tends to turn up documentation for legacy modules that no longer exist, for example, the high-level plotting tools that used to be available through <code>bokeh.charts</code> (prior to version 0.12.0). These are the ones that take pandas DataFrames as input in much the same way that Seaborn plotting functions do. Removing the high-level plotting tools module has simplified Bokeh, and will allow for more focused development going forward. Now, the plotting tools are largely grouped into the <code>bokeh.plotting</code> module, as will be seen in the next exercise and following activity.</p>
			<h3 id="_idParaDest-66"><a id="_idTextAnchor068"/>Exercise 17: Introduction to Interactive Visualization With Bokeh</h3>
			<p>We'll load the required Bokeh modules and show some simple interactive plots that can be made with Bokeh. Please note that the examples in this book have been designed using version 0.12.10 of Bokeh.</p>
			<ol>
				<li value="1">In the <code>lesson-3-workbook.ipynb</code> Jupyter notebook, scroll to <code>Subtopic B: Introduction to Bokeh</code>.</li>
				<li>Like scikit-learn, Bokeh modules are usually loaded in pieces (unlike pandas, for example, where the whole library is loaded at once). Import some basic plotting modules by running the following code:<pre>from bokeh.plotting import figure, show, output_notebook output_notebook()</pre><p>We need to run <code>output_notebook()</code> in order to render the interactive visuals within the Jupyter notebook.</p></li>
				<li>Generate random data to plot by running the following code:<pre>np.random.seed(30)
data = pd.Series(np.random.randn(200),
index=list(range(200)))\
.cumsum() x = data.index
y = data.values</pre><p>The random data is generated using the cumulative sum of a random set of numbers that are distributed about zero. The effect is a trend that looks similar to a stock price time series, for example.</p></li>
				<li>Plot the data with a line plot in Bokeh by running the following code:<pre>p = figure(title='Example plot', x_axis_label='x', y_axis_ label='y')p.line(x, y, legend='Random trend') show(p)</pre><div><img src="img/C13018_03_15.jpg" alt="Figure 3.15: An example data plot&#13;&#10;" width="1800" height="1013"/></div><h6>Figure 3.15: An example data plot</h6><p>We instantiate the figure, as referenced by the variable <code>p</code>, and then plot a line. Running this in Jupyter yields an interactive figure with various options along the right-hand side.</p><p>The top three options (as of version 0.12.10) are <strong class="bold">Pan</strong>, <strong class="bold">Box Zoom</strong>, and <strong class="bold">Wheel Zoom</strong>. Play around with these and experiment with how they work. Use the reset option to re-load the default plot limits.</p></li>
				<li>Other plots can be created with the alternative methods of <code>figure</code>. Draw a scatter plot by running the following code, where we replace <code>line</code> in the preceding code with <code>circle</code>:<pre>size = np.random.rand(200) * 5
p = figure(title='Example plot', x_axis_label='x', y_axis_ label='y')
p.circle(x, y, radius=size, alpha=0.5, legend='Random dots')
show(p)</pre><div><img src="img/C13018_03_16.jpg" alt="Figure 3.16: An example scatter plot&#13;&#10;" width="1800" height="1077"/></div><h6>Figure 3.16: An example scatter plot</h6><p>Here, we've specified the size of each circle using a random set of numbers. A very enticing feature of interactive visualizations is the tooltip. This is a hover tool that allows the user to get information about a point by hovering over it.</p></li>
				<li>In order to add this tool, we're going to use a slightly different method for creating the plot. This will require us to import a couple of new libraries. Run the following code:<pre>from bokeh.plotting import ColumnDataSource from bokeh.models import HoverTool</pre><p>This time, we'll create a data source to pass to the plotting method. This can contain metadata, which can be included in the visualization via the hover tool.</p></li>
				<li>Create random labels and plot the interactive visualization with a hover tool by running the following code:<pre>source = ColumnDataSource(data=dict( x=x,
y=y,
...
...
source=source,
legend='Random dots')
show(p)</pre><h4>Note</h4><p class="callout">For the complete code, refer to the following: <a href="https://bit.ly/2RhpU1r">https://bit.ly/2RhpU1r</a>.</p><div><img src="img/C13018_03_17.jpg" alt="Figure 3.17: A random scatter plot with labels&#13;&#10;" width="1800" height="1135"/></div><h6>Figure 3.17: A random scatter plot with labels</h6><p>We define a data source for the plot by passing a dictionary of key/value pairs to the <code>ColumnDataSource</code> constructor. This source includes the <em class="italics">x</em> location, <em class="italics">y</em> location, and size of each point, along with the random letter <code>A</code>, <code>B</code>, or <code>C</code> for each point. These random letters are assigned as labels for the hover tool, which will also display the size of each point. The <code>bokeh.plotting.figure</code>.</p></li>
				<li>Add pan, zoom, and reset tools to the plot by running the following code:<pre>from bokeh.models import PanTool, BoxZoomTool, WheelZoomTool, ResetTool
...
...
legend='Random dots')
show(p)</pre><p>This code is identical to what was previously shown except for the <code>tools</code> variable, which now references several new tools we've imported from the Bokeh library.</p></li>
			</ol>
			<p>We'll stop the introductory exercise here, but we'll continue creating and exploring plots in the following activity.</p>
			<h3 id="_idParaDest-67"><a id="_idTextAnchor069"/>Activity 4: Exploring Data with Interactive Visualizations</h3>
			<p>You should have completed the previous exercise in order to complete this activity.</p>
			<p>We'll pick up using Bokeh right where we left off with the previous exercise, except instead of using the randomly generated data seen there, we'll instead use the data we scraped from the web in the first part of this chapter. Our aim is to use Bokeh to create interactive visualizations of our scraped data.</p>
			<p>In order to do so, we need to execute the following steps:</p>
			<ol>
				<li value="1">In the <code>lesson-3-workbook.ipynb</code> file, scroll to the <code>Activity B: Interactive visualizations with Bokeh</code> section.</li>
				<li>Load the previously scraped, merged, and cleaned web page data</li>
				<li>Recall what the data looks like by displaying the DataFrame.</li>
				<li>Draw a scatter plot of the population as a function of the interest rate.</li>
				<li>In the data, we see some clear outliers with high populations. Hover over these to see what they are. Select the Box Zoom tool and alter the viewing window to better see the majority of the data.</li>
				<li>Some of the lower population countries appear to have negative interest rates. Select the <strong class="bold">Wheel Zoom</strong> tool and use it to zoom in on this region. Use the <strong class="bold">Pan</strong> tool to re-center the plot, if needed, so that the negative interest rate samples are in view. Hover over some of these and see what countries they correspond to.</li>
				<li>Add a <strong class="bold">Year of last change</strong> column to the DataFrame and add a color based on the date of last interest rate change</li>
				<li>Create a map to group the last change date into color categories.</li>
				<li>Create the colored visualization.</li>
				<li>Looking for patterns, zoom in on the lower population countries.</li>
				<li>Plot the interest rate as a function of the year-over-year population change by running the following code.</li>
				<li>Determine the line of best fit for the previously plotted relationship.</li>
				<li>Re-plot the output obtained in the preceding step and add a line of best fit.</li>
				<li>Explore the plot by using the zoom tools and hovering over interesting samples.<h4>Note</h4><p class="callout">The detailed steps along with the solutions are presented in the <em class="italics">Appendix A</em> (pg. no. 163).</p></li>
			</ol>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor070"/>Summary</h2>
			<p>In this chapter, we scraped web page tables and then used interactive visualizations to study the data.</p>
			<p>We started by looking at how HTTP requests work, focusing on GET requests and their response status codes. Then, we went into the Jupyter Notebook and made HTTP requests with Python using the Requests library. We saw how Jupyter can be used to render HTML in the notebook, along with actual web pages that can be interacted with. After making requests, we saw how Beautiful Soup can be used to parse text from the HTML, and used this library to scrape tabular data.</p>
			<p>After scraping two tables of data, we stored them in pandas DataFrames. The first table contained the central bank interest rates for each country and the second table contained the populations. We combined these into a single table that was then used to create interactive visualizations.</p>
			<p>Finally, we used Bokeh to render interactive visualizations in Jupyter. We saw how to use the Bokeh API to create various customized plots and made scatter plots with specific interactive abilities such as zoom, pan, and hover. In terms of customization, we explicitly showed how to set the point radius and color for each data sample.</p>
			<p>Furthermore, when using Bokeh to explore the scraped population data, the tooltip was utilized to show country names and associated data when hovering over the points.</p>
			<p>Congratulations for completing this introductory course on data science using Jupyter Notebooks! Regardless of your experience with Jupyter and Python coming into the book, you've learned some useful and applicable skills for practical data science!</p>
			<p>Before finishing up, let's quickly recap the topics we've covered in this book.</p>
			<p>The first chapter was an introduction to the Jupyter Notebook platform, where we covered all of the fundamentals. We learned about the interface and how to use and install magic functions. Then, we introduced the Python libraries we'll be using and walked through an exploratory analysis of the <em class="italics">Boston housing</em> dataset.</p>
			<p>In the second chapter, we focused on doing machine learning with Jupyter. We first discussed the steps for developing a predictive analytics plan, and then looked at a few different types of models including SVM, a KNN classifier, and Random Forests.</p>
			<p>Working with an <em class="italics">employee retention</em> dataset, we applied data cleaning methods and then trained models to predict whether an employee has left or not. We also explored more advanced topics such as overfitting, k-fold cross-validation, and validation curves.</p>
			<p>Finally, in the third chapter, we shifted briefly from data analysis to data collection using web scraping and saw how to make HTTP requests and parse the HTML responses in Jupyter. Then, we finished up the book by using interactive visualizations to explore our collected data.</p>
		</div>
	</div></body></html>