["```py\ncat\n\n```", "```py\nHadoop fs -cat URI [URI …]\n\n```", "```py\nchmod\n\n```", "```py\nHadoop fs -chmod [-R] <MODE[,MODE]... &#124; OCTALMODE> URI [URI …]\n\n```", "```py\ncopyFromLocal\n\n```", "```py\nHadoop fs –copyFromLocal<localsrc> URI\n\n```", "```py\ncopyToLocal\n\n```", "```py\nHadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>\n\n```", "```py\ncp\n\n```", "```py\nHadoop fs -cp URI [URI …] <dest>\n\n```", "```py\ndu\n\n```", "```py\nHadoop fs -du URI [URI …]\n\n```", "```py\ndus\n\n```", "```py\nHadoop fs -dus<args>\n\n```", "```py\nget\n\n```", "```py\nHadoop fs -get [-ignorecrc] [-crc] <src><localdst>\n\n```", "```py\nls\n\n```", "```py\nHadoop fs –ls<args>\n\n```", "```py\nmkdir\n\n```", "```py\nHadoop fs –mkdir<paths>\n\n```", "```py\nlv\n\n```", "```py\nHadoop fs -mv URI [URI …] <dest>\n\n```", "```py\nrmr\n\n```", "```py\nHadoop fs -rmr URI [URI …]\n\n```", "```py\nsetrep\n\n```", "```py\nHadoop fs -setrep [-R] <path>\n\n```", "```py\ntail\n\n```", "```py\nHadoop fs -tail [-f] URI\n\n```", "```py\n        // Defining package of the class\n        package com.PACKT.chapter1;\n\n        // Importing java libraries \n        import java.io.*;\n        importjava.util.*;\n        import org.apache.hadoop.io.*;\n        import org.apache.hadoop.mapred.*;\n\n        // Defining the Map class\n        public class Map extends MapReduceBase implements\n                 Mapper<LongWritable, \n                        Text, \n                        Text, \n                        IntWritable>{\n\n        //Defining the map method – for processing the data with // problem specific logic\n        public void map(LongWritable key,\n                        Text value,\n                        OutputCollector<Text,\n                        IntWritable> output,\n                        Reporter reporter) \n                        throws IOException {\n\n        // For breaking the string to tokens and convert them to lowercase\n        StringTokenizer st = new StringTokenizer(value.toString().toLowerCase());\n\n        // For every string tokens\n        while(st.hasMoreTokens()) {\n\n        // Emitting the (key,value) pair with value 1.\n        output.collect(new Text(st.nextToken()), \n                       new IntWritable(1));\n                }\n\n            }\n\n        }\n        ```", "```py\n        // Defining package of the class\n        package com.PACKT.chapter1;\n\n        // Importing java libraries\n        import java.io.*;\n        importjava.util.*;\n        import org.apache.hadoop.io.*;\n        importorg.apache.hadoop.mapred.*;\n\n        // Defining the Reduce class \n        public class Reduce extends MapReduceBase implements\n                  Reducer<Text,\n                          IntWritable,\n                          Text,\n                          IntWritable> {\n\n        // Defining the reduce method for aggregating the //generated output of Map phase\n        public void reduce(Text key,\n                           Iterator<IntWritable> values,\n                           OutputCollector<Text,IntWritable>\n                           output, \n                           Reporter reporter) throws IOException {\n\n        // Setting initial counter value as 0\n        int count = 0;\n\n        // For every element with similar key attribute, increment its counter value by adding 1.\n        while(values.hasNext()) {\n        count += values.next().get();\n                }\n\n        // Emitting the (key,value) pair\n        output.collect(key, new IntWritable(count));\n            }\n        }\n        ```", "```py\n        //Defining package of the class\n        package com.PACKT.chapter1;\n\n        // Importing java libraries\n        import java.io.*;\n        importorg.apache.hadoop.fs.*;\n        import org.apache.hadoop.io.*;\n        importorg.apache.hadoop.mapred.*;\n        importorg.apache.hadoop.util.*;\n        importorg.apache.hadoop.conf.*;\n\n        //Defining wordcount class for job configuration \n          // information\n        public class WordCount extends Configured implements Tool{\n\n        publicint run(String[] args) throws IOException{\n        JobConfconf = new JobConf(WordCount.class);\n        conf.setJobName(\"wordcount\");\n\n        //For defining the output key format\n        conf.setOutputKeyClass(Text.class);\n\n        //For defining the output value format\n        conf.setOutputValueClass(IntWritable.class);\n\n        // For defining the Mapper class implementation\n        conf.setMapperClass(Map.class);\n\n        // For defining the Reducer class implementation\n        conf.setReducerClass(Reduce.class);\n\n        // For defining the type of input format \n        conf.setInputFormat(TextInputFormat.class);\n\n        // For defining the type of output format\n        conf.setOutputFormat(TextOutputFormat.class);\n\n        // For defining the command line argument sequence for // input dataset path\n        FileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n        // For defining the command line argument sequence for // output dataset path\n        FileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n        // For submitting the configuration object\n        JobClient.runJob(conf);\n\n        return 0;\n            }\n\n        // Defining the main() method to start the execution of // the MapReduce program\n        public static void main(String[] args) throws Exception {\n          intexitCode = ToolRunner.run(new WordCount(), args);\n          System.exit(exitCode); } }\n        ```", "```py\n    // create a folder for storing the compiled classes\n    hduser@ubuntu:~/Desktop/PacktPub$ mkdir classes\n\n    // compile the java class files with classpath\n    hduser@ubuntu:~/Desktop/PacktPub$ javac -classpath /usr/local/hadoop/hadoop-core-1.1.0.jar:/usr/local/hadoop/lib/commons-cli-1.2.jar -d classes *.java\n\n    ```", "```py\n    hduser@ubuntu:~/Desktop/PacktPub$ cd classes/\n\n    // create jar of developed java classes\n    hduser@ubuntu:~/Desktop/PacktPub/classes$ jar -cvf wordcount.jar com\n\n    ```", "```py\n    // Go to Hadoop home Directory\n    hduser@ubuntu:~$ cd $HADOOP_HOME\n\n    // Start Hadoop Cluster\n    hduser@ubuntu:/usr/local/hadoop$ bin/start-all.sh\n\n    ```", "```py\n    // Ensure all daemons are running properly \n    hduser@ubuntu:/usr/local/hadoop$ jps\n\n    ```", "```py\n    // Create Hadoop directory for storing the input dataset\n    hduser@ubuntu:/usr/local/hadoop$ bin/Hadoop fs -mkdir /wordcount/input\n\n    ```", "```py\n    // To copying the text files from machine's local\n     // directory in to Hadoop directory\n\n    hduser@ubuntu:/usr/local/hadoop$ bin/hadoopfs -copyFromLocal $HADOOP_HOME/*.txt /wordcount/input/\n\n    ```", "```py\n    // Command for running the Hadoop job by specifying jar, main class, input directory and output directory.\n\n    hduser@ubuntu:/usr/local/hadoop$ bin/hadoop jar wordcount.jar com.PACKT.chapter1.WordCount /wordcount/input/ /wordcount/output/\n\n    ```", "```py\n    // To read the generated output from HDFS directory\n\n    hduser@ubuntu:/usr/local/hadoop$ bin/hadoopfs -cat /wordcount/output/part-00000\n\n    ```"]