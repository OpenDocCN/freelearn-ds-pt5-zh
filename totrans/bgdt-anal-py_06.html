<html><head></head><body>
		<div><h1 id="_idParaDest-139"><em class="italics"><a id="_idTextAnchor153"/>Chapter 6</em></h1>
		</div>
		<div><h1 id="_idParaDest-140"><a id="_idTextAnchor154"/>Exploratory Data Analysis</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Implement the concept of reproducibility with Jupyter notebooks</li>
				<li class="bullets">Perform data gathering in a reproducible way</li>
				<li class="bullets">Implement suitable code practices and standards to keep analysis reproducible</li>
				<li class="bullets">Avoid the duplication of work by using IPython scripts</li>
			</ul>
			<p>In this chapter, we will learn what problem definition is and how to use the KPI analysis techniques to enable coherent and well rounded analysis from the data.</p>
		</div>
		<div><h2 id="_idParaDest-141"><a id="_idTextAnchor155"/>Introduction</h2>
			<p>One of the most important stages, and the initial step of a data science project, is understanding and defining a business problem. However, this cannot be a mere reiteration of the existing problem as a statement or a written report. To investigate a business problem in detail and define its purview, we can either use the existing business metrics to explain the patterns related to it or quantify and analyze the historical data and generate new metrics. Such identified metrics are the <strong class="bold">Key Performance Indicators</strong> (<strong class="bold">KPIs</strong>) that measure the problem at hand and convey to business stakeholders the impact of a problem.</p>
			<p>This chapter is all about understanding and defining a business problem, identifying key metrics related to it, and using these identified and generated KPIs through pandas and similar libraries for descriptive analytics. The chapter also covers how to plan a data science project through a structured approach and methodology, concluding with how to represent a problem using graphical and visualization techniques.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor156"/>Defining a Business Problem</h2>
			<p>A business problem in data science is a <em class="italics">long-term</em> or <em class="italics">short-term</em> challenge faced by a business entity that can prevent business goals being achieved and act as a constraint for growth and sustainability that can otherwise be prevented through an efficient data-driven decision system. Some typical data science business problems are predicting the demand for consumer products in the coming week, optimizing logistic operations for <strong class="bold">third-party logistics</strong> (<strong class="bold">3PL</strong>), and identifying fraudulent transactions in insurance claims.</p>
			<p>Data science and machine learning are not magical technologies that can solve these business problems by just ingesting data into pre-built algorithms. They are complex in terms of the approach and design needed to create end-to-end analytics projects.</p>
			<p>When a business needs such solutions, you may end up in a situation that forms a requirement gap if a clear understanding of the final objective is not set in place. A strong foundation to this starts with defining the business problem quantitatively and then carrying out scoping and solutions in line with the requirements.</p>
			<p>The following are a few examples of common data science use cases that will provide an intuitive idea about common business problems faced by the industry today that are solved through data science and analytics:</p>
			<ul>
				<li>Inaccurate demand/revenue/sales forecasts</li>
				<li>Poor customer conversion, churn, and retention</li>
				<li>Fraud and pricing in the lending industry and insurance</li>
				<li>Ineffective customer and vendor/distributor scoring</li>
				<li>Ineffective recommendation systems for cross-sell/up-sell</li>
				<li>Unpredictable machine failures and maintenance</li>
				<li>Customer sentiment/emotion analysis through text data</li>
				<li>Non-automation of repetitive tasks that require unstructured data analytics</li>
			</ul>
			<p>As we all know, in the last few years, industries have been changing tremendously, driven by the technology and innovation landscape. With the pace of evolving technology, successful businesses adapt to it, which leads to highly evolving and complex business challenges and problems. Understanding new business problems in such a dynamic environment is not a straightforward process. Though, case-by-case, business problems may change, as well as the approaches to them. However, the approach can be generalized to a large extent.</p>
			<p>The following pointers are a broad step-by-step approach for defining and concluding a business problem, and in the following section, a detailed description of each step is provided:</p>
			<ol>
				<li>Problem identification</li>
				<li>Requirement gathering</li>
				<li>Data pipeline and workflow</li>
				<li>Identifying measurable metrics</li>
				<li>Documentation and presentation<h4>Note</h4><p class="callout">The target variable, or the study variable, which is used as the attribute/variable/column in the dataset for studying the business problem, is also known as the <strong class="bold">dependent variable</strong> (<strong class="bold">DV</strong>), and all other attributes that are considered for the analysis are called <strong class="bold">independent variables</strong> (<strong class="bold">IVs</strong>).</p></li>
			</ol>
			<h3 id="_idParaDest-143"><a id="_idTextAnchor157"/>Problem Identification</h3>
			<p>Let's start with an example where an <strong class="bold">Asset Management Company</strong> (<strong class="bold">AMC</strong>) that has a strong hold on customer acquisition in their mutual funds domain, that is, targeting the right customers and onboarding them, is looking for higher customer retention to improve the average customer revenue and wallet share of their premium customers through data science-based solutions.</p>
			<p>Here, the business problem is how to increase the revenue from the existing customers and increase their wallet share.</p>
			<p>The problem statement is "<em class="italics">How do we improve the average customer revenue and increase the wallet share of premium customers through customer retention analytics?</em>" Summarizing the problem as stated will be the first step to defining a business problem.</p>
			<h3 id="_idParaDest-144"><a id="_idTextAnchor158"/>Requirement Gathering</h3>
			<p>Once the problem is identified, have a point-by-point discourse with your client, which can include a <strong class="bold">subject matter expert</strong> (<strong class="bold">SME</strong>) or someone who is well-versed in the problem. </p>
			<p>Endeavor to comprehend the issue from their perspective and make inquiries about the issue from various points of view, understand the requirements, and conclude how you can define the problem from the existing historical data. </p>
			<p>Now and again, you will find that clients themselves can't comprehend the issue well. In such cases, you should work with your client to work out a definition of the issue that is satisfactory to both of you.</p>
			<h3 id="_idParaDest-145"><a id="_idTextAnchor159"/>Data Pipeline and Workflow</h3>
			<p>After you have comprehended the issue in detail, the subsequent stage is to characterize and agree on quantifiable metrics for measuring the problem, that is, agreeing with the clients about the metrics to use for further analysis. In the long run, this will spare you a ton of issues.</p>
			<p>These metrics can be identified with the existing system for tracking the performance of the business, or new metrics can be derived from historical data.</p>
			<p>When you study the metrics for tracking the problem, the data for identifying and quantifying the problem may come from multiple data sources, databases, legacy systems, real-time data, and so on. The data scientist involved with this has to closely work with the client's data management teams to extract and gather the required data and push it into analytical tools for further analysis. For this, there needs to be a strong pipeline for acquiring data. The acquired data is further analyzed to identify its important attributes and how they change over time in order to generate the KPIs. This is a crucial stage in client engagement and working in tandem with their teams goes a long way toward making the work easier.</p>
			<h3 id="_idParaDest-146"><a id="_idTextAnchor160"/>Identifying Measurable Metrics</h3>
			<p>Once the required data is gathered through data pipelines, we can develop descriptive models to analyze historical data and generate insights into the business problem. <strong class="bold">Descriptive models/analytics</strong> is all about knowing <em class="italics">what has happened in the past</em> through time trend analysis and the density distribution of data analysis, among other things. For this, several attributes from the historical data need to be studied in order to gain insights into which of the data attributes are related to the current problem.</p>
			<p>An example, as explained in the previous case, is an AMC that is looking for solutions to their specific business problem with customer retention. We'll look into identifying how we can generate KPIs to understand the problem with retention.</p>
			<p>For this, historical data is mined to analyze the customer transaction patterns of previous investments and derive KPIs from them. A data scientist has to develop these KPIs based on their relevance and efficiency in explaining the variability of the problem, or in this case, the retention of customers.</p>
			<h3 id="_idParaDest-147"><a id="_idTextAnchor161"/>Documentation and Presentation</h3>
			<p>The final step is documenting the identified KPIs, their significant trends, and how they can impact the business in the long run. In the previous case of customer retention, all these metrics—<strong class="bold">length of relationship</strong>, <strong class="bold">average transaction frequency</strong>, <strong class="bold">churn rate</strong>—can act as KPIs and be used to explain the problem quantitatively.</p>
			<p>If we observe the trend in the churn rate, and let's say in this example, it has an increasing trend in the last few months, and if we graphically represent this, the client can easily understand the importance of having predictive churn analytics in place to identify the customer before they churn, and targeting stronger retention.</p>
			<p>The potential of having a retention system in place needs to be presented to the client for which the documentation and graphical representation of the KPIs need to be carried out. In the previous case, the identified KPIs, along with their change in pattern, need to be documented and presented to the client.<a id="_idTextAnchor162"/></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor163"/>Translating a Business Problem into Measurable Metrics and Exploratory Data Analysis (EDA)</h2>
			<p>If a specific business problem comes to us, we need to identify the KPIs that define that business problem and study the data related to it. Beyond generating KPIs related to the problem, looking into the trends and quantifying the problem through <strong class="bold">Exploratory Data Analysis</strong> (<strong class="bold">EDA</strong>) methods will be the next step. <a id="_idTextAnchor164"/></p>
			<p>The approach to explore KPIs is as follows: </p>
			<ul>
				<li>Data gathering</li>
				<li>Analysis of data generation</li>
				<li>KPI visualization</li>
				<li>Feature importance </li>
			</ul>
			<h3 id="_idParaDest-149"><a id="_idTextAnchor165"/>Data Gathering</h3>
			<p>The data that is required for analyzing the problem is part of defining the business problem. However, the selection of attributes from the data will change according to the business problem. Consider the following examples:</p>
			<ul>
				<li>If it is a recommendation engine or churn analysis of customers, we need to look into historical purchases and <strong class="bold">Know Your Customer</strong> (<strong class="bold">KYC</strong>) data, among other data.</li>
				<li>If it is related to forecasting demand, we need to look into daily sales data.</li>
			</ul>
			<p>It needs to be concluded that the required data can change from problem to problem.</p>
			<h3 id="_idParaDest-150"><a id="_idTextAnchor166"/>Analysis of Data Generation</h3>
			<p>From the available data sources, the next step is to identify the metrics related to the defined problem. Apart from the preprocessing of data (refer to <em class="italics">Chapter 1</em>, <em class="italics">The Python Data Science Stack</em>, for details on data manipulation), at times, we need to manipulate the data to generate such metrics, or they can be directly available in the given data.</p>
			<p>For example, let's say we are looking at supervised analysis, such as a <strong class="bold">predictive maintenance problem</strong> (a problem where predictive analytics is used to predict the condition of in-service equipment or machinery before it fails), where sensor- or computer-generated log data is used. Although log data is unstructured, we can identify which of the log files explain the failures of machinery and which do not. Unstructured data is without columns or rows. For example, it can be in XML or a similar format. Computer-generated log data is an example. Such data needs to be converted into columns and rows or make them structured, or label them, that is, provide column names for the data by converting the data into rows and columns. </p>
			<p>Another example is identifying the churn of customers and predicting future customers who may churn in the coming periods, where we have transactional data on purchases with the features related to each purchase. Here, we need to manipulate the data and transform the current data in order to identify which customers have churned and which have not from all the purchase-related data.</p>
			<p>To explain this better, in the raw data, there may be many rows of purchases for each customer, with the date of purchase, the units purchased, the price, and so on. All the purchases related to a customer need to be identified as a single row whether the customer has churned (churned means customers who discontinue using a product or service, also known as customer attrition) or not and with all the related information.</p>
			<p>Here, we will be generating a KPI: <strong class="bold">Churned</strong> or <strong class="bold">Not Churned</strong> attributes for a customer, and similarly for all customers. The identified variable that defines the business problem is the target variable. A target variable is also known as the <strong class="bold">response variable</strong> or <strong class="bold">dependent variable</strong>. In <em class="italics">Exercise XX</em>, <em class="italics">Generate the Feature Importance for the Target Variable and Carry Out EDA</em>, in this chapter, it's captured and defined through the <strong class="bold">churn</strong> attribute.</p>
			<h3 id="_idParaDest-151"><a id="_idTextAnchor167"/>KPI Visualization</h3>
			<p>To understand the trends and patterns in the KPIs, we need to represent them through interactive visualization techniques. We can use different methods, such as boxplot, time-trend graphs, density plots, scatter plots, pie charts, and heatmaps. We will learn more on this in <em class="italics">Exercise XX</em>, <em class="italics">Generate the Feature Importance for the Target Variable and Carry Out EDA</em>, in this chapter.</p>
			<h3 id="_idParaDest-152"><a id="_idTextAnchor168"/>Feature Importance</h3>
			<p>Once the target variable is identified, the other attributes in the data and their importance to it in explaining the variability of the target variable need to be studied. For this, we use association, variance, and correlation methods to establish relations with the target variable of other variables (<strong class="bold">explanatory</strong> or <strong class="bold">independent</strong> variables).</p>
			<p>There are multiple feature-importance methods and algorithms that can be used depending on the type of variables in the study, such as Pearson Correlation, Chi-Square tests, and algorithms based on Gini variable importance, decision trees, and Boruta, among others.</p>
			<h4>Note</h4>
			<p class="callout">The <strong class="bold">target variable</strong> or the <strong class="bold">study variable</strong>, which is used as the attribute/variable/column in the dataset for studying the business problem is also known as the <strong class="bold">dependent variable</strong> (<strong class="bold">DV</strong>), and all other attributes that are considered for the analysis are called <strong class="bold">independent variables</strong> (<strong class="bold">IVs</strong>).</p>
			<p>In the following exercise, we will cover data gathering and analysis-data <strong class="bold">(data that is generated by merging or combining multiple data sources to get one dataset for the analysis) </strong>generation with KPI visualization, and then in the subsequent exercise, we will cover what feature importance is.</p>
			<h3 id="_idParaDest-153"><a id="_idTextAnchor169"/>Exercise 43: Identify the Target Variable and Related KPIs from the Given Data for the Business Problem</h3>
			<p>Let's take the example of a <strong class="bold">subscription problem</strong> in the banking sector. We will use data that is from direct marketing campaigns by a Portuguese banking institution, where a customer either opens a term deposit (ref: <a href="https://www.canstar.com.au/term-deposits/what-is-a-term-deposit/">https://www.canstar.com.au/term-deposits/what-is-a-term-deposit/</a>) or not after the campaign. The subscription problem is characterized or defined contrastingly by every organization. For the most part, the customers who will subscribe to a service (here, it is a term deposit) have higher conversion potential (that is, from lead to customer conversion) to a service or product. Thus, in this problem, subscription metrics, that is, the outcome of historical data, is considered as the target variable or KPI.</p>
			<p>We will use descriptive analytics to explore trends in the data. We will start by identifying and defining the target variable (here, subscribed or not subscribed) and the related KPIs:</p>
			<ol>
				<li value="1">Download the bank.csv data from the following online resources:<ul><li><a href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing">https://archive.ics.uci.edu/ml/datasets/Bank+Marketing</a></li><li><a href="https://archive.ics.uci.edu/ml/machine-learning-databases/00222/">https://archive.ics.uci.edu/ml/machine-learning-databases/00222/</a></li></ul></li>
				<li>Create a folder for the exercise (<code>packt_exercises</code>) and save the downloaded data there.</li>
				<li>Start Jupyter notebook and import all the required libraries as illustrated. Now set the working directory using the <code>os.chdir()</code> function:<pre>import numpy as np
import pandas as pd
import seaborn as sns
import time
import re
import os
import matplotlib.pyplot as plt
sns.set(style="ticks")
os.chdir("/Users/svk/Desktop/packt_exercises")</pre></li>
				<li>Use the following code to read the CSV and explore the dataset:<pre>df = pd.read_csv('bank.csv', sep=';')
df.head(5)
print(df.shape)
df.head(5)
df.info()
df.describe()</pre></li>
				<li>After executing the previous command, you will get output similar to the following:<div><img alt="" src="img/C12913_06_01.jpg"/></div><h6>Figure 6.1: Bank DataFrame</h6><p>When studying the target variable (subscribed or not subscribed—<code>y</code>), it is important to look at the distribution of it. The type of target variable in this dataset is categorical, or of multiple classes. In this case, it's binary (Yes/No).</p><p>When the distribution is skewed to one class, the problem is known as an <em class="italics">imbalance in the variable</em>. We can study the proportion of the target variable using a bar plot. This gives us an idea about how many of each class there is (in this case, how many each of no and yes). The proportion of no is way higher than yes, which explains the imbalance in the data.</p></li>
				<li>Let's execute the following commands to plot a bar plot for the given data:<pre>count_number_susbc = df["y"].value_counts()
sns.barplot(count_number_susbc.index, count_number_susbc.values)
df['y'].value_counts()</pre><p>The output is as follows:</p><div><img alt="Figure 6.2: Bar plot" src="img/Image38462.jpg"/></div><h6>Figure 6.2: Bar plot</h6></li>
				<li>Now, we will take each variable and look at their distribution trends. The following histogram, which is provided as an example, is of the '<code>age'</code> column (attribute) in the dataset. Histograms/density plots are a great way to explore numerical/float variables, similar to bar plots. They can be used for categorical data variables. Here, we will show two numerical variables, <code>age</code> and <code>balance</code>, as examples using a histogram, and two categorical variables, <code>education</code> and <code>month</code>, using bar plots:<pre># histogram for age (using matplotlib)
plt.hist(df['age'], color = 'grey', edgecolor = 'black',
         bins = int(180/5))
# histogram for age (using seaborn)
sns.distplot(df['age'], hist=True, kde=False, 
             bins=int(180/5), color = 'blue',
             hist_kws={'edgecolor':'black'})</pre><p>The histogram is as follows:</p><div><img alt="Figure 6.3: Histogram for age" src="img/C12913_06_03.jpg"/></div><h6>Figure 6.3: Histogram for age</h6></li>
				<li>To plot the histogram for the <code>balance</code> attribute in the dataset, use the following command:<pre># histogram for balance (using matplotlib)
plt.hist(df['balance'], color = 'grey', edgecolor = 'black',
         bins = int(180/5))
# histogram for balance (using seaborn)
sns.distplot(df['balance'], hist=True, kde=False, 
             bins=int(180/5), color = 'blue',
             hist_kws={'edgecolor':'black'})</pre><p>The histogram for balance is as follows:</p><div><img alt="Figure 6.4: Histogram for balance" src="img/C12913_06_04.jpg"/></div><h6>Figure 6.4: Histogram for balance</h6></li>
				<li>Now, using the following code, plot a bar plot for the <code>education</code> attribute in the dataset:<pre># barplot for the variable 'education'
count_number_susbc = df["education"].value_counts()
sns.barplot(count_number_susbc.index, count_number_susbc.values)
df['education'].value_counts()</pre><p>The bar plot for the <code>education</code> attribute is as follows:</p><div><img alt="Figure 6.5: Bar plot for education" src="img/C12913_06_05.jpg"/></div><h6>Figure 6.5: Bar plot for education</h6></li>
				<li>Use the following command to plot a bar plot for the <code>month</code> attribute of the dataset:<pre># barplot for the variable 'month'
count_number_susbc = df["month"].value_counts()
sns.barplot(count_number_susbc.index, count_number_susbc.values)
df['education'].value_counts()</pre><p>The plotted graph is as follows:</p><div><img alt="Figure 6.6: Bar plot for month" src="img/C12913_06_06.jpg"/></div><h6>Figure 6.6: Bar plot for month</h6></li>
				<li>The next task is to generate a distribution for each class of the target variable to compare the distributions. Plot a histogram of the <code>age</code> attribute for the target variable (<code>yes</code>/<code>no</code>):<pre># generate separate list for each subscription type for age
x1 = list(df[df['y'] == 'yes']['age'])
x2 = list(df[df['y'] == 'no']['age'])
# assign colors for each subscription type 
colors = ['#E69F00', '#56B4E9']
names = ['yes', 'no']
# plot the histogram
plt.hist([x1, x2], bins = int(180/15), density=True,
         color = colors, label=names)
# plot formatting
plt.legend()
plt.xlabel('IV')
plt.ylabel('prob distr (IV) for yes and no')
plt.title('Histogram for Yes and No Events w.r.t. IV')</pre><p>The bar plot of the <code>month</code> attribute target variable is as follows:</p><div><img alt="Figure 6.7: Bar plot of the month attribute for the target variable" src="img/C12913_06_07.jpg"/></div><h6>Figure 6.7: Bar plot of the month attribute for the target variable</h6></li>
				<li>Now, using the following command, plot a bar plot for the target variable grouped by month:<pre>df.groupby(["month", "y"]).size().unstack().plot(kind='bar', stacked=True, figsize=(20,10))</pre><p>The plotted graph is as follows:</p></li>
			</ol>
			<div><div><img alt="Figure 6.8: Histogram grouped by month" src="img/C12913_06_08.jpg"/>
				</div>
			</div>
			<h6>Figure 6.8: Histogram grouped by month</h6>
			<p>In this <a id="_idTextAnchor170"/><a id="_idTextAnchor171"/>exercise, we looked into establishing KPIs and the target variable—<strong class="bold">data gathering</strong> and <strong class="bold">analysis-data (data that is generated by merging or combining multiple data sources to get one dataset for analysis) generation</strong>. The KPIs and target variable have been determined—<strong class="bold">KPI visualization</strong>. Now, in the next exercise, we will identify which of the variables are important in terms of explaining the variance of the target variable—<strong class="bold">feature importance</strong>.</p>
			<h3 id="_idParaDest-154"><a id="_idTextAnchor172"/>Exercise 44: Generate the Feature Importance of the Target Variable and Carry Out EDA</h3>
			<p>In the previous exercise, we looked into the trends of the attributes, identifying their distribution, and how we can use various plots and visualization methods to carry these out. Prior to tackling a modeling problem, whether it is a predictive or a classification problem (for example, from the previous marketing campaign data, how to predict future customers that will have the highest probability of converting), we have to pre-process the data and select the important features that will impact the subscription campaign output models. To do this, we have to see the association of the attributes to the outcome (the target variable), that is, how much variability of the target variable is explained by each variable.</p>
			<p>Associations between variables can be drawn using multiple methods; however, we have to consider the data type when choosing a method/algorithm. For example, if we are studying numerical variables (integers that are ordered, floats, and so on), we can use correlation analysis; if we are studying categorical variables with multiple classes, we can use Chi-Square methods. However, there are many algorithms that can handle both together and provide measurable outcomes to compare the importance of variables.</p>
			<p>In this exercise, we will look at how various methods can be used to identify the importance of features:</p>
			<ol>
				<li value="1">Download the <code>bank.csv</code> file and read the data using the following command:<pre>import numpy as np
import pandas as pd
import seaborn as sns
import time
import re
import os
import matplotlib.pyplot as plt
sns.set(style="ticks")
# set the working directory # in the example, the folder 
# 'packt_exercises' is in the desktop
os.chdir("/Users/svk/Desktop/packt_exercises")
# read the downloaded input data (marketing data)
df = pd.read_csv('bank.csv', sep=';')</pre></li>
				<li>Develop a correlation matrix using the following command to identify the correlation between the variables:<pre>df['y'].replace(['yes','no'],[1,0],inplace=True)
df['default'].replace(['yes','no'],[1,0],inplace=True)
df['housing'].replace(['yes','no'],[1,0],inplace=True)
df['loan'].replace(['yes','no'],[1,0],inplace=True)
corr_df = df.corr()
sns.heatmap(corr_df, xticklabels=corr_df.columns.values, yticklabels=corr_df.columns.values, annot = True, annot_kws={'size':12})
heat_map=plt.gcf(); heat_map.set_size_inches(10,5)
plt.xticks(fontsize=10); plt.yticks(fontsize=10); plt.show()</pre><p>The plotted correlation matrix heatmap is as follows:</p><div><img alt="Figure 6.9: Correlation matrix" src="img/C12913_06_09.jpg"/></div><h6>Figure 6.9: Correlation matrix</h6><h4>Note</h4><p class="callout"><code>-1</code> to <code>+1</code>, where a value close to <code>0</code> signifies no relation, -1 signifies a relation where one variable reduces as the other increases (inverse), and +1 signifies that as one variable increases the other also increases (direct).</p><p class="callout">High correlation among independent variables (which are all variables except the target variable) can lead to multicollinearity among the variables, which can affect the predictive model accuracy.</p><h4>Note</h4><p class="callout">Make sure to install boruta if not installed already using the following command:</p><p class="callout"><code>pip install boruta --upgrade</code></p></li>
				<li>Build a feature importance output based on Boruta (a wrapper algorithm around a random forest):<pre># import DecisionTreeClassifier from sklearn and 
# BorutaPy from boruta
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from boruta import BorutaPy
# transform all categorical data types to integers (hot-encoding)
for col_name in df.columns:
    if(df[col_name].dtype == 'object'):
        df[col_name]= df[col_name].astype('category')
        df[col_name] = df[col_name].cat.codes
# generate separate dataframes for IVs and DV (target variable)
X = df.drop(['y'], axis=1).values
Y = df['y'].values
# build RandomForestClassifier, Boruta models and
# related parameter
rfc = RandomForestClassifier(n_estimators=200, n_jobs=4, class_weight='balanced', max_depth=6)
boruta_selector = BorutaPy(rfc, n_estimators='auto', verbose=2)
n_train = len(X)
# fit Boruta algorithm
boruta_selector.fit(X, Y)</pre><p>The output is as follows:</p><div><img alt="Figure 6.10: Fit Boruta algorithm" src="img/C12913_06_10.jpg"/></div><h6>Figure 6.10: Fit Boruta algorithm</h6></li>
				<li>Check the rank of features as follows:<pre>feature_df = pd.DataFrame(df.drop(['y'], axis=1).columns.tolist(), columns=['features'])
feature_df['rank']=boruta_selector.ranking_
feature_df = feature_df.sort_values('rank', ascending=True).reset_index(drop=True)
sns.barplot(x='rank',y='features',data=feature_df)
feature_df</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img alt="Figure 6.11: Boruta output" src="img/C12913_06_11.jpg"/>
				</div>
			</div>
			<h6>Figure 6.<a id="_idTextAnchor173"/>11: Boruta output</h6>
			<h2 id="_idParaDest-155">Structured <a id="_idTextAnchor174"/>Approach to the Data Science Project Life Cycle</h2>
			<p>Embarking on data science projects needs a robust methodology in planning the project, taking into consideration the potential scaling, maintenance, and team structure. We have learned how to define a business problem and quantify it with measurable parameters, so the next stage is a project plan that includes the development of the solution, to the deployment of a consumable business application. </p>
			<p>This topic puts together some of the best industry practices structurally with examples for data science project life cycle management. This approach is an idealized sequence of stages; however, in real applications, the order can change according to the type of solution that is required.</p>
			<p>Typically, a data science project for a single model deployment takes around three months, but this can increase to six months, or even up to a year. Defining a process from data to deployment is the key to reducing the time to deployment.</p>
			<h3 id="_idParaDest-156">Data Scienc<a id="_idTextAnchor175"/>e Project Life Cycle Phases</h3>
			<p>The stages of a data science project life cycle are as follows:</p>
			<ol>
				<li value="1">Understanding and defining the business problem</li>
				<li>Data access and discovery</li>
				<li>Data engineering and pre-processing</li>
				<li>Modeling development and evaluation</li>
			</ol>
			<h3 id="_idParaDest-157">Phase 1: Un<a id="_idTextAnchor176"/>derstanding and Defining the Business Problem</h3>
			<p>Every data science project starts with learning the business domain and framing the business problem. In most organizations, the application of advanced analytics and data science techniques is a fledgling discipline, and most of the data scientists involved will have a limited understanding of the business domains.</p>
			<p>To understand the business problem and the domain, the key stakeholders and <strong class="bold">subject matter experts</strong> (<strong class="bold">SMEs</strong>) need to be identified. Then, the SMEs and the data scientists interact with each other to develop the initial hypothesis and identify the data sources required for the solution's development. This is the first phase of understanding a data science project.</p>
			<p>Once we have a well-structured business problem and the required data is identified, along with the data sources, the next phase of data discovery can be initiated. The first phase is crucial in building a strong base for scoping and the solution approach.</p>
			<h3 id="_idParaDest-158">Phase 2: Da<a id="_idTextAnchor177"/>ta Access and Discovery</h3>
			<p>This phase includes identifying the data sources and building data pipelines and data workflows in order to acquire the data. The nature of the solution and the dependent data can vary from one problem to another in terms of the structure, velocity, and volume.</p>
			<p>At this stage, it's important to identify how to acquire the data from the data sources. This can be through direct connectors (libraries available on Python) using database access credentials, having APIs that provide access to the data, directly crawling data from web sources, or can even be data dumps provided in CSVs for initial prototype developments. Once a strong data pipeline and workflow for acquiring data is established, the data scientists can start exploring the data to prepare the analysis-data <strong class="bold">(data that is generated by merging or combining multiple data sources to get one dataset for analysis).</strong></p>
			<h3 id="_idParaDest-159">Phase 3: Da<a id="_idTextAnchor178"/>ta Engineering and Pre-processing</h3>
			<p>The pre-processing of data means transforming raw data into a form that's consumable by a machine learning algorithm. It is essentially the manipulation of data into a structure that's suitable for further analysis or getting the data into a form that can be input data for modeling purposes. Often, the data required for analysis can reside in several tables, databases, or even external data sources. </p>
			<p>A data scientist needs to identify the required attributes from these data sources and merge the available data tables to get what is required for the analytics model. This is a tedious and time-consuming phase on which data scientists spend a considerable part of the development cycle.</p>
			<p>The pre-processing of data includes activities such as outlier treatment, missing value imputation, scaling features, mapping the data to a Gaussian (or normal) distribution, encoding categorical data, and discretization, among others.</p>
			<p>To develop robust machine learning models, it's important that data is pre-processed efficiently.</p>
			<h4>Note</h4>
			<p class="callout">Python has a few libraries that are used for data pre-processing. scikit-learn has many efficient built-in methods for pre-processing data. The scikit-learn documentation can be found at <a href="https://scikit-learn.org/stable/modules/preprocessing.html">https://scikit-learn.org/stable/modules/preprocessing.html</a>.</p>
			<p>Let's go through the following activity to understand how to carry out data engineering and pre-processing using one of the pre-processing techniques, such as Gaussian normalization. </p>
			<h3 id="_idParaDest-160">Activity 13:<a id="_idTextAnchor179"/><a id="_idTextAnchor180"/><a id="_idTextAnchor181"/> Carry Out Mapping to Gaussian Distribution of Numeric Features from the Given Data</h3>
			<p>Various pre-processing techniques need to be carried out to prepare the data before pushing it into an algorithm.</p>
			<h4>Note</h4>
			<p class="callout">Explore this website to find various methods for pre-processing: <a href="https://scikit-learn.org/stable/modules/preprocessing.html">https://scikit-learn.org/stable/modules/preprocessing.html</a>.</p>
			<p>In this exercise, we will carry out data normalization, which is important for parametric models such as linear regression, logistic regression, and many others:</p>
			<ol>
				<li value="1">Use the <code>bank.csv</code> file and import all the required packages and libraries into the Jupyter notebook.</li>
				<li>Now, determine the numeric data from the DataFrame. Categorize the data according to its type, such as categorical, numeric (float or integer), date, and so on. Let's carry out normalization on numeric data.</li>
				<li>Carry out a normality test and identify the features that have a non-normal distribution.</li>
				<li>Plot the probability density of the features to visually analyze the distribution of the features.</li>
				<li>Prepare the power transformation model and carry out transformations on the identified features to convert them to normal distribution based on the <code>box-cox</code> or <code>yeo-johnson</code> method.</li>
				<li>Apply the transformations on new data (evaluation data) with the same generated parameters for the features that were identified in the training data.<h4>Note</h4><p class="callout">Multiple variables' density plots are shown in the previous plot after the transformations are carried out. The distribution of the features in the plots is closer to a Gaussian distribution.</p><p class="callout">The solution for this activity can be found on page 236.</p></li>
			</ol>
			<p>Once the transformations are carried out, we analyze the normality of the features again to see the effect. You can see that after the transformations, some of the features can have the null hypothesis not rejected (that is, the distribution is still not Gaussian) with almost all features higher <code>p</code> values (refer to point 2 of this activity). Once the transformed data is generated, we bind it back to the numeric data.</p>
			<h3 id="_idParaDest-161">Phase 4: Mo<a id="_idTextAnchor182"/>del Development</h3>
			<p>Once we have a cleaned and pre-processed data, it can be ingested into a machine learning algorithm for exploratory or predictive analysis, or for other applications. Although the approach to a problem may be designed, whether it's a classification, association, or a regression problem, for example, the specific algorithm that needs to considered for the data has to be identified. For example, if it's a classification problem, it could be a decision tree, a support vector machine, or a neural network with many layers. </p>
			<p>For modeling purposes, the data needs to be separated into testing and training data. The model is developed on the training data and its performance (accuracy/error) is evaluated on the testing data. With the algorithms selected, the related parameters with respect to it need to be tuned by the data scientist to develop a robust model.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor183"/>Summary</h2>
			<p>In <a id="_idTextAnchor184"/>this chapter, we learned how to define a business problem from a data science perspective through a well-defined, structured approach. We started by understanding how to approach a business problem, how to gather the requirements from stakeholders and business experts, and how to define the business problem by developing an initial hypothesis.</p>
			<p>Once the business problem was defined with data pipelines and workflows, we looked into understanding how to start the analysis on the gathered data in order to generate the KPIs and carry out descriptive analytics to identify the key trends and patterns in the historical data through various visualization techniques.</p>
			<p>We also learned how a data science project life cycle is structured, from defining the business problem to various pre-processing techniques and model development. In the next chapter, we will be learning how to implement the concept of high reproducibility on a Jupyter notebook, and its importance in development.</p>
		</div>
	</body></html>