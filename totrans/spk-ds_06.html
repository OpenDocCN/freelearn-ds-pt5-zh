<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch06" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 6.   Machine Learning  </h1></div></div></div><p class="calibre11">We are the consumers of machine learning every day, whether we notice or not. E-mail providers such as Google automatically push some incoming mails into the <code class="literal">Spam</code> folder and online shopping sites such as Amazon or social networking sites such as Facebook jump in with unsolicited recommendations that are surprisingly useful. So, what enables these software products to reconnect long lost friends? These are just a few examples of machine learning in action.</p><p class="calibre11">Formally, machine learning is a part of <strong class="calibre19">Artificial Intelligence</strong> (<strong class="calibre19">AI</strong>) which deals with a class of algorithms that can learn from data and make predictions. The techniques and underlying concepts are drawn from the field of statistics. Machine learning exists at the intersection of computer science and statistics and is considered one of the most important components of data science. It has been around for quite some time now, but its complexity has only increased with increase in data and scalability requirements. Machine learning algorithms tend to be resource intensive and iterative in nature, which render them a poor fit for MapReduce paradigm. MapReduce works very well for single pass algorithms but does not cater so well for multi-pass counterparts. The Spark research program was started precisely to address this challenge. Apache Spark is equipped with efficient algorithms in its MLlib library that are designed to perform well even in iterative computational requirements.</p><p class="calibre11">The previous chapter outlined the data analytics' life cycle and its various components such as data cleaning, data transformation, sampling techniques, and graphical techniques to visualize the data, along with concepts covering descriptive statistics and inferential statistics. We also looked at some of the statistical testing that could be performed on the Spark platform. Further to the basics we built up in the previous chapter, we are going to cover in this chapter most of the machine learning algorithms and how to use them to build models on Spark.</p><p class="calibre11">As a prerequisite for this chapter, basic understanding of machine learning algorithms and computer science fundamentals are nice to have. However, we have covered some theoretical basics of the algorithms with right set of practical examples to make those more comprehendible and easy to implement. The topics covered in this chapter are:</p><div><ul class="itemizedlist"><li class="listitem">Introduction to machine learning<div><ul class="itemizedlist1"><li class="listitem">The evolution</li><li class="listitem">Supervised learning</li><li class="listitem">Unsupervised learning</li></ul></div><p class="calibre31">
</p></li><li class="listitem">MLlib and the Pipeline API<div><ul class="itemizedlist1"><li class="listitem">MLlib</li><li class="listitem">ML pipeline</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Introduction to machine learning<div><ul class="itemizedlist1"><li class="listitem">Parametric methods</li><li class="listitem">Non-parametric methods</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Regression methods<div><ul class="itemizedlist1"><li class="listitem">Linear regression </li><li class="listitem">Regularization on regression</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Classification methods<div><ul class="itemizedlist1"><li class="listitem">Logistic regression</li><li class="listitem">Linear Support Vector Machines (SVMs)</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Decision trees<div><ul class="itemizedlist1"><li class="listitem">Impurity measures</li><li class="listitem">Stopping rule</li><li class="listitem">Split canditate</li><li class="listitem">Advantages of decision tress</li><li class="listitem">Example</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Ensembles<div><ul class="itemizedlist1"><li class="listitem">Random forests</li><li class="listitem">Gradient boosted trees</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Multilayer perceptron classifier</li><li class="listitem">Clustering techniques<div><ul class="itemizedlist1"><li class="listitem">K-means clustering</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Summary</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec42" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Introduction</h1></div></div></div><p class="calibre11">Machine learning is all about learning by example data; examples that produce a particular output for a given input. There are various business use cases for machine learning. Let us look at a few examples to get an idea of what exactly it is:</p><div><ul class="itemizedlist"><li class="listitem">A recommendation engine that recommends users what they might be interested in buying</li><li class="listitem">Customer segmentation (grouping customers who share similar characteristics) for marketing campaigns</li><li class="listitem">Disease classification for cancer - malignant/benign</li><li class="listitem">Predictive modeling, for example, sales forecasting, weather forecasting</li><li class="listitem">Drawing business inferences, for example, understanding what effect will change the price of a product have on sales</li></ul></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec62" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The evolution</h2></div></div></div><p class="calibre11">The concept of statistical learning was existent even before the first computer system was introduced. In the nineteenth century, the least squares technique (now called linear regression) had already been developed. For classification problems, Fisher came up with <strong class="calibre19">Linear Discriminant Analysis</strong> (<strong class="calibre19">LDA</strong>). Around the 1940s, an alternative to LDA, known as <strong class="calibre19">logistic regression</strong>, was proposed and all these approaches not only improved with time, but also inspired the development of other new algorithms.</p><p class="calibre11">During those times, computation was a big problem as it was done using pen and paper. So fitting non-linear equations was not quite feasible as it required a lot of computations. After the 1980s, with improvements in technology and the introduction of computer systems, classification/regression trees were introduced. Slowly, with further advancements in technology and computing systems, statistical learning in a way converged with what is now known as machine learning.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec63" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Supervised learning</h2></div></div></div><p class="calibre11">As discussed in the previous section, machine learning is all about learning by example data. Based on how the algorithms understand data and get trained on it, they are broadly divided into two categories: <strong class="calibre19">supervised learning</strong> and <strong class="calibre19">unsupervised learning</strong>.</p><p class="calibre11">Supervised statistical learning involves building a model based on one or more inputs for a particular output. This means that the output that we get can supervise our analysis based on the inputs we supply. In other words, for each observation of the predictor variables (for example, age, education, and expense variables), there is an associated response measurement of the outcome variable (for example, salary). Refer to the following table to get an idea of the example dataset where we are trying to predict the <strong class="calibre19">Salary</strong> based on the <strong class="calibre19">Age</strong>, <strong class="calibre19">Education,</strong> and <strong class="calibre19">Expense</strong> variables:</p><p class="calibre11">
</p><div><img src="img/image_06_001.jpg" alt="Supervised learning" class="calibre96"/></div><p class="calibre11">
</p><p class="calibre11">Supervised algorithms can be used for predicting, estimating, classifying, and other similar requirements which we will cover in the following sections.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec64" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Unsupervised learning</h2></div></div></div><p class="calibre11">Unsupervised statistical learning involves building a model based on one or more inputs but with no intention to produce a specific output. This means that there is no response/output variable to predict explicitly; but the output is usually the groups of data points that share some similar characteristics. Unlike supervised learning, you are not aware of the groups/labels to classify the data points into, per say, and you leave it to the algorithm to decide by itself.</p><p class="calibre11">Here, there is no concept of a <code class="literal">training</code> dataset that is used to <code class="literal">relate</code> the outcome variable with the <code class="literal">predictor</code> variables by building a model and then validate the model using the <code class="literal">test</code> dataset. The output of unsupervised algorithm cannot supervise your analysis based on the inputs you supply. Such algorithms can learn relationships and structure from data. <em class="calibre22">Clustering</em> and <em class="calibre22">Association rule learning</em> are examples of unsupervised learning techniques.</p><p class="calibre11">The following image depicts how clustering is used to group the data items that share some similar characteristics:</p><p class="calibre11">
</p><div><img src="img/image_06_002.jpg" alt="Unsupervised learning" class="calibre97"/></div><p class="calibre11">
</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec43" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>MLlib and the Pipeline API</h1></div></div></div><p class="calibre11">Let us first learn some Spark fundamentals to be able to perform the machine learning operations on it. We will discuss the MLlib and the pipeline API in this section.</p><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec65" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>MLlib</h2></div></div></div><p class="calibre11">MLlib is the machine learning library built on top of Apache Spark which homes most of the algorithms that can be implemented at scale. The seamless integration of MLlib with other components such as GraphX, SQL, and Streaming provides developers with an opportunity to assemble complex, scalable, and efficient workflows relatively easily. The MLlib library consists of common learning algorithms and utilities including classification, regression, clustering, collaborative filtering, and dimensionality reduction.</p><p class="calibre11">MLlib works in conjunction with the <code class="literal">spark.ml</code> package which provides a high level Pipeline API. The fundamental difference between these two packages is that MLlib (<code class="literal">spark.mllib</code>) works on top of RDDs whereas the ML (<code class="literal">spark.ml</code>) package works on top of DataFrames and supports ML Pipeline. Currently, both packages are supported by Spark but it is recommended to use the <code class="literal">spark.ml</code> package.</p><p class="calibre11">Fundamental data types in this library are vectors and matrices. Vectors are local, and may be dense or sparse. Dense vectors are stored as an array of values. Sparse vectors are stored as two arrays; the first array stores the non-zero value indices and the second array stores the actual values. All element values are stored as doubles and indices are stored as integers starting from zero. Understanding the fundamental structures goes a long way in effective use of the libraries and it should help code up any new algorithm from scratch. Let us see some example code for a better understanding of these two vector representations:</p><p class="calibre11">
<strong class="calibre19">Scala</strong>
</p><pre class="programlisting">//Create vectors
scala&gt; import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.linalg.{Vector, Vectors}

//Create dense vector
scala&gt; val dense_v: Vector = Vectors.dense(10.0,0.0,20.0,30.0,0.0)
dense_v: org.apache.spark.ml.linalg.Vector = [10.0,0.0,20.0,30.0,0.0]
scala&gt;

//Create sparse vector: pass size, position index array and value array
scala&gt; val sparse_v1: Vector = Vectors.sparse(5,Array(0,2,3),
       Array(10.0,20.0,30.0))
sparse_v1: org.apache.spark.ml.linalg.Vector = (5,[0,2,3],[10.0,20.0,30.0])
scala&gt;

//Another way to create sparse vector with position, value tuples
scala&gt; val sparse_v2: Vector = Vectors.sparse(5,
        Seq((0,10.0),(2,20.0),(3,30.0)))
sparse_v2: org.apache.spark.ml.linalg.Vector = (5,[0,2,3],[10.0,20.0,30.0])
scala&gt;  
<strong class="calibre19">
Compare vectors 
</strong>
<strong class="calibre19">---------------
</strong>cala&gt; sparse_v1 == sparse_v2
res0: Boolean = true
scala&gt; sparse_v1 == dense_v
res1: Boolean = true      //All three objects are equal but...
scala&gt; dense_v.toString()
res2: String = [10.0,0.0,20.0,30.0,0.0]
scala&gt; sparse_v2.toString()
res3: String = (5,[0,2,3],[10.0,20.0,30.0]) //..internal representation
differs
scala&gt; sparse_v2.toArray
res4: Array[Double] = Array(10.0, 0.0, 20.0, 30.0, 0.0)

<strong class="calibre19">Interchangeable
</strong>---------------
scala&gt; dense_v.toSparse
res5: org.apache.spark.mllib.linalg.SparseVector = (5,[0,2,3]
[10.0,20.0,30.0])
scala&gt; sparse_v1.toDense
res6: org.apache.spark.mllib.linalg.DenseVector = [10.0,0.0,20.0,30.0,0.0]
scala&gt;

<strong class="calibre19">A common operation
</strong>------------------
scala&gt; Vectors.sqdist(sparse_v1,
        Vectors.dense(1.0,2.0,3.0,4.0,5.0))
res7: Double = 1075.0</pre><p class="calibre11">Python:</p><pre class="programlisting">//Create vectors
&gt;&gt;&gt; from pyspark.ml.linalg import Vector, Vectors
//Create vectors
&gt;&gt;&gt; dense_v = Vectors.dense(10.0,0.0,20.0,30.0,0.0)
//Pass size, position index array and value array
&gt;&gt;&gt; sparse_v1 = Vectors.sparse(5,[0,2,3],
                    [10.0,20.0,30.0])
&gt;&gt;&gt; 

//Another way to create sparse vector with position, value tuples
&gt;&gt;&gt; sparse_v2 = Vectors.sparse(5,
                  [[0,10.0],[2,20.0],[3,30.0]])
&gt;&gt;&gt; 

<strong class="calibre19">Compare vectors 
</strong>
<strong class="calibre19">---------------
</strong>&gt;&gt;&gt; sparse_v1 == sparse_v2
True
&gt;&gt;&gt; sparse_v1 == dense_v
True      //All three objects are equal but...
&gt;&gt;&gt; dense_v
DenseVector([10.0, 0.0, 20.0, 30.0, 0.0])
&gt;&gt;&gt; sparse_v1
SparseVector(5, {0: 10.0, 2: 20.0, 3: 30.0}) //..internal representation
differs
&gt;&gt;&gt; sparse_v2
SparseVector(5, {0: 10.0, 2: 20.0, 3: 30.0})
<strong class="calibre19"> 
</strong>
<strong class="calibre19">Interchangeable
</strong>
<strong class="calibre19">----------------
</strong>//Note: as of Spark 2.0.0, toDense and toSparse are not available in pyspark
<strong class="calibre19">
A common operation
</strong>
<strong class="calibre19">-------------------
</strong>&gt;&gt;&gt; Vectors.squared_distance(sparse_v1,
        Vectors.dense(1.0,2.0,3.0,4.0,5.0))
1075.0</pre><p class="calibre11">Matrices may be local or distributed, dense or sparse. A local matrix is stored on a single machine as a single dimensional array. A dense local matrix is stored in column major order (column members are contiguous) whereas a sparse matrix values are stored in <strong class="calibre19">Compressed Sparse Column</strong> (<strong class="calibre19">CSC</strong>) format in column major order. In this format, the matrix is stored in the form of three arrays. The first array contains row indices of non-zero values, the second array has the beginning value index for each column, and the third one is an array of all the non-zero values. Indices are of type integer starting from zero. The first array contains values from zero to the number of rows minus one. The third array has elements of type double. The second array requires some explanation. Every entry in this array corresponds to the index of the first non-zero element in each column. For example, assume that there is only one non-zero element in each column in a 3 by 3 matrix. Then the second array would contain 0,1,2 as its elements. The first array contains row positions and the third array contains three values. If none of the elements in a column are non-zero, you will note the same index repeating in the second array. Let us examine some example code:</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.linalg.{Matrix,Matrices}
import org.apache.spark.ml.linalg.{Matrix, Matrices}

<strong class="calibre19">Create dense matrix
</strong>
<strong class="calibre19">-------------------
</strong>//Values in column major order
Matrices.dense(3,2,Array(9.0,0,0,0,8.0,6))
res38: org.apache.spark.mllib.linalg.Matrix =
9.0  0.0
0.0  8.0
0.0  6.0
<strong class="calibre19">
Create sparse matrix
</strong>
<strong class="calibre19">--------------------
</strong>//1.0 0.0 4.0
0.0 3.0 5.0
2.0 0.0 6.0//
val sm: Matrix = Matrices.sparse(3,3,
        Array(0,2,3,6), Array(0,2,1,0,1,2),
        Array(1.0,2.0,3.0,4.0,5.0,6.0))
sm: org.apache.spark.mllib.linalg.Matrix =
3 x 3 CSCMatrix
(0,0) 1.0
(2,0) 2.0
(1,1) 3.0
(0,2) 4.0
(1,2) 5.0
(2,2) 6.0
<strong class="calibre19">
Sparse matrix, a column of all zeros
</strong>
<strong class="calibre19">------------------------------------
</strong>//third column all zeros
Matrices.sparse(3,4,Array(0,2,3,3,6),
    Array(0,2,1,0,1,2),values).toArray
res85: Array[Double] = Array(1.0, 0.0, 2.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0,
4.0, 5.0, 6.0)<strong class="calibre19"> </strong>
</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><pre class="programlisting">//Create dense matrix
&gt;&gt;&gt; from pyspark.ml.linalg import Matrix, Matrices

//Values in column major order
&gt;&gt;&gt; Matrices.dense(3,2,[9.0,0,0,0,8.0,6])
DenseMatrix(3, 2, [9.0, 0.0, 0.0, 0.0, 8.0, 6.0], False)
&gt;&gt;&gt; 

//Create sparse matrix
//1.0 0.0 4.0
0.0 3.0 5.0
2.0 0.0 6.0//
&gt;&gt;&gt; sm = Matrices.sparse(3,3,
        [0,2,3,6], [0,2,1,0,1,2],
        [1.0,2.0,3.0,4.0,5.0,6.0])
&gt;&gt;&gt; 

//Sparse matrix, a column of all zeros
//third column all zeros
&gt;&gt;&gt; Matrices.sparse(3,4,[0,2,3,3,6],
        [0,2,1,0,1,2],
    values=[1.0,2.0,3.0,4.0,5.0,6.0]).toArray()
array([[ 1.,  0.,  0.,  4.],
       [ 0.,  3.,  0.,  5.],
       [ 2.,  0.,  0.,  6.]])
&gt;&gt;&gt; </pre><p class="calibre11">Distributed matrices are the most sophisticated ones and choosing the right type of distributed matrix is very important. A distributed matrix is backed by one or more RDDs. The row and column indices are of the type <code class="literal">long</code> to support very large matrices. The basic type of distributed matrix is a <code class="literal">RowMatrix</code>, which is simply backed by an RDD of its rows.</p><p class="calibre11">Each row in turn is a local vector. This is suitable when the number of columns is very low. Remember, we need to pass RDDs to create distributed matrices, unlike the local ones. Let us look at an example:</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">scala&gt; import org.apache.spark.mllib.linalg.{Vector,Vectors}
import org.apache.spark.mllib.linalg.{Vector, Vectors}
scala&gt; import org.apache.spark.mllib.linalg.distributed.RowMatrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

scala&gt;val dense_vlist: Array[Vector] = Array(
    Vectors.dense(11.0,12,13,14),
    Vectors.dense(21.0,22,23,24),
    Vectors.dense(31.0,32,33,34))
dense_vlist: Array[org.apache.spark.mllib.linalg.Vector] =
Array([11.0,12.0,13.0,14.0], [21.0,22.0,23.0,24.0], [31.0,32.0,33.0,34.0])
scala&gt;

//Distribute the vector list
scala&gt; val rows  = sc.parallelize(dense_vlist)
rows: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] =
ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:29
scala&gt; val m: RowMatrix = new RowMatrix(rows)
m: org.apache.spark.mllib.linalg.distributed.RowMatrix =
org.apache.spark.mllib.linalg.distributed.RowMatrix@5c5043fe
scala&gt; print("Matrix size is " + m.numRows()+"X"+m.numCols())
Matrix size is 3X4
scala&gt;</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><pre class="programlisting">&gt;&gt;&gt; from pyspark.mllib.linalg import Vector,Vectors
&gt;&gt;&gt; from pyspark.mllib.linalg.distributed import RowMatrix

&gt;&gt;&gt; dense_vlist = [Vectors.dense(11.0,12,13,14),
         Vectors.dense(21.0,22,23,24), Vectors.dense(31.0,32,33,34)]
&gt;&gt;&gt; rows  = sc.parallelize(dense_vlist)
&gt;&gt;&gt; m = RowMatrix(rows)
&gt;&gt;&gt; "Matrix size is {0} X {1}".format(m.numRows(), m.numCols())
'Matrix size is 3 X 4'</pre><p class="calibre11">An <code class="literal">IndexedRowMatrix</code> stores a row index prefixed to the row entry. This is useful in executing joins. You need to pass <code class="literal">IndexedRow</code> objects to create an <code class="literal">IndexedRowMatrix</code>. An <code class="literal">IndexedRow</code> object is a wrapper with a long <code class="literal">Index</code> and a <code class="literal">Vector</code> of row elements.</p><p class="calibre11">A <code class="literal">CoordinatedMatrix</code> stores data as tuples of row, column indexes, and element value. A <code class="literal">BlockMatrix</code> represents a distributed matrix in blocks of local matrices. Methods to convert matrices from one type to another are provided but these are expensive operations and should be used with caution.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec66" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>ML pipeline</h2></div></div></div><p class="calibre11">A real life machine learning workflow is an iterative cycle of data extraction, data cleansing, pre-processing, exploration, feature extraction, model fitting, and evaluation. ML Pipeline on Spark is a simple API for users to set up complex ML workflows. It was designed to address some of the pain areas such as parameter tuning, or training many models based on different splits of data (cross-validation), or different sets of parameters. Writing scripts to automate this whole thing is no more a requirement and can be taken care of within the Pipeline API itself.</p><p class="calibre11">The Pipeline API consists of a series of pipeline stages (implemented as abstractions such as <em class="calibre22">transformers</em> and <em class="calibre22">estimators</em>) to get executed in a desired order.</p><p class="calibre11">In the ML Pipeline, you can invoke the data cleaning/transformation functions as discussed in the previous chapter and call the machine learning algorithms that are available in the MLlib. This can be done in an iterative fashion till you get the desired performance of your model.</p><p class="calibre11">
</p><div><img src="img/image_06_003.jpg" alt="ML pipeline" class="calibre98"/></div><p class="calibre11">
</p><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec51" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Transformer</h3></div></div></div><p class="calibre11">A transformer is an abstraction which implements the <code class="literal">transform()</code> method to convert one DataFrame into another. If the method is a feature transformer, the resulting DataFrame might contain some additional transformed columns based on the operation you performed. However, if the method is a learning model, then the resulting DataFrame would contain an extra column with predicted outcomes.</p></div><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec52" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Estimator</h3></div></div></div><p class="calibre11">An Estimator is an abstraction that can be any learning algorithm which implements the <code class="literal">fit()</code> method to get trained on a DataFrame to produce a model. Technically, this model is a transformer for the given DataFrame.</p><p class="calibre11">Example: Logistic regression is a learning algorithm, hence an estimator. Calling <code class="literal">fit()</code> trains a logistic regression model, which is a resultant model, and hence a transformer which can produce a DataFrame containing a predicted column.</p><p class="calibre11">The following example demonstrates a simple, single stage pipeline.</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">//Pipeline example with single stage to illustrate syntax
scala&gt; import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.Pipeline
scala&gt; import org.apache.spark.ml.feature._
import org.apache.spark.ml.feature._

//Create source data frame
scala&gt; val df = spark.createDataFrame(Seq(
         ("Oliver Twist","Charles Dickens"),
        ("Adventures of Tom Sawyer","Mark Twain"))).toDF(
        "Title","Author")

//Split the Title to tokens
scala&gt; val tok = new Tokenizer().setInputCol("Title").
          setOutputCol("words")
tok: org.apache.spark.ml.feature.Tokenizer = tok_2b2757a3aa5f

//Define a pipeline with a single stage
scala&gt; val p = new Pipeline().setStages(Array(tok))
p: org.apache.spark.ml.Pipeline = pipeline_f5e0de400666

//Run an Estimator (fit) using the pipeline
scala&gt; val model = p.fit(df)
model: org.apache.spark.ml.PipelineModel = pipeline_d00989625bb2

//Examine stages
scala&gt; p.getStages   //Returns a list of stage objects
res1: Array[org.apache.spark.ml.PipelineStage] = Array(tok_55af0061af6d)

// Examine the results
scala&gt; val m = model.transform(df).select("Title","words")
m: org.apache.spark.sql.DataFrame = [Title: string, words: array&lt;string&gt;]
scala&gt; m.select("words").collect().foreach(println)
[WrappedArray(oliver, twist)]
[WrappedArray(adventures, of, tom, sawyer)]</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><pre class="programlisting">//Pipeline example with single stage to illustrate syntax
//Create source data frame
&gt;&gt;&gt; from pyspark.ml.pipeline import Pipeline
&gt;&gt;&gt; from pyspark.ml.feature import Tokenizer
&gt;&gt;&gt;  df = sqlContext.createDataFrame([
    ("Oliver Twist","Charles Dickens"),
    ("Adventures of Tom Sawyer","Mark Twain")]).toDF("Title","Author")
&gt;&gt;&gt; 

//Split the Title to tokens
&gt;&gt;&gt; tok = Tokenizer(inputCol="Title",outputCol="words")

//Define a pipeline with a single stage
&gt;&gt;&gt; p = Pipeline(stages=[tok])

//Run an Estimator (fit) using the pipeline
&gt;&gt;&gt; model = p.fit(df)

//Examine stages
&gt;&gt;&gt; p.getStages()  //Returns a list of stage objects
[Tokenizer_4f35909c4c504637a263]

// Examine the results
&gt;&gt;&gt; m = model.transform(df).select("Title","words")
&gt;&gt;&gt; [x[0] for x in m.select("words").collect()]
[[u'oliver', u'twist'], [u'adventures', u'of', u'tom', u'sawyer']]
&gt;&gt;&gt; </pre><p class="calibre11">The above example showed pipeline creation and execution although with a single stage, a Tokenizer in this context. Spark provides several "feature transformers" out of the box. These feature transformers are quite handy during data cleaning and data preparation phases.</p><p class="calibre11">The following example shows a real world example of converting raw text into  feature vectors. If you are not familiar with TF-IDF, read this short tutorial from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.tfidf.com">http://www.tfidf.com</a>.</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.Pipeline
scala&gt; import org.apache.spark.ml.feature._
import org.apache.spark.ml.feature._
scala&gt; 

//Create a dataframe
scala&gt; val df2 = spark.createDataset(Array(
         (1,"Here is some text to illustrate pipeline"),
         (2, "and tfidf, which stands for term frequency inverse document
frequency"
         ))).toDF("LineNo","Text")

//Define feature transformations, which are the pipeline stages
// Tokenizer splits text into tokens
scala&gt; val tok = new Tokenizer().setInputCol("Text").
             setOutputCol("Words")
tok: org.apache.spark.ml.feature.Tokenizer = tok_399dbfe012f8

// HashingTF maps a sequence of words to their term frequencies using hashing
// Larger value of numFeatures reduces hashing collision possibility
scala&gt; val tf = new HashingTF().setInputCol("Words").setOutputCol("tf").setNumFeatures(100)
tf: org.apache.spark.ml.feature.HashingTF = hashingTF_e6ad936536ea
// IDF, Inverse Docuemnt Frequency is a statistical weight that reduces weightage of commonly occuring words
scala&gt; val idf = new IDF().setInputCol("tf").setOutputCol("tf_idf")
idf: org.apache.spark.ml.feature.IDF = idf_8af1fecad60a
// VectorAssembler merges multiple columns into a single vector column
scala&gt; val va = new VectorAssembler().setInputCols(Array("tf_idf")).setOutputCol("features")
va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_23205c3f92c8
//Define pipeline
scala&gt; val tfidf_pipeline = new Pipeline().setStages(Array(tok,tf,idf,va))
val tfidf_pipeline = new Pipeline().setStages(Array(tok,tf,idf,va))
scala&gt; tfidf_pipeline.getStages
res2: Array[org.apache.spark.ml.PipelineStage] = Array(tok_399dbfe012f8, hashingTF_e6ad936536ea, idf_8af1fecad60a, vecAssembler_23205c3f92c8)
scala&gt;

//Now execute the pipeline
scala&gt; val result = tfidf_pipeline.fit(df2).transform(df2).select("words","features").first()
result: org.apache.spark.sql.Row = [WrappedArray(here, is, some, text, to, illustrate, pipeline),(100,[0,3,35,37,69,81],[0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644])]</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><pre class="programlisting">//A realistic, multi-step pipeline that converts text to TF_ID
&gt;&gt;&gt; from pyspark.ml.pipeline import Pipeline
&gt;&gt;&gt; from pyspark.ml.feature import Tokenizer, HashingTF, IDF, VectorAssembler, \
               StringIndexer, VectorIndexer

//Create a dataframe
&gt;&gt;&gt; df2 = sqlContext.createDataFrame([
    [1,"Here is some text to illustrate pipeline"],
    [2,"and tfidf, which stands for term frequency inverse document
frequency"
    ]]).toDF("LineNo","Text")

//Define feature transformations, which are the pipeline stages
//Tokenizer splits text into tokens
&gt;&gt;&gt; tok = Tokenizer(inputCol="Text",outputCol="words")

// HashingTF maps a sequence of words to their term frequencies using
hashing

// Larger the numFeatures, lower the hashing collision possibility
&gt;&gt;&gt; tf = HashingTF(inputCol="words", outputCol="tf",numFeatures=1000)

// IDF, Inverse Docuemnt Frequency is a statistical weight that reduces
weightage of commonly occuring words
&gt;&gt;&gt; idf = IDF(inputCol = "tf",outputCol="tf_idf")

// VectorAssembler merges multiple columns into a single vector column
&gt;&gt;&gt; va = VectorAssembler(inputCols=["tf_idf"],outputCol="features")

//Define pipeline
&gt;&gt;&gt; tfidf_pipeline = Pipeline(stages=[tok,tf,idf,va])
&gt;&gt;&gt; tfidf_pipeline.getStages()
[Tokenizer_4f5fbfb6c2a9cf5725d6, HashingTF_4088a47d38e72b70464f, IDF_41ddb3891541821c6613, VectorAssembler_49ae83b800679ac2fa0e]
&gt;&gt;&gt;

//Now execute the pipeline
&gt;&gt;&gt; result = tfidf_pipeline.fit(df2).transform(df2).select("words","features").collect()
&gt;&gt;&gt; [(x[0],x[1]) for x in result]
[([u'here', u'is', u'some', u'text', u'to', u'illustrate', u'pipeline'], SparseVector(1000, {135: 0.4055, 169: 0.4055, 281: 0.4055, 388: 0.4055, 400: 0.4055, 603: 0.4055, 937: 0.4055})), ([u'and', u'tfidf,', u'which', u'stands', u'for', u'term', u'frequency', u'inverse', u'document', u'frequency'], SparseVector(1000, {36: 0.4055, 188: 0.4055, 333: 0.4055, 378: 0.4055, 538: 0.4055, 597: 0.4055, 727: 0.4055, 820: 0.4055, 960: 0.8109}))]
&gt;&gt;&gt; </pre><p class="calibre11">This example has created and executed a multi-stage pipeline that has converted text to a feature vector that can be processed by machine learning algorithms. Let us see a few more features before we move on.</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.feature._
import org.apache.spark.ml.feature._
scala&gt;

//Basic examples illustrating features usage
//Look at model examples for more feature examples
//Binarizer converts continuous value variable to two discrete values based on given threshold
scala&gt; import scala.util.Random
import scala.util.Random
scala&gt; val nums = Seq.fill(10)(Random.nextDouble*100)
...
scala&gt; val numdf = spark.createDataFrame(nums.map(Tuple1.apply)).toDF("raw_nums")
numdf: org.apache.spark.sql.DataFrame = [raw_nums: double]
scala&gt; val binarizer = new Binarizer().setInputCol("raw_nums").
            setOutputCol("binary_vals").setThreshold(50.0)
binarizer: org.apache.spark.ml.feature.Binarizer = binarizer_538e392f56db
scala&gt; binarizer.transform(numdf).select("raw_nums","binary_vals").show(2)
+------------------+-----------+
|          raw_nums|binary_vals|
+------------------+-----------+
|55.209245003482884|        1.0|
| 33.46202184060426|        0.0|
+------------------+-----------+
scala&gt;
 
//Bucketizer to convert continuous value variables to desired set of discrete values
scala&gt; val split_vals:Array[Double] = Array(0,20,50,80,100) //define intervals
split_vals: Array[Double] = Array(0.0, 20.0, 50.0, 80.0, 100.0)
scala&gt; val b = new Bucketizer().
           setInputCol("raw_nums").
           setOutputCol("binned_nums").
           setSplits(split_vals)
b: org.apache.spark.ml.feature.Bucketizer = bucketizer_a4dd599e5977
scala&gt; b.transform(numdf).select("raw_nums","binned_nums").show(2)
+------------------+-----------+
|          raw_nums|binned_nums|
+------------------+-----------+
|55.209245003482884|        2.0|
| 33.46202184060426|        1.0|
+------------------+-----------+
scala&gt;

//Bucketizer is effectively equal to binarizer if only two intervals are
given 
scala&gt; new Bucketizer().setInputCol("raw_nums").
        setOutputCol("binned_nums").setSplits(Array(0,50.0,100.0)).
        transform(numdf).select("raw_nums","binned_nums").show(2)
+------------------+-----------+
|          raw_nums|binned_nums|
+------------------+-----------+
|55.209245003482884|        1.0|
| 33.46202184060426|        0.0|
+------------------+-----------+
scala&gt;</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><pre class="programlisting">//Some more features
&gt;&gt;&gt; from pyspark.ml import feature, pipeline
&gt;&gt;&gt; 

//Basic examples illustrating features usage
//Look at model examples for more examples
//Binarizer converts continuous value variable to two discrete values based on given threshold
&gt;&gt;&gt; import random
&gt;&gt;&gt; nums = [random.random()*100 for x in range(1,11)]
&gt;&gt;&gt; numdf = sqlContext.createDataFrame(
             [[x] for x in nums]).toDF("raw_nums")
&gt;&gt;&gt; binarizer = feature.Binarizer(threshold= 50,
       inputCol="raw_nums", outputCol="binary_vals")
&gt;&gt;&gt; binarizer.transform(numdf).select("raw_nums","binary_vals").show(2)
+------------------+-----------+
|          raw_nums|binary_vals|
+------------------+-----------+
| 95.41304359504672|        1.0|
|41.906045589243405|        0.0|
+------------------+-----------+
&gt;&gt;&gt; 

//Bucketizer to convert continuous value variables to desired set of discrete values
&gt;&gt;&gt; split_vals = [0,20,50,80,100] //define intervals
&gt;&gt;&gt; b =
feature.Bucketizer(inputCol="raw_nums",outputCol="binned_nums",splits=split
vals)
&gt;&gt;&gt; b.transform(numdf).select("raw_nums","binned_nums").show(2)
+------------------+-----------+
|          raw_nums|binned_nums|
+------------------+-----------+
| 95.41304359504672|        3.0|
|41.906045589243405|        1.0|
+------------------+-----------+

//Bucketizer is effectively equal to binarizer if only two intervals are
given 
&gt;&gt;&gt; feature.Bucketizer(inputCol="raw_nums",outputCol="binned_nums",                  
                       splits=[0,50.0,100.0]).transform(numdf).select(
                       "raw_nums","binned_nums").show(2)
+------------------+-----------+
|          raw_nums|binned_nums|
+------------------+-----------+
| 95.41304359504672|        1.0|
|41.906045589243405|        0.0|
+------------------+-----------+
&gt;&gt;&gt; </pre></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec44" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Introduction to machine learning</h1></div></div></div><p class="calibre11">In the previous sections of the book, we learnt how the response/outcome variable is related to the predictor variables, typically in a supervised learning context. There are various different names for both of those types of variables that people use these days. Let us see some of the synonymous terms for them and we will use them interchangeably in the book:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Input variables (X)</strong>: Features, predictors, explanatory variables, independent variables</li><li class="listitem"><strong class="calibre19">Output variables (Y)</strong>: Response variable, dependent variable</li></ul></div><p class="calibre11">If there is a relation between <em class="calibre22">Y</em> and <em class="calibre22">X</em> where <em class="calibre22">X=X<sub class="calibre58">1</sub>, X<sub class="calibre58">2</sub>, X<sub class="calibre58">3</sub>,..., X<sub class="calibre58">n</sub></em> (n different predictors) then it can be written as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_004.jpg" alt="Introduction to machine learning" class="calibre99"/></div><p class="calibre11">
</p><p class="calibre11">Here <img src="img/image_06_005.jpg" alt="Introduction to machine learning" class="calibre100"/>is a function that represents how <em class="calibre22">X</em> describes <em class="calibre22">Y</em> and is unknown! This is what we figure out using the observed data points at hand. The term </p><div><img src="img/image_06_006.jpg" alt="Introduction to machine learning" class="calibre101"/></div><p class="calibre11"> is a random error term with mean zero and is independent of <em class="calibre22">X</em>.</p><p class="calibre11">There are basically two types of errors associated with such an equation - reducible errors and irreducible errors. As the name suggests, a reducible error is associated with the function and can be minimized by improving the accuracy of </p><div><img src="img/image_06_007.jpg" alt="Introduction to machine learning" class="calibre102"/></div><p class="calibre11"> by using a better learning algorithm or by tuning the same algorithm. Since <em class="calibre22">Y</em> is also a function of</p><div><img src="img/image_06_008.jpg" alt="Introduction to machine learning" class="calibre103"/></div><p class="calibre11">, which is independent of <em class="calibre22">X</em>, there would still be some error associated that cannot be addressed. This is called an irreducible error (</p><div><img src="img/image_06_009.jpg" alt="Introduction to machine learning" class="calibre101"/></div><p class="calibre11">). There are always some factors which influence the outcome variable but are not considered in building the model (as they are unknown most of the time), and contribute to the irreducible error term. So, our approaches discussed throughout this book will only be focused on minimizing the reducible error.</p><p class="calibre11">Most of the machine learning models that we build can be used for either prediction or for inference, or a combination of both. For some of the algorithms, the function </p><div><img src="img/image_06_010.jpg" alt="Introduction to machine learning" class="calibre102"/></div><p class="calibre11"> can be represented as an equation which tells us how the dependent variable <em class="calibre22">Y</em> is related to the independent variables (<em class="calibre22">X1</em>, <em class="calibre22">X2</em>,..., <em class="calibre22">Xn</em>). In such cases, we can do both inference and prediction. However, some of the algorithms are black box, where we can only predict and no inference is possible, because how <em class="calibre22">Y</em> is related to <em class="calibre22">X</em> is unknown.</p><p class="calibre11">Note that the linear machine learning models can be more apt for an inference setting because they are more interpretable to business users. However, on a prediction setting, there can be better algorithms providing more accurate predictions but they are less interpretable. When inference is the target, we should prefer the restrictive models such as linear regression for better interpretability, and when only prediction is the goal, we may choose to use highly flexible models such as <strong class="calibre19">Support Vector Machines</strong> (<strong class="calibre19">SVM</strong>) that are less interpretable and more accurate (this may not hold true in all cases, however). You need to be careful in choosing an algorithm based on the business requirement, by accounting for the trade-off between interpretability and accuracy. Let us dive deeper into understanding the fundamentals behind these concepts.</p><p class="calibre11">Basically, we need a set of data points (training data) to build a model to estimate </p><div><img src="img/image_06_011.jpg" alt="Introduction to machine learning" class="calibre104"/></div><p class="calibre11">
<em class="calibre22">(X)</em> so that <em class="calibre22">Y = </em>
</p><div><img src="img/image_06_012.jpg" alt="Introduction to machine learning" class="calibre104"/></div><p class="calibre11">
<em class="calibre22">(X)</em>. Broadly, such learning methods can be either parametric or non-parametric.</p><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec67" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Parametric methods</h2></div></div></div><p class="calibre11">Parametric methods follow a two-step process. In the first step, you assume the shape of </p><div><img src="img/image_06_013.jpg" alt="Parametric methods" class="calibre104"/></div><p class="calibre11">
<em class="calibre22">()</em>. For example, <em class="calibre22">X</em> is linearly related to <em class="calibre22">Y</em>, so the function of <em class="calibre22">X,</em> which is </p><div><img src="img/image_06_014.jpg" alt="Parametric methods" class="calibre104"/></div><p class="calibre11">
<em class="calibre22">(X),</em> can be represented with a linear equation as shown next:</p><p class="calibre11">
</p><div><img src="img/Beta1.jpg" alt="Parametric methods" class="calibre105"/></div><p class="calibre11">
</p><p class="calibre11">After the model is selected, the second step is to estimate the parameters <em class="calibre22">Î²0</em>, <em class="calibre22">Î²1</em>,..., <em class="calibre22">Î²n</em> by using the data points at hand to train the model, so that:</p><p class="calibre11">
</p><div><img src="img/Beta-2.jpg" alt="Parametric methods" class="calibre106"/></div><p class="calibre11">
</p><p class="calibre11">The one disadvantage to this parametric approach is that our assumption of linearity for <img src="img/image_06_016.jpg" alt="Parametric methods" class="calibre107"/>
<em class="calibre22">()</em> might not hold true in real life situations.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec68" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Non-parametric methods</h2></div></div></div><p class="calibre11">We do not make any assumptions about the linear relation between <em class="calibre22">Y</em> and <em class="calibre22">X</em> as well as data distributions of variables, and hence the form of </p><div><img src="img/image_06_017.jpg" alt="Non-parametric methods" class="calibre104"/></div><p class="calibre11">
<em class="calibre22">()</em> in non-parametric. Since it does not assume any form of </p><div><img src="img/image_06_018.jpg" alt="Non-parametric methods" class="calibre104"/></div><p class="calibre11">
<em class="calibre22">()</em>, it can produce better results by fitting well with data points, which could be an advantage.</p><p class="calibre11">So, the non-parametric methods require more data points compared to parametric methods to estimate </p><div><img src="img/image_06_019.jpg" alt="Non-parametric methods" class="calibre104"/></div><p class="calibre11">
<em class="calibre22">()</em> accurately. Note however, it can lead to overfitting problems if not handled properly. We will discuss more on this as we move further.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec45" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Regression methods</h1></div></div></div><p class="calibre11">Regression methods are a type of supervised learning. If the response variable is quantitative/continuous (takes on numeric values such as age, salary, height, and so on), then the problem can be called a regression problem regardless of the explanatory variables' type. There are various kinds of modeling techniques to address the regression problems. In this section, our focus will be on linear regression techniques and some different variations of it.</p><p class="calibre11">Regression methods can be used to predict any real valued outcomes. Following are a few examples:</p><div><ul class="itemizedlist"><li class="listitem">Predict the salary of an employee based on his educational level, location, type of job, and so on</li><li class="listitem">Predict stock prices</li><li class="listitem">Predict buying potential of a customer</li><li class="listitem">Predict the time a machine would take before failing</li></ul></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec69" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Linear regression</h2></div></div></div><p class="calibre11">Further to what we discussed in the previous section <em class="calibre22">Parametric methods</em>, after the assumption of linearity is made for </p><div><img src="img/image_06_020.jpg" alt="Linear regression" class="calibre104"/></div><p class="calibre11">
<em class="calibre22">(X)</em>, we need the training data to fit a model that would describe the relation between explanatory variables (denoted as <em class="calibre22">X</em>) and the response variable (denoted as <em class="calibre22">Y</em>). When there is only one explanatory variable present, it is called simple linear regression and when there are multiple explanatory variables present, it is called multiple linear regression. The simple linear regression is all about fitting a straight line in a 2-D setting, and when there are say two predictor variables, it would fit a plane in a 3-D setting, and so on for higher dimensional settings when there are more than two variables.</p><p class="calibre11">The usual form of a linear regression equation can be represented as:</p><p class="calibre11">Y' = </p><div><img src="img/image_06_021.jpg" alt="Linear regression" class="calibre104"/></div><p class="calibre11">(X) + </p><div><img src="img/image_06_022.jpg" alt="Linear regression" class="calibre101"/></div><p class="calibre11">
</p><p class="calibre11">Here <em class="calibre22">Y'</em> represents the predicted outcome variable.</p><p class="calibre11">A linear regression equation with only one predictor variable can be given as:</p><p class="calibre11">
</p><div><img src="img/Beta11.jpg" alt="Linear regression" class="calibre108"/></div><p class="calibre11">
</p><p class="calibre11">A linear regression equation with multiple predictor variables can be given as:</p><p class="calibre11">
</p><div><img src="img/Beta22.jpg" alt="Linear regression" class="calibre109"/></div><p class="calibre11">
</p><p class="calibre11">Here <img src="img/image_06_025.jpg" alt="Linear regression" class="calibre110"/> is the irreducible error term independent of <em class="calibre22">X</em> and has a mean of zero. We do not have any control over it, but we can work towards optimizing </p><div><img src="img/image_06_026.jpg" alt="Linear regression" class="calibre104"/></div><p class="calibre11">
<em class="calibre22">(X)</em>. Since none of the models can achieve a 100 percent accuracy, there would always be some error associated with it because of the irreducible error component (</p><div><img src="img/image_06_027.jpg" alt="Linear regression" class="calibre101"/></div><p class="calibre11">).</p><p class="calibre11">The most common approach of fitting a linear regression is called <strong class="calibre19">least squares</strong>, also known as, the <strong class="calibre19">Ordinary Least Squares</strong> (<strong class="calibre19">OLS</strong>) approach. This method finds the regression line that best fits the observed data points by minimizing the sum of squares of the vertical deviations from each data point to the regression line. To get a better understanding on how the linear regression works, let us look at a simple linear regression of the following form for now:</p><p class="calibre11">
</p><div><img src="img/Beta33.jpg" alt="Linear regression" class="calibre111"/></div><p class="calibre11">
</p><p class="calibre11">Where, <em class="calibre22">Î²0</em> is the Y-intercept of the regression line and <em class="calibre22">Î²1</em> defines the slope of the line. What it means is that <em class="calibre22">Î²1</em> is the average change in <em class="calibre22">Y</em> for every one unit change in <em class="calibre22">X</em>. Let us take an example with <em class="calibre22">X</em> and <em class="calibre22">Y</em>:</p><div><table border="1" class="calibre12"><colgroup class="calibre13"><col class="calibre14"/><col class="calibre14"/></colgroup><tbody class="calibre15"><tr class="calibre16"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">X</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">Y</strong>
</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">1</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">12</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">2</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">20</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">3</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">13</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">4</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">38</p>
</td></tr><tr class="calibre21"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">5</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">27</p>
</td></tr></tbody></table></div><p class="calibre11">If we fit a linear regression line through the data points as shown in the preceding table, then it would appear as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_028.jpg" alt="Linear regression" class="calibre112"/></div><p class="calibre11">
</p><p class="calibre11">The red vertical lines in the preceding figure indicate the error of prediction which can be defined as the difference between the actual <em class="calibre22">Y</em> value and the predicted <em class="calibre22">Y'</em> value. If you square these differences and sum them up, it is called the <strong class="calibre19">Sum of Squared Error</strong> (<strong class="calibre19">SSE</strong>), which is the most common measure that is used to find the best fitting line. The following table shows how to calculate the SSE:</p><div><table border="1" class="calibre12"><colgroup class="calibre13"><col class="calibre14"/><col class="calibre14"/><col class="calibre14"/><col class="calibre14"/><col class="calibre14"/></colgroup><tbody class="calibre15"><tr class="calibre16"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">X</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">Y</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">Y'</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">Y-Y'</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">(Y-Y') 2</strong>
</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">1</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">12</p>
</td><td class="calibre17">
<p class="calibre18">12.4</p>
</td><td class="calibre17">
<p class="calibre18">0.4</p>
</td><td class="calibre17">
<p class="calibre18">0.16</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">2</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">20</p>
</td><td class="calibre17">
<p class="calibre18">17.2</p>
</td><td class="calibre17">
<p class="calibre18">2.8</p>
</td><td class="calibre17">
<p class="calibre18">7.84</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">3</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">13</p>
</td><td class="calibre17">
<p class="calibre18">22</p>
</td><td class="calibre17">
<p class="calibre18">-9</p>
</td><td class="calibre17">
<p class="calibre18">81</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">4</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">38</p>
</td><td class="calibre17">
<p class="calibre18">26.8</p>
</td><td class="calibre17">
<p class="calibre18">11.2</p>
</td><td class="calibre17">
<p class="calibre18">125.44</p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">
<strong class="calibre19">5</strong>
</p>
</td><td class="calibre17">
<p class="calibre18">27</p>
</td><td class="calibre17">
<p class="calibre18">31.6</p>
</td><td class="calibre17">
<p class="calibre18">-4.6</p>
</td><td class="calibre17">
<p class="calibre18">21.16</p>
</td></tr><tr class="calibre113"><td class="calibre17">
</td><td class="calibre17">
</td><td class="calibre17">
</td><td class="calibre17">
<p class="calibre18">SUM</p>
</td><td class="calibre17">
<p class="calibre18">235.6</p>
</td></tr></tbody></table></div><p class="calibre11">In the above table, the term <strong class="calibre19">(Y-Y')</strong> is called the residual. The <strong class="calibre19">Residual Sum of Squares</strong> (<strong class="calibre19">RSS</strong>) can be represented as:</p><p class="calibre11">
<em class="calibre22">RSS = residual<sub class="calibre58">1</sub><sup class="calibre57">2 </sup>+ residual<sub class="calibre58">2</sub><sup class="calibre57">2 </sup>+ residual<sub class="calibre58">3</sub><sup class="calibre57">2</sup> + ......+ residual<sub class="calibre58">n</sub><sup class="calibre57">2</sup></em>
</p><p class="calibre11">Note that regression is highly susceptible to outliers and can introduce huge RSS error if not handled prior to applying regression.</p><p class="calibre11">After a regression line is fit into the observed data points, you should examine the residuals by plotting them on the Y-Axis against explanatory the variable on the X-Axis. If the plot is nearly a straight line, then your assumption about linear relationship is valid, or else it may indicate the presence of some kind of non-linear relationship. In case of the presence of nonlinear relationships, you may have to account for the non-linearity. One of the techniques is by adding higher order polynomials to the equation.</p><p class="calibre11">We saw that RSS was an important characteristic in fitting the regression line (while building the model). Now, to assess how good your regression fit is (once the model is built), you need two other statistics - <strong class="calibre19">Residual Standard Error</strong> (<strong class="calibre19">RSE</strong>) and <strong class="calibre19">R<sup class="calibre57">2</sup></strong> statistic.</p><p class="calibre11">We discussed the irreducible error component <em class="calibre22">Îµ</em>, because of which there would always be some level of error with your regression (even if your equation exactly fits your data points and you have estimated the coefficients properly). RSE is an estimate of standard deviation of <em class="calibre22">Îµ</em> which can be defined as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_029.jpg" alt="Linear regression" class="calibre114"/></div><p class="calibre11">
</p><p class="calibre11">This means that the actual values would deviate from the true regression line by a factor of RSE on an average.</p><p class="calibre11">Since RSE is actually measured in the units of <em class="calibre22">Y</em> (refer to how we calculated RSS in the previous section), it is difficult to say that it is the only best statistic for the model accuracy.</p><p class="calibre11">So, an alternative approach was introduced, called the R<sup class="calibre57">2</sup> statistic (also known as the coefficient of determination). The formula to calculate R<sup class="calibre57">2</sup> is as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_030.jpg" alt="Linear regression" class="calibre115"/></div><p class="calibre11">
</p><p class="calibre11">The <strong class="calibre19">Total Sum of Squares</strong> (<strong class="calibre19">TSS</strong>) can be calculated as:</p><p class="calibre11">
</p><div><img src="img/image_06_031.jpg" alt="Linear regression" class="calibre116"/></div><p class="calibre11">
</p><p class="calibre11">Note here that TSS measures the total variance inherent in <em class="calibre22">Y</em> even before performing the regression to predict <em class="calibre22">Y</em>. Observe that there is no <em class="calibre22">Y'</em> in it. On the contrary, RSS represents the variability in <em class="calibre22">Y</em> that is unexplained after regression. This means that (<em class="calibre22">TSS - RSS</em>) is able to explain the variability in response after regression is performed.</p><p class="calibre11">The <em class="calibre22">R<sup class="calibre57">2</sup></em> statistic usually ranges from 0 to 1, but can be negative if the fit is worse than fitting just a horizontal line, but that is rarely the case. A value close to 1 indicates that the regression equation could explain a large proportion of the variability in the response variable and is a good fit. On the contrary, a value close to 0 indicates that the regression did not explain much of the variance in the response variable and is not a good fit. As an example, an <em class="calibre22">R<sup class="calibre57">2</sup></em> of 0.25 means that 25 percent of the variance in <em class="calibre22">Y</em> is explained by <em class="calibre22">X</em> and is indicating to tune the model for improvement.</p><p class="calibre11">Let us now discuss how to address the non-linearity in the dataset through regression. As discussed earlier, when you find nonlinear relations, it needs to be handled properly. To model a non-linear equation using the same linear regression technique, you have to create the higher order features, which will be treated as just another variable by the regression technique. For example, if <em class="calibre22">salary</em> is a feature/variable that is predicting the <em class="calibre22">buying potential</em>, and we find that there is a non-linear relationship between them, then we might create a feature called (<em class="calibre22">salary3</em>) depending on how much of the non-linearity needs to be addressed. Note that while you create such higher order features, you also have to keep the base features. In this example, you have to use both (<em class="calibre22">salary</em>) and (<em class="calibre22">salary3</em>) in the regression equation.</p><p class="calibre11">So far, we have kind of assumed that all the predictor variables are continuous. What if there are categorical predictors? In such cases, we have to dummy-code those variables (say 1 for male and 0 for female) so that the regression technique generates two equations, one for gender = male (the equation will have the gender variable) and the other for gender = female (the equation will not have the gender variable as it will be dropped as coded 0). At times, with very few categorical variables, it may be a good idea to divide the dataset based on the levels of categorical variables and build separate models for them.</p><p class="calibre11">One major advantage of the least squares linear regression is that it explains how the outcome variable is related to the predictor variables. This makes it very interpretable and can be used to draw inferences as well as to do predictions.</p><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec53" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Loss function</h3></div></div></div><p class="calibre11">Many machine learning problems can be formulated as a convex optimization problem. The objective of this problem is to find the values of the coefficients for which the squared loss is minimum. This objective function has basically two components - regularizer and the loss function. The regularizer is there to control the complexity of the model (so it does not overfit) and the loss function is there to estimate the coefficients of the regression function for which squared loss (RSS) is minimum.</p><p class="calibre11">The loss function used for least squares is called <strong class="calibre19">squared loss</strong>, as shown next:</p><p class="calibre11">
</p><div><img src="img/image_06_032.jpg" alt="Loss function" class="calibre117"/></div><p class="calibre11">
</p><p class="calibre11">Here <em class="calibre22">Y</em> is the response variable (real valued), <em class="calibre22">W</em> is the weight vector (value of the coefficients), and <em class="calibre22">X</em> is the feature vector. So </p><div><img src="img/Capture-1.jpg" alt="Loss function" class="calibre118"/></div><p class="calibre11"> gives the predicted values which we equate with the actual values <em class="calibre22">Y</em> to find the squared loss that needs to be minimized.</p><p class="calibre11">The algorithm used to estimate the coefficients is called <strong class="calibre19">gradient descent</strong>. There are different types of loss functions and optimization algorithms for different kinds of machine learning algorithms which we will cover as and when needed.</p></div><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec54" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Optimization</h3></div></div></div><p class="calibre11">Ultimately, the linear methods have to optimize the loss function. Under the hood, linear methods use convex optimization methods to optimize the objective functions. MLlib has <strong class="calibre19">Stochastic Gradient Descent</strong> (<strong class="calibre19">SGD</strong>) and <strong class="calibre19">Limited Memory - Broyden-Fletcher-Goldfarb-Shanno</strong> (<strong class="calibre19">L-BFGS</strong>) supported out of the box. Currently, most algorithm APIs support SGD and a few support L-BFGS.</p><p class="calibre11">SGD is a first-order optimization technique that works best for large scale data and distributed computing environment. Optimization problems whose objective function (loss function) is written as a sum are best suited to be solved using SGD.</p><p class="calibre11">L-BFGS is an optimization algorithm in the family of quasi-Newton methods to solve the optimization problems. L-BFGS often achieves a rapider convergence compared with other first-order optimization techniques such as SGD.</p><p class="calibre11">Some of the linear methods available in MLlib support both SGD and L-BFGS. You should choose one over the other depending on the objective function under consideration. In general, L-BFGS is recommended over SGD as it converges faster but you need to evaluate carefully based on the requirement.</p></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec70" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Regularizations on regression</h2></div></div></div><p class="calibre11">With large weights (coefficient values), it is easier to overfit the model. Regularization is a technique used mainly to eliminate the overfitting problem by controlling the complexity of the model. This is usually done when you see a difference between the model performance on training data and test data. If the training performance is more than that of the test data, it could be a case of overfitting (high variance case).</p><p class="calibre11">To address this, a regularization technique was introduced that would penalize the loss function. It is always recommended to use any of the regularizations techniques, especially when the training data has a small number of observations.</p><p class="calibre11">Before we discuss further on the regularization techniques, we have to understand what <em class="calibre22">bias</em> and <em class="calibre22">variance</em> mean in a supervised learning setting and why there is always a trade-off associated. While both are related to errors, a <em class="calibre22">biased</em> model means that it is biased towards some erroneous assumption and may miss the relation between the predictor variables and the response variable to some extent. This is a case of underfitting! On the other hand, a <em class="calibre22">high variance</em> model means that it tries to touch every data point and ends up modelling the random noise present in the dataset. It represents the case of overfitting.</p><p class="calibre11">Linear regression with the L2 penalty (L2 regularization) is called <strong class="calibre19">ridge regression</strong> and with the L1 penalty (L1 regularization) is called <strong class="calibre19">lasso regression</strong>. When both L1 and L2 penalties are used together, it is called <strong class="calibre19">elastic net regression</strong>. We will discuss them one by one in the following section.</p><p class="calibre11">L2 regularized problems are usually easy to solve compared to L1 regularized problems due to smoothness, but the L1 regularized problems can cause sparsity in weights leading to smaller and more interpretable models. Because of this, lasso is at times used for feature selection.</p><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec55" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Ridge regression</h3></div></div></div><p class="calibre11">When we add the L2 penalty (also known as the <strong class="calibre19">shrinkage penalty</strong>) to the loss function of least squares, it becomes the ridge regression, as shown next:</p><p class="calibre11">
</p><div><img src="img/image_06_034.jpg" alt="Ridge regression" class="calibre119"/></div><p class="calibre11">
</p><p class="calibre11">Here <em class="calibre22">λ</em> (greater than 0) is a tuning parameter which is determined separately. The second term in the preceding equation is called the shrinkage penalty and can be small only if the coefficients (<em class="calibre22">Î²0</em>, <em class="calibre22">Î²1</em>...and so on) are small and close to 0. When <em class="calibre22">λ = 0</em>, the ridge regression becomes least squares. As lambda approaches infinity, the regression coefficients approach zero (but are never zero).</p><p class="calibre11">The ridge regression generates different sets of coefficient values for each value of <em class="calibre22">λ</em>. So, the lambda value needs to be carefully selected using cross-validation. As we increase the lambda value, the flexibility of the regression line decreases, thereby decreasing variance and increasing bias.</p><p class="calibre11">Note that the shrinkage penalty is applied to all the explanatory variables except the intercept term <em class="calibre22">Î²0</em>.</p><p class="calibre11">The ridge regression works really well when the training data is less or even in the case where the number of predictors or features are more than the number of observations. Also, the computation needed for ridge is almost the same as that of least squares.</p><p class="calibre11">Since ridge does not reduce any coefficient value to zero, all the variables will be present in the model which can make it less interpretable if the number of variables is high.</p></div><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec56" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Lasso regression</h3></div></div></div><p class="calibre11">Lasso was introduced after ridge. When we add the L1 penalty to the loss function of least squares, it becomes lasso regression, as shown next:</p><p class="calibre11">
</p><div><img src="img/image_06_035.jpg" alt="Lasso regression" class="calibre120"/></div><p class="calibre11">
</p><p class="calibre11">The difference here is that instead of taking the squared coefficients, it takes the mod of the coefficient. Unlike ridge, it can force some of its coefficients to be exactly zero which can result in elimination of some of the variables. So, lasso can be used for variable selection as well!</p><p class="calibre11">Lasso generates different sets of coefficient values for each value of lambda. So lambda value needs to be carefully selected using cross-validation. Like ridge, as you increase lambda, variance decreases and bias increases.</p><p class="calibre11">Lasso produces better interpretable models compared to ridge because it usually has a subset of the total number of variables. When there are many categorical variables, it is advisable to choose lasso over ridge.</p><p class="calibre11">In reality, neither ridge nor lasso is always better over the other. Lasso usually performs well with a small number of predictor variables that have substantial coefficients and the rest have very small coefficients. Ridge usually performs better when there are many predictors and almost all have substantial yet similar coefficient sizes.</p><p class="calibre11">Ridge is good for grouped selection and can also address multicollinearity problems. Lasso, on the other hand, cannot do grouped selection and tends to pick only one of the predictors. Also, if a group of predictors are highly correlated amongst themselves, Lasso tends to pick only one of them and shrink the others to zero.</p></div><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec57" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Elastic net regression</h3></div></div></div><p class="calibre11">When we add both L1 and L2 penalties to the loss function of least squares, it becomes elastic net regression, as shown next:</p><p class="calibre11">
</p><div><img src="img/image_06_036.jpg" alt="Elastic net regression" class="calibre121"/></div><p class="calibre11">
</p><p class="calibre11">Following are the advantages of elastic net regression:</p><div><ul class="itemizedlist"><li class="listitem">Enforces sparsity and helps remove least effective variables</li><li class="listitem">Encourages grouping effect</li><li class="listitem">Combines the strengths of both ridge and lasso</li></ul></div><p class="calibre11">The Naive version of elastic net regression incurs a double shrinkage problem which leads to increased bias and poorer prediction accuracy. To address this, one approach could be rescaling the estimated coefficients by multiplying (<em class="calibre22">1+ λ2</em>) with them:</p><p class="calibre11">
<strong class="calibre19">Scala</strong>
</p><pre class="programlisting">import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.LinearRegressionModel
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
scala&gt; import org.apache.spark.ml.regression.{LinearRegression,LinearRegressionModel}
import org.apache.spark.ml.regression.{LinearRegression,LinearRegressionModel}
// Load the data
scala&gt; val data = spark.read.format("libsvm").load("data/mllib/sample_linear_regression_data.txt")
data: org.apache.spark.sql.DataFrame = [label: double, features: vector]

// Build the model
scala&gt; val lrModel = new LinearRegression().fit(data)

//Note: You can change ElasticNetParam, MaxIter and RegParam
// Defaults are 0.0, 100 and 0.0
lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_aa788bcebc42

//Check Root Mean Squared Error
scala&gt; println("Root Mean Squared Error = " + lrModel.summary.rootMeanSquaredError)
Root Mean Squared Error = 10.16309157133015</pre><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">&gt;&gt;&gt; from pyspark.ml.regression import LinearRegression, LinearRegressionModel
&gt;&gt;&gt;

// Load the data
&gt;&gt;&gt; data = spark.read.format("libsvm").load("data/mllib/sample_linear_regression_data.txt")
&gt;&gt;&gt; 

// Build the model
&gt;&gt;&gt; lrModel = LinearRegression().fit(data)

//Note: You can change ElasticNetParam, MaxIter and RegParam
// Defaults are 0.0, 100 and 0.0
//Check Root Mean Squared Error
&gt;&gt;&gt; print "Root Mean Squared Error = ", lrModel.summary.rootMeanSquaredError
Root Mean Squared Error = 10.16309157133015
&gt;&gt;&gt; </pre></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec46" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Classification methods</h1></div></div></div><p class="calibre11">If the response variable is qualitative/categorical (takes on categorical values such as gender, loan default, marital status, and such), then the problem can be called a classification problem regardless of the explanatory variables' type. There are various types of classification methods, but we will focus on logistic regression and Support Vector Machines in this section.</p><p class="calibre11">Following are a few examples of some implications of classification methods:</p><div><ul class="itemizedlist"><li class="listitem">A customer buys a product or does not buy it</li><li class="listitem">A person is diabetic or not diabetic</li></ul></div><div><ul class="itemizedlist"><li class="listitem">An individual applying for a loan would default or not</li><li class="listitem">An e-mail receiver would read the e-mail or not</li></ul></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec71" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Logistic regression</h2></div></div></div><p class="calibre11">Logistic regression measures the relation between the explanatory variables and the categorical response variable. We do not use linear regression for the categorical response variable because the response variable is not on a continuous scale and hence the error terms are not normally distributed.</p><p class="calibre11">So logistic regression is a classification algorithm. Instead of modelling the response variable <em class="calibre22">Y</em> directly, logistic regression models the probability distribution of <em class="calibre22">P(Y</em>|<em class="calibre22">X)</em> that <em class="calibre22">Y</em> belongs to a particular category. The conditional distribution of (<em class="calibre22">Y</em>|<em class="calibre22">X</em>) is a Bernoulli distribution rather than a Gaussian distribution. The logistic regression equation can be represented as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_037.jpg" alt="Logistic regression" class="calibre122"/></div><p class="calibre11">
</p><p class="calibre11">For a two class classification, the output of the model should be restricted to only one of the two classes (say either 0 or 1). Since logistic regression predicts probabilities and not classes directly, we use a logistic function (also known as the, <em class="calibre22">sigmoid function</em>) to restrict the output to a single class:</p><p class="calibre11">
</p><div><img src="img/image_06_038.jpg" alt="Logistic regression" class="calibre123"/></div><p class="calibre11">
</p><p class="calibre11">Solving for the preceding equation gives us the following:</p><p class="calibre11">
</p><div><img src="img/Capture-2.jpg" alt="Logistic regression" class="calibre124"/></div><p class="calibre11">
</p><p class="calibre11">It can be further simplified as:</p><p class="calibre11">
</p><div><img src="img/image_06_040.jpg" alt="Logistic regression" class="calibre125"/></div><p class="calibre11">
</p><p class="calibre11">The quantity on the left <em class="calibre22">P(X)/1-P(X)</em> is called the <em class="calibre22">odds</em>. The value of odds ranges from 0 to infinity. The values close to 0 indicate very less probability and the ones bigger in numbers indicate high probability. At times odds are used directly instead of probabilities, depending on the situation.</p><p class="calibre11">If we take the log of the odds, it becomes log-odd or logit and can be shown as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_041.jpg" alt="Logistic regression" class="calibre126"/></div><p class="calibre11">
</p><p class="calibre11">You can see from the previous equation that logit is linearly related to <em class="calibre22">X</em>.</p><p class="calibre11">In the situation where there are two classes, 1 and 0, then we predict <em class="calibre22">Y = 1</em> if <em class="calibre22">p &gt;= 0.5</em> and <em class="calibre22">Y = 0</em> when <em class="calibre22">p &lt; 0.5</em>. So logistic regression is actually a linear classifier with decision boundary at <em class="calibre22">p = 0.5</em>. There could be business cases where <em class="calibre22">p</em> is just not set to 0.5 by default and you may have to figure out the right value using some mathematical techniques.</p><p class="calibre11">A method known as maximum likelihood is used to fit the model by computing the regression coefficients, and the algorithm can be a gradient descent like in a linear regression setting.</p><p class="calibre11">In logistic regression, the loss function should address the misclassification rate. So, the loss function used for logistic regression is called <em class="calibre22">logistic loss</em>, as shown next:</p><p class="calibre11">
</p><div><img src="img/image_06_042.jpg" alt="Logistic regression" class="calibre127"/></div><p class="calibre11">
</p><div><div><h3 class="title5"><a id="note11" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Note</h3><p class="calibre25">Note that logistic regression is also prone to overfitting when you use higher order polynomial to better fit a model. To solve this, you can use regularization terms like you did in linear regression. As of this writing, Spark does not support regularized logistic regression so we will skip this part for now.</p></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec47" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Linear Support Vector Machines (SVM)</h1></div></div></div><p class="calibre11">
<strong class="calibre19">Support Vector Machines</strong> (<strong class="calibre19">SVM</strong>) is a type of supervised machine learning algorithm and can be used for both classification and regression. However, it is more popular in addressing the classification problems, and since Spark offers it as an SVM classifier, we will limit our discussion to the classification setting only. When used as a classifier, unlike logistic regression, it is a non-probabilistic classifier.</p><p class="calibre11">The SVM has evolved from a simple classifier called the <strong class="calibre19">maximal margin classifier</strong>. Since the maximal margin classifier required that the classes be separable by a linear boundary, it could not be applied to many datasets. So it was extended to an improved version called the <strong class="calibre19">support vector classifier</strong> that could address the cases where the classes overlapped and there were no clear separation between the classes. The support vector classifier was further extended to what we call an SVM to accommodate the non-linear class boundaries. Let us discuss the evolution of the SVM step by step so we get a clear understanding of how it works.</p><p class="calibre11">If there are <em class="calibre22">p</em> dimensions (features) in a dataset, then we fit a hyperplane in that p-dimensional space whose equation can be defined as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_043.jpg" alt="Linear Support Vector Machines (SVM)" class="calibre128"/></div><p class="calibre11">
</p><p class="calibre11">This hyperplane is called the separating hyperplane that forms the decision boundary. The result will be classified based on the result; if greater than 0, then on one side and if less than 0, then on the other side, as shown in the following figure:</p><p class="calibre11">
</p><div><img src="img/image_06_044.jpg" alt="Linear Support Vector Machines (SVM)" class="calibre129"/></div><p class="calibre11">
</p><p class="calibre11">Observe in the preceding figure that there can be multiple hyperplanes (they can be infinite). There should be a reasonable way to choose the best hyperplane. This is where we select the maximal margin hyperplane. If you compute the perpendicular distance of all data points to the separating hyperplane, then the smallest distance would be called as the margin. So, for the maximal margin classifier, the hyperplane should have the highest margin.</p><p class="calibre11">The training observations that are close yet equidistant from the separating hyperplane are known as support vectors. For any slight change in the support vectors, the hyperplane would also get reoriented. These support vectors actually define the margin. Now, what if the two classes under consideration are not separable? We would probably want a classifier that does not perfectly separate the two classes and has a softer boundary that allows some level of misclassification as well. This requirement led to the introduction of the support vector classifier (also known as the soft margin classifier).</p><p class="calibre11">Mathematically, it is the slack variable in the equation that allows for misclassification. Also, there is a tuning parameter in the support vector classifier which should be selected using cross-validation. This tuning parameter is the one that trades off between bias and variance and should be handled with care. When it is large, the margin is wider and includes many support vectors, and has low variance and high bias. If it is small, then the margin will have fewer support vectors and the classifier will have low bias but high variance.</p><p class="calibre11">The loss function for the SVM can be represented as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_045.jpg" alt="Linear Support Vector Machines (SVM)" class="calibre130"/></div><p class="calibre11">
</p><p class="calibre11">As of this writing, Spark supports only linear SVMs. By default, linear SVMs are trained with an L2 regularization. Spark also supports alternative L1 regularization.</p><p class="calibre11">So far so good! But how would the support vector classifier work when there is a non-linear boundary between the classes, as shown in the following image:</p><p class="calibre11">
</p><div><img src="img/image_06_046.jpg" alt="Linear Support Vector Machines (SVM)" class="calibre131"/></div><p class="calibre11">
</p><p class="calibre11">Any linear classifier, such as a support vector classifier, would perform very poorly in the preceding situation. If it draws a straight line through the data points, then the classes would not be separated properly. This is a case of non-linear class boundaries. A solution to this problem is the SVM. In other words, when a support vector classifier is fused with a non-linear kernel, it becomes an SVM.</p><p class="calibre11">Similar to the way we introduced higher order polynomial terms in the regression equation to account for the non-linearity, something can also be done in the SVM context. The SVM uses something called kernels to take care of different kinds of non-linearity in the dataset; different kernels for different kinds of non-linearity. Kernel methods map the data into higher dimensional space as the data might get well separated if it does so. Also, it makes distinguishing different classes easier. Let us discuss a few of the important kernels so as to be able to select the right one.</p><div><div><div><div><h2 class="title3"><a id="ch06lvl3sec58" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Linear kernel</h2></div></div></div><p class="calibre11">This is one of the most basic type of kernels that allows us to pick out only lines or hyperplanes. It is equivalent to a support vector classifier. It cannot address the non-linearity if present in the dataset.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl3sec59" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Polynomial kernel</h2></div></div></div><p class="calibre11">This allows us to address some level of non-linearity to the extent of the order of polynomials. This works well when the training data is normalized. This kernel usually has more hyperparameters and therefore increases the complexity of the model.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl3sec60" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Radial Basis Function kernel</h2></div></div></div><p class="calibre11">When you are not really sure of which kernel to use, <strong class="calibre19">Radial Basis Function</strong> (<strong class="calibre19">RBF</strong>) can be a good default choice. It allows you to pick out even circles or hyperspheres. Though this usually performs better than linear or polynomial kernel, it does not perform well when the number of features is huge.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl3sec61" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sigmoid kernel</h2></div></div></div><p class="calibre11">The sigmoid kernel has its roots in neural networks. So, an SVM with a sigmoid kernel is equivalent to a neural network with a two layered perceptron.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl2sec72" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Training an SVM</h1></div></div></div><p class="calibre11">While training an SVM, the modeler has to take a number of decisions:</p><div><ul class="itemizedlist"><li class="listitem">How to pre-process the data (transformation and scaling). The categorical variables should be converted to numeric ones by dummifying them. Also, scaling the numeric values is needed (either 0 to 1 or -1 to +1).</li><li class="listitem">Which kernel to use (check using cross-validation if you are unable to visualize the data and/ or conclude on it).</li><li class="listitem">What parameters to set for the SVM: penalty parameter and the kernel parameter (find using cross-validation or grid search)</li></ul></div><p class="calibre11">If needed, you can use an entropy based feature selection to include only the important features in your model.</p><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">scala&gt; import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}
import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}
scala&gt; import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
scala&gt; import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.util.MLUtils
scala&gt;

// Load training data in LIBSVM format.
scala&gt; val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:84
scala&gt;

// Split data into training (60%) and test (40%).
scala&gt; val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)
splits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:29)
scala&gt; val training = splits(0).cache()
training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at &lt;console&gt;:29
scala&gt; val test = splits(1)
test: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at &lt;console&gt;:29
scala&gt;

// Run training algorithm to build the model
scala&gt; val model = SVMWithSGD.train(training, numIterations=100)
model: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 692, numClasses = 2, threshold = 0.0
scala&gt;

// Clear the default threshold.
scala&gt; model.clearThreshold()
res1: model.type = org.apache.spark.mllib.classification.SVMModel: intercept =
0.0, numFeatures = 692, numClasses = 2, threshold = None
scala&gt;

// Compute raw scores on the test set.
scala&gt; val scoreAndLabels = test.map { point =&gt;
       val score = model.predict(point.features)
      (score, point.label)
      }
scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] =
MapPartitionsRDD[213] at map at &lt;console&gt;:37
scala&gt;

// Get evaluation metrics.
scala&gt; val metrics = new BinaryClassificationMetrics(scoreAndLabels)
metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@3106aebb
scala&gt; println("Area under ROC = " + metrics.areaUnderROC())
Area under ROC = 1.0
scala&gt;</pre><div><div><h3 class="title2"><a id="note12" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Note</h3><p class="calibre25">
<code class="literal">mllib</code> has already entered maintenance mode and SVM is still not available under ml so only Scala code is provided for illustration.</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec48" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Decision trees</h1></div></div></div><p class="calibre11">A decision tree is a non-parametric supervised learning algorithm which can be used for both classification and regression. Decision trees are like inverted trees with the root node at the top and leaf nodes forming downwards. There are different algorithms to split the dataset into branch-like segments. Each leaf node is assigned to a class that represents the most appropriate target values.</p><p class="calibre11">Decision trees do not require any scaling or transformations of the dataset and work as the data is. They can handle both categorical and continuous features, and also address non-linearity in the dataset. At its core, a decision tree is a greedy algorithm (it considers the best split at the moment and does not take into consideration the future scenarios) that performs a recursive binary partitioning of the feature space. Splitting is done based on information gain at each node because information gain measures how well a given attribute separates the training examples as per the target class or value. The first split happens for the feature that generates maximum information gain and becomes the root node.</p><p class="calibre11">The information gain at a node is the difference between the parent node impurity and the weighted sum of two child node impurities. To estimate information gain, Spark currently has two impurity measures for classification problems and one impurity measure for regression, as explained next.</p><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec73" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Impurity measures</h2></div></div></div><p class="calibre11">Impurity is a measure of homogeneity and the best criteria for recursive partitioning. By calculating the impurity, the best split candidate is decided. Most of the impurity measures are probability based:</p><p class="calibre11">
<em class="calibre22">Probability of a class = number of observations of that class / total number of observations</em>
</p><p class="calibre11">Let us spend some time on different types of important impurity measures that are supported by Spark.</p><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec62" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Gini Index</h3></div></div></div><p class="calibre11">The Gini Index is mainly intended for the continuous attributes or features in a dataset. If not, it would assume that all the attributes and features are continuous. The split makes the child nodes more <em class="calibre22">purer</em> than the parent node. Gini tends to find the largest class - the class of response variable that has got the maximum observations. It can be defined as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_047.jpg" alt="Gini Index" class="calibre132"/></div><p class="calibre11">
</p><p class="calibre11">If all observations of a response belong to a single class, then probability <em class="calibre22">P</em> of that class <em class="calibre22">j</em>, that is (<em class="calibre22">Pj</em>), will be 1 as there is only one class, and <em class="calibre22">(Pj)2</em> would also be 1. This makes the Gini Index to be zero.</p></div><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec63" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Entropy</h3></div></div></div><p class="calibre11">Entropy is mainly intended for the categorical attributes or features in a dataset. It can be defined as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_048.jpg" alt="Entropy" class="calibre133"/></div><p class="calibre11">
</p><p class="calibre11">If all observations of a response belong to a single class, then the probability of that class (<em class="calibre22">Pj</em>) will be 1, and <em class="calibre22">log(P)</em> would be zero. This makes the entropy to be zero.</p><p class="calibre11">The following graph depicts the probability of a fair coin toss:</p><p class="calibre11">
</p><div><img src="img/Capture-3.jpg" alt="Entropy" class="calibre134"/></div><p class="calibre11">
</p><p class="calibre11">Just to explain the preceding graph, if you toss a fair coin, the probability of a head or a tail would be 0.5, so there will be maximum observations at a probability of 0.5.</p><p class="calibre11">If the data sample is completely homogeneous then the entropy will be zero, and if the sample can be equally divided into two, then the entropy will be one.</p><p class="calibre11">It is a little slower to compute than Gini because it has to compute the log as well.</p></div><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec64" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Variance</h3></div></div></div><p class="calibre11">Unlike the Gini Index and entropy, variance is used for calculating information gain for regression problems. Variance can be defined as:</p><p class="calibre11">
</p><div><img src="img/image_06_050.jpg" alt="Variance" class="calibre135"/></div><p class="calibre11">
</p></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec74" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Stopping rule</h2></div></div></div><p class="calibre11">The recursive tree construction is stopped at a node when one of the following conditions is met:</p><div><ul class="itemizedlist"><li class="listitem">The node depth is equal to the <code class="literal">maxDepth</code> training parameter</li><li class="listitem">No split candidate leads to an information gain greater than <code class="literal">minInfoGain</code></li><li class="listitem">No split candidate produces child nodes, each of which have at least a <code class="literal">minInstancesPerNode</code> training instances</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec75" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Split candidates</h2></div></div></div><p class="calibre11">A dataset typically has a mixture of categorical and continuous features. How the features get split further into split candidates is something we should understand because we at times need some level of control over them to build a better model.</p><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec65" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Categorical features</h3></div></div></div><p class="calibre11">For a categorical feature with <em class="calibre22">M</em> possible values (categories), one could come up with <em class="calibre22">2(M-1)-1</em> split candidates. Whether for binary classification or regression, the number of split candidates can be reduced to <em class="calibre22">M-1</em> by ordering the categorical feature values by the average label.</p><p class="calibre11">For example, consider a binary classification (0/1) problem with one categorical feature that has three categories A, B, and C, and their corresponding proportions of label-1 response variables are 0.2, 0.6, and 0.4 respectively. In this case, the categorical features can be ordered as A, C, B. So, the two split candidates (<em class="calibre22">M-1</em> = <em class="calibre22">3-1</em> = <em class="calibre22">2</em>) can be <em class="calibre22">A | (C, B)</em> and <em class="calibre22">A, (C | B)</em> where '<em class="calibre22">|'</em> denotes the split.</p></div><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec66" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Continuous features</h3></div></div></div><p class="calibre11">For a continuous feature variable, there can be a chance that no two values are the same (at least we can assume so). If there are <em class="calibre22">n</em> observations, then <em class="calibre22">n</em> split candidates might not be a good idea, especially in a big data setting.</p><p class="calibre11">In Spark, it is done by performing a quantile calculation on a sample of data, and binning the data accordingly. You can still have control over the maximum bins that you would like to allow, using the <code class="literal">maxBins</code> parameter. The maximum default value for <code class="literal">maxBins</code> is <code class="literal">32</code>.</p></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec76" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Advantages of decision trees</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">They are simple to understand and interpret, so easy to explain to business users</li><li class="listitem">They works for both classification and regression</li><li class="listitem">Both qualitative and quantitative data can be accommodated in constructing the decision trees</li></ul></div><p class="calibre11">Information gains in decision trees are biased in favor of the attributes with more levels.</p></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec77" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Disadvantages of decision trees</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">They do not work that greatly for effectively continuous outcome variables</li><li class="listitem">Performance is poor when there are many classes and the dataset is small</li><li class="listitem">Axis parallel split reduces the accuracy</li><li class="listitem">They suffer from high variance as they try to fit almost all data points</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec78" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Example</h2></div></div></div><p class="calibre11">Implementation - wise there are no major differences between classification and regression trees. Let us have a look at the practical implementation of it on Spark.</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">//Assuming ml.Pipeline and ml.features are already imported
scala&gt; import org.apache.spark.ml.classification.{
        DecisionTreeClassifier, DecisionTreeClassificationModel}
import org.apache.spark.ml.classification.{DecisionTreeClassifier,
DecisionTreeClassificationModel}
scala&gt;
/prepare train data
scala&gt; val f:String = "&lt;Your path&gt;/simple_file1.csv"
f: String = &lt;your path&gt;/simple_file1.csv
scala&gt; val trainDF = spark.read.options(Map("header"-&gt;"true",
            "inferSchema"-&gt;"true")).csv(f)
trainDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int]

scala&gt;

 //define DecisionTree pipeline
//StringIndexer maps labels(String or numeric) to label indices
//Maximum occurrence label becomes 0 and so on
scala&gt; val lblIdx = new StringIndexer().
                setInputCol("Label").
                setOutputCol("indexedLabel")
lblIdx: org.apache.spark.ml.feature.StringIndexer = strIdx_3a7bc9c1ed0d
scala&gt;

// Create labels list to decode predictions
scala&gt; val labels = lblIdx.fit(trainDF).labels
labels: Array[String] = Array(2, 1, 3)
scala&gt;

//Define Text column indexing stage
scala&gt; val fIdx = new StringIndexer().
                setInputCol("Text").
              setOutputCol("indexedText")
fIdx: org.apache.spark.ml.feature.StringIndexer = strIdx_49253a83c717

// VectorAssembler
scala&gt; val va = new VectorAssembler().
              setInputCols(Array("indexedText")).
              setOutputCol("features")
va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_764720c39a85

//Define Decision Tree classifier. Set label and features vector
scala&gt; val dt = new DecisionTreeClassifier().
            setLabelCol("indexedLabel").
            setFeaturesCol("features")
dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_84d87d778792

//Define label converter to convert prediction index back to string
scala&gt; val lc = new IndexToString().
                setInputCol("prediction").
                setOutputCol("predictedLabel").
                setLabels(labels)
lc: org.apache.spark.ml.feature.IndexToString = idxToStr_e2f4fa023665
scala&gt;

//String the stages together to form a pipeline
scala&gt; val dt_pipeline = new Pipeline().setStages(
          Array(lblIdx,fIdx,va,dt,lc))
dt_pipeline: org.apache.spark.ml.Pipeline = pipeline_d4b0e884dcbf
scala&gt;
//Apply pipeline to the train data
scala&gt; val resultDF = dt_pipeline.fit(trainDF).transform(trainDF)

//Check results. Watch Label and predictedLabel column values match
resultDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int ... 6 more
fields]
scala&gt;
resultDF.select("Text","Label","features","prediction","predictedLabel").show()
+----+-----+--------+----------+--------------+
|Text|Label|features|prediction|predictedLabel|
+----+-----+--------+----------+--------------+
|   A|    1|   [1.0]|       1.0|             1|
|   B|    2|   [0.0]|       0.0|             2|
|   C|    3|   [2.0]|       2.0|             3|
|   A|    1|   [1.0]|       1.0|             1|
|   B|    2|   [0.0]|       0.0|             2|
+----+-----+--------+----------+--------------+
scala&gt;

//Prepare evaluation data
scala&gt; val eval:String = "&lt;Your path&gt;/simple_file2.csv"
eval: String = &lt;Your path&gt;/simple_file2.csv
scala&gt; val evalDF = spark.read.options(Map("header"-&gt;"true",
            "inferSchema"-&gt;"true")).csv(eval)
evalDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int]
scala&gt;

//Apply the same pipeline to the evaluation data
scala&gt; val eval_resultDF = dt_pipeline.fit(evalDF).transform(evalDF)
eval_resultDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int ... 7
more fields]

//Check evaluation results
scala&gt;
eval_resultDF.select("Text","Label","features","prediction","predictedLabel").sh
w()
+----+-----+--------+----------+--------------+
|Text|Label|features|prediction|predictedLabel|
+----+-----+--------+----------+--------------+
|   A|    1|   [0.0]|       1.0|             1|
|   A|    1|   [0.0]|       1.0|             1|
|   A|    2|   [0.0]|       1.0|             1|
|   B|    2|   [1.0]|       0.0|             2|
|   C|    3|   [2.0]|       2.0|             3|
+----+-----+--------+----------+--------------+
//Note that predicted label for the third row is 1 as against Label(2) as
expected
<strong class="calibre19"> 
</strong>
<strong class="calibre19"> 
Python:</strong>

//Model training example
&gt;&gt;&gt; from pyspark.ml.pipeline import Pipeline
&gt;&gt;&gt; from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler,
IndexToString
&gt;&gt;&gt; from pyspark.ml.classification import DecisionTreeClassifier,
DecisionTreeClassificationModel
&gt;&gt;&gt; 

//prepare train data
&gt;&gt;&gt; file_location = "../work/simple_file1.csv"
&gt;&gt;&gt; trainDF = spark.read.csv(file_location,header=True,inferSchema=True)

 //Read file
&gt;&gt;&gt;

//define DecisionTree pipeline
//StringIndexer maps labels(String or numeric) to label indices
//Maximum occurrence label becomes 0 and so on
&gt;&gt;&gt; lblIdx = StringIndexer(inputCol = "Label",outputCol = "indexedLabel")

// Create labels list to decode predictions
&gt;&gt;&gt; labels = lblIdx.fit(trainDF).labels
&gt;&gt;&gt; labels
[u'2', u'1', u'3']
&gt;&gt;&gt; 

//Define Text column indexing stage
&gt;&gt;&gt; fidx = StringIndexer(inputCol="Text",outputCol="indexedText")

// Vector assembler
&gt;&gt;&gt; va = VectorAssembler(inputCols=["indexedText"],outputCol="features")

//Define Decision Tree classifier. Set label and features vector
&gt;&gt;&gt; dt = DecisionTreeClassifier(labelCol="indexedLabel",featuresCol="features")

//Define label converter to convert prediction index back to string
&gt;&gt;&gt; lc = IndexToString(inputCol="prediction",outputCol="predictedLabel",
                       labels=labels)

//String the stages together to form a pipeline
&gt;&gt;&gt; dt_pipeline = Pipeline(stages=[lblIdx,fidx,va,dt,lc])
&gt;&gt;&gt;
&gt;&gt;&gt; 

//Apply decision tree pipeline
&gt;&gt;&gt; dtModel = dt_pipeline.fit(trainDF)
&gt;&gt;&gt; dtDF = dtModel.transform(trainDF)
&gt;&gt;&gt; dtDF.columns
['Text', 'Label', 'indexedLabel', 'indexedText', 'features', 'rawPrediction',
'probability', 'prediction', 'predictedLabel']
&gt;&gt;&gt; dtDF.select("Text","Label","indexedLabel","prediction",
"predictedLabel").show()
+----+-----+------------+----------+--------------+
|Text|Label|indexedLabel|prediction|predictedLabel|
+----+-----+------------+----------+--------------+
|   A|    1|         1.0|       1.0|             1|
|   B|    2|         0.0|       0.0|             2|
|   C|    3|         2.0|       2.0|             3|
|   A|    1|         1.0|       1.0|             1|
|   B|    2|         0.0|       0.0|             2|
+----+-----+------------+----------+--------------+

&gt;&gt;&gt;

&gt;&gt;&gt; //prepare evaluation dataframe
&gt;&gt;&gt; eval_file_path = "../work/simple_file2.csv"
&gt;&gt;&gt; evalDF = spark.read.csv(eval_file_path,header=True, inferSchema=True) 

//Read eval file
&gt;&gt;&gt; eval_resultDF = dt_pipeline.fit(evalDF).transform(evalDF)
&gt;&gt;&gt; eval_resultDF.columns
['Text', 'Label', 'indexedLabel', 'indexedText', 'features', 'rawPrediction', 'probability', 'prediction', 'predictedLabel']
&gt;&gt;&gt; eval_resultDF.select("Text","Label","indexedLabel","prediction",
"predictedLabel").show()
+----+-----+------------+----------+--------------+
|Text|Label|indexedLabel|prediction|predictedLabel|
+----+-----+------------+----------+--------------+
|   A|    1|         1.0|       1.0|             1|
|   A|    1|         1.0|       1.0|             1|
|   A|    2|         0.0|       1.0|             1|
|   B|    2|         0.0|       0.0|             2|
|   C|    3|         2.0|       2.0|             3|
+----+-----+------------+----------+--------------+
&gt;&gt;&gt; 

Accompanying data files:
<strong class="calibre19">simple_file1.csv
</strong>Text,Label
A,1
B,2
C,3
A,1
B,2<strong class="calibre19">simple_file2.csv
</strong>Text,Label
A,1
A,1
A,2
B,2
C,3</pre></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec49" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Ensembles</h1></div></div></div><p class="calibre11">As the name suggests, ensemble methods use multiple learning algorithms to obtain a more accurate model in terms of prediction accuracy. Usually these techniques require more computing power and make the model more complex, which makes it difficult to interpret. Let us discuss the various types of ensemble techniques available on Spark.</p><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec79" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Random forests</h2></div></div></div><p class="calibre11">A random forest is an ensemble technique for the decision trees. Before we get to random forests, let us see how it has evolved. We know that decision trees usually have high variance issues and tend to overfit the model. To address this, a concept called <em class="calibre22">bagging</em> (also known as bootstrap aggregating) was introduced. For the decision trees, the idea was to take multiple training sets (bootstrapped training sets) from the dataset and create separate decision trees out of those, and then average them out for regression trees. For the classification trees, we can take the majority vote or the most commonly occurring class from all the trees. These trees grew deep and were not pruned at all. This definitely reduced the variance though the individual trees might have high variance.</p><p class="calibre11">One problem with the plain bagging approach was that for most of the bootstrapped training sets, the strong predictors took their positions at the top split which almost made the bagged trees look similar. This meant that the prediction also looked similar and if you averaged them out, then it did not reduce the variance to the extent expected. To address this, a technique was needed which would take a similar approach as that of bagged trees but eliminate the correlation amongst the trees, hence the <em class="calibre22">random forest</em>.</p><p class="calibre11">In this approach, you build bootstrapped training samples to create decision trees, but the only difference is that every time a split happens, a random sample of P predictors are chosen from a total of say K predictors. This is how a random forest injects randomness to this approach. As a thumb rule, we can take P as the square root of Q.</p><p class="calibre11">Like in the case of bagging, in this approach you also average the predictions if your goal is regression and take the majority vote if the goal is classification. Spark provides some tuning parameters to tune this model, which are as follows:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">numTrees</code>: You can specify the number of trees to consider in the random forest. If the numbers are high then the variance in prediction would be less, but the time required would be more.</li><li class="listitem"><code class="literal">maxDepth</code>: You can specify the maximum depth of each tree. An increased depth makes the trees more powerful in terms of prediction accuracy. Though they tend to overfit the individual trees, the overall output is still good because we average the results anyway, which reduces the variance.</li></ul></div><div><ul class="itemizedlist"><li class="listitem"><code class="literal">subsamplingRate</code>: This parameter is mainly used to speed up training. It is used to set the bootstrapped training sample size. A value less than 1.0 speeds up the performance.</li><li class="listitem"><code class="literal">featureSubsetStrategy</code>: This parameter can also help speed up the execution. It is used to set the number of features to use as split candidates for every node. It should be set carefully as too low or too high a value can impact the accuracy of the model.</li></ul></div><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec67" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Advantages of random forests</h3></div></div></div><div><ul class="itemizedlist"><li class="listitem">They run faster as the execution happens in parallel</li><li class="listitem">They are less prone to overfitting</li><li class="listitem">They are easy to tune</li><li class="listitem">Prediction accuracy is more compared to trees or bagged trees</li><li class="listitem">They work well even when the predictor variables are a mixture of categorical and continuous features, and do not require scaling</li></ul></div></div></div><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec80" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Gradient-Boosted Trees</h2></div></div></div><p class="calibre11">Like random forests, <strong class="calibre19">Gradient-Boosted Trees</strong> (<strong class="calibre19">GBTs</strong>) are also an ensemble of trees. They can be applied to both classification and regression problems. Unlike bagged trees or random forests, where trees are built in parallel on independent datasets and are independent of each other, GBTs are built sequentially. Each tree is grown using the result of the previously grown tree. Note that GBTs do not work on bootstrapped samples.</p><p class="calibre11">On each iteration, GBTs use the current ensemble at hand to predict the labels for the training instances and compares them with true labels and estimates the error. The training instances with poor prediction accuracy get relabeled so that the decision trees get corrected in the next iteration based on the error rate for the previous mistakes.</p><p class="calibre11">The mechanism behind finding the error rate and relabeling the instances is based on the loss function. GBTs are designed to reduce this loss function for every iteration. The following types of loss functions are supported by Spark:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Log loss</strong>: This is used for classification problems.</li></ul></div><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Squared error (L2 loss)</strong>: This is used for regression problems and is set by default. It is the summation of the squared differences between the actual and predicted output for all the observations. Outliers should be treated well for this loss function to perform well.</li><li class="listitem"><strong class="calibre19">Absolute error (L1 loss)</strong>: This is also used for regression problems. It is the summation of the absolute differences between the actual and predicted output for all the observations. It is more robust to outliers compared to squared error.</li></ul></div><p class="calibre11">Spark provides some tuning parameters to tune this model, which are as follows:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">loss</code>: You can pass a loss function as discussed in the previous section, depending on the dataset you are dealing with and whether you intend to do classification or regression.</li><li class="listitem"><code class="literal">numIterations</code>: Each iteration produces only one tree! If you set this very high, then the time needed for execution will also be high as the operation would be sequential and can also lead to overfitting. It should be carefully set for better performance and accuracy.</li><li class="listitem"><code class="literal">learningRate</code>: This is not really a tuning parameter. If the algorithm's behavior is unstable then reducing this can help stabilize the model.</li><li class="listitem"><code class="literal">algo</code>: <em class="calibre22">Classification</em> or <em class="calibre22">regression</em> is set based on what you want.</li></ul></div><p class="calibre11">GBTs can overfit the models with a greater number of trees, so Spark provides the <code class="literal">runWithValidation</code> method to prevent overfitting.</p><div><div><h3 class="title5"><a id="tip13" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Tip</h3><p class="calibre25">As of this writing, GBTs on Spark do not yet support multiclass classification.</p></div></div><p class="calibre11">Let us look at an example to illustrate GBTs in action. The example dataset contains average marks and attendance of twenty students. The data also contains result as Pass or Fail, which follow a set of criteria. However, a couple of students (ids 1009 and 1020) were "granted" Pass status event though they did not really qualify. Now our task is to check if the models pick up these two students are not.</p><p class="calibre11">The Pass criteria are as follows:</p><div><ul class="itemizedlist"><li class="listitem">Marks should be at least 40 and Attendance should be at least "Enough"</li><li class="listitem">If Marks are between 40 and 60, then attendance should be "Full" to pass</li></ul></div><p class="calibre11">The following example also emphasizes on reuse of pipeline stages across multiple models. So, we build a DecisionTree classifier first and then a GBT. We build two different pipelines that share stages.</p><p class="calibre11">
<strong class="calibre19">Input</strong>:</p><pre class="programlisting">// Marks &lt; 40 = Fail
// Attendence == Poor =&gt; Fail
// Marks &gt;40 and attendence Full =&gt; Pass
// Marks &gt; 60 and attendence Enough or Full =&gt; Pass
// Two exceptions were studentId 1009 and 1020 who were granted Pass
//This example also emphasizes the reuse of pipeline stages
// Initially the code trains a DecisionTreeClassifier
// Then, same stages are reused to train a GBT classifier</pre><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.feature._
scala&gt; import org.apache.spark.ml.Pipeline
scala&gt; import org.apache.spark.ml.classification.{DecisionTreeClassifier,
                                   DecisionTreeClassificationModel}
scala&gt; case class StResult(StudentId:String, Avg_Marks:Double,
        Attendance:String, Result:String)
scala&gt; val file_path = "../work/StudentsPassFail.csv"
scala&gt; val source_ds = spark.read.options(Map("header"-&gt;"true",
            "inferSchema"-&gt;"true")).csv(file_path).as[StResult]
source_ds: org.apache.spark.sql.Dataset[StResult] = [StudentId: int, Avg_Marks:
double ... 2 more fields]
scala&gt;
//Examine source data
scala&gt; source_ds.show(4)
+---------+---------+----------+------+
|StudentId|Avg_Marks|Attendance|Result|
+---------+---------+----------+------+
|     1001|     48.0|      Full|  Pass|
|     1002|     21.0|    Enough|  Fail|
|     1003|     24.0|    Enough|  Fail|
|     1004|      4.0|      Poor|  Fail|
+---------+---------+----------+------+

scala&gt;           
//Define preparation pipeline
scala&gt; val marks_bkt = new Bucketizer().setInputCol("Avg_Marks").
        setOutputCol("Mark_bins").setSplits(Array(0,40.0,60.0,100.0))
marks_bkt: org.apache.spark.ml.feature.Bucketizer = bucketizer_5299d2fbd1b2
scala&gt; val att_idx = new StringIndexer().setInputCol("Attendance").
        setOutputCol("Att_idx")
att_idx: org.apache.spark.ml.feature.StringIndexer = strIdx_2db54ba5200a
scala&gt; val label_idx = new StringIndexer().setInputCol("Result").
        setOutputCol("Label")
label_idx: org.apache.spark.ml.feature.StringIndexer = strIdx_20f4316d6232
scala&gt;

//Create labels list to decode predictions
scala&gt; val resultLabels = label_idx.fit(source_ds).labels
resultLabels: Array[String] = Array(Fail, Pass)
scala&gt; val va = new VectorAssembler().setInputCols(Array("Mark_bins","Att_idx")).
                  setOutputCol("features")
va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5dc2dbbef48c
scala&gt; val dt = new DecisionTreeClassifier().setLabelCol("Label").
         setFeaturesCol("features")
dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_e8343ae1a9eb
scala&gt; val lc = new IndexToString().setInputCol("prediction").
             setOutputCol("predictedLabel").setLabels(resultLabels)
lc: org.apache.spark.ml.feature.IndexToString = idxToStr_90b6693d4313
scala&gt;

//Define pipeline
scala&gt;val dt_pipeline = new
Pipeline().setStages(Array(marks_bkt,att_idx,label_idx,va,dt,lc))
dt_pipeline: org.apache.spark.ml.Pipeline = pipeline_95876bb6c969
scala&gt; val dtModel = dt_pipeline.fit(source_ds)
dtModel: org.apache.spark.ml.PipelineModel = pipeline_95876bb6c969
scala&gt; val resultDF = dtModel.transform(source_ds)
resultDF: org.apache.spark.sql.DataFrame = [StudentId: int, Avg_Marks: double ...
10 more fields]
scala&gt; resultDF.filter("Label != prediction").select("StudentId","Label","prediction","Result","predictedLabel").show()
+---------+-----+----------+------+--------------+
|StudentId|Label|prediction|Result|predictedLabel|
+---------+-----+----------+------+--------------+\
|     1009|  1.0|       0.0|  Pass|          Fail|
|     1020|  1.0|       0.0|  Pass|          Fail|
+---------+-----+----------+------+--------------+

//Note that the difference is in the student ids that were granted pass

//Same example using Gradient boosted tree classifier, reusing the pipeline stages
scala&gt; import org.apache.spark.ml.classification.GBTClassifier
import org.apache.spark.ml.classification.GBTClassifier
scala&gt; val gbt = new GBTClassifier().setLabelCol("Label").
              setFeaturesCol("features").setMaxIter(10)
gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_cb55ae2174a1
scala&gt; val gbt_pipeline = new
Pipeline().setStages(Array(marks_bkt,att_idx,label_idx,va,gbt,lc))
gbt_pipeline: org.apache.spark.ml.Pipeline = pipeline_dfd42cd89403
scala&gt; val gbtResultDF = gbt_pipeline.fit(source_ds).transform(source_ds)
gbtResultDF: org.apache.spark.sql.DataFrame = [StudentId: int, Avg_Marks: double ... 8 more fields]
scala&gt; gbtResultDF.filter("Label !=
prediction").select("StudentId","Label","Result","prediction","predictedLabel").show()
+---------+-----+------+----------+--------------+
|StudentId|Label|Result|prediction|predictedLabel|
+---------+-----+------+----------+--------------+
|     1009|  1.0|  Pass|       0.0|          Fail|
|     1020|  1.0|  Pass|       0.0|          Fail|
+---------+-----+------+----------+--------------+</pre><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">&gt;&gt;&gt; from pyspark.ml.pipeline import Pipeline
&gt;&gt;&gt; from pyspark.ml.feature import Bucketizer, StringIndexer, VectorAssembler, IndexToString
&gt;&gt;&gt; from pyspark.ml.classification import DecisionTreeClassifier,
DecisionTreeClassificationModel
&gt;&gt;&gt; 

//Get source file
&gt;&gt;&gt; file_path = "../work/StudentsPassFail.csv"
&gt;&gt;&gt; source_df = spark.read.csv(file_path,header=True,inferSchema=True)
&gt;&gt;&gt; 

//Examine source data
&gt;&gt;&gt; source_df.show(4)
+---------+---------+----------+------+
|StudentId|Avg_Marks|Attendance|Result|
+---------+---------+----------+------+
|     1001|     48.0|      Full|  Pass|
|     1002|     21.0|    Enough|  Fail|
|     1003|     24.0|    Enough|  Fail|
|     1004|      4.0|      Poor|  Fail|
+---------+---------+----------+------+

//Define preparation pipeline
&gt;&gt;&gt; marks_bkt = Bucketizer(inputCol="Avg_Marks",
        outputCol="Mark_bins", splits=[0,40.0,60.0,100.0])
&gt;&gt;&gt; att_idx = StringIndexer(inputCol = "Attendance",
        outputCol="Att_idx")
&gt;&gt;&gt; label_idx = StringIndexer(inputCol="Result",
                   outputCol="Label")
&gt;&gt;&gt; 

//Create labels list to decode predictions
&gt;&gt;&gt; resultLabels = label_idx.fit(source_df).labels
&gt;&gt;&gt; resultLabels
[u'Fail', u'Pass']
&gt;&gt;&gt; 
&gt;&gt;&gt; va = VectorAssembler(inputCols=["Mark_bins","Att_idx"],
                         outputCol="features")
&gt;&gt;&gt; dt = DecisionTreeClassifier(labelCol="Label", featuresCol="features")
&gt;&gt;&gt; lc = IndexToString(inputCol="prediction",outputCol="predictedLabel",
             labels=resultLabels)
&gt;&gt;&gt; dt_pipeline = Pipeline(stages=[marks_bkt, att_idx, label_idx,va,dt,lc])
&gt;&gt;&gt; dtModel = dt_pipeline.fit(source_df)
&gt;&gt;&gt; resultDF = dtModel.transform(source_df)
&gt;&gt;&gt;

//Look for obervatiuons where prediction did not match
&gt;&gt;&gt; resultDF.filter("Label != prediction").select(
         "StudentId","Label","prediction","Result","predictedLabel").show()
+---------+-----+----------+------+--------------+
|StudentId|Label|prediction|Result|predictedLabel|
+---------+-----+----------+------+--------------+
|     1009|  1.0|       0.0|  Pass|          Fail|
|     1020|  1.0|       0.0|  Pass|          Fail|
+---------+-----+----------+------+--------------+

//Note that the difference is in the student ids that were granted pass
&gt;&gt;&gt; 
//Same example using Gradient boosted tree classifier, reusing the pipeline
stages
&gt;&gt;&gt; from pyspark.ml.classification import GBTClassifier
&gt;&gt;&gt; gbt = GBTClassifier(labelCol="Label", featuresCol="features",maxIter=10)
&gt;&gt;&gt; gbt_pipeline = Pipeline(stages=[marks_bkt,att_idx,label_idx,va,gbt,lc])
&gt;&gt;&gt; gbtResultDF = gbt_pipeline.fit(source_df).transform(source_df)
&gt;&gt;&gt; gbtResultDF.columns
['StudentId', 'Avg_Marks', 'Attendance', 'Result', 'Mark_bins', 'Att_idx',
'Label', 'features', 'prediction', 'predictedLabel']
&gt;&gt;&gt; gbtResultDF.filter("Label !=
prediction").select("StudentId","Label","Result","prediction","predictedLabel").show()
+---------+-----+------+----------+--------------+
|StudentId|Label|Result|prediction|predictedLabel|
+---------+-----+------+----------+--------------+
|     1009|  1.0|  Pass|       0.0|          Fail|
|     1020|  1.0|  Pass|       0.0|          Fail|
+---------+-----+------+----------+--------------+</pre></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec50" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Multilayer perceptron classifier</h1></div></div></div><p class="calibre11">A <strong class="calibre19">multilayer perceptron classifier</strong> (<strong class="calibre19">MLPC</strong>) is a feedforward artificial neural network with multiple layers of nodes connected to each other in a directed fashion. It uses a supervised learning technique called <em class="calibre22">backpropagation</em> for training the network.</p><p class="calibre11">Nodes in the intermediary layer use the sigmoid function to restrict the output between 0 and 1, and the nodes in the output layer use the <code class="literal">softmax</code> function, which is a generalized version of the sigmoid function.</p><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
scala&gt; import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
scala&gt; import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.mllib.util.MLUtils

// Load training data
scala&gt; val data = MLUtils.loadLibSVMFile(sc,
"data/mllib/sample_multiclass_classification_data.txt").toDF()
data: org.apache.spark.sql.DataFrame = [label: double, features: vector]

//Convert mllib vectors to ml Vectors for spark 2.0+. Retain data for previous versions
scala&gt; val data2 = MLUtils.convertVectorColumnsToML(data)
data2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]

// Split the data into train and test
scala&gt; val splits = data2.randomSplit(Array(0.6, 0.4), seed = 1234L)
splits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([label: double, features: vector], [label: double, features: vector])
scala&gt; val train = splits(0)
train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]
scala&gt; val test = splits(1)
test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]

// specify layers for the neural network:
// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)
scala&gt; val layers = Array[Int](4, 5, 4, 3)
layers: Array[Int] = Array(4, 5, 4, 3)

// create the trainer and set its parameters
scala&gt; val trainer = new MultilayerPerceptronClassifier().
           setLayers(layers).setBlockSize(128).
           setSeed(1234L).setMaxIter(100)
trainer: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_edfa49fbae3c

// train the model
scala&gt; val model = trainer.fit(train)
model: org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel = mlpc_edfa49fbae3c

// compute accuracy on the test set
scala&gt; val result = model.transform(test)
result: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]
scala&gt; val predictionAndLabels = result.select("prediction", "label")
predictionAndLabels: org.apache.spark.sql.DataFrame = [prediction: double, label: double]
scala&gt; val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_a4f43d85f261
scala&gt; println("Accuracy:" + evaluator.evaluate(predictionAndLabels))
Accuracy:0.9444444444444444
 
<strong class="calibre19">Python:
</strong>&gt;&gt;&gt; from pyspark.ml.classification import MultilayerPerceptronClassifier
&gt;&gt;&gt; from pyspark.ml.evaluation import MulticlassClassificationEvaluator
&gt;&gt;&gt; from pyspark.mllib.util import MLUtils
&gt;&gt;&gt;

  //Load training data
&gt;&gt;&gt; data = spark.read.format("libsvm").load(      "data/mllib/sample_multiclass_classification_data.txt")

//Convert mllib vectors to ml Vectors for spark 2.0+. Retain data for previous versions
&gt;&gt;&gt; data2 = MLUtils.convertVectorColumnsToML(data)
&gt;&gt;&gt;

 // Split the data into train and test
&gt;&gt;&gt; splits = data2.randomSplit([0.6, 0.4], seed = 1234L)
&gt;&gt;&gt; train, test = splits[0], splits[1]
&gt;&gt;&gt;

 // specify layers for the neural network:
 // input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)
&gt;&gt;&gt; layers = [4,5,4,3] 

// create the trainer and set its parameters
&gt;&gt;&gt; trainer = MultilayerPerceptronClassifier(layers=layers, blockSize=128,
                 seed=1234L, maxIter=100)
// train the model
&gt;&gt;&gt; model = trainer.fit(train)
&gt;&gt;&gt;
 
// compute accuracy on the test set
&gt;&gt;&gt; result = model.transform(test)
&gt;&gt;&gt; predictionAndLabels = result.select("prediction", "label")
&gt;&gt;&gt; evaluator = MulticlassClassificationEvaluator().setMetricName("accuracy")
&gt;&gt;&gt; print "Accuracy:",evaluator.evaluate(predictionAndLabels)
Accuracy: 0.901960784314
&gt;&gt;&gt; </pre></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec51" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Clustering techniques</h1></div></div></div><p class="calibre11">Clustering is an unsupervised learning technique where there is no response variable to supervise the model. The idea is to cluster the data points that have some level of similarity. Apart from exploratory data analysis, it is also used as a part of a supervised pipeline where classifiers or regressors can be built on the distinct clusters. There are a bunch of clustering techniques available. Let us look into a few important ones that are supported by Spark.</p><div><div><div><div><h2 class="title3"><a id="ch06lvl2sec81" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>K-means clustering</h2></div></div></div><p class="calibre11">K-means is one of the most common clustering techniques. The k-means problem is to find cluster centers that minimize the intra-class variance, that is, the sum of squared distances from each data point being clustered to its cluster center (the center that is closest to it). You have to specify in advance the number of clusters you want in the dataset.</p><p class="calibre11">Since it uses the Euclidian distance measure to find the differences between the data points, the features need to be scaled to a comparable unit prior to using k-means. The Euclidian distance can be better explained in a graphical way as follows:</p><p class="calibre11">
</p><div><img src="img/image_06_051.jpg" alt="K-means clustering" class="calibre136"/></div><p class="calibre11">
</p><p class="calibre11">Given a set of data points (<em class="calibre22">x1</em>, <em class="calibre22">x2</em>, ..., <em class="calibre22">xn</em>) with as many dimensions as the number of variables, k-means clustering aims to partition the n observations into k (less than <em class="calibre22">n</em>) sets where <em class="calibre22">S = {S1, S2, ..., Sk}</em>, so as to minimize the <strong class="calibre19">within-cluster sum of squares</strong> (<strong class="calibre19">WCSS</strong>). In other words, its objective is to find:</p><p class="calibre11">
</p><div><img src="img/image_06_052.jpg" alt="K-means clustering" class="calibre137"/></div><p class="calibre11">
</p><p class="calibre11">Spark requires the following parameters to be passed to this algorithm:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">k</code>: This is the number of desired clusters.</li><li class="listitem"><code class="literal">maxIterations</code>: This is the maximum number of iterations to run.</li><li class="listitem"><code class="literal">initializationMode</code>: This specifies either random initialization or initialization via k-means||.</li><li class="listitem"><code class="literal">runs</code>: This is the number of times to run the k-means algorithm (k-means is not guaranteed to find a globally optimal solution, and when run multiple times on a given dataset, the algorithm returns the best clustering result).</li><li class="listitem"><code class="literal">initializationSteps</code>: This determines the number of steps in the k-means|| algorithm.</li><li class="listitem"><code class="literal">epsilon</code>: This determines the distance threshold within which we consider k-means to have converged.</li><li class="listitem"><code class="literal">initialModel</code>: This is an optional set of cluster centers used for initialization. If this parameter is supplied, only one run is performed.</li></ul></div><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec68" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Disadvantages of k-means</h3></div></div></div><div><ul class="itemizedlist"><li class="listitem">It works only on the numeric features</li><li class="listitem">It requires scaling before implementing the algorithm</li><li class="listitem">It is susceptible to local optima (the solution to this is k-means++)</li></ul></div></div><div><div><div><div><h3 class="title4"><a id="ch06lvl3sec69" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Example</h3></div></div></div><p class="calibre11">Let us run k-means clustering on the same students data.</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.clustering.{KMeans, KMeansModel}
import org.apache.spark.ml.clustering.{KMeans, KMeansModel}
scala&gt; import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.linalg.Vectors
scala&gt;

//Define pipeline for kmeans. Reuse the previous stages in ENSEMBLES
scala&gt; val km = new KMeans()
km: org.apache.spark.ml.clustering.KMeans = kmeans_b34da02bd7c8
scala&gt; val kmeans_pipeline = new
Pipeline().setStages(Array(marks_bkt,att_idx,label_idx,va,km,lc))
kmeans_pipeline: org.apache.spark.ml.Pipeline = pipeline_0cd64aa93a88

//Train and transform
scala&gt; val kmeansDF = kmeans_pipeline.fit(source_ds).transform(source_ds)
kmeansDF: org.apache.spark.sql.DataFrame = [StudentId: int, Avg_Marks: double ... 8 more fields]

//Examine results
scala&gt; kmeansDF.filter("Label != prediction").count()
res17: Long = 13

</pre><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">&gt;&gt;&gt; from pyspark.ml.clustering import KMeans, KMeansModel
&gt;&gt;&gt; from pyspark.ml.linalg import Vectors
&gt;&gt;&gt; 

//Define pipeline for kmeans. Reuse the previous stages in ENSEMBLES
&gt;&gt;&gt; km = KMeans()
&gt;&gt;&gt; kmeans_pipeline = Pipeline(stages = [marks_bkt, att_idx, label_idx,va,km,lc])

//Train and transform
&gt;&gt;&gt; kmeansDF = kmeans_pipeline.fit(source_df).transform(source_df)
&gt;&gt;&gt; kmeansDF.columns
['StudentId', 'Avg_Marks', 'Attendance', 'Result', 'Mark_bins', 'Att_idx', 'Label', 'features', 'prediction', 'predictedLabel']
&gt;&gt;&gt; kmeansDF.filter("Label != prediction").count()
4</pre></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec52" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we explained various machine learning algorithms, how they are implemented in the MLlib library and how they can be used with the pipeline API for a streamlined execution. The concepts were covered with Python and Scala code examples for a ready reference.</p><p class="calibre11">In the next chapter, we will discuss how Spark supports R programming language focusing on some of the algorithms and their executions similar to what we covered in this chapter.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec53" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><p class="calibre11">Supported algorithms in MLlib:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://spark.apache.org/docs/latest/mllib-guide.html">http://spark.apache.org/docs/latest/mllib-guide.html</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://spark.apache.org/docs/latest/mllib-decision-tree.html">http://spark.apache.org/docs/latest/mllib-decision-tree.html</a></li></ul></div><p class="calibre11">Spark ML Programming Guide:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://spark.apache.org/docs/latest/ml-guide.html">http://spark.apache.org/docs/latest/ml-guide.html</a></li></ul></div><p class="calibre11">Advanced datascience on spark.pdf from June 2015 summit slides:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html">https://databricks.com/blog/2015/07/29/new-features-in-machine-learning-pipelines-in-spark-1-4.html</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html">https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html">https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</a></li></ul></div></div></div>



  </body></html>