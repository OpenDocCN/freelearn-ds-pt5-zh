<html><head></head><body>
        <section id="E9OE01-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Spark Tuning</h1>
                
            
            <article>
                
<div class="book-info-bottom-author-body"><em class="calibre8">"Harpists spend 90 percent of their lives tuning their harps and 10 percent playing out of tune."</em></div>
<p class="cdpalignright">- Igor Stravinsky</p>
<p class="mce-root">In this chapter, we will dig deeper into Apache Spark internals and see that while Spark is great in making us feel like we are using just another Scala collection, we don't have to forget that Spark actually runs in a distributed system. Therefore, some extra care should be taken. In a nutshell, the following topics will be covered in this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Monitoring Spark jobs</li>
<li class="mce-root1">Spark configuration</li>
<li class="mce-root1">Common mistakes in Spark app development</li>
<li class="mce-root1">Optimization techniques</li>
</ul>


            </article>

            
        </section>
    

        <section id="EAMUI1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Monitoring Spark jobs</h1>
                
            
            <article>
                
<p class="mce-root">Spark provides web UI for monitoring all the jobs running or completed on computing nodes (drivers or executors). In this section, we will discuss in brief how to monitor Spark jobs using Spark web UI with appropriate examples. We will see how to monitor the progress of jobs (including submitted, queued, and running jobs). All the tabs in the Spark web UI will be discussed briefly. Finally, we will discuss the logging procedure in Spark for better tuning.</p>


            </article>

            
        </section>
    

        <section id="EBLF41-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Spark web interface</h1>
                
            
            <article>
                
<p class="mce-root">The web UI (also known as Spark UI) is the web interface for running Spark applications to monitor the execution of jobs on a web browser such as Firefox or Google Chrome. When a SparkContext launches, a web UI that displays useful information about the application gets started on port 4040 in standalone mode. The Spark web UI is available in different ways depending on whether the application is still running or has finished its execution.</p>
<p class="mce-root">Also, you can use the web UI after the application has finished its execution by persisting all the events using <kbd class="calibre11">EventLoggingListener</kbd>. The <kbd class="calibre11">EventLoggingListener</kbd>, however, cannot work alone, and the incorporation of the Spark history server is required. Combining these two features, the following facilities can be achieved:</p>
<ul class="calibre9">
<li class="mce-root1">A list of scheduler stages and tasks</li>
<li class="mce-root1">A summary of RDD sizes</li>
<li class="mce-root1">Memory usage</li>
<li class="mce-root1">Environmental information</li>
<li class="mce-root1">Information about the running executors</li>
</ul>
<p class="mce-root">You can access the UI at <kbd class="calibre11">http://&lt;driver-node&gt;:4040</kbd> in a web browser. For example, a Spark job submitted and running as a standalone mode can be accessed at <kbd class="calibre11">http://localhost:4040</kbd>.</p>
<div class="packt_infobox">Note that if multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040, 4041, 4042, and so on. By default, this information will be available for the duration of your Spark application only. This means that when your Spark job finishes its execution, the binding will no longer be valid or accessible.</div>
<p class="mce-root">As long as the job is running, stages can be observed on Spark UI. However, to view the web UI after the job has finished the execution, you could try setting <kbd class="calibre11">spark.eventLog.enabled</kbd> as true before submitting your Spark jobs. This forces Spark to log all the events to be displayed in the UI that are already persisted on storage such as local filesystem or HDFS.</p>
<p class="mce-root">In the previous chapter, we saw how to submit a Spark job to a cluster. Let's reuse one of the commands for submitting the k-means clustering, as follows:</p>
<p class="mce-root">Â </p>
<pre class="calibre19">
<strong class="calibre1"># Run application as standalone mode on 8 cores</strong><br class="title-page-name"/><strong class="calibre1">SPARK_HOME/bin/spark-submit \</strong><br class="title-page-name"/><strong class="calibre1">  --class org.apache.spark.examples.KMeansDemo \</strong><br class="title-page-name"/><strong class="calibre1">  --master local[8] \</strong><br class="title-page-name"/><strong class="calibre1">  KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \</strong><br class="title-page-name"/><strong class="calibre1">  Saratoga_NY_Homes.txt</strong>
</pre>
<p class="mce-root">If you submit the job using the preceding command, you will not be able to see the status of the jobs that have finished the execution, so to make the changes permanent, use the following two options:</p>
<pre class="calibre19">
spark.eventLog.enabled=true <br class="title-page-name"/>spark.eventLog.dir=file:///home/username/log"
</pre>
<p class="mce-root">By setting the preceding two configuration variables, we asked the Spark driver to make the event logging enabled to be saved at <kbd class="calibre11">file:///home/username/log</kbd>.</p>
<p class="mce-root">In summary, with the following changes, your submitting command will be as follows:</p>
<pre class="calibre19">
<strong class="calibre1"># Run application as standalone mode on 8 cores</strong><br class="title-page-name"/><strong class="calibre1">SPARK_HOME/bin/spark-submit \</strong><br class="title-page-name"/><strong class="calibre1">  --conf "spark.eventLog.enabled=true" \</strong><br class="title-page-name"/><strong class="calibre1">  --conf "spark.eventLog.dir=file:///tmp/test" \</strong><br class="title-page-name"/><strong class="calibre1">  --class org.apache.spark.examples.KMeansDemo \</strong><br class="title-page-name"/><strong class="calibre1">  --master local[8] \</strong><br class="title-page-name"/><strong class="calibre1">  KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \</strong><br class="title-page-name"/><strong class="calibre1">  Saratoga_NY_Homes.txt</strong>
</pre>
<div class="cdpaligncenter"><img class="image-border212" src="../images/00201.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 1:</strong> Spark web UI</div>
<p class="mce-root">As shown in the preceding screenshot, Spark web UI provides the following tabs:</p>
<ul class="calibre9">
<li class="mce-root1"><span>Jobs</span></li>
<li class="mce-root1"><span>Stages</span></li>
<li class="mce-root1"><span>Storage</span></li>
<li class="mce-root1"><span>Environment</span></li>
<li class="mce-root1"><span>Executors</span></li>
<li class="mce-root1"><span>SQL</span></li>
</ul>
<p class="mce-root">It is to be noted that all the features may not be visible at once as they are lazily created on demand, for example, while running a streaming job.</p>


            </article>

            
        </section>
    

        <section id="ECJVM1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Jobs</h1>
                
            
            <article>
                
<p class="mce-root">Depending upon the SparkContext, the <span>Jobs</span> tab shows the status of all the Spark jobs in a Spark application. When you access the <span>Jobs</span> tab on the Spark UI using a web browser at <kbd class="calibre11">http://localhost:4040</kbd> (for standalone mode), you should observe the following options:</p>
<ul class="calibre9">
<li class="mce-root1"><span>User</span>: This shows the active user who has submitted the Spark job</li>
<li class="mce-root1"><span>Total Uptime</span>: This shows the total uptime for the jobs</li>
<li class="mce-root1"><span>Scheduling Mode</span>: In most cases, it is first-in-first-out (aka FIFO)</li>
<li class="mce-root1"><span>Active Jobs</span>: This shows the number of active jobs</li>
<li class="mce-root1"><span>Completed Jobs</span>: This shows the number of completed jobs</li>
<li class="mce-root1"><span>Event Timeline</span>: This shows the timeline of a job that has completed its execution</li>
</ul>
<p class="mce-root">Internally, the <span>Jobs</span> tab is represented by the <kbd class="calibre11">JobsTab</kbd> class, which is a custom <span>SparkUI</span> tab with the jobs prefix. The <span>Jobs</span> tab uses <kbd class="calibre11">JobProgressListener</kbd> to access statistics about the Spark jobs to display the above information on the page. Take a look at the following screenshot:</p>
<div class="cdpaligncenter"><img class="image-border213" src="../images/00135.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 2:</strong> The jobs tab in the Spark web UI</div>
<p class="cdpalignleft1">If you further expand the <span>Active Jobs</span> option in the <span>Jobs</span> tab, you will be able to see the execution plan, status, number of completed stages, and the job ID of that particular job as <span>DAG Visualization,</span> as shown in the following:</p>
<div class="cdpaligncenter"><img class="image-border214" src="../images/00185.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 3:</strong> The DAG visualization for task in the Spark web UI (abridged)</div>
<p class="mce-root">When a user enters the code in the Spark console (for example, Spark shell or using Spark submit), Spark Core creates an operator graph. This is basically what happens when a user executes an action (<span>for example,</span> reduce, collect, count, first, take, countByKey, saveAsTextFile) or transformation (<span>for example,</span> map, flatMap, filter, mapPartitions, sample, union, intersection, distinct) on an RDD (which are immutable objects) at a particular node.</p>
<div class="cdpaligncenter"><img class="image-border215" src="../images/00187.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 4:</strong> DAG scheduler transforming RDD lineage into stage DAG</div>
<p class="mce-root">During the transformation or action, <strong class="calibre1">Directed Acyclic Graph</strong> (<strong class="calibre1">DAG</strong>) information is used to restore the node to last transformation and actions (refer to <em class="calibre8">Figure 4</em> and <em class="calibre8">Figure 5</em> for a clearer picture) to maintain the data resiliency. Finally, the graph is submitted to a DAG scheduler.</p>
<div class="packt_infobox"><span class="field">How does Spark compute the DAG from the RDD and subsequently execute the task?</span><br class="calibre23"/>
At a high level, when any action is called on the RDD, Spark creates the DAG and submits it to the DAG scheduler. The DAG scheduler divides operators into stages of tasks. A stage comprises tasks based on partitions of the input data. The DAG scheduler pipelines operators together. For example, many map operators can be scheduled in a single stage. The final result of a DAG scheduler is a set of stages. The stages are passed on to the task scheduler. The task scheduler launches tasks through the cluster manager (Spark Standalone/YARN/Mesos). The task scheduler doesn't know about the dependencies of the stages. The worker executes the tasks on the stage.</div>
<p class="mce-root">The DAG scheduler then keeps track of which RDDs the stage outputs materialized from. It then finds a minimal schedule to run jobs and divides the related operators into stages of tasks. Based on partitions of the input data, a stage comprises multiple tasks. Then, operators are pipelined together with the DAG scheduler. Practically, more than one map or reduce operator (for example) can be scheduled in a single stage.</p>
<div class="cdpaligncenter"><img class="image-border216" src="../images/00191.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 5:</strong> Executing action leads to new ResultStage and ActiveJob in DAGScheduler</div>
<p class="mce-root">Two fundamental concepts in DAG scheduler are jobs and stages. Thus, it has to track them through internal registries and counters. Technically speaking, DAG scheduler is a part of SparkContext's initialization that works exclusively on the driver (immediately after the task scheduler and scheduler backend are ready). DAG scheduler is responsible for three major tasks in Spark execution. It computes an execution DAG, that is, DAG of stages, for a job. It determines the preferred node to run each task on and handles failures due to shuffle output files being lost.</p>
<div class="cdpaligncenter1"><img class="image-border217" src="../images/00193.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 6:</strong> DAGScheduler as created by SparkContext with other services</div>
<p class="mce-root">The final result of a DAG scheduler is a set of stages. Therefore, most of the statistics and the status of the job can be seen using this visualization, for example, execution plan, status, number of completed stages, and the job ID of that particular job.</p>


            </article>

            
        </section>
    

        <section id="EDIG81-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Stages</h1>
                
            
            <article>
                
<p class="mce-root">The <span>Stages</span> tab in Spark UI shows the current status of all stages of all jobs in a Spark application, including two optional pages for the tasks and statistics for a stage and pool details. Note that this information is available only when the application works in a fair scheduling mode. You should be able to access the <span>Stages</span> tab at <kbd class="calibre11">http://localhost:4040/stages</kbd>. Note that when there are no jobs submitted, the tab shows nothing but the title. The Stages tab shows the stages in a Spark application. The following stages can be seen in this tab:</p>
<ul class="calibre9">
<li class="mce-root1"><span>Active Stages</span></li>
<li class="mce-root1"><span>Pending Stages</span></li>
<li class="mce-root1"><span>Completed Stages</span></li>
</ul>
<p class="mce-root">For example, when you submit a Spark job locally, you should be able to see the following status:</p>
<div class="cdpaligncenter"><img class="image-border218" src="../images/00265.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 7:</strong> The stages for all jobs in the Spark web UI</div>
<p class="mce-root">In this case, there's only one stage that is an active stage. However, in the upcoming chapters, we will be able to observe other stages when we will submit our Spark jobs to AWS EC2 clusters.</p>
<p class="mce-root">To further dig down to the summary of the completed jobs, click on any link contained in the <span>Description</span> column and you should find the related statistics on execution time as metrics. An approximate time of min, median, 25th percentile, 75th percentile, and max for the metrics can also be seen in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border219" src="../images/00378.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 8:</strong> The summary for completed jobs on the Spark web UI</div>
<p class="mce-root">Your case might be different as I have executed and submitted only two jobs for demonstration purposes during the writing of this book. You can see other statistics on the executors as well. For my case, I submitted these jobs in the standalone mode by utilizing 8 cores and 32 GB of RAM. In addition to these, information related to the executor, such as ID, IP address with the associated port number, task completion time, number of tasks (including number of failed tasks, killed tasks, and succeeded tasks), and input size of the dataset per records are shown.</p>
<p class="mce-root">The other section in the image shows other information related to these two tasks, for example, index, ID, attempts, status, locality level, host information, launch time, duration, <strong class="calibre1">Garbage Collection</strong> (<strong class="calibre1">GC</strong>) time, and so on.</p>


            </article>

            
        </section>
    

        <section id="EEH0Q1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Storage</h1>
                
            
            <article>
                
<p class="mce-root">The <span>Storage</span> tab shows the size and memory use for each RDD, DataFrame, or Dataset. You should be able to see the storage-related information of RDDs, DataFrames, or Datasets. The following figure shows storage metadata such as RDD name, storage level, the number of cache partitions, the percentage of a fraction of the data that was cached, and the size of the RDD in the main memory:</p>
<div class="cdpaligncenter"><img class="image-border220" src="../images/00229.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 9:</strong> Storage tab shows space consumed by an RDD in disk</div>
<p class="cdpalignleft1">Note that if the RDD cannot be cached in the main memory, disk space will be used instead. A more detailed discussion will be carried out in a later section of this chapter.</p>
<div class="cdpaligncenter"><img class="image-border221" src="../images/00067.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 10:</strong> Data distribution and the storage used by the RDD in disk</div>


            </article>

            
        </section>
    

        <section id="EFFHC1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Environment</h1>
                
            
            <article>
                
<p class="mce-root">The <span>Environment</span> tab shows the environmental variables that are currently set on your machine (that is, driver). More specifically, runtime information such as <span>Java Home</span>, <span>Java Version</span>, and <span>Scala Version</span> can be seen under <span>Runtime Information</span>. Spark properties such as Spark application ID, app name, and driver host information, driver port, executor ID, master URL, and the schedule mode can be seen. Furthermore, other system-related properties and job properties such as AWT toolkit version, file encoding type (<span>for example,</span> UTF-8), and file encoding package information (for example, sun.io) can be seen under <span>System Properties</span>.</p>
<div class="cdpaligncenter"><img class="image-border222" src="../images/00249.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 11:</strong> Environment tab on Spark web UI</div>


            </article>

            
        </section>
    

        <section id="EGE1U1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Executors</h1>
                
            
            <article>
                
<p class="mce-root">The <span>Executors</span> tab uses <kbd class="calibre11">ExecutorsListener</kbd> to collect information about executors for a Spark application. An executor is a distributed agent that is responsible for executing tasks. Executors are instantiated in different ways. For example, they can be instantiated when <kbd class="calibre11">CoarseGrainedExecutorBackend</kbd> receives <kbd class="calibre11">RegisteredExecutor</kbd> message for Spark Standalone and YARN. The second case is when a Spark job is submitted to Mesos. The Mesos's <kbd class="calibre11">MesosExecutorBackend</kbd> gets registered. The third case is when you run your Spark jobs locally, that is, <kbd class="calibre11">LocalEndpoint</kbd> is created. An executor typically runs for the entire lifetime of a Spark application, which is called static allocation of executors, although you can also opt in for dynamic allocation. The executor backends exclusively manage all the executors in a computing node or clusters. An executor reports heartbeat and partial metrics for active tasks to the <strong class="calibre1">HeartbeatReceiver</strong> RPC endpoint on the driver periodically and the results are sent to the driver. They also provide in-memory storage for RDDs that are cached by user programs through block manager. Refer to the following figure for a clearer idea on this:</p>
<div class="cdpaligncenter1"><img class="image-border223" src="../images/00071.jpeg"/><strong class="calibre1"><br class="title-page-name"/></strong></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 12:</strong> Spark driver instantiates an executor that is responsible for HeartbeatReceiver's Heartbeat message handler</div>
<p class="mce-root">When an executor starts, it first registers with the driver and communicates directly to execute tasks, as shown in the following figure:</p>
<div class="cdpaligncenter1"><img class="image-border224" src="../images/00009.jpeg"/><strong class="calibre1"><br class="title-page-name"/></strong></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 13:</strong> Launching tasks on executor using TaskRunners</div>
<p class="mce-root">You should be able to access the <span>Executors</span> tab at <kbd class="calibre11">http://localhost:4040/executors</kbd>.</p>
<div class="cdpaligncenter1"><img class="image-border31" src="../images/00084.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 14:</strong> Executor tab on Spark web UI</div>
<p class="cdpalignleft1">As shown in the preceding figure, <span>Executor ID</span>, <span>Address</span>, <span>Status</span>, <span>RDD Blocks</span>, <span>Storage Memory</span>, <span>Disk Used</span>, <span>Cores</span>, <span>Active Tasks</span>, <span>Failed Tasks</span>, <span>Complete Tasks</span>, <span>Total Tasks</span>, <span>Task Time (GC Time)</span>, <span>Input</span>, <span>Shuffle Read</span>, <span>Shuffle Write,</span> and <span>Thread Dump</span> about the executor can be seen.</p>


            </article>

            
        </section>
    

        <section id="EHCIG1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">SQL</h1>
                
            
            <article>
                
<p class="mce-root">The <span>SQL</span> tab in the Spark UI displays all the accumulator values per operator. You should be able to access the SQL tab at <kbd class="calibre11">http://localhost:4040/SQL/</kbd>. It displays all the SQL query executions and underlying information by default. However, the SQL tab displays the details of the SQL query execution only after a query has been selected.</p>
<p class="mce-root">A detailed discussion on SQL is out of the scope of this chapter. Interested readers should refer to <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#sql" class="calibre10">http://spark.apache.org/docs/latest/sql-programming-guide.html#sql</a> for more on how to submit an SQL query and see its result output.</p>


            </article>

            
        </section>
    

        <section id="EIB321-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Visualizing Spark application using web UI</h1>
                
            
            <article>
                
<p class="mce-root">When a Spark job is submitted for execution, a web application UI is launched that displays useful information about the application. An event timeline displays the relative ordering and interleaving of application events. The timeline view is available on three levels: across all jobs, within one job, and within one stage. The timeline also shows executor allocation and deallocation.</p>
<div class="cdpaligncenter"><img class="image-border225" src="../images/00325.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 15:</strong> Spark jobs executed as DAG on Spark web UI</div>


            </article>

            
        </section>
    

        <section id="EJ9JK1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Observing the running and completed Spark jobs</h1>
                
            
            <article>
                
<p class="mce-root">To access and observe the running and the completed Spark jobs, open <kbd class="calibre11">http://spark_driver_host:4040</kbd> in a web browser. Note that you will have to replace <kbd class="calibre11">spark_driver_host</kbd> with an IP address or hostname accordingly.</p>
<div class="packt_infobox">Note that if multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040, 4041, 4042, and so on. By default, this information will be available for the duration of your Spark application only. This means that when your Spark job finishes its execution, the binding will no longer be valid or accessible.</div>
<p class="mce-root">Now, to access the active jobs that are still executing, click on the <span>Active Jobs</span> link and you will see the related information of those jobs. On the other hand, to access the status of the completed jobs, click on <span>Completed Jobs</span> and you will see the information as DAG style as discussed in the preceding section.</p>
<div class="cdpaligncenter"><img class="image-border226" src="../images/00129.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 16:</strong> Observing the running and completed Spark jobs</div>
<p class="mce-root">You can achieve these by clicking on the job description link under the <span>Active Jobs</span> or <span>Completed Jobs</span>.</p>


            </article>

            
        </section>
    

        <section id="EK8461-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Debugging Spark applications using logs</h1>
                
            
            <article>
                
<p class="mce-root">Seeing the information about all running Spark applications depends on which cluster manager you are using. You should follow these instructions while debugging your Spark application:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Spark Standalone</strong>: Go to the Spark master UI at <kbd class="calibre11">http://master:18080</kbd>. The master and each worker show cluster and the related job statistics. In addition, a detailed log output for each job is also written to the working directory of each worker. We will discuss how to enable the logging manually using the <kbd class="calibre11">log4j</kbd> with Spark.</li>
<li class="mce-root1"><strong class="calibre1">YARN</strong>: If your cluster manager is YARN, and suppose that you are running your Spark jobs on the Cloudera (or any other YARN-based platform), then go to the YARN applications page in the Cloudera Manager Admin Console. Now, to debug Spark applications running on YARN, view the logs for the Node Manager role. To make this happen, open the log event viewer and then filter the event stream to choose a time window and log level and to display the Node Manager source. You can access logs through the command as well. The format of the command is as follows:</li>
</ul>
<pre class="calibre19">
<strong class="calibre1">     yarn logs -applicationId &lt;application ID&gt; [OPTIONS]</strong>
</pre>
<p class="mce-root">For example, the following are the valid commands for these IDs:</p>
<pre class="calibre19">
<strong class="calibre1">     yarn logs -applicationId application_561453090098_0005                               </strong><br class="title-page-name"/><strong class="calibre1">     yarn logs -applicationId application_561453090070_0005 userid</strong>
</pre>
<p class="mce-root">Note that the user IDs are different. However, this is only true if <kbd class="calibre11">yarn.log-aggregation-enable</kbd> is true in <kbd class="calibre11">yarn-site.xml</kbd> and the application has already finished the execution.</p>


            </article>

            
        </section>
    

        <section>

                            <header id="EL6KO2-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">Logging with log4j with Spark</h1>
                
            
            <article>
                
<p class="mce-root">Spark uses <kbd class="calibre11">log4j</kbd> for its own logging. All the operations that happen backend get logged to the Spark shell console (which is already configured to the underlying storage). Spark provides a template of <kbd class="calibre11">log4j</kbd> as a property file, and we can extend and modify that file for logging in Spark. Move to the <kbd class="calibre11">SPARK_HOME/conf</kbd> directory and you should see the <kbd class="calibre11">log4j.properties.template</kbd> file. This could help us as the starting point for our own logging system.</p>
<p class="mce-root">Now, let's create our own custom logging system while running a Spark job. When you are done, rename the file as <kbd class="calibre11">log4j.properties</kbd> and put it under the same directory (that is, project tree). A sample snapshot of the file can be seen as follows:</p>
<div class="cdpaligncenter"><img class="image-border227" src="../images/00112.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 17:</strong> A snap of the log4j.properties file</div>
<p class="mce-root">By default, everything goes to console and file. However, if you want to bypass all the noiser logs to a system file located at, say, <kbd class="calibre11">/var/log/sparkU.log</kbd>, then you can set these properties in the <kbd class="calibre11">log4j.properties</kbd> file as follows:</p>
<pre class="calibre19">
<strong class="calibre1">log4j.logger.spark.storage=INFO, RollingAppender</strong><br class="title-page-name"/><strong class="calibre1">log4j.additivity.spark.storage=false</strong><br class="title-page-name"/><strong class="calibre1">log4j.logger.spark.scheduler=INFO, RollingAppender</strong><br class="title-page-name"/><strong class="calibre1">log4j.additivity.spark.scheduler=false</strong><br class="title-page-name"/><strong class="calibre1">log4j.logger.spark.CacheTracker=INFO, RollingAppender</strong><br class="title-page-name"/><strong class="calibre1">log4j.additivity.spark.CacheTracker=false</strong><br class="title-page-name"/><strong class="calibre1">log4j.logger.spark.CacheTrackerActor=INFO, RollingAppender</strong><br class="title-page-name"/><strong class="calibre1">log4j.additivity.spark.CacheTrackerActor=false</strong><br class="title-page-name"/><strong class="calibre1">log4j.logger.spark.MapOutputTrackerActor=INFO, RollingAppender</strong><br class="title-page-name"/><strong class="calibre1">log4j.additivity.spark.MapOutputTrackerActor=false</strong><br class="title-page-name"/><strong class="calibre1">log4j.logger.spark.MapOutputTracker=INFO, RollingAppender</strong><br class="title-page-name"/><strong class="calibre1">log4j.additivty.spark.MapOutputTracker=false</strong>
</pre>
<p class="mce-root">Basically, we want to hide all logs Spark generates so that we don't have to deal with them in the shell. We redirect them to be logged in the filesystem. On the other hand, we want our own logs to be logged in the shell and a separate file so that they don't get mixed up with the ones from Spark. From here, we will point Splunk to the files where our own logs are, which in this particular case is <kbd class="calibre11">/var/log/sparkU.log</kbd><em class="calibre8">.</em></p>
<p class="mce-root">Â </p>
<p class="mce-root">Then the<kbd class="calibre11">log4j.properties</kbd> file is picked up by Spark when the application starts, so we don't have to do anything aside from placing it in the mentioned location.</p>
<p class="mce-root">Now let's see how we can create our own logging system. Look at the following code and try to understand what is happening here:</p>
<pre class="calibre19">
import org.apache.spark.{SparkConf, SparkContext}<br class="title-page-name"/>import org.apache.log4j.LogManager<br class="title-page-name"/>import org.apache.log4j.Level<br class="title-page-name"/>import org.apache.log4j.Logger<br class="title-page-name"/><br class="title-page-name"/>object MyLog {<br class="title-page-name"/> def main(args: Array[String]):Unit= {<br class="title-page-name"/>   // Stting logger level as WARN<br class="title-page-name"/>   val log = LogManager.getRootLogger<br class="title-page-name"/>   log.setLevel(Level.WARN)<br class="title-page-name"/><br class="title-page-name"/>   // Creating Spark Context<br class="title-page-name"/>   val conf = new SparkConf().setAppName("My App").setMaster("local[*]")<br class="title-page-name"/>   val sc = new SparkContext(conf)<br class="title-page-name"/><br class="title-page-name"/>   //Started the computation and printing the logging information<br class="title-page-name"/>   log.warn("Started")                        <br class="title-page-name"/>   val data = sc.parallelize(1 to 100000)<br class="title-page-name"/>   log.warn("Finished")<br class="title-page-name"/> }<br class="title-page-name"/>}
</pre>
<p class="mce-root">The preceding code conceptually logs only the warning message. It first prints the warning message and then creates an RDD by parallelizing numbers from 1 to 100,000. Once the RDD job is finished, it prints another warning log. However, there is a problem we haven't noticed yet with the earlier code segment.</p>
<p class="mce-root">One drawback of the <kbd class="calibre11">org.apache.log4j.Logger</kbd> class is that it is not serializable (refer to the optimization technique section for more details), which implies that we cannot use it inside a <em class="calibre8">closure</em> while doing operations on some parts of the Spark API. For example, if you try to execute the following code, you should experience an exception that says Task not serializable:</p>
<pre class="calibre19">
object MyLog {<br class="title-page-name"/>  def main(args: Array[String]):Unit= {<br class="title-page-name"/>    // Stting logger level as WARN<br class="title-page-name"/>    val log = LogManager.getRootLogger<br class="title-page-name"/>    log.setLevel(Level.WARN)<br class="title-page-name"/>    // Creating Spark Context<br class="title-page-name"/>    val conf = new SparkConf().setAppName("My App").setMaster("local[*]")<br class="title-page-name"/>    val sc = new SparkContext(conf)<br class="title-page-name"/>    //Started the computation and printing the logging information<br class="title-page-name"/>    log.warn("Started")<br class="title-page-name"/>    val i = 0<br class="title-page-name"/>    val data = sc.parallelize(i to 100000)<br class="title-page-name"/>    data.foreach(i =&gt; log.info("My number"+ i))<br class="title-page-name"/>    log.warn("Finished")<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">To solve this problem is also easy; just declare the Scala object with <kbd class="calibre11">extends Serializable</kbd> and now the code looks like the following:</p>
<pre class="calibre19">
class MyMapper(n: Int) extends Serializable{<br class="title-page-name"/>  @transient lazy val log = org.apache.log4j.LogManager.getLogger<br class="title-page-name"/>                                ("myLogger")<br class="title-page-name"/>  def MyMapperDosomething(rdd: RDD[Int]): RDD[String] =<br class="title-page-name"/>   rdd.map{ i =&gt;<br class="title-page-name"/>    log.warn("mapping: " + i)<br class="title-page-name"/>    (i + n).toString<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">So what is happening in the preceding code is that the closure can't be neatly distributed to all partitions since it can't close on the logger; hence, the whole instance of type <kbd class="calibre11">MyMapper</kbd> is distributed to all partitions; once this is done, each partition creates a new logger and uses it for logging.</p>
<p class="mce-root">In summary, the following is the complete code that helps us to get rid of this problem:</p>
<pre class="calibre19">
package com.example.Personal<br class="title-page-name"/>import org.apache.log4j.{Level, LogManager, PropertyConfigurator}<br class="title-page-name"/>import org.apache.spark._<br class="title-page-name"/>import org.apache.spark.rdd.RDD<br class="title-page-name"/><br class="title-page-name"/>class MyMapper(n: Int) extends Serializable{<br class="title-page-name"/>  @transient lazy val log = org.apache.log4j.LogManager.getLogger<br class="title-page-name"/>                                ("myLogger")<br class="title-page-name"/>  def MyMapperDosomething(rdd: RDD[Int]): RDD[String] =<br class="title-page-name"/>   rdd.map{ i =&gt;<br class="title-page-name"/>    log.warn("Serialization of: " + i)<br class="title-page-name"/>    (i + n).toString<br class="title-page-name"/>  }<br class="title-page-name"/>}<br class="title-page-name"/><br class="title-page-name"/>object MyMapper{<br class="title-page-name"/>  def apply(n: Int): MyMapper = new MyMapper(n)<br class="title-page-name"/>}<br class="title-page-name"/><br class="title-page-name"/>object MyLog {<br class="title-page-name"/>  def main(args: Array[String]) {<br class="title-page-name"/>    val log = LogManager.getRootLogger<br class="title-page-name"/>    log.setLevel(Level.WARN)<br class="title-page-name"/>    val conf = new SparkConf().setAppName("My App").setMaster("local[*]")<br class="title-page-name"/>    val sc = new SparkContext(conf)<br class="title-page-name"/>    log.warn("Started")<br class="title-page-name"/>    val data = sc.parallelize(1 to 100000)<br class="title-page-name"/>    val mapper = MyMapper(1)<br class="title-page-name"/>    val other = mapper.MyMapperDosomething(data)<br class="title-page-name"/>    other.collect()<br class="title-page-name"/>    log.warn("Finished")<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">The output is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">17/04/29 15:33:43 WARN root: Started </strong><br class="title-page-name"/><strong class="calibre1">.</strong><br class="title-page-name"/><strong class="calibre1">.</strong><br class="title-page-name"/><strong class="calibre1">17/04/29 15:31:51 WARN myLogger: mapping: 1 </strong><br class="title-page-name"/><strong class="calibre1">17/04/29 15:31:51 WARN myLogger: mapping: 49992</strong><br class="title-page-name"/><strong class="calibre1">17/04/29 15:31:51 WARN myLogger: mapping: 49999</strong><br class="title-page-name"/><strong class="calibre1">17/04/29 15:31:51 WARN myLogger: mapping: 50000 </strong><br class="title-page-name"/><strong class="calibre1">.</strong><br class="title-page-name"/><strong class="calibre1">.                                                                                </strong><br class="title-page-name"/><strong class="calibre1">17/04/29 15:31:51 WARN root: Finished</strong>
</pre>
<p class="mce-root">We will discuss the built-in logging of Spark in the next section.</p>


            </article>

            
        </section>
    

        <section id="EM55A1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Spark configuration</h1>
                
            
            <article>
                
<p class="mce-root">There are a number of ways to configure your Spark jobs. In this section, we will discuss these ways. More specifically, according to Spark 2.x release, there are three locations to configure the system:</p>
<ul class="calibre9">
<li class="mce-root1">Spark properties</li>
<li class="mce-root1">Environmental variables</li>
<li class="mce-root1">Logging</li>
</ul>


            </article>

            
        </section>
    

        <section id="EN3LS1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Spark properties</h1>
                
            
            <article>
                
<p class="mce-root">As discussed previously, Spark properties control most of the application-specific parameters and can be set using a <kbd class="calibre11">SparkConf</kbd> object of Spark. Alternatively, these parameters can be set through the Java system properties. <kbd class="calibre11">SparkConf</kbd> allows you to configure some of the common properties as follows:</p>
<pre class="calibre19">
setAppName() // App name <br class="title-page-name"/>setMaster() // Master URL <br class="title-page-name"/>setSparkHome() // Set the location where Spark is installed on worker nodes. <br class="title-page-name"/>setExecutorEnv() // Set single or multiple environment variables to be used when launching executors. <br class="title-page-name"/>setJars() // Set JAR files to distribute to the cluster. <br class="title-page-name"/>setAll() // Set multiple parameters together.
</pre>
<p class="mce-root">An application can be configured to use a number of available cores on your machine. For example, we could initialize an application with two threads as follows. Note that we run with <kbd class="calibre11">local [2]</kbd>, meaning two threads, which represents minimal parallelism and using <kbd class="calibre11">local [*]</kbd>, which utilizes all the available cores in your machine. Alternatively, you can specify the number of executors while submitting Spark jobs with the following spark-submit script:</p>
<pre class="calibre19">
val conf = new SparkConf() <br class="title-page-name"/>             .setMaster("local[2]") <br class="title-page-name"/>             .setAppName("SampleApp") <br class="title-page-name"/>val sc = new SparkContext(conf)
</pre>
<p class="mce-root">There might be some special cases where you need to load Spark properties dynamically when required. You can do this while submitting a Spark job through the spark-submit script. More specifically, you may want to avoid hardcoding certain configurations in <kbd class="calibre11">SparkConf</kbd>.</p>
<div class="packt_infobox"><span class="field">Apache Spark precedence:</span><br class="calibre23"/>
Spark has the following precedence on the submitted jobs: configs coming from a config file have the lowest priority. The configs coming from the actual code have higher priority with respect to configs coming from a config file, and configs coming from the CLI through the Spark-submit script have higher priority.</div>
<p class="mce-root">For instance, if you want to run your application with different masters, executors, or different amounts of memory, Spark allows you to simply create an empty configuration object, as follows:</p>
<pre class="calibre19">
val sc = new SparkContext(new SparkConf())
</pre>
<p class="mce-root">Then you can provide the configuration for your Spark job at runtime as follows:</p>
<pre class="calibre19">
<strong class="calibre1">SPARK_HOME/bin/spark-submit </strong><br class="title-page-name"/><strong class="calibre1"> --name "SmapleApp" \</strong><br class="title-page-name"/><strong class="calibre1"> --class org.apache.spark.examples.KMeansDemo \</strong><br class="title-page-name"/><strong class="calibre1"> --master mesos://207.184.161.138:7077 \ # Use your IP address</strong><br class="title-page-name"/><strong class="calibre1"> --conf spark.eventLog.enabled=false </strong><br class="title-page-name"/><strong class="calibre1"> --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails" \    </strong><br class="title-page-name"/><strong class="calibre1"> --deploy-mode cluster \</strong><br class="title-page-name"/><strong class="calibre1"> --supervise \</strong><br class="title-page-name"/><strong class="calibre1"> --executor-memory 20G \</strong><br class="title-page-name"/><strong class="calibre1"> myApp.jar</strong>
</pre>
<p class="mce-root"><kbd class="calibre11">SPARK_HOME/bin/spark-submit</kbd> will also read configuration options from <kbd class="calibre11">SPARK_HOME /conf/spark-defaults.conf</kbd>, in which each line consists of a key and a value separated by whitespace. An example is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">spark.master  spark://5.6.7.8:7077 </strong><br class="title-page-name"/><strong class="calibre1">spark.executor.memor y   4g  </strong><br class="title-page-name"/><strong class="calibre1">spark.eventLog.enabled true </strong><br class="title-page-name"/><strong class="calibre1">spark.serializer org.apache.spark.serializer.KryoSerializer</strong>
</pre>
<p class="mce-root">Values that are specified as flags in the properties file will be passed to the application and merged with those ones specified through <kbd class="calibre11">SparkConf</kbd>. Finally, as discussed earlier, the application web UI at <kbd class="calibre11">http://&lt;driver&gt;:4040</kbd> lists all the Spark properties under the <span>Environment</span> tab.</p>


            </article>

            
        </section>
    

        <section id="EO26E1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Environmental variables</h1>
                
            
            <article>
                
<p class="mce-root">Environment variables can be used to set the setting in the computing nodes or machine settings. For example, IP address can be set through the <kbd class="calibre11">conf/spark-env.sh</kbd> script on each computing node. The following table lists the name and the functionality of the environmental variables that need to be set:</p>
<div class="cdpaligncenter"><img class="image-border228" src="../images/00116.gif"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 18:</strong> Environmental variables and their meaning</div>


            </article>

            
        </section>
    

        <section id="EP0N01-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Logging</h1>
                
            
            <article>
                
<p class="mce-root">Finally, logging can be configured through the <kbd class="calibre11">log4j.properties</kbd> file under your Spark application tree, as discussed in the preceding section. Spark uses log4j for logging. There are several valid logging levels supported by log4j with Spark; they are as follows:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Log Level</strong></td>
<td class="calibre7"><strong class="calibre1">Usages</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">OFF</td>
<td class="calibre7">This is the most specific, which allows no logging at all</td>
</tr>
<tr class="calibre6">
<td class="calibre7">FATAL</td>
<td class="calibre7">This is the most specific one that shows fatal errors with little data</td>
</tr>
<tr class="calibre6">
<td class="calibre7">ERROR</td>
<td class="calibre7">This shows only the general errors</td>
</tr>
<tr class="calibre6">
<td class="calibre7">WARN</td>
<td class="calibre7">This shows warnings that are recommended to be fixed but not mandatory</td>
</tr>
<tr class="calibre6">
<td class="calibre7">INFO</td>
<td class="calibre7">This shows information required for your Spark job</td>
</tr>
<tr class="calibre6">
<td class="calibre7">DEBUG</td>
<td class="calibre7">While debugging, those logs will be printed</td>
</tr>
<tr class="calibre6">
<td class="calibre7">TRACE</td>
<td class="calibre7">This provides the least specific error trace with a lot of data</td>
</tr>
<tr class="calibre6">
<td class="calibre7">ALL</td>
<td class="calibre7">Least specific message with all data</td>
</tr>
</tbody>
</table>
<div class="cdpaligncenter1"><strong class="calibre1">Table 1:</strong> Log level with log4j and Spark</div>
<p class="mce-root">You can set up the default logging for Spark shell in <kbd class="calibre11">conf/log4j.properties</kbd>. In standalone Spark applications or while in a Spark Shell session, use <kbd class="calibre11">conf/log4j.properties.template</kbd> as a starting point. In an earlier section of this chapter, we suggested you put the <kbd class="calibre11">log4j.properties</kbd> file under your project directory while working on an IDE-based environment like Eclipse. However, to disable logging completely, you should use the following <kbd class="calibre11">conf/log4j.properties.template</kbd> as <kbd class="calibre11">log4j.properties</kbd> . Just set the <kbd class="calibre11">log4j.logger.org</kbd> flags as OFF, as follows:</p>
<pre class="calibre19">
log4j.logger.org=OFF
</pre>
<p class="mce-root">In the next section, we will discuss some common mistakes made by the developer or programmer while developing and submitting Spark jobs.</p>


            </article>

            
        </section>
    

        <section id="EPV7I1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Common mistakes in Spark app development</h1>
                
            
            <article>
                
<p class="mce-root">Common mistakes that happen often are application failure, a slow job that gets stuck due to numerous factors, mistakes in the aggregation, actions or transformations, an exception in the main thread and, of course, <strong class="calibre1">Out Of Memory</strong> (<strong class="calibre1">OOM</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header id="EQTO42-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">Application failure</h1>
                
            
            <article>
                
<p class="mce-root">Most of the time, application failure happens because one or more stages fail eventually. As discussed earlier in this chapter, Spark jobs comprise several stages. Stages aren't executed independently: for instance, a processing stage can't take place before the relevant input-reading stage. So, suppose that stage 1 executes successfully but stage 2 fails to execute, the whole application fails eventually. This can be shown as follows:</p>
<div class="cdpaligncenter"><img class="image-border229" src="../images/00119.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 19:</strong> Two stages in a typical Spark job</div>
<p class="mce-root">To show an example, suppose you have the following three RDD operations as stages. The same can be visualized as shown in <em class="calibre8">Figure 20</em>, <em class="calibre8">Figure 21</em>, and <em class="calibre8">Figure 22</em>:</p>
<pre class="calibre19">
val rdd1 = sc.textFile(âhdfs://data/data.csvâ)<br class="title-page-name"/>                       .map(someMethod)<br class="title-page-name"/>                       .filter(filterMethod)   
</pre>
<div class="cdpaligncenter"><img class="image-border230" src="../images/00157.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 20:</strong> Stage 1 for rdd1</div>
<pre class="calibre19">
val rdd2 = sc.hadoopFile(âhdfs://data/data2.csvâ)<br class="title-page-name"/>                      .groupByKey()<br class="title-page-name"/>                      .map(secondMapMethod)
</pre>
<p class="mce-root">Conceptually, this can be shown in <em class="calibre8">Figure 21</em>, which first parses the data using the <kbd class="calibre11">hadoopFile()</kbd> method, groups it using the <kbd class="calibre11">groupByKey()</kbd> method, and finally, maps it:</p>
<div class="cdpaligncenter"><img class="image-border231" src="../images/00090.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 21:</strong> Stage 2 for rdd2</div>
<pre class="calibre19">
val rdd3 = rdd1.join(rdd2).map(thirdMapMethod)
</pre>
<p class="cdpalignleft1">Conceptually, this can be shown in <em class="calibre8">Figure 22</em>, which first parses the data, joins it, and finally, maps it:</p>
<div class="cdpaligncenter"><img class="image-border232" src="../images/00144.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 22:</strong> Stage 3 for rdd3</div>
<p class="mce-root">Now you can perform an aggregation function, for example, collect, as follows:</p>
<pre class="calibre19">
rdd3.collect()
</pre>
<p class="mce-root">Well! You have developed a Spark job consisting of three stages. Conceptually, this can be shown as follows:</p>
<div class="cdpaligncenter"><img class="image-border233" src="../images/00041.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 23:</strong> three stages for the rdd3.collect() operation</div>
<p class="mce-root">Now, if one of the stages fails, your job will fail eventually. As a result, the final <kbd class="calibre11">rdd3.collect()</kbd> <span>statement</span> wi<span>ll throw an exception about stage failure. Moreover, you may have issues with the following four factors:</span></p>
<ul class="calibre9">
<li class="mce-root1">Mistakes in the aggregation operation</li>
<li class="mce-root1">Exceptions in the main thread</li>
<li class="mce-root1">OOP</li>
<li class="mce-root1">Class not found exception while submitting jobs using the <kbd class="calibre11">spark-submit</kbd> script</li>
<li class="mce-root1">Misconception about some API/methods in Spark core library</li>
</ul>
<p class="mce-root">To get rid of the aforementioned issues, our general suggestion is to ensure that you have not made any mistakes while performing any map, flatMap, or aggregate operations. Second, ensure that there are no flaws in the main method while developing your application with Java or Scala. Sometimes you don't see any syntax error in your code, but it's important that you have developed some small test cases for your application. Most common exceptions that occur in the main method are as follows:</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">java.lang.noclassdeffounderror</kbd></li>
<li class="mce-root1"><kbd class="calibre11">java.lang.nullpointerexception</kbd></li>
<li class="mce-root1"><kbd class="calibre11">java.lang.arrayindexoutofboundsexception</kbd></li>
<li class="mce-root1"><kbd class="calibre11">java.lang.stackoverflowerror</kbd></li>
<li class="mce-root1"><kbd class="calibre11">java.lang.classnotfoundexception</kbd></li>
<li class="mce-root1"><kbd class="calibre11">java.util.inputmismatchexception</kbd></li>
</ul>
<p class="mce-root">These exceptions can be avoided with the careful coding of your Spark application. Alternatively, use Eclipse's (or any other IDEs) code debugging features extensively to get rid of the semantic error to avoid the exception. For the third problem, that is, OOM, it's a very common problem. It is to be noted that Spark requires at least 8 GB of main memory, with sufficient disk space available for the standalone mode. On the other hand, to get the full cluster computing facilities, this requirement is often high.</p>
<div class="packt_tip">Preparing a JAR file including all the dependencies to execute Spark jobs is of paramount importance. Many practitioners use Google's Guava; it is included in most distributions, yet it doesn't guarantee backward compatibility. This means that sometimes your Spark job won't find a Guava class even if you explicitly provided it; this happens because one of the two versions of the Guava libraries takes precedence over the other, and this version might not include a required class. In order to overcome this issue, you usually resort to shading.</div>
<p class="mce-root">Make sure that you have set the Java heap space with âXmx parameter with a sufficiently large value if you're coding using IntelliJ, Vim, Eclipse, Notepad, and so on. While working with cluster mode, you should specify the executor memory while submitting Spark jobs using the Spark-submit script. Suppose you have a CSV to be parsed and do some predictive analytics using a random forest classifier, you might need to specify the right amount of memory, say 20 GB, as follows:</p>
<pre class="calibre19">
<strong class="calibre1">--executor-memory 20G</strong>
</pre>
<p class="mce-root">Even if you receive the OOM error, you can increase this amount to, say, 32 GB or more. Since random forest is computationally intensive, requiring larger memory, this is just an example of random forest. You might experience similar issues while just parsing your data. Even a particular stage may fail due to this OOM error. Therefore, make sure that you are aware of this error.</p>
<p class="mce-root">For the <kbd class="calibre11">class not found exception</kbd>, make sure that you have included your main class in the resulting JAR file. The JAR file should be prepared with all the dependencies to execute your Spark job on the cluster nodes. We will provide a step-by-step JAR preparation guideline in <a href="part0511.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 17</a>, <em class="calibre8">Time to Go to ClusterLand - Deploying Spark on a Cluster.</em></p>
<p class="mce-root">For the last issue, we can provide some examples of some misconceptions about Spark Core library. For example, when you use the <kbd class="calibre11">wholeTextFiles</kbd> method to prepare RDDs or DataFrames from multiple files, Spark does not run in parallel; in cluster mode for YARN, it may run out of memory sometimes.</p>
<p class="mce-root">Once, I experienced an issue where, at first, I copied six files in my S3 storage to HDFS. Then, I tried to create an RDD, as follows:</p>
<pre class="calibre19">
sc.wholeTextFiles("/mnt/temp") // note the location of the data files is /mnt/temp/
</pre>
<p class="mce-root">Then, I tried to process those files line by line using a UDF. When I looked at my computing nodes, I saw that only one executor was running per file. However, I then got an error message saying that YARN had run out of memory. Why so? The reasons are as follows:</p>
<ul class="calibre9">
<li class="mce-root1">The goal of <kbd class="calibre11">wholeTextFiles</kbd> is to have only one executor for each file to be processed</li>
<li class="mce-root1">If you use <kbd class="calibre11">.gz</kbd> files, for example, you will have only one executor per file, maximum</li>
</ul>


            </article>

            
        </section>
    

        <section id="ERS8M1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Slow jobs or unresponsiveness</h1>
                
            
            <article>
                
<p class="mce-root">Sometimes, if the SparkContext cannot connect to a Spark standalone master, then the driver may display errors such as the following:</p>
<pre class="calibre19">
<strong class="calibre1">02/05/17 12:44:45 ERROR AppClient$ClientActor: All masters are unresponsive! Giving up. </strong><br class="title-page-name"/><strong class="calibre1">02/05/17 12:45:31 ERROR SparkDeploySchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up. </strong><br class="title-page-name"/><strong class="calibre1">02/05/17 12:45:35 ERROR TaskSchedulerImpl: Exiting due to error from cluster scheduler: Spark cluster looks down</strong>
</pre>
<p class="mce-root">At other times, the driver is able to connect to the master node but the master is unable to communicate back to the driver. Then, multiple attempts to connect are made even though the driver will report that it could not connect to the Master's log directory.</p>
<p class="mce-root">Furthermore, you might often experience very slow performance and progress in your Spark jobs. This happens because your driver program is not that fast to compute your jobs. As discussed earlier, sometimes a particular stage may take a longer time than usual because there might be a shuffle, map, join, or aggregation operation involved. Even if the computer is running out of disk storage or main memory, you may experience these issues. For example, if your master node does not respond or you experience unresponsiveness from the computing nodes for a certain period of time, you might think that your Spark job has halted and become stagnant at a certain stage:</p>
<div class="cdpaligncenter"><img class="image-border234" src="../images/00224.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 24:</strong> An example log for executor/driver unresponsiveness</div>
<p class="mce-root">Potential solutions could be several, including the following:</p>
<ol class="calibre14">
<li value="1" class="mce-root1">Check to make sure that workers and drivers are correctly configured to connect to the Spark master on the exact address listed in the Spark master web UI/logs. Then, explicitly supply the Spark cluster's master URL when starting your Spark shell:</li>
</ol>
<pre class="calibre19">
<strong class="calibre1">      $ bin/spark-shell --master spark://master-ip:7077</strong>
</pre>
<ol start="2" class="calibre14">
<li value="2" class="mce-root1">Set <kbd class="calibre11">SPARK_LOCAL_IP</kbd> to a cluster-addressable hostname for the driver, master, and worker processes.</li>
</ol>
<p class="mce-root">Sometimes, we experience some issues due to hardware failure. For example, if the filesystem in a computing node closes <span>unexpectedly,</span> that is, an I/O exception, your Spark job will eventually fail too. This is obvious because your Spark job cannot write the resulting RDDs or data to store to the local filesystem or HDFS. This also implies that DAG operations cannot be performed due to the stage failures.</p>
<p class="mce-root">Sometimes, this I/O exception occurs due to an underlying disk failure or other hardware failures. This often provides logs, as follows:</p>
<div class="cdpaligncenter"><img class="image-border235" src="../images/00048.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 25:</strong> An example filesystem closed</div>
<p class="mce-root">Nevertheless, you often experience slow job computing performance because your Java GC is somewhat busy with, or cannot do, the GC fast. For example, the following figure shows that for task 0, it took 10 hours to finish the GC! I experienced this issue in 2014, when I was new to Spark. Control of these types of issues, however, is not in our hands. Therefore, our recommendation is that you should make the JVM free and try submitting the jobs again.</p>
<div class="cdpaligncenter"><img class="image-border236" src="../images/00352.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 26:</strong> An example where GC stalled in between</div>
<p class="mce-root">The fourth factor could be the slow response or slow job performance is due to the lack of data serialization. This will be discussed in the next section. The fifth factor could be the memory leak in the code that will tend to make your application consume more memory, leaving the files or logical devices open. Therefore, make sure that there is no option that tends to be a memory leak. For example, it is a good practice to finish your Spark application by calling <kbd class="calibre11">sc.stop()</kbd> or <kbd class="calibre11">spark.stop()</kbd>. This will make sure that one SparkContext is still open and active. Otherwise, you might get unwanted exceptions or issues. The sixth issue is that we often keep too many open files, and this sometimes creates <kbd class="calibre11">FileNotFoundException</kbd> in the shuffle or merge stage.</p>


            </article>

            
        </section>
    

        <section id="ESQP81-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Optimization techniques</h1>
                
            
            <article>
                
<p class="mce-root">There are several aspects of tuning Spark applications toward better optimization techniques. In this section, we will discuss how we can further optimize our Spark applications by applying data serialization by tuning the main memory with better memory management. We can also optimize performance by tuning the data structure in your Scala code while developing Spark applications. The storage, on the other hand, can be maintained well by utilizing serialized RDD storage.</p>
<p class="mce-root">One of the most important aspects is garbage collection, and it's tuning if you have written your Spark application using Java or Scala. We will look at how we can also tune this for optimized performance. For distributed environment- and cluster-based system, a level of parallelism and data locality has to be ensured. Moreover, performance could further be improved by using broadcast variables.</p>


            </article>

            
        </section>
    

        <section id="ETP9Q1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data serialization</h1>
                
            
            <article>
                
<p class="mce-root">Serialization is an important tuning for performance improvement and optimization in any distributed computing environment. Spark is not an exception, but Spark jobs are often data and computing extensive. Therefore, if your data objects are not in a good format, then you first need to convert them into serialized data objects. This demands a large number of bytes of your memory. Eventually, the whole process will slow down the entire processing and computation drastically.</p>
<p class="mce-root">As a result, you often experience a slow response from the computing nodes. This means that we sometimes fail to make 100% utilization of the computing resources. It is true that Spark tries to keep a balance between convenience and performance. This also implies that data serialization should be the first step in Spark tuning for better performance.</p>
<p class="mce-root">Spark provides two options for data serialization: Java serialization and Kryo serialization libraries:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Java serialization:</strong> Spark serializes objects using Java's <kbd class="calibre11">ObjectOutputStream</kbd> framework. You handle the serialization by creating any class that implements <kbd class="calibre11">java.io.Serializable</kbd>. Java serialization is very flexible but often quite slow, which is not suitable for large data object serialization.</li>
<li class="mce-root1"><strong class="calibre1">Kryo serialization:</strong> You can also use Kryo library to serialize your data objects more quickly. Compared to Java serialization, Kryo serialization is much faster, with 10x speedup and is compact than that of Java. However, it has one issue, that is, it does not support all the serializable types, but you need to require your classes to be registered.</li>
</ul>
<p class="mce-root">You can start using Kryo by initializing your Spark job with a <kbd class="calibre11">SparkConf</kbd> and calling <kbd class="calibre11">conf.set(spark.serializer, org.apache.spark.serializer.KryoSerializer)</kbd>. To register your own custom classes with Kryo, use the <kbd class="calibre11">registerKryoClasses</kbd> method, as follows:</p>
<pre class="calibre19">
val conf = new SparkConf()<br class="title-page-name"/>               .setMaster(âlocal[*]â)<br class="title-page-name"/>               .setAppName(âMyAppâ)<br class="title-page-name"/>conf.registerKryoClasses(Array(classOf[MyOwnClass1], classOf[MyOwnClass2]))<br class="title-page-name"/>val sc = new SparkContext(conf)
</pre>
<p class="mce-root">If your objects are large, you may also need to increase the <kbd class="calibre11">spark.kryoserializer.buffer</kbd> config. This value needs to be large enough to hold the largest object you serialize. Finally, if you don't register your custom classes, Kryo still works; however, the full class name with each object needs to be stored, which is wasteful indeed.</p>
<p class="mce-root">For example, in the logging subsection at the end of the monitoring Spark jobs section, the logging and computing can be optimized using the <kbd class="calibre11">Kryo</kbd> serialization. At first, just create the <kbd class="calibre11">MyMapper</kbd> class as a normal class (that is, without any serialization), as follows:</p>
<pre class="calibre19">
class MyMapper(n: Int) { // without any serialization<br class="title-page-name"/>  @transient lazy val log = org.apache.log4j.LogManager.getLogger("myLogger")<br class="title-page-name"/>  def MyMapperDosomething(rdd: RDD[Int]): RDD[String] = rdd.map { i =&gt;<br class="title-page-name"/>    log.warn("mapping: " + i)<br class="title-page-name"/>    (i + n).toString<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">Now, let's register this class as a <kbd class="calibre11">Kyro</kbd> serialization class and then set the <kbd class="calibre11">Kyro</kbd> serialization as follows:</p>
<pre class="calibre19">
conf.registerKryoClasses(Array(classOf[MyMapper])) // register the class with Kyro<br class="title-page-name"/>conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") // set Kayro serialization
</pre>
<p class="mce-root">That's all you need. The full source code of this example is given in the following. You should be able to run and observe the same output, but an optimized one as compared to the previous example:</p>
<pre class="calibre19">
package com.chapter14.Serilazition<br class="title-page-name"/>import org.apache.spark._<br class="title-page-name"/>import org.apache.spark.rdd.RDD<br class="title-page-name"/>class MyMapper(n: Int) { // without any serilization<br class="title-page-name"/>  @transient lazy val log = org.apache.log4j.LogManager.getLogger<br class="title-page-name"/>                                ("myLogger")<br class="title-page-name"/>  def MyMapperDosomething(rdd: RDD[Int]): RDD[String] = rdd.map { i =&gt;<br class="title-page-name"/>    log.warn("mapping: " + i)<br class="title-page-name"/>    (i + n).toString<br class="title-page-name"/>  }<br class="title-page-name"/>}<br class="title-page-name"/>//Companion object<br class="title-page-name"/>object MyMapper {<br class="title-page-name"/>  def apply(n: Int): MyMapper = new MyMapper(n)<br class="title-page-name"/>}<br class="title-page-name"/>//Main object<br class="title-page-name"/>object KyroRegistrationDemo {<br class="title-page-name"/>  def main(args: Array[String]) {<br class="title-page-name"/>    val log = LogManager.getRootLogger<br class="title-page-name"/>    log.setLevel(Level.WARN)<br class="title-page-name"/>    val conf = new SparkConf()<br class="title-page-name"/>      .setAppName("My App")<br class="title-page-name"/>      .setMaster("local[*]")<br class="title-page-name"/>    conf.registerKryoClasses(Array(classOf[MyMapper2]))<br class="title-page-name"/>     // register the class with Kyro<br class="title-page-name"/>    conf.set("spark.serializer", "org.apache.spark.serializer<br class="title-page-name"/>             .KryoSerializer") // set Kayro serilazation<br class="title-page-name"/>    val sc = new SparkContext(conf)<br class="title-page-name"/>    log.warn("Started")<br class="title-page-name"/>    val data = sc.parallelize(1 to 100000)<br class="title-page-name"/>    val mapper = MyMapper(1)<br class="title-page-name"/>    val other = mapper.MyMapperDosomething(data)<br class="title-page-name"/>    other.collect()<br class="title-page-name"/>    log.warn("Finished")<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>
<p class="mce-root">The output is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">17/04/29 15:33:43 WARN root: Started <br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>17/04/29 15:31:51 WARN myLogger: mapping: 1 <br class="title-page-name"/>17/04/29 15:31:51 WARN myLogger: mapping: 49992<br class="title-page-name"/>17/04/29 15:31:51 WARN myLogger: mapping: 49999<br class="title-page-name"/>17/04/29 15:31:51 WARN myLogger: mapping: 50000 <br class="title-page-name"/>.<br class="title-page-name"/>.                                                                                <br class="title-page-name"/>17/04/29 15:31:51 WARN root: Finished</strong>
</pre>
<p class="mce-root">Well done! Now let's have a quick look at how to tune the memory. We will look at some advanced strategies to make sure the efficient use of the main memory in the next section.</p>


            </article>

            
        </section>
    

        <section id="EUNQC1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Memory tuning</h1>
                
            
            <article>
                
<p class="mce-root">In this section, we will discuss some advanced strategies that can be used by users like you to make sure that an efficient use of memory is carried out while executing your Spark jobs. More specifically, we will show how to calculate the memory usages of your objects. We will suggest some advanced ways to improve it by optimizing your data structures or by converting your data objects in a serialized format using Kryo or Java serializer. Finally, we will look at how to tune Spark's Java heap size, cache size, and the Java garbage collector.</p>
<p class="mce-root">There are three considerations in tuning memory usage:</p>
<ul class="calibre9">
<li class="mce-root1">The amount of memory used by your objects: You may even want your entire dataset to fit in the memory</li>
<li class="mce-root1">The cost of accessing those objects</li>
<li class="mce-root1">The overhead of garbage collection: If you have a high turnover in terms of objects</li>
</ul>
<p class="mce-root">Although Java objects are fast enough to access, they can easily consume a factor of 2 to 5x more space than the actual (aka raw) data in their original fields. For example, each distinct Java object has 16 bytes of overhead with an object header. A Java string, for example, has almost 40 bytes of extra overhead over the raw string. Furthermore, Java collection classes like <kbd class="calibre11">Set</kbd>, <kbd class="calibre11">List</kbd>, <kbd class="calibre11">Queue</kbd>, <span><kbd class="calibre11">ArrayList</kbd>, <kbd class="calibre11">Vector</kbd>, <kbd class="calibre11">LinkedList</kbd>, <kbd class="calibre11">PriorityQueue</kbd>, <kbd class="calibre11">HashSet</kbd>, <kbd class="calibre11">LinkedHashSet</kbd>, <kbd class="calibre11">TreeSet</kbd></span>, <span>and so on, are also used</span>. The linked data structures, on the other hand, are too complex, occupying too much extra space since there is a wrapper object for each entry in the data structure. Finally, the collections of primitive types frequently store them in the memory as boxed objects, such as <kbd class="calibre11">java.lang.Double</kbd> and <kbd class="calibre11">java.lang.Integer</kbd>.</p>


            </article>

            
        </section>
    

        <section id="EVMAU1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Memory usage and management</h1>
                
            
            <article>
                
<p class="mce-root">Memory usages by your Spark application and underlying computing nodes can be categorized as execution and storage. Execution memory is used during the computation in merge, shuffles, joins, sorts, and aggregations. On the other hand, storage memory is used for caching and propagating internal data across the cluster. In short, this is due to the large amount of I/O across the network.</p>
<div class="packt_infobox">Technically, Spark caches network data locally. While working with Spark iteratively or interactively, caching or persistence are optimization techniques in Spark. These two help in saving interim partial results so that they can be reused in subsequent stages. Then these interim results (as RDDs) can be kept in memory (default) or more solid storage, such as disk, and/or replicated. Furthermore, RDDs can be cached using cache operations too. They can also be persisted using a persist operation. The difference between cache and persist operations is purely syntactic. The cache is a synonym of persisting or persists (<kbd class="calibre22">MEMORY_ONLY</kbd>), that is, cache is merely persisted with the default storage level <kbd class="calibre22">MEMORY_ONLY</kbd>.</div>
<p class="mce-root">If you go under the Storage tab in your Spark web UI, you should observe the memory/storage used by an RDD, DataFrame, or Dataset object, as shown in <em class="calibre8">Figure 10</em>. Although there are two relevant configurations for tuning memory in Spark, users do not need to readjust them. The reason is that the default values set in the configuration files are enough for your requirements and workloads.</p>
<p class="mce-root">spark.memory.fraction is the size of the unified region as a fraction of (JVM heap space - 300 MB) (default 0.6). The rest of the space (40%) is reserved for user data structures, internal metadata in Spark, and safeguarding against OOM errors in case of sparse and unusually large records. On the other hand, <kbd class="calibre11">spark.memory.storageFraction</kbd> expresses the size of R storage space as a fraction of the unified region (default is 0.5). The default value of this parameter is 50% of Java heap space, that is, 300 MB.</p>
<div class="packt_infobox">A more detailed discussion on memory usage and storage is given in <a href="part0458.html#DKP1K1-21aec46d8593429cacea59dbdcd64e1c" class="calibre21">Chapter 15</a>, <em class="calibre25">Text Analytics Using Spark ML</em>.</div>
<p class="mce-root">Now, one question might arise in your mind: which storage level to choose? To answer this question, Spark storage levels provide you with different trade-offs between memory usage and CPU efficiency. If your RDDs fit comfortably with the default storage level (MEMORY_ONLY), let your Spark driver or master go with it. This is the most memory-efficient option, allowing operations on the RDDs to run as fast as possible. You should let it go with this, because this is the most memory-efficient option. This also allows numerous operations on the RDDs to be done as fast as possible.</p>
<p class="mce-root">If your RDDs do not fit the main memory, that is, if <kbd class="calibre11">MEMORY_ONLY</kbd> does not work out, you should try using <kbd class="calibre11">MEMORY_ONLY_SER</kbd>. It is strongly recommended to not spill your RDDs to disk unless your <strong class="calibre1">UDF</strong> (aka <strong class="calibre1">user-defined function</strong> that you have defined for processing your dataset) is too expensive. This also applies if your UDF filters a large amount of the data during the execution stages. In other cases, recomputing a partition, that is, repartition, may be faster for reading data objects from disk. Finally, if you want fast fault recovery, use the replicated storage levels.</p>
<p class="mce-root">In summary, there are the following StorageLevels available and supported in Spark 2.x: (number _2 in the name denotes 2 replicas):</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">DISK_ONLY</kbd>: This is for disk-based operation for RDDs</li>
<li class="mce-root1"><kbd class="calibre11">DISK_ONLY_2</kbd>: This is for disk-based operation for RDDs for 2 replicas</li>
<li class="mce-root1"><kbd class="calibre11">MEMORY_ONLY</kbd>: This is the default for cache operation in memory for RDDs</li>
<li class="mce-root1"><kbd class="calibre11">MEMORY_ONLY_2</kbd>: This is the default for cache operation in memory for RDDs with 2 replicas</li>
<li class="mce-root1"><kbd class="calibre11">MEMORY_ONLY_SER</kbd>: If your RDDs do not fit the main memory, that is, if <kbd class="calibre11">MEMORY_ONLY</kbd> does not work out, this option particularly helps in storing data objects in a serialized form</li>
<li class="mce-root1"><kbd class="calibre11">MEMORY_ONLY_SER_2</kbd>: If your RDDs do not fit the main memory, <span>that is,</span> if <kbd class="calibre11">MEMORY_ONLY</kbd> does not work out with 2 replicas, this option also helps in storing data objects in a serialized form</li>
<li class="mce-root1"><kbd class="calibre11">MEMORY_AND_DISK</kbd>: Memory and disk (aka combined) based RDD persistence</li>
<li class="mce-root1"><kbd class="calibre11">MEMORY_AND_DISK_2</kbd>: Memory and disk (aka combined) based RDD persistence with 2 replicas</li>
<li class="mce-root1"><kbd class="calibre11">MEMORY_AND_DISK_SER</kbd>: If <kbd class="calibre11">MEMORY_AND_DISK</kbd> does not work, it can be used</li>
<li class="mce-root1"><kbd class="calibre11">MEMORY_AND_DISK_SER_2</kbd>: If <kbd class="calibre11">MEMORY_AND_DISK</kbd> does not work with 2 replicas, this option can be used</li>
<li class="mce-root1"><kbd class="calibre11">OFF_HEAP</kbd>: Does not allow writing into Java heap space</li>
</ul>
<div class="packt_infobox">Note that cache is a synonym of persist (<kbd class="calibre22">MEMORY_ONLY</kbd>). This means that cache is solely persist with the default storage level, that is, <kbd class="calibre22">MEMORY_ONLY</kbd>. Detailed information can be found at <a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-StorageLevel.html" class="calibre21">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-StorageLevel.html</a>.</div>


            </article>

            
        </section>
    

        <section id="F0KRG1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Tuning the data structures</h1>
                
            
            <article>
                
<p class="mce-root">The first way to reduce extra memory usage is to avoid some features in the Java data structure that impose extra overheads. For example, pointer-based data structures and wrapper objects contribute to nontrivial overheads. To tune your source code with a better data structure, we provide some suggestions here, which can be useful.</p>
<p class="mce-root">First, design your data structures such that you use arrays of objects and primitive types more. Thus, this also suggests using standard Java or Scala collection classes like <kbd class="calibre11">Set</kbd>, <kbd class="calibre11">List</kbd>, <kbd class="calibre11">Queue</kbd>, <span><kbd class="calibre11">ArrayList</kbd>, <kbd class="calibre11">Vector</kbd>, <kbd class="calibre11">LinkedList</kbd>, <kbd class="calibre11">PriorityQueue</kbd>, <kbd class="calibre11">HashSet</kbd>, <kbd class="calibre11">LinkedHashSet</kbd>, and <kbd class="calibre11">TreeSet</kbd></span> more frequently.</p>
<p class="mce-root">Second, when possible, avoid using nested structures with a lot of small objects and pointers so that your source code becomes more optimized and concise. Third, when possible, consider using numeric IDs and sometimes using enumeration objects rather than using strings for keys. This is recommended because, as we have already stated, a single Java string object creates an extra overhead of 40 bytes. Finally, if you have less than 32 GB of main memory (that is, RAM), set the JVM flag <kbd class="calibre11">-XX:+UseCompressedOops</kbd> to make pointers 4 bytes instead of 8.</p>
<div class="packt_infobox">The earlier option can be set in the <kbd class="calibre22">SPARK_HOME/conf/spark-env.sh.template</kbd>. Just rename the file as <kbd class="calibre22">spark-env.sh</kbd> and set the value straight away!</div>


            </article>

            
        </section>
    

        <section id="F1JC21-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Serialized RDD storage</h1>
                
            
            <article>
                
<p class="mce-root">As discussed already, despite other types of memory tuning, when your objects are too large to fit in the main memory or disk efficiently, a simpler and better way of reducing memory usage is storing them in a serialized form.</p>
<div class="packt_tip">This can be done using the serialized storage levels in the RDD persistence API, such as <kbd class="calibre22">MEMORY_ONLY_SER</kbd>. For more information, refer to the previous section on memory management and start exploring available options.</div>
<p class="mce-root">If you specify using <kbd class="calibre11">MEMORY_ONLY_SER</kbd>, Spark will then store each RDD partition as one large byte array. However, the only downside of this approach is that it can slow down data access times. This is reasonable and obvious too; fairly speaking, there's no way to avoid it since each object needs to deserialize on the fly back while reusing.</p>
<div class="packt_tip">As discussed previously, we highly recommend using Kryo serialization instead of Java serialization to make data access a bit faster.</div>


            </article>

            
        </section>
    

        <section id="F2HSK1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Garbage collection tuning</h1>
                
            
            <article>
                
<p class="mce-root">Although it is not a major problem in your Java or Scala programs that just read an RDD sequentially or randomly once and then execute numerous operations on it, <strong class="calibre1">Java Virtual Machine</strong> (<strong class="calibre1">JVM</strong>) GC can be problematic and complex if you have a large amount of data objects w.r.t RDDs stored in your driver program. When the JVM needs to remove obsolete and unused objects from the old objects to make space for the newer ones, it is mandatory to identify them and remove them from the memory eventually. However, this is a costly operation in terms of processing time and storage. You might be wondering that the cost of GC is proportional to the number of Java objects stored in your main memory. Therefore, we strongly suggest you tune your data structure. Also, having fewer objects stored in your memory is recommended.</p>
<p class="mce-root">The first step in GC tuning is collecting the related statistics on how frequently garbage collection by JVM occurs on your machine. The second statistic needed in this regard is the amount of time spent on GC by JVM on your machine or computing nodes. This can be achieved by adding <kbd class="calibre11">-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps</kbd> to the Java options in your IDE, such as Eclipse, in the JVM startup arguments and specifying a name and location for our GC log file, as follows:</p>
<div class="cdpaligncenter"><img class="image-border237" src="../images/00213.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 27:</strong> Setting GC verbose on Eclipse</div>
<p class="mce-root">Alternatively, you can specify <kbd class="calibre11">verbose:gc</kbd> while submitting your Spark jobs using the Spark-submit script, as follows:</p>
<pre class="calibre19">
<strong class="calibre1">--conf âspark.executor.extraJavaOptions = -verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps"</strong>
</pre>
<p class="mce-root">In short, when specifying GC options for Spark, you must determine where you want the GC options specified, on the executors or on the driver. When you submit your jobs, specify <kbd class="calibre11">--driver-java-options -XX:+PrintFlagsFinal -verbose:gc</kbd> and so on. For the executor, specify <kbd class="calibre11">--conf spark.executor.extraJavaOptions=-XX:+PrintFlagsFinal -verbose:gc</kbd> and so on.</p>
<p class="mce-root">Now, when your Spark job is executed, you will be able to see the logs and messages printed in the worker's node at <kbd class="calibre11">/var/log/logs</kbd> each time a GC occurs. The downside of this approach is that these logs will not be on your driver program but on your cluster's worker nodes.</p>
<div class="packt_tip">It is to be noted that <kbd class="calibre22">verbose:gc</kbd> only prints appropriate message or logs after each GC collection. Correspondingly, it prints details about memory. However, if you are interested in looking for more critical issues, such as a memory leak, <kbd class="calibre22">verbose:gc</kbd> may not be enough. In that case, you can use some visualization tools, such as jhat and VisualVM. A better way of GC tuning in your Spark application can be read at <a href="https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html" class="calibre21">https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html</a>.</div>


            </article>

            
        </section>
    

        <section id="F3GD61-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Level of parallelism</h1>
                
            
            <article>
                
<p class="mce-root">Although you can control the number of map tasks to be executed through optional parameters to the <kbd class="calibre11">SparkContext.text</kbd> file, Spark sets the same on each file according to its size automatically. In addition to this, for a distributed <kbd class="calibre11">reduce</kbd> operation such as <kbd class="calibre11">groupByKey</kbd> and <kbd class="calibre11">reduceByKey</kbd>, Spark uses the largest parent RDD's number of partitions. However, sometimes, we make one mistake, that is, not utilizing the full computing resources for your nodes in a computing cluster. As a result, the full computing resources will not be fully exploited unless you set and specify the level of parallelism for your Spark job explicitly. Therefore, you should set the level of parallelism as the second argument.</p>
<div class="packt_tip">For more on this option, please refer to <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" class="calibre21">https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions.</a></div>
<p class="mce-root">Alternatively, you can do it by setting the config property spark.default.parallelism to change the default. For operations such as parallelizing with no parent RDDs, the level of parallelism depends on the cluster manager, that is, standalone, Mesos, or YARN. For the local mode, set the level of parallelism equal to the number of cores on the local machine. For Mesos or YARN, set fine-grained mode to 8. In other cases, the total number of cores on all executor nodes or 2, whichever is larger, and in general, 2-3 tasks per CPU core in your cluster is recommended.</p>


            </article>

            
        </section>
    

        <section id="F4ETO1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Broadcasting</h1>
                
            
            <article>
                
<p class="mce-root">A broadcast variable enables a Spark developer to keep a read-only copy of an instance or class variable cached on each driver program, rather than transferring a copy of its own with the dependent tasks. However, an explicit creation of a broadcast variable is useful only when tasks across multiple stages need the same data in deserialize form.</p>
<p class="mce-root">In Spark application development, using the broadcasting option of SparkContext can reduce the size of each serialized task greatly. This also helps to reduce the cost of initiating a Spark job in a cluster. If you have a certain task in your Spark job that uses large objects from the driver program, you should turn it into a broadcast variable.</p>
<p class="mce-root">To use a broadcast variable in a Spark application, you can instantiate it using <kbd class="calibre11">SparkContext.broadcast</kbd>. Later on, use the value method from the class to access the shared value as follows:</p>
<pre class="calibre19">
val m = 5<br class="title-page-name"/>val bv = sc.broadcast(m)
</pre>
<p class="mce-root">Output/log: <kbd class="calibre11">bv: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(0)</kbd></p>
<pre class="calibre19">
bv.value()
</pre>
<div class="cdpalignleft">Output/log: <kbd class="calibre34">res0: Int = 1</kbd></div>
<div class="cdpaligncenter"><img class="image-border238" src="../images/00359.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 28:</strong> Broadcasting a value from driver to executors</div>
<p class="cdpalignleft1">The Broadcast feature of Spark uses the <strong class="calibre1">SparkContext</strong> to create broadcast values. After that, the <strong class="calibre1">BroadcastManager</strong> and <strong class="calibre1">ContextCleaner</strong> are used to control their life cycle, as shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border239" src="../images/00368.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 29:</strong> SparkContext broadcasts the variable/value using BroadcastManager and ContextCleaner</div>
<p class="mce-root">Spark application in the driver program automatically prints the serialized size of each task on the driver. Therefore, you can decide whether your tasks are too large to make it parallel. If your task is larger than 20 KB, it's probably worth optimizing.</p>


            </article>

            
        </section>
    

        <section id="F5DEA1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data locality</h1>
                
            
            <article>
                
<p class="mce-root">Data locality means how close the data is to the code to be processed. Technically, data locality can have a nontrivial impact on the performance of a Spark job to be executed locally or in cluster mode. As a result, if the data and the code to be processed are tied together, computation is supposed to be much faster. Usually, shipping a serialized code from a driver to an executor is much faster since the code size is much smaller than that of data.</p>
<p class="mce-root">In Spark application development and job execution, there are several levels of locality. In order from closest to farthest, the level depends on the current location of the data you have to process:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Data Locality</strong></td>
<td class="calibre7"><strong class="calibre1">Meaning</strong></td>
<td class="calibre7"><strong class="calibre1">Special Notes</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">PROCESS_LOCAL</kbd></td>
<td class="calibre7">Data and code are in the same location</td>
<td class="calibre7">Best locality possible</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">NODE_LOCAL</kbd></td>
<td class="calibre7">Data and the code are on the same node, for example, data stored on HDFS</td>
<td class="calibre7">A bit slower than <kbd class="calibre34">PROCESS_LOCAL</kbd> since the data has to propagate across the processes and network</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">NO_PREF</kbd></td>
<td class="calibre7">The data is accessed equally from somewhere else</td>
<td class="calibre7">Has no locality preference</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">RACK_LOCAL</kbd></td>
<td class="calibre7">The data is on the same rack of servers over the network</td>
<td class="calibre7">Suitable for large-scale data processing</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">ANY</kbd></td>
<td class="calibre7">The data is elsewhere on the network and not in the same rack</td>
<td class="calibre7">Not recommended unless there are no other options available</td>
</tr>
</tbody>
</table>
<div class="cdpaligncenter1"><strong class="calibre1">Table 2:</strong> Data locality and Spark</div>
<p class="mce-root">Spark is developed such that it prefers to schedule all tasks at the best locality level, but this is not guaranteed and not always possible either. As a result, based on the situation in the computing nodes, Spark switches to lower locality levels if available computing resources are too occupied. Moreover, if you would like to have the best data locality, there are two choices for you:</p>
<ul class="calibre9">
<li class="mce-root1">Wait until a busy CPU gets free to start a task on your data on the same server or same node</li>
<li class="mce-root1">Immediately start a new one, which requires moving data there</li>
</ul>


            </article>

            
        </section>
    

        <section id="F6BUS1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="mce-root">In this chapter, we discussed some advanced topics of Spark toward making your Spark job's performance better. We discussed some basic techniques to tune your Spark jobs. We discussed how to monitor your jobs by accessing Spark web UI. We discussed how to set Spark configuration parameters. We also discussed some common mistakes made by Spark users and provided some recommendations. Finally, we discussed some optimization techniques that help tune Spark applications.</p>
<p class="mce-root">In the next chapter, you will see how to test Spark applications and debug to solve most common issues.</p>


            </article>

            
        </section>
    </body></html>