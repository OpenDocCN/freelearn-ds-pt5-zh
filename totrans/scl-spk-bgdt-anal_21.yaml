- en: Interactive Data Analytics with Apache Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From a data science perspective, interactive visualization of your data analysis
    is also important. Apache Zeppelin, is a web-based notebook for interactive and
    large-scale data analytics with multiple backends and interpreters, such as Spark,
    Scala, Python, JDBC, Flink, Hive, Angular, Livy, Alluxio, PostgreSQL, Ignite,
    Lens, Cassandra, Kylin, Elasticsearch, JDBC, HBase, BigQuery, Pig, Markdown, Shell,
    and even more.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no doubt about Spark''s ability to handle large-scale datasets in
    a scalable and fast way. However, one thing in Spark is missing--there is no real-time
    or interactive visualization support with it. Considering the aforementioned exciting
    features of Zeppelin, in this chapter, we will discuss how to use Apache Zeppelin
    for large-scale data analytics using Spark as the interpreter in the backend.
    In summary, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Apache Zeppelin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation and getting started
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collaboration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Apache Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Zeppelin is a web-based notebook that enables you to do data analytics
    in an interactive way. Using Zeppelin, you can make beautiful data-driven, interactive,
    and collaborative documents with SQL, Scala, and more. The Apache Zeppelin interpreter
    concept allows any language/data-processing backend to be plugged into Zeppelin.
    Currently, Apache Zeppelin supports many interpreters, such as Apache Spark, Python,
    JDBC, Markdown, and Shell. Apache Zeppelin is a relatively new technology from
    the Apache Software Foundation, which enables data scientists, engineers, and
    practitioners to take the advantage of the data exploration, visualization, sharing,
    and collaboration features.
  prefs: []
  type: TYPE_NORMAL
- en: Installation and getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since using the other interpreters is not the goal of this book, but using
    Spark on Zeppelin is, all the code will be written using Scala. In this section,
    therefore, we will show how to configure Zeppelin using the binary package that
    contains only the Spark interpreter. Apache Zeppelin officially supports and is
    tested on the following environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Requirements** | **Value/Version** | **Other Requirements** |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle JDK | 1.7 or higher | Set `JAVA_HOME` |'
  prefs: []
  type: TYPE_TB
- en: '| OS | macOS 10.X+ Ubuntu 14.X+'
  prefs: []
  type: TYPE_NORMAL
- en: CentOS 6.X+
  prefs: []
  type: TYPE_NORMAL
- en: Windows 7 Pro SP1+ | - |
  prefs: []
  type: TYPE_NORMAL
- en: Installation and configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As shown in the preceding table, Java is required to execute Spark codes on
    Zeppelin. Therefore, if not set up, install and set up Java on any of the aforementioned
    platforms. Alternatively, you can refer to [Chapter 1](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduction to Scala*, to learn how to set up Java on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The latest release of Apache Zeppelin can be downloaded from [https://zeppelin.apache.org/download.html](https://zeppelin.apache.org/download.html).
    Each release comes with three options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary package with all the interpreters**: It contains support for many
    interpreters. For example, Spark, JDBC, Pig, Beam, Scio, BigQuery, Python, Livy,
    HDFS, Alluxio, Hbase, Scalding, Elasticsearch, Angular, Markdown, Shell, Flink,
    Hive, Tajo, Cassandra, Geode, Ignite, Kylin, Lens, Phoenix, and PostgreSQL are
    currently supported in Zeppelin.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Binary package with the Spark interpreter**: It usually contains only the
    Spark interpreter. It also contains the interpreter net-install script.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Source**: You can also build Zeppelin with all the latest changes from the
    GitHub repo (more to follow).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To show how to install and configure Zeppelin, we have downloaded the binary
    package from the following site mirror:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz](http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz)'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have downloaded it, unzip it somewhere in your machine. Suppose that
    the path where you have unzipped the file is `/home/Zeppelin/`.
  prefs: []
  type: TYPE_NORMAL
- en: Building from source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can also build Zeppelin with all the latest changes from the GitHub repo.
    If you want to build from source, you must first install the following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Git: any version'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maven: 3.1.x or higher'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JDK: 1.7 or higher'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'npm: the latest version'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'libfontconfig: the latest version'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you haven''t installed Git and Maven yet, check the build requirement instructions
    from [http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements](http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements).
    However, due to page limitation, we have not discussed all the steps in detail.
    If you''re interested, you should refer to this URL for more details: [http://zeppelin.apache.org/docs/snapshot/install/build.html](http://zeppelin.apache.org/docs/snapshot/install/build.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Starting and stopping Apache Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On all Unix-like platforms (for example, Ubuntu, macOS, and so on), use the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If the preceding command is successfully executed, you should observe the following
    logs on the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00061.jpeg)**Figure 1**: Starting Zeppelin from the Ubuntu terminal'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are on Windows, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After Zeppelin has started successfully, go to `http://localhost:8080` with
    your web browser and you will see that Zeppelin is running. More specifically,
    you''ll see the following view on your browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpeg)**Figure 2**: Zeppelin is running on http://localhost:8080'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations; you have successfully installed Apache Zeppelin! Now, let's
    move on to Zeppelin and get started with our data analytics once we have configured
    the preferred interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to stop Zeppelin from the command line, issue the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Creating notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you are on `http://localhost:8080/`, you can explore different options
    and menus that help you understand how to get familiar with Zeppelin. You can
    find more on Zeppelin and its user-friendly UI at [https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html](https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html)
    (you can refer to the latest quick start documentation too, based on the available
    versions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s first create a sample notebook and get started. As shown in the
    following figure, you can create a new notebook by clicking on the Create new
    note option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.jpeg)**Figure 3**: Creating a sample Zeppelin notebook'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the previous figure, the default interpreter is selected as Spark.
    In the drop-down list, you will also see only Spark, since we have download the
    Spark-only binary package for Zeppelin.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the interpreter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every interpreter belongs to an interpreter group. An interpreter group is a
    unit of start/stop interpreters. By default, every interpreter belongs to a single
    group, but the group might contain more interpreters. For example, the Spark interpreter
    group includes Spark support, pySpark, Spark SQL, and the dependency loader. If
    you want to execute an SQL statement on Zeppelin, you should specify the interpreter
    type using the `%` sign; for example, for using SQL, you should use `%sql`; for
    mark-down, use `%md`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, refer to the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.jpeg)**Figure 4**: The interpreter properties for using Spark
    on Zeppelin Data ingestion'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, once you have created the notebook, you can start writing Spark code
    directly in the code section. For this simple example, we will use the bank dataset,
    which is publicly available for research and can be downloaded from [https://archive.ics.uci.edu/ml/machine-learning-databases/00222/](https://archive.ics.uci.edu/ml/machine-learning-databases/00222/),
    courtesy of S. Moro, R. Laureano, and P. Cortez, Using Data Mining for Bank Direct
    Marketing: An Application of the CRISP-DM Methodology. The dataset contains data
    such as age, job title, marital status, education, if s/he is a defaulter, bank
    balance, housing, if borrower loaned from the bank, and so on, about the customer
    of the bank in a CSV format. A sample of the dataset is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.jpeg)**Figure 5**: A sample of the bank dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s first load the data on the Zeppelin notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon the execution of this line of code, create a new paragraph and name it
    as the data ingestion paragraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00098.jpeg)**Figure 6**: Data ingesting paragraph'
  prefs: []
  type: TYPE_NORMAL
- en: If you see the preceding image carefully, the code worked and we did not need
    to define the Spark context. The reason is that it is already defined there as
    `sc`. You don't even need to define Scala implicitly. We will see an example of
    this later.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s create a case class that will tell us how to pick the selected
    fields from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, split each line, filter out the header (starts with `age`), and map it
    into the `Bank` case class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, convert to DataFrame and create a temporal table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows that all the code snippets were executed successfully
    without showing any errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00102.jpeg)**Figure 7**: Data process paragraph'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it more transparent, let''s see the status marked in green color (in
    the top-right corner of the image), as follows, after the code has been executed
    for each case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00114.jpeg)**Figure 8**: A successful execution of Spark code in each
    paragraph'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s load some data to play with the following SQL command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that the preceding line of code is a pure SQL statement that selects the
    names of all the customers whose age is greater than or equal to 45 (that is,
    age distribution). Finally, it counts the number for the same customer group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see how the preceding SQL statement works on the temp view (that
    is, `bank`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.jpeg)**Figure 9**: SQL query that selects the names of all the
    customers with age distribution [Tabular]'
  prefs: []
  type: TYPE_NORMAL
- en: Now you can select graph options, such as histogram, pie-chart, bar chart, and
    so on, from the tab near the table icon (in the result section). For example,
    using histogram, you can see the corresponding count for `age group >=45`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.jpeg)**Figure 10**: SQL query that selects the names of all the
    customers with age distribution [Histogram]'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how it looks using a pie-chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00328.jpeg)**Figure 11**: SQL query that selects the names all the
    customers with age distribution [pie-chart]'
  prefs: []
  type: TYPE_NORMAL
- en: Fantastic! We are now almost ready to do more complex data analytics problems
    using Zeppelin.
  prefs: []
  type: TYPE_NORMAL
- en: Complex data analytics with Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how to perform more complex analytics using Zeppelin.
    At first, we will formalize the problem, and then, will explore the dataset that
    will be used. Finally, we will apply some visual analytics and machine learning
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The problem definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will build a spam classifier for classifying the raw text
    as spam or ham. We will also show how to evaluate such a model. We will try to
    focus using and working with the DataFrame API. In the end, the spam classifier
    model will help you distinguish between spam and ham messages. The following image
    shows a conceptual view of two messages (spam and ham respectively):'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/00333.jpeg)](https://blog.codecentric.de/files/2016/06/ham-vs-spam.png)**Figure
    12**: Spam and Ham example'
  prefs: []
  type: TYPE_NORMAL
- en: We power some basic machine learning techniques to build and evaluate such a
    classifier for this kind of problem. In particular, the logistic regression algorithm
    will be used for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset descripting and exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The spam data set that we downloaded from [https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)
    consists of 5,564 SMS, which have been classified by hand as either ham or spam.
    Only 13.4% of these SMSes are spam. This means that the dataset is skewed and
    provides only a few examples of spam. This is something to keep in mind, as it
    can introduce bias while training models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00336.jpeg)**Figure 13**: A snap of the SMS dataset'
  prefs: []
  type: TYPE_NORMAL
- en: So, what does this data look like? As you might have seen, social media text
    can really get dirty, containing slang words, misspelled words, missing whitespaces,
    abbreviated words, such as *u*, *urs*, *yrs*, and so on, and, often, a violation
    of grammar rules. It sometimes even contains trivial words in the messages. Thus,
    we need to take care of these issues as well. In the following steps, we will
    encounter these issues for a better interpretation of the analytics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Load the required packages and APIs on Zeppelin** - Let''s load
    the required packages and APIs and create the first paragraph, before we ingest
    the dataset on Zeppelin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00345.jpeg)**Figure 14**: Package/APIs load paragraph'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2\. Load and parse the dataset** - We''ll use the CSV parsing library
    by Databricks (that is, `com.databricks.spark.csv`) to read the data into the
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00351.jpeg)**Figure 15**: Data ingesting/load paragraph'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3\. Using** `StringIndexer` **to create numeric labels** - Since the
    labels in the original DataFrame are categorical, we will have to convert them
    back so that we can feed them to or use them in the machine learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00357.jpeg)**Figure 16**: The StringIndexer paragraph, and the output
    shows the raw labels, original texts, and corresponding labels.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4\. Using** `RegexTokenizer` **to create a bag of words** - We''ll use
    `RegexTokenizer` to remove unwanted words and create a bag of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00363.jpeg)**Figure 17**: The RegexTokenizer paragraph, and the output
    shows the raw labels, original texts, corresponding labels, and tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5\. Removing stop words and creating a filtered** **DataFrame** - We''ll
    remove stop words and create a filtered DataFrame for visual analytics. Finally,
    we show the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00329.jpeg)**Figure 18**: StopWordsRemover paragraph and the output
    shows the raw labels, original texts, corresponding labels, tokens, and filtered
    tokens without the stop words'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 6\. Finding spam messages/words and their frequency** - Let''s try to
    create a DataFrame containing only the spam words, along with their respective
    frequency, to understand the context of the messages in the dataset. We can create
    a paragraph on Zeppelin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00349.jpeg)**Figure 19**: Spam tokens with a frequency paragraph'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see them in the graph using SQL queries. The following query selects
    all the tokens with frequencies of more than 100\. Then, we sort the tokens in
    a descending order of their frequency. Finally, we use the dynamic forms to limit
    the number of records. The first one is just a raw tabular format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.jpeg)**Figure 20**: Spam tokens with a frequency visualization
    paragraph [Tabular]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we''ll use a bar diagram, which provides more visual insights. We can
    now see that the most frequent words in the spam messages are call and free, with
    a frequency of 355 and 224 respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00096.jpeg)**Figure 21**: Spam tokens with a frequency visualization
    paragraph [Histogram]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, using the pie chart provides much better and wider visibility, especially
    if you specify the column range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00145.jpeg)**Figure 22**: Spam tokens with a frequency visualization
    paragraph [Pie chart]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7\. Using HashingTF for term frequency** - Use `HashingTF` to generate
    the term frequency of each filtered token, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00251.jpeg)**Figure 23**: HashingTF paragraph, and the output shows
    the raw labels, original texts, corresponding labels, tokens, filtered tokens,
    and corresponding term-frequency for each row'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8\. Using IDF for Term frequency-inverse document frequency (TF-IDF)**
    - TF-IDF is a feature vectorization method widely used in text mining to reflect
    the importance of a term to a document in the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00085.jpeg)**Figure 24**: IDF paragraph, and the output shows the raw
    labels, original texts, corresponding labels, tokens, filtered tokens, term-frequency,
    and the corresponding IDFs for each row'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bag of words:** The bag of words assigns a value of `1` for every occurrence
    of a word in a sentence. This is probably not ideal, as each category of the sentence,
    most likely, has the same frequency of *the*, *and*, and other words; whereas
    words such as *viagra* and *sale* probably should have an increased importance
    in figuring out whether or not the text is spam.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TF-IDF:** This is the acronym for Text Frequency – Inverse Document Frequency.
    This term is essentially the product of text frequency and inverse document frequency
    for each word. This is commonly used in the bag of words methodology in NLP or
    text analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using TF-IDF:** Let''s take a look at word frequency. Here, we consider the
    frequency of a word in an individual entry, that is, term. The purpose of calculating
    text frequency (TF) is to find terms that appear to be important in each entry.
    However, words such as *the* and *and* may appear very frequently in every entry.
    We want to downweigh the importance of these words, so we can imagine that multiplying
    the preceding TF by the inverse of the whole document frequency might help find
    important words. However, since a collection of texts (a corpus) may be quite
    large, it is common to take the logarithm of the inverse document frequency. In
    short, we can imagine that high values of TF-IDF might indicate words that are
    very important to determining what a document is about. Creating the TF-IDF vectors
    requires us to load all the text into memory and count the occurrences of each
    word before we can start training our model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 9\. Using VectorAssembler to generate raw features for the Spark ML
    pipeline** - As you saw in the previous step, we have only the filtered tokens,
    labels, TF, and IDF. However, there are no associated features that can be fed
    into any ML models. Thus, we need to use the Spark VectorAssembler API to create
    features based on the properties in the previous DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00101.jpeg)**Figure 25**: The VectorAssembler paragraph that shows
    using VectorAssembler for feature creations'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 10\. Preparing the training and test set** - Now we need to prepare
    the training and test set. The training set will be used to train the Logistic
    Regression model in Step 11*,* and the test set will be used to evaluate the model
    in Step 12\. Here, I make it 75% for the training and 25% for the test. You can
    adjust it accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00117.jpeg)**Figure 26**: Preparing training/test set paragraph'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 11\. Training binary logistic regression model** - Since, the problem
    itself is a binary classification problem, we can use a binary logistic regression
    classifier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00133.jpeg)**Figure 27**: LogisticRegression paragraph that shows how
    to train the logistic regression classifier with the necessary labels, features,
    regression parameters, elastic net param, and maximum iterations'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, here, for better results, we have iterated the training for 200 times.
    We have set the regression parameter and elastic net params a very low -i.e. 0.0001
    for making the training more intensive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 12\. Model evaluation** - Let''s compute the raw prediction for the
    test set. Then, we instantiate the raw prediction using the binary classifier
    evaluator, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/00335.jpeg)****Figure 28**: Model evaluator paragraph'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s compute the accuracy of the model for the test set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00346.jpeg)**Figure 29**: Accuracy calculation paragraph'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is pretty impressive. However, if you were to go with the model tuning
    using cross-validation, for example, you could gain even higher accuracy. Finally,
    we will compute the confusion matrix to get more insight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00350.jpeg)**Figure 30**: Confusion paragraph shows the number of correct
    and incorrect predictions summarized with count values and broken down by each
    class'
  prefs: []
  type: TYPE_NORMAL
- en: Data and results collaborating
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Furthermore, Apache Zeppelin provides a feature for publishing your notebook
    paragraph results. Using this feature, you can show the Zeppelin notebook paragraph
    results on your own website. It''s very straightforward; just use the `<iframe>`
    tag on your page. If you want to share the link of your Zeppelin notebook, the
    first step to publish your paragraph result is Copy a paragraph link. After running
    a paragraph in your Zeppelin notebook, click the gear button located on the right-hand.
    Then, click Link this paragraph in the menu, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00355.jpeg)**Figure 31**: Linking the paragraph'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, just copy the provided link, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00358.jpeg)**Figure 32**: Getting the link for paragraph sharing with
    collaborators'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, even if you want to publish the copied paragraph, you may use the `<iframe>`
    tag on your website. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can show off your beautiful visualization results on your website.
    This is more or less the end of our data analytics journey with Apache Zeppelin.
    For more inforamtion and related updates, you should visit the official website
    of Apache Zeppelin at [https://zeppelin.apache.org/](https://zeppelin.apache.org/);
    you can even subscribe to Zeppelin users at [users-subscribe@zeppelin.apache.org](mailto:users-subscribe@zeppelin.apache.org).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Zeppelin is a web-based notebook that enables you to do data analytics
    in an interactive way. Using Zeppelin, you can make beautiful data-driven, interactive,
    and collaborative documents with SQL, Scala, and more. It is gaining more popularity
    by the day, since more features are being added to recent releases. However, due
    to page limitations, and to make you more focused on using Spark only, we have
    shown examples that are only suitable for using Spark with Scala. However, you
    can write your Spark code in Python and test your notebook with similar ease.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to use Apache Zeppelin for large-scale data
    analytics using Spark in the backend as the interpreter. We saw how to install
    and get started with Zeppelin. We then saw how to ingest your data and parse and
    analyse it for better visibility. Then, we saw how to visualize it for better
    insights. Finally, we saw how to share the Zeppelin notebook with collaborators.
  prefs: []
  type: TYPE_NORMAL
