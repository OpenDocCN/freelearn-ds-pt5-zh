<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 7. Unsupervised Machine Learning"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch07" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 7. Unsupervised Machine Learning</h1></div></div></div><p class="calibre11">In the previous chapter, we learned about supervised machine learning algorithms and how we can use them in real-world scenarios.</p><p class="calibre11">Unsupervised learning is a little bit different and harder. The aim is to have the system learn something, but we ourselves don't know what to learn. There are two approaches to the unsupervised learning.</p><p class="calibre11">One approach is to find the similarities/patterns in the datasets. Then we can create clusters of these similar points. We make the assumption that the clusters that we found can be classified and can be provided with a label.</p><p class="calibre11">The algorithm itself cannot assign names because it doesn't have any. It can only find the clusters based on the similarities, but nothing more than that. To actually be able to find meaningful clusters, a good size of dataset is required.</p><p class="calibre11">It is used extensively in finding similar users, recommender systems, text classification, and so on.</p><p class="calibre11">We will discuss various clustering algorithms in detail. In this chapter, we will learn:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Working with unlabeled data.</li><li class="listitem">What is unsupervised learning?</li><li class="listitem">What is clustering?</li><li class="listitem">Different types of clustering.</li><li class="listitem">The K-Means algorithm and Bisecting K-means. Its strengths and weaknesses.</li><li class="listitem">Hierarchical clustering.</li><li class="listitem">Agglomerative clustering. Its strengths and weaknesses.</li><li class="listitem">The DBSCAN algorithm.</li></ul></div><p class="calibre11">We should also discuss the second approach before we can start delving deep into clustering. It will tell us how different clustering is from this approach and the use cases. The second approach is a kind of reinforcement learning. This involves rewards to indicate success to the algorithm. There are no explicit categorizations done. This type of algorithm is best suited for real-world algorithms. In this algorithm, the system behaves on the previous rewards or the punishments it got. This kind of learning can be powerful because there is no prejudice and there are no pre-classified observations.</p><p class="calibre11">This calculates the possibility of every action and knows beforehand what action will lead to what kind of result.</p><p class="calibre11">This trial and error method is computationally intensive and consumes a lot of time. Let's discuss the clustering approach that is not based on trial and error.</p><div class="calibre2" title="Understanding clustering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec61" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding clustering</h1></div></div></div><p class="calibre11">Clustering is a technique to divide data into groups (clusters) that are useful and meaningful. The clusters are formed capturing the natural structure of the data, which have meaningful relations with each other. It is also possible that this is only used at the preparation or the summarization stage for the other algorithms or further analysis. Cluster analysis has roles in many fields, such as biology, pattern recognition, information retrieval, and so on.</p><p class="calibre11">Clustering has applications in different fields:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Information retrieval</strong></span>: To segregate the information into particular clusters is an important step in searching and retrieving information from the numerous sources or a big pool of data. Let's use the example of news aggregating websites. They create clusters of similar types of news making it easier for the user to go through the interesting sections.<p class="calibre44">These news types can also have sub-classes creating a hierarchical view. For example, in the sports news section, we can have Football, Cricket, and Tennis, and other sports.</p></li><li class="listitem"><span class="strong"><strong class="calibre19">Biology</strong></span>: Clustering finds a great use in biology. After years of research, biologists have classified most of the living things in hierarchies. Using the features of these classes, unknowns can be classified. Also, the existing data can be used to find similarities and interesting patterns.</li><li class="listitem"><span class="strong"><strong class="calibre19">Marketing</strong></span>: Companies use customer and sales data to create clusters of similar users or segments where targeted promotions/campaigns can be run to get the maximum return on  investment.</li><li class="listitem"><span class="strong"><strong class="calibre19">Weather</strong></span>: Cluster analysis is used extensively in climate and weather analysis. Weather stations generate huge amount of data. Clustering is used to generate insights on this data and find out the patterns and important information.</li></ul></div><div class="calibre2" title="How are clusters formed?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec96" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>How are clusters formed?</h2></div></div></div><p class="calibre11">There are many methods to form clusters. Let's discuss some basic approaches of cluster creation:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Start with grouping the data objects. This grouping should only happen based on the data that is describing the objects.</li><li class="listitem">Similar objects are grouped together. They may show a relationship with each other.</li><li class="listitem">Dissimilar objects are kept in other clusters.</li></ul></div><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_07_001.jpg" alt="How are clusters formed?" class="calibre190"/></div><p class="calibre11">
</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The preceding plot clearly shows us some distinct clusters that are formed when there are more similarities between different data objects in a cluster and dissimilarities with data objects from other clusters.</li></ul></div><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_07_002.jpg" alt="How are clusters formed?" class="calibre191"/></div><p class="calibre11">
</p><p class="calibre11">But in this particular representation of the data points, we can see that there are no definite clusters that can be formed. This is when there is some similarity between the data objects of the different clusters.</p></div><div class="calibre2" title="Types of clustering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec97" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Types of clustering</h2></div></div></div><p class="calibre11">There are different types of clustering mechanisms depending on the various factors:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Nested or un-nested—hierarchical or partitional</li><li class="listitem">Overlapping, exclusive, and fuzzy</li><li class="listitem">Partial versus complete</li></ul></div><div class="calibre2" title="Hierarchical clustering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec43" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Hierarchical clustering</h3></div></div></div><p class="calibre11">If the clusters do not form subsets, then the cluster is said to be un-nested. Therefore, partitional clustering is defined as the creation of well-defined clusters, which do not overlap with each other. In such a cluster, the data points are located in one and only one cluster alone.</p><p class="calibre11">If the clusters have subclusters within them, then it is called hierarchical clustering.</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_07_03.jpg" alt="Hierarchical clustering" class="calibre192"/></div><p class="calibre11">
</p><p class="calibre11">The preceding diagram represents a hierarchical cluster. Hierarchical clusters are the clusters organized as a tree.</p><p class="calibre11">Here, each cluster has its own child cluster. Each node can also be thought of as an individual system, having its own clusters obtained through partitioning.</p></div><div class="calibre2" title="Overlapping, exclusive, and fuzzy clustering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec44" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Overlapping, exclusive, and fuzzy clustering</h3></div></div></div><p class="calibre11">The techniques which leads to creation of different types of clusters can be categorized into three approaches:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Exclusive clusters</strong></span>: In the section, How are clusters formed? We saw two images representing two different types of clusters. In the first image, we saw that the clusters are well defined and have a good separation between them. These are called exclusive clusters. In these clusters, the data points have a definite dissimilarity from the data points of the other clusters.</li><li class="listitem"><span class="strong"><strong class="calibre19">Overlapping clusters</strong></span>: In the second image, we saw that there is no such definite boundary to separate two clusters. Here some of the data points can exist in any of the clusters. This situation comes when there is no such feature to distinguish the data point into any of the clusters.</li><li class="listitem"><span class="strong"><strong class="calibre19">Fuzzy clustering</strong></span>: Fuzzy clustering is a unique concept. Here the data point belongs to each and every cluster and its relationship is defined by the weight which is between 1 (belongs exclusively) to 0 (doesn't belong). Therefore, clusters are considered as fuzzy sets. By the probabilistic rule, a constraint is added that the sum of weights of all the data points should be equal to 1.</li></ul></div><p class="calibre11">Fuzzy clustering is also known as probabilistic clustering. Generally, to have a definite relation, the data point is associated with the cluster for whom it has the highest membership weight.</p></div><div class="calibre2" title="Differences between partial versus complete clustering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec45" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Differences between partial versus complete clustering</h3></div></div></div><p class="calibre11">In complete clustering, all the data points are assigned to a cluster because they accurately represent features of the cluster. These types of clusters are called complete clusters.</p><p class="calibre11">There can be some data points that may not belong to any of the clusters. This is when these data points represent noise or are outliers to the cluster. Such data points are not taken in any of the clusters, and this is called partial clustering.</p></div></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="K-means clustering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec62" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>K-means clustering</h1></div></div></div><p class="calibre11">K-means is the most popular of the clustering techniques because of its ease of use and implementation. It also has a partner by the name of K-medoid. These partitioning methods create level-one partitioning of the dataset. Let's discuss K-means in detail.</p><div class="calibre2" title="K-means algorithm"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec98" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>K-means algorithm</h2></div></div></div><p class="calibre11">K-means start with a prototype. It takes centroids of data points from the dataset. This technique is used for the objects lying in the n-dimensional space.</p><p class="calibre11">The technique involves choosing the K number of centroids. This K is specified by the user and is chosen considering various factors. It defines how many clusters we want. So, choosing a higher or lower than the required K can lead to undesired results.</p><p class="calibre11">Now going forward, each point is assigned to its nearest centroid. As many points get associated with a specific centroid, a cluster is formed. The centroid can get updated depending on the points that are part of the current cluster.</p><p class="calibre11">This process is done repeatedly until the centroid gets constant.</p><div class="calibre2" title="Algorithm of K-means"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec46" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Algorithm of K-means</h3></div></div></div><p class="calibre11">Understanding K-means algorithm will give us a better view of how to approach the problem. Let's understand step by step the K-means algorithm:</p><div class="calibre2"><ol class="orderedlist"><li class="listitem1">As per the defined K, select the number of centroids.</li><li class="listitem1">Assign data points to the nearest centroid. This step will form the clusters.</li><li class="listitem1">Compute the centroid of the cluster again.</li><li class="listitem1">Repeat steps 2 and 3 until the centroid gets constant.</li></ol></div><p class="calibre11">In the first step, we use the mean as the centroid.</p><p class="calibre11">Step 4 says to repeat the earlier steps of the algorithm. This can sometimes lead to a large number of iterations with very little change. So, we generally use repeat steps 2 and 3 only if the newer computed centroid has more than 1% change.</p></div><div class="calibre2" title="Associating the data points with the closest centroid"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec47" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Associating the data points with the closest centroid</h3></div></div></div><p class="calibre11">How do we measure the distance between the computed centroid and the data point?</p><p class="calibre11">We use the Euclidean (L2) distance as the measure, and we assume that the data points are in the Euclidean space. We can also use different proximity measures if required, for example, Manhattan (L1) can also be used for the Euclidean space.</p><p class="calibre11">As the algorithm processes similarities with the different data points, it is good to have only the required set of features of the data points. With higher dimensional data, the computation increases drastically as it has to compute for each and every dimension iteratively.</p><p class="calibre11">There are some choices of the distance measure that can be used:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Manhattan (L1)</strong></span>: This takes a median as the centroid. It works on the function to minimize the sum of the L1 distance of an object from the centroid of its cluster.</li><li class="listitem"><span class="strong"><strong class="calibre19">Squared Euclidean (L2^2)</strong></span>: This takes a mean as the centroid. It works on the function to minimize the sum of the squared of the L2 distance of an object from the centroid of the cluster.</li><li class="listitem"><span class="strong"><strong class="calibre19">Cosine</strong></span>: This takes a mean as the centroid. It works on the function to maximize the sum of the cosine similarity of an object from the centroid of the cluster.</li><li class="listitem"><span class="strong"><strong class="calibre19">Bregman divergence</strong></span>: This takes a mean as the centroid. It minimizes the sum of the Bregman divergence of an object from the centroid of the cluster.</li></ul></div></div><div class="calibre2" title="How to choose the initial centroids?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec48" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>How to choose the initial centroids?</h3></div></div></div><p class="calibre11">This is a very important step in the K-means algorithm. We start off by choosing the initial centroids randomly. This generally results in very poor clusters. Even if these centroids are well distributed, we do not get even close to the desired clusters.</p><p class="calibre11">There is a technique to address this problem—multiple runs with different initial centroids. After this, the set of the clusters is chosen, which has the minimum <span class="strong"><strong class="calibre19">Sum of Squares error </strong></span>(<span class="strong"><strong class="calibre19">SSE</strong></span>). This may not always work well and may not always be feasible because of the size of the dataset and the computation power required.</p><p class="calibre11">In repeating the random initializing, the centroid may not be able to overcome the problem, but we have other techniques that we can use:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Using hierarchical clustering, we can start with taking some sample points and use hierarchical clustering to make a cluster. Now we can take out the K number of clusters from this clustering and use the centroids of these clusters as the initial centroids. There are some constraints to this approach:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">The sample data should not be large (expensive computation).</li><li class="listitem">With the number of the desired clusters, K should be small.</li></ul></div></li><li class="listitem">Another technique is to get the centroid of all the points. From this centroid, we find the point that is separated at maximum. We follow this process to get the maximum distant centroids, which are also randomly chosen. But there are some issues with this approach:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">It is computationally intensive to find out the farthest point.</li><li class="listitem">This approach sometimes produces undesirable results when there are outliers in the dataset. Therefore, we may not get the dense regions as required.</li></ul></div></li></ul></div></div><div class="calibre2" title="Time-space complexity of K-means algorithms"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec49" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Time-space complexity of K-means algorithms</h3></div></div></div><p class="calibre11">K-means doesn't require that much space as we only need to store the data points and the centroids.</p><p class="calibre11">The storage requirement of a K-means algorithm <span class="strong"><em class="calibre23">O((m+K)n)</em></span>, where:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre23">m</em></span> is number of points</li><li class="listitem"><span class="strong"><em class="calibre23">n</em></span> is number of attributes</li></ul></div><p class="calibre11">The time requirements of the K-means algorithm may vary, but generally they too are modest. The time increases linearly with the number of data points.</p><p class="calibre11">Time requirements of a K-means algorithm: <span class="strong"><em class="calibre23">O(I*K*m*n)</em></span>, where:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre23">I</em></span> is number of iterations required to converge to a centroid</li></ul></div><p class="calibre11">K-means works best if the number of clusters required is significantly smaller than the number of data points on which the K-means is directly proportional to.</p></div></div><div class="calibre2" title="Issues with K-means"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec99" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Issues with K-means</h2></div></div></div><p class="calibre11">There are some issues associated with the basic K-means clustering algorithm. Let's discuss these issues in detail.</p><div class="calibre2" title="Empty clusters in K-means"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec50" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Empty clusters in K-means</h3></div></div></div><p class="calibre11">There can be a situation where we get empty clusters. This is when there are no points allocated to a particular given cluster during the phase where points are assigned. This can be resolved as follows:</p><div class="calibre2"><ol class="orderedlist"><li class="listitem1">We choose a different centroid to the current choice. If it is not done, the squared error will be much larger than the threshold.</li><li class="listitem1">To choose a different centroid, we follow the same approach of finding the farthest such point from the current centroid. This generally eliminates the point that was contributing to the squared error.</li><li class="listitem1">If we are getting multiple empty clusters, then we have to repeat this process again several times.</li></ol></div></div><div class="calibre2" title="Outliers in the dataset"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec51" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Outliers in the dataset</h3></div></div></div><p class="calibre11">When we are working with the squared error, then the outliers can be the decisive factor and can influence the clusters that are formed. This means that when there are outliers in the dataset, then we may not achieve the desired cluster or the cluster that truly represents the grouped data points may not similar features.</p><p class="calibre11">This also leads to a higher sum of squared errors. Therefore, a common practice is to remove the outliers before applying the clustering algorithm.</p><p class="calibre11">There can also be some situations where we may not want to remove the outliers. Some of the points, such as unusual activity on the Web, excessive credit, and so on, are interesting and important to the business.</p></div></div><div class="calibre2" title="Different types of cluster"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec100" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Different types of cluster</h2></div></div></div><p class="calibre11">There are limitations with K-means. The most common limitation of K-means is that it faces difficulty in identifying the natural clusters. By natural clusters we mean:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Non-spherical/circular in shape</li><li class="listitem">Clusters of different sizes</li><li class="listitem">Clusters of different densities</li></ul></div><div class="tip" title="Tip"><div class="inner"><h3 class="title7"><a id="tip6" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Tip</h3><p class="calibre27">K-means can fail if there are few denser clusters and a not so dense cluster.</p></div></div><p class="calibre11">Here is a diagram of clusters of different sizes:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_07_004.jpg" alt="Different types of cluster" class="calibre193"/></div><p class="calibre11">
</p><p class="calibre11">The preceding figure has two images. In the first image, we have original points and in the second image, we have three K-means clusters. We can see that these are not accurate. This happens when clusters are of different sizes.</p><p class="calibre11">Here is a diagram of clusters of different densities:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_07_005.jpg" alt="Different types of cluster" class="calibre194"/></div><p class="calibre11">
</p><p class="calibre11">The preceding figure has two images. In the first image, we have original points and in the second image, we have three K-means clusters. The clusters are of different densities.</p><p class="calibre11">Here is a diagram of non-globular clusters:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_07_006.jpg" alt="Different types of cluster" class="calibre195"/></div><p class="calibre11">
</p><p class="calibre11">The preceding figure has two images. In the first image, we have original points and in the second image, we have two K-means clusters. The clusters are non-circular or non-globular in nature and the K-means algorithm was not able to detect them properly.</p><div class="calibre2" title="K-means – strengths and weaknesses"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec52" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>K-means – strengths and weaknesses</h3></div></div></div><p class="calibre11">There are many strengths and a few weaknesses of K-means. Let's discuss the strengths first:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">K-means can be used for various types of data.</li><li class="listitem">It is simple to understand and implement.</li><li class="listitem">It is efficient, even with repeated and multiple iterations.</li><li class="listitem">Bisecting K-means, a variant of simple K-means is more efficient. We will discuss that later in more detail.</li></ul></div><p class="calibre11">Some weaknesses or drawbacks of K-means clustering include:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">It is not suitable for every type of data.</li><li class="listitem">As seen in the previous examples, it doesn't work well for clusters of different densities, sizes, or non-globular clusters.</li><li class="listitem">There are issues when there are outliers in the dataset.</li><li class="listitem">K-means has a big constraint in that it makes the cluster by computing the center. Therefore, our data should be such that can have a "center".</li></ul></div></div></div><div class="calibre2" title="Bisecting K-means algorithm"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec101" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Bisecting K-means algorithm</h2></div></div></div><p class="calibre11">Bisecting K-means is an extension of the simple K-means algorithm. Here we find out the K clusters by splitting the set of all points into two clusters. Then we take one of these clusters and split it again. The process continues until the K clusters are formed.</p><p class="calibre11">The algorithm of bisecting K-means is:</p><div class="calibre2"><ol class="orderedlist"><li class="listitem1">First we need to initialize the list of clusters that will have the cluster consisting of all the data points.</li><li class="listitem1">Repeat the following:<div class="calibre2"><ol class="orderedlist1"><li class="listitem1">Now we remove one cluster from the list of the clusters</li><li class="listitem1">We now do trials of bisecting the cluster multiple times</li><li class="listitem1">For n=1 to the number of trials in the previous step</li></ol></div><p class="calibre44">
</p></li><li class="listitem1">The cluster is bisected using K-means:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Two clusters are selected from the result that has the lowest total sum of squared errors</li><li class="listitem">These two clusters are added to the list of the clusters</li></ul></div><p class="calibre44">
</p></li><li class="listitem1">The previous steps are performed until we have the K clusters in the list.</li></ol></div><p class="calibre11">There are several ways we can split a cluster:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Largest cluster</li><li class="listitem">Cluster which has the largest sum of squared errors</li><li class="listitem">Both</li></ul></div><p class="calibre11">We will use the iris dataset from the RDatasets for this example:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_07_007.jpg" alt="Bisecting K-means algorithm" class="calibre196"/></div><p class="calibre11">
</p><p class="calibre11">This is a simple example of the famous iris dataset. We are clustering the data points using <code class="literal">PetalLength</code> and <code class="literal">PetalWidth</code>.</p><p class="calibre11">The result is as follows:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_07_008.jpg" alt="Bisecting K-means algorithm" class="calibre197"/></div><p class="calibre11">
</p></div><div class="calibre2" title="Getting deep into hierarchical clustering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec102" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Getting deep into hierarchical clustering</h2></div></div></div><p class="calibre11">This is the second most used clustering technique after K-means. Let's take the same example again:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_07_03.jpg" alt="Getting deep into hierarchical clustering" class="calibre192"/></div><p class="calibre11">
</p><p class="calibre11">Here, the top most root represents all the data points or one cluster. Now we have three sub-clusters represented by nodes. All these three clusters have two sub-clusters. And these sub-clusters have further sub-clusters in them. These sub-clusters help in finding the clusters that are pure - that means, those which share most of the features.</p><p class="calibre11">There are two ways with which we can approach hierarchical clustering:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Agglomerative</strong></span>: This is based on the concept of cluster proximity. We initially start with treating each point as the individual cluster and then step by step we merge the closest pairs.</li><li class="listitem"><span class="strong"><strong class="calibre19">Divisive</strong></span>: Here we start with one cluster containing all the data points and then we start splitting it until clusters having individual points are left. In this case, we decide how the splitting should be done.</li></ul></div><p class="calibre11">Hierarchical clusters are represented as a tree-like diagram, also known as a dendogram. This is used to represent a cluster-subcluster relationship and how the clusters are merged or split (agglomerative or divisive).</p></div><div class="calibre2" title="Agglomerative hierarchical clustering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec103" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Agglomerative hierarchical clustering</h2></div></div></div><p class="calibre11">This is the bottom-up approach of hierarchical clustering. Here, each observation is treated as an individual cluster. Pairs of these clusters are merged together on the basis of similarity and we move up.</p><p class="calibre11">These clusters are merged together based on the smallest distance. When these two clusters are merged, they are treated as a new cluster. These steps are repeated when there is one single cluster left in the pool of data points.</p><p class="calibre11">The algorithm of agglomerative hierarchical clustering is:</p><div class="calibre2"><ol class="orderedlist"><li class="listitem1">Firstly, the proximity matrix is computed.</li><li class="listitem1">The two closest clusters are merged.</li><li class="listitem1">The proximity matrix created in the first step is updated after the merging of the two clusters.</li><li class="listitem1">Step 2 and step 3 are repeated until there is only one cluster remaining.</li></ol></div><div class="calibre2" title="How proximity is computed"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec53" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>How proximity is computed</h3></div></div></div><p class="calibre11">Step 3 in the previous algorithm is a very important step. It is the proximity measure between the two clusters.</p><p class="calibre11">There are various ways to define this:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">MIN</strong></span>: The two closest points of different clusters define the proximity of these clusters. This is the shortest distance.</li><li class="listitem"><span class="strong"><strong class="calibre19">MAX</strong></span>: Opposite to MIN, MAX takes the farthest point in the clusters and computes the proximity between these two which is taken as the proximity of these clusters.</li><li class="listitem"><span class="strong"><strong class="calibre19">Average</strong></span>: One other approach is to take the average of all the data points of the different clusters and compute the proximity according to these points.</li></ul></div><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_07_10.jpg" alt="How proximity is computed" class="calibre198"/></div><p class="calibre11">
</p><p class="calibre11">The preceding diagram depicts the proximity measure using MIN.</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_07_11.jpg" alt="How proximity is computed" class="calibre199"/></div><p class="calibre11">
</p><p class="calibre11">The preceding diagram depicts the proximity measure using MAX.</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_07_12.jpg" alt="How proximity is computed" class="calibre200"/></div><p class="calibre11">
</p><p class="calibre11">The preceding diagram depicts the proximity measure using Average.</p><p class="calibre11">These methods are also known as:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Single Linkage</strong></span>: MIN</li><li class="listitem"><span class="strong"><strong class="calibre19">Complete Linkage</strong></span>: MAX</li><li class="listitem"><span class="strong"><strong class="calibre19">Average Linkage</strong></span>: Average</li></ul></div><p class="calibre11">There is also another method known as the centroid method.</p><p class="calibre11">In the centroid method, the proximity distance is computed using two mean vectors of the clusters. At every stage, the two clusters are combined depending on which has the smallest centroid distance.</p><p class="calibre11">Let's take the following example:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_07_13.jpg" alt="How proximity is computed" class="calibre201"/></div><p class="calibre11">
</p><p class="calibre11">The preceding diagram shows seven points in an x-y plane. If we start to do agglomerative hierarchical clustering, the process would be as follows:</p><div class="calibre2"><ol class="orderedlist"><li class="listitem1">{1},{2},{3},{4},{5},{6},{7}.</li><li class="listitem1">{1},{2,3},{4},{5},{6},{7}.</li><li class="listitem1">{1,7},{2,3},{4},{5},{6},{7}.</li><li class="listitem1">{1,7},{2,3},{4,5},{6}.</li><li class="listitem1">{1,7},{2,3,6},{4,5}.</li><li class="listitem1">{1,7},{2,3,4,5,6}.</li><li class="listitem1">{1,2,3,4,5,6,7}.</li></ol></div><p class="calibre11">This was broken down into seven steps to make the complete whole cluster.</p><p class="calibre11">This can also be shown by the following dendogram:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_07_014.jpg" alt="How proximity is computed" class="calibre202"/></div><p class="calibre11">
</p><p class="calibre11">This represents the previous seven steps of the agglomerative hierarchical clustering.</p></div><div class="calibre2" title="Strengths and weaknesses of hierarchical clustering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec54" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Strengths and weaknesses of hierarchical clustering</h3></div></div></div><p class="calibre11">The hierarchical clustering discussed earlier is sometimes more or less suited to a given problem. We will be able to comprehend this by understanding strengths and weaknesses of the hierarchical clustering:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Agglomerative clustering lacks a global objective function. Such algorithms get the benefit of not having the local minima and no issues in choosing the initial points.</li><li class="listitem">Agglomerative clustering handles clusters of different sizes well.</li><li class="listitem">It is considered that agglomerative clustering produces better quality clusters.</li><li class="listitem">Agglomerative clustering is generally computationally expensive and doesn't work well with high-dimensional data.</li></ul></div></div></div><div class="calibre2" title="Understanding the DBSCAN technique"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec104" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding the DBSCAN technique</h2></div></div></div><p class="calibre11">
<span class="strong"><strong class="calibre19">DBSCAN</strong></span> refers to <span class="strong"><strong class="calibre19">Density-based Spatial Clustering of Applications with Noise</strong></span>. It is a data clustering algorithm that uses density-based expansion of the seed (starting) points to find the clusters.</p><p class="calibre11">It locates the regions of high density and separates them from the others using the low densities between them.</p><div class="calibre2" title="So, what is density?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec55" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>So, what is density?</h3></div></div></div><p class="calibre11">In the center-based approach, the density is computed at a particular point in the dataset by using the number of points in the specified radius. This is easy to implement and the density of the point is dependent on the specified radius.</p><p class="calibre11">For example, a large radius corresponds to the density at point <span class="strong"><em class="calibre23">m</em></span>, where m is the number of data points inside the radius. If the radius is small, then the density can be 1 because only one point exists.</p></div><div class="calibre2" title="How are points classified using center-based density"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec56" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>How are points classified using center-based density</h3></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Core point</strong></span>: The points that lie inside the density-based cluster are the core points. These lie in the interior of the dense region.</li><li class="listitem"><span class="strong"><strong class="calibre19">Border point</strong></span>: These points lie within the cluster, but are not the core points. They lie in the neighborhood of the core points. These lie on the boundary or edge of the dense region.</li><li class="listitem"><span class="strong"><strong class="calibre19">Noise points</strong></span>: The points that are not the core points or the border points are the noise points.</li></ul></div></div><div class="calibre2" title="DBSCAN algorithm"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec57" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>DBSCAN algorithm</h3></div></div></div><p class="calibre11">Points very close to each other are put together in the same cluster. Points lying close to these points are also put together. Points that are very far (noise points) are discarded.</p><p class="calibre11">The algorithm of the DBSCAN is given by:</p><div class="calibre2"><ol class="orderedlist"><li class="listitem1">Points are labeled as core, border, or noise points.</li><li class="listitem1">Noise points are eliminated.</li><li class="listitem1">An edge is formed between the core points using the special radius.</li><li class="listitem1">These core points are made into a cluster.</li><li class="listitem1">Border points associated to these core points are assigned to these clusters.</li></ol></div></div><div class="calibre2" title="Strengths and weaknesses of the DBSCAN algorithm"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch07lvl3sec58" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Strengths and weaknesses of the DBSCAN algorithm</h3></div></div></div><p class="calibre11">The hierarchical clustering discussed earlier is sometimes more or less suited to a given problem. We will be able to comprehend this by understanding strengths and weaknesses of the hierarchical clustering.</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">DBSCAN can handle clusters of different shapes and sizes. It is able to do this because it creates the definition of the cluster using the density.</li><li class="listitem">It is resistant to noise. It is able to perform better than K-means in terms of finding more clusters.</li><li class="listitem">DBSCAN faces issues with datasets that have varied densities.</li><li class="listitem">Also, it has issues dealing with high-dimensional data because it becomes difficult to find densities in such data.</li><li class="listitem">It's computationally intensive when computing nearest neighbors.</li></ul></div></div></div><div class="calibre2" title="Cluster validation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec105" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Cluster validation</h2></div></div></div><p class="calibre11">Cluster validation is important as it tells us that the generated clusters are relevant or not. Important points to consider when dealing with the cluster validation include:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">It has the ability to distinguish whether non-random structure in the data actually exists or not</li><li class="listitem">It has the ability to determine the actual number of clusters</li><li class="listitem">It has the ability to evaluate how the data is fit to the cluster</li><li class="listitem">It should be able to compare two sets of clusters to find out which cluster is better</li></ul></div></div><div class="calibre2" title="Example"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec106" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Example</h2></div></div></div><p class="calibre11">We will be using <code class="literal">ScikitLearn.jl</code> in our example of agglomerative hierarchical clustering and DBSCAN.</p><p class="calibre11">As discussed previously, <code class="literal">ScikitLearn.jl</code> aims to provide a similar library such as the actual scikit-learn for Python.</p><p class="calibre11">We will first add the required packages to our environment:</p><pre class="programlisting">
<span class="strong"><strong class="calibre19">julia&gt; Pkg.update() 
julia&gt; Pkg.add("ScikitLearn") 
julia&gt; Pkg.add("PyPlot") 
</strong></span>
</pre><p class="calibre11">This also requires us to have the scikit-learn in our Python environment. If it is not already installed, we can install it using:</p><pre class="programlisting">
<span class="strong"><strong class="calibre19">$ conda install scikit-learn 
</strong></span>
</pre><p class="calibre11">After this, we can start with our example. We will try out the different clustering algorithms available in <code class="literal">ScikitLearn.jl</code>. This is provided in the examples of <code class="literal">ScikitLearn.jl</code>:</p><pre class="programlisting">
<span class="strong"><strong class="calibre19">julia&gt; @sk_import datasets: (make_circles, make_moons, make_blobs) 
julia&gt; @sk_import cluster: (estimate_bandwidth, MeanShift, MiniBatchKMeans, AgglomerativeClustering, SpectralClustering) 
 
julia&gt; @sk_import cluster: (DBSCAN, AffinityPropagation, Birch) 
julia&gt; @sk_import preprocessing: StandardScaler 
julia&gt; @sk_import neighbors: kneighbors_graph 
</strong></span>
</pre><p class="calibre11">We imported the datasets from the official scikit-learn library and the clustering algorithms. As some of these are dependent on the distance measure of neighbors, we also imported kNN:</p><pre class="programlisting">
<span class="strong"><strong class="calibre19">julia&gt; srand(33) 
 
julia&gt; # Generate datasets. 
 
julia&gt; n_samples = 1500 
julia&gt; noisy_circles = make_circles(n_samples=n_samples, factor=.5, noise=.05) 
julia&gt; noisy_moons = make_moons(n_samples=n_samples, noise=.05) 
julia&gt; blobs = make_blobs(n_samples=n_samples, random_state=8) 
julia&gt; no_structure = rand(n_samples, 2), nothing 
</strong></span>
</pre><p class="calibre11">This particular snippet will generate the required datasets. The dataset generated will be of good enough size to test these different algorithms:</p><pre class="programlisting">
<span class="strong"><strong class="calibre19">julia&gt; colors0 = collect("bgrcmykbgrcmykbgrcmykbgrcmyk") 
julia&gt; colors = vcat(fill(colors0, 20)...) 
 
julia&gt; clustering_names = [ 
    "MiniBatchKMeans", "AffinityPropagation", "MeanShift", 
    "SpectralClustering", "Ward", "AgglomerativeClustering", 
    "DBSCAN", "Birch"]; 
</strong></span>
</pre><p class="calibre11">We assigned names to these algorithms and colors to fill the image:</p><pre class="programlisting">
<span class="strong"><strong class="calibre19">julia&gt; figure(figsize=(length(clustering_names) * 2 + 3, 9.5)) 
julia&gt; subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01) 
 
julia&gt; plot_num = 1 
 
julia&gt; datasets = [noisy_circles, noisy_moons, blobs, no_structure] 
</strong></span>
</pre><p class="calibre11">Now, we assign how the images will be formed for different algorithms and datasets:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_07_15.jpg" alt="Example" class="calibre203"/></div><p class="calibre11">
</p><p class="calibre11">Here, we are normalizing the dataset to easily select the parameters, and initializing the <code class="literal">kneighbors_graph</code> for the algorithms requiring the distance measure:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_07_16.jpg" alt="Example" class="calibre204"/></div><p class="calibre11">
</p><p class="calibre11">Here, we are creating the clustering estimators, which are required by the algorithms to behave accordingly to the use case:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_07_17.jpg" alt="Example" class="calibre205"/></div><p class="calibre11">
</p><p class="calibre11">The similar estimators for different algorithms.</p><p class="calibre11">After this, we use these algorithms on our datasets:</p><pre class="programlisting">for (name, algorithm) in zip(clustering_names, clustering_algorithms) 
    fit!(algorithm, X) 
    y_pred = nothing 
    try 
        y_pred = predict(algorithm, X) 
    catch e 
        if isa(e, KeyError) 
            y_pred = map(Int, algorithm[:labels_]) 
            clamp!(y_pred, 0, 27) # not sure why some algorithms return -1 
        else rethrow() end 
    end 
    subplot(4, length(clustering_algorithms), plot_num) 
    if i_dataset == 1 
        title(name, size=18) 
    end 
 
    for y_val in unique(y_pred) 
        selected = y_pred.==y_val 
        scatter(X[selected, 1], X[selected, 2], color=string(colors0[y_val+1]), s=10) 
    end 
 
    xlim(-2, 2) 
    ylim(-2, 2) 
    xticks(()) 
    yticks(()) 
    plot_num += 1 
end 
</pre><p class="calibre11">The result obtained is as follows:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_07_018.jpg" alt="Example" class="calibre206"/></div><p class="calibre11">
</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">We can see that agglomerative clustering and DBSCAN performed really well in the first two datasets</li><li class="listitem">Agglomerative clustering didn't perform well in the third dataset, whereas DBSCAN did</li><li class="listitem">Agglomerative clustering and DBSCAN both performed poorly on the fourth dataset</li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec63" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we learned about unsupervised learning and how it is different from Supervised learning. We discussed various use cases where Unsupervised learning is used.</p><p class="calibre11">We went through the different Unsupervised learning algorithms and discussed their algorithms, and strengths and weaknesses over each other.</p><p class="calibre11">We discussed various clustering techniques and how clusters are formed. We learned how different the clustering algorithms are from each other and how they are suited to particular use cases.</p><p class="calibre11">We learned about K-means, Hierarchical clustering, and DBSCAN.</p><p class="calibre11">In the next chapter, we will learn about Ensemble learning.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec64" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/JuliaLang/julia">https://github.com/JuliaLang/julia</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/JuliaStats/Clustering.jl">https://github.com/JuliaStats/Clustering.jl</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://juliastats.github.io/">http://juliastats.github.io/</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/stevengj/PyCall.jl">https://github.com/stevengj/PyCall.jl</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/cstjean/ScikitLearn.jl">https://github.com/cstjean/ScikitLearn.jl</a></li></ul></div></div></div>



  </body></html>