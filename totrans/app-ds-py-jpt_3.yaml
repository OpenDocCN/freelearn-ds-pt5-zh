- en: '3'
  prefs: []
  type: TYPE_NORMAL
- en: Web Scraping and Interactive Visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe how HTTP requests work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scrape tabular data from a web page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build and transform Pandas DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create interactive visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will learn the fundamentals of HTTP requests, scrape web
    page data, and then create interactive visualizations using the Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far in this book, we have focused on using Jupyter to build reproducible
    data analysis pipelines and predictive models. We'll continue to explore these
    topics in this chapter, but the main focus here is data acquisition. In particular,
    we will show you how data can be acquired from the web using HTTP requests. This
    will involve scraping web pages by requesting and parsing HTML. We will then wrap
    up this chapter by using interactive visualization techniques to explore the data
    we've collected.
  prefs: []
  type: TYPE_NORMAL
- en: The amount of data available online is huge and relatively easy to acquire.
    It's also continuously growing and becoming increasingly important. Part of this
    continual growth is the result of an ongoing global shift from newspapers, magazines,
    and TV to online content. With customized newsfeeds available all the time on
    cell phones, and live-news sources such as Facebook, Reddit, Twitter, and YouTube,
    it's difficult to imagine the historical alternatives being relevant much longer.
    Amazingly, this accounts for only some of the increasingly massive amounts of
    data available online.
  prefs: []
  type: TYPE_NORMAL
- en: With this global shift toward consuming content using HTTP services (blogs,
    news sites, Netflix, and so on), there are plenty of opportunities to use data-driven
    analytics. For example, Netflix looks at the movies a user watches and predicts
    what they will like. This prediction is used to determine the suggested movies
    that appear. In this chapter, however, we won't be looking at "business-facing"
    data as such, but instead we will see how the client can leverage the internet
    as a database. Never before has this amount and variety of data been so easily
    accessible. We'll use web-scraping techniques to collect data, and then we'll
    explore it with interactive visualizations in Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive visualization is a visual form of data representation, which helps
    users understand the data using graphs or charts. Interactive visualization helps
    a developer or analyst present data in a simple form, which can be understood
    by non-technical personnel too.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping Web Page Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the spirit of leveraging the internet as a database, we can think about acquiring
    data from web pages either by scraping content or by interfacing with web APIs.
    Generally, scraping content means getting the computer to read data that was intended
    to be displayed in a human-readable format. This is in contradistinction to web
    APIs, where data is delivered in machine-readable formats—the most common being
    JSON.
  prefs: []
  type: TYPE_NORMAL
- en: In this topic, we will focus on web scraping. The exact process for doing this
    will depend on the page and desired content. However, as we will see, it's quite
    easy to scrape anything we need from an HTML page so long as we have an understanding
    of the underlying concepts and tools. In this topic, we'll use Wikipedia as an
    example and scrape tabular content from an article. Then, we'll apply the same
    techniques to scrape
  prefs: []
  type: TYPE_NORMAL
- en: data from a page on an entirely separate domain. But first, we'll take some
    time to introduce HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to HTTP Requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Hypertext Transfer Protocol, or HTTP for short, is the foundation of data
    communication for the internet. It defines how a page should be requested and
    how the response should look. For example, a client can request an Amazon page
    of laptops for sale, a Google search of local restaurants, or their Facebook feed.
    Along with the URL, the request will contain the user agent and available browsing
    cookies among the contents of the request header. The user agent tells the server
    what browser and device the client is using, which is usually used to provide
    the most user-friendly version of the web page's response. Perhaps they have recently
    logged in to the web page; such information would be stored in a cookie that might
    be used to automatically log the user in.
  prefs: []
  type: TYPE_NORMAL
- en: These details of HTTP requests and responses are taken care of under the hood
    thanks to web browsers. Luckily for us, today the same is true when making requests
    with high-level languages such as Python. For many purposes, the contents of request
    headers can be largely ignored. Unless otherwise specified, these are automatically
    generated in Python when requesting a URL. Still, for the purposes of troubleshooting
    and understanding the responses yielded by our requests, it's useful to have a
    foundational understanding of HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of HTTP methods, such as GET, HEAD, POST, and PUT. The
    first two are used for requesting that data be sent from the server to the client,
    whereas the last two are used for sending data to the server.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take a look at this GET request example for the `User-Agent` is Mozilla/5.0,
    which corresponds to a standard desktop browser. Among other lines in the header,
    we note the `Accept` and `Accept-Language` fields, which specify the acceptable
    content types and language of the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'These HTTP methods are summarized below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GET**: Retrieves the information from the specified URL'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HEAD**: Retrieves the meta information from the HTTP header of the specified
    URL'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**POST**: Sends the attached information for appending to the resource(s) at
    the specified URL'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PUT**: Sends the attached information for replacing the resource(s) at the
    specified URL'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A**GET** request is sent each time we type a web page address into our browser
    and press Enter. For web scraping, this is usually the only HTTP method we are
    interested in, and it's the only method we'll be using in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the request has been sent, a variety of response types can be returned
    from the server. These are labeled with 100-level to 500-level codes, where the
    first digit in the code represents the response class. These can be described
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1xx**: Informational response, for example, server is processing a request.
    It''s uncommon to see this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2xx**: Success, for example, page has loaded properly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3xx**: Redirection, for example, the requested resource has been moved and
    we were redirected to a new URL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4xx**: Client error, for example, the requested resource does not exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5xx**: Server error, for example, the website server is receiving too much
    traffic and could not fulfill the request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the purposes of web scraping, we usually only care about the response class,
    that is, the first digit of the response code. However, there exist subcategories
    of responses within each class that offer more granularity on what's going on.
    For example, a 401 code indicates an unauthorized response, whereas a 404 code
    indicates a page not found response. This distinction is noteworthy because a
    404 would indicate we've requested a page that does not exist, whereas 401 tells
    us we need to log in to view the particular resource.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how HTTP requests can be done in Python and explore some of these
    topics using the Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Making HTTP Requests in the Jupyter Notebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we've talked about how HTTP requests work and what type of responses
    we should expect, let's see how this can be done in Python. We'll use a library
    called `urllib`, for making HTTP requests, but `urllib` in the official Python
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Requests** is a great choice for making simple and advanced web requests.
    It allows for all sorts of customization with respect to headers, cookies, and
    authorization. It tracks redirects and provides methods for returning specific
    page content such as JSON. Furthermore, there''s an extensive suite of advanced
    features. However, it does not allow JavaScript to be rendered.'
  prefs: []
  type: TYPE_NORMAL
- en: Oftentimes, servers return HTML with JavaScript code snippets included, which
    are automatically run in the browser on load time. When requesting content with
    Python using Requests, this JavaScript code is visible, but it does not run. Therefore,
    any elements that would be altered or created by doing so are missing. Often,
    this does not affect the ability to get the desired information, but in some cases
    we may need to render the JavaScript in order to scrape the page properly. For
    doing this, we could use a library like Selenium.
  prefs: []
  type: TYPE_NORMAL
- en: This has a similar API to the Requests library, but provides support for rendering
    JavaScript using web drivers. It can even run JavaScript commands on live pages,
    for example, to change the text color or scroll to the bottom of the page.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For more information, refer to: [http://docs.python-requests.org/en/master/user/advanced/](http://docs.python-requests.org/en/master/user/advanced/)
    and [http://selenium-python.readthedocs.io/.](http://selenium-python.readthedocs.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: Let's dive into an exercise using the Requests library with Python in a Jupyter
    Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14: Handling HTTP Requests With Python in a Jupyter Notebook'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Start the `NotebookApp` from the project directory by executing jupyter notebook.
    Navigate to the `lesson-3` directory and open up the l`esson- 3-workbook.ipynb`
    file. Find the cell near the top where the packages are loaded and run it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are going to request a web page and then examine the response object. There
    are many different libraries for making requests and many choices for exactly
    how to do so with each. We'll only use the Requests library, as it provides excellent
    documentation, advanced features, and a simple API.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scroll down to `Subtopic A: Introduction to HTTP requests` and run the first
    cell in that section to import the Requests library. Then, prepare a request by
    running the cell containing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use the Request class to prepare a GET request to the jupyter.org homepage.
    By specifying the user agent as Mozilla/5.0, we are asking for a response that
    would be suitable for a standard desktop browser. Finally, we prepare the request.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the docstring for the "prepared request" req, by running the cell containing `req?`:![
    Figure 3.1: Printing the docstring for req'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_03_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.1: Printing the docstring for req'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking at its usage, we see how the request can be sent using a session. This
    is similar to opening a web browser (starting a session) and then requesting a
    URL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make the request and store the response in a variable named page, by running
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code returns the HTTP response, as referenced by the page variable. By
    using the `with` statement, we initialize a session whose scope is limited to
    the indented code block. This means we do not have to worry about explicitly closing
    the session, as it is done automatically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the next two cells in the notebook to investigate the response. The string
    representation of page should indicate a 200 status code response. This should
    agree with the `status_code` attribute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Save the response text to the `page_html` variable and take a look at the head
    of the string with `page_html[:1000]`:![Figure 3.2: The HTML response text](img/C13018_03_02.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 3.2: The HTML response text'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As expected, the response is HTML. We can format this output better with the
    help of `BeautifulSoup`, a library which will be used extensively for HTML parsing
    later in this section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the head of the formatted HTML by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We import `BeautifulSoup` and then print the output, where newlines are indented
    depending on their hierarchy in the HTML structure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can take this a step further and actually display the HTML in Jupyter by
    using the IPython display module. Do this by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.3: The output obtained when no images are loaded'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_03_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.3: The output obtained when no images are loaded'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s compare this to the live website, which can be opened in Jupyter using
    an IFrame. Do this by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.4: Rendering of the entire Jupyter website'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_03_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.4: Rendering of the entire Jupyter website'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we see the full site rendered, including JavaScript and external resources.
    In fact, we can even click on the hyperlinks and load those pages in the IFrame,
    just like a regular browsing session.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It's good practice to close the IFrame after using it. This prevents it from
    eating up memory and processing power. It can be closed by selecting the cell
    and clicking **Current Outputs** | **Clear** from the Cell menu in the Jupyter
    Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recall how we used a prepared request and session to request this content as
    a string in Python. This is often done using a shorthand method instead. The drawback
    is that we do not have as much customization of the request header, but that's
    usually fine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make a request to [http://www.python.org/](http://www.python.org/) by running
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The string representation of the page (as displayed beneath the cell) should
    indicate a 200 status code, indicating a successful response.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the next two cells. Here, we print the `url` and history attributes of our
    page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The URL returned is not what we input; notice the difference? We were redirected
    from the input URL, [http://www.python.org/,](http://www.python.org/) to the secured
    version of that page, [https://www.python.org/](http://www.python.org/). The difference
    is indicated by an additional s at the start of the URL, in the protocol. Any
    redirects are stored in the history attribute; in this case, we find one page
    in here with status code 301 (permanent redirect), corresponding to the original
    URL requested.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we're comfortable making requests, we'll turn our attention to parsing
    the HTML. This can be something of an art, as there are usually multiple ways
    to approach it, and the best method often depends on the details of the specific
    HTML in question.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing HTML in the Jupyter Notebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When scraping data from a web page, after making the request, we must extract
    the data from the response content. If the content is HTML, then the easiest way
    to do this is with a high-level parsing library such as Beautiful Soup. This is
    not to say it's the only way; in principle, it would be possible to pick out the
    data using regular expressions or Python string methods such as split, but pursuing
    either of these options would be an inefficient use of time and could easily lead
    to errors. Therefore, it's generally frowned upon and instead, the use of a trustworthy
    parsing tool is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand how content can be extracted from HTML, it's important
    to know the fundamentals of HTML. For starters, HTML stands for Hyper Text Markup
    Language. Like Markdown or XML (eXtensible Markup Language), it's simply a language
    for marking up text. In HTML, the display text is contained within the content
    section of HTML elements, where element attributes specify how that element should
    appear on the page.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: Fundamental blocks of HTML'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_03_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.5: Fundamental blocks of HTML'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking at the anatomy of an HTML element, as seen in the preceding picture,
    we see the content enclosed between start and end tags. In this example, the tags
    are `<p>` for paragraph; other common tag types are `<div>` (text block), `<table>`
    (data table),
  prefs: []
  type: TYPE_NORMAL
- en: '`<h1>` (heading), `<img>` (image), and `<a>` (hyperlinks). Tags have attributes,
    which can hold important metadata. Most commonly, this metadata is used to specify
    how the element text should appear on the page. This is where CSS files come into
    play. The attributes can store other useful information, such as the hyperlink
    `href` in an `<a>` tag, which specifies a URL link, or the alternate alt label
    in an `<img>` tag, which specifies the text to display if the image resource cannot
    be loaded.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's turn our attention back to the Jupyter Notebook and parse some HTML!
    Although not necessary when following along with this exercise, it's very helpful
    in real-world situations to use the developer tools in Chrome or Firefox to help
    identify the HTML elements of interest. We'll include instructions for doing this
    with Chrome in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 15: Parsing HTML With Python in a Jupyter Notebook'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In `lesson-3-workbook.ipynb` file, scroll to the top of `Subtopic B: Parsing
    HTML` with Python.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this exercise, we'll scrape the central bank interest rates for each country,
    as reported by Wikipedia. Before diving into the code, let's first open up the
    web page containing this data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Go to [https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates](https://en.wikipedia.org/wiki/List_of_countries_by_central_bank_interest_rates)
    in a web browser. Use Chrome, if possible, as later in this exercise we'll show
    you how to view and search the HTML using Chrome's developer tools.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking at the page, we see very little content other than a big list of countries
    and their interest rates. This is the table we'll be scraping.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Return to the Jupyter Notebook and load the HTML as a Beautiful Soup object
    so that it can be parsed. Do this by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use Python's default html.parser as the parser, but third-party parsers such
    as `lxml` may be used instead, if desired. Usually, when working with a new object
    like this Beautiful Soup one, it's a good idea to pull up the docstring by doing
    `soup?`. However, in this case, the docstring is not particularly informative.
    Another tool for exploring Python objects is `pdir`, which lists all of an object's
    attributes and methods (this can be installed with pip install `pdir2`). It's
    basically a formatted version of Python's built-in `dir` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Display the attributes and methods for the BeautifulSoup object by running
    the following code. This will run, regardless of whether or not the `pdir` external
    library is installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we see a list of methods and attributes that can be called on soup. The
    most commonly used function is probably `find_all`, which returns a list of elements
    that match the given criteria.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Get the h1 heading for the page with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Usually, pages only have one H1 (top-level heading) element, so it's no surprise
    that we only find one here.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the next couple of cells. We redefine H1 to the first (and only) list element
    with `h1 = h1[0]`, and then print out the HTML element attributes with `h1.attrs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We see the class and ID of this element, which can both be referenced by CSS
    code to define the style of this element.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Get the HTML element content (that is, the visible text) by printing `h1.text`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get all the images on the page by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are lots of images on the page. Most of these are for the country flags.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the source of each image by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use a list comprehension to iterate through the elements, selecting the `src`
    attribute of each (so long as that attribute is actually available).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let's scrape the table. We'll use Chrome's developer tools to hunt down
    the element this is contained within.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.6: Scraping the table on the target web page'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.6: Scraping the table on the target web page'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: If not already done, open the Wikipedia page we're looking at in Chrome. Then,
    in the browser, select **Developer Tools** from the **View** menu. A sidebar will
    open. The HTML is available to look at from the **Elements** tab in Developer
    Tools.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the little arrow in the top left of the tools sidebar. This allows us
    to hover over the page and see where the HTML element is located, in the **Elements**
    section of the sidebar:![Figure 3.7: Arrow Icon for locating the HTML element'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_03_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.7: Arrow Icon for locating the HTML element'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Hover over the body to see how the table is contained within the div that has
    `id="bodyContent"`:![Figure 3.8: HTML code for table on the target web page'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_03_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.8: HTML code for table on the target web page'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Select that `div` by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can now seek out the table within this subset of the full HTML. Usually,
    tables are organized into headers `<th>`, rows `<tr>`, and data entries `<td>`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Get the table headers by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we see three headers. In the content of each is a break element `<br/>`,
    which will make the text a bit more difficult to cleanly parse.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Get the text by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we get the content with the `get_text` method, and then run the replace
    string method to remove the newline resulting from the `<br/>` element.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To get the data, we'll first perform some tests and then scrape all the data
    in a single cell.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Get the data for each cell in the second `<tr>` (row) element by running the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We find all the row elements, pick out the third one, and then find the three
    data elements inside that.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's look at the resulting data and see how to parse the text from each row.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the next couple of cells to print `d1` and its text attribute:![Figure
    3.9: Printing d1 and its text attribute'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_03_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.9: Printing d1 and its text attribute'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We're getting some undesirable characters at the front. This can be solved by
    searching for only the text of the `<a>` tag.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run `d1.find('a').text` to return the properly *cleaned* data for that cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the next couple of cells to print `d2` and its text. This data appears to
    be clean enough to convert directly into a float.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the next couple of cells to print `d3` and its text:![Figure 3.10: Printing
    d3 and its text attribute'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.10: Printing `d3` and its text attribute'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Similar to `d1`, we see that it would be better to get only the span element's
    text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Properly parse the date for this table entry by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''re ready to perform the full scrape by iterating over the row elements
    `<th>`. Run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the complete code, refer to the following: [https://bit.ly/2EKMNbV](https://bit.ly/2EKMNbV).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We iterate over the rows, ignoring any that contain more than three data elements.
    These rows will not correspond to data in the table we are interested in. Rows
    that do have three data elements are assumed to be in the table, and we parse
    the text from these as identified during the testing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The text parsing is done inside a `try/except` statement, which will catch any
    errors and allow this row to be skipped without stopping the iteration. Any rows
    that raise errors due to this statement should be looked at. The data for these
    could be recorded manually or accounted for by altering the scraping loop and
    re-running it. In this case, we'll ignore any errors for the sake of time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the head of the scraped data list by running `print(data[:10])`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll visualize this data later in the chapter. For now, save the data to
    a CSV file by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we are using semicolons to separate the fields.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 3: Web Scraping With Jupyter Notebooks'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You should have completed the previous exercise in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we are going to get the population of each country. Then,
    in the next topic, this will be visualized along with the interest rate data scraped
    in the previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The page we look at in this activity is available here: [http://www.worldometers.info/world-population/population-by-country/](http://www.worldometers.info/world-population/population-by-country/).'
  prefs: []
  type: TYPE_NORMAL
- en: Our aim is to apply the basic of web scrapping to a new web page and scrape
    some more data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This page may have changed since this document was created. If this URL no
    longer leads to a table of country populations, please use this Wikipedia page
    instead: [https://en.wikipedia.org/wiki/List_of_countries_by_population(United_Nations)](https://en.wikipedia.org/wiki/List_of_countries_by_population(United_Nations)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do this, the following steps have to be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: Scrape the data from the web page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `lesson-3-workbook.ipynb` Jupyter Notebook, scroll to `Activity A: Web
    scraping with Python`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `url` variable and load an IFrame of our page in the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Close the IFrame by selecting the cell and clicking **Current Outputs** | Clear
    from the **Cell** menu in the Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Request the page and load it as a `BeautifulSoup` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the H1 for the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get and print the table headings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select first three columns and parse the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the data for a sample row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many columns of data do we have? Print the length of `row_data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the first elements of the row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the data elements d1, d2, and d3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking at the `row_data` output, we can find out how to correctly parse the
    data. Select the content of the `<a>` element in the first data element, and then
    simply get the text from the others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scrape and parse the table data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the head of the scraped data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, save the data to a CSV file for later use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The detailed steps along with the solutions are presented in the *Appendix
    A*     (pg. no. 160).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To summarize, we've seen how Jupyter Notebooks can be used for web scraping.
    We started this chapter by learning about HTTP methods and status codes. Then,
    we used the Requests library to actually perform HTTP requests with Python and
    saw how the Beautiful Soup library can be used to parse the HTML responses.
  prefs: []
  type: TYPE_NORMAL
- en: Our Jupyter Notebook turned out to be a great tool for this type of work. We
    were able to explore the results of our web requests and experiment with various
    HTML parsing techniques. We were also able to render the HTML and even load a
    live version of the web page inside the notebook!
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next topic of this chapter, we shift to a completely new topic: interactive
    visualizations. We''ll see how to create and display interactive charts right
    inside the notebook, and use these charts as a way to explore the data we have
    just collected.'
  prefs: []
  type: TYPE_NORMAL
- en: Interactive Visualizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizations are quite useful as a means of extracting information from a
    dataset. For example, with a bar graph it's very easy to distinguish the value
    distribution, compared to looking at the values in a table. Of course, as we have
    seen earlier in this book, they can be used to study patterns in the dataset that
    would otherwise be quite difficult to identify. Furthermore, they can be used
    to help explain a dataset to an unfamiliar party. If included in a blog post,
    for example, they can boost reader interest levels and be used to break up blocks
    of text.
  prefs: []
  type: TYPE_NORMAL
- en: When thinking about interactive visualizations, the benefits are similar to
    static visualizations, but enhanced because they allow for active exploration
    on the viewer's part. Not only do they allow the viewer to answer questions they
    may have about the data, they also think of new questions while exploring. This
    can benefit a separate party such as a blog reader or co-worker, but also a creator,
    as it allows for easy ad hoc exploration of the data in detail, without having
    to change any code.
  prefs: []
  type: TYPE_NORMAL
- en: In this topic, we'll discuss and show how to use Bokeh to build interactive
    visualizations in Jupyter. Prior to this, however, we'll briefly revisit pandas
    DataFrames, which play an important role in doing data visualization with Python.
  prefs: []
  type: TYPE_NORMAL
- en: Building a DataFrame to Store and Organize Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we've seen time and time again in this book, pandas is an integral part of
    doing data science with Python and Jupyter Notebooks. DataFrames offer a way to
    organize and store labeled data, but more importantly, pandas provides time saving
    methods for transforming data within a DataFrame. Examples we have seen in this
    book include dropping duplicates, mapping dictionaries to columns, applying functions
    over columns, and filling in missing values.
  prefs: []
  type: TYPE_NORMAL
- en: With respect to visualizations, DataFrames offer methods for creating all sorts
    of matplotlib graphs, including `df.plot.barh()`, `df.plot.hist()`, and more.
    The interactive visualization library Bokeh previously relied on pandas DataFrames
    for their *high-level charts*. These worked similar to Seaborn, as we saw earlier
    in the previous chapter, where a DataFrame is passed to the plotting function
    along with the specific columns to plot. The most recent version of Bokeh, however,
    has dropped support for this behavior. Instead, plots are now created in much
    the same way as matplotlib, where the data can be stored in simple lists or NumPy
    arrays. The point of this discussion is that DataFrames are not entirely necessary,
    but still very helpful for organizing and manipulating the data prior to visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 16: Building and Merging Pandas DataFrames'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's dive right into an exercise, where we'll continue working on the country
    data we scraped earlier. Recall that we extracted the central bank interest rates
    and populations of each country, and saved the results in CSV files. We'll load
    the data from these files and merge them into a DataFrame, which will then be
    used as the data source for the interactive visualizations to follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `lesson-3-workbook.ipynb` of the Jupyter Notebook, scroll to the `Subtopic A:
    Building a DataFrame to store and organize data` subsection in the `Topic B` section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are first going to load the data from the CSV files, so that it's back to
    the state it was in after scraping. This will allow us to practice building DataFrames
    from Python objects, as opposed to using the `pd.read_csv` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: When using `pd.read_csv`, the datatype for each column will be inferred from
    the string input. On the other hand, when using `pd.DataFrame` as we do here,
    the datatype is instead taken as the type of the input variables. In our case, as will
    be seen, we read the file and do not bother converting the variables to numeric
    or date-time until after instantiating the DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the CSV files into lists by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check what the resulting lists look like by running the next two cells. We
    should see an output similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, the data is in a standard Python list structure, just as it was after scraping
    from the web pages in the previous sections. We're now going to create two DataFrames
    and merge them, so that all of the data is organized within one object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the standard DataFrame constructor to create the two DataFrames by running
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This isn't the first time we've used this function in this book. Here, we pass
    the lists of data (as seen previously) and the corresponding column names. The
    input data can also be of dictionary type, which can be useful when each column
    is contained in a separate list.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we're going to clean up each DataFrame. Starting with the interest rates
    one, let's print the head and tail, and list the data types.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When displaying the entire DataFrame, the default maximum number of rows is
    60 (for version 0.18.1). Let''s reduce this to 10 by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the head and tail of the interest rates DataFrame by running the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.11: Table for interest rates by country'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_03_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.11: Table for interest rates by country'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the data types by running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Pandas has assigned each column as a string datatype, which makes sense because
    the input variables were all strings. We'll want to change these to string, float,
    and datetime, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Convert to the proper datatypes by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use `astype` to cast the Interest Rate values as floats, setting `copy=False`
    to save memory. Since the date values are given in such an easy-to-read format,
    these can be converted simply by using `pd.to_datetime`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check the new datatypes of each column by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As can be seen, everything is now in the proper format.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s apply the same procedure to the other DataFrame. Run the next few cells
    to repeat the preceding steps for `df_populations`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.12: Table for population by country'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.12: Table for population by country'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Then, run this code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To cast the numeric columns as a float, we had to first apply some modifications
    to the strings in this case. We stripped away any commas from the populations
    and removed the percent sign from the Yearly Change column, using string methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we're going to merge the DataFrames on the country name for each row. Keep
    in mind that these are still the raw country names as scraped from the web, so
    there might be some work involved with matching the strings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Merge the DataFrames by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We pass the population data in the left DataFrame and the interest rates in
    the right one, performing an outer match on the country columns. This will result
    in `NaN` values where the two do not overlap.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the sake of time, let''s just look at the most populated countries to see
    whether we missed matching any. Ideally, we would want to check everything. Look
    at the most populous countries by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.13: The table for most populous countries'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_03_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.13: The table for most populous countries'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: It looks like U.S. didn't match up. This is because it's listed as *United States*
    in the interest rates data. Let's remedy this.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fix the label for U.S. in the populations table by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We rename the country for the populations DataFrame with the use of the `loc`
    method to locate that row.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let's merge the DataFrames properly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Re-merge the DataFrames on the country names, but this time use an inner merge
    to remove the `NaN` values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are left with two identical columns in the merged DataFrame. Drop one of
    them by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rename the columns by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are left with the following merged and cleaned DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.14: Ouput after cleaning and merging tables'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_03_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.14: Ouput after cleaning and merging tables'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that we have all the data in a nicely organized table, we can move on to
    the fun part: visualizing it. Let''s save this table to a CSV file for later use,
    and then move on to discuss how visualizations can be created with Bokeh.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Write the merged data to a CSV file for later use with the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Introduction to Bokeh
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bokeh is an interactive visualization library for Python. Its goal is to provide
    similar functionality to D3, the popular interactive visualization library for
    JavaScript. Bokeh functions very differently than D3, which is not surprising
    given the differences between Python and JavaScript. Overall, it's much simpler
    and it doesn't allow nearly as much customization as D3 does. This works to its
    advantage though, as it's much easier to use, and it still boasts an excellent
    suite of features that we'll explore in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Let's dive right into a quick exercise with the Jupyter Notebook and introduce
    Bokeh by example.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There is good documentation online for Bokeh, but much of it is outdated. Searching
    something like Bokeh bar plot in Google still tends to turn up documentation for
    legacy modules that no longer exist, for example, the high-level plotting tools
    that used to be available through `bokeh.charts` (prior to version 0.12.0). These
    are the ones that take pandas DataFrames as input in much the same way that Seaborn
    plotting functions do. Removing the high-level plotting tools module has simplified
    Bokeh, and will allow for more focused development going forward. Now, the plotting
    tools are largely grouped into the `bokeh.plotting` module, as will be seen in
    the next exercise and following activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 17: Introduction to Interactive Visualization With Bokeh'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We'll load the required Bokeh modules and show some simple interactive plots
    that can be made with Bokeh. Please note that the examples in this book have been
    designed using version 0.12.10 of Bokeh.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `lesson-3-workbook.ipynb` Jupyter notebook, scroll to `Subtopic B: Introduction
    to Bokeh`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Like scikit-learn, Bokeh modules are usually loaded in pieces (unlike pandas,
    for example, where the whole library is loaded at once). Import some basic plotting
    modules by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We need to run `output_notebook()` in order to render the interactive visuals
    within the Jupyter notebook.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate random data to plot by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The random data is generated using the cumulative sum of a random set of numbers
    that are distributed about zero. The effect is a trend that looks similar to a
    stock price time series, for example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the data with a line plot in Bokeh by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.15: An example data plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_03_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.15: An example data plot'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We instantiate the figure, as referenced by the variable `p`, and then plot
    a line. Running this in Jupyter yields an interactive figure with various options
    along the right-hand side.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The top three options (as of version 0.12.10) are **Pan**, **Box Zoom**, and
    **Wheel Zoom**. Play around with these and experiment with how they work. Use
    the reset option to re-load the default plot limits.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Other plots can be created with the alternative methods of `figure`. Draw a
    scatter plot by running the following code, where we replace `line` in the preceding
    code with `circle`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.16: An example scatter plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_03_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.16: An example scatter plot'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we've specified the size of each circle using a random set of numbers.
    A very enticing feature of interactive visualizations is the tooltip. This is
    a hover tool that allows the user to get information about a point by hovering
    over it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to add this tool, we''re going to use a slightly different method
    for creating the plot. This will require us to import a couple of new libraries.
    Run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This time, we'll create a data source to pass to the plotting method. This can
    contain metadata, which can be included in the visualization via the hover tool.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create random labels and plot the interactive visualization with a hover tool
    by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the complete code, refer to the following: [https://bit.ly/2RhpU1r](https://bit.ly/2RhpU1r).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.17: A random scatter plot with labels'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_03_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.17: A random scatter plot with labels'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We define a data source for the plot by passing a dictionary of key/value pairs
    to the `ColumnDataSource` constructor. This source includes the *x* location,
    *y* location, and size of each point, along with the random letter `A`, `B`, or
    `C` for each point. These random letters are assigned as labels for the hover
    tool, which will also display the size of each point. The `bokeh.plotting.figure`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add pan, zoom, and reset tools to the plot by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code is identical to what was previously shown except for the `tools` variable,
    which now references several new tools we've imported from the Bokeh library.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We'll stop the introductory exercise here, but we'll continue creating and exploring
    plots in the following activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4: Exploring Data with Interactive Visualizations'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You should have completed the previous exercise in order to complete this activity.
  prefs: []
  type: TYPE_NORMAL
- en: We'll pick up using Bokeh right where we left off with the previous exercise,
    except instead of using the randomly generated data seen there, we'll instead
    use the data we scraped from the web in the first part of this chapter. Our aim
    is to use Bokeh to create interactive visualizations of our scraped data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do so, we need to execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `lesson-3-workbook.ipynb` file, scroll to the `Activity B: Interactive
    visualizations with Bokeh` section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the previously scraped, merged, and cleaned web page data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recall what the data looks like by displaying the DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a scatter plot of the population as a function of the interest rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the data, we see some clear outliers with high populations. Hover over these
    to see what they are. Select the Box Zoom tool and alter the viewing window to
    better see the majority of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some of the lower population countries appear to have negative interest rates.
    Select the **Wheel Zoom** tool and use it to zoom in on this region. Use the **Pan**
    tool to re-center the plot, if needed, so that the negative interest rate samples
    are in view. Hover over some of these and see what countries they correspond to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a **Year of last change** column to the DataFrame and add a color based
    on the date of last interest rate change
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a map to group the last change date into color categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the colored visualization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking for patterns, zoom in on the lower population countries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the interest rate as a function of the year-over-year population change
    by running the following code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the line of best fit for the previously plotted relationship.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-plot the output obtained in the preceding step and add a line of best fit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the plot by using the zoom tools and hovering over interesting samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The detailed steps along with the solutions are presented in the *Appendix
    A*     (pg. no. 163).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we scraped web page tables and then used interactive visualizations
    to study the data.
  prefs: []
  type: TYPE_NORMAL
- en: We started by looking at how HTTP requests work, focusing on GET requests and
    their response status codes. Then, we went into the Jupyter Notebook and made
    HTTP requests with Python using the Requests library. We saw how Jupyter can be
    used to render HTML in the notebook, along with actual web pages that can be interacted
    with. After making requests, we saw how Beautiful Soup can be used to parse text
    from the HTML, and used this library to scrape tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: After scraping two tables of data, we stored them in pandas DataFrames. The
    first table contained the central bank interest rates for each country and the
    second table contained the populations. We combined these into a single table
    that was then used to create interactive visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we used Bokeh to render interactive visualizations in Jupyter. We saw
    how to use the Bokeh API to create various customized plots and made scatter plots
    with specific interactive abilities such as zoom, pan, and hover. In terms of
    customization, we explicitly showed how to set the point radius and color for
    each data sample.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, when using Bokeh to explore the scraped population data, the tooltip
    was utilized to show country names and associated data when hovering over the
    points.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations for completing this introductory course on data science using
    Jupyter Notebooks! Regardless of your experience with Jupyter and Python coming
    into the book, you've learned some useful and applicable skills for practical
    data science!
  prefs: []
  type: TYPE_NORMAL
- en: Before finishing up, let's quickly recap the topics we've covered in this book.
  prefs: []
  type: TYPE_NORMAL
- en: The first chapter was an introduction to the Jupyter Notebook platform, where
    we covered all of the fundamentals. We learned about the interface and how to
    use and install magic functions. Then, we introduced the Python libraries we'll
    be using and walked through an exploratory analysis of the *Boston housing* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the second chapter, we focused on doing machine learning with Jupyter. We
    first discussed the steps for developing a predictive analytics plan, and then
    looked at a few different types of models including SVM, a KNN classifier, and
    Random Forests.
  prefs: []
  type: TYPE_NORMAL
- en: Working with an *employee retention* dataset, we applied data cleaning methods
    and then trained models to predict whether an employee has left or not. We also
    explored more advanced topics such as overfitting, k-fold cross-validation, and
    validation curves.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the third chapter, we shifted briefly from data analysis to data
    collection using web scraping and saw how to make HTTP requests and parse the
    HTML responses in Jupyter. Then, we finished up the book by using interactive
    visualizations to explore our collected data.
  prefs: []
  type: TYPE_NORMAL
