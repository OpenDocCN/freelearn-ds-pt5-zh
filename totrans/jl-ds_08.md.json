["```py\nPkg.update() Pkg.add(\"RandomForests\") \n\n```", "```py\nRandomForestClassifier(;n_estimators::Int=10, \n                        max_features::Union(Integer, FloatingPoint, Symbol)=:sqrt, \n                        max_depth=nothing, \n                        min_samples_split::Int=2, \n                        criterion::Symbol=:gini) \n\n```", "```py\nRandomForestRegressor(;n_estimators::Int=10, \n                       max_features::Union(Integer, FloatingPoint, Symbol)=:third, \n                       max_depth=nothing, \n                       min_samples_split::Int=2) \n\n```", "```py\nPkg.add(\"DecisionTree\") \n\n```", "```py\nusing RDatasets: dataset \nusing DecisionTree \n\n```", "```py\niris = dataset(\"datasets\", \"iris\") \nfeatures = convert(Array, iris[:, 1:4]); \nlabels = convert(Array, iris[:, 5]); \n\n```", "```py\n# train full-tree classifier \nmodel = build_tree(labels, features) \n# prune tree: merge leaves having >= 90% combined purity (default: 100%) \nmodel = prune_tree(model, 0.9) \n# pretty print of the tree, to a depth of 5 nodes (optional) \nprint_tree(model, 5) \n\n```", "```py\n# apply learned model \napply_tree(model, [5.9,3.0,5.1,1.9]) \n# get the probability of each label \napply_tree_proba(model, [5.9,3.0,5.1,1.9], [\"setosa\", \"versicolor\", \"virginica\"]) \n# run n-fold cross validation for pruned tree, \n# using 90% purity threshold pruning, and 3 CV folds \naccuracy = nfoldCV_tree(labels, features, 0.9, 3) \n\n```", "```py\nFold 1 \nClasses:   \n3x3 Array{Int64,2}: \n 15   0   0 \n  1  13   0 \n  0   1  20 \nAny[\"setosa\",\"versicolor\",\"virginica\"] \nMatrix:    \nAccuracy:  \n3x3 Array{Int64,2}: \n 18   0  0 \n  0  18  5 \n  0   1  8 \n3x3 Array{Int64,2}: \n 17   0   0 \n  0  11   2 \n  0   3  17 \n0.96 \nKappa:    0.9391727493917275 \n\nFold 2 \nClasses:  Any[\"setosa\",\"versicolor\",\"virginica\"] \nMatrix:    \nAccuracy: 0.88 \nKappa:    0.8150431565967939 \n\nFold 3 \nClasses:  Any[\"setosa\",\"versicolor\",\"virginica\"] \nMatrix:    \nAccuracy: 0.9 \nKappa:    0.8483929654335963 \n\nMean Accuracy: 0.9133333333333332 \n\n```", "```py\n# train random forest classifier \n# using 2 random features, 10 trees, 0.5 portion of samples per tree (optional), and a maximum tree depth of 6 (optional) \nmodel = build_forest(labels, features, 2, 10, 0.5, 6) \n\n```", "```py\n3x3 Array{Int64,2}: \n 14   0   0 \n  2  15   0 \n  0   5  14 \n3x3 Array{Int64,2}: \n 19   0   0 \n  0  15   3 \n  0   0  13 \n3x3 Array{Int64,2}: \n 17   0   0 \n  0  14   1 \n  0   0  18 \n\n```", "```py\n# apply learned model \napply_forest(model, [5.9,3.0,5.1,1.9]) \n# get the probability of each label \napply_forest_proba(model, [5.9,3.0,5.1,1.9], [\"setosa\", \"versicolor\", \"virginica\"]) \n# run n-fold cross validation for forests \n# using 2 random features, 10 trees, 3 folds and 0.5 of samples per tree (optional) \naccuracy = nfoldCV_forest(labels, features, 2, 10, 3, 0.5) \n\n```", "```py\nFold 1 \nClasses:  Any[\"setosa\",\"versicolor\",\"virginica\"] \nMatrix:    \nAccuracy: 0.86 \nKappa:    0.7904191616766468 \n\nFold 2 \nClasses:  Any[\"setosa\",\"versicolor\",\"virginica\"] \nMatrix:    \nAccuracy: 0.94 \nKappa:    0.9096929560505719 \n\nFold 3 \nClasses:  Any[\"setosa\",\"versicolor\",\"virginica\"] \nMatrix:    \nAccuracy: 0.98 \nKappa:    0.9698613622664255 \n\nMean Accuracy: 0.9266666666666666 \n\n3-element Array{Float64,1}: \n 0.86 \n 0.94 \n 0.98 \n\n```", "```py\nn, m = 10^3, 5 ; \nfeatures = randn(n, m); \nweights = rand(-2:2, m); \nlabels = features * weights; \n# train regression tree, using an averaging of 5 samples per leaf (optional) \nmodel = build_tree(labels, features, 5) \napply_tree(model, [-0.9,3.0,5.1,1.9,0.0]) \n# run n-fold cross validation, using 3 folds, averaging of 5 samples per leaf (optional) \n# returns array of coefficients of determination (R^2) \nr2 = nfoldCV_tree(labels, features, 3, 5) \n\n```", "```py\nFold 1 \nMean Squared Error:     3.300846200596437 \nCorrelation Coeff:      0.8888432175516764 \nCoeff of Determination: 0.7880527098784421 \n\nFold 2 \nMean Squared Error:     3.453954624611847 \nCorrelation Coeff:      0.8829598153801952 \nCoeff of Determination: 0.7713110081750566 \n\nFold 3 \nMean Squared Error:     3.694792045651598 \nCorrelation Coeff:      0.8613929927227013 \nCoeff of Determination: 0.726445409019041 \n\nMean Coeff of Determination: 0.7619363756908465 \n\n3-element Array{Float64,1}: \n 0.788053 \n 0.771311 \n 0.726445 \n\n```", "```py\n# train regression forest, using 2 random features, 10 trees, \n# averaging of 5 samples per leaf (optional), 0.7 of samples per tree (optional) \nmodel = build_forest(labels,features, 2, 10, 5, 0.7) \n# apply learned model \napply_forest(model, [-0.9,3.0,5.1,1.9,0.0]) \n# run n-fold cross validation on regression forest \n# using 2 random features, 10 trees, 3 folds, averaging of 5 samples/leaf (optional), \n# and 0.7 porition of samples per tree (optional) \n# returns array of coefficients of determination (R^2) \nr2 = nfoldCV_forest(labels, features, 2, 10, 3, 5, 0.7) \n\n```", "```py\nFold 1 \nMean Squared Error:     1.9810655619597397 \nCorrelation Coeff:      0.9401674806129654 \nCoeff of Determination: 0.8615574830022655 \n\nFold 2 \nMean Squared Error:     1.9359831066335886 \nCorrelation Coeff:      0.950439305213504 \nCoeff of Determination: 0.8712750380735376 \n\nFold 3 \nMean Squared Error:     2.120355686915558 \nCorrelation Coeff:      0.9419270107183548 \nCoeff of Determination: 0.8594402239360724 \n\nMean Coeff of Determination: 0.8640909150039585 \n\n3-element Array{Float64,1}: \n 0.861557 \n 0.871275 \n 0.85944  \n\n```"]