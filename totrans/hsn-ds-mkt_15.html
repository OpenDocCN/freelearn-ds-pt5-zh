<html><head></head><body>
        

                            
                    <h1 class="header-title">Retaining Customers</h1>
                
            
            
                
<p>As customers have more options for similar content to consume or similar products and services to shop for, it has become more difficult for many businesses to retain their customers and not lose them to other competitors. As the cost of acquiring new customers is typically higher than that of retaining and keeping existing customers, customer churn is becoming more and more of a concern than ever before. In order to retain existing customers and not lose them to competitors, businesses should not only try to understand their customers and their customers' needs and interests, but they should also be able to identify which customers are highly likely to churn and how to retain these customers at churn risk.</p>
<p>In this chapter, we are going to dive deeper into customer churn and how it hurts businesses, as well as how to retain existing customers. We will discuss some of the common reasons for customers leaving businesses and look at how data science can help reduce the risk of losing customers. As a way of predicting customer churn, we will learn about what an artificial neural network model is and its applications in different areas, as well as how we can build one using Python and R.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Customer churn and retention</li>
<li>Artificial neural networks</li>
<li>Predicting customer churn with Python</li>
<li>Predicting customer churn with R</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Customer churn and retention</h1>
                
            
            
                
<p><strong>Customer churn</strong> is when a customer decides to stop using services, content, or products from a company. As we have briefly discussed in <a href="72e8f4ee-7f95-4acc-928d-d33c9fc31bd6.xhtml" target="_blank">Chapter 7</a>, <em>Exploratory Analysis for Customer Behavior</em>, when we discussed customer analytics, it is much less expensive to retain existing customers than to acquire new customers, and the revenue from repeat customers is typically higher than that form new customers. In competitive industries, where a business faces many competitors, the cost of new customer acquisition is even higher, and retaining existing customers becomes more important for such businesses.</p>
<p>There are many reasons behind customers leaving a business. Some of the common reasons why customers churn are poor customer service, not finding enough value in the products or services, lack of communications, and lack of customer loyalty. The first step to retaining these customers is to monitor customer churn rates over time. If the churn rate is generally high or is increasing over time, then it will be a good idea to dedicate some resources to improving customer retention.</p>
<p>In order to improve the customer retention rate, the top priority should be to understand the customer better. You can survey customers who have already churned to understand why they left. You can also survey existing customers to understand what their needs are and what their pain points are. A data science and data analytics approach would be to look into the data. For example, you can look at customers' web activity data and understand where they spend the most time, whether there were errors on the pages that they were looking at, or whether their search results did not return good content. You can also look into the customer service call logs to understand how long their wait time was, what their complaints were, and how their issues were handled. Conducting deep analyses on these data points can reveal the problems that a business is facing in retaining its existing customers.</p>
<p>When analyzing for customer churn, you can also utilize some of the topics we have discussed in this book. You can apply what we have learned from <a href="73a716c6-6a84-4785-b04e-87651d0a29d1.xhtml" target="_blank">Chapter 5</a>, <em>Product Analytics</em>, and <a href="d3ba7047-2873-4b03-9a44-4c1d55b84178.xhtml" target="_blank">Chapter 6</a>, <em>Recommending the Right Products</em>, to understand which products serve the customer needs and interests the best, and recommend the right products so that you can deliver more personalized content. You can also use what we have learned from <a href="72e8f4ee-7f95-4acc-928d-d33c9fc31bd6.xhtml" target="_blank">Chapter 7</a>, <em>Exploratory Analysis for Customer Behavior</em>, and <a href="5955002d-2a75-4d5a-aa6a-86710a3bf00e.xhtml" target="_blank">Chapter 10</a>,<strong> </strong><em>Data-Driven Customer Segmentation</em>, to understand the customer behavior better and the different segments of customers. Another way is to build a machine learning model that can predict which customers are likely to churn and target and retain these specific customers that are at higher risk of churn. In the following sections, we will discuss how to build a neural network model to identify those customers with higher risk of churn for customer retention.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Artificial neural networks</h1>
                
            
            
                
<p>The<strong> artificial neural network</strong> (<strong>ANN</strong>) model is a machine learning model that is inspired by how a human brain functions. Recent successful applications of ANN models in image recognition, voice recognition, and robotics have proven their predictive power and usefulness in various industries. You might have heard the term <strong>deep learning</strong>. This is a type of ANN model where the number of layers between the input and output layers is large. It is best explained with the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/94952b7d-1e29-44d5-9886-ebc15f2b8781.png" style="width:19.67em;height:22.25em;"/></p>
<p class="CDPAlignLeft CDPAlign">This diagram shows a simple case of an ANN model with one hidden layer. The circles in this diagram represent artificial neurons or nodes, which model those neurons in human brains. The arrows represent how signals are transmitted from one neuron to another. As this diagram suggests, an ANN model learns by finding the patterns or the weights of signals from each input neuron to the neuron in the next layer, which best predicts the output. </p>
<p>The specific type of an ANN model that we will be experimenting with in the following programming exercises is a <strong>multilayer perceptron</strong> (<strong>MLP</strong>) model. Simply put, an MLP model is a neural network model that has at least one or more hidden layers of nodes. Including one layer for the input and another layer for the output, the MLP model consists of at least three or more layers of nodes. The diagram we just looked at is the simplest case of an MLP model, where there is only one hidden layer.</p>
<p>ANN models can be utilized in many areas of marketing. Using neural network models by BrainMaker, Microsoft increased its direct mail response rate from 4.9% to 8.2%. This helped Microsoft to bring in the same amount of revenue for 35% less cost. Similarly, for the marketing engagement prediction problems we discussed in <a href="4f5163a1-c34a-495f-bc5f-e02f9b2a2052.xhtml" target="_blank">Chapter 8</a>, <em>Predicting the Likelihood of Marketing Engagement</em>, we could have used neural network models, instead of random forest models. We can also use neural network models for the customer segmentation problems that we discussed in <a href="5955002d-2a75-4d5a-aa6a-86710a3bf00e.xhtml" target="_blank">Chapter 10</a>, <em>Data-Driven Customer Segmentation</em>. In the following programming exercises, we will discuss how we can use ANN models to predict which customers are likely to churn.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Predicting customer churn with Python</h1>
                
            
            
                
<p>In this section, we are going to discuss how to use an ANN model to predict the customers at the risk of leaving, or customers who are highly likely to churn. By the end of this section, we will have built a customer churn prediction model using an ANN model. We will be mainly using the <kbd>pandas</kbd>, <kbd>matplotlib</kbd>, and <kbd>keras</kbd> packages to analyze, visualize, and build machine learning models. For those readers who would like to use R, instead of Python, for this exercise, you can skip to the next section.</p>
<p>For this exercise, we will be using one of the publicly available datasets from the IBM Watson Analytics community, which can be found at this link: <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/">https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/</a>. You can follow this link and download the data, which is available in XLSX format, named <kbd>WA_Fn-UseC_-Telco-Customer-Churn.xlsx</kbd>. Once you have downloaded this data, you can load it into your Jupyter Notebook by running the following command:</p>
<pre>import pandas as pd<br/><br/>df = pd.read_excel('../data/WA_Fn-UseC_-Telco-Customer-Churn.xlsx')</pre>
<p>The DataFrame, <kbd>df</kbd>, is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/82ebeb7f-fa1b-4fb8-9cb8-3c1e3f1f5048.png"/></p>
<p>There are 21 variables in this dataset, where our goal is to predict the target variable, <kbd>Churn</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data analysis and preparation</h1>
                
            
            
                
<p>As you may notice by looking at the data, there are a few things we need to do before we can start building machine learning models. In this section, we are going to transform continuous variables that have monetary values and encode the target variable, <kbd>Churn</kbd>, as well as other categorical variables. To do so, perform the following steps:</p>
<ol>
<li><strong>Target variable encoding</strong>: As you may have noticed from the data, the target variable, <kbd>Churn</kbd>, has two values: <kbd>Yes</kbd> and <kbd>No</kbd>. We are going to encode these values as <kbd>1</kbd> for <kbd>Yes</kbd> and <kbd>0</kbd> for <kbd>No</kbd>. The code to encode the target variable looks like the following:</li>
</ol>
<pre>        df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)</pre>
<p style="padding-left: 60px">To get the overall churn rate, you can simply run the following code:</p>
<pre>        df['Churn'].mean()</pre>
<p style="padding-left: 60px">The output of this code is around 0.27, which suggests that about 27% of customers have churned. A 27% churn rate is not a small number; rather, it is high enough for a business to worry about the overall customer churn and come up with a solution to retain these customers. In the following modeling section, we will discuss how to predict customers who are likely to churn with this data and use these predictions to retain customers.</p>
<ol start="2">
<li><strong>Handling missing values in the TotalCharges column</strong>: If you looked through the <kbd>TotalCharges</kbd> column in the dataset, you may have noticed that there are some records with no <kbd>TotalCharges</kbd> values. Since there are only <kbd>11</kbd> records with missing <kbd>TotalCharges</kbd> values, we are going to simply ignore and drop those records with missing values. Take a look at the following code:</li>
</ol>
<pre>        df['TotalCharges'] = df['TotalCharges'].replace(' ',   <br/>                             np.nan).astype(float)<br/><br/>        df = df.dropna()</pre>
<p style="padding-left: 60px">As you may notice from this code, we are simply replacing the blank space values with <kbd>nan</kbd> values. Then, we are dropping all the records with <kbd>nan</kbd> values by using the <kbd>dropna</kbd> function.</p>
<ol start="3">
<li><strong>Transforming continuous variables</strong>: The next step is to scale the continuous variables. Take a look at the following summary statistics for continuous variables:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/9b23f08b-ff28-4ecd-93d4-1605191cfbe0.png" style="width:28.08em;height:20.25em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">You can get these summary statistics using the following code:</p>
<pre>        df[['tenure', 'MonthlyCharges', 'TotalCharges']].describe()</pre>
<p style="padding-left: 60px">As you can see from the summary statistics, the three <kbd>tenure</kbd>, <kbd>MonthlyCharges</kbd>, and <kbd>TotalCharges</kbd> continuous variables all have different scales. The <kbd>tenure</kbd> variable, ranges from <kbd>1</kbd> to <kbd>72</kbd>, while the <kbd>TotalCharges</kbd> variable , ranges from <kbd>18.8</kbd> to <kbd>8684.8</kbd>. ANN models typically perform better with scaled or normalized features. Take a look at the following code for normalizing these three features:</p>
<pre>        df['MonthlyCharges'] = np.log(df['MonthlyCharges'])<br/>        df['MonthlyCharges'] = (df['MonthlyCharges'] -    <br/>        df['MonthlyCharges'].mean())/df['MonthlyCharges'].std()<br/><br/>        df['TotalCharges'] = np.log(df['TotalCharges'])<br/>        df['TotalCharges'] = (df['TotalCharges'] -       <br/>        df['TotalCharges'].mean())/df['TotalCharges'].std()<br/><br/>        df['tenure'] = (df['tenure'] - df['tenure'].mean())/df['tenure'].std()</pre>
<p style="padding-left: 60px">As you can see from this code, we apply log-transform first and then normalize the continuous variables by subtracting by the mean and dividing the values by standard deviations. The results look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c3feef9a-26f3-4d7f-b6af-3c1fd1bbd9ce.png" style="width:31.83em;height:18.33em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">As you see from this output, all the variables now have a mean of <kbd>0</kbd> and a standard deviation of <kbd>1</kbd>. We are going to use these normalized variables for future model building.</p>
<ol start="4">
<li><strong>One-hot encoding categorical variables</strong>: As you can see from the data, there are many categorical variables. Let's first take a look at the number of unique values each column has. Take a look at the following code:</li>
</ol>
<pre>        for col in list(df.columns):<br/>            print(col, df[col].nunique())</pre>
<p style="padding-left: 60px">You can use the <kbd>nunique</kbd> function to count the number of unique values in each column. The output of this code looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/8607d3f9-078b-46c8-8795-36677f850605.png" style="width:11.67em;height:23.58em;"/></p>
<p class="CDPAlignLeft CDPAlign">As this output suggests, there are <kbd>7032</kbd> unique customer IDs, <kbd>2</kbd> unique genders, <kbd>3</kbd> unique values for <kbd>MultipleLines</kbd>, and <kbd>6530</kbd> unique values for <kbd>TotalCharges</kbd>. We have handled the <kbd>tenure</kbd>, <kbd>MonthlyCharges</kbd>, and <kbd>TotalCharges</kbd> variables, in the previous step, so we are going to focus on those variables with <kbd>2</kbd> to <kbd>4</kbd> unique values.</p>
<p class="CDPAlignLeft CDPAlign">Let's take a look at the distributions of some of these categorical variables. First, to view the distribution of the data between males and females, you can use the following code for visualization:</p>
<pre>df.groupby('gender').count()['customerID'].plot(<br/>    kind='bar', color='skyblue', grid=True, figsize=(8,6), title='Gender'<br/>)<br/>plt.show()</pre>
<p>The plot looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1336 image-border" src="img/d45a82ef-8941-4a1e-b045-603f6a250358.png" style="width:35.50em;height:29.67em;"/></p>
<p class="CDPAlignLeft CDPAlign">As you can see from this bar plot, the distribution of the data across different genders is roughly equally distributed. You can use the same code to view the distribution of the data across different values of <kbd>InternetService</kbd> and <kbd>PaymentMethod</kbd>. Take a look at the following plots:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c9f241ba-b160-4d32-9ed5-6d5a3f4789cb.png" style="width:25.00em;height:21.50em;"/><br/>
<img src="img/02f7e148-eae1-433d-839d-846866528189.png" style="width:25.42em;height:25.92em;"/></p>
<p class="CDPAlignLeft CDPAlign">The first plot shows the distribution of the data across three different categories of the <kbd>InternetService</kbd> variable, and the second plot shows the distribution of the data across four different categories of the <kbd>PaymentMethod</kbd> variable. As you can see from these plots, we can easily visualize and understand what the distributions of categorical variables look like using bar plots. We recommend that you draw bar plots for other categorical variables to get a better understanding of the data distribution.</p>
<p class="CDPAlignLeft CDPAlign">Now, we are going to apply one-hot encoding for these categorical variables. Take a look at the following code:</p>
<pre>dummy_cols = []<br/><br/>sample_set = df[['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']].copy(deep=True)<br/><br/>for col in list(df.columns):<br/>    if col not in ['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn'] and df[col].nunique() &lt; 5:<br/>        dummy_vars = pd.get_dummies(df[col])<br/>        dummy_vars.columns = [col+str(x) for x in dummy_vars.columns] <br/>        sample_set = pd.concat([sample_set, dummy_vars], axis=1)</pre>
<p>As you can see from this code, we are using the <kbd>get_dummies</kbd> function in the <kbd>pandas</kbd> package to create dummy variables for each categorical variable. Then, we concatenate these newly created dummy variables back to the <kbd>sample_set</kbd> variable, which will be used for training models in the following section. The results are shown in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/e0661e37-7736-4081-a012-d09060d67881.png"/></p>
<p>Once you have completed these four steps, it is time to start building ANN models for customer churn predictions. Move onto the next section for ANN modeling!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">ANN with Keras</h1>
                
            
            
                
<p>For building ANN models in Python, we are going to use <kbd>keras</kbd> package, which is a high-level neural networks library. For more details, we recommend you visit their official documentation at the following link: <a href="https://keras.io/">https://keras.io/</a>. Before we can use this package for building ANN models, we need to install two packages: <kbd>tensorflow</kbd> and <kbd>keras</kbd>. The <kbd>keras</kbd> package uses <kbd>tensorflow</kbd> as a backend for building neural network models, so we need to install <kbd>tensorflow</kbd> first. You can install these two packages using the following <kbd>pip</kbd> commands in your Terminal:</p>
<pre>pip install tensorflow<br/>pip install keras</pre>
<p>Once you have installed these two packages, we can finally start building our first neural network models. In this exercise, we are going to build a neural network model with one hidden layer. Take a look at the following code first:</p>
<pre>from keras.models import Sequential<br/>from keras.layers import Dense<br/><br/>model = Sequential()<br/>model.add(Dense(16, input_dim=len(features), activation='relu'))<br/>model.add(Dense(8, activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))</pre>
<p>Let's take a closer look at this code. First, we are using a <kbd>Sequential</kbd> model here, which is the type of model where the layers are stacked linearly and looks similar to the diagram we saw in the earlier section about the MLP model. The first layer is an input layer, where <kbd>input_dim</kbd> is simply the number of features or columns in the sample set and the number of output units is <kbd>16</kbd>. We are using the <kbd>relu</kbd> activation function for this input layer. Then, in the hidden layer, the number of output units is <kbd>8</kbd> and the activation function to be used is <kbd>relu</kbd>. Lastly, the output layer has one output unit, which is the probability of customer churn, and we use the <kbd>sigmoid</kbd> activation function in this layer. You can experiment with different numbers of output units and activation functions for your exercise.</p>
<p>The final step to build a neural network model with the <kbd>keras</kbd> package is to compile this model. Take a look at the following code:</p>
<pre>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</pre>
<p>Here, we are using the <kbd>adam</kbd> optimizer, which is one of the most commonly and frequently used optimization algorithms. Since our target variable is binary, we are using <kbd>binary_crossentropy</kbd> as the loss function. Lastly, this model will use the <kbd>accuracy</kbd> metric to evaluate model performance during training.</p>
<p>Before we start training this neural network model, we will need to split our sample set into train and test sets. Take a look at the following code:</p>
<pre>from sklearn.model_selection import train_test_split<br/><br/>target_var = 'Churn'<br/>features = [x for x in list(sample_set.columns) if x != target_var]<br/><br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    sample_set[features], <br/>    sample_set[target_var], <br/>    test_size=0.3<br/>)</pre>
<p>As you can see from this code, we are using the <kbd>train_test_split</kbd> function of the <kbd>scikit-learn</kbd> package. For our exercise, we will use 70% of the sample set for training and 30% for testing. Now we can train our neural network model using the following code:</p>
<pre>model.fit(X_train, y_train, epochs=50, batch_size=100)</pre>
<p>Here, we are using <kbd>100</kbd> samples as <kbd>batch_size</kbd>, from which the model is going to learn to predict each time, and <kbd>50</kbd> as the number of <kbd>epochs</kbd>, which is the number of complete passes through the entire training set. Once you run this code, you will see output that looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c3546491-4c7b-410c-80d6-47b584305a45.png" style="width:35.42em;height:25.00em;"/></p>
<p>As you can see from this output, <kbd>loss</kbd> typically decreases and the accuracy (<kbd>acc</kbd>) improves in each epoch. However, the rate of model performance improvement decreases over time. As you can see from this output, there are big improvements in the loss and accuracy measures in the first few epochs and the amount of performance gain decreases over time. You can monitor this process and decide to stop when the amount of performance gain is minimal.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Model evaluations</h1>
                
            
            
                
<p>Now that we have built our first neural network model, let's evaluate its performance. We are going to look at the overall accuracy, precision, and recall, as well as the <strong>receiver operating characteristic</strong> (<strong>ROC</strong>) curve and area under the curve (AUC). First, take a look at the following code for computing accuracy, precision, and recall:</p>
<pre>from sklearn.metrics import accuracy_score, precision_score, recall_score<br/><br/>in_sample_preds = [round(x[0]) for x in model.predict(X_train)]<br/>out_sample_preds = [round(x[0]) for x in model.predict(X_test)]<br/><br/># Accuracy<br/>print('In-Sample Accuracy: %0.4f' % accuracy_score(y_train, in_sample_preds))<br/>print('Out-of-Sample Accuracy: %0.4f' % accuracy_score(y_test, out_sample_preds))<br/><br/># Precision<br/>print('In-Sample Precision: %0.4f' % precision_score(y_train, in_sample_preds))<br/>print('Out-of-Sample Precision: %0.4f' % precision_score(y_test, out_sample_preds))<br/><br/># Recall<br/>print('In-Sample Recall: %0.4f' % recall_score(y_train, in_sample_preds))<br/>print('Out-of-Sample Recall: %0.4f' % recall_score(y_test, out_sample_preds))</pre>
<p>You should be familiar with this code, as we used the same evaluation metrics in <a href="4f5163a1-c34a-495f-bc5f-e02f9b2a2052.xhtml" target="_blank">Chapter 8</a>, <em>Predicting the Likelihood of Marketing Engagement</em>. The output of this code in our case looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/7494f39e-a14f-4eda-8c43-7f86d4fca089.png" style="width:14.75em;height:9.83em;"/></p>
<p>Due to some randomness in the model, your results might differ from these numbers. As you can see from this output, the accuracy of predicting whether a customer will churn or not in the test set is about <kbd>0.79</kbd>, suggesting the model is correct roughly about 80% of the time. The out-of-sample precision suggests that the model is correct about 66% of the time that it predicts that the customer is going to churn, and the out-of-sample recall suggests that the model captures roughly 52% of the churn cases. </p>
<p>Next, we can compute the AUC numbers, using the following code:</p>
<pre>from sklearn.metrics import roc_curve, auc<br/><br/>in_sample_preds = [x[0] for x in model.predict(X_train)]<br/>out_sample_preds = [x[0] for x in model.predict(X_test)]<br/><br/>in_sample_fpr, in_sample_tpr, in_sample_thresholds = roc_curve(y_train, in_sample_preds)<br/>out_sample_fpr, out_sample_tpr, out_sample_thresholds = roc_curve(y_test, out_sample_preds)<br/><br/>in_sample_roc_auc = auc(in_sample_fpr, in_sample_tpr)<br/>out_sample_roc_auc = auc(out_sample_fpr, out_sample_tpr)<br/><br/>print('In-Sample AUC: %0.4f' % in_sample_roc_auc)<br/>print('Out-Sample AUC: %0.4f' % out_sample_roc_auc)</pre>
<p>The output of this code looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0ec3005a-e8e1-48e0-a8ec-293c4ffff915.png" style="width:11.17em;height:2.25em;"/></p>
<p>To visualize this data in the ROC curve, you can use the following code:</p>
<pre>plt.figure(figsize=(10,7))<br/><br/>plt.plot(<br/>    out_sample_fpr, out_sample_tpr, color='darkorange', label='Out-Sample ROC curve (area = %0.4f)' % in_sample_roc_auc<br/>)<br/>plt.plot(<br/>    in_sample_fpr, in_sample_tpr, color='navy', label='In-Sample ROC curve (area = %0.4f)' % out_sample_roc_auc<br/>)<br/>plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')<br/>plt.grid()<br/>plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.05])<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.title('ROC Curve')<br/>plt.legend(loc="lower right")<br/><br/>plt.show()</pre>
<p>And the output looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/99716b1c-5a70-4197-b28d-3a7297340dac.png" style="width:44.33em;height:32.00em;"/></p>
<p>Along with the accuracy, precision, and recall measures that we looked at previously, the AUC and the ROC curve also suggest that the model captures and predicts those customers at churn risk pretty well. As you can see from these evaluation outputs, it is better to use the output of this model for identifying the customers who are likely to churn than simply guessing who they will be. By focusing on those customers with high churn probabilities from this model in your marketing strategies, you can try to retain those customers at churn risks in a more cost-effective way.</p>
<p>The full code for this exercise can be found in this repository: <a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.11/python/CustomerRetention.ipynb">https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.11/python/CustomerRetention.ipynb</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Predicting customer churn with R</h1>
                
            
            
                
<p>In this section, we are going to discuss how to use an ANN model to predict the customers at risk of leaving or customers who are highly likely to churn. By the end of this section, we will have built a customer churn prediction model using the ANN model. We will be mainly using the <kbd>dplyr</kbd>, <kbd>ggplot2</kbd>, and <kbd>keras</kbd> libraries to analyze, visualize, and build machine learning models. For those readers who would like to use Python, instead of R, for this exercise, see the previous section.</p>
<p>For this exercise, we will be using one of the publicly available datasets from the IBM Watson Analytics community, which can be found at this link: <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/">https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/</a>. You can follow this link and download the data, which is available in XLSX format, named <kbd>WA_Fn-UseC_-Telco-Customer-Churn.xlsx</kbd>. Once you have downloaded this data, you can load it into your RStudio environment by running the following command:</p>
<pre>library(readxl)<br/><br/>#### 1. Load Data ####<br/>df &lt;- read_excel(<br/>  path="~/Documents/data-science-for-marketing/ch.11/data/WA_Fn-UseC_-Telco-Customer-Churn.xlsx"<br/>)</pre>
<p>The DataFrame, <kbd>df</kbd>, should look as in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d24ceede-a86b-41b1-bbd9-7666d58b0ced.png"/></p>
<p>There are 21 variables in this dataset, where our goal is to predict the target variable, <kbd>Churn</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data analysis and preparation</h1>
                
            
            
                
<p>As you may have noticed by looking at the data, there are a few things we need to do before we start building machine learning models. In this section, we are going to transform continuous variables that have monetary values and encode the target variable, <kbd>Churn</kbd>, as well as other categorical variables. To do so, perform the following steps:</p>
<ol>
<li class="CDPAlignLeft CDPAlign"><strong>Handling missing values in the data</strong>: If you looked through the <kbd>TotalCharges</kbd> column in the dataset, you may have noticed that there are some records with no <kbd>TotalCharges</kbd> values. Since there are only <kbd>11</kbd> records with missing <kbd>TotalCharges</kbd> values, we are going to simply ignore and drop those records with missing values. Take a look at the following code:</li>
</ol>
<pre>        library(tidyr)<br/><br/>        df &lt;- df %&gt;% drop_na()</pre>
<p style="padding-left: 60px">As you may notice from this code, we are using the <kbd>drop_na</kbd> function in the <kbd>tidyr</kbd> package, which drops all records with <kbd>NA</kbd> values.</p>
<ol start="2">
<li><strong>Categorical variables</strong>: As you can see from the data, there are many categorical variables. Let's first take a look at the number of unique values each column has. Take a look at the following code:</li>
</ol>
<pre>       apply(df, 2, function(x) length(unique(x)))</pre>
<p style="padding-left: 60px">You can use the <kbd>unique</kbd> function to get the unique values in each column. By applying this function across all the columns in <kbd>df</kbd>, the output of this code looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/1f273b2e-a384-444f-8f1f-7db9920dd7b9.png" style="width:41.58em;height:12.67em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">As this output suggests, there are <kbd>7032</kbd> unique customer IDs, <kbd>2</kbd> unique genders, <kbd>3</kbd> unique values for <kbd>MultipleLines</kbd>, and <kbd>6530</kbd> unique values for <kbd>TotalCharges</kbd>. The <kbd>tenure</kbd>, <kbd>MonthlyCharges</kbd>, and <kbd>TotalCharges</kbd> variables, are continuous variables, where each variable can take any value and the rest are the categorical variables. </p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">We are going to take a look at the distributions of some of these categorical variables. First, to view the distribution of the data between male and female, you can use the following code for visualization:</p>
<pre>        ggplot(df %&gt;% group_by(gender) %&gt;% summarise(Count=n()),<br/>         aes(x=gender, y=Count)) +<br/>          geom_bar(width=0.5, stat="identity") +<br/>          ggtitle('') +<br/>          xlab("Gender") +<br/>          ylab("Count") +<br/>          theme(plot.title = element_text(hjust = 0.5))</pre>
<p style="padding-left: 60px">The plot looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/6260b535-4c47-4c74-80b2-af0ba8d0fafe.png" style="width:40.58em;height:27.17em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">As you can see from this bar plot, the distribution of the data across the two genders is roughly equally distributed. You can use the same code to view the distribution of the data across different values of <kbd>InternetService</kbd> and <kbd>PaymentMethod</kbd>. Take a look at the following plots:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/680a8c86-bc81-438c-aef3-ba25bb90aee9.png" style="width:32.25em;height:22.58em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img src="img/37b195d5-6b3d-4733-b792-c19443bf1187.png" style="width:35.17em;height:24.75em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">The first plot shows the distribution of the data across three different categories of the <kbd>InternetService</kbd> variable and the second plot shows the distribution of the data across four different categories of the <kbd>PaymentMethod</kbd> variable. As you can see from these plots, we can easily visualize and understand what the distributions of categorical variables look like using bar plots. We recommend that you draw bar plots for other categorical variables to get a better understanding of the data distribution.</p>
<ol start="3">
<li><strong>Transforming and encoding variables</strong>: The next step is to transform the continuous variables and encode the binary-class categorical variables. Take a look at the following code:</li>
</ol>
<pre>        # Binary &amp; Continuous Vars<br/>        sampleDF &lt;- df %&gt;%<br/>         select(tenure, MonthlyCharges, TotalCharges, gender, Partner,    <br/>        Dependents, PhoneService, PaperlessBilling, Churn) %&gt;%<br/>         mutate(<br/>          # transforming continuous vars<br/>         tenure=(tenure - mean(tenure))/sd(tenure),<br/>          MonthlyCharges=(log(MonthlyCharges) -      <br/>        mean(log(MonthlyCharges)))/sd(log(MonthlyCharges)),<br/>         TotalCharges=(log(TotalCharges) -     <br/>         mean(log(TotalCharges)))/sd(log(TotalCharges)),<br/><br/>          # encoding binary categorical vars<br/>         gender=gender %&gt;% as.factor() %&gt;% as.numeric() - 1,<br/>         Partner=Partner %&gt;% as.factor() %&gt;% as.numeric() - 1,<br/>        Dependents=Dependents %&gt;% as.factor() %&gt;% as.numeric() - 1,<br/>         PhoneService=PhoneService %&gt;% as.factor() %&gt;% as.numeric() - 1,<br/>         PaperlessBilling=PaperlessBilling %&gt;% as.factor() %&gt;% as.numeric() - 1,<br/>          Churn=Churn %&gt;% as.factor() %&gt;% as.numeric() - 1<br/>          )</pre>
<p style="padding-left: 60px">As you can see from this code, we are simply encoding those variables with only two categories, <kbd>gender</kbd>, <kbd>Partner</kbd>, <kbd>Dependents</kbd>, <kbd>PhoneService</kbd>, <kbd>PaperlessBilling</kbd>, and <kbd>Churn</kbd>, with <kbd>0</kbd>s and <kbd>1</kbd>s. Then, we apply log transformations to the two continuous variables that have monetary values, <kbd>MonthlyCharges</kbd> and <kbd>TotalCharges</kbd>. Also, we standardize all three continuous variables, <kbd>tenure</kbd>, <kbd>MonthlyCharges</kbd>, and <kbd>TotalCharges</kbd>, so that these variables center around <kbd>0</kbd> and have standard deviations of <kbd>1</kbd>. This is because ANN models typically perform better with scaled or normalized features. After transformations, the distributions of these three continuous variables look as in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b8f430c2-8ecb-4a6b-91cf-6ab156c6950c.png" style="width:27.92em;height:10.00em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">As you can see, the means of these three transformed variables are <kbd>0</kbd> and the standard deviations are <kbd>1</kbd>. Whereas, before this transformation, the distributions looked like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/5e6d440f-d902-48a1-a6d9-1c66d4ede730.png" style="width:27.67em;height:10.75em;"/></p>
<ol start="4">
<li><strong>One-hot encoding categorical variables</strong>: There is one last set of variables we need to transform: multi-class categorical variables that have three or more categories. We are going to apply one-hot encoding and create dummy variables for these variables. Take a look at the following code:</li>
</ol>
<pre>        # Dummy vars<br/>         # install.packages('dummies')<br/>          library(dummies)<br/><br/>        sampleDF &lt;- cbind(sampleDF, dummy(df$MultipleLines, sep="."))<br/>         names(sampleDF) = gsub("sampleDF", "MultipleLines", names(sampleDF))</pre>
<p style="padding-left: 60px">As you can see from this code, we are using the <kbd>dummies</kbd> library to create dummy variables. Using the <kbd>dummy</kbd> function of this package, we can apply one-hot encoding and create dummy variables for each multi-class categorical variable. Since the <kbd>dummy</kbd> function prepends <kbd>sampleDF</kbd> to the names of the newly created dummy variables, we can replace it with corresponding variable name by using the <kbd>gsub</kbd> function. We are going to apply the same logic to the rest of the categorical variables, as shown in the following code:</p>
<pre>        sampleDF &lt;- cbind(sampleDF, dummy(df$InternetService, sep="."))<br/>        names(sampleDF) = gsub("sampleDF", "InternetService", names(sampleDF))<br/><br/>        sampleDF &lt;- cbind(sampleDF, dummy(df$OnlineSecurity, sep="."))<br/>        names(sampleDF) = gsub("sampleDF", "OnlineSecurity", names(sampleDF))<br/><br/>        sampleDF &lt;- cbind(sampleDF, dummy(df$OnlineBackup, sep="."))<br/>         names(sampleDF) = gsub("sampleDF", "OnlineBackup", names(sampleDF))<br/><br/>        sampleDF &lt;- cbind(sampleDF, dummy(df$DeviceProtection, sep="."))<br/>        names(sampleDF) = gsub("sampleDF", "DeviceProtection", names(sampleDF))<br/><br/>         sampleDF &lt;- cbind(sampleDF, dummy(df$TechSupport, sep="."))<br/>         names(sampleDF) = gsub("sampleDF", "TechSupport", names(sampleDF))<br/><br/>         sampleDF &lt;- cbind(sampleDF, dummy(df$StreamingTV, sep="."))<br/>         names(sampleDF) = gsub("sampleDF", "StreamingTV", names(sampleDF))<br/><br/>         sampleDF &lt;- cbind(sampleDF, dummy(df$StreamingMovies, sep="."))<br/>          names(sampleDF) = gsub("sampleDF", "StreamingMovies", names(sampleDF))<br/><br/>          sampleDF &lt;- cbind(sampleDF, dummy(df$Contract, sep="."))<br/>          names(sampleDF) = gsub("sampleDF", "Contract", names(sampleDF))<br/><br/>         sampleDF &lt;- cbind(sampleDF, dummy(df$PaymentMethod, sep="."))<br/>        names(sampleDF) = gsub("sampleDF", "PaymentMethod", names(sampleDF))</pre>
<p>The results are shown in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3f7333f7-b81f-4fae-8a53-06624b08ad67.png"/></p>
<p>Once you have completed these four steps, it is time to start building ANN models for customer churn predictions. Move onto the next section for ANN modeling!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">ANN with Keras</h1>
                
            
            
                
<p>For building ANN models in R, we are going to use the <kbd>keras</kbd> package, which is a high-level neural networks library. For more details, we recommend you visit their official documentation at the following link: <a href="https://keras.io/">https://keras.io/</a>. Before we can use this package for building ANN models, we need to install two libraries:<kbd>tensorflow</kbd> and <kbd>keras</kbd>. The <kbd>keras</kbd> package uses <kbd>tensorflow</kbd> as a backend for building neural network models, so we need to install <kbd>tensorflow</kbd> first. You can install these two packages using the following commands in your RStudio:</p>
<pre>install.packages("devtools")<br/>devtools::install_github("rstudio/tensorflow")<br/>library(tensorflow)<br/>install_tensorflow()<br/><br/>devtools::install_github("rstudio/keras")<br/>library(keras)<br/>install_keras()</pre>
<p>Once you have installed these two libraries, we can finally start building our first neural network models. In this exercise, we are going to build a neural network model with one hidden layer. Take a look at the following code first:</p>
<pre>model &lt;- keras_model_sequential() <br/>model %&gt;% <br/>  layer_dense(units = 16, kernel_initializer = "uniform", activation = 'relu', input_shape=ncol(train)-1) %&gt;% <br/>  layer_dense(units = 8, kernel_initializer = "uniform", activation = 'relu') %&gt;%<br/>  layer_dense(units = 1, kernel_initializer = "uniform", activation = 'sigmoid') %&gt;% <br/>  compile(<br/>    optimizer = 'adam',<br/>    loss = 'binary_crossentropy',<br/>    metrics = c('accuracy')<br/>  )</pre>
<p>Let's take a closer look at this code. First, we are building a <kbd>Sequential</kbd> model here, <kbd>keras_model_sequential</kbd>, which is the type of model where the layers are stacked linearly and looks similar to the diagram we saw in the earlier section about the MLP model. The first layer, <kbd>layer_dense</kbd>, is an input layer, where <kbd>input_shape</kbd> is simply the number of features or columns in the sample set and the number of output units is <kbd>16</kbd>. We are using the <kbd>relu</kbd> activation function for this input layer. Then, in the hidden layer, the number of output units is <kbd>8</kbd> and the activation function to be used is <kbd>relu</kbd>. Lastly, the output layer has one output unit, which is the probability of customer churn, and we use the <kbd>sigmoid</kbd> activation function in this layer. You can experiment with different numbers of output units and activation functions for your exercise. Lastly, we need to compile this model, using the <kbd>compile</kbd> function. Here, we are using the <kbd>adam</kbd> optimizer, which is one of the most frequently used optimization algorithms. Since our target variable is binary, we are using <kbd>binary_crossentropy</kbd> as the <kbd>loss</kbd> function. Lastly, this model will use the <kbd>accuracy</kbd> metric to evaluate model performance during training.</p>
<p>Before we start training this neural network model, we will need to split our sample set into train and test sets. Take a look at the following code:</p>
<pre>library(caTools)<br/><br/>sample &lt;- sample.split(sampleDF$Churn, SplitRatio = .7)<br/><br/>train &lt;- as.data.frame(subset(sampleDF, sample == TRUE))<br/>test &lt;- as.data.frame(subset(sampleDF, sample == FALSE))<br/><br/>trainX &lt;- as.matrix(train[,names(train) != "Churn"])<br/>trainY &lt;- train$Churn<br/>testX &lt;- as.matrix(test[,names(test) != "Churn"])<br/>testY &lt;- test$Churn</pre>
<p>As you can see from this code, we are using the <kbd>sample.split</kbd> function of the <kbd>caTools</kbd> package. For our exercise, we will use 70% of the sample set for training and 30% for testing. Now we can train our neural network model using the following code:</p>
<pre>history &lt;- model %&gt;% fit(<br/>  trainX, <br/>  trainY, <br/>  epochs = 50, <br/>  batch_size = 100, <br/>  validation_split = 0.2<br/>)</pre>
<p>Here, we are using <kbd>100</kbd> samples as <kbd>batch_size</kbd>, from which the model is going to learn to predict every time, and <kbd>50</kbd> as the number of <kbd>epochs</kbd>, which is the number of complete passes through the entire training set. Once you run this code, you will see the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/35a5fae0-f1a2-42df-903e-c27aba09ef92.png"/></p>
<p>As you can see from this output, <kbd>loss</kbd> typically decreases and the accuracy (<kbd>acc</kbd>) improves in each epoch. However, the rate of model performance improvements decreases over time. As you can see from this output, there are big improvements in the loss and accuracy measures in the first few epochs and the amount of performance gain decreases over time. You can monitor this process and decide to stop when the amount of performance gain is minimal.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Model evaluations</h1>
                
            
            
                
<p>Now that we have built our first neural network model, let's evaluate its performance. We are going to look at the overall accuracy, precision, and recall, as well as the ROC curve and AUC. First, take a look at the following code for computing accuracy, precision, and recall:</p>
<pre># Evaluating ANN model<br/>inSamplePreds &lt;- as.double(model %&gt;% predict_classes(trainX))<br/>outSamplePreds &lt;- as.double(model %&gt;% predict_classes(testX))<br/><br/># - Accuracy, Precision, and Recall<br/>inSampleAccuracy &lt;- mean(trainY == inSamplePreds)<br/>outSampleAccuracy &lt;- mean(testY == outSamplePreds)<br/>print(sprintf('In-Sample Accuracy: %0.4f', inSampleAccuracy))<br/>print(sprintf('Out-Sample Accuracy: %0.4f', outSampleAccuracy))<br/><br/>inSamplePrecision &lt;- sum(inSamplePreds &amp; trainY) / sum(inSamplePreds)<br/>outSamplePrecision &lt;- sum(outSamplePreds &amp; testY) / sum(outSamplePreds)<br/>print(sprintf('In-Sample Precision: %0.4f', inSamplePrecision))<br/>print(sprintf('Out-Sample Precision: %0.4f', outSamplePrecision))<br/><br/>inSampleRecall &lt;- sum(inSamplePreds &amp; trainY) / sum(trainY)<br/>outSampleRecall &lt;- sum(outSamplePreds &amp; testY) / sum(testY)<br/>print(sprintf('In-Sample Recall: %0.4f', inSampleRecall))<br/>print(sprintf('Out-Sample Recall: %0.4f', outSampleRecall))</pre>
<p>You should be familiar with this code, as we used the same evaluation metrics in <a href="4f5163a1-c34a-495f-bc5f-e02f9b2a2052.xhtml" target="_blank">Chapter 8</a>, <em>Predicting the Likelihood of Marketing Engagement</em>. The output of this code in our case looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d902f785-3d48-4cf3-a2cf-ea30f0ec423c.png" style="width:24.50em;height:10.00em;"/></p>
<p>Due to some randomness in the model, your results might differ from these numbers. As you can see from this output, the accuracy of predicting whether a customer will churn or not in the test set is about <kbd>0.83</kbd>, suggesting the model is correct roughly about 83% of the time. The out-of-sample precision suggests that the model is correct about 72% of the time it predicts that the customer is going to churn, and the out-of-sample recall suggests that the model captures roughly 58% of the churn cases. </p>
<p>Next, we can compute the AUC and plot the ROC curve, using the following code:</p>
<pre># - ROC &amp; AUC<br/>library(ROCR)<br/><br/>outSamplePredProbs &lt;- as.double(predict(model, testX))<br/><br/>pred &lt;- prediction(outSamplePredProbs, testY)<br/>perf &lt;- performance(pred, measure = "tpr", x.measure = "fpr") <br/>auc &lt;- performance(pred, measure='auc')@y.values[[1]]<br/><br/>plot(<br/>  perf, <br/>  main=sprintf('Model ROC Curve (AUC: %0.2f)', auc), <br/>  col='darkorange', <br/>  lwd=2<br/>) + grid()<br/>abline(a = 0, b = 1, col='darkgray', lty=3, lwd=2)</pre>
<p>And the output looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9f54bb8f-6c32-4411-86f8-36fb52b697b3.png" style="width:30.75em;height:20.00em;"/></p>
<p>Along with the accuracy, precision, and recall measures that we looked at previously, the AUC and the ROC curve also suggest that the model captures and predicts those customers at churn risk pretty well. As you can see from these evaluation outputs, it is better to use the output of this model for identifying the customers who are likely to churn than simply guessing who they will be. By focusing on those customers with high churn probabilities from this model in your marketing strategies, you can try to retain those customers at churn risk in a more cost-effective way.</p>
<p>The full code for this exercise can be found in this repository: <a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.11/R/CustomerRetention.R">https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.11/R/CustomerRetention.R</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we have learned about customer churn and retention. We have discussed the reasons why customer churn hurts businesses. More specifically, we have learned how retaining existing customers is much less expensive than acquiring new customers. We have shown some of the common reasons why customers leave a company, such as poor customer service, not finding enough value in products or services, lack of communications, and lack of customer loyalty. In order to understand why customers leave, we could conduct surveys or analyze customer data to understand their needs and pain points better. We have also discussed how we can train ANN models to identify those customers who are at risk of churning. Through programming exercises, we have learned how to use the <kbd>keras</kbd> library to build and train ANN models in Python and R.</p>
<p>In the following chapter, we are going to learn about A/B testing and how it can be used to determine the best marketing strategy among different options. We are going to discuss how to compute statistical significance in Python and R to help marketers decide which marketing strategy to choose among different ideas.</p>


            

            
        
    </body></html>