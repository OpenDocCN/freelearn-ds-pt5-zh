["```py\n// From RDD: Create an RDD and convert to DataFrame\n>>> employees = sc.parallelize([(1, \"John\", 25), (2, \"Ray\", 35), (3, \"Mike\", 24), (4, \"Jane\", 28), (5, \"Kevin\", 26), (6, \"Vincent\", 35), (7, \"James\", 38), (8, \"Shane\", 32), (9, \"Larry\", 29), (10, \"Kimberly\", 29), (11, \"Alex\", 28), (12, \"Garry\", 25), (13, \"Max\", 31)]).toDF([\"emp_id\",\"name\",\"age\"])\n>>>\n\n// From JSON: reading a JSON file\n>>> salary = sqlContext.read.json(\"./salary.json\")\n>>> designation = sqlContext.read.json(\"./designation.json\")\n```", "```py\n// From RDD: Create an RDD and convert to DataFrame\nscala> val employees = sc.parallelize(List((1, \"John\", 25), (2, \"Ray\", 35), (3, \"Mike\", 24), (4, \"Jane\", 28), (5, \"Kevin\", 26), (6, \"Vincent\", 35), (7, \"James\", 38), (8, \"Shane\", 32), (9, \"Larry\", 29), (10, \"Kimberly\", 29), (11, \"Alex\", 28), (12, \"Garry\", 25), (13, \"Max\", 31))).toDF(\"emp_id\",\"name\",\"age\")\nemployees: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 1 more field]\nscala> // From JSON: reading a JSON file\nscala> val salary = spark.read.json(\"./salary.json\")\nsalary: org.apache.spark.sql.DataFrame = [e_id: bigint, salary: bigint]\nscala> val designation = spark.read.json(\"./designation.json\")\ndesignation: org.apache.spark.sql.DataFrame = [id: bigint, role: string]\n```", "```py\n// Creating the final data matrix using the join operation\n>>> final_data = employees.join(salary, employees.emp_id == salary.e_id).join(designation, employees.emp_id == designation.id).select(\"emp_id\", \"name\", \"age\", \"role\", \"salary\")\n>>> final_data.show(5)\n+------+-----+---+---------+------+\n|emp_id| name|age|     role|salary|\n+------+-----+---+---------+------+\n|     1| John| 25|Associate| 10000|\n|     2|  Ray| 35|  Manager| 12000|\n|     3| Mike| 24|  Manager| 12000|\n|     4| Jane| 28|Associate|  null|\n|     5|Kevin| 26|  Manager|   120|\n+------+-----+---+---------+------+\nonly showing top 5 rows\n```", "```py\n// Creating the final data matrix using the join operation\nscala> val final_data = employees.join(salary, $\"emp_id\" === $\"e_id\").join(designation, $\"emp_id\" === $\"id\").select(\"emp_id\", \"name\", \"age\", \"role\", \"salary\")\nfinal_data: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 3 more fields]\n```", "```py\n// Dropping rows with missing value(s)\n>>> clean_data = final_data.na.drop()\n>>> \n// Replacing missing value by mean\n>>> import math\n>>> from pyspark.sql import functions as F\n>>> mean_salary = math.floor(salary.select(F.mean('salary')).collect()[0][0])\n>>> clean_data = final_data.na.fill({'salary' : mean_salary})\n>>> \n//Another example for missing value treatment\n>>> authors = [['Thomas','Hardy','June 2, 1840'],\n       ['Charles','Dickens','7 February 1812'],\n        ['Mark','Twain',None],\n        ['Jane','Austen','16 December 1775'],\n      ['Emily',None,None]]\n>>> df1 = sc.parallelize(authors).toDF(\n       [\"FirstName\",\"LastName\",\"Dob\"])\n>>> df1.show()\n+---------+--------+----------------+\n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|   Thomas|   Hardy|    June 2, 1840|\n|  Charles| Dickens| 7 February 1812|\n|     Mark|   Twain|            null|\n|     Jane|  Austen|16 December 1775|\n|    Emily|    null|            null|\n+---------+--------+----------------+\n\n// Drop rows with missing values\n>>> df1.na.drop().show()\n+---------+--------+----------------+\n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|   Thomas|   Hardy|    June 2, 1840|\n|  Charles| Dickens| 7 February 1812|\n|     Jane|  Austen|16 December 1775|\n+---------+--------+----------------+\n\n// Drop rows with at least 2 missing values\n>>> df1.na.drop(thresh=2).show()\n+---------+--------+----------------+\n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|   Thomas|   Hardy|    June 2, 1840|\n|  Charles| Dickens| 7 February 1812|\n|     Mark|   Twain|            null|\n|     Jane|  Austen|16 December 1775|\n+---------+--------+----------------+\n\n// Fill all missing values with a given string\n>>> df1.na.fill('Unknown').show()\n+---------+--------+----------------+\n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|   Thomas|   Hardy|    June 2, 1840|\n|  Charles| Dickens| 7 February 1812|\n|     Mark|   Twain|         Unknown|\n|     Jane|  Austen|16 December 1775|\n|    Emily| Unknown|         Unknown|\n+---------+--------+----------------+\n\n// Fill missing values in each column with a given string\n>>> df1.na.fill({'LastName':'--','Dob':'Unknown'}).show()\n+---------+--------+----------------+\n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|   Thomas|   Hardy|    June 2, 1840|\n|  Charles| Dickens| 7 February 1812|\n|     Mark|   Twain|         Unknown|\n|     Jane|  Austen|16 December 1775|\n|    Emily|      --|         Unknown|\n+---------+--------+----------------+\n```", "```py\n//Missing value treatment\n// Dropping rows with missing value(s)\nscala> var clean_data = final_data.na.drop() //Note the var declaration instead of val\nclean_data: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 3 more fields]\nscala>\n\n// Replacing missing value by mean\nscal> val mean_salary = final_data.select(floor(avg(\"salary\"))).\n            first()(0).toString.toDouble\nmean_salary: Double = 20843.0\nscal> clean_data = final_data.na.fill(Map(\"salary\" -> mean_salary)) \n\n//Reassigning clean_data\nclean_data: org.apache.spark.sql.DataFrame = [emp_id: int, name: string ... 3 more fields]\nscala>\n\n//Another example for missing value treatment\nscala> case class Author (FirstName: String, LastName: String, Dob: String)\ndefined class Author\nscala> val authors = Seq(\n        Author(\"Thomas\",\"Hardy\",\"June 2, 1840\"),\n        Author(\"Charles\",\"Dickens\",\"7 February 1812\"),\n        Author(\"Mark\",\"Twain\",null),\n        Author(\"Emily\",null,null))\nauthors: Seq[Author] = List(Author(Thomas,Hardy,June 2, 1840),\n   Author(Charles,Dickens,7 February 1812), Author(Mark,Twain,null),\n   Author(Emily,null,null))\nscala> val ds1 = sc.parallelize(authors).toDS()\nds1: org.apache.spark.sql.Dataset[Author] = [FirstName: string, LastName: string ... 1 more field]\nscala> ds1.show()\n+---------+--------+---------------+\n|FirstName|LastName|            Dob|\n+---------+--------+---------------+\n|   Thomas|   Hardy|   June 2, 1840|\n|  Charles| Dickens|7 February 1812|\n|     Mark|   Twain|           null|\n|    Emily|    null|           null|\n+---------+--------+---------------+\nscala>\n\n// Drop rows with missing values\nscala> ds1.na.drop().show()\n+---------+--------+---------------+\n|FirstName|LastName|            Dob|\n+---------+--------+---------------+\n|   Thomas|   Hardy|   June 2, 1840|\n|  Charles| Dickens|7 February 1812|\n+---------+--------+---------------+\nscala>\n\n//Drop rows with at least 2 missing values\n//Note that there is no direct scala function to drop rows with at least n missing values\n//However, you can drop rows containing under specified non nulls\n//Use that function to achieve the same result\nscala> ds1.na.drop(minNonNulls = df1.columns.length - 1).show()\n//Fill all missing values with a given string\nscala> ds1.na.fill(\"Unknown\").show()\n+---------+--------+---------------+\n|FirstName|LastName|            Dob|\n+---------+--------+---------------+\n|   Thomas|   Hardy|   June 2, 1840|\n|  Charles| Dickens|7 February 1812|\n|     Mark|   Twain|        Unknown|\n|    Emily| Unknown|        Unknown|\n+---------+--------+---------------+\nscala>\n\n//Fill missing values in each column with a given string\nscala> ds1.na.fill(Map(\"LastName\"->\"--\",\n                    \"Dob\"->\"Unknown\")).show()\n+---------+--------+---------------+\n|FirstName|LastName|            Dob|\n+---------+--------+---------------+\n|   Thomas|   Hardy|   June 2, 1840|\n|  Charles| Dickens|7 February 1812|\n|     Mark|   Twain|        Unknown|\n|    Emily|      --|        Unknown|\n+---------+--------+---------------+\n```", "```py\n// Identify outliers and replace them with mean\n//The following example reuses the clean_data dataset and mean_salary computed in previous examples\n>>> mean_salary\n20843.0\n>>> \n//Compute deviation for each row\n>>> devs = final_data.select(((final_data.salary - mean_salary) ** 2).alias(\"deviation\"))\n\n//Compute standard deviation\n>>> stddev = math.floor(math.sqrt(devs.groupBy().\n          avg(\"deviation\").first()[0]))\n\n//check standard deviation value\n>>> round(stddev,2)\n30351.0\n>>> \n//Replace outliers beyond 2 standard deviations with the mean salary\n>>> no_outlier = final_data.select(final_data.emp_id, final_data.name, final_data.age, final_data.salary, final_data.role, F.when(final_data.salary.between(mean_salary-(2*stddev), mean_salary+(2*stddev)), final_data.salary).otherwise(mean_salary).alias(\"updated_salary\"))\n>>> \n//Observe modified values\n>>> no_outlier.filter(no_outlier.salary != no_outlier.updated_salary).show()\n+------+----+---+------+-------+--------------+\n|emp_id|name|age|salary|   role|updated_salary|\n+------+----+---+------+-------+--------------+\n|    13| Max| 31|120000|Manager|       20843.0|\n+------+----+---+------+-------+--------------+\n>>>\n\n```", "```py\n// Identify outliers and replace them with mean\n//The following example reuses the clean_data dataset and mean_salary computed in previous examples\n//Compute deviation for each row\nscala> val devs = clean_data.select(((clean_data(\"salary\") - mean_salary) *\n        (clean_data(\"salary\") - mean_salary)).alias(\"deviation\"))\ndevs: org.apache.spark.sql.DataFrame = [deviation: double]\n\n//Compute standard deviation\nscala> val stddev = devs.select(sqrt(avg(\"deviation\"))).\n            first().getDouble(0)\nstddev: Double = 29160.932595617614\n\n//If you want to round the stddev value, use BigDecimal as shown\nscala> scala.math.BigDecimal(stddev).setScale(2,\n             BigDecimal.RoundingMode.HALF_UP)\nres14: scala.math.BigDecimal = 29160.93\nscala>\n\n//Replace outliers beyond 2 standard deviations with the mean salary\nscala> val outlierfunc = udf((value: Long, mean: Double) => {if (value > mean+(2*stddev)\n            || value < mean-(2*stddev)) mean else value})\n\n//Use the UDF to compute updated_salary\n//Note the usage of lit() to wrap a literal as a column\nscala> val no_outlier = clean_data.withColumn(\"updated_salary\",\n            outlierfunc(col(\"salary\"),lit(mean_salary)))\n\n//Observe modified values\nscala> no_outlier.filter(no_outlier(\"salary\") =!=  //Not !=\n             no_outlier(\"updated_salary\")).show()\n+------+----+---+-------+------+--------------+\n|emp_id|name|age|   role|salary|updated_salary|\n+------+----+---+-------+------+--------------+\n|    13| Max| 31|Manager|120000|       20843.0|\n+------+----+---+-------+------+--------------+\n```", "```py\n// Deleting the duplicate rows\n>>> authors = [['Thomas','Hardy','June 2,1840'],\n    ['Thomas','Hardy','June 2,1840'],\n    ['Thomas','H',None],\n    ['Jane','Austen','16 December 1775'],\n    ['Emily',None,None]]\n>>> df1 = sc.parallelize(authors).toDF(\n      [\"FirstName\",\"LastName\",\"Dob\"])\n>>> df1.show()\n+---------+--------+----------------+\n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|   Thomas|   Hardy|    June 2, 1840|\n|   Thomas|   Hardy|    June 2, 1840|\n|   Thomas|       H|            null|\n|     Jane|  Austen|16 December 1775|\n|    Emily|    null|            null|\n+---------+--------+----------------+\n\n// Drop duplicated rows\n>>> df1.dropDuplicates().show()\n+---------+--------+----------------+\n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|    Emily|    null|            null|\n|     Jane|  Austen|16 December 1775|\n|   Thomas|       H|            null|\n|   Thomas|   Hardy|    June 2, 1840|\n+---------+--------+----------------+\n\n// Drop duplicates based on a sub set of columns\n>>> df1.dropDuplicates(subset=[\"FirstName\"]).show()\n+---------+--------+----------------+\n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|    Emily|    null|            null|\n|   Thomas|   Hardy|    June 2, 1840|\n|     Jane|  Austen|16 December 1775|\n+---------+--------+----------------+\n>>> \n```", "```py\n//Duplicate values treatment\n// Reusing the Author case class\n// Deleting the duplicate rows\nscala> val authors = Seq(\n            Author(\"Thomas\",\"Hardy\",\"June 2,1840\"),\n            Author(\"Thomas\",\"Hardy\",\"June 2,1840\"),\n            Author(\"Thomas\",\"H\",null),\n            Author(\"Jane\",\"Austen\",\"16 December 1775\"),\n            Author(\"Emily\",null,null))\nauthors: Seq[Author] = List(Author(Thomas,Hardy,June 2,1840), Author(Thomas,Hardy,June 2,1840), Author(Thomas,H,null), Author(Jane,Austen,16 December 1775), Author(Emily,null,null))\nscala> val ds1 = sc.parallelize(authors).toDS()\nds1: org.apache.spark.sql.Dataset[Author] = [FirstName: string, LastName: string ... 1 more field]\nscala> ds1.show()\n+---------+--------+----------------+\n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|   Thomas|   Hardy|     June 2,1840|\n|   Thomas|   Hardy|     June 2,1840|\n|   Thomas|       H|            null|\n|     Jane|  Austen|16 December 1775|\n|    Emily|    null|            null|\n+---------+--------+----------------+\nscala>\n\n// Drop duplicated rows\nscala> ds1.dropDuplicates().show()\n+---------+--------+----------------+                                          \n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|     Jane|  Austen|16 December 1775|\n|    Emily|    null|            null|\n|   Thomas|   Hardy|     June 2,1840|\n|   Thomas|       H|            null|\n+---------+--------+----------------+\nscala>\n\n// Drop duplicates based on a sub set of columns\nscala> ds1.dropDuplicates(\"FirstName\").show()\n+---------+--------+----------------+                                           \n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|    Emily|    null|            null|\n|     Jane|  Austen|16 December 1775|\n|   Thomas|   Hardy|     June 2,1840|\n+---------+--------+----------------+\n```", "```py\n// Merging columns\n//Create a udf to concatenate two column values\n>>> import pyspark.sql.functions\n>>> concat_func = pyspark.sql.functions.udf(lambda name, age: name + \"_\" + str(age))\n\n//Apply the udf to create merged column\n>>> concat_df = final_data.withColumn(\"name_age\", concat_func(final_data.name, final_data.age))\n>>> concat_df.show(4)\n+------+----+---+---------+------+--------+\n|emp_id|name|age|     role|salary|name_age|\n+------+----+---+---------+------+--------+\n|     1|John| 25|Associate| 10000| John_25|\n|     2| Ray| 35|  Manager| 12000|  Ray_35|\n|     3|Mike| 24|  Manager| 12000| Mike_24|\n|     4|Jane| 28|Associate|  null| Jane_28|\n+------+----+---+---------+------+--------+\nonly showing top 4 rows\n// Adding constant to data\n>>> data_new = concat_df.withColumn(\"age_incremented\",concat_df.age + 10)\n>>> data_new.show(4)\n+------+----+---+---------+------+--------+---------------+\n|emp_id|name|age|     role|salary|name_age|age_incremented|\n+------+----+---+---------+------+--------+---------------+\n|     1|John| 25|Associate| 10000| John_25|             35|\n|     2| Ray| 35|  Manager| 12000|  Ray_35|             45|\n|     3|Mike| 24|  Manager| 12000| Mike_24|             34|\n|     4|Jane| 28|Associate|  null| Jane_28|             38|\n+------+----+---+---------+------+--------+---------------+\nonly showing top 4 rows\n>>> \n\n//Replace values in a column\n>>> df1.replace('Emily','Charlotte','FirstName').show()\n+---------+--------+----------------+\n|FirstName|LastName|             Dob|\n+---------+--------+----------------+\n|   Thomas|   Hardy|    June 2, 1840|\n|  Charles| Dickens| 7 February 1812|\n|     Mark|   Twain|            null|\n|     Jane|  Austen|16 December 1775|\n|Charlotte|    null|            null|\n+---------+--------+----------------+\n\n// If the column name argument is omitted in replace, then replacement is applicable to all columns\n//Append new columns based on existing values in a column\n//Give 'LastName' instead of 'Initial' if you want to overwrite\n>>> df1.withColumn('Initial',df1.LastName.substr(1,1)).show()\n+---------+--------+----------------+-------+\n|FirstName|LastName|             Dob|Initial|\n+---------+--------+----------------+-------+\n|   Thomas|   Hardy|    June 2, 1840|      H|\n|  Charles| Dickens| 7 February 1812|      D|\n|     Mark|   Twain|            null|      T|\n|     Jane|  Austen|16 December 1775|      A|\n|    Emily|    null|            null|   null|\n+---------+--------+----------------+-------+\n```", "```py\n// Merging columns\n//Create a udf to concatenate two column values\nscala> val concatfunc = udf((name: String, age: Integer) =>\n                           {name + \"_\" + age})\nconcatfunc: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function2>,StringType,Some(List(StringType, IntegerType)))\nscala>\n\n//Apply the udf to create merged column\nscala> val concat_df = final_data.withColumn(\"name_age\",\n                         concatfunc($\"name\", $\"age\"))\nconcat_df: org.apache.spark.sql.DataFrame =\n         [emp_id: int, name: string ... 4 more fields]\nscala> concat_df.show(4)\n+------+----+---+---------+------+--------+\n|emp_id|name|age|     role|salary|name_age|\n+------+----+---+---------+------+--------+\n|     1|John| 25|Associate| 10000| John_25|\n|     2| Ray| 35|  Manager| 12000|  Ray_35|\n|     3|Mike| 24|  Manager| 12000| Mike_24|\n|     4|Jane| 28|Associate|  null| Jane_28|\n+------+----+---+---------+------+--------+\nonly showing top 4 rows\nscala>\n\n// Adding constant to data\nscala> val addconst = udf((age: Integer) => {age + 10})\naddconst: org.apache.spark.sql.expressions.UserDefinedFunction =\n      UserDefinedFunction(<function1>,IntegerType,Some(List(IntegerType)))\nscala> val data_new = concat_df.withColumn(\"age_incremented\",\n                 addconst(col(\"age\")))\ndata_new: org.apache.spark.sql.DataFrame =\n     [emp_id: int, name: string ... 5 more fields]\nscala> data_new.show(4)\n+------+----+---+---------+------+--------+---------------+\n|emp_id|name|age|     role|salary|name_age|age_incremented|\n+------+----+---+---------+------+--------+---------------+\n|     1|John| 25|Associate| 10000| John_25|             35|\n|     2| Ray| 35|  Manager| 12000|  Ray_35|             45|\n|     3|Mike| 24|  Manager| 12000| Mike_24|             34|\n|     4|Jane| 28|Associate|  null| Jane_28|             38|\n+------+----+---+---------+------+--------+---------------+\nonly showing top 4 rows\n\n// Replace values in a column\n//Note: As of Spark 2.0.0, there is no replace on DataFrame/ Dataset does not work so .na. is a work around\nscala> ds1.na.replace(\"FirstName\",Map(\"Emily\" -> \"Charlotte\")).show()\n+---------+--------+---------------+\n|FirstName|LastName|            Dob|\n+---------+--------+---------------+\n|   Thomas|   Hardy|   June 2, 1840|\n|  Charles| Dickens|7 February 1812|\n|     Mark|   Twain|           null|\n|Charlotte|    null|           null|\n+---------+--------+---------------+\nscala>\n\n// If the column name argument is \"*\" in replace, then replacement is applicable to all columns\n//Append new columns based on existing values in a column\n//Give \"LastName\" instead of \"Initial\" if you want to overwrite\nscala> ds1.withColumn(\"Initial\",ds1(\"LastName\").substr(1,1)).show()\n+---------+--------+---------------+-------+\n|FirstName|LastName|            Dob|Initial|\n+---------+--------+---------------+-------+\n|   Thomas|   Hardy|   June 2, 1840|      H|\n|  Charles| Dickens|7 February 1812|      D|\n|     Mark|   Twain|           null|      T|\n|    Emily|    null|           null|   null|\n+---------+--------+---------------+-------+\n```", "```py\n// Date conversions\n//Create udf for date conversion that converts incoming string to YYYY-MM-DD format\n// The function assumes month is full month name and year is always 4 digits\n// Separator is always a space or comma\n// Month, date and year may come in any order\n//Reusing authors data\n>>> authors = [['Thomas','Hardy','June 2, 1840'],\n        ['Charles','Dickens','7 February 1812'],\n        ['Mark','Twain',None],\n        ['Jane','Austen','16 December 1775'],\n        ['Emily',None,None]]\n>>> df1 = sc.parallelize(authors).toDF(\n      [\"FirstName\",\"LastName\",\"Dob\"])\n>>> \n\n// Define udf\n//Note: You may create this in a script file and execute with execfile(filename.py)\n>>> def toDate(s):\n import re\n year = month = day = \"\"\n if not s:\n  return None\n mn = [0,'January','February','March','April','May',\n  'June','July','August','September',\n  'October','November','December']\n\n //Split the string and remove empty tokens\n l = [tok for tok in re.split(\",| \",s) if tok]\n\n//Assign token to year, month or day\n for a in l:\n  if a in mn:\n   month = \"{:0>2d}\".format(mn.index(a))\n  elif len(a) == 4:\n   year = a\n  elif len(a) == 1:\n   day = '0' + a\n  else:\n   day = a\n return year + '-' + month + '-' + day\n>>> \n\n//Register the udf\n>>> from pyspark.sql.functions import udf\n>>> from pyspark.sql.types import StringType\n>>> toDateUDF = udf(toDate, StringType())\n\n//Apply udf\n>>> df1.withColumn(\"Dob\",toDateUDF(\"Dob\")).show()\n+---------+--------+----------+\n|FirstName|LastName|       Dob|\n+---------+--------+----------+\n|   Thomas|   Hardy|1840-06-02|\n|  Charles| Dickens|1812-02-07|\n|     Mark|   Twain|      null|\n|     Jane|  Austen|1775-12-16|\n|    Emily|    null|      null|\n+---------+--------+----------+\n>>> \n```", "```py\n//Date conversions\n//Create udf for date conversion that converts incoming string to YYYY-MM-DD format\n// The function assumes month is full month name and year is always 4 digits\n// Separator is always a space or comma\n// Month, date and year may come in any order\n//Reusing authors case class and data\n>>> val authors = Seq(\n        Author(\"Thomas\",\"Hardy\",\"June 2, 1840\"),\n        Author(\"Charles\",\"Dickens\",\"7 February 1812\"),\n        Author(\"Mark\",\"Twain\",null),\n        Author(\"Jane\",\"Austen\",\"16 December 1775\"),\n        Author(\"Emily\",null,null))\nauthors: Seq[Author] = List(Author(Thomas,Hardy,June 2, 1840), Author(Charles,Dickens,7 February 1812), Author(Mark,Twain,null), Author(Jane,Austen,16 December 1775), Author(Emily,null,null))\nscala> val ds1 = sc.parallelize(authors).toDS()\nds1: org.apache.spark.sql.Dataset[Author] = [FirstName: string, LastName: string ... 1 more field]\nscala>\n\n// Define udf\n//Note: You can type :paste on REPL to paste  multiline code. CTRL + D signals end of paste mode\ndef toDateUDF = udf((s: String) => {\n    var (year, month, day) = (\"\",\"\",\"\")\n    val mn = List(\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\n        \"June\",\"July\",\"August\",\"September\",\n        \"October\",\"November\",\"December\")\n    //Tokenize the date string and remove trailing comma, if any\n    if(s != null) {\n      for (x <- s.split(\" \")) {\n        val token = x.stripSuffix(\",\")\n        token match {\n        case \"\" =>\n        case x if (mn.contains(token)) =>\n            month = \"%02d\".format(mn.indexOf(token))\n        case x if (token.length() == 4) =>\n            year = token\n        case x =>\n            day = token\n        }\n     }   //End of token processing for\n     year + \"-\" + month + \"-\" + day=\n   } else {\n       null\n   }\n})\ntoDateUDF: org.apache.spark.sql.expressions.UserDefinedFunction\nscala>\n\n//Apply udf and convert date strings to standard form YYYY-MM-DD\nscala> ds1.withColumn(\"Dob\",toDateUDF(ds1(\"Dob\"))).show()\n+---------+--------+----------+\n|FirstName|LastName|       Dob|\n+---------+--------+----------+\n|   Thomas|   Hardy| 1840-06-2|\n|  Charles| Dickens| 1812-02-7|\n|     Mark|   Twain|      null|\n|     Jane|  Austen|1775-12-16|\n|    Emily|    null|      null|\n+---------+--------+----------+\n```", "```py\n/* ”Sample” function is defined for DataFrames (not RDDs) which takes three parameters:\nwithReplacement - Sample with replacement or not (input: True/False)\nfraction - Fraction of rows to generate (input: any number between 0 and 1 as per your requirement of sample size)\nseed - Seed for sampling (input: Any random seed)\n*/\n>>> sample1 = data_new.sample(False, 0.6) //With random seed as no seed value specified\n>>> sample2 = data_new.sample(False, 0.6, 10000) //With specific seed value of 10000\n```", "```py\nscala> val sample1 = data_new.sample(false, 0.6) //With random seed as no seed value specified\nsample1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [emp_id: int, name: string ... 5 more fields]\nscala> val sample2 = data_new.sample(false, 0.6, 10000) //With specific seed value of 10000\nsample2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [emp_id: int, name: string ... 5 more fields]\n```", "```py\n>>> mean_age = data_new.agg({'age': 'mean'}).first()[0]\n>>> age_counts = data_new.groupBy(\"age\").agg({\"age\": \"count\"}).alias(\"freq\")\n>>> mode_age = age_counts.sort(age_counts[\"COUNT(age)\"].desc(), age_counts.age.asc()).first()[0]\n>>> print(mean_age, mode_age)\n(29.615384615384617, 25)\n>>> age_counts.sort(\"count(age)\",ascending=False).show(2)\n+---+----------+                                                               \n|age|count(age)|\n+---+----------+\n| 28|         3|\n| 29|         2|\n+---+----------+\nonly showing top 2 rows\n```", "```py\n//Reusing data_new created \nscala> val mean_age = data_new.select(floor(avg(\"age\"))).first().getLong(0)\nmean_age: Long = 29\nscala> val mode_age = data_new.groupBy($\"age\").agg(count($\"age\")).\n                 sort($\"count(age)\".desc, $\"age\").first().getInt(0)\nmode_age: Int = 28\nscala> val age_counts = data_new.groupBy(\"age\").agg(count($\"age\") as \"freq\")\nage_counts: org.apache.spark.sql.DataFrame = [age: int, freq: bigint]\nscala> age_counts.sort($\"freq\".desc).show(2)\n+---+----+                                                                     \n|age|freq|\n+---+----+\n| 35|   2|\n| 28|   2|\n+---+----+\n```", "```py\n//Reusing data_new created before\nimport math\n>>> range_salary = data_new.agg({'salary': 'max'}).first()[0] - data_new.agg({'salary': 'min'}).first()[0]\n>>> mean_salary = data_new.agg({'salary': 'mean'}).first()[0]\n>>> salary_deviations = data_new.select(((data_new.salary - mean_salary) *\n       (data_new.salary - mean_salary)).alias(\"deviation\"))\n>>> stddev_salary = math.sqrt(salary_deviations.agg({'deviation' : \n'avg'}).first()[0])\n>>> variance_salary = salary_deviations.groupBy().avg(\"deviation\").first()[0]\n>>> print(round(range_salary,2), round(mean_salary,2),\n      round(variance_salary,2), round(stddev_salary,2))\n(119880.0, 20843.33, 921223322.22, 30351.66)\n>>> \n```", "```py\n//Reusing data_new created before\nscala> val range_salary = data_new.select(max(\"salary\")).first().\n          getLong(0) - data_new.select(min(\"salary\")).first().getLong(0)\nrange_salary: Long = 119880\nscala> val mean_salary = data_new.select(floor(avg(\"salary\"))).first().getLong(0)\nmean_salary: Long = 20843\nscala> val salary_deviations = data_new.select(((data_new(\"salary\") - mean_salary)\n                     * (data_new(\"salary\") - mean_salary)).alias(\"deviation\"))\nsalary_deviations: org.apache.spark.sql.DataFrame = [deviation: bigint]\nscala> val variance_salary = { salary_deviations.select(avg(\"deviation\"))\n                                       .first().getDouble(0) }\nvariance_salary: Double = 9.212233223333334E8\nscala> val stddev_salary = { salary_deviations\n                    .select(sqrt(avg(\"deviation\")))\n                    .first().getDouble(0) }\nstddev_salary: Double = 30351.660948510435\n```", "```py\n>>> import numpy\n>>> from pyspark.mllib.stat import Statistics\n// Create an RDD of number vectors\n//This example creates an RDD with 5 rows with 5 elements each\n>>> observations = sc.parallelize(numpy.random.random_integers(0,100,(5,5)))\n// Compute column summary statistics.\n//Note that the results may vary because of random numbers\n>>> summary = Statistics.colStats(observations)\n>>> print(summary.mean())       // mean value for each column\n>>> print(summary.variance())  // column-wise variance\n>>> print(summary.numNonzeros())// number of nonzeros in each column\n```", "```py\nscala> import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors\nscala> import org.apache.spark.mllib.stat.{\n          MultivariateStatisticalSummary, Statistics}\nimport org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\n// Create an RDD of number vectors\n//This example creates an RDD with 5 rows with 5 elements each\nscala> val observations = sc.parallelize(Seq.fill(5)(Vectors.dense(Array.fill(5)(\n                    scala.util.Random.nextDouble))))\nobservations: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[43] at parallelize at <console>:27\nscala>\n// Compute column summary statistics.\n//Note that the results may vary because of random numbers\nscala> val summary = Statistics.colStats(observations)\nsummary: org.apache.spark.mllib.stat.MultivariateStatisticalSummary = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@36836161\nscala> println(summary.mean)  // mean value for each column\n[0.5782406967737089,0.5903954680966121,0.4892908815930067,0.45680701799234835,0.6611492334819364]\nscala> println(summary.variance)    // column-wise variance\n[0.11893608153330748,0.07673977181967367,0.023169197889513014,0.08882605965192601,0.08360159585590332]\nscala> println(summary.numNonzeros) // number of nonzeros in each column\n[5.0,5.0,5.0,5.0,5.0]\n```", "```py\n//Histogram\n>>>from random import randint\n>>> numRDD = sc.parallelize([randint(0,9) for x in xrange(1,1001)])\n// Generate histogram data for given bucket count\n>>> numRDD.histogram(5)\n([0.0, 1.8, 3.6, 5.4, 7.2, 9], [202, 213, 215, 188, 182])\n//Alternatively, specify ranges\n>>> numRDD.histogram([0,3,6,10])\n([0, 3, 6, 10], [319, 311, 370])\n```", "```py\n//Histogram\nscala> val numRDD = sc.parallelize(Seq.fill(1000)(\n                    scala.util.Random.nextInt(10)))\nnumRDD: org.apache.spark.rdd.RDD[Int] =\n     ParallelCollectionRDD[0] at parallelize at <console>:24\n// Generate histogram data for given bucket count\nscala> numRDD.histogram(5)\nres10: (Array[Double], Array[Long]) = (Array(0.0, 1.8, 3.6, 5.4, 7.2, 9.0),Array(194, 209, 215, 195, 187))\nscala>\n//Alternatively, specify ranges\nscala> numRDD.histogram(Array(0,3.0,6,10))\nres13: Array[Long] = Array(293, 325, 382)\n```", "```py\n //Chi-Square test\n>>> from pyspark.mllib.linalg import Vectors, Matrices\n>>> from pyspark.mllib.stat import Statistics\n>>> import random\n>>> \n//Make a vector of frequencies of events\n>>> vec = Vectors.dense( random.sample(xrange(1,101),10))\n>>> vec\nDenseVector([45.0, 40.0, 93.0, 66.0, 56.0, 82.0, 36.0, 30.0, 85.0, 15.0])\n// Get Goodnesss of fit test results\n>>> GFT_Result = Statistics.chiSqTest(vec)\n// Here the ‘goodness of fit test’ is conducted because your input is a vector\n//Make a contingency matrix\n>>> mat = Matrices.dense(5,6,random.sample(xrange(1,101),30))\\\n//Get independense test results\\\\\n>>> IT_Result = Statistics.chiSqTest(mat)\n// Here the ‘independence test’ is conducted because your input is a vector\n//Examine the independence test results\n>>> print(IT_Result)\nChi squared test summary:\nmethod: pearson\ndegrees of freedom = 20\nstatistic = 285.9423808343265\npValue = 0.0\nVery strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n```", "```py\nscala> import org.apache.spark.mllib.linalg.{Vectors, Matrices}\nimport org.apache.spark.mllib.linalg.{Vectors, Matrices} \n\nscala> import org.apache.spark.mllib.stat.Statistics \n\nscala> val vec = Vectors.dense( Array.fill(10)(               scala.util.Random.nextDouble))vec: org.apache.spark.mllib.linalg.Vector = [0.4925741159101148,....] \n\nscala> val GFT_Result = Statistics.chiSqTest(vec)GFT_Result: org.apache.spark.mllib.stat.test.ChiSqTestResult =Chi squared test summary:\nmethod: pearson\ndegrees of freedom = 9\nstatistic = 1.9350768763253192\npValue = 0.9924531181394086\nNo presumption against null hypothesis: observed follows the same distribution as expected..\n// Here the ‘goodness of fit test’ is conducted because your input is a vector\nscala> val mat = Matrices.dense(5,6, Array.fill(30)(scala.util.Random.nextDouble)) // a contingency matrix\nmat: org.apache.spark.mllib.linalg.Matrix =..... \nscala> val IT_Result = Statistics.chiSqTest(mat)\nIT_Result: org.apache.spark.mllib.stat.test.ChiSqTestResult =Chi squared test summary:\nmethod: pearson\ndegrees of freedom = 20\nstatistic = 2.5401190679900663\npValue = 0.9999990459111089\nNo presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n// Here the ‘independence test’ is conducted because your input is a vector\n\n```", "```py\n>>> from pyspark.mllib.stat import Statistics\n>>> import random \n// Define two series\n//Number of partitions and cardinality of both Ser_1 and Ser_2 should be the same\n>>> Ser_1 = sc.parallelize(random.sample(xrange(1,101),10))       \n// Define Series_1>>> Ser_2 = sc.parallelize(random.sample(xrange(1,101),10))       \n// Define Series_2 \n>>> correlation = Statistics.corr(Ser_1, Ser_2, method = \"pearson\") \n//if you are interested in Spearman method, use “spearman” switch instead\n>>> round(correlation,2)-0.14\n>>> correlation = Statistics.corr(Ser_1, Ser_2, method =\"spearman\")\n>>> round(correlation,2)-0.19//Check on matrix//The following statement creates 100 rows of 5 elements each\n>>> data = sc.parallelize([random.sample(xrange(1,51),5) for x in range(100)])\n>>> correlMatrix = Statistics.corr(data, method = \"pearson\") \n//method may be spearman as per you requirement\n>>> correlMatrix\narray([[ 1.        ,  0.09889342, -0.14634881,  0.00178334,  0.08389984],       [ 0.09889342,  1.        , -0.07068631, -0.02212963, -0.1058252 ],       [-0.14634881, -0.07068631,  1.        , -0.22425991,  0.11063062],       [ 0.00178334, -0.02212963, -0.22425991,  1.        , -0.04864668],       [ 0.08389984, -0.1058252 ,  0.11063062, -0.04864668,  1.        \n]])\n>>> \n\n```", "```py\nscala> val correlation = Statistics.corr(Ser_1, Ser_2, \"pearson\")correlation: Double = 0.43217145308272087 \n//if you are interested in Spearman method, use “spearman” switch instead\nscala> val correlation = Statistics.corr(Ser_1, Ser_2, \"spearman\")correlation: Double = 0.4181818181818179 \nscala>\n//Check on matrix\n//The following statement creates 100 rows of 5 element Vectors\nscala> val data = sc.parallelize(Seq.fill(100)(Vectors.dense(Array.fill(5)(              scala.util.Random.nextDouble))))\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[37] at parallelize at <console>:27 \nscala> val correlMatrix = Statistics.corr(data, method=\"pearson\") \n//method may be spearman as per you requirement\ncorrelMatrix: org.apache.spark.mllib.linalg.Matrix =1.0                    -0.05478051936343809  ... (5 total)-0.05478051936343809   1.0                   ..........\n```"]