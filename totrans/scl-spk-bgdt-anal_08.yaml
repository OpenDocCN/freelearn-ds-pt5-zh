- en: Introduce a Little Structure - Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"One machine can do the work of fifty ordinary men. No machine can do the work
    of one extraordinary man."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Elbert Hubbard'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to use Spark for the analysis of structured
    data (unstructured data, such as a document containing arbitrary text or some
    other format has to be transformed into a structured form); we will see how DataFrames/datasets
    are the corner stone here, and how Spark SQL''s APIs make querying structured
    data simple yet robust. Moreover, we introduce datasets and see the difference
    between datasets, DataFrames, and RDDs. In a nutshell, the following topics will
    be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL and DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame and SQL API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame schema
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: datasets and encoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading and saving data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL and DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before Apache Spark, Apache Hive was the go-to technology whenever anyone wanted
    to run an SQL-like query on a large amount of data. Apache Hive essentially translated
    SQL queries into MapReduce-like, like logic, automatically making it very easy
    to perform many kinds of analytics on big data without actually learning to write
    complex code in Java and Scala.
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of Apache Spark, there was a paradigm shift in how we can perform
    analysis on big data scale. Spark SQL provides an easy-to-use SQL-like layer on
    top of Apache Spark's distributed computation abilities. In fact, Spark SQL can
    be used as an online analytical processing database.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00297.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Spark SQL works by parsing the SQL-like statement into an **Abstract Syntax
    Tree** (**AST**), subsequently converting that plan to a logical plan and then
    optimizing the logical plan into a physical plan that can be executed. The final
    execution uses the underlying DataFrame API, making it very easy for anyone to
    use DataFrame APIs by simply using an SQL-like interface rather than learning
    all the internals. Since this book dives into technical details of various APIs,
    we will primarily cover the DataFrame APIs, showing Spark SQL API in some places
    to contrast the different ways of using the APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, DataFrame API is the underlying layer beneath Spark SQL. In this chapter,
    we will show you how to create DataFrames using various techniques, including
    SQL queries and performing operations on the DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: A DataFrame is an abstraction of the **Resilient Distributed dataset** (**RDD**),
    dealing with higher level functions optimized using catalyst optimizer and also
    highly performant via the Tungsten Initiative. You can think of a dataset as an
    efficient table of an RDD with heavily optimized binary representation of the
    data. The binary representation is achieved using encoders, which serializes the
    various objects into a binary structure for much better performance than RDD representation.
    Since DataFrames uses the RDD internally anyway, a DataFrame/dataset is also distributed
    exactly like an RDD, and thus is also a distributed dataset. Obviously, this also
    means datasets are immutable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an illustration of the binary representation of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: datasets were added in Spark 1.6 and provide the benefits of strong typing on
    top of DataFrames. In fact, since Spark 2.0, the DataFrame is simply an alias
    of a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '`org.apache.spark.sql` defines type `DataFrame` as a `dataset[Row]`, which
    means that most of the APIs will work well with both datasets and `DataFrames`'
  prefs: []
  type: TYPE_NORMAL
- en: '**type DataFrame = dataset[Row]**'
  prefs: []
  type: TYPE_NORMAL
- en: A DataFrame is conceptually similar to a table in a Relational Database. Hence,
    a DataFrame contains rows of data, with each row comprised of several columns.
  prefs: []
  type: TYPE_NORMAL
- en: One of the first things we need to keep in mind is that, just like RDDs, DataFrames
    are immutable. This property of DataFrames being immutable means every transformation
    or action creates a new DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Let's start looking more into DataFrames and how they are different from RDDs.
    RDD's, as seen before, represent a low-level API of data manipulation in Apache
    Spark. The DataFrames were created on top of RDDs to abstract the low-level inner
    workings of RDDs and expose high-level APIs, which are easier to use and provide
    a lot of functionality out-of-the-box. DataFrame was created by following similar
    concepts found in the Python pandas package, R language, Julia language, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned before, DataFrames translate the SQL code and domain specific
    language expressions into optimized execution plans to be run on top of Spark
    Core APIs in order for the SQL statements to perform a wide variety of operations.
    DataFrames support many different types of input data sources and many types of
    operations. These includes all types of SQL operations, such as joins, group by,
    aggregations, and window functions, as most of the databases. Spark SQL is also
    quite similar to the Hive query language, and since Spark provides a natural adapter
    to Apache Hive, users who have been working in Apache Hive can easily transfer
    their knowledge, applying it to Spark SQL, thus minimizing the transition time.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames essentially depend on the concept of a table, as seen previously.
    The table can be operated on very similar to how Apache Hive works. In fact, many
    of the operations on the tables in Apache Spark are similar to how Apache Hive
    handles tables and operates on those tables. Once you have a table that is the
    DataFrame, the DataFrame can be registered as a table and you can operate on the
    data using Spark SQL statements in lieu of DataFrame APIs.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames depend on the catalyst optimizer and the Tungsten performance improvements,
    so let's briefly examine how catalyst optimizer works. A catalyst optimizer creates
    a parsed logical plan from the input SQL and then analyzes the logical plan by
    looking at all the various attributes and columns used in the SQL statement. Once
    the analyzed logical plan is created, catalyst optimizer further tries to optimize
    the plan by combining several operations and also rearranging the logic to get
    better performance.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand the catalyst optimizer, think about it as a common sense
    logic Optimizer which can reorder operations such as filters and transformations,
    sometimes grouping several operations into one so as to minimize the amount of
    data that is shuffled across the worker nodes. For example, catalyst optimizer
    may decide to broadcast the smaller datasets when performing joint operations
    between different datasets. Use explain to look at the execution plan of any data
    frame. The catalyst optimizer also computes statistics of the DataFrame's columns
    and partitions, improving the speed of execution.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if there are transformations and filters on the data partitions,
    then the order in which we filter data and apply transformations matters a lot
    to the overall performance of the operations. As a result of all the optimizations,
    the optimized logical plan is generated, which is then converted into a physical
    plan. Obviously, several physical plans are possibilities to execute the same
    SQL statement and generate the same result. The cost optimization logic determines
    and picks a good physical plan, based on cost optimizations and estimations.
  prefs: []
  type: TYPE_NORMAL
- en: Tungsten performance improvements are another key ingredient in the secret sauce
    behind the phenomenal performance improvements offered by Spark 2.x compared to
    the previous releases, such as Spark 1.6 and older. Tungsten implements a complete
    overhaul of memory management and other performance improvements. Most important
    memory management improvements use binary encoding of the objects and referencing
    them in both off-heap and on-heap memory. Thus, Tungsten allows the usage of office
    heap memory using the binary encoding mechanism to encode all the objects. Binary
    encoded objects take up much less memory. Project Tungsten also improves shuffle
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: The data is typically loaded into DataFrames through the `DataFrameReader`,
    and data is saved from DataFrames through `DataFrameWriter`.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame API and SQL API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The creation of a DataFrame can be done in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: By executing SQL queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading external data such as Parquet, JSON, CSV, text, Hive, JDBC, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting RDDs to data frames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A DataFrame can be created by loading a CSV file. We will look at a CSV `statesPopulation.csv`,
    which is being loaded as a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: The CSV has the following format of US states populations from years 2010 to
    2016.
  prefs: []
  type: TYPE_NORMAL
- en: '| **State** | **Year** | **Population** |'
  prefs: []
  type: TYPE_TB
- en: '| Alabama | 2010 | 4785492 |'
  prefs: []
  type: TYPE_TB
- en: '| Alaska | 2010 | 714031 |'
  prefs: []
  type: TYPE_TB
- en: '| Arizona | 2010 | 6408312 |'
  prefs: []
  type: TYPE_TB
- en: '| Arkansas | 2010 | 2921995 |'
  prefs: []
  type: TYPE_TB
- en: '| California | 2010 | 37332685 |'
  prefs: []
  type: TYPE_TB
- en: Since this CSV has a header, we can use it to quickly load into a DataFrame
    with an implicit schema detection.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the DataFrame is loaded, it can be examined for the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`option("header", "true").option("inferschema", "true").option("sep", ",")`
    tells Spark that the CSV has a `header`; a comma separator is used to separate
    the fields/columns and also that schema can be inferred implicitly.'
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame works by parsing the logical plan, analyzing the logical plan, optimizing
    the plan, and then finally executing the physical plan of execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using explain on DataFrame shows the plan of execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A DataFrame can also be registered as a table name (shown as follows), which
    will then allow you to type SQL statements like a relational Database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the DataFrame as a structured DataFrame or a table, we can run
    commands to operate on the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you see in the preceding piece of code, we have written an SQL-like statement
    and executed it using `spark.sql` API.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the Spark SQL is simply converted to the DataFrame API for execution
    and the SQL is only a DSL for ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `sort` operation on the DataFrame, you can order the rows in the DataFrame
    by any column. We see the effect of descending `sort` using the `Population` column
    as follows. The rows are ordered by the Population in a descending order.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Using `groupBy` we can group the DataFrame by any column. The following is the
    code to group the rows by `State` and then add up the `Population` counts for
    each `State`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Using the `agg` operation, you can perform many different operations on columns
    of the DataFrame, such as finding the `min`, `max`, and `avg` of a column. You
    can also perform the operation and rename the column at the same time to suit
    your use case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Naturally, the more complicated the logic gets, the execution plan also gets
    more complicated. Let''s look at the plan for the preceding operation of `groupBy`
    and `agg` API invocations to better understand what is really going on under the
    hood. The following is the code showing the execution plan of the group by and
    summation of population per `State`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: DataFrame operations can be chained together very well so that the execution
    takes advantage of the cost optimization (Tungsten performance improvements and
    catalyst optimizer working together).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also chain the operations together in a single statement, as shown as
    follows, where we not only group the data by `State` column and then sum the `Population`
    value, but also sort the DataFrame by the summation column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding chained operation consists of multiple transformations and actions,
    which can be visualized using the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s also possible to create multiple aggregations at the same time, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Pivots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pivoting is a great way of transforming the table to create a different view,
    more suitable to doing many summarizations and aggregations. This is accomplished
    by taking the values of a column and making each of the values an actual column.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this better, let's pivot the rows of the DataFrame by `Year` and
    examine the result, which shows that, now, the column `Year` created several new
    columns by converting each unique value into an actual column. The end result
    of this is that, now, instead of just looking at year columns, we can use the
    per year columns created to summarize and aggregate by `Year`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DataFrame also supports Filters, which can be used to quickly filter the DataFrame
    rows to generate new DataFrames. The Filters enable very important transformations
    of the data to narrow down the DataFrame to our use case. For example, if all
    you want is to analyze the state of California, then using `filter` API performs
    the elimination of non-matching rows on every partition of data, thus improving
    the performance of the operations.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the execution plan for the filtering of the DataFrame to only
    consider the state of California.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we can seen the execution plan, let''s now execute the `filter` command,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: User-Defined Functions (UDFs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: UDFs define new column-based functions that extend the functionality of Spark
    SQL. Often, the inbuilt functions provided in Spark do not handle the exact need
    we have. In such cases, Apache Spark supports the creation of UDFs, which can
    be used.
  prefs: []
  type: TYPE_NORMAL
- en: '`udf()` internally calls a case class User-Defined Function, which itself calls
    ScalaUDF internally.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through an example of an UDF which simply converts State column values
    to uppercase.
  prefs: []
  type: TYPE_NORMAL
- en: First, we create the function we need in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Then, we have to encapsulate the created function inside the `udf` to create
    the UDF.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have created the `udf`, we can use it to convert the State column
    to uppercase.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Schema   structure of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A schema is the description of the structure of your data and can be either
    Implicit or Explicit.
  prefs: []
  type: TYPE_NORMAL
- en: Since the DataFrames are internally based on the RDD, there are two main methods
    of converting existing RDDs into datasets. An RDD can be converted into a dataset
    by using reflection to infer the schema of the RDD. A second method for creating
    datasets is through a programmatic interface, using which you can take an existing
    RDD and provide a schema to convert the RDD into a dataset with schema.
  prefs: []
  type: TYPE_NORMAL
- en: In order to create a DataFrame from an RDD by inferring the schema using reflection,
    the Scala API for Spark provides case classes which can be used to define the
    schema of the table. The DataFrame is created programmatically from the RDD, because
    the case classes are not easy to use in all cases. For instance, creating a case
    classes on a 1000 column table is time consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Implicit schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us look at an example of loading a **CSV** (c**omma-separated Values**)
    file into a DataFrame. Whenever a text file contains a header, read API can infer
    the schema by reading the header line. We also have the option to specify the
    separator to be used to split the text file lines.
  prefs: []
  type: TYPE_NORMAL
- en: We read the `csv` inferring the schema from the header line and uses comma (`,`)
    as the separator. We also show use of `schema` command and `printSchema` command
    to verify the schema of the input file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Explicit schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A schema is described using `StructType`, which is a collection of `StructField`
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: '`StructType` and `StructField` belong to the `org.apache.spark.sql.types` package.'
  prefs: []
  type: TYPE_NORMAL
- en: DataTypes such as `IntegerType`, `StringType` also belong to the `org.apache.spark.sql.types`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: Using these imports, we can define a custom explicit schema.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the necessary classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a schema with two columns/fields-an `Integer` followed by a `String`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s easy to print the newly created `schema`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'There is also an option to print JSON, which is as follows, using `prettyJson`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'All the data types of Spark SQL are located in the package `org.apache.spark.sql.types`.
    You can access them by doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Encoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark 2.x supports a different way of defining schema for complex data types.
    First, let's look at a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Encoders must be imported using the import statement in order for you to use
    Encoders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at a simple example of defining a tuple as a data type to be used
    in the dataset APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code looks complicated to use all the time, so we can also define
    a case class for our need and then use it. We can define a case class `Record`
    with two fields-an `Integer` and a `String`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `Encoders` , we can easily create a `schema` on top of the case class,
    thus allowing us to use the various APIs with ease:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'All the data types of Spark SQL are located in the package **`org.apache.spark.sql.types`**.
    You can access them by doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You should use the `DataTypes` object in your code to create complex Spark
    SQL types such as arrays or maps, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the data types supported in Spark SQL APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data type** | **Value type in Scala** | **API to access or create a data
    type** |'
  prefs: []
  type: TYPE_TB
- en: '| `ByteType` | `Byte` | `ByteType` |'
  prefs: []
  type: TYPE_TB
- en: '| `ShortType` | `Short` | `ShortType` |'
  prefs: []
  type: TYPE_TB
- en: '| `IntegerType` | `Int` | `IntegerType` |'
  prefs: []
  type: TYPE_TB
- en: '| `LongType` | `Long` | `LongType` |'
  prefs: []
  type: TYPE_TB
- en: '| `FloatType` | `Float` | `FloatType` |'
  prefs: []
  type: TYPE_TB
- en: '| `DoubleType` | `Double` | `DoubleType` |'
  prefs: []
  type: TYPE_TB
- en: '| `DecimalType` | `java.math.BigDecimal` | `DecimalType` |'
  prefs: []
  type: TYPE_TB
- en: '| `StringType` | `String` | `StringType` |'
  prefs: []
  type: TYPE_TB
- en: '| `BinaryType` | `Array[Byte]` | `BinaryType` |'
  prefs: []
  type: TYPE_TB
- en: '| `BooleanType` | `Boolean` | `BooleanType` |'
  prefs: []
  type: TYPE_TB
- en: '| `TimestampType` | `java.sql.Timestamp` | `TimestampType` |'
  prefs: []
  type: TYPE_TB
- en: '| `DateType` | `java.sql.Date` | `DateType` |'
  prefs: []
  type: TYPE_TB
- en: '| `ArrayType` | `scala.collection.Seq` | `ArrayType(elementType, [containsNull])`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `MapType` | `scala.collection.Map` | `MapType(keyType, valueType, [valueContainsNull])`
    Note: The default value of `valueContainsNull` is `true`. |'
  prefs: []
  type: TYPE_TB
- en: '| `StructType` | `org.apache.spark.sql.Row` | `StructType(fields)` Note: fields
    is a `Seq` of `StructFields`. Also, two fields with the same name are not allowed.
    |'
  prefs: []
  type: TYPE_TB
- en: Loading and saving datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to have data read into the cluster as input and output or results written
    back to the storage to do anything practical with our code. Input data can be
    read from a variety of datasets and sources such as Files, Amazon S3 storage,
    Databases, NoSQLs, and Hive, and the output can similarly also be saved to Files,
    S3, Databases, Hive, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Several systems have support for Spark via a connector, and this number is growing
    day by day as more systems are latching onto the Spark processing framework.
  prefs: []
  type: TYPE_NORMAL
- en: Loading datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark SQL can read data from external storage systems such as files, Hive tables,
    and JDBC databases through the `DataFrameReader` interface.
  prefs: []
  type: TYPE_NORMAL
- en: The format of the API call is `spark.read.inputtype`
  prefs: []
  type: TYPE_NORMAL
- en: Parquet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive Table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JDBC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ORC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at a couple of simple examples of reading CSV files into DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Saving datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark SQL can save data to external storage systems such as files, Hive tables
    and JDBC databases through `DataFrameWriter` interface.
  prefs: []
  type: TYPE_NORMAL
- en: The format of the API call is `dataframe``.write.outputtype`
  prefs: []
  type: TYPE_NORMAL
- en: Parquet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ORC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JDBC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at a couple of examples of writing or saving a DataFrame to a CSV
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Aggregations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aggregation is the method of collecting data based on a condition and performing
    analytics on the data. Aggregation is very important to make sense of data of
    all sizes, as just having raw records of data is not that useful for most use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you look at the following table and then the aggregated view,
    it is obvious that just raw records do not help you understand the data.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a table containing one temperature measurement per day for every city
    in the world for five years.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown in the following is a table containing records of average temperature
    per day per city:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **City** | **Date** | **Temperature** |'
  prefs: []
  type: TYPE_TB
- en: '| Boston | 12/23/2016 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| New York | 12/24/2016 | 36 |'
  prefs: []
  type: TYPE_TB
- en: '| Boston | 12/24/2016 | 30 |'
  prefs: []
  type: TYPE_TB
- en: '| Philadelphia | 12/25/2016 | 34 |'
  prefs: []
  type: TYPE_TB
- en: '| Boston | 12/25/2016 | 28 |'
  prefs: []
  type: TYPE_TB
- en: 'If we want to compute the average temperature per city for all the days we
    have measurements for in the above table, we can see results which look similar
    to the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **City** | **Average Temperature** |'
  prefs: []
  type: TYPE_TB
- en: '| Boston | 30 - *(32 + 30 + 28)/3* |'
  prefs: []
  type: TYPE_TB
- en: '| New York | 36 |'
  prefs: []
  type: TYPE_TB
- en: '| Philadelphia | 34 |'
  prefs: []
  type: TYPE_TB
- en: Aggregate functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most aggregations can be done using functions that can be found in the `org.apache.spark.sql.functions`
    package. In addition, custom aggregation functions can also be created, also known
    as **User Defined Aggregation Functions** (**UDAF**).
  prefs: []
  type: TYPE_NORMAL
- en: Each grouping operation returns a `RelationalGroupeddataset`, on which you can
    specify aggregations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will load the sample data to illustrate all the different types of aggregate
    functions in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Count
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Count is the most basic aggregate function, which simply counts the number of
    rows for the column specified. An extension is the `countDistinct`, which also
    eliminates duplicates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `count` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at examples of invoking `count` and `countDistinct` on the DataFrame
    to print the row counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: First
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gets the first record in the `RelationalGroupeddataset.`
  prefs: []
  type: TYPE_NORMAL
- en: 'The `first` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at an example of invoking `first` on the DataFrame to output the
    first row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Last
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gets the last record in the `RelationalGroupeddataset`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `last` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at an example of invoking `last` on the DataFrame to output the last
    row.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: approx_count_distinct
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Approximate distinct count is much faster at approximately counting the distinct
    records rather than doing an exact count, which usually needs a lot of shuffles
    and other operations. While the approximate count is not 100% accurate, many use
    cases can perform equally well even without an exact count.
  prefs: []
  type: TYPE_NORMAL
- en: The `approx_count_distinct` API has several implementations, as follows. The
    exact API used depends on the specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at an example of invoking `approx_count_distinct` on the DataFrame
    to print the approximate count of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Min
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The minimum of the column value of one of the columns in the DataFrame. An example
    is if you want to find the minimum temperature of a city.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `min` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at an example of invoking `min` on the DataFrame to print the minimum
    Population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Max
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The maximum of the column value of one of the columns in the DataFrame. An example
    is if you want to find the maximum temperature of a city.
  prefs: []
  type: TYPE_NORMAL
- en: The `max` API has several implementations, as follows. The exact API used depends
    on the specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at an example of invoking `max` on the DataFrame to print the maximum
    Population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Average
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The average of the values is calculated by adding the values and dividing by
    the number of values.
  prefs: []
  type: TYPE_NORMAL
- en: Average of 1,2,3 is (1 + 2 + 3) / 3 = 6/3 = 2
  prefs: []
  type: TYPE_NORMAL
- en: 'The `avg` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at an example of invoking `avg` on the DataFrame to print the average
    population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Sum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computes the sum of the values of the column. Optionally, `sumDistinct` can
    be used to only add up distinct values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sum` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at an example of invoking `sum` on the DataFrame to print the summation
    (total) `Population`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Kurtosis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kurtosis is a way of quantifying differences in the shape of distributions,
    which may look very similar in terms of means and variances, yet are actually
    different. In such cases, kurtosis becomes a good measure of the weight of the
    distribution at the tail of the distribution, as compared to the middle of the
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The `kurtosis` API has several implementations, as follows. The exact API used
    depends on the specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at an example of invoking `kurtosis` on the DataFrame on the `Population`
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Skewness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Skewness measures the asymmetry of the values in your data around the average
    or mean.
  prefs: []
  type: TYPE_NORMAL
- en: The `skewness` API has several implementations, as follows. The exact API used
    depends on the specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at an example of invoking `skewness` on the DataFrame on the Population
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Variance is the average of the squared differences of each of the values from
    the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `var` API has several implementations, as follows. The exact API used depends
    on the specific use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at an example of invoking `var_pop` on the DataFrame measuring
    variance of `Population`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Standard deviation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standard deviation is the square root of the variance (see previously).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `stddev` API has several implementations, as follows. The exact API used
    depends on the specific use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at an example of invoking `stddev` on the DataFrame printing the
    standard deviation of `Population`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Covariance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Covariance is a measure of the joint variability of two random variables. If
    the greater values of one variable mainly corresponds with the greater values
    of the other variable, and the same holds for the lesser values, then the variables
    tend to show similar behavior and the covariance is positive. If the opposite
    is true, and the greater values of one variable correspond with the lesser values
    of the other variable, then the covariance is negative.
  prefs: []
  type: TYPE_NORMAL
- en: The `covar` API has several implementations, as follows. The exact API used
    depends on the specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at an example of invoking `covar_pop` on the DataFrame to calculate
    the covariance between the year and population columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: groupBy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common task seen in data analysis is to group the data into grouped categories
    and then perform calculations on the resultant groups of data.
  prefs: []
  type: TYPE_NORMAL
- en: A quick way to understand grouping is to imagine being asked to assess what
    supplies you need for your office very quickly. You could start looking around
    you and just group different types of items, such as pens, paper, staplers, and
    analyze what you have and what you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run `groupBy` function on the `DataFrame` to print aggregate counts
    of each State:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also `groupBy` and then apply any of the aggregate functions seen previously,
    such as `min`, `max`, `avg`, `stddev`, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Rollup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rollup is a multi-dimensional aggregation used to perform hierarchical or nested
    calculations. For example, if we want to show the number of records for each State+Year
    group, as well as for each State (aggregating over all years to give a grand total
    for each `State` irrespective of the `Year`), we can use `rollup` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The `rollup` calculates the count for state and year, such as California+2014,
    as well as California state (adding up all years).
  prefs: []
  type: TYPE_NORMAL
- en: Cube
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cube is a multi-dimensional aggregation used to perform hierarchical or nested
    calculations just like rollup, but with the difference that cube does the same
    operation for all dimensions. For example, if we want to show the number of records
    for each `State` and `Year` group, as well as for each `State` (aggregating over
    all Years to give a grand total for each State irrespective of the `Year`), we
    can use rollup as follows. In addition, `cube` also shows a grand total for each
    Year (irrespective of the `State`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Window functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Window functions allow you to perform aggregations over a window of data rather
    than entire data or some filtered data. The use cases of such window functions
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Cumulative sum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delta from previous value for same key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted moving average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best way to understand window functions is to imagine a sliding window
    over the larger dataset universe. You can specify a window looking at three rows
    T-1, T, and T+1, and by performing a simple calculation. You can also specify
    a window of latest/most recent ten values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The API for the window specification requires three properties, the `partitionBy()`,
    `orderBy()`, and the `rowsBetween()`. The `partitionBy` chunks the data into the
    partitions/groups as specified by `partitionBy()`. `orderBy()` is used to order
    the data within each partition of data.
  prefs: []
  type: TYPE_NORMAL
- en: The `rowsBetween()` specifies the window frame or the span of the sliding window
    to perform the calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To try out the windows function, there are certain packages that are needed.
    You can import the necessary packages using import directives, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Now, you are ready to write some code to learn about the window functions. Let's
    create a window specification for the partitions sorted by `Population` and partitioned
    by `State`. Also, specify that we want to consider all rows until the current
    row as part of the `Window`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Compute the `rank` over the window specification. The result will be a rank
    (row number) added to each row, as long as it falls within the `Window` specified.
    In this example, we chose to partition by `State` and then order the rows of each
    `State` further by descending order. Hence, all State rows have their own rank
    numbers assigned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: ntiles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ntiles is a popular aggregation over a window and is commonly used to divide
    input dataset into n parts. For example, in predictive analytics, deciles (10
    parts) are often used to first group the data and then divide it into 10 parts
    to get a fair distribution of data. This is a natural function of the window function
    approach, hence ntiles is a good example of how window functions can help.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we want to partition the `statesPopulationDF` by `State` (window
    specification was shown previously), order by population, and then divide into
    two portions, we can use `ntile` over the `windowspec`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: As shown previously, we have used `Window` function and `ntile()` together to
    divide the rows of each `State` into two equal portions.
  prefs: []
  type: TYPE_NORMAL
- en: A popular use of this function is to compute deciles used in data science Models.
  prefs: []
  type: TYPE_NORMAL
- en: Joins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional databases, joins are used to join one transaction table with
    another lookup table to generate a more complete view. For example, if you have
    a table of online transactions by customer ID and another table containing the
    customer city and customer ID, you can use join to generate reports on the transactions
    by city.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transactions table**: The following table has three columns, the **CustomerID**,
    the **Purchased item,** and how much the customer paid for the item:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **CustomerID** | **Purchased item** | **Price paid** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Headphone | 25.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Watch | 100.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Keyboard | 20.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Mouse | 10.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Cable | 10.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Headphone | 30.00 |'
  prefs: []
  type: TYPE_TB
- en: '**Customer Info table:** The following table has two columns, the **CustomerID**
    and the **City** the customer lives in:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **CustomerID** | **City** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Boston |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | New York |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Philadelphia |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Boston |'
  prefs: []
  type: TYPE_TB
- en: 'Joining the transaction table with the customer info table will generate a
    view as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **CustomerID** | **Purchased item** | **Price paid** | **City** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Headphone | 25.00 | Boston |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Watch | 100.00 | New York |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Keyboard | 20.00 | Philadelphia |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Mouse | 10.00 | Boston |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Cable | 10.00 | Boston |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Headphone | 30.00 | Philadelphia |'
  prefs: []
  type: TYPE_TB
- en: 'Now, we can use this joined view to generate a report of **Total sale price**
    by **City**:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **City** | **#Items** | **Total sale price** |'
  prefs: []
  type: TYPE_TB
- en: '| Boston | 3 | 45.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Philadelphia | 2 | 50.00 |'
  prefs: []
  type: TYPE_TB
- en: '| New York | 1 | 100.00 |'
  prefs: []
  type: TYPE_TB
- en: Joins are an important function of Spark SQL, as they enable you to bring two
    datasets together, as seen previously. Spark, of course, is not only meant to
    generate reports, but is used to process data on a petabyte scale to handle real-time
    streaming use cases, machine learning algorithms, or plain analytics. In order
    to accomplish these goals, Spark provides the API functions needed.
  prefs: []
  type: TYPE_NORMAL
- en: A typical join between two datasets takes place using one or more keys of the
    left and right datasets and then evaluates a conditional expression on the sets
    of keys as a Boolean expression. If the result of the Boolean expression returns
    true, then the join is successful, else the joined DataFrame will not contain
    the corresponding join.
  prefs: []
  type: TYPE_NORMAL
- en: 'The join API has 6 different implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use one of the APIs to understand how to use join APIs ; however, you
    can choose to use other APIs depending on the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Note that joins will be covered in detail in the next few sections.
  prefs: []
  type: TYPE_NORMAL
- en: Inner workings of join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Join works by operating on the partitions of a DataFrame using the multiple
    executors. However, the actual operations and the subsequent performance depends
    on the type of `join` and the nature of the datasets being joined. In the next
    section, we will look at the types of joins.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join between two big datasets involves shuffle join where partitions of both
    left and right datasets are spread across the executors. Shuffles are expensive
    and it''s important to analyze the logic to make sure the distribution of partitions
    and shuffles is done optimally. The following is an illustration of how shuffle
    join works internally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00166.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Broadcast join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A join between one large dataset and a smaller dataset can be done by broadcasting
    the smaller dataset to all executors where a partition from the left dataset exists.
    The following is an illustration of how a broadcast join works internally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Join types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following is a table of the different types of joins. This is important,
    as the choice made when joining two datasets makes all the difference in the output,
    and also the performance.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Join type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| **inner** | The inner join compares each row from *left* to rows from *right*
    and combines matched pair of rows from *left* and *right* datasets only when both
    have non-NULL values. |'
  prefs: []
  type: TYPE_TB
- en: '| **cross** | The cross join matches every row from *left* with every row from
    *right* generating a Cartesian cross product. |'
  prefs: []
  type: TYPE_TB
- en: '| **outer, full, fullouter** | The full outer Join gives all rows in *left*
    and *right* filling in NULL if only in *right* or *left*. |'
  prefs: []
  type: TYPE_TB
- en: '| **leftanti** | The leftanti Join gives only rows in *left* based on non-existence
    on *right* side. |'
  prefs: []
  type: TYPE_TB
- en: '| **left, leftouter** | The leftouter Join gives all rows in *left* plus common
    rows of *left* and *right* (inner join). Fills in NULL if not in *right*. |'
  prefs: []
  type: TYPE_TB
- en: '| **leftsemi** | The leftsemi Join gives only rows in *left* based on existence
    on *right* side. The does not include *right-*side values. |'
  prefs: []
  type: TYPE_TB
- en: '| **right, rightouter** | The rightouter Join gives all rows in *right* plus
    common rows of *left* and *right* (inner join). Fills in NULL if not in *left*.
    |'
  prefs: []
  type: TYPE_TB
- en: We will examine how the different join types work by using the sample datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Inner join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inner join results in rows from both `statesPopulationDF` and `statesTaxRatesDF`
    when state is non-NULL in both datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Join the two datasets by the state column as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run the `explain()` on the `joinDF` to look at the execution plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Left outer join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Left outer join results in all rows from `statesPopulationDF`, including any
    common in `statesPopulationDF` and `statesTaxRatesDF`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00273.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Join the two datasets by the state column, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Right outer join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Right outer join results in all rows from `statesTaxRatesDF`, including any
    common in `statesPopulationDF` and `statesTaxRatesDF`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00319.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Join the two datasets by the `State` column as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Outer join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outer join results in all rows from `statesPopulationDF` and `statesTaxRatesDF`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00245.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Join the two datasets by the `State` column as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Left anti join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Left anti join results in rows from only `statesPopulationDF` if, and only if,
    there is NO corresponding row in `statesTaxRatesDF`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Join the two datasets by the `State` column as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Left semi join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Left semi join results in rows from only `statesPopulationDF` if, and only if,
    there is a corresponding row in `statesTaxRatesDF`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Join the two datasets by the state column as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Cross join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross join matches every row from *left* with every row from *right,* generating
    a Cartesian cross product.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00312.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Join the two datasets by the `State` column as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: You can also use join with cross jointype instead of calling the cross join
    API. `statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State").isNotNull,
    "cross").count`.
  prefs: []
  type: TYPE_NORMAL
- en: Performance implications of join
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The join type chosen directly impacts the performance of the join. This is because
    joins require the shuffling of data between executors to execute the tasks, hence
    different joins, and even the order of the joins, need to be considered when using
    join.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a table you could use to refer to when writing `Join` code:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Join type** | **Performance considerations and tips** |'
  prefs: []
  type: TYPE_TB
- en: '| **inner** | Inner join requires the left and right tables to have the same
    column. If you have duplicate or multiple copies of the keys on either the left
    or right side, the join will quickly blow up into a sort of a Cartesian join,
    taking a lot longer to complete than if designed correctly to minimize the multiple
    keys. |'
  prefs: []
  type: TYPE_TB
- en: '| **cross** | Cross Join matches every row from *left* with every row from
    *right,* generating a Cartesian cross product. This is to be used with caution,
    as this is the worst performant join, to be used in specific use cases only. |'
  prefs: []
  type: TYPE_TB
- en: '| **outer, full, fullouter** | Fullouter Join gives all rows in *left* and
    *right* filling in NULL if only in *right* or *left*. If used on tables with little
    in common, can result in very large results and thus slow performance. |'
  prefs: []
  type: TYPE_TB
- en: '| **leftanti** | Leftanti Join gives only rows in *left* based on non-existence
    on *right* side. Very good performance, as only one table is fully considered
    and the other is only checked for the join condition. |'
  prefs: []
  type: TYPE_TB
- en: '| **left, leftouter** | Leftouter Join gives all rows in *left* plus common
    rows of *left* and *right* (inner join). Fills in NULL if not in *right*. If used
    on tables with little in common, can result in very large results and thus slow
    performance. |'
  prefs: []
  type: TYPE_TB
- en: '| **leftsemi** | Leftsemi Join gives only rows in *left* based on existence
    on *right* side. Does not include *right* side values. Very good performance,
    as only one table is fully considered and other is only checked for the join condition.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **right, rightouter** | Rightouter Join gives all rows in *right* plus common
    rows of *left* and *right* (inner join). Fills in NULL if not in *left*. Performance
    is similar to the leftouter join mentioned previously in this table. |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the origin of DataFrames and how Spark SQL provides
    the SQL interface on top of DataFrames. The power of DataFrames is such that execution
    times have decreased manyfold over original RDD-based computations. Having such
    a powerful layer with a simple SQL-like interface makes them all the more powerful.
    We also looked at various APIs to create, and manipulate DataFrames, as well as
    digging deeper into the sophisticated features of aggregations, including `groupBy`,
    `Window`, `rollup`, and `cubes`. Finally, we also looked at the concept of joining
    datasets and the various types of joins possible, such as inner, outer, cross,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the exciting world of real-time data processing
    and analytics in the [Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c),
    *Stream Me Up, Scotty - Spark Streaming*.
  prefs: []
  type: TYPE_NORMAL
