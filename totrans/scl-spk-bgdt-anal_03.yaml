- en: Functional Programming Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Object-oriented programming makes code understandable by encapsulating moving
    parts. Functional programming makes code understandable by minimizing moving parts."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Michael Feathers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Scala and Spark is a very good combination for learning big data analytics.
    However, along with the OOP paradigm, we also need to know-how why functional
    concepts are important for writing Spark applications that eventually analyze
    your data. As mentioned in the previous chapters, Scala supports two programming
    paradigms: the Object-Oriented Programming paradigm and the Functional programming
    concepts. In [Chapter 2](part0058.html#1NA0K1-21aec46d8593429cacea59dbdcd64e1c),
    *Object-Oriented Scala*, we explored the OOP paradigm in which we have seen how
    to represent real-world objects in blueprints (classes) and then instantiate them
    into objects having real memory representation.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on the second paradigm (i.e. functional programming).
    We will see what functional programming is and how Scala supports it, why it matters,
    and the related advantages of using this concept. More specifically, we will learn
    several topics, such as why Scala is an arsenal for the data scientist, why it
    is important to learn the Spark paradigm, pure functions, and **higher-order functions**
    (**HOFs**). A real-life use case using HOF will also be shown in this chapter.
    Then, we will see how to handle exceptions in the higher-order functions outside
    collections using the standard library of Scala. Finally, we will learn how functional
    Scala affects an object's mutability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to functional programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional Scala for the data scientists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why functional programming and Scala are important for learning Spark?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pure functions and higher-order functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using higher-order functions: A real-life use case'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error handling in functional Scala
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional programming and data mutability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to functional programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In computer science, `functional programming` (FP) is a programming paradigm
    and a unique style of building the structure and elements of computer programs.
    This uniqueness helps treat the computation as the evaluation of mathematical
    functions and avoids changing-state and mutable data. Thus, by using the FP concept,
    you can learn to code in your own style that ensures the immutability of your
    data. In other words, FP is about writing pure functions, about removing hidden
    inputs and outputs as far as we can, so that as much of our code as possible *just*
    describes a relationship between inputs and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: This is not a new concept but the `Lambda Calculus`, which provides the basis
    of FP, was first introduced in the 1930s. However, in the realm of programming
    language, the term functional programming refers to a new style of declarative
    programming paradigm that means programming can be done with the help of control,
    declarations, or expressions instead of classical statements commonly used in
    an old programming language, such as C.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of functional programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some exciting and cool features in FP paradigms such as `composition`,
    `pipelining`, and `higher order functions` that help to avoid writing unfunctional
    code. Alternatively, at least later on, this helps translate a unfunctional program
    into a functional style towards an imperative one. Finally, now let's see how
    we can define the term functional programming from the computer science perspective.
    Functional programming is a common computer science concept in which computations
    and the building structure of the program are treated as if you are evaluating
    mathematical functions that support immutable data and avoid state change. In
    functional programming, each function has the same mapping or output for the same
    input argument values.
  prefs: []
  type: TYPE_NORMAL
- en: With the need for a complex software comes the need for good structured programs
    and software that are not difficult to write and are debuggable. We also need
    to write extendable code that will save us programming costs in the future and
    can contribute to easy writing and debugging of the code; even more modular software
    that is easy to extend and requires less programming efforts. Due to the latter
    contribution of functional programming, modularity, functional programming is
    considered as a great advantage for software development.
  prefs: []
  type: TYPE_NORMAL
- en: 'In functional programming, there is a basic building block in its structure
    called functions without side effects (or at least very few) in most of your code.
    Without side effects, the order of evaluation really doesn''t matter. When it
    comes to programming languages views, there are methods to force a particular
    order. In some FP languages (for example, eager languages such as Scheme), which
    have no evaluation order on arguments, you could nest these expressions in their
    own lambda forms as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In functional programming, writing mathematical functions in which the execution
    order doesn't matter usually makes your code more readable. Sometimes, one will
    argue that we need functions with side effects to be there as well. Actually,
    this is one of the major disadvantages of most functional programming languages
    since it's typically difficult to write functions that don't require any I/O;
    on the other hand, these function that requires I/O are difficult to implement
    in functional programming. From *Figure 1*, it can be seen that Scala is also
    a hybrid language that evolved by taking features from imperative languages such
    as Java and functional language such as Lisp.
  prefs: []
  type: TYPE_NORMAL
- en: But fortunately, here we are dealing with a mixed language in which object-oriented
    and functional programming paradigms are allowed and hence writing such functions
    that require I/O is quite easy. Functional programming also has major advantages
    over basic programming, such as comprehensions and caching.
  prefs: []
  type: TYPE_NORMAL
- en: One of the major advantages of functional programming is brevity because with
    functional programming you can write more compact and concise code. Also, concurrency
    is considered one of the major advantages, which is done more easily in functional
    programming. Therefore, functional languages such as Scala provide many other
    features and tools that encourage coders to make an entire paradigm shift to a
    more mathematical way of thinking.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00062.jpeg)**Figure 1:** Shows a conceptual view of using functional
    programming concepts'
  prefs: []
  type: TYPE_NORMAL
- en: 'By narrowing the focus to only a small number of composable abstract concepts,
    such as functions, function composition, and abstract algebra, FP concept provides
    several advantages over other paradigms. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Closer alignment to mathematical thinking:** You tend to spell out your ideas
    in a format close to mathematical definitions rather than iterative programs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No (or at least fewer) side effects:** Your functions do not influence other
    functions, which is great for concurrency and parallelization, and also for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fewer lines of code without sacrificing conceptual clarity:** Lisp is more
    powerful than non-functional languages. Although it''s true that you need to spend
    a greater proportion of your project thinking than writing, you will probably
    find that you are more productive eventually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these exciting features, functional programming achieves significant expressive
    power. For example, machine learning algorithms can take hundreds of lines of
    imperative code to implement yet they can be defined in just a handful of equations.
  prefs: []
  type: TYPE_NORMAL
- en: Functional Scala for the data scientists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For performing interactive data cleaning, processing, munging, and analysis,
    many data scientists use R or Python as their favorite tool. However, there are
    many data scientists who tend to get very attached to their favorite tool--that
    is, Python or R and try to solve all data analytics problems or jobs using that
    tool. Thus, introducing them to a new tool can be very challenging in most circumstances
    as the new tool has more syntax and a new set of patterns to learn before using
    the new tool to solve their purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other APIs in Spark written in Python and R such as PySpark and SparkR
    respectively that allow you to use them from Python or R. However, most Spark
    books and online examples are written in Scala. Arguably, we think that learning
    how to work with Spark using the same language on which the Spark code has been
    written will give you many advantages over Java, Python, or R as a data scientist:'
  prefs: []
  type: TYPE_NORMAL
- en: Better performance and removes the data processing overhead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides access to the latest and greatest features of Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helps to understand the Spark philosophy in a transparent way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing data means that you are writing Scala code to retrieve data from the
    cluster using Spark and its APIs (that is, SparkR, SparkSQL, Spark Streaming,
    Spark MLlib, and Spark GraphX). Alternatively, you're developing a Spark application
    using Scala to manipulate that data locally on your own machine. In both cases,
    Scala is your real friend and will pay you dividends in time.
  prefs: []
  type: TYPE_NORMAL
- en: Why FP and Scala for learning Spark?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss why we will learn Spark to solve our data analytics
    problem. We will then discuss why the functional programming concepts in Scala
    are particularly important to make data analysis easier for the data scientists.
    We will also discuss the Spark programming model and its ecosystem to make them
    clearer.
  prefs: []
  type: TYPE_NORMAL
- en: Why Spark?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark is a lightning fast cluster computing framework and is mainly designed
    for fast computations. Spark is based on the Hadoop MapReduce model and uses MapReduce
    in more forms and types of computation, such as interactive queries and stream
    processing. One of the main features of Spark is in-memory processing, which helps
    increase the performance and processing speed of an application. Spark supports
    a wide range of applications and workloads, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch-based applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterative algorithms that were not possible to run fast before
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactive query and streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, it doesn''t require much time for you to learn Spark and implement it
    in your applications without the need to understand the inner details of concurrency
    and distributed systems. Spark was implemented in 2009 at AMPLab of UC Berkeley.
    In 2010, they decided to make it open source. Then, Spark became an Apache release
    in 2013 and since then Spark has been considered as the most famous/used Apache-released
    software. Apache Spark became very famous because of its features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fast computations**: Spark helps you to run applications that are faster
    than Hadoop because of its golden feature--in-memory processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support for multiple programming languages**: Apache Spark provides wrappers
    and built-in APIs in different languages such as Scala, Java, Python, or even
    R.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More analytics**: As mentioned earlier, Spark supports MapReduce operations
    and it also supports more advanced analytics such as **machine learning** (**MLlib**),
    data streaming, and algorithms for graph processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As mentioned earlier, Spark is built on top of the Hadoop software and you
    can deploy Spark in different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standalone cluster**: This means that Spark will run on top of **Hadoop Distributed
    File System** (**HDFS**) and space will actually be allocated to HDFS. Spark and
    MapReduce will run side by side to serve all the Spark jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop YARN cluster**: This means that Spark simply runs on YARN without
    any root privileges or pre-installations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mesos cluster**: When a driver program creates a Spark job and starts assigning
    related tasks for scheduling, Mesos determines which computing nodes will handle
    which tasks. We assume that you have already configured and installed Mesos on
    your machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deploy on pay-as-you-go cluster**: You can deploy Spark jobs in real cluster
    mode on AWS EC2\. To make your applications run on Spark cluster mode and for
    better scalability, you can consider **Amazon Elastic Compute Cloud** (**EC2**)
    services as **Infrastructure as a Service** (**IaaS**) or **Platform as a Service**
    (**PaaS**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to [Chapter 17](part0511.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c),
    *Time to Go to ClusterLand - Deploying Spark on a Cluster* and [Chapter 18](part0550.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c),
    *Testing and Debugging Spark* for how to deploy your data analytics application
    using Scala and Spark on a real cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Scala and the Spark programming model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark programming starts with a dataset or a few, usually residing in some
    form of distributed and persistent storage such as HDFS. A typical RDD programming
    model that Spark provides can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: From an environment variable, Spark context (the Spark shell provides you with
    a Spark Context or you can make your own, this will be described later in this
    chapter) creates an initial data reference RDD object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform the initial RDD to create more RDD objects following the functional
    programming style (to be discussed later on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Send the code, algorithms, or applications from the driver program to the cluster
    manager nodes. Then, the cluster manager provides a copy to each computing node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing nodes hold a reference to the RDDs in their partition (again, the
    driver program also holds a data reference). However, computing nodes could have
    the input dataset provided by the cluster manager as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After a transformation (via either narrow or wider transformation), the result
    to be generated is a brand new RDD, since the original one will not be mutated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the RDD object or more (specifically, data reference) is materialized
    through an action to dump the RDD into the storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The driver program can ask the computing nodes for a chunk of results for the
    analysis or visualization of a program.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wait! So far we have moved smoothly. We suppose you will ship your application
    code to the computing nodes in the cluster. Still, you will have to upload or
    send the input datasets to the cluster to be distributed among the computing nodes.
    Even during the bulk upload, you will have to transfer the data across the network.
    We also argue that the size of the application code and results are negligible
    or trivial. Another obstacle is if you want Spark to process the data at scale
    computation, it might require data objects to be merged from multiple partitions
    first. This means we will need to shuffle data among the worker/computing nodes
    that is usually done by `partition()`, `intersection()`, and `join()` transformation
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Scala and the Spark ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To provide more enhancement and additional big data processing capabilities,
    Spark can be configured and run on top of existing Hadoop-based clusters. The
    core APIs in Spark, on the other hand, are written in Java, Scala, Python, and
    R. Compared to MapReduce, with the more general and powerful programming model,
    Spark also provides several libraries that are part of the Spark ecosystems for
    additional capabilities for general-purpose data processing and analytics, graph
    processing, large-scale structured SQL, and **Machine Learning** (**ML**) areas.
  prefs: []
  type: TYPE_NORMAL
- en: "The Spark ecosystem consists of the following components as shown (for details\
    \ please refer [Chapter 16\uFEFF](part0480.html#E9OE01-21aec46d8593429cacea59dbdcd64e1c),\
    \ *Spark Tuning*):"
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Spark core**: This is the underlying engine for the Spark platform
    on which all the other functionalities are built. Also, it''s the one that provides
    in-memory processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark SQL**: As mentioned Spark core is the underlying engine and all the
    other components or features are built upon it. Spark SQL is the Spark component
    that provides support for different data structures (structured and semi-structured
    data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark streaming**: This component is responsible for streaming data for analytics
    and converts them into mini batches that can be used later on for analytics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLlib (Machine Learning Library)**: MLlib is a machine learning framework
    that supports lots of ML algorithms in a distributed fashion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphX**: A distributed graph framework built on top of Spark to express
    user-defined graph components in a parallel fashion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned earlier, most functional programming languages allow the user to
    write nice, modular, and extensible code. Also, functional programming encourages
    safe ways of programming by writing functions that look like mathematical functions.
    Now, how did Spark make all the APIs work as a single unit? It was possible because
    of the advancement in the hardware and of course, the functional programming concepts.
    Since adding syntactic sugar to easily do lambda expressions is not sufficient
    to make a language functional, this is just the start.
  prefs: []
  type: TYPE_NORMAL
- en: Although the RDD concept in Spark works quite well, there are many use cases
    where it's a bit complicated due to its immutability. For the following example
    which is the classic example of calculating an average, make the source code robust
    and readable; of course, to reduce the overall cost, one does not want to first
    compute totals, then counts, even if the data is cached in the main memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The DataFrames API (this will be discussed in the later chapters in detail)
    produces equally terse and readable code where the functional API fits well for
    most use cases and minimizes the MapReduce stages; there are many shuffles that
    can cost dramatically and the key reasons for this are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Large code bases require static typing to eliminate trivial mistakes, such as
    *aeg* instead of *age* instantly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex code requires transparent APIs to communicate design clearly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2x speed-ups in the DataFrames API via under-the-hood mutation can be equally
    achieved by encapsulating state via OOP and using mapPartitions and combineByKey
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexibility and Scala features are required to build functionality quickly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The combination of OOP and FP with Spark can make a pretty hard problem easier
    in Barclays. For example, in Barclays, recently an application called Insights
    Engine has been developed to execute an arbitrary number N of near-arbitrary SQL-like
    queries. The application can execute them in a way that can scale with increasing
    N.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's talk about pure functions, higher order functions, and anonymous functions,
    which are the three important concepts in the functional programming of Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Pure functions and higher-order functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the computer science perspective, functions can have many forms such as
    first order functions, higher-order functions, or pure functions. This is also
    true from the mathematics point of view. Using a higher-order function is a function
    one of the following can be performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Takes one or more functions as arguments to do some operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns a function as its result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All other functions except the higher-order functions are first-order functions.
    However, from the mathematics point of view, higher-order functions are also called
    **operators** or **functionals**. On the other hand, if the return value of a
    function is only determined by its input and of course without observable side
    effects, it is called a **pure function**.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will briefly discuss why and how to use different functional
    paradigms in Scala. Especially, pure functions, and higher-order functions will
    be discussed. At the end of this section, a brief overview of using anonymous
    functions will also be provided since this is used frequently while developing
    a Spark application using Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Pure functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important principles of functional programming is pure functions.
    So what are pure functions and why do we care about them? In this section, we
    will address this important feature of functional programming. One of the best
    practices of functional programming is to implement your programs such that the
    core of your program/application is made from pure functions and all the I/O functions
    or side effects such as network overhead and exceptions are in an exposed external
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: So what are the benefits of pure functions? Pure functions are normally smaller
    than normal functions (although it depends on other factors such as programming
    language) and even easier to interpret and understand for the human brain because
    it looks like a mathematical function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet, you might argue against this since most developers still find imperative
    programming more understandable! Pure functions are much easier to implement and
    test. Let''s demonstrate this by an example. Suppose we have the following two
    separate functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'So in the previous two examples, if you want to test the `pureFunc` pure function,
    we just assert the return value that''s coming from the pure function with what
    we are expecting based on our input such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'But on the other side, if we wanted to test our `notpureFunc` impure function
    then we need to redirect the standard output and then apply assertion on it. The
    next practical tip is that functional programming makes programmers more productive
    because, as mentioned earlier, pure functions are smaller and easier to write
    and you can easily compose them together. Also, the duplication of code is minimal
    and you can easily reuse your code. Now let''s demonstrate this advantage with
    a better example. Consider these two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'However, there might be side effects of mutability; using a pure function (that
    is, without mutability) helps us reason about and test code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This one is advantageous and very easy to interpret and use. However, let''s
    see another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, consider how confusing this could be: what will be the output in a multithreaded
    environment? As you can see, we can easily use our pure function, `pureMul`, to
    multiply any sequence of numbers, unlike our `notpureMul` impure function. Let''s
    demonstrate this by the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code for the preceding examples can be shown as follows (methods
    were called using some real values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As discussed earlier, you can consider pure functions as one of the most important
    features of functional programming and as a best practice; you need to build the
    core of your application using pure functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Functions versus methods:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the programming realm, a **function** is a piece of code called by a name.
    Data (as an argument or as a parameter) can be passed to operate on and can return
    data (optionally). All data passed to a function is passed explicitly. A **method,**
    on the other hand, is also a piece of code that is called by a name too. However,
    a method is always associated with an object. Sounds similar? Well! In most cases,
    a method is identical to a function except for two key differences:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. A method is implicitly passed the object on which it was called.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. A method is able to operate on data that is contained within the class.
  prefs: []
  type: TYPE_NORMAL
- en: It is already stated in the previous chapter that an object is an instance of
    a class--the class is the definition, the object is an instance of that data.
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to learn about higher-order functions. However, before that, we
    should learn one more important concept in functional Scala--**anonymous functions**.
    Through this, we will also learn how to use the lambda expression with functional
    Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Anonymous functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes in your code, you don''t want to define a function prior to its usage,
    maybe because you will use it in one place. In functional programming, there''s
    a type of function that is very suitable to this situation. It''s called an anonymous
    function. Let''s demonstrate the use of anonymous functions using the previous
    example of transferring money:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s call the `TransferMoney()` method with some real value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Lambda expression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As already stated, Scala supports first-class functions, which means functions
    can be expressed in function-literal syntax as well; functions can be represented
    by objects, called function values. Try the following expression, it creates a
    successor function for integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scala> var apply = (x:Int) => x+1`'
  prefs: []
  type: TYPE_NORMAL
- en: '`apply: Int => Int = <function1>`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The apply variable is now a function that can be used in the usual way as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scala> var x = apply(7)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`x: Int = 8`'
  prefs: []
  type: TYPE_NORMAL
- en: 'What we have done here is simply use the core of a function: the argument list
    followed by the function arrow and the body of the function. This one is not black
    magic but a full-fledged function, only without a given name--that is, anonymous.
    If you define a function this way, there will be no way to refer to that function
    afterward and hence you couldn''t call that function afterward because without
    a name it''s an anonymous one. Also, we have a so-called **lambda expression**!
    It''s just the pure, anonymous definition of a function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'So, in the previous example instead of declaring a separate `callback` function,
    we passed an anonymous function directly and it did the same job just like the
    `bankFee` function. You can also omit the type in the anonymous function and it
    will be directly inferred based on the passed argument like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s demonstrate the previous example on the Scala shell as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)**Figure 6:** Use of the anonymous function in Scala'
  prefs: []
  type: TYPE_NORMAL
- en: Some programming languages that have functional support use the name lambda
    function instead of anonymous function.
  prefs: []
  type: TYPE_NORMAL
- en: Higher-order functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Scala's functional programming, you are allowed to pass functions as parameters
    and even return a function as a result from another function; this defines what
    are called higher-order functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s demonstrate this feature by an example. Consider the following function
    `testHOF` that takes another function `func` and then applies this function to
    its second argument value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: After demonstrating the basics of Scala's functional programming, now we are
    ready to move to more complex cases of functional programming. As mentioned earlier,
    we can define a higher-order function as a function that accepts other functions
    as arguments and it returns them as a result. If you are coming from an object-oriented
    programming background, you will find it very a different approach, but it will
    become easier to understand as we go on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining a simple function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous function is a very simple one. It''s a function that accepts an
    Int value and then returns a quarter of this value in a `Double` type. Let''s
    define another simple function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The second function `addTwo` is more trivial than the first one. It accepts
    an `Int` value and then adds 2 to it. As you can see, these two functions have
    something in common. Both of them accept `Int` and return another processed value
    that we can call `AnyVal`. Now, let''s define a higher-order function that accepts
    another function among its parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding function `applyFuncOnRange` accepts two `Int`
    values that work as a beginning and end to a sequence and it accepts a function
    that has the `Int => AnyVal` signature just like the previously defined simple
    functions (`quarterMakder` and `addTwo`). Now let's demonstrate our previous higher-order
    function by passing one of the two simple functions to it as a third argument
    (if you want to pass your own function then make sure that it has the same signature
    `Int => AnyVal`).
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala syntax for loop with ranges:** The simplest syntax of using a for loop
    with ranges in Scala is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`for( var x <- range ){`'
  prefs: []
  type: TYPE_NORMAL
- en: '`statement(s)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the `range` could be a range of numbers and is represented as `i` to
    `j` or sometimes like `i` until `j`. The left-arrow `←` operator is called a generator
    because it''s generating individual values from a range. Let''s see a concrete
    example of this feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '`object UsingRangeWithForLoop {`'
  prefs: []
  type: TYPE_NORMAL
- en: '`def main(args: Array[String]):Unit= {`'
  prefs: []
  type: TYPE_NORMAL
- en: '`var i = 0;`'
  prefs: []
  type: TYPE_NORMAL
- en: '`// for loop execution with a range`'
  prefs: []
  type: TYPE_NORMAL
- en: '`for( i <- 1 to 10){`'
  prefs: []
  type: TYPE_NORMAL
- en: '`println( "Value of i: " + i )`'
  prefs: []
  type: TYPE_NORMAL
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Value of i: 1`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Value of i: 2`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Value of i: 3`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Value of i: 4`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Value of i: 5`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Value of i: 6`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Value of i: 7`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Value of i: 8`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Value of i: 9`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Value of i: 10`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first define our functions before starting to use them as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00070.jpeg)**Figure 2:** An example of defining a higher-order function
    in Scala'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s start by calling our higher-order function `applyFuncOnRange` and
    passing the `quarterMaker` function as a third argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00074.jpeg)**Figure 3:** Calling a higher-order function'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can even apply the other function `addTwo` since it has the same signature
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00079.jpeg)**Figure 4:** An alternative way of calling a higher-order
    function'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going into more examples, let''s define what''s called a callback function.
    A callback function is a function that can be passed as an argument to another
    function. Other functions are simply normal functions. Let''s demonstrate more
    examples of using different callback functions. Consider the following higher-order
    function, which is responsible for transferring a specific amount of money from
    your account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After calling the `TransferMoney` function on 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: From a functional programming point of view, this code is not ready to be integrated
    into the banking system because you need to apply different validations on the
    money parameters, such as it has to be positive and greater than the specific
    amount specified by the bank. However, here we are just demonstrating the use
    of high-order functions and callback functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, this example works as follows: you want to transfer a specific amount of
    money to another bank account or money agent. The bank has a specific fee to be
    applied depending on the amount that you are transferring and here comes the role
    of the callback function. It takes the amount of money to transfer and applies
    the bank fee to it in order to come up with the total amount.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `TransferMoney` function takes two parameters: the first one is the money
    to be transferred and the second one is a callback function with the signature
    `Double => Double` that the function applies to the money argument to determine
    the bank fee over the transferred money.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.jpeg)**Figure 5:** Calling and giving extra power to the higher-order
    function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete source code of the preceding examples can be seen as follows (we
    called the methods using some real values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: By using callback functions, you are giving extra power to the higher-order
    function; so, it's a very powerful mechanism to make your program more elegant,
    flexible, and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Function as a return value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned, higher-order functions also support returning a function as a
    result. Let''s demonstrate this by an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the previous example as shown in the following screenshot; it shows
    how to use the function as a return value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.jpeg)**Figure 7:** Function as a return value'
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code of the preceding example can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now before stopping our discussion on HFO, let's see a real-life example, that
    is, currying using HFO.
  prefs: []
  type: TYPE_NORMAL
- en: Using higher-order functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you work in a restaurant as a chef and one of your colleagues ask you
    a question: Implement a **HOF** (**higher-order function**) that performs currying.
    Looking for clues? Suppose you have the following two signatures for your HOF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, implement a function that performs uncurrying as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, how could you use HOFs to perform the currying operation? Well, you could
    create a trait that encapsulates the signatures of two HOFs (that is, curry and
    uncurry) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can implement and extend this trait as an object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here I have implemented the uncurry first since it's easier. The two curly braces
    after the equals sign are an anonymous function literal for taking two arguments
    (that is, `a` and `b` of types `X` and `Y` respectively). Then, these two arguments
    can be used in a function that also returns a function. Then, it passes the second
    argument to the returned function. Finally, it returns the value of the second
    function. The second function literal takes one argument and returns a new function,
    that is, `curry()`. Eventually, it returns a function when called returns another
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it comes: how to use the preceding object that extends the base trait in
    a real-life implementation. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding object and inside the main method:'
  prefs: []
  type: TYPE_NORMAL
- en: The `addSpicy` holds a function that takes a long as a type and adds 1 to it
    and then prints 4.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `increment` holds a function which takes a long as a type and adds 2 to
    it and finally prints 3.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `unspicedAdd` holds a function which adds 1 and takes a long as type. Finally,
    it prints 7.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In mathematics and computer science, currying is the technique of translating
    the evaluation of a function that takes multiple arguments (or a tuple of arguments)
    into evaluating a sequence of functions, each with a a single argument. Currying
    is related to, but not the same as, partial application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Currying:** Currying is useful in both practical and theoretical settings.
    In functional programming languages, and many others, it provides a way of automatically
    managing how arguments are passed to functions and exceptions. In theoretical
    computer science, it provides a way to study functions with multiple arguments
    in simpler theoretical models, which provide only one argument.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Uncurrying:** Uncurrying is the dual transformation to currying, and can
    be seen as a form of defunctionalization. It takes a function `f` whose return
    value is another function `g` and yields a new function `f′` that takes as parameters
    the arguments for both `f` and `g`, and returns, as a result, the application
    of `f` and subsequently, `g`, to those arguments. The process can be iterated.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen how to deal with pure, higher-order, and anonymous functions
    in Scala. Now, let's have a brief overview on how to extend the higher-order function
    using `Throw`, `Try`, `Either`, and `Future` in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Error handling in functional Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we focused on ensuring that the body of a Scala function does what it's
    supposed to and doesn't do anything else (that is, an error or exception). Now,
    in order to make use of any programming and to avoid producing error-prone code
    then you need to know how to catch exceptions and handle errors in this language.
    We will see how to extend higher-order functions outside collections using some
    special features of Scala such as `Try`, `Either`, and `Future`.
  prefs: []
  type: TYPE_NORMAL
- en: Failure and exceptions in Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At first, let''s define what we mean by failures in general (source: [https://tersesystems.com/2012/12/27/error-handling-in-scala/](https://tersesystems.com/2012/12/27/error-handling-in-scala/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unexpected internal failure**: The operation fails as the result of an unfulfilled
    expectation, such as a null pointer reference, violated assertions, or simply
    bad state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expected internal failure**: The operation fails deliberately as a result
    of internal state, that is, a blacklist or circuit breaker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expected external failure**: The operation fails because it is told to process
    some raw input, and will fail if the raw input cannot be processed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unexpected external failure**: The operation fails because a resource that
    the system depends on is not there: there''s a loose file handle, the database
    connection fails, or the network is down'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unfortunately, there are no concrete ways of stopping failures unless the failures
    are due to some manageable exceptions. On the other hand, Scala makes *checked
    versus unchecked* very simple: it doesn''t have checked exceptions. All exceptions
    are unchecked in Scala, even `SQLException` and `IOException`, and so on. Now
    let''s see how to handle such exceptions at least.'
  prefs: []
  type: TYPE_NORMAL
- en: Throwing exceptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Scala method can throw an exception because of the unexpected workflow. You
    create an exception object and then you throw it with the throw keyword as follows.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note that the primary goal of using exception handling is not to produce friendly
    messages but to exit the normal flow of your Scala program.
  prefs: []
  type: TYPE_NORMAL
- en: Catching exception using try and catch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scala allows you to try/catch any exception in a single block and then perform
    pattern matching against it using case blocks. The basic syntax of using `try...catch`
    in Scala is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, if you throw an exception, then you need to use the `try...catch` block
    in order to handle it nicely without crashing with an internal exception message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If there''s no file named `data.txt`, in the path/data under your project tree,
    you will experience `FileNotFoundException` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's have a brief example of using the `finally` clause in Scala to make
    the `try...catch` block complete.
  prefs: []
  type: TYPE_NORMAL
- en: Finally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you want to execute your code regardless of an exception being thrown
    or not, then you should use the `finally` clause. You can place it inside the
    `try block` as follows. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, here''s the complete example of using `try...catch...finally`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will discuss another powerful feature in Scala called `Either`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Either
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Either[X, Y]` is an instance that contains either an instance of `X` or an
    instance of `Y` but not both. We call these subtypes left and right of Either.
    Creating an Either is trivial. But it''s very powerful sometimes to use it in
    your program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we pass any arbitrary URL that doesn''t contain `xxx` then we will
    get a `Scala.io.Source` wrapped in a `Right` subtype. If the URL contains `xxx`,
    then we will get a `String` wrapped in a `Left` subtype. To make the preceding
    statement clearer, let''s see the output of the preceding code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will explore another interesting feature of Scala called `Future` that
    is used to execute tasks in a non-blocking way. This is also a better way to handle
    the results when they finish.
  prefs: []
  type: TYPE_NORMAL
- en: Future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you simply want to run tasks in a non-blocking way and need a way to handle
    the results when they finish, Scala provides you with Futures, for example, if
    you want to make multiple web service calls in a parallel fashion and work with
    the results after the web service handles all these calls. An example of using
    Future is provided in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Run one task, but block
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following example demonstrates how to create a Future and then block the
    sequence of execution in order to wait for its result. Creating Futures is trivial.
    You just need to pass it to the code that you want. The following example performs
    2+2 in the future and then returns the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Await.result` method waits up to 2 seconds till the `Future` returns the
    result; if it doesn''t return the result within 2 seconds, it throws the following
    exception you might want to handle or catch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: It's time to wrap up this chapter. However, I would like to take the chance
    to discuss an important view of mine about functional programming with Scala and
    object mutability.
  prefs: []
  type: TYPE_NORMAL
- en: Functional programming and data mutability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pure functional programming is one of the best practices in functional programming
    and you should stick to it. Writing pure functions will make your programming
    life easier and you will be able to write code that's easy to maintain and extend.
    Also, if you want to parallelize your code then it will be easier to do so if
    you write pure functions.
  prefs: []
  type: TYPE_NORMAL
- en: If you're an FP purist, one drawback of using functional programming in Scala
    is that Scala supports both OOP and FP (see *Figure 1*), and therefore it's possible
    to mix the two coding styles in the same code base. In this chapter, we have seen
    several examples showing that writing pure functions is easy. However, combining
    them into a complete application is difficult. You might agree that advanced topics
    such as monads make FP intimidating.
  prefs: []
  type: TYPE_NORMAL
- en: I talked to many people and they think that the recursion doesn't feel reasonably
    natural. When you use immutable objects, you can never mutate them with something
    else. There aren't times when you are allowed to do that. That's the whole point
    of immutable objects! Sometimes what I have experienced is that a pure function
    and data input or output really mixes up. However, when you need to mutate, you
    can create a copy of the object containing your mutated field. Thus, theoretically,
    there's no need to *mix up*. Lastly, using only immutable values and recursion
    can potentially lead to performance problems in terms of CPU usage and RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored some functional programming concepts in Scala.
    We have seen what functional programming is and how Scala supports it, why it
    matters, and the advantages of using functional concepts. We have seen why learning
    FP concepts is important in learning the Spark paradigm. Pure functions, anonymous
    functions, and higher-order functions were discussed with suitable examples. Later
    in this chapter, we saw how to handle exceptions in the higher-order functions
    outside collections using the standard library of Scala. Finally, we discussed
    how functional Scala affects object mutability.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will provide an in-depth analysis on the Collections
    API, one of the most prominent features of the standard library.
  prefs: []
  type: TYPE_NORMAL
