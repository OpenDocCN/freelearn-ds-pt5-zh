<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Analytics Study: NLP and Big Data with Twitter Sentiment Analysis</h1></div></div></div><div><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"> </td><td valign="top"><p><em>"Data is the new oil."</em></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td colspan="2" align="right" valign="top" style="text-align: center">--<em>Unknown</em></td></tr></table></div><p>In this chapter we are going to look at two important fields of AI and data science: <strong>natural language processing</strong> (<strong>NLP</strong>) and big data analysis. For the supporting sample application, we re-implement the <em>Sentiment analysis of Twitter hashtags</em> project described in <a class="link" href="ch01.xhtml" title="Chapter 1. Programming and Data Science – A New Toolset">Chapter 1</a>, <em>Programming and Data Science – A New Toolset</em>, but this time we leverage Jupyter Notebooks and PixieDust to build live dashboards that analyze data from a stream of tweets related <a id="id414" class="indexterm"/>to a particular entity, such as a product offered by a company, for example, to provide sentiment information as well as information about other trending entities extracted from the same tweets. At the end of this chapter, the reader will learn how to integrate cloud-based NLP services such as <em>IBM Watson Natural Language Understanding</em> into their application as well as perform data analysis at (Twitter) scale with frameworks such as Apache Spark.</p><p>As always, we'll show how to operationalize the analytics by implementing a live dashboard as a PixieApp that runs directly in the Jupyter Notebook.</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec48"/>Getting started with Apache Spark</h1></div></div></div><p>The term <em>big data</em> can rightly feel vague and imprecise. What is the cut-off for considering any <a id="id415" class="indexterm"/>dataset big data? Is it 10 GB, 100 GB, 1 TB or more? One definition that I like is: big data is when the data cannot fit into the memory available in a single machine. For years, data scientists have been forced to sample large datasets, so they could fit into a single machine, but that started to change as parallel computing frameworks that are able to distribute the data into a cluster of machines made it possible to work with the dataset in its entirety, provided of course that the cluster had enough machines. At the same time, advances in cloud technologies made it possible to provision on demand a cluster of machines that are adapted to the size of the dataset.</p><p>Today, there <a id="id416" class="indexterm"/>are multiple frameworks (most of the time available as open source) that can provide robust, flexible parallel computing capabilities. Some of the <a id="id417" class="indexterm"/>most popular include Apache Hadoop (<a class="ulink" href="http://hadoop.apache.org">http://hadoop.apache.org</a>), Apache Spark (<a class="ulink" href="https://spark.apache.org">https://spark.apache.org</a>) and Dask (<a class="ulink" href="https://dask.pydata.org">https://dask.pydata.org</a>). For our <em>Twitter Sentiment Analysis</em> application, we'll use Apache <a id="id418" class="indexterm"/>Spark, which provides excellent performances in the area of scalability, programmability, and speed. In addition, many cloud providers <a id="id419" class="indexterm"/>offer some flavor of Spark as a Service giving the ability to create on demand an appropriately sized Spark cluster in minutes.</p><p>Some Spark as a Service cloud providers include:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Microsoft Azure: <a class="ulink" href="https://azure.microsoft.com/en-us/services/hdinsight/apache-spark">https://azure.microsoft.com/en-us/services/hdinsight/apache-spark</a></li><li class="listitem" style="list-style-type: disc">Amazon Web Services: <a class="ulink" href="https://aws.amazon.com/emr/details/spark">https://aws.amazon.com/emr/details/spark</a></li><li class="listitem" style="list-style-type: disc">Google Cloud: <a class="ulink" href="https://cloud.google.com/dataproc">https://cloud.google.com/dataproc</a></li><li class="listitem" style="list-style-type: disc">Databricks: <a class="ulink" href="https://databricks.com">https://databricks.com</a></li><li class="listitem" style="list-style-type: disc">IBM Cloud: <a class="ulink" href="https://www.ibm.com/cloud/analytics-engine">https://www.ibm.com/cloud/analytics-engine</a></li></ul></div><div><div><h3 class="title"><a id="note181"/>Note</h3><p>
<strong>Note</strong>: Apache Spark can also be easily installed on a local machine for testing purposes, in which case, the cluster nodes are simulated using threads.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec33"/>Apache Spark architecture</h2></div></div></div><p>The following <a id="id420" class="indexterm"/>diagram shows the main components of the Apache Spark framework:</p><div><img src="img/B09699_07_01.jpg" alt="Apache Spark architecture" width="1000" height="654"/><div><p>Spark high level architecture</p></div></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Spark SQL</strong>: The core <a id="id421" class="indexterm"/>data structure of this component is the Spark DataFrame, which enables users who know the SQL language, to effortlessly work with structured data.</li><li class="listitem" style="list-style-type: disc"><strong>Spark Streaming</strong>: Module <a id="id422" class="indexterm"/>used to work with streaming data. As we'll see later on, we'll use this module and more specifically Structured Streaming (which was introduced in Spark 2.0) in our sample application.</li><li class="listitem" style="list-style-type: disc"><strong>MLlib</strong>: Module <a id="id423" class="indexterm"/>that provides a feature-rich machine learning library that works on a Spark scale.</li><li class="listitem" style="list-style-type: disc"><strong>GraphX</strong>: Module <a id="id424" class="indexterm"/>used for performing the graph-parallel computation.</li></ul></div><p>There are <a id="id425" class="indexterm"/>mainly two ways of working with a Spark cluster as illustrated in the following diagram:</p><div><img src="img/B09699_07_02.jpg" alt="Apache Spark architecture" width="1000" height="713"/><div><p>Two ways to work with a Spark cluster</p></div></div><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>spark-submit</strong>: Shell script <a id="id426" class="indexterm"/>used to launch Spark applications on a cluster</li><li class="listitem" style="list-style-type: disc"><strong>Notebooks</strong>: Interactively <a id="id427" class="indexterm"/>execute code statements against a Spark cluster</li></ul></div><p>Covering the <code class="literal">spark-submit</code> shell script is beyond the scope of this book, but official documentation can be found at: <a class="ulink" href="https://spark.apache.org/docs/latest/submitting-applications.html">https://spark.apache.org/docs/latest/submitting-applications.html</a>. For the rest of this chapter, we'll focus on interacting with the Spark cluster via Jupyter Notebooks.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec34"/>Configuring Notebooks to work with Spark</h2></div></div></div><p>The instructions <a id="id428" class="indexterm"/>in this section only cover installing Spark locally for development and testing. Manually installing Spark in a cluster is beyond the scope of this book. If a real cluster is needed, it is highly recommended to use a cloud-based service.</p><p>By default, local Jupyter Notebooks are installed with plain Python Kernels. To work with Spark, users must use the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Install Spark locally by downloading a binary distribution from <a class="ulink" href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a>.</li><li class="listitem">Generate a kernel specification in a temporary directory using the following command:<div><pre class="programlisting">
<strong>ipython kernel install --prefix /tmp</strong>
</pre></div><div><div><h3 class="title"><a id="note182"/>Note</h3><p>
<strong>Note</strong>: The preceding command may generate a warning message that can be safely ignored as long as the following message is stated:</p><p>
<code class="literal">Installed kernelspec python3 in /tmp/share/jupyter/kernels/python3</code>
</p></div></div></li><li class="listitem">Go to <code class="literal">/tmp/share/jupyter/kernels/python3</code>, and edit the <code class="literal">kernel.json</code> file to add the following key to the JSON object (replace <code class="literal">&lt;&lt;spark_root_path&gt;&gt;</code> with the directory path where you installed Spark and <code class="literal">&lt;&lt;py4j_version&gt;&gt;</code> with the version installed on your system):<div><pre class="programlisting">"env": {
    "PYTHONPATH": "&lt;&lt;spark_root_path&gt;&gt;/python/:&lt;&lt;spark_root_path&gt;&gt;/python/lib/py4j-&lt;&lt;py4j_version&gt;&gt;-src.zip",
    "SPARK_HOME": "&lt;&lt;spark_root_path&gt;&gt;",
    "PYSPARK_SUBMIT_ARGS": "--master local[10] pyspark-shell",
    "SPARK_DRIVER_MEMORY": "10G",
    "SPARK_LOCAL_IP": "127.0.0.1",
    "PYTHONSTARTUP": "&lt;&lt;spark_root_path&gt;&gt;/python/pyspark/shell.py"
}</pre></div></li><li class="listitem">You may <a id="id429" class="indexterm"/>also want to customize the <code class="literal">display_name</code> key to make it unique and easily recognizable from the Juptyer UI. If you need to know the list of existing kernels, you can use the following command:<div><pre class="programlisting">
<strong>jupyter kernelspec list</strong>
</pre></div><p>The preceding command will give you a list of kernel names and associated paths on the local filesystem. From the path, you can open the <code class="literal">kernel.json</code> file to access the <code class="literal">display_name</code> value. For example:</p><div><pre class="programlisting">
<strong>    Available kernels:</strong>
<strong>      pixiedustspark16</strong>
<strong>    /Users/dtaieb/Library/Jupyter/kernels/pixiedustspark16</strong>
<strong>      pixiedustspark21</strong>
<strong>    /Users/dtaieb/Library/Jupyter/kernels/pixiedustspark21</strong>
<strong>      pixiedustspark22</strong>
<strong>    /Users/dtaieb/Library/Jupyter/kernels/pixiedustspark22</strong>
<strong>      pixiedustspark23</strong>
<strong>    /Users/dtaieb/Library/Jupyter/kernels/pixiedustspark23</strong>
</pre></div></li><li class="listitem">Install the <a id="id430" class="indexterm"/>kernel with the edited files using the following command:<div><pre class="programlisting">
<strong>jupyter kernelspec install /tmp/share/jupyter/kernels/python3</strong>
</pre></div><div><div><h3 class="title"><a id="note183"/>Note</h3><p>Note: Depending on the environment, you may receive a "permission denied" error when running the preceding command. In this case, you may want to run the command with the admin privileges using <code class="literal">sudo</code> or use the <code class="literal">--user</code> switch as follows:</p><p>
<code class="literal">jupyter kernelspec install --user /tmp/share/jupyter/kernels/python3</code>
</p></div></div><p>For more information about install ation options, you can use the <code class="literal">-h</code> switch. For example:</p><div><pre class="programlisting">
<strong>    jupyter kernelspec install -h</strong>
</pre></div></li><li class="listitem">Restart the Notebook server and start using the new PySpark kernel.</li></ol></div><p>Fortunately, PixieDust provides an <code class="literal">install</code> script to automate the preceding manual steps.</p><div><div><h3 class="title"><a id="note184"/>Note</h3><p>You can find detailed documentation for this script here:</p><p>
<a class="ulink" href="https://pixiedust.github.io/pixiedust/install.html">https://pixiedust.github.io/pixiedust/install.html</a>
</p></div></div><p>In short, using the automated PixieDust <code class="literal">install</code> script requires the following command to be issued and the on-screen instructions to be followed:</p><div><pre class="programlisting">
<strong>jupyter pixiedust install</strong>
</pre></div><p>We'll dive deeper into the Spark programming model later in this chapter, but for now, let's define in the next section, the MVP requirements for our <em>Twitter Sentiment Analysis</em> application.</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec49"/>Twitter sentiment analysis application</h1></div></div></div><p>As always, we <a id="id431" class="indexterm"/>start by defining the requirements for our MVP version:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Connect to Twitter to get a stream of real-time tweets filtered by a query string provided by the user</li><li class="listitem" style="list-style-type: disc">Enrich the tweets to add sentiment information and relevant entities extracted from the text</li><li class="listitem" style="list-style-type: disc">Display a dashboard with various statistics about the data using live charts that are updated at specified intervals</li><li class="listitem" style="list-style-type: disc">The system should be able to scale up to Twitter data size</li></ul></div><p>The following <a id="id432" class="indexterm"/>diagram shows the first version of our application architecture:</p><div><img src="img/B09699_07_03.jpg" alt="Twitter sentiment analysis application" width="1000" height="736"/><div><p>Twitter sentiment architecture version 1</p></div></div><p>For version 1, the application will be entirely implemented in a single Python Notebook and will call out to an external service for the NLP part. To be able to scale, we will certainly have to externalize some of the processing outside of the Notebook, but for development and testing, I found that being able to contain the whole application in a single Notebook significantly increases productivity.</p><p>As for libraries and frameworks, we'll use Tweepy (<a class="ulink" href="http://www.tweepy.org">http://www.tweepy.org</a>) for connecting <a id="id433" class="indexterm"/>to Twitter, Apache Spark Structured Streaming (<a class="ulink" href="https://spark.apache.org/streaming">https://spark.apache.org/streaming</a>) for processing the streaming data in a distributed <a id="id434" class="indexterm"/>cluster and the Watson Developer Cloud Python SDK (<a class="ulink" href="https://github.com/watson-developer-cloud/python-sdk">https://github.com/watson-developer-cloud/python-sdk</a>) to access the <a id="id435" class="indexterm"/>IBM Watson Natural <a id="id436" class="indexterm"/>Language Understanding (<a class="ulink" href="https://www.ibm.com/watson/services/natural-language-understanding">https://www.ibm.com/watson/services/natural-language-understanding</a>) service.</p></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec50"/>Part 1 – Acquiring the data with Spark Structured Streaming</h1></div></div></div><p>To acquire <a id="id437" class="indexterm"/>the data, we use Tweepy which provides an elegant Python client library to access the Twitter APIs. The APIs covered by Tweepy are very extensive and covering them in detail is beyond the scope of this book, but you can find the complete API reference at the Tweepy official website: <a class="ulink" href="http://tweepy.readthedocs.io/en/v3.6.0/cursor_tutorial.html">http://tweepy.readthedocs.io/en/v3.6.0/cursor_tutorial.html</a>.</p><p>You can install the Tweepy library directly from PyPi using the <code class="literal">pip install</code> command. The following command shows how to install it from a Notebook using the <code class="literal">!</code> directive:</p><div><pre class="programlisting">
<strong>!pip install tweepy</strong>
</pre></div><div><div><h3 class="title"><a id="note185"/>Note</h3><p>
<strong>Note</strong>: The current Tweepy version used is 3.6.0. Do not forget to restart the kernel after installing the library.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec35"/>Architecture diagram for the data pipeline</h2></div></div></div><p>Before we <a id="id438" class="indexterm"/>start diving into each component of the data pipeline, it would be good to take a look at its overall architecture and understand the computation flow.</p><p>As shown in the following diagram, we start by creating a Tweepy stream that writes raw data in CSV files. We then create a Spark Streaming DataFrame that reads the CSV files and is periodically updated with new data. From the Spark Streaming DataFrame, we create a Spark structured query using SQL and store its results in a Parquet database:</p><div><img src="img/B09699_07_04.jpg" alt="Architecture diagram for the data pipeline" width="1000" height="361"/><div><p>Streaming computation flow</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec36"/>Authentication with Twitter</h2></div></div></div><p>Before using <a id="id439" class="indexterm"/>any of the Twitter APIs, it is recommended to authenticate with the system. One of the most commonly <a id="id440" class="indexterm"/>used authentication mechanism is the OAuth 2.0 protocol (<a class="ulink" href="https://oauth.net">https://oauth.net</a>) which enables third-party applications to access a service on the web. The first thing you need to do is acquire a set of key strings that are used by the OAuth protocol to authenticate you:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Consumer key</strong>: String that <a id="id441" class="indexterm"/>uniquely identifies the client app (a.k.a. the API Key).</li><li class="listitem" style="list-style-type: disc"><strong>Consumer secret</strong>: Secret <a id="id442" class="indexterm"/>string known only to the application and the Twitter OAuth server. It can be thought of like a password.</li><li class="listitem" style="list-style-type: disc"><strong>Access token</strong>: String used <a id="id443" class="indexterm"/>to authenticate your requests. This token is also used during the authorization phase to determine the level of access for the application.</li><li class="listitem" style="list-style-type: disc"><strong>Access token secret</strong>: Similar to <a id="id444" class="indexterm"/>the consumer secret, this is a secret string sent with the access token to be used as a password.</li></ul></div><p>To generate the preceding key strings, you need to go to <a class="ulink" href="http://apps.twitter.com">http://apps.twitter.com</a>, provide authentication with your regular Twitter user ID and password and follow these steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Create a new Twitter app using the <strong>Create New App</strong> button.</li><li class="listitem">Fill out the application details, agree to the Developer agreement and click on <strong>Create your Twitter application</strong> button.<div><div><h3 class="title"><a id="tip08"/>Tip</h3><p>
<strong>Note</strong>: Make sure that your mobile phone number is added to your profile or you'll get an error when creating the Twitter application.</p><p>You can provide a random URL for the mandatory <strong>Website</strong> input and leave the <strong>URL</strong> input blank as this is an optional callback URL.</p></div></div></li><li class="listitem">Click on the <strong>Keys and Access Tokens</strong> tab to get the consumer and access token. At any time, you can regenerate these tokens using the buttons available on this page. If you do so, you'll need to also update the value in your application code.</li></ol></div><p>For easier <a id="id445" class="indexterm"/>code maintenance, let's put these tokens in their own variables at the top of the Notebook and create the <code class="literal">tweepy.OAuthHandler</code> class that we'll use later on:</p><div><pre class="programlisting">from tweepy import OAuthHandler
# Go to http://apps.twitter.com and create an app.
# The consumer key and secret will be generated for you after
consumer_key="XXXX"
consumer_secret="XXXX"

# After the step above, you will be redirected to your app's page.
# Create an access token under the "Your access token" section
access_token="XXXX"
access_token_secret="XXXX"

<strong>auth = OAuthHandler(consumer_key, consumer_secret)</strong>
<strong>auth.set_access_token(access_token, access_token_secret)</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec37"/>Creating the Twitter stream</h2></div></div></div><p>For implementing <a id="id446" class="indexterm"/>our application, we only need to use the Twitter streaming API that is documented here: <a class="ulink" href="http://tweepy.readthedocs.io/en/v3.5.0/streaming_how_to.html">http://tweepy.readthedocs.io/en/v3.5.0/streaming_how_to.html</a>. In this step, we create a Twitter stream that stores the incoming data into CSV files on the local filesystem. This is done using a custom <code class="literal">RawTweetsListener</code> class that inherits from <code class="literal">tweepy.streaming.StreamListener</code>. Custom processing of the incoming data is done by overriding the <code class="literal">on_data</code> method.</p><p>In our case, we want to transform the incoming data from JSON to CSV using <code class="literal">DictWriter</code> from the standard Python <code class="literal">csv</code> module. Because the Spark Streaming file input source triggers only when new files are created in the input directory, we can't simply append the data into an existing file. Instead, we buffer the data into an array and write it to disk once the buffer has reached capacity.</p><div><div><h3 class="title"><a id="note186"/>Note</h3><p>For simplicity, the implementation doesn't include cleaning up the files once they have been processed. Another minor limitation of this implementation is that we currently wait until the buffer is filled to write the file which theoretically could take a long time if no new tweets appear.</p></div></div><p>The <a id="id447" class="indexterm"/>code for the <code class="literal">RawTweetsListener</code> is shown here:</p><div><pre class="programlisting">from six import iteritems
import json
import csv
from tweepy.streaming import StreamListener
class RawTweetsListener(StreamListener):
    def __init__(self):
        self.buffered_data = []
        self.counter = 0

    def flush_buffer_if_needed(self):
        "Check the buffer capacity and write to a new file if needed"
        length = len(self.buffered_data)
        if length &gt; 0 and length % 10 == 0:
            with open(os.path.join( <strong>output_dir</strong>,
                "tweets{}.csv".format(self.counter)), "w") as fs:
                self.counter += 1
                csv_writer = csv.DictWriter( fs,
                    fieldnames = fieldnames)
                <strong>for data in self.buffered_data:</strong>
<strong>                    csv_writer.writerow(data)</strong>
            self.buffered_data = []

    def on_data(self, data):
        def transform(key, value):
            return transforms[key](value) if key in transforms else value

        self.buffered_data.append(
            {key:transform(key,value) \
                 for key,value in iteritems(json.loads(data)) \
                 if key in fieldnames}
        )
        self.flush_buffer_if_needed()
        return True

    def on_error(self, status):
        print("An error occured while receiving streaming data: {}".format(status))
        return False</pre></div><div><div><h3 class="title"><a id="note187"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode1.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode1.py</a>
</p></div></div><p>A few <a id="id448" class="indexterm"/>important things to notice from the preceding code are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Each tweet coming from the Twitter API contains a lot of data, and we pick which field to keep using the <code class="literal">field_metadata</code> variable. We also define a global variable <code class="literal">fieldnames</code> that holds the list of fields to capture from the stream, and a <code class="literal">transforms</code> variable that contains a dictionary with all the field names that have a transform function as a key and the transform function itself as a value:<div><pre class="programlisting">from pyspark.sql.types import StringType, DateType
from bs4 import BeautifulSoup as BS
fieldnames = [f["name"] for f in field_metadata]
transforms = {
    item['name']:item['transform'] for item in field_metadata if "transform" in item
}
field_metadata = [
    {"name": "created_at","type": DateType()},
    {"name": "text", "type": StringType()},
    {"name": "source", "type": StringType(),
         "transform": lambda s: BS(s, "html.parser").text.strip()
    }
]</pre></div><div><div><h3 class="title"><a id="note188"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode2.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode2.py</a>
</p></div></div></li><li class="listitem" style="list-style-type: disc">The CSV files are written in <code class="literal">output_dir</code> which is defined in its own variable. At start time, we first remove the directory and its contents:<div><pre class="programlisting">import shutil
def ensure_dir(dir, delete_tree = False):
    if not os.path.exists(dir):
        os.makedirs(dir)
    elif delete_tree:
        shutil.rmtree(dir)
        os.makedirs(dir)
    return os.path.abspath(dir)

root_dir = ensure_dir("output", delete_tree = True)
<strong>output_dir = ensure_dir(os.path.join(root_dir, "raw"))</strong>
</pre></div><div><div><h3 class="title"><a id="note189"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode3.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode3.py</a>
</p></div></div></li><li class="listitem" style="list-style-type: disc">The <code class="literal">field_metadata</code> contains the Spark DataType that we'll use later on to build the schema when creating the Spark streaming query.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">field_metadata</code> also contains an optional transform <code class="literal">lambda</code> function to cleanse the value before being written to disk. For reference, a lambda function in Python is an anonymous function defined inline (see <a class="ulink" href="https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions">https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions</a>). We use it for the source field that is often returned as an HTML fragment. In this lambda function, we use the BeautifulSoup library (which was also used in the previous chapter) to extract only the text as shown in the following snippet:<div><pre class="programlisting">lambda s: BS(s, "html.parser").text.strip()</pre></div></li></ul></div><p>Now that the <code class="literal">RawTweetsListener</code> is created, we define a <code class="literal">start_stream</code> function that we'll use <a id="id449" class="indexterm"/>later on in the PixieApp. This function takes an array of search terms as input and starts a new stream using the <code class="literal">filter</code> method:</p><div><pre class="programlisting">from tweepy import Stream
def start_stream(queries):
    "Asynchronously start a new Twitter stream"
    stream = <strong>Stream(auth, RawTweetsListener())</strong>
<strong>    stream.filter(track=queries, async=True)</strong>
    return stream</pre></div><div><div><h3 class="title"><a id="note190"/>Note</h3><p>Notice the <code class="literal">async=True</code> parameter passed to <code class="literal">stream.filter</code>. This is needed to make sure that the function doesn't block, which would prevent us from running any other code in the Notebook.</p><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode4.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode4.py</a>
</p></div></div><p>The following code starts the stream that will receive tweets containing the word <code class="literal">baseball</code> in it:</p><div><pre class="programlisting">stream = start_stream(["baseball"])</pre></div><p>When running <a id="id450" class="indexterm"/>the preceding code, no output is generated in the Notebook. However, you can see the files (that is, <code class="literal">tweets0.csv</code>, <code class="literal">tweets1.csv</code>, and so on.) being generated in the output directory (that is, <code class="literal">../output/raw</code>) from the path where the Notebook is being run.</p><p>To stop the stream, we simply call the <code class="literal">disconnect</code> method, as shown here:</p><div><pre class="programlisting">stream.disconnect()</pre></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec38"/>Creating a Spark Streaming DataFrame</h2></div></div></div><p>Referring to <a id="id451" class="indexterm"/>the architecture diagram, the next step is to create a Spark Streaming DataFrame <code class="literal">tweets_sdf</code> that uses the <code class="literal">output_dir</code> as the source file input. We can think of a Streaming DataFrame as an unbounded table where new rows are continuously added as new data arrives from the stream.</p><div><div><h3 class="title"><a id="note191"/>Note</h3><p>
<strong>Note</strong>: Spark Structured Streaming supports multiple types of input source including File, Kafka, Socket, and Rate. (Both Socket and Rate are used only for testing.)</p></div></div><p>The following diagram is taken from the Spark website and does a great job explaining how new data is appended to the Streaming DataFrame:</p><div><img src="img/B09699_07_05.jpg" alt="Creating a Spark Streaming DataFrame" width="1000" height="511"/><div><p>Streaming DataFrame flow</p><p>Source: <a class="ulink" href="https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png">https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png</a></p></div></div><p>The Spark Streaming Python API provides an elegant way to create the Streaming DataFrame using the <code class="literal">spark.readStream</code> property which creates a new <code class="literal">pyspark.sql.streamingreamReader</code> object that conveniently lets you chain method calls with the added benefit of creating clearer code (see <a class="ulink" href="https://en.wikipedia.org/wiki/Method_chaining">https://en.wikipedia.org/wiki/Method_chaining</a> for more details on this pattern).</p><p>For example, to create a CSV file stream, we call the format method with <code class="literal">csv</code>, chain the applicable <a id="id452" class="indexterm"/>options and call the <code class="literal">load</code> method with the path of the directory:</p><div><pre class="programlisting">schema = StructType(
[StructField(f["name"], f["type"], True) for f in field_metadata]
)
csv_sdf = spark.readStream\
    .format("csv")\
<strong>    .option("schema", schema)\</strong>
    .option("multiline", True)\
<strong>    .option("dateFormat", 'EEE MMM dd kk:mm:ss Z y')\</strong>
    .option("ignoreTrailingWhiteSpace", True)\
    .option("ignoreLeadingWhiteSpace", True)\
    .load(output_dir)</pre></div><div><div><h3 class="title"><a id="note192"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode5.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode5.py</a>
</p></div></div><p>
<code class="literal">spark.readStream</code> also provides a convenient high-level <code class="literal">csv</code> method that takes the path as the first argument and keyword arguments for the options:</p><div><pre class="programlisting">csv_sdf = spark.readStream \
    .csv(
        output_dir,
        schema=schema,
        multiLine = True,
        dateFormat = 'EEE MMM dd kk:mm:ss Z y',
        ignoreTrailingWhiteSpace = True,
        ignoreLeadingWhiteSpace = True
    )</pre></div><div><div><h3 class="title"><a id="note193"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode6.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode6.py</a>
</p></div></div><p>You can verify that the <code class="literal">csv_sdf</code> DataFrame is indeed a Streaming DataFrame by calling the <code class="literal">isStreaming</code> method which should return <code class="literal">true</code>. The following code also adds a call to <code class="literal">printSchema</code> to verify that the schema follows the <code class="literal">field_metadata</code> configuration as expected:</p><div><pre class="programlisting">print(csv_sdf.isStreaming)
csv_sdf.printSchema()</pre></div><p>Returns:</p><div><pre class="programlisting">root
 |-- created_at: date (nullable = true)
 |-- text: string (nullable = true)
 |-- source: string (nullable = true)</pre></div><p>Before <a id="id453" class="indexterm"/>continuing to the next step, it is important to understand how the <code class="literal">csv_sdf</code> Streaming DataFrame fits in the Structured Streaming programming model and what limitations it has. At its core, the Spark low-level APIs define the <strong>Resilient Distributed Dataset</strong> (<strong>RDD</strong>) data structure <a id="id454" class="indexterm"/>which encapsulates all the underlying complexity of managing the distributed data. Features like fault-tolerance (cluster nodes that crashes for any reason are transparently restarted with no intervention from the developer) are automatically handled by the framework. There are two types of RDD operations: transformations and actions. <strong>Transformations</strong> are logical operations on an existing RDD that are not immediately executed on the cluster until an action is invoked (lazy execution). The output of a transformation is a new RDD. Internally, Spark maintains an RDD acyclic directed graph that keeps track of all the lineage resulting in the creation of the RDD, which is useful when recovering from server failure. Example transformations include <code class="literal">map</code>, <code class="literal">flatMap</code>, <code class="literal">filter</code>, <code class="literal">sample</code>, and <code class="literal">distinct</code>. The same goes for transformations on DataFrames (which internally are backed by RDDs) that have the benefit of including SQL queries. On the other hand, <strong>actions</strong> do not produce other RDDs, but rather perform an operation on the actual distributed data to return a non-RDD value. Examples of actions include <code class="literal">reduce</code>, <code class="literal">collect</code>, <code class="literal">count</code>, and <code class="literal">take</code>.</p><p>As mentioned before, <code class="literal">csv_sdf</code> is a Streaming DataFrame, which means that the data is continuously added to it and as such we are only able to apply transformations to it, not actions. To circumvent this problem, we must first create a streaming query using <code class="literal">csv_sdf.writeStream</code> which is a <code class="literal">pyspark.sql.streaming.DataStreamWriter</code> object. The streaming query is responsible for sending the results to an output sink. We can then run the streaming query using the <code class="literal">start()</code> method.</p><p>Spark Streaming supports multiple output sink types:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>File</strong>: All the classic file formats are supported, including JSON, CSV, and Parquet</li><li class="listitem" style="list-style-type: disc"><strong>Kafka</strong>: Write directly to one or more Kafka topics</li><li class="listitem" style="list-style-type: disc"><strong>Foreach</strong>: Run arbitrary computations on each element in the collection</li><li class="listitem" style="list-style-type: disc"><strong>Console</strong>: Prints the output to the system console (used mainly for debugging)</li><li class="listitem" style="list-style-type: disc"><strong>Memory</strong>: Output is stored in memory</li></ul></div><p>In the <a id="id455" class="indexterm"/>next section, we'll create and run a structured query on <code class="literal">csv_sdf</code> with an output sink that stores the output in Parquet format.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec39"/>Creating and running a structured query</h2></div></div></div><p>Using the <code class="literal">tweets_sdf</code> Streaming <a id="id456" class="indexterm"/>DataFrame, we create a streaming query <code class="literal">tweet_streaming_query</code> that writes the data into a Parquet format using the <em>append</em> output mode.</p><div><div><h3 class="title"><a id="note194"/>Note</h3><p>
<strong>Note</strong>: Spark streaming queries support three output modes: <strong>complete</strong> where the entire table is written at each trigger, <strong>append</strong> where only the delta rows since the last trigger are written, and <strong>update</strong> where only the rows that were modified are written.</p></div></div><p>Parquet is a <a id="id457" class="indexterm"/>columnar database format that provides an efficient, scalable storage for distributed analytics. You can find <a id="id458" class="indexterm"/>more information about the Parquet format at: <a class="ulink" href="https://parquet.apache.org">https://parquet.apache.org</a>.</p><p>The following code creates and starts the <code class="literal">tweet_streaming_query</code> streaming query:</p><div><pre class="programlisting">tweet_streaming_query = csv_sdf \
    .writeStream \
    .format("parquet") \
<strong>    .option("path", os.path.join(root_dir, "output_parquet")) \</strong>
<strong>    .trigger(processingTime="2 seconds") \</strong>
<strong>    .option("checkpointLocation", os.path.join(root_dir, "output_chkpt")) \</strong>
    .start()</pre></div><div><div><h3 class="title"><a id="note195"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode7.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode7.py</a>
</p></div></div><p>Similarly, you can stop the streaming query by using the <code class="literal">stop()</code> method as follows:</p><div><pre class="programlisting">tweet_streaming_query.stop()</pre></div><p>In the preceding code, we use the <code class="literal">path</code> option to specify the location of the Parquet files, and the <code class="literal">checkpointLocation</code> to specify the location of the recovery data that would be used in case of a server failure. We also specify the trigger interval for new data to be read from <a id="id459" class="indexterm"/>the stream and new rows to be added to the Parquet database.</p><p>For testing <a id="id460" class="indexterm"/>purpose, you can also use the <code class="literal">console</code> sink to see the new rows being read every time a new raw CSV file is generated in the <code class="literal">output_dir</code> directory:</p><div><pre class="programlisting">tweet_streaming_query = csv_sdf.writeStream\
    .outputMode("append")\
    .format("console")\
    .trigger(processingTime='2 seconds')\
    .start()</pre></div><div><div><h3 class="title"><a id="note196"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode8.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode8.py</a>
</p></div></div><p>You can see the results in the system output of the master node of your Spark cluster (you will need to physically access the master node machine and look at the log files, since, unfortunately, the output is not printed into the Notebook itself because the operation is executed in a different process. Location of the log files depends on the cluster management software; please refer to the specific documentation for more information).</p><p>Here are sample results displayed for a particular batch (identifiers have been masked):</p><div><pre class="programlisting">-------------------------------------------
Batch: 17
-------------------------------------------
+----------+--------------------+-------------------+
|created_at|                text|             source|
+----------+--------------------+-------------------+
|2018-04-12|RT @XXXXXXXXXXXXX...|Twitter for Android|
|2018-04-12|RT @XXXXXXX: Base...| Twitter for iPhone|
|2018-04-12|That's my roommat...| Twitter for iPhone|
|2018-04-12|He's come a long ...| Twitter for iPhone|
|2018-04-12|RT @XXXXXXXX: U s...| Twitter for iPhone|
|2018-04-12|Baseball: Enid 10...|   PushScoreUpdates|
|2018-04-12|Cubs and Sox aren...| Twitter for iPhone|
|2018-04-12|RT @XXXXXXXXXX: T...|          RoundTeam|
|2018-04-12|@XXXXXXXX that ri...| Twitter for iPhone|
|2018-04-12|RT @XXXXXXXXXX: S...| Twitter for iPhone|
+----------+--------------------+-------------------+</pre></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec40"/>Monitoring active streaming queries</h2></div></div></div><p>When a <a id="id461" class="indexterm"/>streaming query is started, cluster resources are allocated by Spark. Therefore, it is important to manage and monitor these queries to make sure that you don't run out of cluster resources. At any time, you can get a list of all the running queries as shown in the following code:</p><div><pre class="programlisting">print(spark.streams.active)</pre></div><p>Results:</p><div><pre class="programlisting">[&lt;pyspark.sql.streaming.StreamingQuery object at 0x12d7db6a0&gt;, &lt;pyspark.sql.streaming.StreamingQuery object at 0x12d269c18&gt;]</pre></div><p>You can then dive into the details of each query by using the following query monitoring properties:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">id</code>: Returns a unique identifier for the query that persists across restarts from checkpoint data</li><li class="listitem" style="list-style-type: disc"><code class="literal">runId</code>: Returns a unique ID generated for the current session</li><li class="listitem" style="list-style-type: disc"><code class="literal">explain()</code>: Prints detailed explanations of the query</li><li class="listitem" style="list-style-type: disc"><code class="literal">recentProgress</code>: Returns an array of the most recent progress updates</li><li class="listitem" style="list-style-type: disc"><code class="literal">lastProgress:</code> Returns the most recent progress</li></ul></div><p>The following code prints the most recent progress for each active query:</p><div><pre class="programlisting">import json
for query in spark.streams.active:
    print("-----------")
    print("id: {}".format(query.id))
    print(json.dumps(query.lastProgress, indent=2, sort_keys=True))</pre></div><div><div><h3 class="title"><a id="note197"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode9.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode9.py</a>
</p></div></div><p>Results for the first query are shown here:</p><div><pre class="programlisting">-----------
id: b621e268-f21d-4eef-b6cd-cb0bc66e53c4
{
  "batchId": 18,
  "durationMs": {
    "getOffset": 4,
    "triggerExecution": 4
  },
  "id": "b621e268-f21d-4eef-b6cd-cb0bc66e53c4",
  "inputRowsPerSecond": 0.0,
  "name": null,
  "numInputRows": 0,
  "processedRowsPerSecond": 0.0,
  "runId": "d2459446-bfad-4648-ae3b-b30c1f21be04",
  "sink": {
    "description": "org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@586d2ad5"
  },
  "sources": [
    {
      "description": "FileStreamSource[file:/Users/dtaieb/cdsdev/notebookdev/Pixiedust/book/Chapter7/output/raw]",
      "endOffset": {
        "logOffset": 17
      },
      "inputRowsPerSecond": 0.0,
      "numInputRows": 0,
      "processedRowsPerSecond": 0.0,
      "startOffset": {
        "logOffset": 17
      }
    }
  ],
  "stateOperators": [],
  "timestamp": "2018-04-12T21:40:10.004Z"
}</pre></div><p>As an <a id="id462" class="indexterm"/>exercise for the reader, it would be useful to build a PixieApp that provides a live dashboard with updated details about each active streaming query.</p><div><div><h3 class="title"><a id="note198"/>Note</h3><p>
<strong>Note</strong>: We'll show how to build this PixieApp in <em>Part 3 – Create a real-time dashboard PixieApp</em>.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec41"/>Creating a batch DataFrame from the Parquet files</h2></div></div></div><div><div><h3 class="title"><a id="note199"/>Note</h3><p>
<strong>Note</strong>: For the rest of this chapter, we define a batch Spark DataFrame as a classic Spark DataFrame, that is non-streaming.</p></div></div><p>The last step <a id="id463" class="indexterm"/>of this streaming computation flow is to create one or more batch DataFrames that we can use for building our analytics and data visualizations. We can think of this last step as taking a snapshot of the data for deeper analysis.</p><p>There are two ways to programmatically load a batch DataFrame from a Parquet file:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Using <code class="literal">spark.read</code> (notice that we don't use <code class="literal">spark.readStream</code> as we did earlier):<div><pre class="programlisting">parquet_batch_df = spark.read.parquet(os.path.join(root_dir, "output_parquet"))</pre></div></li><li class="listitem" style="list-style-type: disc">Using <code class="literal">spark.sql</code>:<div><pre class="programlisting">parquet_batch_df = spark.sql(
"<strong>select * from parquet.'{}'</strong>".format(
os.path.join(root_dir, "output_parquet")
)
)</pre></div><div><div><h3 class="title"><a id="note200"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode10.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode10.py</a>
</p></div></div></li></ul></div><p>The benefit of this method is that we can use any ANSI SQL query to load the data, instead of using the equivalent low-level DataFrame APIs that we would have to use in the first method.</p><p>We can then periodically refresh the data by rerunning the preceding code and recreating the DataFrame. We are now ready to create further analysis on the data by, for example, running the PixieDust <code class="literal">display()</code> method on it in order to create visualizations:</p><div><pre class="programlisting">import pixiedust
display(parquet_batch_df)</pre></div><p>We select the <strong>Bar Chart</strong> menu and drag and drop the <code class="literal">source</code> field in the <strong>Keys</strong> field area. Since we want to show only the top 10 tweets, we set this value in the <strong># of Rows to Display</strong> field. The following screenshot shows the PixieDust options dialog:</p><div><img src="img/B09699_07_06.jpg" alt="Creating a batch DataFrame from the Parquet files" width="849" height="748"/><div><p>Options dialog for showing the top 10 sources of tweets</p></div></div><p>After <a id="id464" class="indexterm"/>clicking <strong>OK</strong>, we see the following results:</p><div><img src="img/B09699_07_07.jpg" alt="Creating a batch DataFrame from the Parquet files" width="1000" height="730"/><div><p>Chart showing the number of tweets related to baseball by source</p></div></div><p>In this section, we've seen how to use the Tweepy library to create a Twitter stream, clean the raw <a id="id465" class="indexterm"/>data and store it in CSV files, create a Spark Streaming DataFrame, run streaming queries on it and store the output in a Parquet database, create a batch DataFrame from the Parquet file, and visualize the data using PixieDust <code class="literal">display()</code>.</p><div><div><h3 class="title"><a id="note201"/>Note</h3><p>The complete notebook for <em>Part 1 – Acquiring the data with Spark Structured Streaming</em> can be found here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%201.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%201.ipynb</a>
</p></div></div><p>In the next part, we'll look at enriching the data with sentiment and entity extraction using the IBM Watson Natural Language Understanding service.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec51"/>Part 2 – Enriching the data with sentiment and most relevant extracted entity</h1></div></div></div><p>In this part, we enrich the Twitter data with sentiment information, for example, <em>positive</em>, <em>negative</em>, and <em>neutral</em>. We also want to extract the most relevant entity from the tweet, for example, sport, organization, and location. This extra information will be analyzed and visualized <a id="id466" class="indexterm"/>by the real-time dashboard that we'll build <a id="id467" class="indexterm"/>in the next section. The algorithms used to extract sentiment and entity from an unstructured text belong to a field of computer <a id="id468" class="indexterm"/>science and artificial intelligence called <strong>natural language processing</strong> (<strong>NLP</strong>). There are plenty of tutorials available on the web that provide algorithm examples on how to extract sentiment. For example, you can find a comprehensive text analytic tutorial on the scikit-learn repo at <a class="ulink" href="https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/working_with_text_data.rst">https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/working_with_text_data.rst</a>.</p><p>However, for this sample application, we are not going to build our own NLP algorithm. Instead, we'll choose a cloud-based service that provides text analytics such as sentiment and entity extraction. This approach works very well when you have generic requirements such as do not require training custom models, but even then, most of the service providers now provide tooling to do so. There are major advantages to use a cloud-based provider over creating your own model such as saving on the development time and much better accuracy and performance. With a simple REST call, we'll be able to generate the data we need <a id="id469" class="indexterm"/>and integrate it into the flow of our application. Also, it would be very easy to change providers if needed as the code responsible for interfacing with the service is well isolated.</p><p>For this sample application, we'll use the <strong>IBM Watson Natural Language Understanding</strong> (<strong>NLU</strong>) service which is a part of the IBM Watson family of cognitive services, and available on IBM Cloud.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec42"/>Getting started with the IBM Watson Natural Language Understanding service</h2></div></div></div><p>The process <a id="id470" class="indexterm"/>of provisioning a new service is usually the same for every cloud provider. After logging in, you go to a service catalog page where you can search for a particular service.</p><p>To log in to the IBM Cloud, just go to <a class="ulink" href="https://console.bluemix.net">https://console.bluemix.net</a> and create a free IBM account if you don't already have one. Once in the dashboard, there are multiple ways to search for the IBM Watson NLU service:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Click on the top left-hand menu, and select <strong>Watson</strong>, select <strong>Browse services</strong>, and find the <strong>Natural Language Understanding</strong> entry in the list of services.</li><li class="listitem" style="list-style-type: disc">Click on the <strong>Create Resource</strong> button in the top-right corner to get to the catalog. Once in the catalog, you can search for <code class="literal">Natural Language Understanding</code> in the search bar as shown in the following screenshot:<div><img src="img/B09699_07_08.jpg" alt="Getting started with the IBM Watson Natural Language Understanding service" width="1000" height="543"/><div><p>Searching for Watson NLU in the service catalog</p></div></div></li></ul></div><p>You can then click on <strong>Natural Language Understanding</strong> to provision a new instance. It is not unusual that cloud providers offer a free or trial-based plan for some services and fortunately <a id="id471" class="indexterm"/>Watson NLU provides one of these, with the limitation that you can train only one custom model with a maximum of 30,000 NLU items processed per month (which is adequate for our sample application). After selecting the <strong>Lite</strong> (free) plan and clicking on the <strong>Create</strong> button, the newly provisioned instance will appear on the dashboard and is ready to accept requests.</p><div><div><h3 class="title"><a id="note202"/>Note</h3><p>
<strong>Note</strong>: After creating the service, you may be redirected to the NLU service <em>getting started document</em>. If so, simply navigate back to the dashboard where you should see the new service instance listed.</p></div></div><p>The next step is to test the service from our Notebook by making a REST call. Every service provides detailed documentation on how to use it including the API reference. From the Notebook, we could use the requests package to make GET, POST, PUT, or DELETE calls according to the API reference, but it is highly recommended to check whether the service offers SDKs with high-level programmatic access to the APIs. </p><p>Fortunately, IBM Watson provides the <code class="literal">watson_developer_cloud</code> open source library which includes multiple open source SDKs supporting some of the most popular languages, including Java, Python, and Node.js. For this project, we'll use the Python SDK with source code and code examples located here: <a class="ulink" href="https://github.com/watson-developer-cloud/python-sdk">https://github.com/watson-developer-cloud/python-sdk</a>.</p><p>The following <code class="literal">pip</code> command installs the <code class="literal">watson_developer_cloud</code> package directly from the Jupyter Notebook:</p><div><pre class="programlisting">
<strong>!pip install Watson_developer_cloud</strong>
</pre></div><div><div><h3 class="title"><a id="note203"/>Note</h3><p>Notice the <code class="literal">!</code> in front of the command that signifies that it's a shell command.</p><p>
<strong>Note</strong>: Don't forget to restart the kernel once installation is complete.</p></div></div><p>Most cloud <a id="id472" class="indexterm"/>service providers use a common pattern to let consumers authenticate with the service, which consists of generating a set of credentials from the service console dashboard that will be embedded in the client application. To generate the credentials, simply click on the <strong>Service credentials</strong> tab of your Watson NLU instance and click on the <strong>New credential</strong> button.</p><p>This will generate a new set of credentials in JSON format as shown in the following screenshot:</p><div><img src="img/B09699_07_09.jpg" alt="Getting started with the IBM Watson Natural Language Understanding service" width="1000" height="646"/><div><p>Generating new credentials for the Watson NLU service</p></div></div><p>Now that we have the credentials to our service, we can create a <code class="literal">NaturalLanguageUnderstandingV1</code> object that will provide programmatic access to the REST APIs, as shown in the following code:</p><div><pre class="programlisting">from watson_developer_cloud import NaturalLanguageUnderstandingV1
from watson_developer_cloud.natural_language_understanding_v1 import Features, SentimentOptions, EntitiesOptions

nlu = NaturalLanguageUnderstandingV1(
    version='2017-02-27',
    username='XXXX',
    password='XXXX'
)</pre></div><div><div><h3 class="title"><a id="note204"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode11.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode11.py</a></p><p><strong>Note</strong>: In the preceding code, replace the <code class="literal">XXXX</code> text with the appropriate username and password from the service credentials.</p><p>The <code class="literal">version</code> argument refers to a specific version of the API. To know the latest version, go to the official documentation page located here:</p><p>
<a class="ulink" href="https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1">https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1</a>
</p></div></div><p>Before continuing <a id="id473" class="indexterm"/>with building the application, let's take a moment to understand the text analytics capabilities offered by the Watson Natural Language service which include:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Sentiment</li><li class="listitem" style="list-style-type: disc">Entities</li><li class="listitem" style="list-style-type: disc">Concepts</li><li class="listitem" style="list-style-type: disc">Categories</li><li class="listitem" style="list-style-type: disc">Emotion</li><li class="listitem" style="list-style-type: disc">Keywords</li><li class="listitem" style="list-style-type: disc">Relations</li><li class="listitem" style="list-style-type: disc">Semantic roles</li></ul></div><p>In our application, enriching the Twitter data happens in the <code class="literal">RawTweetsListener</code> where we create an <code class="literal">enrich</code> method that will be invoked from the <code class="literal">on_data</code> handler method. In this method, we call the <code class="literal">nlu.analyze</code> method with the Twitter data and a feature list that includes sentiment and entities only as shown in the following code:</p><div><div><h3 class="title"><a id="note205"/>Note</h3><p>
<strong>Note</strong>: The <code class="literal">[[RawTweetsListener]]</code> notation means that the following code is part of a class called <code class="literal">RawTweetsListener</code> and that the user should not attempt to run the code as is without the complete class. As always, you can always refer to the complete notebook for reference.</p></div></div><div><pre class="programlisting">[[RawTweetsListener]]
def enrich(self, data):
    try:
        response = <strong>nlu.analyze(</strong>
<strong>            text = data['text'],</strong>
<strong>            features = Features(</strong>
<strong>                sentiment=SentimentOptions(),</strong>
<strong>                entities=EntitiesOptions()</strong>
<strong>            )</strong>
<strong>        )</strong>
        data["sentiment"] = response["sentiment"]["document"]["label"]
        top_entity = response["entities"][0] if len(response["entities"]) &gt; 0 else None
        data["entity"] = top_entity["text"] if top_entity is not None else ""
        data["entity_type"] = top_entity["type"] if top_entity is not None else ""
        return data
    except Exception as e:
<strong>        self.warn("Error from Watson service while enriching data: {}".format(e))</strong>
</pre></div><div><div><h3 class="title"><a id="note206"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode12.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode12.py</a>
</p></div></div><p>The results <a id="id474" class="indexterm"/>are then stored in the <code class="literal">data</code> object which will be written to the CSV files. We also guard against unexpected exceptions skipping the current tweet and logging a warning message instead of letting the exception bubble up which would stop the Twitter stream.</p><div><div><h3 class="title"><a id="note207"/>Note</h3><p>
<strong>Note</strong>: The most common exception happens when the tweet data is in a language that is not supported by the service.</p><p>We use the <code class="literal">@Logger</code> decorator described in <a class="link" href="ch05.xhtml" title="Chapter 5. Python and PixieDust Best Practices and Advanced Concepts">Chapter 5</a>, <em>Python and PixieDust Best Practices and Advanced Concepts</em> to log messages against the PixieDust logging framework. As a reminder, you can use the <code class="literal">%pixiedustLog</code> magic from another cell to view the log messages.</p></div></div><p>We still <a id="id475" class="indexterm"/>need to change the schema metadata to include the new fields as follows:</p><div><pre class="programlisting">field_metadata = [
    {"name": "created_at", "type": DateType()},
    {"name": "text", "type": StringType()},
    {"name": "source", "type": StringType(),
         "transform": lambda s: BS(s, "html.parser").text.strip()
    },
<strong>    {"name": "sentiment", "type": StringType()},</strong>
<strong>    {"name": "entity", "type": StringType()},</strong>
<strong>    {"name": "entity_type", "type": StringType()}</strong>
]</pre></div><div><div><h3 class="title"><a id="note208"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode13.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode13.py</a>
</p></div></div><p>Finally, we update <code class="literal">on_data</code> handler to invoke the <code class="literal">enrich</code> method as follows:</p><div><pre class="programlisting">def on_data(self, data):
    def transform(key, value):
        return transforms[key](value) if key in transforms else value
    <strong>data = self.enrich(json.loads(data))</strong>
<strong>    if data is not None:</strong>
        self.buffered_data.append(
            {key:transform(key,value) \
                for key,value in iteritems(data) \
                if key in fieldnames}
        )
        self.flush_buffer_if_needed()
    return True
</pre></div><div><div><h3 class="title"><a id="note209"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode14.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode14.py</a>
</p></div></div><p>When we restart the Twitter stream and create the Spark Streaming DataFrame, we can verify that we have the correct schema using the following code:</p><div><pre class="programlisting">schema = StructType(
    [StructField(f["name"], f["type"], True) for f in field_metadata]
)
csv_sdf = spark.readStream \
    .csv(
        output_dir,
        schema=schema,
        multiLine = True,
        dateFormat = 'EEE MMM dd kk:mm:ss Z y',
        ignoreTrailingWhiteSpace = True,
        ignoreLeadingWhiteSpace = True
    )
csv_sdf.printSchema()</pre></div><div><div><h3 class="title"><a id="note210"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode15.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode15.py</a>
</p></div></div><p>Which <a id="id476" class="indexterm"/>shows the following results as expected:</p><div><pre class="programlisting">root
 |-- created_at: date (nullable = true)
 |-- text: string (nullable = true)
 |-- source: string (nullable = true)
<strong> |-- sentiment: string (nullable = true)</strong>
<strong> |-- entity: string (nullable = true)</strong>
<strong> |-- entity_type: string (nullable = true)</strong>
</pre></div><p>Similarly, when we run the structured query with the <code class="literal">console</code> sink, data is displayed in batches in the console of the Spark master node as shown here:</p><div><pre class="programlisting">-------------------------------------------
Batch: 2
-------------------------------------------
+----------+---------------+---------------+---------+------------+-------------+
|created_at|           text|         source|<strong>sentiment|      entity|  entity_type|</strong>
+----------+---------------+---------------+---------+------------+-------------+
|2018-04-14|Some little ...| Twitter iPhone| <strong>positive|        Drew|       Person|d</strong>
|2018-04-14|RT @XXXXXXXX...| Twitter iPhone|  <strong>neutral| @XXXXXXXXXX|TwitterHandle|</strong>
|2018-04-14|RT @XXXXXXXX...| Twitter iPhone|  <strong>neutral|    baseball|        Sport|</strong>
|2018-04-14|RT @XXXXXXXX...| Twitter Client|  <strong>neutral| @XXXXXXXXXX|TwitterHandle|</strong>
|2018-04-14|RT @XXXXXXXX...| Twitter Client| <strong>positive| @XXXXXXXXXX|TwitterHandle|</strong>
|2018-04-14|RT @XXXXX: I...|Twitter Android| <strong>positive| Greg XXXXXX|       Person|</strong>
|2018-04-14|RT @XXXXXXXX...| Twitter iPhone| <strong>positive| @XXXXXXXXXX|TwitterHandle|</strong>
|2018-04-14|RT @XXXXX: I...|Twitter Android| <strong>positive| Greg XXXXXX|       Person|</strong>
|2018-04-14|Congrats to ...|Twitter Android| <strong>positive|    softball|        Sport|</strong>
|2018-04-14|translation:...| Twitter iPhone|  <strong>neutral|        null|         null|</strong>
+----------+---------------+---------------+---------+------------+-------------+</pre></div><p>Finally, we <a id="id477" class="indexterm"/>run the structured query with the Parquet <code class="literal">output</code> sink, create a batch DataFrame, and explore the data using the PixieDust <code class="literal">display()</code> to show, for example, a count of tweets by sentiment (<code class="literal">positive</code>, <code class="literal">negative</code>, <code class="literal">neutral</code>) clustered by the entity as shown in the following chart:</p><div><img src="img/B09699_07_10.jpg" alt="Getting started with the IBM Watson Natural Language Understanding service" width="1000" height="650"/><div><p>Bar chart showing the number of tweets by sentiment clustered by entities</p></div></div><div><div><h3 class="title"><a id="note211"/>Note</h3><p>The complete notebook for <em>Part 2 – Enrich the data with sentiment and most relevant extracted entity</em> is located here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%202.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%202.ipynb</a></p><p>If you are running it, I encourage you to experiment by adding more fields to the schema, run different SQL queries, and visualize the data with PixieDust <code class="literal">display()</code>.</p></div></div><p>In the <a id="id478" class="indexterm"/>next section, we'll build a dashboard that displays multiple metrics about the Twitter data.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec52"/>Part 3 – Creating a real-time dashboard PixieApp</h1></div></div></div><p>As always, we first need to define the requirements for the MVP version of the dashboard. This time we'll borrow a tool from the agile methodology called a <strong>user story</strong> which <a id="id479" class="indexterm"/>describes the features we want to build from <a id="id480" class="indexterm"/>the perspective of the user. The agile methodology also prescribes fully understanding the context of the different users that will interact with the software by categorizing them into personas. In our case, we will only use one persona: <em>Frank the marketing director who wants to get real-time insights from what consumers are talking about on social media</em>.</p><p>The user <a id="id481" class="indexterm"/>story goes like this:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Frank enters a search query like for example a product name</li><li class="listitem" style="list-style-type: disc">A dashboard is then presented that displays a set of charts showing metrics about user sentiments (positive, negative, neutral)</li><li class="listitem" style="list-style-type: disc">The dashboard also contains a word cloud of all the entities being uttered in the tweets</li><li class="listitem" style="list-style-type: disc">Additionally, the dashboard has an option to display the real-time progress of all the Spark Streaming queries that are currently active</li></ul></div><div><div><h3 class="title"><a id="note212"/>Note</h3><p>
<strong>Note</strong>: The last feature is not really needed for Frank, but we show it here anyway as an example implementation of the exercise given earlier.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec43"/>Refactoring the analytics into their own methods</h2></div></div></div><p>Before we <a id="id482" class="indexterm"/>start, we need to refactor the code that starts the Twitter stream and creates the Spark Streaming DataFrame into their own method that we will invoke in the PixieApp.</p><p>The <code class="literal">start_stream,</code> <code class="literal">start_streaming_dataframe</code>, and <code class="literal">start_parquet_streaming_query</code> methods are as follows:</p><div><pre class="programlisting">def start_stream(queries):
    "Asynchronously start a new Twitter stream"
    stream = Stream(auth, RawTweetsListener())
    stream.filter(track=queries, languages=["en"], async=True)
    return stream</pre></div><div><div><h3 class="title"><a id="note213"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode16.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode16.py</a>
</p></div></div><div><pre class="programlisting">def start_streaming_dataframe(output_dir):
    "Start a Spark Streaming DataFrame from a file source"
    schema = StructType(
        [StructField(f["name"], f["type"], True) for f in field_metadata]
    )
    return spark.readStream \
        .csv(
            output_dir,
            schema=schema,
            multiLine = True,
            timestampFormat = 'EEE MMM dd kk:mm:ss Z yyyy',
            ignoreTrailingWhiteSpace = True,
            ignoreLeadingWhiteSpace = True
        )</pre></div><div><div><h3 class="title"><a id="note214"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode17.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode17.py</a>
</p></div></div><div><pre class="programlisting">def start_parquet_streaming_query(csv_sdf):
    """
    Create and run a streaming query from a Structured DataFrame
    outputing the results into a parquet database
    """
    streaming_query = csv_sdf \
      .writeStream \
      .format("parquet") \
      .option("path", os.path.join(root_dir, "output_parquet")) \
      .trigger(processingTime="2 seconds") \
      .option("checkpointLocation", os.path.join(root_dir, "output_chkpt")) \
      .start()
    return streaming_query</pre></div><div><div><h3 class="title"><a id="note215"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode18.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode18.py</a>
</p></div></div><p>As part <a id="id483" class="indexterm"/>of the preparation work, we also need to manage the life cycle of the different streams that will be created by the PixieApp and make sure that the underlying resources are correctly stopped when the user restarts the dashboard. To help with that, we create a <code class="literal">StreamsManager</code> class that encapsulates the Tweepy <code class="literal">twitter_stream</code> and the CSV Streaming DataFrame. This class has a <code class="literal">reset</code> method that will stop the <code class="literal">twitter_stream</code>, stop all the active streaming queries, delete all the output files created from the previous queries, and start a new one with a new query string. If the <code class="literal">reset</code> method is called without a query string, then we don't start new streams.</p><p>We also create a global <code class="literal">streams_manager</code> instance that will keep track of the current state even if the dashboard is restarted. Since the user can rerun the cell that contains the global <code class="literal">streams_manager</code> we need to make sure that the <code class="literal">reset</code> method is automatically invoked when <a id="id484" class="indexterm"/>the current global instance is deleted. For that, we override the object's <code class="literal">__del__</code> method, which is Python's way of implementing a destructor and call <code class="literal">reset</code>.</p><p>The code for <code class="literal">StreamsManager</code> is shown here:</p><div><pre class="programlisting">class StreamsManager():
    def __init__(self):
        self.twitter_stream = None
        self.csv_sdf = None

    def reset(self, search_query = None):
        if self.twitter_stream is not None:
            self.twitter_stream.disconnect()
        #stop all the active streaming queries and re_initialize the directories
        for query in spark.streams.active:
            query.stop()
        # initialize the directories
        self.root_dir, self.output_dir = init_output_dirs()
        # start the tweepy stream
        self.twitter_stream = start_stream([search_query]) if search_query is not None else None
        # start the spark streaming stream
        self.csv_sdf = start_streaming_dataframe(output_dir) if search_query is not None else None

<strong>    def __del__(self):</strong>
<strong>        # Automatically called when the class is garbage collected</strong>
<strong>        self.reset()</strong>

streams_manager = StreamsManager()</pre></div><div><div><h3 class="title"><a id="note216"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode19.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode19.py</a>
</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec44"/>Creating the PixieApp</h2></div></div></div><p>Like in <a class="link" href="ch06.xhtml" title="Chapter 6. Analytics Study: AI and Image Recognition with TensorFlow">Chapter 6</a>, <em>Analytics Study: AI and Image Recognition with TensorFlow</em>, we'll use the <code class="literal">TemplateTabbedApp</code> class again <a id="id485" class="indexterm"/>to create a tab layout with two PixieApps:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">TweetInsightApp</code>: Lets the user specify a query string and shows the real-time dashboard associated with it</li><li class="listitem" style="list-style-type: disc"><code class="literal">StreamingQueriesApp</code>: Monitors the progress of the active structured queries</li></ul></div><p>In the <a id="id486" class="indexterm"/>default route of the <code class="literal">TweetInsightApp</code>, we return a fragment that asks the user for the query string as follows:</p><div><pre class="programlisting">from pixiedust.display.app import *
@PixieApp
class TweetInsightApp():
    @route()
    def main_screen(self):
        return """
&lt;style&gt;
    div.outer-wrapper {
        display: table;width:100%;height:300px;
    }
    div.inner-wrapper {
        display: table-cell;vertical-align: middle;height: 100%;width: 100%;
    }
&lt;/style&gt;
&lt;div class="outer-wrapper"&gt;
    &lt;div class="inner-wrapper"&gt;
        &lt;div class="col-sm-3"&gt;&lt;/div&gt;
        &lt;div class="input-group col-sm-6"&gt;
          &lt;input id="query{{prefix}}" type="text" class="form-control"
              value=""
              placeholder="Enter a search query (e.g. baseball)"&gt;
          &lt;span class="input-group-btn"&gt;
            &lt;button class="btn btn-default" type="button"
<strong>            pd_options="search_query=$val(query{{prefix}})"&gt;</strong>
                Go
            &lt;/button&gt;
          &lt;/span&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
        """

TweetInsightApp().run()</pre></div><div><div><h3 class="title"><a id="note217"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode20.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode20.py</a>
</p></div></div><p>The following <a id="id487" class="indexterm"/>screenshot shows the results of running the preceding code:</p><div><div><h3 class="title"><a id="note218"/>Note</h3><p>
<strong>Note</strong>: We'll create the main <code class="literal">TwitterSentimentApp</code> PixieApp that has the tabbed layout and includes this class later on in this section. For now, we are only showing the <code class="literal">TweetInsightApp</code> child app in isolation.</p></div></div><p> </p><div><img src="img/B09699_07_11.jpg" alt="Creating the PixieApp" width="1000" height="431"/><div><p>Welcome screen for the Twitter Sentiment Dashboard</p></div></div><p>
</p><p>In the <code class="literal">Go</code> button, we invoke the <code class="literal">search_query</code> route with the query string provided by the user. In this route, we first start the various streams and create a batch DataFrame stored in a class variable called <code class="literal">parquet_df</code> from the output directory where the Parquet database is located. We then return the HTML fragment that is composed of three widgets showing the following metrics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Bar chart for each of the three sentiments clustered by entities</li><li class="listitem" style="list-style-type: disc">Line chart subplots showing the distribution of the tweets by sentiment</li><li class="listitem" style="list-style-type: disc">A word cloud for the entities</li></ul></div><p>Each of the widgets is calling a specific route at a regular interval using the <code class="literal">pd_refresh_rate</code> attribute documented in <a class="link" href="ch05.xhtml" title="Chapter 5. Python and PixieDust Best Practices and Advanced Concepts">Chapter 5</a>, <em>Python and PixieDust Best Practices and Advanced Concepts</em>. We also make sure to reload the <code class="literal">parquet_df</code> variable to pick up the new data that has arrived since the last time. This variable is then referenced in the <code class="literal">pd_entity</code> attribute for displaying the chart.</p><p>The following <a id="id488" class="indexterm"/>code shows the implementation for the <code class="literal">search_query</code> route:</p><div><pre class="programlisting">import time
[[TweetInsightApp]]
@route(search_query="*")
    def do_search_query(self, search_query):
        streams_manager.reset(search_query)
        start_parquet_streaming_query(streams_manager.csv_sdf)
<strong>        while True:</strong>
<strong>            try:</strong>
<strong>                parquet_dir = os.path.join(root_dir,</strong>
<strong>                    "output_parquet")</strong>
<strong>                self.parquet_df = spark.sql("select * from parquet.'{}'".format(parquet_dir))</strong>
<strong>                break</strong>
<strong>            except:</strong>
<strong>                time.sleep(5)</strong>
        return """
&lt;div class="container"&gt;
<strong>    &lt;div id="header{{prefix}}" class="row no_loading_msg"</strong>
<strong>        pd_refresh_rate="5000" pd_target="header{{prefix}}"&gt;</strong>
<strong>        &lt;pd_script&gt;</strong>
<strong>print("Number of tweets received: {}".format(streams_manager.twitter_stream.listener.tweet_count))</strong>
<strong>        &lt;/pd_script&gt;</strong>
<strong>    &lt;/div&gt;</strong>
    &lt;div class="row" style="min-height:300px"&gt;
        &lt;div class="col-sm-5"&gt;
            &lt;div id="metric1{{prefix}}" pd_refresh_rate="10000"
                class="no_loading_msg"
                pd_options="display_metric1=true"
                pd_target="metric1{{prefix}}"&gt;
            &lt;/div&gt;
        &lt;/div&gt;
        &lt;div class="col-sm-5"&gt;
            &lt;div id="metric2{{prefix}}" pd_refresh_rate="12000"
                class="no_loading_msg"
                pd_options="display_metric2=true"
                pd_target="metric2{{prefix}}"&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;

    &lt;div class="row" style="min-height:400px"&gt;
        &lt;div class="col-sm-offset-1 col-sm-10"&gt;
            &lt;div id="word_cloud{{prefix}}" pd_refresh_rate="20000"
                class="no_loading_msg"
                pd_options="display_wc=true"
                pd_target="word_cloud{{prefix}}"&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
        """</pre></div><div><div><h3 class="title"><a id="note219"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode21.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode21.py</a>
</p></div></div><p>There are <a id="id489" class="indexterm"/>multiple things to notice from the preceding code:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The output directory for the Parquet files may not be ready when we try to load the <code class="literal">parquet_df</code> batch DataFrame, which would cause an exception. To solve this timing issue, we wrap the code into a <code class="literal">try...except</code> statement and wait for 5 seconds using <code class="literal">time.sleep(5)</code>.</li><li class="listitem" style="list-style-type: disc">We also display the current count of tweets in the header. To do this we add a <code class="literal">&lt;div&gt;</code> element that refreshes every 5 seconds, with a <code class="literal">&lt;pd_script&gt;</code> that prints the current count of tweets using <code class="literal">streams_manager.twitter_stream.listener.tweet_count</code> which is a variable we added to the <code class="literal">RawTweetsListener</code> class. We also updated the <code class="literal">on_data()</code> method to increment the <code class="literal">tweet_count</code> variable every time a new tweet arrives as shown in the following code:<div><pre class="programlisting">[[TweetInsightApp]]
def on_data(self, data):
        def transform(key, value):
            return transforms[key](value) if key in transforms else value
        data = self.enrich(json.loads(data))
        if data is not None:
<strong>            self.tweet_count += 1</strong>
            self.buffered_data.append(
                {key:transform(key,value) \
                     for key,value in iteritems(data) \
                     if key in fieldnames}
            )
            self.flush_buffer_if_needed()
        return True</pre></div><p>Also, to avoid flickering, we prevent the displaying of the <em>loading spinner</em> image using <code class="literal">class="no_loading_msg" </code>in the <code class="literal">&lt;div&gt;</code> element.</p></li><li class="listitem" style="list-style-type: disc">We invoke three different routes (<code class="literal">display_metric1</code>, <code class="literal">display_metric2</code>, and <code class="literal">display_wc</code>) that are responsible for displaying the three widgets respectively.<p>The <code class="literal">display_metric1</code> and <code class="literal">display_metric2</code> routes are very similar. They return a div with <code class="literal">parquet_df</code> as the <code class="literal">pd_entity</code> and a custom <code class="literal">&lt;pd_options&gt;</code> child element that contains the JSON configuration passed to the PixieDust <code class="literal">display()</code> layer.</p></li></ul></div><p>The following <a id="id490" class="indexterm"/>code shows the implementation for the <code class="literal">display_metric1</code> route:</p><div><pre class="programlisting">[[TweetInsightApp]]
@route(display_metric1="*")
    def do_display_metric1(self, display_metric1):
        parquet_dir = os.path.join(root_dir, "output_parquet")
        self.parquet_df = spark.sql("select * from parquet.'{}'".format(parquet_dir))
        return """
&lt;div class="no_loading_msg" pd_render_onload pd_entity="parquet_df"&gt;
    &lt;pd_options&gt;
    {
      "legend": "true",
      "keyFields": "sentiment",
      "clusterby": "entity_type",
      "handlerId": "barChart",
      "rendererId": "bokeh",
      "rowCount": "10",
      "sortby": "Values DESC",
      "noChartCache": "true"
    }
    &lt;/pd_options&gt;
&lt;/div&gt;
        """</pre></div><div><div><h3 class="title"><a id="note220"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode22.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode22.py</a>
</p></div></div><p>The <code class="literal">display_metric2</code> route follows a similar pattern but with a different set of <code class="literal">pd_options</code> attributes.</p><p>The last route is <code class="literal">display_wc</code> and is responsible for displaying the word cloud for the entities. This route <a id="id491" class="indexterm"/>uses the <code class="literal">wordcloud</code> Python library that you can install with the following command:</p><div><pre class="programlisting">
<strong>!pip install wordcloud</strong>
</pre></div><div><div><h3 class="title"><a id="note221"/>Note</h3><p>
<strong>Note</strong>: As always, don't forget to restart the kernel once installation is complete.</p></div></div><p>We use the <code class="literal">@captureOutput</code> decorator documented in <a class="link" href="ch05.xhtml" title="Chapter 5. Python and PixieDust Best Practices and Advanced Concepts">Chapter 5</a>, <em>Python and PixieDust Best Practices and Advanced Concepts</em> as shown here:</p><div><pre class="programlisting">import matplotlib.pyplot as plt
from wordcloud import WordCloud

[[TweetInsightApp]]
@route(display_wc="*")
<strong>@captureOutput</strong>
def do_display_wc(self):
    <strong>text = "\n".join(</strong>
<strong>        [r['entity'] for r in self.parquet_df.select("entity").collect() if r['entity'] is not None]</strong>
<strong>    )</strong>
    plt.figure( figsize=(13,7) )
    plt.axis("off")
    plt.imshow(
        WordCloud(width=750, height=350).generate(text),
        interpolation='bilinear'
    )</pre></div><div><div><h3 class="title"><a id="note222"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode23.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode23.py</a>
</p></div></div><p>The text passed to the <code class="literal">WordCloud</code> class is generated from collecting all the entities in the <code class="literal">parquet_df</code> batch DataFrame.</p><p>The following screenshot shows the dashboard after letting a Twitter stream, created with the search query <code class="literal">baseball</code>, run for a little while:</p><div><img src="img/B09699_07_12.jpg" alt="Creating the PixieApp" width="1000" height="864"/><div><p>Twitter Sentiment Dashboard for the search query "baseball"</p></div></div><p>The second <a id="id492" class="indexterm"/>PixieApp is used to monitor the streaming queries that are actively running. The main route returns an HTML fragment that has a <code class="literal">&lt;div&gt;</code> element that invokes the <code class="literal">show_progress</code> route at regular intervals (5000 ms) as shown in the following code:</p><div><pre class="programlisting">@PixieApp
class StreamingQueriesApp():
    @route()
    def main_screen(self):
        return """
&lt;div class="no_loading_msg" <strong>pd_refresh_rate="5000" pd_options="show_progress=true"</strong>&gt;
&lt;/div&gt;
        """</pre></div><div><div><h3 class="title"><a id="note223"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode24.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode24.py</a>
</p></div></div><p>In the <code class="literal">show_progress</code> route we use the <code class="literal">query.lastProgress</code> monitoring API described earlier in this chapter, iterate over the JSON object using Jinja2 <code class="literal">{%for%}</code> loop and display <a id="id493" class="indexterm"/>the results in a table as shown in the following code:</p><div><pre class="programlisting">@route(show_progress="true")
    def do_show_progress(self):
        return """
{%for query in this.spark.streams.active%}
    &lt;div&gt;
    &lt;div class="page-header"&gt;
        &lt;h1&gt;Progress Report for Spark Stream: {{query.id}}&lt;/h1&gt;
    &lt;div&gt;
    &lt;table&gt;
        &lt;thead&gt;
          &lt;tr&gt;
             &lt;th&gt;metric&lt;/th&gt;
             &lt;th&gt;value&lt;/th&gt;
          &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
<strong>            {%for key, value in query.lastProgress.items()%}</strong>
<strong>            &lt;tr&gt;</strong>
<strong>                &lt;td&gt;{{key}}&lt;/td&gt;</strong>
<strong>                &lt;td&gt;{{value}}&lt;/td&gt;</strong>
<strong>            &lt;/tr&gt;</strong>
<strong>            {%endfor%}</strong>
        &lt;/tbody&gt;
    &lt;/table&gt;
{%endfor%}
        """</pre></div><div><div><h3 class="title"><a id="note224"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode25.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode25.py</a>
</p></div></div><p>The following screenshot shows the streaming query monitoring PixieApp:</p><div><img src="img/B09699_07_13.jpg" alt="Creating the PixieApp" width="1000" height="450"/><div><p>Live monitoring of the active Spark streaming queries</p></div></div><p>The last step <a id="id494" class="indexterm"/>is to put together the complete application using the <code class="literal">TemplateTabbedApp</code> class as shown in the following code:</p><div><pre class="programlisting">from pixiedust.display.app import *
<strong>from pixiedust.apps.template import TemplateTabbedApp</strong>

@PixieApp
class TwitterSentimentApp(<strong>TemplateTabbedApp</strong>):
    def setup(self):
<strong>        self.apps = [</strong>
<strong>            {"title": "Tweets Insights", "app_class": "TweetInsightApp"},</strong>
<strong>            {"title": "Streaming Queries", "app_class": "StreamingQueriesApp"}</strong>
<strong>        ]</strong>

app = TwitterSentimentApp()
app.run()</pre></div><div><div><h3 class="title"><a id="note225"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode26.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode26.py</a>
</p></div></div><p>Part 3 of our sample application is now complete; you can find the fully-built Notebook here:</p><div><div><h3 class="title"><a id="note226"/>Note</h3><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%203.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%203.ipynb</a>
</p></div></div><p>In the next section, we discuss ways to make the data pipeline of our application more scalable by <a id="id495" class="indexterm"/>using Apache Kafka for event streaming and IBM Streams Designer for data enrichment of the streaming data.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec53"/>Part 4 – Adding scalability with Apache Kafka and IBM Streams Designer</h1></div></div></div><div><div><h3 class="title"><a id="note227"/>Note</h3><p>
<strong>Note</strong>: This section is optional. It demonstrates how to re-implement parts of the data pipeline with cloud-based streaming services to achieve greater scalability</p></div></div><p>Implementing <a id="id496" class="indexterm"/>the entire data pipeline in a single Notebook gave us high productivity during development and testing. We can experiment <a id="id497" class="indexterm"/>with the code and test the changes very rapidly with a very small footprint. Also, performances have been reasonable because we have been working with a relatively small amount of data. However, it is quite obvious that we wouldn't use this architecture in production and the next question we need to ask ourselves is where are the bottlenecks that would prevent the application from scaling as the quantity of streaming data coming from Twitter increases dramatically.</p><p>In this section, we identify two areas for improvement:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In the Tweepy stream, the incoming data is sent to the <code class="literal">RawTweetsListener</code> instance for processing using the <code class="literal">on_data</code> method. We need to make sure to spend as little time as possible in this method otherwise the system will fall behind as the amount of incoming data increases. In the current implementation, the data is enriched synchronously by making an external call to the Watson NLU service; it is then buffered and eventually written to disk. To fix this issue, we send the data to a Kafka service, which is a highly scalable, fault tolerant streaming platform using a publish/subscribe pattern for processing a high volume of data. We also use the Streaming Analytics service, which will consume data from Kafka and enrich it by invoking the Watson NLU service. Both services are available on the IBM Cloud.<div><div><h3 class="title"><a id="note228"/>Note</h3><p>
<strong>Note</strong>: There are alternative open source frameworks that we could have used for processing the streaming data, such as, for example, Apache Flink (<a class="ulink" href="https://flink.apache.org">https://flink.apache.org</a>) or Apache Storm (<a class="ulink" href="http://storm.apache.org">http://storm.apache.org</a>).</p></div></div></li><li class="listitem" style="list-style-type: disc">In the current implementation, the data is stored as CSV files, and we create a Spark Streaming DataFrame with the output directory as the source. This step consumes time and resources on the Notebook and the local environment. Instead, we can have the Streaming Analytics write back the enriched <a id="id498" class="indexterm"/>events in a different topic and create a Spark Streaming DataFrame with the Message Hub service as the Kafka input source.</li></ul></div><p>The following <a id="id499" class="indexterm"/>diagram shows the updated architecture for our sample application:</p><div><img src="img/B09699_07_14.jpg" alt="Part 4 – Adding scalability with Apache Kafka and IBM Streams Designer" width="1000" height="516"/><div><p>Scaling the architecture with Kafka and Streams Designer</p></div></div><p>In the next few sections, we will implement the updated architecture, starting with streaming the tweets to Kafka.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec45"/>Streaming the raw tweets to Kafka</h2></div></div></div><p>Provisioning a Kafka / Message Hub service instance on IBM Cloud follows the same pattern as <a id="id500" class="indexterm"/>the steps we used to provision the Watson NLU service. We first locate and select the service in the catalog, pick a pricing plan and click <strong>Create</strong>. We then open the service dashboard and select the <strong>Service credentials</strong> tab to create new credentials as shown in the following screenshot:</p><div><img src="img/B09699_07_15.jpg" alt="Streaming the raw tweets to Kafka" width="1000" height="550"/><div><p>Creating new credentials for the Message Hub service</p></div></div><p>As is the case for all the services available on IBM Cloud, the credentials come in the form of a JSON object that we'll need to store in its own variable in the Notebook as shown in the following code (again, don't forget to replace the <code class="literal">XXXX</code> text with your username and password from the service credentials):</p><div><pre class="programlisting">message_hub_creds = {
  "instance_id": "XXXXX",
  "mqlight_lookup_url": "https://mqlight-lookup-prod02.messagehub.services.us-south.bluemix.net/Lookup?serviceId=XXXX",
  "api_key": "XXXX",
  "kafka_admin_url": "https://kafka-admin-prod02.messagehub.services.us-south.bluemix.net:443",
  "kafka_rest_url": "https://kafka-rest-prod02.messagehub.services.us-south.bluemix.net:443",
  "kafka_brokers_sasl": [
    "kafka03-prod02.messagehub.services.us-south.bluemix.net:9093",
    "kafka01-prod02.messagehub.services.us-south.bluemix.net:9093",
    "kafka02-prod02.messagehub.services.us-south.bluemix.net:9093",
    "kafka05-prod02.messagehub.services.us-south.bluemix.net:9093",
    "kafka04-prod02.messagehub.services.us-south.bluemix.net:9093"
  ],
  "user": "XXXX",
  "password": "XXXX"
}</pre></div><div><div><h3 class="title"><a id="note229"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode27.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode27.py</a>
</p></div></div><p>As for <a id="id501" class="indexterm"/>interfacing with Kafka, we have a choice between multiple good client libraries. I have tried many of them, but the one I ended up using most often is <code class="literal">kafka-python</code> (<a class="ulink" href="https://github.com/dpkp/kafka-python">https://github.com/dpkp/kafka-python</a>) which has the advantage of being a pure Python implementation and is thereby easier to install.</p><p>To install it from the Notebook, use the following command:</p><div><pre class="programlisting">
<strong>!pip install kafka-python</strong>
</pre></div><div><div><h3 class="title"><a id="note230"/>Note</h3><p>
<strong>Note</strong>: As always, do not forget to restart the kernel after installing any libraries.</p></div></div><p>The <code class="literal">kafka-python</code> library provides a <code class="literal">KafkaProducer</code> class for writing the data as messages into the service, which we'll need to configure with the credentials we created earlier. There are multiple Kafka configuration options available and going over all of them is beyond the scope of this book. The required options are related to authentication, host servers, and API version.</p><p>The following code is implemented in the <code class="literal">__init__</code> constructor of <code class="literal">RawTweetsListener</code> class. It creates a <code class="literal">KafkaProducer</code> instance and stores it as a class variable:</p><div><pre class="programlisting">[[RawTweetsListener]]
context = ssl.create_default_context()
context.options &amp;= ssl.OP_NO_TLSv1
context.options &amp;= ssl.OP_NO_TLSv1_1
kafka_conf = {
    'sasl_mechanism': 'PLAIN',
    'security_protocol': 'SASL_SSL',
    'ssl_context': context,
    "bootstrap_servers": message_hub_creds["kafka_brokers_sasl"],
    "sasl_plain_username": message_hub_creds["user"],
    "sasl_plain_password": message_hub_creds["password"],
    "api_version":(0, 10, 1),
    "value_serializer" : lambda v: json.dumps(v).encode('utf-8')
}
<strong>self.producer = KafkaProducer(**kafka_conf)</strong>
</pre></div><div><div><h3 class="title"><a id="note231"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode28.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode28.py</a>
</p></div></div><p>We configure <a id="id502" class="indexterm"/>a lambda function for the <code class="literal">value_serializer</code> key that serializes JSON objects which is the format we'll be using for our data.</p><div><div><h3 class="title"><a id="note232"/>Note</h3><p>
<strong>Note</strong>: We need to specify the <code class="literal">api_version</code> key because otherwise, the library would try to autodiscover its value which would cause a <code class="literal">NoBrokerAvailable</code> exception to be raised due to a bug in the <code class="literal">kafka-python</code> library reproducible only on Macs. A fix for this bug has not yet been provided at the time of writing this book.</p></div></div><p>We now need to update the <code class="literal">on_data</code> method to send the tweets data to Kafka using the <code class="literal">tweets</code> topic. A Kafka topic is like a channel that applications can publish or subscribe to. It is important to have the topic already created before attempting to write into it otherwise an exception will be raised. This is done in the following <code class="literal">ensure_topic_exists</code> method:</p><div><pre class="programlisting">import requests
import json

def ensure_topic_exists(topic_name):
    response = <strong>requests.post(</strong>
<strong>                message_hub_creds["kafka_rest_url"] +</strong>
<strong>                "/admin/topics",</strong>
<strong>                data = json.dumps({"name": topic_name}),</strong>
<strong>                headers={"X-Auth-Token": message_hub_creds["api_key"]}</strong>
<strong>            )</strong>
    if response.status_code != 200 and \
       response.status_code != 202 and \
       response.status_code != 422 and \
       response.status_code != 403:
        raise Exception(response.json())</pre></div><div><div><h3 class="title"><a id="note233"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode29.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode29.py</a>
</p></div></div><p>In the preceding code, we make a POST request into the path <code class="literal">/admin/topic</code> with a JSON payload that contains the name of the topic we want to create. The request must be authenticated using the API key provided in the credentials and the <code class="literal">X-Auth-Token</code> header. We also <a id="id503" class="indexterm"/>make sure to ignore HTTP error codes 422 and 403 which indicate that the topic already exists.</p><p>The code for the <code class="literal">on_data</code> method now looks much simpler as shown here:</p><div><pre class="programlisting">[[RawTweetsListener]]
def on_data(self, data):
    self.tweet_count += 1
<strong>    self.producer.send(</strong>
<strong>        self.topic,</strong>
<strong>        {key:transform(key,value) \</strong>
<strong>            for key,value in iteritems(json.loads(data)) \</strong>
<strong>            if key in fieldnames}</strong>
<strong>    )</strong>
    return True</pre></div><div><div><h3 class="title"><a id="note234"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode30.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode30.py</a>
</p></div></div><p>As we can see, with this new code, we're spending as little time as possible in the <code class="literal">on_data</code> method, which is the goal we wanted to achieve. The tweet data is now flowing into the Kafka <code class="literal">tweets</code> topic, ready to be enriched by the Streaming Analytics service which we'll discuss in the next section.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec46"/>Enriching the tweets data with the Streaming Analytics service</h2></div></div></div><p>For this step, we'll need to use Watson Studio which is an integrated cloud-based IDE that provides <a id="id504" class="indexterm"/>various tools for working with data, including machine learning / deep learning models, Jupyter Notebooks, stream flows, and more. Watson Studio is a companion tool to IBM Cloud accessible at <a class="ulink" href="https://datascience.ibm.com">https://datascience.ibm.com</a>, and therefore no extra sign up is required.</p><p>Once logged in to Watson Studio, we create a new project which we'll call <code class="literal">Thoughtful Data Science</code>.</p><div><div><h3 class="title"><a id="note235"/>Note</h3><p>
<strong>Note</strong>: It is OK to select the default options when creating a project.</p></div></div><p>We then go to the <strong>Settings</strong> tab to create a Streaming Analytics service, which will be the engine that powers our enrichment process and associate it with the project. Note that we could also have created the service in the IBM Cloud catalog as we did for the other services used in this chapter, but since we still have to associate it with the project, we might as well do the creation in Watson Studio too.</p><p>In the <strong>Settings</strong> tab, we scroll to the <strong>Associated services</strong> section and click on the <strong>Add service</strong> drop-down to select <strong>Streaming Analytics</strong>. In the next page, you have the choice between <strong>Existing</strong> and <strong>New</strong>. Select <strong>New</strong> and follow the steps to create the service. Once done, the newly created service should be associated with the project as shown in the following screenshot:</p><div><div><h3 class="title"><a id="note236"/>Note</h3><p>
<strong>Note</strong>: If there are multiple free options, it is OK to pick any one of them.</p></div></div><div><img src="img/B09699_07_16.jpg" alt="Enriching the tweets data with the Streaming Analytics service" width="623" height="146"/><div><p>Associating the Streaming Analytics service with the project</p></div></div><p>We are now ready to create the stream flow that defines the enrichment processing of our tweet data.</p><p>We go to the <strong>Assets</strong> tab, scroll down to the <strong>Streams flows</strong> section and click on the <strong>New streams flow</strong> button. In the next page, we give a name, select the Streaming Analytics service, select <strong>Manually</strong> and click on the <strong>Create</strong> button.</p><p>We are <a id="id505" class="indexterm"/>now in the Streams Designer which is composed of a palette of operators on the left and a canvas where we can graphically build our stream flow. For our sample application, we'll need to pick three operators from the palette and drag and drop them into the canvas:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Message Hub from the Sources section of the palette</strong>: Input source for our data. Once in the canvas, we rename it <code class="literal">Source Message Hub</code> (by double- clicking on it to enter edit mode).</li><li class="listitem" style="list-style-type: disc"><strong>Code from the Processing and analytics section</strong>: It will contain the data enrichment Python code that invokes the Watson NLU service. We rename the operator to <code class="literal">Enrichment</code>.</li><li class="listitem" style="list-style-type: disc"><strong>Message Hub from the Targets section of the palette</strong>: Output source for the enriched data. We rename it to <code class="literal">Target Message Hub</code>.</li></ul></div><p>Next, we create a connection between the <strong>Source Message Hub</strong> and <strong>Enrichment</strong> and between <strong>Enrichment</strong> and the <strong>Target Message Hub</strong>. To create a connection between two operators, simply grab the output port at the end of the first operator and drag it to the input port of the other operator. Notice that a source operator has only one output port on the right of the box to denote that it only supports outgoing connections, while a target operator has only one input port on the left to denote that it only supports incoming connections. Any operator from the <strong>PROCESSING AND ANALYTICS</strong> section has two ports on the left and right as they accept both incoming and outgoing connections.</p><p>The following screenshot shows the fully completed canvas:</p><div><img src="img/B09699_07_17.jpg" alt="Enriching the tweets data with the Streaming Analytics service" width="1000" height="497"/><div><p>Tweet enrichment stream flow</p></div></div><p>Let's now look at the configuration of each of these three operators.</p><div><div><h3 class="title"><a id="note237"/>Note</h3><p>
<strong>Note</strong>: To complete this section, make sure to run the code that generates topics to the Message Hub instance that we discussed in the previous section. Otherwise, the Message Hub instance will be empty, and no schema will be detected.</p></div></div><p>Click on <a id="id506" class="indexterm"/>the source Message Hub. An animated pane on the right appears with the options to select the Message Hub instance that contains the tweets. The first time, you'll need to create a connection to the Message Hub instance. Select <code class="literal">tweets</code> as the topic. Click on the <strong>Edit Output Schema</strong> and then <strong>Detect Schema</strong> to have the schema autopopulated from the data. You can also preview the live streaming data using the <strong>Show Preview</strong> button as shown in the following screenshot:</p><div><img src="img/B09699_07_18.jpg" alt="Enriching the tweets data with the Streaming Analytics service" width="1000" height="605"/><div><p>Setting the schema and previewing the live streaming data</p></div></div><p>Now select the <strong>Code</strong> operator to implement the code that invokes the Watson NLU. The animated <a id="id507" class="indexterm"/>contextual right-hand pane contains a Python code editor with boilerplate code that includes the required functions to implement, namely <code class="literal">init(state)</code> and <code class="literal">process(event, state)</code>.</p><p>In the <code class="literal">init</code> method, we instantiate the <code class="literal">NaturalLanguageUnderstandingV1</code> instance as shown in the following code:</p><div><pre class="programlisting">import sys
from watson_developer_cloud import NaturalLanguageUnderstandingV1
from watson_developer_cloud.natural_language_understanding_v1 import Features, SentimentOptions, EntitiesOptions

# init() function will be called once on pipeline initialization
# @state a Python dictionary object for keeping state. The state object is passed to the process function
def init(state):
    # do something once on pipeline initialization and save in the state object
<strong>    state["nlu"] = NaturalLanguageUnderstandingV1(</strong>
<strong>        version='2017-02-27',</strong>
<strong>        username='XXXX',</strong>
<strong>        password='XXXX'</strong>
<strong>    )</strong>
</pre></div><div><div><h3 class="title"><a id="note238"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode31.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode31.py</a></p><p><strong>Note</strong>: We need to install the <code class="literal">Watson_developer_cloud</code> library via the <strong>Python packages</strong> link located above the Python editor window in the right-hand contextual pane as shown in the following screenshot:</p><div><img src="img/B09699_07_19.jpg" alt="Enriching the tweets data with the Streaming Analytics service" width="554" height="744"/><div><p>Adding the watson_cloud_developer package to the stream flow</p></div></div></div></div><p>The process <a id="id508" class="indexterm"/>method is invoked on every event data. We use it to invoke the Watson NLU and add the extra information to the event object as shown in the following code:</p><div><pre class="programlisting"># @event a Python dictionary object representing the input event tuple as defined by the input schema
# @state a Python dictionary object for keeping state over subsequent function calls
# return must be a Python dictionary object. It will be the output of this operator.
# Returning None results in not submitting an output tuple for this invocation.
# You must declare all output attributes in the Edit Schema window.
def <strong>process</strong>(event, state):
    # Enrich the event, such as by:
    # event['wordCount'] = len(event['phrase'].split())
    try:
        event['text'] = event['text'].replace('"', "'")
<strong>        response = state["nlu"].analyze(</strong>
<strong>            text = event['text'],</strong>
<strong>            features=Features(sentiment=SentimentOptions(), entities=EntitiesOptions())</strong>
<strong>        )</strong>
        event["sentiment"] = response["sentiment"]["document"]["label"]
        top_entity = response["entities"][0] if len(response["entities"]) &gt; 0 else None
        event["entity"] = top_entity["text"] if top_entity is not None else ""
        event["entity_type"] = top_entity["type"] if top_entity is not None else ""
    except Exception as e:
        return None
<strong>    return event</strong>
</pre></div><div><div><h3 class="title"><a id="note239"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode32.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode32.py</a></p><p><strong>Note</strong>: We must also declare all output variables by using the <strong>Edit Output Schema</strong> link as shown in the following screenshot:</p><div><img src="img/B09699_07_20.jpg" alt="Enriching the tweets data with the Streaming Analytics service" width="1000" height="565"/><div><p>Declaring all output variables for the Code operator</p></div></div></div></div><p>Finally, we configure the target Message Hub to use the <code class="literal">enriched_tweets</code> topic. Note that you'll need to manually create the topic the first time by going into the dashboard of the Message Hub instance on the IBM Cloud and clicking on the <strong>Add Topic</strong> button.</p><p>We then <a id="id509" class="indexterm"/>save the stream flow using the <strong>Save</strong> button in the main toolbar. Any errors in the flow, whether it be a compile error in the code, a service configuration error or any other errors, will be shown in the notification pane. After we make sure that there is no error, we can run the flow using the <strong>Run</strong> button which takes us to the streams flow live monitoring screen. This screen is composed of multiple panes. The main pane shows the different operators with the data represented as little balls flowing in a virtual pipe between operators. We can click on a pipe to show the events payload in a pane on the right. This is really useful for debugging as we can visualize how the data is transformed through each operator.</p><div><div><h3 class="title"><a id="note240"/>Note</h3><p>
<strong>Note</strong>: Streams Designer also supports adding Python logging messages in the code operator which can then be downloaded on your local machine for analysis. You can learn more about this functionality here:</p><p>
<a class="ulink" href="https://dataplatform.cloud.ibm.com/docs/content/streaming-pipelines/downloading_logs.html">https://dataplatform.cloud.ibm.com/docs/content/streaming-pipelines/downloading_logs.html</a>
</p></div></div><p>The following screenshot shows the stream flow live monitoring screen:</p><div><img src="img/B09699_07_21.jpg" alt="Enriching the tweets data with the Streaming Analytics service" width="1000" height="556"/><div><p>Live monitoring screen for the Twitter Sentiment Analysis stream flow</p></div></div><p>We now <a id="id510" class="indexterm"/>have our enriched tweets flowing in the Message Hub instance using the <code class="literal">enriched_tweets</code> topic. In the next section, we show how to create a Spark Streaming DataFrame using the Message Hub instance as the input source.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec47"/>Creating a Spark Streaming DataFrame with a Kafka input source</h2></div></div></div><p>In this final <a id="id511" class="indexterm"/>step, we create a Spark Streaming DataFrame that consumes the enriched tweets from the <code class="literal">enriched_tweets</code> Kafka topic of the Message Hub service. For this, we use the built-in Spark Kafka connector specifying the topic we want to subscribe to in the <code class="literal">subscribe</code> option. We also need to specify the list of Kafka servers in the <code class="literal">kafka.bootstrap.servers</code> option, by reading it from the global <code class="literal">message_hub_creds</code> variable that we created earlier.</p><div><div><h3 class="title"><a id="note241"/>Note</h3><p>
<strong>Note</strong>: You have probably noticed that different systems use different names for this option making it more error prone. Fortunately, in case of a misspelling, an exception with an explicit root cause message will be displayed.</p></div></div><p>The preceding options are for Spark Streaming, and we still need to configure the Kafka credentials <a id="id512" class="indexterm"/>so that the lower level Kafka consumer can be properly authenticated with the Message Hub service. To properly pass these consumer properties to Kafka, we do not use the <code class="literal">.option</code> method, but rather we create a <code class="literal">kafka_options</code> dictionary that we pass to the load method as shown in the following code:</p><div><pre class="programlisting">def start_streaming_dataframe():
    "Start a Spark Streaming DataFrame from a Kafka Input source"
    schema = StructType(
        [StructField(f["name"], f["type"], True) for f in field_metadata]
    )
<strong>    kafka_options = {</strong>
<strong>        "kafka.ssl.protocol":"TLSv1.2",</strong>
<strong>        "kafka.ssl.enabled.protocols":"TLSv1.2",</strong>
<strong>        "kafka.ssl.endpoint.identification.algorithm":"HTTPS",</strong>
<strong>        'kafka.sasl.mechanism': 'PLAIN',</strong>
<strong>        'kafka.security.protocol': 'SASL_SSL'</strong>
<strong>    }</strong>
    return spark.readStream \
        .format("kafka") \
<strong>        .option("kafka.bootstrap.servers", ",".join(message_hub_creds["kafka_brokers_sasl"])) \</strong>
<strong>        .option("subscribe", "enriched_tweets") \</strong>
<strong>        .load(**kafka_options)</strong>
</pre></div><div><div><h3 class="title"><a id="note242"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode33.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode33.py</a>
</p></div></div><p>You would think that we're done with the code at this point since the rest of the Notebook should work unchanged from <em>Part 3 – Create a real-time dashboard PixieApp</em>. This would be correct until we run the Notebook and start seeing exceptions with Spark complaining that the Kafka connector cannot be found. This is because the Kafka connector is not included in the core distribution of Spark and must be installed separately. </p><p>Unfortunately, these types of problems which are infrastructural in nature and are not directly related to the task at hand, happen all the time and we end up spending a lot of time trying to fix them. Searching on Stack Overflow or any other technical site usually yields a solution rapidly, but in some cases, the answer is not obvious. In this case, because we are running in a Notebook and not in a <code class="literal">spark-submit</code> script, there isn't much help available, and we have to experiment ourselves until we find the solution. To install the <code class="literal">spark-sql-kafka</code>, we need to edit the <code class="literal">kernel.json</code> file discussed earlier in this chapter, and add the following option to the <code class="literal">"PYSPARK_SUBMIT_ARGS"</code> entry:</p><div><pre class="programlisting">--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0</pre></div><p>When the kernel restarts, this configuration will automatically download the dependencies and cache them locally.</p><p>It should <a id="id513" class="indexterm"/>all work now right? Well, not yet. We still have to configure Kafka security to use the credentials to our Message Hub service which uses SASL as the security protocol. For that, we need to provide a <strong>JAAS</strong> (short for, <strong>Java Authentication and Authorization Service</strong>) configuration file that will contain the username and password for the service. The latest version <a id="id514" class="indexterm"/>of Kafka provides a flexible mechanism to programmatically configure the security using a consumer property called <code class="literal">sasl.jaas.config</code>. Unfortunately, the latest version of Spark (2.3.0 as of the time of writing) has not yet updated to the latest version of Kafka. So, we have to fall back to the other way of configuring JAAS which is to set a JVM system property called <code class="literal">java.security.auth.login.config</code> with the path to a <code class="literal">jaas.conf</code> configuration file.</p><p>We first create the <code class="literal">jaas.conf</code> in a directory of our choice and add the following content to it:</p><div><pre class="programlisting">KafkaClient {
    org.apache.kafka.common.security.plain.PlainLoginModule required
<strong>    username="XXXX"</strong>
<strong>    password="XXXX";</strong>
};</pre></div><p>In the preceding content, replace the <code class="literal">XXXX</code> text with the username and password taken from the Message Hub service credentials.</p><p>We then add the following configuration to the <code class="literal">"PYSPARK_SUBMIT_ARGS"</code> entry of <code class="literal">kernel.json</code>:</p><div><pre class="programlisting">--driver-java-options=-Djava.security.auth.login.config=&lt;&lt;jaas.conf path&gt;&gt;</pre></div><p>For reference, here is a sample <code class="literal">kernel.json</code> that contains these configurations:</p><div><pre class="programlisting">{
 "language": "python",
 "env": {
  "SCALA_HOME": "/Users/dtaieb/pixiedust/bin/scala/scala-2.11.8",
  "PYTHONPATH": "/Users/dtaieb/pixiedust/bin/spark/spark-2.3.0-bin-hadoop2.7/python/:/Users/dtaieb/pixiedust/bin/spark/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip",
  "SPARK_HOME": "/Users/dtaieb/pixiedust/bin/spark/spark-2.3.0-bin-hadoop2.7",
  "PYSPARK_SUBMIT_ARGS": <strong>"--driver-java-options=-Djava.security.auth.login.config=/Users/dtaieb/pixiedust/jaas.conf</strong> --jars /Users/dtaieb/pixiedust/bin/cloudant-spark-v2.0.0-185.jar --driver-class-path /Users/dtaieb/pixiedust/data/libs/* --master local[10] <strong>--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0</strong> pyspark-shell",
  "PIXIEDUST_HOME": "/Users/dtaieb/pixiedust",
  "SPARK_DRIVER_MEMORY": "10G",
  "SPARK_LOCAL_IP": "127.0.0.1",
  "PYTHONSTARTUP": "/Users/dtaieb/pixiedust/bin/spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/shell.py"
 },
 "display_name": "Python with Pixiedust (Spark 2.3)",
 "argv": [
  "python",
  "-m",
  "ipykernel",
  "-f",
  "{connection_file}"
 ]
}
</pre></div><div><div><h3 class="title"><a id="note243"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode34.json">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode34.json</a></p><p><strong>Note</strong>: We should always restart the Notebook server when modifying <code class="literal">kernel.json</code> to make sure that all new configurations are properly reloaded.</p></div></div><p>The rest <a id="id515" class="indexterm"/>of the Notebook code doesn't change, and the PixieApp dashboard should work the same.</p><div><div><h3 class="title"><a id="note244"/>Note</h3><p>We have now completed Part 4 of our sample application; you can find the complete notebook here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%204.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%204.ipynb</a>
</p></div></div><p>The extra code we had to write at the end of this section reminds us that the journey of working with data is never a straight line. We have to be prepared to deal with obstacles that can be different in nature: a bug in a dependency library or a limitation in an external <a id="id516" class="indexterm"/>service. Surmounting these obstacles doesn't have to stop the project for a long time. Since we're using mostly open-source components, we can leverage a large community of like-minded developers on social sites such as Stack Overflow, get new ideas and code samples, and experiment quickly on a Jupyter Notebook.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec54"/>Summary</h1></div></div></div><p>In this chapter, we've built a data pipeline that analyzes large quantities of streaming data containing unstructured text and applies NLP algorithms coming from external cloud services to extract sentiment and other important entities found in the text. We also built a PixieApp dashboard that displays live metrics with insights extracted from the tweets. We've also discussed various techniques for analyzing data at scale, including Apache Spark Structured Streaming, Apache Kafka, and IBM Streaming Analytics. As always, the goal of these sample applications is to show the art of the possible in building data pipelines with a special focus on leveraging existing frameworks, libraries, and cloud services.</p><p>In the next chapter, we'll discuss time series analysis, which is another great data science topic with a lot of industry applications, which we'll illustrate by building a <em>Financial Portfolio</em> analysis application.</p></div></div>



  </body></html>