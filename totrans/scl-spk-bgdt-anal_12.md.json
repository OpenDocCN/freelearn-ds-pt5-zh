["```py\nval creditRDD = parseRDD(sc.textFile(\"data/germancredit.csv\")).map(parseCredit) \n\n```", "```py\ndef parseRDD(rdd: RDD[String]): RDD[Array[Double]] = { \nrdd.map(_.split(\",\")).map(_.map(_.toDouble)) \n  } \n\n```", "```py\ndef parseCredit(line: Array[Double]): Credit = { \nCredit( \nline(0), line(1) - 1, line(2), line(3), line(4), line(5), \nline(6) - 1, line(7) - 1, line(8), line(9) - 1, line(10) - 1, \nline(11) - 1, line(12) - 1, line(13), line(14) - 1, line(15) - 1, \nline(16) - 1, line(17) - 1, line(18) - 1, line(19) - 1, line(20) - 1) \n  } \n\n```", "```py\ncase class Credit( \ncreditability: Double, \nbalance: Double, duration: Double, history: Double, purpose: Double, amount: Double, \nsavings: Double, employment: Double, instPercent: Double, sexMarried: Double, guarantors: Double, \nresidenceDuration: Double, assets: Double, age: Double, concCredit: Double, apartment: Double, \ncredits: Double, occupation: Double, dependents: Double, hasPhone: Double, foreign: Double) \n\n```", "```py\nval sqlContext = new SQLContext(sc) \nimport sqlContext._ \nimport sqlContext.implicits._ \nval creditDF = creditRDD.toDF().cache() \n\n```", "```py\ncreditDF.createOrReplaceTempView(\"credit\") \n\n```", "```py\ncreditDF.show\n\n```", "```py\nsqlContext.sql(\"SELECT creditability, avg(balance) as avgbalance, avg(amount) as avgamt, avg(duration) as avgdur  FROM credit GROUP BY creditability \").show \n\n```", "```py\ncreditDF.describe(\"balance\").show \n\n```", "```py\ncreditDF.groupBy(\"creditability\").avg(\"balance\").show \n\n```", "```py\nval featureCols = Array(\"balance\", \"duration\", \"history\", \"purpose\", \"amount\", \"savings\", \"employment\", \"instPercent\", \"sexMarried\",\n\"guarantors\", \"residenceDuration\", \"assets\", \"age\", \"concCredit\",\n\"apartment\", \"credits\", \"occupation\", \"dependents\", \"hasPhone\",\n\"foreign\") \n\n```", "```py\nval assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\") \nval df2 = assembler.transform(creditDF) \n\n```", "```py\ndf2.select(\"features\").show\n\n```", "```py\nval labelIndexer = new StringIndexer().setInputCol(\"creditability\").setOutputCol(\"label\") \nval df3 = labelIndexer.fit(df2).transform(df2) \ndf3.select(\"label\", \"features\").show\n\n```", "```py\nval splitSeed = 5043 \nval Array(trainingData, testData) = df3.randomSplit(Array(0.80, 0.20), splitSeed) \n\n```", "```py\nval classifier = new RandomForestClassifier() \n      .setImpurity(\"gini\") \n      .setMaxDepth(30) \n      .setNumTrees(30) \n      .setFeatureSubsetStrategy(\"auto\") \n      .setSeed(1234567) \n      .setMaxBins(40) \n      .setMinInfoGain(0.001) \n\n```", "```py\nval model = classifier.fit(trainingData)\n\n```", "```py\nval predictions = model.transform(testData) \n\n```", "```py\npredictions.select(\"label\",\"rawPrediction\", \"probability\", \"prediction\").show()\n\n```", "```py\nval binaryClassificationEvaluator = new BinaryClassificationEvaluator() \n      .setLabelCol(\"label\") \n      .setRawPredictionCol(\"rawPrediction\") \n\n```", "```py\nval accuracy = binaryClassificationEvaluator.evaluate(predictions) \nprintln(\"The accuracy before pipeline fitting: \" + accuracy) \n\n```", "```py\nprintln(\"Area Under ROC before tuning: \" + printlnMetric(\"areaUnderROC\"))         \nprintln(\"Area Under PRC before tuning: \"+  printlnMetric(\"areaUnderPR\")) \nArea Under ROC before tuning: 0.8453079178885631 Area Under PRC before tuning: 0.751921784149243\n\n```", "```py\ndef printlnMetric(metricName: String): Double = { \n  val metrics = binaryClassificationEvaluator.setMetricName(metricName)\n                                             .evaluate(predictions) \n  metrics \n} \n\n```", "```py\nval rm = new RegressionMetrics( \npredictions.select(\"prediction\", \"label\").rdd.map(x => \n        (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))) \n\n```", "```py\nprintln(\"MSE: \" + rm.meanSquaredError) \nprintln(\"MAE: \" + rm.meanAbsoluteError) \nprintln(\"RMSE Squared: \" + rm.rootMeanSquaredError) \nprintln(\"R Squared: \" + rm.r2) \nprintln(\"Explained Variance: \" + rm.explainedVariance + \"\\n\") \n\n```", "```py\nMSE: 0.2578947368421053\nMAE: 0.2578947368421053\nRMSE Squared: 0.5078333750770082\nR Squared: -0.13758553274682295\nExplained Variance: 0.16083102493074794\n\n```", "```py\nval paramGrid = new ParamGridBuilder()\n                    .addGrid(classifier.maxBins, Array(25, 30))\n                    .addGrid(classifier.maxDepth, Array(5, 10))\n                    .addGrid(classifier.numTrees, Array(20, 70))\n                    .addGrid(classifier.impurity, Array(\"entropy\", \"gini\"))\n                    .build()\n\n```", "```py\nval cv = new CrossValidator()\n             .setEstimator(pipeline)\n             .setEvaluator(binaryClassificationEvaluator)\n             .setEstimatorParamMaps(paramGrid)\n             .setNumFolds(10)\nval pipelineFittedModel = cv.fit(trainingData)\n\n```", "```py\nval predictions2 = pipelineFittedModel.transform(testData) \n\n```", "```py\nval accuracy2 = binaryClassificationEvaluator.evaluate(predictions2)\nprintln(\"The accuracy after pipeline fitting: \" + accuracy2)\n\n```", "```py\nThe accuracy after pipeline fitting: 0.8313782991202348\n\n```", "```py\ndef printlnMetricAfter(metricName: String): Double = { \nval metrics = binaryClassificationEvaluator.setMetricName(metricName).evaluate(predictions2) \nmetrics \n    } \nprintln(\"Area Under ROC after tuning: \" + printlnMetricAfter(\"areaUnderROC\"))     \nprintln(\"Area Under PRC after tuning: \"+  printlnMetricAfter(\"areaUnderPR\"))\n\n```", "```py\nArea Under ROC after tuning: 0.8313782991202345\n Area Under PRC after tuning: 0.7460301367852662\n\n```", "```py\nval rm2 = new RegressionMetrics(predictions2.select(\"prediction\", \"label\").rdd.map(x => (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))) \n println(\"MSE: \" + rm2.meanSquaredError) \nprintln(\"MAE: \" + rm2.meanAbsoluteError) \nprintln(\"RMSE Squared: \" + rm2.rootMeanSquaredError) \nprintln(\"R Squared: \" + rm2.r2) \nprintln(\"Explained Variance: \" + rm2.explainedVariance + \"\\n\")  \n\n```", "```py\nMSE: 0.268421052631579\n MAE: 0.26842105263157895\n RMSE Squared: 0.5180936716768301\n R Squared: -0.18401759530791795\n Explained Variance: 0.16404432132963992\n\n```", "```py\npipelineFittedModel \n      .bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel] \n      .stages(0) \n      .extractParamMap \nprintln(\"The best fitted model:\" + pipelineFittedModel.bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel].stages(0)) \n\n```", "```py\nThe best fitted model:RandomForestClassificationModel (uid=rfc_1fcac012b37c) with 70 trees\n\n```", "```py\nval ratigsFile = \"data/ratings.csv\"\nval df1 = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", true).load(ratigsFile)\nval ratingsDF = df1.select(df1.col(\"userId\"), df1.col(\"movieId\"), df1.col(\"rating\"), df1.col(\"timestamp\"))\nratingsDF.show(false)\n\n```", "```py\nval moviesFile = \"data/movies.csv\"\nval df2 = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(moviesFile)\nval moviesDF = df2.select(df2.col(\"movieId\"), df2.col(\"title\"), df2.col(\"genres\"))\n\n```", "```py\nratingsDF.createOrReplaceTempView(\"ratings\")\nmoviesDF.createOrReplaceTempView(\"movies\")\n\n```", "```py\nval numRatings = ratingsDF.count()\nval numUsers = ratingsDF.select(ratingsDF.col(\"userId\")).distinct().count()\nval numMovies = ratingsDF.select(ratingsDF.col(\"movieId\")).distinct().count()\nprintln(\"Got \" + numRatings + \" ratings from \" + numUsers + \" users on \" + numMovies + \" movies.\")\n\n```", "```py\n// Get the max, min ratings along with the count of users who have rated a movie.\nval results = spark.sql(\"select movies.title, movierates.maxr, movierates.minr, movierates.cntu \"\n       + \"from(SELECT ratings.movieId,max(ratings.rating) as maxr,\"\n       + \"min(ratings.rating) as minr,count(distinct userId) as cntu \"\n       + \"FROM ratings group by ratings.movieId) movierates \"\n       + \"join movies on movierates.movieId=movies.movieId \"\n       + \"order by movierates.cntu desc\") \nresults.show(false) \n\n```", "```py\n// Show the top 10 mostactive users and how many times they rated a movie\nval mostActiveUsersSchemaRDD = spark.sql(\"SELECT ratings.userId, count(*) as ct from ratings \"\n               + \"group by ratings.userId order by ct desc limit 10\")\nmostActiveUsersSchemaRDD.show(false)\n\n```", "```py\n// Find the movies that user 668 rated higher than 4\nval results2 = spark.sql(\n\"SELECT ratings.userId, ratings.movieId,\"\n         + \"ratings.rating, movies.title FROM ratings JOIN movies\"\n         + \"ON movies.movieId=ratings.movieId\"\n         + \"where ratings.userId=668 and ratings.rating > 4\")\nresults2.show(false)\n\n```", "```py\n// Split ratings RDD into training RDD (75%) & test RDD (25%)\nval splits = ratingsDF.randomSplit(Array(0.75, 0.25), seed = 12345L)\nval (trainingData, testData) = (splits(0), splits(1))\nval numTraining = trainingData.count()\nval numTest = testData.count()\nprintln(\"Training: \" + numTraining + \" test: \" + numTest)\n\n```", "```py\nval ratingsRDD = trainingData.rdd.map(row => {\n  val userId = row.getString(0)\n  val movieId = row.getString(1)\n  val ratings = row.getString(2)\n  Rating(userId.toInt, movieId.toInt, ratings.toDouble)\n})\n\n```", "```py\nval testRDD = testData.rdd.map(row => {\n  val userId = row.getString(0)\n  val movieId = row.getString(1)\n  val ratings = row.getString(2)\n  Rating(userId.toInt, movieId.toInt, ratings.toDouble)\n}) \n\n```", "```py\nval rank = 20\nval numIterations = 15\nval lambda = 0.10\nval alpha = 1.00\nval block = -1\nval seed = 12345L\nval implicitPrefs = false\nval model = new ALS()\n           .setIterations(numIterations)\n           .setBlocks(block)\n           .setAlpha(alpha)\n           .setLambda(lambda)\n           .setRank(rank)\n           .setSeed(seed)\n           .setImplicitPrefs(implicitPrefs)\n           .run(ratingsRDD) \n\n```", "```py\n// Making Predictions. Get the top 6 movie predictions for user 668\nprintln(\"Rating:(UserID, MovieID, Rating)\")\nprintln(\"----------------------------------\")\nval topRecsForUser = model.recommendProducts(668, 6)\nfor (rating <- topRecsForUser) {\n  println(rating.toString())\n}\nprintln(\"----------------------------------\")\n\n```", "```py\nvar rmseTest = computeRmse(model, testRDD, true)\nprintln(\"Test RMSE: = \" + rmseTest) //Less is better \n\n```", "```py\n  def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating], implicitPrefs: Boolean): Double = {\n    val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product)))\n    val predictionsAndRatings = predictions.map { x => ((x.user, x.product), x.rating)\n  }.join(data.map(x => ((x.user, x.product), x.rating))).values\n  if (implicitPrefs) {\n    println(\"(Prediction, Rating)\")\n    println(predictionsAndRatings.take(5).mkString(\"\\n\"))\n  }\n  math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).mean())\n}\n\n```", "```py\nTest RMSE: = 0.9019872589764073\n\n```", "```py\nobject topicModellingwithLDA {\n  def main(args: Array[String]): Unit = {\n    val lda = new LDAforTM() // actual computations are done here\n    val defaultParams = Params().copy(input = \"data/docs/\") \n    // Loading the parameters\n    lda.run(defaultParams) // Training the LDA model with the default\n                              parameters.\n  }\n} \n\n```", "```py\nval spark = SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n    .appName(\"LDA for topic modelling\")\n    .getOrCreate() \n\n```", "```py\n// Load documents, and prepare them for LDA.\n\nval preprocessStart = System.nanoTime()\nval (corpus, vocabArray, actualNumTokens) = preprocess(params.input, params.vocabSize, params.stopwordFile)  \n\n```", "```py\nval initialrdd = spark.sparkContext.wholeTextFiles(paths).map(_._2)\ninitialrdd.cache()  \n\n```", "```py\nval rdd = initialrdd.mapPartitions { partition =>\n  val morphology = new Morphology()\n  partition.map { value => helperForLDA.getLemmaText(value, morphology) }\n}.map(helperForLDA.filterSpecialCharacters)\n\n```", "```py\n<dependency>\n    <groupId>edu.stanford.nlp</groupId>\n    <artifactId>stanford-corenlp</artifactId>\n    <version>3.6.0</version>\n</dependency>\n<dependency>\n    <groupId>edu.stanford.nlp</groupId>\n    <artifactId>stanford-corenlp</artifactId>\n    <version>3.6.0</version>\n    <classifier>models</classifier>\n</dependency>\n\n```", "```py\ndef getLemmaText(document: String, morphology: Morphology) = {\n  val string = new StringBuilder()\n  val value = new Document(document).sentences().toList.flatMap { a =>\n  val words = a.words().toList\n  val tags = a.posTags().toList\n  (words zip tags).toMap.map { a =>\n    val newWord = morphology.lemma(a._1, a._2)\n    val addedWoed = if (newWord.length > 3) {\n      newWord\n    } else { \"\" }\n      string.append(addedWoed + \" \")\n    }\n  }\n  string.toString()\n} \n\n```", "```py\nrdd.cache()\ninitialrdd.unpersist()\nval df = rdd.toDF(\"docs\")\ndf.show() \n\n```", "```py\nval tokenizer = new RegexTokenizer().setInputCol(\"docs\").setOutputCol(\"rawTokens\") \n\n```", "```py\nval stopWordsRemover = new StopWordsRemover().setInputCol(\"rawTokens\").setOutputCol(\"tokens\")\nstopWordsRemover.setStopWords(stopWordsRemover.getStopWords ++ customizedStopWords)\n\n```", "```py\nval countVectorizer = new CountVectorizer().setVocabSize(vocabSize).setInputCol(\"tokens\").setOutputCol(\"features\") \n\n```", "```py\nval pipeline = new Pipeline().setStages(Array(tokenizer, stopWordsRemover, countVectorizer))\n\n```", "```py\nval model = pipeline.fit(df)\nval documents = model.transform(df).select(\"features\").rdd.map {\n  case Row(features: MLVector) =>Vectors.fromML(features)\n}.zipWithIndex().map(_.swap)\n\n```", "```py\n(documents, model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary, documents.map(_._2.numActives).sum().toLong)\n\n```", "```py\nprintln()\nprintln(\"Training corpus summary:\")\nprintln(\"-------------------------------\")\nprintln(\"Training set size: \" + actualCorpusSize + \" documents\")\nprintln(\"Vocabulary size: \" + actualVocabSize + \" terms\")\nprintln(\"Number of tockens: \" + actualNumTokens + \" tokens\")\nprintln(\"Preprocessing time: \" + preprocessElapsed + \" sec\")\nprintln(\"-------------------------------\")\nprintln()\n\n```", "```py\nTraining corpus summary:\n -------------------------------\n Training set size: 18 documents\n Vocabulary size: 21607 terms\n Number of tockens: 75758 tokens\n Preprocessing time: 39.768580981 sec\n **-------------------------------**\n\n```", "```py\nval lda = new LDA()\n\n```", "```py\nval optimizer = params.algorithm.toLowerCase match {\n  case \"em\" => new EMLDAOptimizer\n  case \"online\" => new OnlineLDAOptimizer().setMiniBatchFraction(0.05 + 1.0 / actualCorpusSize)\n  case _ => throw new IllegalArgumentException(\"Only em is supported, got ${params.algorithm}.\")\n}\n\n```", "```py\nlda.setOptimizer(optimizer)\n  .setK(params.k)\n  .setMaxIterations(params.maxIterations)\n  .setDocConcentration(params.docConcentration)\n  .setTopicConcentration(params.topicConcentration)\n  .setCheckpointInterval(params.checkpointInterval)\n\n```", "```py\n //Setting the parameters before training the LDA model\ncase class Params(input: String = \"\",\n                  k: Int = 5,\n                  maxIterations: Int = 20,\n                  docConcentration: Double = -1,\n                  topicConcentration: Double = -1,\n                  vocabSize: Int = 2900000,\n                  stopwordFile: String = \"data/stopWords.txt\",\n                  algorithm: String = \"em\",\n                  checkpointDir: Option[String] = None,\n                  checkpointInterval: Int = 10)\n\n```", "```py\nif (params.checkpointDir.nonEmpty) {\n  spark.sparkContext.setCheckpointDir(params.checkpointDir.get)\n}\n\n```", "```py\nval startTime = System.nanoTime()\n//Start training the LDA model using the training corpus \nval ldaModel = lda.run(corpus)\nval elapsed = (System.nanoTime() - startTime) / 1e9\nprintln(s\"Finished training LDA model.  Summary:\") \nprintln(s\"t Training time: $elapsed sec\")\n\n```", "```py\nif (ldaModel.isInstanceOf[DistributedLDAModel]) {\n  val distLDAModel = ldaModel.asInstanceOf[DistributedLDAModel]\n  val avgLogLikelihood = distLDAModel.logLikelihood / actualCorpusSize.toDouble\n  println(\"The average log likelihood of the training data: \" +  avgLogLikelihood)\n  println()\n}\n\n```", "```py\nThe average log-likelihood of the training data: -208599.21351837728  \n\n```", "```py\nval topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 10)\nprintln(topicIndices.length)\nval topics = topicIndices.map {case (terms, termWeights) => terms.zip(termWeights).map { case (term, weight) => (vocabArray(term.toInt), weight) } }\n\n```", "```py\nvar sum = 0.0\nprintln(s\"${params.k} topics:\")\ntopics.zipWithIndex.foreach {\n  case (topic, i) =>\n  println(s\"TOPIC $i\")\n  println(\"------------------------------\")\n  topic.foreach {\n    case (term, weight) =>\n    println(s\"$termt$weight\")\n    sum = sum + weight\n  }\n  println(\"----------------------------\")\n  println(\"weight: \" + sum)\n  println()\n\n```", "```py\n    5 topics:\n    TOPIC 0\n    ------------------------------\n    think 0.0105511077762379\n    look  0.010393384083882656\n    know  0.010121680765600402\n    come  0.009999416569525854\n    little      0.009880422850906338\n    make  0.008982740529851225\n    take  0.007061048216197747\n    good  0.007040301924830752\n    much  0.006273732732002744\n    well  0.0062484438391950895\n    ----------------------------\n    weight: 0.0865522792882307\n\n    TOPIC 1\n    ------------------------------\n    look  0.008658099588372216\n    come  0.007972622171954474\n    little      0.007596460821298818\n    hand  0.0065409990798624565\n    know  0.006314616294309573\n    lorry 0.005843633203040061\n    upon  0.005545300032552888\n    make  0.005391780686824741\n    take  0.00537353581562707\n    time  0.005030870790464942\n    ----------------------------\n    weight: 0.15082019777253794\n\n    TOPIC 2\n    ------------------------------\n    captain     0.006865463831587792\n    nautilus    0.005175561004431676\n    make  0.004910586984657019\n    hepzibah    0.004378298053191463\n    water 0.004063096964497903\n    take  0.003959626037381751\n    nemo  0.0037687537789531005\n    phoebe      0.0037683642100062313\n    pyncheon    0.003678496229955977\n    seem  0.0034594205003318193\n    ----------------------------\n    weight: 0.19484786536753268\n\n    TOPIC 3\n    ------------------------------\n    fogg  0.009552022075897986\n    rodney      0.008705705501603078\n    make  0.007016635545801613\n    take  0.00676049232003675\n    passepartout      0.006295907851484774\n    leave 0.005565220660514245\n    find  0.005077555215275536\n    time  0.004852923943330551\n    luke  0.004729546554304362\n    upon  0.004707181805179265\n    ----------------------------\n    weight: 0.2581110568409608\n\n    TOPIC 4\n    ------------------------------\n    dick  0.013754147765988699\n    thus  0.006231933402776328\n    ring  0.0052746290878481926\n    bear  0.005181637978658836\n    fate  0.004739983892853129\n    shall 0.0046221874997173906\n    hand  0.004610810387565958\n    stand 0.004121100025638923\n    name  0.0036093879729237\n    trojan      0.0033792362039766505\n    ----------------------------\n    weight: 0.31363611105890865\n\n```", "```py\npackage com.chapter11.SparkMachineLearning\n\nimport edu.stanford.nlp.process.Morphology\nimport edu.stanford.nlp.simple.Document\nimport org.apache.log4j.{ Level, Logger }\nimport scala.collection.JavaConversions._\nimport org.apache.spark.{ SparkConf, SparkContext }\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.linalg.{ Vector => MLVector }\nimport org.apache.spark.mllib.clustering.{ DistributedLDAModel, EMLDAOptimizer, LDA, OnlineLDAOptimizer }\nimport org.apache.spark.mllib.linalg.{ Vector, Vectors }\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{ Row, SparkSession }\n\nobject topicModellingwithLDA {\n  def main(args: Array[String]): Unit = {\n    val lda = new LDAforTM() // actual computations are done here\n    val defaultParams = Params().copy(input = \"data/docs/\") \n    // Loading the parameters to train the LDA model\n    lda.run(defaultParams) // Training the LDA model with the default\n                              parameters.\n  }\n}\n//Setting the parameters before training the LDA model\ncaseclass Params(input: String = \"\",\n                 k: Int = 5,\n                 maxIterations: Int = 20,\n                 docConcentration: Double = -1,\n                 topicConcentration: Double = -1,\n                 vocabSize: Int = 2900000,\n                 stopwordFile: String = \"data/docs/stopWords.txt\",\n                 algorithm: String = \"em\",\n                 checkpointDir: Option[String] = None,\n                 checkpointInterval: Int = 10)\n\n// actual computations for topic modeling are done here\nclass LDAforTM() {\n  val spark = SparkSession\n              .builder\n              .master(\"local[*]\")\n              .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n              .appName(\"LDA for topic modelling\")\n              .getOrCreate()\n\n  def run(params: Params): Unit = {\n    Logger.getRootLogger.setLevel(Level.WARN)\n    // Load documents, and prepare them for LDA.\n    val preprocessStart = System.nanoTime()\n    val (corpus, vocabArray, actualNumTokens) = preprocess(params\n                      .input, params.vocabSize, params.stopwordFile)\n    val actualCorpusSize = corpus.count()\n    val actualVocabSize = vocabArray.length\n    val preprocessElapsed = (System.nanoTime() - preprocessStart) / 1e9\n    corpus.cache() //will be reused later steps\n    println()\n    println(\"Training corpus summary:\")\n    println(\"-------------------------------\")\n    println(\"Training set size: \" + actualCorpusSize + \" documents\")\n    println(\"Vocabulary size: \" + actualVocabSize + \" terms\")\n    println(\"Number of tockens: \" + actualNumTokens + \" tokens\")\n    println(\"Preprocessing time: \" + preprocessElapsed + \" sec\")\n    println(\"-------------------------------\")\n    println()\n    // Instantiate an LDA model\n    val lda = new LDA()\n    val optimizer = params.algorithm.toLowerCase match {\n      case \"em\" => new EMLDAOptimizer\n      // add (1.0 / actualCorpusSize) to MiniBatchFraction be more\n         robust on tiny datasets.\n     case \"online\" => new OnlineLDAOptimizer()\n                  .setMiniBatchFraction(0.05 + 1.0 / actualCorpusSize)\n      case _ => thrownew IllegalArgumentException(\"Only em, online are\n                             supported but got ${params.algorithm}.\")\n    }\n    lda.setOptimizer(optimizer)\n      .setK(params.k)\n      .setMaxIterations(params.maxIterations)\n      .setDocConcentration(params.docConcentration)\n      .setTopicConcentration(params.topicConcentration)\n      .setCheckpointInterval(params.checkpointInterval)\n    if (params.checkpointDir.nonEmpty) {\n      spark.sparkContext.setCheckpointDir(params.checkpointDir.get)\n    }\n    val startTime = System.nanoTime()\n    //Start training the LDA model using the training corpus\n    val ldaModel = lda.run(corpus)\n    val elapsed = (System.nanoTime() - startTime) / 1e9\n    println(\"Finished training LDA model. Summary:\")\n    println(\"Training time: \" + elapsed + \" sec\")\n    if (ldaModel.isInstanceOf[DistributedLDAModel]) {\n      val distLDAModel = ldaModel.asInstanceOf[DistributedLDAModel]\n      val avgLogLikelihood = distLDAModel.logLikelihood /\n                             actualCorpusSize.toDouble\n      println(\"The average log likelihood of the training data: \" +\n              avgLogLikelihood)\n      println()\n    }\n    // Print the topics, showing the top-weighted terms for each topic.\n    val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 10)\n    println(topicIndices.length)\n    val topics = topicIndices.map {case (terms, termWeights) =>\n                 terms.zip(termWeights).map { case (term, weight) =>\n                 (vocabArray(term.toInt), weight) } }\n    var sum = 0.0\n    println(s\"${params.k} topics:\")\n    topics.zipWithIndex.foreach {\n      case (topic, i) =>\n      println(s\"TOPIC $i\")\n      println(\"------------------------------\")\n      topic.foreach {\n        case (term, weight) =>\n        term.replaceAll(\"\\\\s\", \"\")\n        println(s\"$term\\t$weight\")\n        sum = sum + weight\n      }\n      println(\"----------------------------\")\n      println(\"weight: \" + sum)\n      println()\n    }\n    spark.stop()\n  }\n  //Pre-processing of the raw texts\nimport org.apache.spark.sql.functions._\ndef preprocess(paths: String, vocabSize: Int, stopwordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n  import spark.implicits._\n  //Reading the Whole Text Files\n  val initialrdd = spark.sparkContext.wholeTextFiles(paths).map(_._2)\n  initialrdd.cache()\n  val rdd = initialrdd.mapPartitions { partition =>\n    val morphology = new Morphology()\n    partition.map {value => helperForLDA.getLemmaText(value,\n                                                      morphology)}\n  }.map(helperForLDA.filterSpecialCharacters)\n    rdd.cache()\n    initialrdd.unpersist()\n    val df = rdd.toDF(\"docs\")\n    df.show()\n    //Customizing the stop words\n    val customizedStopWords: Array[String] = if(stopwordFile.isEmpty) {\n      Array.empty[String]\n    } else {\n      val stopWordText = spark.sparkContext.textFile(stopwordFile)\n                            .collect()\n      stopWordText.flatMap(_.stripMargin.split(\",\"))\n    }\n    //Tokenizing using the RegexTokenizer\n    val tokenizer = new RegexTokenizer().setInputCol(\"docs\")\n                                       .setOutputCol(\"rawTokens\")\n    //Removing the Stop-words using the Stop Words remover\n    val stopWordsRemover = new StopWordsRemover()\n                       .setInputCol(\"rawTokens\").setOutputCol(\"tokens\")\n    stopWordsRemover.setStopWords(stopWordsRemover.getStopWords ++\n                                  customizedStopWords)\n    //Converting the Tokens into the CountVector\n    val countVectorizer = new CountVectorizer().setVocabSize(vocabSize)\n                        .setInputCol(\"tokens\").setOutputCol(\"features\")\n    val pipeline = new Pipeline().setStages(Array(tokenizer,\n                                    stopWordsRemover, countVectorizer))\n    val model = pipeline.fit(df)\n    val documents = model.transform(df).select(\"features\").rdd.map {\n      case Row(features: MLVector) => Vectors.fromML(features)\n    }.zipWithIndex().map(_.swap)\n    //Returning the vocabulary and tocken count pairs\n    (documents, model.stages(2).asInstanceOf[CountVectorizerModel]\n     .vocabulary, documents.map(_._2.numActives).sum().toLong)\n    }\n  }\n  object helperForLDA {\n    def filterSpecialCharacters(document: String) = \n      document.replaceAll(\"\"\"[! @ # $ % ^ & * ( ) _ + - − ,\n                          \" ' ; : . ` ? --]\"\"\", \" \")\n    def getLemmaText(document: String, morphology: Morphology) = {\n      val string = new StringBuilder()\n      val value =new Document(document).sentences().toList.flatMap{a =>\n      val words = a.words().toList\n      val tags = a.posTags().toList\n      (words zip tags).toMap.map { a =>\n        val newWord = morphology.lemma(a._1, a._2)\n        val addedWoed = if (newWord.length > 3) {\n          newWord\n        } else { \"\" }\n        string.append(addedWoed + \" \")\n      }\n    }\n    string.toString()\n  }\n}\n\n```"]