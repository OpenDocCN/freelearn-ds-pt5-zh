<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 10.  Putting It All Together"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch10" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 10.   Putting It All Together  </h1></div></div></div><p class="calibre11">Big data analytics is revolutionizing the way businesses are run and has paved the way for several hitherto unimagined opportunities. Almost every enterprise, individual researcher, or investigative journalist has lots of data to process. We need a concise approach to start from raw data and arrive at meaningful insights based on the questions at hand.</p><p class="calibre11">We have covered various aspects of data science using Apache Spark in previous chapters. We started off discussing big data analytics requirements and how Apache spark fits in. Gradually, we looked into the Spark programming model, RDDs, and DataFrame abstractions and learnt how unified data access is enabled by Spark datasets along with the streaming aspect of continuous applications. Then we covered the entire breadth of the data analysis life cycle using Apache Spark followed by machine learning. We learnt structured and unstructured data analytics on Spark and explored the visualization aspects for data engineers and scientists, as well as business users.</p><p class="calibre11">All the previously discussed chapters helped us understand one concise aspect per chapter. We are now equipped to traverse the entire data science life cycle. In this chapter, we shall take up an end-to-end case study and apply all that we have learned so far. We will not introduce any new concepts; this will help apply the knowledge gained so far and strengthen our understanding. However, we have reiterated some concepts without going into too much detail, to make this chapter self-contained. The topics covered in this chapter are roughly the same as the steps in the data analytics life cycle:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">A quick recap</li><li class="listitem">Introducing a case study</li><li class="listitem">Framing the business problem</li><li class="listitem">Data acquisition and data cleansing</li><li class="listitem">Developing the hypothesis</li><li class="listitem">Data exploration</li></ul></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Data preparation</li><li class="listitem">Model building</li><li class="listitem">Data visualization</li><li class="listitem">Communicating the results to business users</li><li class="listitem">Summary</li></ul></div><div class="calibre2" title="A quick recap"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec74" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>A quick recap</h1></div></div></div><p class="calibre11">We already discussed in detail the various steps involved in a typical data science project separately in different chapters. Let us quickly glance through what we have covered already and touch upon some important aspects. A high-level overview of the steps involved may appear as in the following figure:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_10_001.jpg" alt="A quick recap" class="calibre157"/></div><p class="calibre11">
</p><p class="calibre11">In the preceding pictorial representation, we have tried to explain the steps involved in a data science project at a higher level, mostly generic to many data science assignments. Many more substeps are actually present at every stage, but may differ from project to project.</p><p class="calibre11">It is very difficult for data scientists to find the best approach and steps to follow in the beginning. Generally, data science projects do not have a well-defined life cycle such as the <span class="strong"><strong class="calibre19">Software Development Life Cycle</strong></span> (<span class="strong"><strong class="calibre19">SDLC</strong></span>). It is usually the case that data science projects get tramped into delivery delays with repeated hold-ups, as most of the steps in the life cycle are iterative. Also, there could be cyclic dependencies across teams that add to the complexity and cause delay in execution. However, while working on big data analytics projects, it is important as well as advantageous for data scientists to follow a well-defined data science workflow, irrespective of different business cases. This not only helps in an organized execution, but also helps us stay focused on the objective, as data science projects are inherently agile in most cases. Also, it is recommended that you plan for some level of research on data, domain, and algorithms for any given project.</p><p class="calibre11">In this chapter, we may not be able to accommodate all the granular steps in a single flow, but will address the important areas to give you a heads-up. We will try to look at some different coding examples that we have not covered in the previous chapters.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Introducing a case study"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec75" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Introducing a case study</h1></div></div></div><p class="calibre11">We will be exploring Academy Awards demographics in this chapter. You can download the data from the GitHub repository at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv">https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv</a>.</p><p class="calibre11">This dataset is based on the data provided at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.crowdflower.com/data-for-everyone">http://www.crowdflower.com/data-for-everyone</a>. It contains demographic details such as race, birthplace, and age. Rows are around 400 and it can be easily processed on a simple home computer, so you can do a <span class="strong"><strong class="calibre19">Proof of Concept</strong></span> (<span class="strong"><strong class="calibre19">POC</strong></span>) on executing a data science project on Spark.</p><p class="calibre11">Just start by downloading the file and inspecting the data. The data may look fine but as you take a closer look, you will notice that it is not "clean". For example, the date of birth column does not follow the same format. Some years are in two-digit format whereas some are in four-digit format. Birthplace does not have country for locations within the USA.</p><p class="calibre11">Likewise, you will also notice that the data looks skewed, with more "white" race people from the USA. But you might have felt that the trend has changed toward later years. You have not used any tools or techniques so far, just had a quick glance at the data. In the real world of data science, this seemingly trivial activity can be quite helpful further down the life cycle. You get to develop a feel for the data at hand and simultaneously hypothesize about the data. This brings you to the very first step in the workflow.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The business problem"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec76" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The business problem</h1></div></div></div><p class="calibre11">As iterated before, the most important aspect of any data science project is the question at hand. Having a clear understanding on <span class="strong"><em class="calibre22">what problem are we trying to solve?</em></span> This is critical to the success of the project. It also drives what is considered as relevant data and what is not. For example, in the current case study, if what we want to look at is the demographics, then movie name and person name are irrelevant. At times, there is no specific question at hand! <span class="strong"><em class="calibre22">What then?</em></span> Even when there is no specific question, the business may still have some objective, or data scientists and domain experts can work together to find the area of business to work on. To understand the business, functions, problem statement, or data, the data scientists start with "Questioning". It not only helps in defining the workflow, but helps in sourcing the right data to work on.</p><p class="calibre11">As an example, if the business focus is on demographics information, a formal business problem statement can be defined as:</p><p class="calibre11">
<span class="strong"><em class="calibre22">What is the impact of the race and country of origin among Oscar award winners?</em></span>
</p><p class="calibre11">In real-world, scenarios this step will not be this straightforward. Framing the right question is the collective responsibility of the data scientist, strategy team, domain experts, and the project owner. Since the whole exercise is futile if it does not serve the purpose, a data scientist has to consult all stakeholders and try to elicit as much information as possible from them. However, they may end up getting invaluable insights or "hunches". All of these combined form the core of the initial hypothesis and also help the data scientist to understand what exactly they should look for.</p><p class="calibre11">The situations where there is no specific question at hand that the business is trying to find an answer for are even more interesting to deal with, but can be complex in executing!</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Data acquisition and data cleansing"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec77" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data acquisition and data cleansing</h1></div></div></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Data acquisition</strong></span> is the logical next step. It may be as simple as selecting data from a single spreadsheet or it may be an elaborate several months project in itself. A data scientist has to collect as much relevant data as possible. 'Relevant' is the keyword here. Remember, more relevant data beats clever algorithms.</p><p class="calibre11">We have already covered how to source data from heterogeneous data sources and consolidate it to form a single data matrix, so we will not iterate the same fundamentals here. Instead, we source our data from a single source and extract a subset of it.</p><p class="calibre11">Now it is time to view the data and start cleansing it. The scripts presented in this chapter tend to be longer than the previous examples but still are no means of production quality. Real-world work requires a lot more exception checks and performance tuning:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala</strong></span>
</p><pre class="programlisting">//Load tab delimited file 
scala&gt; val fp = "&lt;YourPath&gt;/Oscars.txt" 
scala&gt; val init_data = spark.read.options(Map("header"-&gt;"true", "sep" -&gt; "\t","inferSchema"-&gt;"true")).csv(fp) 
//Select columns of interest and ignore the rest 
&gt;&gt;&gt; val awards = init_data.select("birthplace", "date_of_birth", 
        "race_ethnicity","year_of_award","award").toDF( 
         "birthplace","date_of_birth","race","award_year","award") 
awards: org.apache.spark.sql.DataFrame = [birthplace: string, date_of_birth: string ... 3 more fields] 
//register temporary view of this dataset 
scala&gt; awards.createOrReplaceTempView("awards") 
 
//Explore data 
&gt;&gt;&gt; awards.select("award").distinct().show(10,false) //False =&gt; do not truncate 
+-----------------------+                                                        
|award                  | 
+-----------------------+ 
|Best Supporting Actress| 
|Best Director          | 
|Best Actress           | 
|Best Actor             | 
|Best Supporting Actor  | 
+-----------------------+ 
//Check DOB quality. Note that length varies based on month name 
scala&gt; spark.sql("SELECT distinct(length(date_of_birth)) FROM awards ").show() 
+---------------------+                                                          
|length(date_of_birth)| 
+---------------------+ 
|                   15| 
|                    9| 
|                    4| 
|                    8| 
|                   10| 
|                   11| 
+---------------------+ 
 
//Look at the value with unexpected length 4 Why cant we show values for each of the length type ?  
scala&gt; spark.sql("SELECT date_of_birth FROM awards WHERE length(date_of_birth) = 4").show() 
+-------------+ 
|date_of_birth| 
+-------------+ 
|         1972| 
+-------------+ 
//This is an invalid date. We can either drop this record or give some meaningful value like 01-01-1972 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python</strong></span>
</p><pre class="programlisting">    //Load tab delimited file
    &gt;&gt;&gt; init_data = spark.read.csv("&lt;YOURPATH&gt;/Oscars.txt",sep="\t",header=True)
    //Select columns of interest and ignore the rest
    &gt;&gt;&gt; awards = init_data.select("birthplace", "date_of_birth",
            "race_ethnicity","year_of_award","award").toDF(
             "birthplace","date_of_birth","race","award_year","award")
    //register temporary view of this dataset
    &gt;&gt;&gt; awards.createOrReplaceTempView("awards")
    scala&gt;
    //Explore data
    &gt;&gt;&gt; awards.select("award").distinct().show(10,False) //False =&gt; do not truncate
    +-----------------------+                                                       
    |award                  |
    +-----------------------+
    |Best Supporting Actress|
    |Best Director          |
    |Best Actress           |
    |Best Actor             |
    |Best Supporting Actor  |
    +-----------------------+
    //Check DOB quality
    &gt;&gt;&gt; spark.sql("SELECT distinct(length(date_of_birth)) FROM awards ").show()
    +---------------------+                                                         
    |length(date_of_birth)|
    +---------------------+
    |                   15|
    |                    9|
    |                    4|
    |                    8|
    |                   10|
    |                   11|
    +---------------------+
    //Look at the value with unexpected length 4. Note that length varies based on month name
    &gt;&gt;&gt; spark.sql("SELECT date_of_birth FROM awards WHERE length(date_of_birth) = 4").show()
    +-------------+
    |date_of_birth|
    +-------------+
    |         1972|
    +-------------+
    //This is an invalid date. We can either drop this record or give some meaningful value like 01-01-1972
</pre><p class="calibre11">The preceding code snippet downloads a tab-separated text file, loads the desired columns into a DataFrame, and registers a temporary table. The rest of the code is very similar to basic SQL statements that just explored data.</p><p class="calibre11">Most of the datasets contain a <code class="literal">date</code> field and unless they come from a single, controlled data source, it is highly likely that they will differ in their formats and are almost always a candidate for cleaning.</p><p class="calibre11">For the dataset at hand, you might also have noticed that<code class="literal"> date_of_birth</code> and <code class="literal">birthplace</code> require a lot of cleaning. The following code shows two <span class="strong"><strong class="calibre19">user-defined functions</strong></span> (<span class="strong"><strong class="calibre19">UDFs</strong></span>) that clean <code class="literal">date_of_birth</code> and <code class="literal">birthplace</code> respectively. These UDFs work on a single data element at a time and they are just ordinary Scala/Python functions. These user defined functions should be registered so that they can be used from within a SQL statement. The final step is to create a cleaned data frame that will participate in further analysis.</p><p class="calibre11">Notice the following logic for cleaning <code class="literal">birthplace.</code> It is a weak logic because we are assuming that any string ending with two characters is an American state. We have to compare them against a list of valid abbreviations. Similarly, assuming two-digit years are always from the twentieth century is another error-prone assumption. Depending on the use case, a data scientist/data engineer has to take a call whether retaining more rows is important or only quality data should be included. All such decisions should be neatly documented for reference:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//UDF to clean date 
//This function takes 2 digit year and makes it 4 digit 
// Any exception returns an empty string 
scala&gt; def fncleanDate(s:String) : String = {  
  var cleanedDate = "" 
  val dateArray: Array[String] = s.split("-") 
  try{    //Adjust year 
     var yr = dateArray(2).toInt 
     if (yr &lt; 100) {yr = yr + 1900 } //make it 4 digit 
     cleanedDate = "%02d-%s-%04d".format(dateArray(0).toInt, 
                dateArray(1),yr) 
     } catch { case e: Exception =&gt; None } 
     cleanedDate } 
fncleanDate: (s: String)String 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python:</strong></span>
</p><pre class="programlisting">    //This function takes 2 digit year and makes it 4 digit
    // Any exception returns an empty string
    &gt;&gt;&gt; def fncleanDate(s):
          cleanedDate = ""
          dateArray = s.split("-")
          try:    //Adjust year
             yr = int(dateArray[2])
             if (yr &lt; 100):
                  yr = yr + 1900 //make it 4 digit
             cleanedDate = "{0}-{1}-{2}".format(int(dateArray[0]),
                      dateArray[1],yr)
          except :
              None
          return cleanedDate
</pre><p class="calibre11">The UDF to clean date accepts a hyphenated date string and splits it. If the last component, which is the year, is two digits long, then it is assumed to be a twentieth-century date and 1900 is added to bring it to four-digit format.</p><p class="calibre11">The following UDF appends the country as USA if the country string is either New York City or the last component is two characters long, where it is assumed to be a state in the USA:</p><pre class="programlisting">//UDF to clean birthplace 
// Data explorartion showed that  
// A. Country is omitted for USA 
// B. New York City does not have State code as well 
//This function appends country as USA if 
// A. the string contains New York City  (OR) 
// B. if the last component is of length 2 (eg CA, MA) 
scala&gt; def fncleanBirthplace(s: String) : String = { 
        var cleanedBirthplace = "" 
        var strArray : Array[String] =  s.split(" ") 
        if (s == "New York City") 
           strArray = strArray ++ Array ("USA") 
        //Append country if last element length is 2 
        else if (strArray(strArray.length-1).length == 2) 
            strArray = strArray ++ Array("USA") 
        cleanedBirthplace = strArray.mkString(" ") 
        cleanedBirthplace } 
</pre><p class="calibre11">Python:</p><pre class="programlisting">    &gt;&gt;&gt; def fncleanBirthplace(s):
            cleanedBirthplace = ""
            strArray = s.split(" ")
            if (s == "New York City"):
                strArray += ["USA"]  //Append USA
            //Append country if last element length is 2
            elif (len(strArray[len(strArray)-1]) == 2):
                strArray += ["USA"]
            cleanedBirthplace = " ".join(strArray)
            return cleanedBirthplace
</pre><p class="calibre11">The UDFs should be registered if you want to access them from SELECT strings:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//Register UDFs 
scala&gt; spark.udf.register("fncleanDate",fncleanDate(_:String)) 
res10: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType))) 
scala&gt; spark.udf.register("fncleanBirthplace", fncleanBirthplace(_:String)) 
res11: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType))) 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python:</strong></span>
</p><pre class="programlisting">    &gt;&gt;&gt; from pyspark.sql.types import StringType
    &gt;&gt;&gt; sqlContext.registerFunction("cleanDateUDF",fncleanDate, StringType())
    &gt;&gt;&gt; sqlContext.registerFunction( "cleanBirthplaceUDF",fncleanBirthplace, StringType())
</pre><p class="calibre11">Clean the data frame using the UDFs. Perform the following cleanup operations:</p><div class="calibre2"><ol class="orderedlist"><li class="listitem1">Call UDFs <code class="literal">fncleanDate</code> and <code class="literal">fncleanBirthplace</code> to fix birthplace and country.</li><li class="listitem1">Subtract birth year from <code class="literal">award_year</code> to get <code class="literal">age</code> at the time of receiving the award.</li><li class="listitem1">Retain <code class="literal">race</code> and <code class="literal">award</code> as they are.</li></ol></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//Create cleaned data frame 
scala&gt; var cleaned_df = spark.sql ( 
            """SELECT fncleanDate (date_of_birth) dob, 
               fncleanBirthplace(birthplace) birthplace, 
               substring_index(fncleanBirthplace(birthplace),' ',-1)  
                               country, 
               (award_year - substring_index(fncleanDate( date_of_birth),'-',-1)) age, race, award FROM awards""") 
cleaned_df: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 4 more fields] 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python:</strong></span>
</p><pre class="programlisting">//Create cleaned data frame 
&gt;&gt;&gt; from pyspark.sql.functions import substring_index&gt;&gt;&gt; cleaned_df = spark.sql (            """SELECT cleanDateUDF (date_of_birth) dob,               cleanBirthplaceUDF(birthplace) birthplace,               substring_index(cleanBirthplaceUDF(birthplace),' ',-1) country,               (award_year - substring_index(cleanDateUDF( date_of_birth),               '-',-1)) age, race, award FROM awards""")</pre><p class="calibre11">The last line requires some explanation. The UDFs are used similar to SQL functions and the expressions are aliased to meaningful names. We have added a computed column <code class="literal">age</code> because we would like to validate the impact of age also. The <code class="literal">substring_index</code> function  searches the first argument for the second argument. <code class="literal">-1</code> indicates to look for the first occurrence from the right.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Developing the hypothesis"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec78" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Developing the hypothesis</h1></div></div></div><p class="calibre11">A hypothesis is your best guess about what the outcome will be. You form your initial hypothesis based on the question, conversations with stakeholders, and also by looking at the data. You may form one or more hypotheses for a given problem. This initial hypothesis serves as a roadmap that guides you through the exploratory analysis. Developing a hypothesis is very important to statistically approve or not approve a statement, and not just by looking at the data as a data matrix or even through visuals. This is because our perception built by just looking at the data may be incorrect and rather deceptive at times.</p><p class="calibre11">Now you know that your final result may or may not prove the hypothesis to be correct. Coming to the case study we have considered for this lesson, we arrive at the following initial hypotheses:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Award winners are mostly white</li><li class="listitem">Most of the award winners are from the USA</li><li class="listitem">Best actors and actresses tend to be younger than best directors</li></ul></div><p class="calibre11">Now that we have formalized our hypotheses, we are all set to move forward with the next steps in the life cycle..</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Data exploration"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec79" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data exploration</h1></div></div></div><p class="calibre11">Now that we have a clean data frame with relevant data and the initial hypothesis, it is time to really explore what we have. The DataFrames abstraction provides functions such as <code class="literal">group by</code> out of the box for you to look around. You may register the cleaned data frame as a table and run the time-tested SQL statements to do just the same.</p><p class="calibre11">This is also the time to plot a few graphs. This phase of visualization is the exploratory analysis mentioned in the data visualization chapter. The objectives of this exploration are greatly influenced by the initial information you garner from the business stakeholders and the hypothesis. In other words, your discussions with the stakeholders help you know what to look for.</p><p class="calibre11">There are some general guidelines that are applicable for almost all data science assignments, but again subjective to different use cases. Let us look at some generic ones:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Look for missing data and treat it. We have already discussed various ways to do this in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch05.xhtml" title="Chapter 5. Data Analysis on Spark">Chapter 5</a>, <span class="strong"><em class="calibre22">Data Analysis on Spark</em></span>.</li><li class="listitem">Find the outliers in the dataset and treat them. We have discussed this aspect as well. Please note that there are cases where what we think of as outliers and normal data points may change depending on the use case.</li><li class="listitem">Perform univariate analysis, wherein you explore each variable in the dataset separately. Frequency distribution or percentile distribution are quite common. Perhaps plot some graphs to get a better idea. This will also help you prepare your data before getting into data modeling.</li><li class="listitem">Validate your initial hypothesis.</li><li class="listitem">Check minimum and maximum values of numerical data. If the variation is too high in any column, that could be a candidate for data normalization or scaling.</li></ul></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Check distinct values in categorical data (string values such as city names) and their frequencies. If there are too many distinct values (aka levels) in any column, you may have to look for ways to reduce the number of levels. If one level is occurring almost always, then this column is not helping the model to differentiate between the possible outcomes. Such columns are likely candidates for removal. At the exploration stage, you just figure out such candidate columns and let the data preparation phase take care of the actual action.</li></ul></div><p class="calibre11">In our current dataset, we do not have any missing data and we do not have any numerical data that might create any challenge. However, some missing values might creep in when invalid dates are processed. So, the following code covers the remaining action items. This code assumes that <code class="literal">cleaned_df</code> is already created:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala/Python:</strong></span>
</p><pre class="programlisting">cleaned_df = cleaned_df.na.drop //Drop rows with missing values 
cleaned_df.groupBy("award","country").count().sort("country","award","count").show(4,False) 
+-----------------------+---------+-----+                                        
|award                  |country  |count| 
+-----------------------+---------+-----+ 
|Best Actor             |Australia|1    | 
|Best Actress           |Australia|1    | 
|Best Supporting Actor  |Australia|1    | 
|Best Supporting Actress|Australia|1    | 
+-----------------------+---------+-----+ 
//Re-register data as table 
cleaned_df.createOrReplaceTempView("awards") 
//Find out levels (distinct values) in each categorical variable 
spark.sql("SELECT count(distinct country) country_count, count(distinct race) race_count, count(distinct award) award_count from awards").show() 
+-------------+----------+-----------+                                           
|country_count|race_count|award_count| 
+-------------+----------+-----------+ 
|           34|         6|          5| 
+-------------+----------+-----------+ 
</pre><p class="calibre11">The following visualizations correspond to the initial hypotheses. Note that two of our hypotheses were found to be correct but the third one was not. These visualizations are created using zeppelin:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_10_002.jpg" alt="Data exploration" class="calibre158"/></div><p class="calibre11">
</p><p class="calibre11">Note here that the all hypotheses cannot just be validated through visuals, as they can be deceptive at times. So proper statistical tests such as t-tests, ANOVA, Chi-squared tests, correlation tests, and so on need to be performed as applicable. We will not get into the details in this section. Please refer to <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch05.xhtml" title="Chapter 5. Data Analysis on Spark">Chapter 5</a>, <span class="strong"><em class="calibre22">Data Analysis on Spark</em></span>, for further details.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Data preparation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec80" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data preparation</h1></div></div></div><p class="calibre11">The data exploration stage helped us identify all the issues that needed to be fixed before proceeding to the modeling stage. Each individual issue requires careful thought and deliberation to choose the best fix. Here are some common issues and the possible fixes. The best fix is dependent on the problem at hand and/or the business context.</p><div class="calibre2" title="Too many levels in a categorical variable"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch10lvl2sec114" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Too many levels in a categorical variable</h2></div></div></div><p class="calibre11">This is one of the most common issues we face. The treatment of this issue is dependent on multiple factors:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">If the column is almost always unique, for example, it is a transaction ID or timestamp, then it does not participate in modeling unless you are deriving new features from it. You may safely drop the column without losing any information content. You usually drop it during the data cleansing stage itself.</li><li class="listitem">If it is possible to replace the levels with coarser-grained levels (for example, state or country instead of city) that make sense in the current context, then usually that is the best way to fix this issue.</li><li class="listitem">You may want to add dummy columns with 0 or 1 values for each distinct level. For example, if you have 100 levels in a single column, you add 100 columns instead. At most, one column will have 1 at any observation (row). This is called <span class="strong"><strong class="calibre19">one-hot encoding</strong></span> and Spark provides this out of the box through the <code class="literal">ml.features</code> package.</li><li class="listitem">Another option is to retain the most frequent levels. You may even attach each of these levels to one of the dominant levels that is somehow considered "nearer" to this level. Also, you may bundle up the remaining into a single bucket, say, <code class="literal">Others</code>.</li><li class="listitem">There is no hard and fast rule for an absolute limit to the number of levels. It depends on what granularity you require in each individual feature and the performance constraints.</li></ul></div><p class="calibre11">The current dataset has too many levels in the categorical variable <code class="literal">country</code>. We chose to retain the most frequent levels and bundle the remaining into <code class="literal">Others</code>:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//Country has too many values. Retain top ones and bundle the rest 
//Check out top 6 countries with most awards. 
scala&gt; val top_countries_df = spark.sql("SELECT country, count(*) freq FROM awards GROUP BY country ORDER BY freq DESC LIMIT 6") 
top_countries_df: org.apache.spark.sql.DataFrame = [country: string, freq: bigint] 
scala&gt; top_countries_df.show() 
+-------+----+                                                                   
|country|freq| 
+-------+----+ 
|    USA| 289| 
|England|  57| 
| France|   9| 
| Canada|   8| 
|  Italy|   7| 
|Austria|   7| 
+-------+----+ 
//Prepare top_countries list 
scala&gt; val top_countries = top_countries_df.select("country").collect().map(x =&gt; x(0).toString) 
top_countries: Array[String] = Array(USA, England, New York City, France, Canada, Italy) 
//UDF to fix country. Retain top 6 and bundle the rest into "Others" 
scala&gt; import org.apache.spark.sql.functions.udf 
import org.apache.spark.sql.functions.udf 
scala &gt; val setCountry = udf ((s: String) =&gt; 
        { if (top_countries.contains(s)) {s} else {"Others"}}) 
setCountry: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType))) 
//Apply udf to overwrite country 
scala&gt; cleaned_df = cleaned_df.withColumn("country", setCountry(cleaned_df("country"))) 
cleaned_df: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 4 more fields] 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python:</strong></span>
</p><pre class="programlisting">    //Check out top 6 countries with most awards.
    &gt;&gt;&gt; top_countries_df = spark.sql("SELECT country, count(*) freq FROM awards GROUP BY country ORDER BY freq DESC LIMIT 6")
    &gt;&gt;&gt; top_countries_df.show()
    +-------+----+                                                                  
    |country|freq|
    +-------+----+
    |    USA| 289|
    |England|  57|
    | France|   9|
    | Canada|   8|
    |  Italy|   7|
    |Austria|   7|
    +-------+----+
    &gt;&gt;&gt; top_countries = [x[0] for x in top_countries_df.select("country").collect()]
    //UDF to fix country. Retain top 6 and bundle the rest into "Others"
    &gt;&gt;&gt; from pyspark.sql.functions import udf
    &gt;&gt;&gt; from pyspark.sql.types import StringType
    &gt;&gt;&gt; setCountry = udf(lambda s: s if s in top_countries else "Others", StringType())
    //Apply UDF
    &gt;&gt;&gt; cleaned_df = cleaned_df.withColumn("country", setCountry(cleaned_df["country"]))
</pre></div><div class="calibre2" title="Numerical variables with too much variation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch10lvl2sec115" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Numerical variables with too much variation</h2></div></div></div><p class="calibre11">Sometimes numerical data values may vary by several orders of magnitude. For example, if you are looking at the annual income of individuals, it may vary a lot. Z-score normalization (standardization) and min-max scaling are two popular choices to deal with such data. Spark includes both of these transformations out of the box in the <code class="literal">ml.features</code> package.</p><p class="calibre11">Our current dataset does not have any such variable. The only numerical variable we have is age and its value is uniformly two digits. That's one less issue to fix.</p><p class="calibre11">Please note that it is not always necessary to normalize such data. If you are comparing two variables that are in two different scales, or if you are using a clustering algorithm or SVM classifier, or any other scenario where there is really a need to normalize the data, you may normalize the data.</p><div class="calibre2" title="Missing data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch10lvl3sec70" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Missing data</h3></div></div></div><p class="calibre11">This is a major area of concern. Any observations where the target itself is missing should be removed from the training data. The remaining observations may be retained with some imputed values or removed as per the requirements. You should be very careful in imputing the missing values; it may lead to misleading output otherwise! It may seem very easy to just go ahead and substitute average values in the blank cells of a continuous variable, but this may not be the right approach.</p><p class="calibre11">Our current case study does not have any missing data so there is no scope for treating it. However, let us look at an example.</p><p class="calibre11">Let's assume you have a student's dataset that you are dealing with, and it has data from class-1 to class-5. If there are some missing <code class="literal">Age</code> values and you just find the average of the whole column and substitute, then that would rather become an outlier and could lead to vague results. You may choose to find the average of only the class that the student is in, and then impute that value. This is at least a better approach, but may not be a perfect one. In most of the cases, you will have to give weightage to other variables as well. If you do so, you may end up building a predictive model to find the missing values and this can be a great approach!</p></div><div class="calibre2" title="Continuous data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch10lvl3sec71" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Continuous data</h3></div></div></div><p class="calibre11">Numerical data is often continuous and must be discretized because it is a prerequisite to some of the algorithms. It is usually split into different buckets or ranges of values. However, there could be cases where you may not just uniformly bucket based on the range of your data, you may have to consider the variance or standard deviation or any other applicable reason to bucket properly. Now, deciding the number of buckets is also at the discretion of the data scientist, but that too needs careful analysis. Too few buckets reduces granularity and too many buckets is just about the same as having too many categorical levels. In our case study, <code class="literal">age</code> is an example of such data and we need to discretize it. We split it into different buckets. For example, look at this pipeline stage, which converts <code class="literal">age</code> to 10 buckets:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">scala&gt; val splits = Array(Double.NegativeInfinity, 35.0, 45.0, 55.0, 
          Double.PositiveInfinity) 
splits: Array[Double] = Array(-Infinity, 35.0, 45.0, 55.0, Infinity) 
scala&gt; val bucketizer = new Bucketizer().setSplits(splits). 
                 setInputCol("age").setOutputCol("age_buckets") 
bucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_a25c5d90ac14 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python:</strong></span>
</p><pre class="programlisting">    &gt;&gt;&gt; splits = [-float("inf"), 35.0, 45.0, 55.0,
                   float("inf")]
    &gt;&gt;&gt; bucketizer = Bucketizer(splits = splits, inputCol = "age",
                        outputCol = "age_buckets")
</pre></div><div class="calibre2" title="Categorical data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch10lvl3sec72" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Categorical data</h3></div></div></div><p class="calibre11">We have discussed the need for discretizing continuous data and converting it to categories or buckets. We have also discussed the introduction of dummy variables, one for each distinct value of a categorical variable. There is one more common data preparation practice where we convert categorical levels to numerical (discrete) data. This is required because many machine learning algorithms work with numerical data, integers, and real-valued numbers, or some other situation may demand it. So, we need to convert categorical data into numerical data.</p><p class="calibre11">There can be downsides to this approach. Introducing an order into inherently unordered data may not be logical at times. For example, assigning numbers such as 0, 1, 2, 3 to the colors "red", "green", "blue", and "black", respectively, does not make sense. This is because we cannot say that red is one unit distant from "green" and so is "green" from "blue"! If applicable, introducing dummy variables makes more sense in many such cases.</p></div><div class="calibre2" title="Preparing the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch10lvl3sec73" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Preparing the data</h3></div></div></div><p class="calibre11">Having discussed the common issues and possible fixes, let us see how to prepare our current dataset. We have already covered the too many levels issue related code fix. The following example shows the rest. It converts all the features into a single features column. It also sets aside some data for testing the models. This code heavily relies on the <code class="literal">ml.features</code> package, which was designed to support the data preparation phase. Note that this piece of code is just defining what needs to be done. The transformations are not carried out as yet. These will become stages in subsequently defined pipelines. Execution is deferred as late as possible, until the actual model is built. The Catalyst optimizer finds the optimal route to implement the pipeline:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//Define pipeline to convert categorical labels to numerical labels 
scala&gt; import org.apache.spark.ml.feature.{StringIndexer, Bucketizer, VectorAssembler} 
import org.apache.spark.ml.feature.{StringIndexer, Bucketizer, VectorAssembler} 
scala&gt; import org.apache.spark.ml.Pipeline 
import org.apache.spark.ml.Pipeline 
//Race 
scala&gt; val raceIdxer = new StringIndexer(). 
           setInputCol("race").setOutputCol("raceIdx") 
raceIdxer: org.apache.spark.ml.feature.StringIndexer = strIdx_80eddaa022e6 
//Award (prediction target) 
scala&gt; val awardIdxer = new StringIndexer(). 
         setInputCol("award").setOutputCol("awardIdx") 
awardIdxer: org.apache.spark.ml.feature.StringIndexer = strIdx_256fe36d1436 
//Country 
scala&gt; val countryIdxer = new StringIndexer(). 
         setInputCol("country").setOutputCol("countryIdx") 
countryIdxer: org.apache.spark.ml.feature.StringIndexer = strIdx_c73a073553a2 
 
//Convert continuous variable age to buckets 
scala&gt; val splits = Array(Double.NegativeInfinity, 35.0, 45.0, 55.0, 
          Double.PositiveInfinity) 
splits: Array[Double] = Array(-Infinity, 35.0, 45.0, 55.0, Infinity) 
 
scala&gt; val bucketizer = new Bucketizer().setSplits(splits). 
                 setInputCol("age").setOutputCol("age_buckets") 
bucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_a25c5d90ac14 
 
//Prepare numerical feature vector by clubbing all individual features 
scala&gt; val assembler = new VectorAssembler().setInputCols(Array("raceIdx", 
          "age_buckets","countryIdx")).setOutputCol("features") 
assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_8cf17ee0cd60 
 
//Define data preparation pipeline 
scala&gt; val dp_pipeline = new Pipeline().setStages( 
          Array(raceIdxer,awardIdxer, countryIdxer, bucketizer, assembler)) 
dp_pipeline: org.apache.spark.ml.Pipeline = pipeline_06717d17140b 
//Transform dataset 
scala&gt; cleaned_df = dp_pipeline.fit(cleaned_df).transform(cleaned_df) 
cleaned_df: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 9 more fields] 
//Split data into train and test datasets 
scala&gt; val Array(trainData, testData) = 
        cleaned_df.randomSplit(Array(0.7, 0.3)) 
trainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [dob: string, birthplace: string ... 9 more fields] 
testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [dob: string, birthplace: string ... 9 more fields] 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python:</strong></span>
</p><pre class="programlisting">    //Define pipeline to convert categorical labels to numcerical labels
    &gt;&gt;&gt; from pyspark.ml.feature import StringIndexer, Bucketizer, VectorAssembler
    &gt;&gt;&gt; from pyspark.ml import Pipelin
    //Race
    &gt;&gt;&gt; raceIdxer = StringIndexer(inputCol= "race", outputCol="raceIdx")
    //Award (prediction target)
    &gt;&gt;&gt; awardIdxer = StringIndexer(inputCol = "award", outputCol="awardIdx")
    //Country
    &gt;&gt;&gt; countryIdxer = StringIndexer(inputCol = "country", outputCol = "countryIdx")
    
    //Convert continuous variable age to buckets
    &gt;&gt;&gt; splits = [-float("inf"), 35.0, 45.0, 55.0,
                   float("inf")]
    &gt;&gt;&gt; bucketizer = Bucketizer(splits = splits, inputCol = "age",
                        outputCol = "age_buckets")
    &gt;&gt;&gt;
    //Prepare numerical feature vector by clubbing all individual features
    &gt;&gt;&gt; assembler = VectorAssembler(inputCols = ["raceIdx", 
              "age_buckets","countryIdx"], outputCol = "features")
    
    //Define data preparation pipeline
    &gt;&gt;&gt; dp_pipeline = Pipeline(stages = [raceIdxer,
             awardIdxer, countryIdxer, bucketizer, assembler])
    //Transform dataset
    &gt;&gt;&gt; cleaned_df = dp_pipeline.fit(cleaned_df).transform(cleaned_df)
    &gt;&gt;&gt; cleaned_df.columns
    ['dob', 'birthplace', 'country', 'age', 'race', 'award', 'raceIdx', 'awardIdx', 'countryIdx', 'age_buckets', 'features']
    
    //Split data into train and test datasets
    &gt;&gt;&gt; trainData, testData = cleaned_df.randomSplit([0.7, 0.3])
</pre><p class="calibre11">After carrying out all data preparation activity, you will end up with a completely numeric data with no missing values and with manageable levels in each attribute. You may have already dropped any attributes that may not add much value to the analysis on hand. This is what we call the <span class="strong"><strong class="calibre19">final data matrix</strong></span>. You are all set now to start modeling your data. So, first you split your source data into train data and test data. Models are "trained" using train data and "tested" using test data. Note that the split is random and you may end up with different train and test partitions if you redo the split.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Model building"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec81" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Model building</h1></div></div></div><p class="calibre11">A model is a representation of things, a rendering or description of reality. Just like a model of a physical building, data science models attempt to make sense of the reality; in this case, the reality is the underlying relationships between the features and the predicted variable. They may not be 100 percent accurate, but still very useful to give some deep insights into our business space based on the data.</p><p class="calibre11">There are several machine learning algorithms that help us model data and Spark provides many of them out of the box. However, which model to build is still a million dollar question. It depends on various factors, such as interpretability-accuracy trade-off, how much data you have at hand, categorical or numerical variables, time and memory constraints, and so on. In the following code example, we have just trained a few models at random to show you how it can be done.</p><p class="calibre11">We'll be predicting the award type based on race, age, and country. We'll be using the DecisionTreeClassifier, RandomForestClassifier, and OneVsRest algorithms. These three are chosen arbitrarily. All of them work with multiclass labels and are simple to understand. We have used the following evaluation metrics provided by the <code class="literal">ml</code> package:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Accuracy</strong></span>: The ratio of correctly predicted observations.</li></ul></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Weighted Precision</strong></span>: Precision is the ratio of correct positive observations to all positive observations. Weighted precision takes the frequency of individual classes into account.</li><li class="listitem"><span class="strong"><strong class="calibre19">Weighted Recall</strong></span>: Recall is the ratio of positives to actual positives. Actual positives are the sum of true positives and false negatives. Weighted Recall takes the frequency of individual classes into account.</li><li class="listitem"><span class="strong"><strong class="calibre19">F1</strong></span>: The default evaluation measure. This is the weighted average of Precision and Recall.</li></ul></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.Pipeline 
import org.apache.spark.ml.Pipeline 
scala&gt; import org.apache.spark.ml.classification.DecisionTreeClassifier 
import org.apache.spark.ml.classification.DecisionTreeClassifier 
 
//Use Decision tree classifier 
scala&gt; val dtreeModel = new DecisionTreeClassifier(). 
           setLabelCol("awardIdx").setFeaturesCol("features"). 
           fit(trainData) 
dtreeModel: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_76c9e80680a7) of depth 5 with 39 nodes 
 
//Run predictions using testData 
scala&gt; val dtree_predictions = dtreeModel.transform(testData) 
dtree_predictions: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 12 more fields] 
 
//Examine results. Your results may vary due to randomSplit 
scala&gt; dtree_predictions.select("award","awardIdx","prediction").show(4) 
+--------------------+--------+----------+ 
|               award|awardIdx|prediction| 
+--------------------+--------+----------+ 
|       Best Director|     1.0|       1.0| 
|        Best Actress|     0.0|       0.0| 
|        Best Actress|     0.0|       0.0| 
|Best Supporting A...|     4.0|       3.0| 
+--------------------+--------+----------+ 
 
//Compute prediction mismatch count 
scala&gt; dtree_predictions.filter(dtree_predictions("awardIdx") =!= dtree_predictions("prediction")).count() 
res10: Long = 88 
scala&gt; testData.count 
res11: Long = 126 
//Predictions match with DecisionTreeClassifier model is about 30% ((126-88)*100/126) 
 
 
//Train Random forest 
scala&gt; import org.apache.spark.ml.classification.RandomForestClassifier 
import org.apache.spark.ml.classification.RandomForestClassifier 
scala&gt; import org.apache.spark.ml.classification.RandomForestClassificationModel 
import org.apache.spark.ml.classification.RandomForestClassificationModel 
scala&gt; import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer} 
import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer} 
 
//Build model 
scala&gt; val RFmodel = new RandomForestClassifier(). 
        setLabelCol("awardIdx"). 
        setFeaturesCol("features"). 
        setNumTrees(6).fit(trainData) 
RFmodel: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_c6fb8d764ade) with 6 trees 
//Run predictions on the same test data using Random Forest model 
scala&gt; val RF_predictions = RFmodel.transform(testData) 
RF_predictions: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 12 more fields] 
//Check results 
scala&gt; RF_predictions.filter(RF_predictions("awardIdx") =!= RF_predictions("prediction")).count() 
res29: Long = 87 //Roughly the same as DecisionTreeClassifier 
 
//Try OneVsRest Logistic regression technique 
scala&gt; import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest} 
import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest} 
//This model requires a base classifier 
scala&gt; val classifier = new LogisticRegression(). 
            setLabelCol("awardIdx"). 
            setFeaturesCol("features"). 
            setMaxIter(30). 
            setTol(1E-6). 
            setFitIntercept(true) 
classifier: org.apache.spark.ml.classification.LogisticRegression = logreg_82cd24368c87 
 
//Fit OneVsRest model 
scala&gt; val ovrModel = new OneVsRest(). 
           setClassifier(classifier). 
           setLabelCol("awardIdx"). 
           setFeaturesCol("features"). 
           fit(trainData) 
ovrModel: org.apache.spark.ml.classification.OneVsRestModel = oneVsRest_e696c41c0bcf 
//Run predictions 
scala&gt; val OVR_predictions = ovrModel.transform(testData) 
predictions: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 10 more fields] 
//Check results 
scala&gt; OVR_predictions.filter(OVR_predictions("awardIdx") =!= OVR_predictions("prediction")).count()          
res32: Long = 86 //Roughly the same as other models 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python:</strong></span>
</p><pre class="programlisting">    &gt;&gt;&gt; from pyspark.ml import Pipeline
    &gt;&gt;&gt; from pyspark.ml.classification import DecisionTreeClassifier
    
    //Use Decision tree classifier
    &gt;&gt;&gt; dtreeModel = DecisionTreeClassifier(labelCol = "awardIdx", featuresCol="features").fit(trainData)
    
    //Run predictions using testData
    &gt;&gt;&gt; dtree_predictions = dtreeModel.transform(testData)
    
    //Examine results. Your results may vary due to randomSplit
    &gt;&gt;&gt; dtree_predictions.select("award","awardIdx","prediction").show(4)
    +--------------------+--------+----------+
    |               award|awardIdx|prediction|
    +--------------------+--------+----------+
    |       Best Director|     1.0|       4.0|
    |       Best Director|     1.0|       1.0|
    |       Best Director|     1.0|       1.0|
    |Best Supporting A...|     4.0|       3.0|
    +--------------------+--------+----------+
    
    &gt;&gt;&gt; dtree_predictions.filter(dtree_predictions["awardIdx"] != dtree_predictions["prediction"]).count()
    92
    &gt;&gt;&gt; testData.count()
    137
    &gt;&gt;&gt;
    //Predictions match with DecisionTreeClassifier model is about 31% ((133-92)*100/133)
    
    //Train Random forest
    &gt;&gt;&gt; from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel
    &gt;&gt;&gt; from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer
    &gt;&gt;&gt; from pyspark.ml.evaluation import MulticlassClassificationEvaluator
    
    //Build model
    &gt;&gt;&gt; RFmodel = RandomForestClassifier(labelCol = "awardIdx", featuresCol = "features", numTrees=6).fit(trainData)
    
    //Run predictions on the same test data using Random Forest model
    &gt;&gt;&gt; RF_predictions = RFmodel.transform(testData)
    //Check results
    &gt;&gt;&gt; RF_predictions.filter(RF_predictions["awardIdx"] != RF_predictions["prediction"]).count()
    94     //Roughly the same as DecisionTreeClassifier
    
    //Try OneVsRest Logistic regression technique
    &gt;&gt;&gt; from pyspark.ml.classification import LogisticRegression, OneVsRest
    
    //This model requires a base classifier
    &gt;&gt;&gt; classifier = LogisticRegression(labelCol = "awardIdx", featuresCol="features",
                  maxIter = 30, tol=1E-6, fitIntercept = True)
    //Fit OneVsRest model
    &gt;&gt;&gt; ovrModel = OneVsRest(classifier = classifier, labelCol = "awardIdx",
                    featuresCol = "features").fit(trainData)
    //Run predictions
    &gt;&gt;&gt; OVR_predictions = ovrModel.transform(testData)
    //Check results
    &gt;&gt;&gt; OVR_predictions.filter(OVR_predictions["awardIdx"] != OVR_predictions["prediction"]).count()
    90  //Roughly the same as other models
</pre><p class="calibre11">So far, we have tried a few models and found that they gives us roughly the same performance. There are various other ways to validate the model performance. This again depends on the algorithm you have used, the business context, and the outcome produced. Let us look at some metrics that are offered out of the box in the <code class="literal">spark.ml.evaluation</code> package:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator 
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator 
//F1 
scala&gt; val f1_eval = new MulticlassClassificationEvaluator(). 
                     setLabelCol("awardIdx") //Default metric is F1 
f1_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_e855a949bb0e 
 
//WeightedPrecision 
scala&gt; val wp_eval = new MulticlassClassificationEvaluator(). 
                     setMetricName("weightedPrecision").setLabelCol("awardIdx") 
wp_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_44fd64e29d0a 
 
//WeightedRecall 
scala&gt; val wr_eval = new MulticlassClassificationEvaluator(). 
                     setMetricName("weightedRecall").setLabelCol("awardIdx") 
wr_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_aa341966305a 
//Compute measures for all models 
scala&gt; val f1_eval_list = List (dtree_predictions, RF_predictions, OVR_predictions) map ( 
           x =&gt; f1_eval.evaluate(x)) 
f1_eval_list: List[Double] = List(0.2330854098674473, 0.2330854098674473, 0.2330854098674473) 
scala&gt; val wp_eval_list = List (dtree_predictions, RF_predictions, OVR_predictions) map ( 
           x =&gt; wp_eval.evaluate(x)) 
wp_eval_list: List[Double] = List(0.2661599224979506, 0.2661599224979506, 0.2661599224979506) 
 
scala&gt; val wr_eval_list = List (dtree_predictions, RF_predictions, OVR_predictions) map ( 
           x =&gt; wr_eval.evaluate(x)) 
wr_eval_list: List[Double] = List(0.31746031746031744, 0.31746031746031744, 0.31746031746031744) 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python:</strong></span>
</p><pre class="programlisting">    &gt;&gt;&gt; from pyspark.ml.evaluation import MulticlassClassificationEvaluator
    
    //F1
    &gt;&gt;&gt; f1_eval = MulticlassClassificationEvaluator(labelCol="awardIdx") //Default metric is F1
    //WeightedPrecision
    &gt;&gt;&gt; wp_eval = MulticlassClassificationEvaluator(labelCol="awardIdx", metricName="weightedPrecision")
    //WeightedRecall
    &gt;&gt;&gt; wr_eval = MulticlassClassificationEvaluator(labelCol="awardIdx", metricName="weightedRecall")
    //Accuracy
    &gt;&gt;&gt; acc_eval = MulticlassClassificationEvaluator(labelCol="awardIdx", metricName="Accuracy")
    //Compute measures for all models
    &gt;&gt;&gt; f1_eval_list = [ f1_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]
    &gt;&gt;&gt; wp_eval_list = [ wp_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]
    &gt;&gt;&gt; wr_eval_list = [ wr_eval.evaluate(x) for x in <span class="strong"><strong class="calibre19">[dtree_predictions, </strong></span>RF_predictions, OVR_predictions]]
    //Print results for DecisionTree, Random Forest and OneVsRest
    &gt;&gt;&gt; f1_eval_list
    [0.2957949866055487, 0.2645186821042419, 0.2564967990214734]
    &gt;&gt;&gt; wp_eval_list
    [0.3265407181548341, 0.31914852065228005, 0.25295826631254753]
    &gt;&gt;&gt; wr_eval_list
    [0.3082706766917293, 0.2932330827067669, 0.3233082706766917]
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Output:</strong></span>
</p><div class="informaltable"><table border="1" class="calibre12"><colgroup class="calibre13"><col class="calibre14"/><col class="calibre14"/><col class="calibre14"/><col class="calibre14"/></colgroup><tbody class="calibre15"><tr class="calibre16"><td class="calibre17">
</td><td class="calibre17">
<p class="calibre18"><span class="strong"><strong class="calibre19">Decision tree</strong></span></p>
</td><td class="calibre17">
<p class="calibre18"><span class="strong"><strong class="calibre19">Random Forest</strong></span></p>
</td><td class="calibre17">
<p class="calibre18"><span class="strong"><strong class="calibre19">OneVsRest</strong></span></p>
</td></tr><tr class="calibre20"><td class="calibre17">
<p class="calibre18">F1</p>
</td><td class="calibre17">
<p class="calibre18">0.29579</p>
</td><td class="calibre17">
<p class="calibre18">0.26451</p>
</td><td class="calibre17">
<p class="calibre18">0.25649</p>
</td></tr><tr class="calibre16"><td class="calibre17">
<p class="calibre18">WeightedPrecision</p>
</td><td class="calibre17">
<p class="calibre18">0.32654</p>
</td><td class="calibre17">
<p class="calibre18">0.26451</p>
</td><td class="calibre17">
<p class="calibre18">0.25295</p>
</td></tr><tr class="calibre21"><td class="calibre17">
<p class="calibre18">WeightedRecall</p>
</td><td class="calibre17">
<p class="calibre18">0.30827</p>
</td><td class="calibre17">
<p class="calibre18">0.29323</p>
</td><td class="calibre17">
<p class="calibre18">0.32330</p>
</td></tr></tbody></table></div><p class="calibre11">Upon validating the model performance, you will have to tune the model as much as possible. Now, tuning can happen both ways, at the data level and at the algorithm level. Feeding the right data that an algorithm expects is very important. The problem is that whatever data you feed in, the algorithm may still give some output - it never complains! So, apart from cleaning the data properly by treating missing values, treating univariate and multivariate outliers, and so on, you can create many more relevant features. This feature engineering is usually treated as the most important aspect of data science. Having decent domain expertise helps to engineer better features. Now, coming to the algorithmic aspect of tuning, there is always scope for working on optimizing the parameters that we pass to an algorithm. You may choose to use grid search to find the optimal parameters. Also, data scientists should question themselves on which loss function to use and why, and, out of GD, SGD, L-BFGS, and so on, which algorithm to use to optimize the loss function and why.</p><p class="calibre11">Please note that the preceding approach is intended just to demonstrate how to perform the steps on Spark. Selecting one algorithm over the other by just looking at the accuracy level may not be the best way. Selecting an algorithm depends on the type of data you are dealing with, the outcome variable, the business problem/requirement, computational challenges, interpretability, and many others.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Data visualization"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec82" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data visualization</h1></div></div></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Data visualization</strong></span> is something which is needed every now and then from the time you take on a data science assignment. Before building any model, preferably, you will have to visualize each variable to see their distributions to understand their characteristics and also find outliers so you can treat them. Simple tools such as scatterplot, box plot, bar chart, and so on are a few versatile, handy tools for such purposes. Also, you will have to use the visuals in most of the steps to ensure you are heading in the right direction.</p><p class="calibre11">Every time you want to collaborate with business users or stakeholders, it is always a good practice to convey your analysis through visuals. Visuals can accommodate more data in them in a more meaningful way and are inherently intuitive in nature.</p><p class="calibre11">Please note that most data science assignment outcomes are preferably represented through visuals and dashboards to business users. We already have a dedicated chapter on this topic, so we won't go deeper into it.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Communicating the results to business users"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec83" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Communicating the results to business users</h1></div></div></div><p class="calibre11">In real-life scenarios, it is mostly the case that you have to keep communicating with the business intermittently. You might have to build several models before concluding on a final production-ready model and communicate the results to the business.</p><p class="calibre11">An implementable model does not always depend on accuracy; you might have to bring in other measures such as sensitivity, specificity, or an ROC curve, and also represent your results through visuals such as a Gain/Lift chart or an output of a K-S test with statistical significance. Note that these techniques require business users' input. This input often guides the way you build the models or set thresholds. Let us look at a few examples to better understand how it works:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">If a regressor predicts the probability of an event occurring, then blindly setting the threshold to 0.5 and assuming anything above 0.5 is 1 and less than 0.5 is 0 may not be the best way! You may use an ROC curve and take a rather more scientific or logical decision.</li><li class="listitem">False-negative predictions for diagnosis of a cancer test may not be desirable at all! This is an extreme case of life risk.</li></ul></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">E-mail campaigning is cheaper compared to delivery of hard copies. So the business may decide to send e-mails to the recipients who are predicted with less than 0.5 (say 0.35) probability.</li></ul></div><p class="calibre11">Notice that the preceding decisions are influenced heavily by business users or the problem owners, and data scientists work closely with them to take a call on such cases.</p><p class="calibre11">Again, as discussed already, the right visuals are the most preferred way to communicate the results to the business.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec84" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we have taken up a case study and completed the data analytics life cycle end to end. During the course of building a data product, we have applied the knowledge gained so far in the previous chapters. We have stated a business problem, formed an initial hypothesis, acquired data, and prepared it for model building. We have tried building multiple models and found a suitable model.</p><p class="calibre11">In the next chapter, which is also the final chapter, we will discuss building real-world applications using Spark.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch10lvl1sec85" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><p class="calibre11"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www2.sas.com/proceedings/forum2007/073-2007.pdf">http://www2.sas.com/proceedings/forum2007/073-2007.pdf</a>.</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/">https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/</a>.</p><p class="calibre11"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf">http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf</a>.</p></div></div>



  </body></html>