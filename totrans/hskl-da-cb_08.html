<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Clustering and Classification</h1></div></div></div><p>This chapter demonstrates algorithms that intelligently cluster and categorize data:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the k-means clustering algorithm</li><li class="listitem" style="list-style-type: disc">Implementing hierarchical clustering</li><li class="listitem" style="list-style-type: disc">Using a hierarchical clustering library</li><li class="listitem" style="list-style-type: disc">Finding the number of clusters</li><li class="listitem" style="list-style-type: disc">Clustering words by their lexemes</li><li class="listitem" style="list-style-type: disc">Classifying the parts of speech of words</li><li class="listitem" style="list-style-type: disc">Identifying key words in a corpus of text</li><li class="listitem" style="list-style-type: disc">Training a parts-of-speech tagger</li><li class="listitem" style="list-style-type: disc">Implementing a decision tree classifier</li><li class="listitem" style="list-style-type: disc">Implementing a k-Nearest Neighbors classifier</li><li class="listitem" style="list-style-type: disc">Visualizing points using Graphics.EasyPlot</li></ul></div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec98"/>Introduction</h1></div></div></div><div><img src="img/ch08.jpg" alt="Introduction"/></div><p>Computer algorithms are becoming better and better at analyzing large datasets. As their performance enhances, their ability to detect interesting patterns in data also improves. </p><p>The first few algorithms in this chapter demonstrate how to look at thousands of points and identify clusters. A <strong>cluster</strong><a id="id456" class="indexterm"/> is simply a congregation of points defined by how closely they lie together. This measure of "closeness" is entirely up to us. One of the most popular closeness metrics is the Euclidian distance. </p><p>We can understand clusters by looking up at the night sky and pointing at stars that appear together. Our ancestors found it convenient to name "clusters" of stars, of which we refer to as constellations. We will be finding our own constellations in the "sky" of data points.</p><p>This chapter also focuses on classifying words. We will label words by their parts of speech as well as topic.</p><p>We will implement our own decision tree to classify practical data. Lastly, we will visualize clusters and points using plotting libraries.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec99"/>Implementing the k-means clustering algorithm</h1></div></div></div><p>The k-means clustering algorithm partitions <a id="id457" class="indexterm"/>data into k different groups. These k groupings are called clusters, and the location of these clusters are adjusted iteratively. We compute the arithmetic mean of all the points in a group to obtain a centroid point that we use, replacing the previous cluster location. </p><p>Hopefully, after this succinct <a id="id458" class="indexterm"/>explanation, the name <em>k-means clustering</em> no longer sounds completely foreign. One of the best places to learn more about this algorithm is on <a id="id459" class="indexterm"/>Coursera: <a class="ulink" href="https://class.coursera.org/ml-003/lecture/78">https://class.coursera.org/ml-003/lecture/78</a>.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec265"/>How to do it…</h2></div></div></div><p>Create a new file, which we call <code class="literal">Main.hs</code>, and perform the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Import the following built-in libraries:<div><pre class="programlisting">import Data.Map (Map)
import qualified Data.Map as Map
import Data.List (minimumBy, sort, transpose)
import Data.Ord (comparing)</pre></div></li><li class="listitem">Define a type synonym for points shown as follows:<div><pre class="programlisting">type Point = [Double] </pre></div></li><li class="listitem">Define the Euclidian distance function between two points:<div><pre class="programlisting">dist :: Point -&gt; Point -&gt; Double

dist a b = sqrt $ sum $ map (^2) $ zipWith (-) a b</pre></div></li><li class="listitem">Define the assignment step in the k-means algorithm. Each point will be assigned to its closest centroid:<div><pre class="programlisting">assign :: [Point] -&gt; [Point] -&gt; Map Point [Point]

assign centroids points = 
  Map.fromListWith (++) [(assignPoint p, [p]) | p&lt;- points]

  where assignPoint p = 
    minimumBy (comparing (dist p)) centroids</pre></div></li><li class="listitem">Define the relocation step in the k-means algorithm. Each centroid is relocated to the arithmetic mean of its corresponding points:<div><pre class="programlisting">relocate :: Map Point [Point] -&gt; Map Point [Point]

relocate centroidsMap = 
  Map.foldWithKey insertCenter Map.empty centroidsMap
  where insertCenter _ ps m = Map.insert (center ps) ps m
        center [] = [0,0]
        center ps = map average (transpose ps)
        average xs = sum xs / fromIntegral (length xs)</pre></div></li><li class="listitem">Run the <a id="id460" class="indexterm"/>k-means algorithm repeatedly until the centroids no longer move around:<div><pre class="programlisting">kmeans :: [Point] -&gt; [Point] -&gt; [Point]

kmeans centroids points = 
if converged 
then centroids 
else kmeans (Map.keys newCentroidsMap) points

where converged = 
        all (&lt; 0.00001) $ zipWith dist 
             (sort centroids) (Map.keys newCentroidsMap)

      newCentroidsMap = 
        relocate (assign centroids points)

      equal a b = dist a b &lt; 0.00001 </pre></div></li><li class="listitem">Test out the clustering with a couple of hardcoded points. The usual way to implement k-means chooses the starting centroids randomly. However, in this recipe, we will simply take the first k points:<div><pre class="programlisting">main = do
let points = [ [0,0], [1,0], [0,1], [1,1]
             , [7,5], [9,6], [8,7] ]
let centroids = kmeans (take 2 points) points
print centroids </pre></div></li><li class="listitem">After the algorithm converges, the resulting centroids will be as follows:<div><pre class="programlisting">
<strong>$ runhaskell Main.hs</strong>

<strong>[[0.5,0.5],[8.0,6.0]]</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec266"/>How it works…</h2></div></div></div><p>The algorithm<a id="id461" class="indexterm"/> repeatedly follows two procedures until the clusters are found. The first procedure is to partition the points by assigning each point to its closest centroid. The following diagram shows the data assignment step. Initially, there are three centroids represented by a star, square, and circle around three different points. The first part of the algorithm assigns each point a corresponding centroid.</p><div><img src="img/6331OS_08_01.jpg" alt="How it works…"/></div><p>The next step is to relocate the centroids to the center, or arithmetic mean, of their corresponding points. In the following diagram, the arithmetic mean of each cluster is computed, and the centroid is shifted to the new center:</p><div><img src="img/6331OS_08_02.jpg" alt="How it works…"/></div><p>This algorithm continues until the centroids no longer move around. The final categorization of each point is the cluster to which each point belongs.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec267"/>There's more…</h2></div></div></div><p>Although easy<a id="id462" class="indexterm"/> to implement and understand, this algorithm has a couple of limitations. The output of the k-means clustering algorithm is sensitive to the initial centroids chosen. Also, using the Euclidian distance metric forces the clusters to be described only by circular regions. Another limitation of k-means clustering is that the initial number of clusters k must be specified by the user. The user should visualize the data and use their judgment to determine the number of clusters before beginning the algorithm. Moreover, the convergence condition for the algorithm is an issue for special edge-cases.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec268"/>See also</h2></div></div></div><p>For another type of clustering algorithm, see the next recipe on <em>Implementing hierarchical clustering</em>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec100"/>Implementing hierarchical clustering</h1></div></div></div><p>Another way to cluster<a id="id463" class="indexterm"/> data is by first assuming each data item as its own cluster. We can then take a step back and merge together two of the nearest clusters. This process forms a hierarchy of clusters.</p><p>Take, for example, an analogy relating to islands and water level. An island is nothing more than a mountain tip surrounded by water. Imagine we have islands scattered across a sea. If we were to<a id="id464" class="indexterm"/> slowly drop the water level of the sea, two nearby small islands would merge into a larger island because they are connected to the same mountain formation. We can stop the water level from dropping any time we have the desired number of larger islands.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec269"/>How to do it…</h2></div></div></div><p>In a new file, which we name <code class="literal">Main.hs</code>, insert this code:</p><div><ol class="orderedlist arabic"><li class="listitem">Import the built-in functions:<div><pre class="programlisting">import Data.Map (Map, (!), delete)
import qualified Data.Map as Map
import Data.Ord (comparing)
import Data.List (sort, tails, transpose, minimumBy)</pre></div></li><li class="listitem">Define a type synonym for points:<div><pre class="programlisting">type Point = [Double]</pre></div></li><li class="listitem">Define a convenience function to compute the arithmetic mean of list of points:<div><pre class="programlisting">center :: [Point] -&gt; Point

center points = map average (transpose points)
  where average xs = sum xs / fromIntegral (length xs)</pre></div></li><li class="listitem">Combine the two clusters that are nearest to each other:<div><pre class="programlisting">merge :: Map Point [Point] -&gt; Map Point [Point]

merge m = 
        Map.insert (center [a,b]) ((m ! a) ++ (m ! b)) newM

where (a,b) = nearest (Map.keys m)

            newM = Map.delete b (Map.delete a m)

            equal a b = dist a b &lt; 0.00001

            dist a b = sqrt $ sum $ map (^2) $ zipWith (-) a b

            nearest points = 
                  minimumBy (comparing (uncurry dist)) 
                  [(a, b) | (a : rest) &lt;- tails points, b &lt;- rest] </pre></div></li><li class="listitem">Run the hierarchical algorithm until there are k clusters:<div><pre class="programlisting">run :: Int -&gt; Map Point [Point] -&gt; Map Point [Point]

run k m = if length (Map.keys m) == k 
          then m 
          else run k (merge m)</pre></div></li><li class="listitem">Initialize <a id="id465" class="indexterm"/>so that every point is its own cluster:<div><pre class="programlisting">initialize :: [Point] -&gt; Map Point [Point]

initialize points = 
  foldl (\m p -&gt; Map.insert p [p] m) Map.empty points</pre></div></li><li class="listitem">Test the clustering algorithm on some input:<div><pre class="programlisting">main = do
      let points = [ [0,0], [1,0], [0,1], [1,1]
                     , [7,5], [9,6], [8,7]]
      let centroids = Map.keys $ run 2 (initialize points)
      print centroids</pre></div></li><li class="listitem">The algorithm will output the following centroids:<div><pre class="programlisting">
<strong>$ runhaskell Main.hs</strong>

<strong>[[0.5,0.5],[7.75,5.75]]</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec270"/>How it works…</h2></div></div></div><p>There are two main ways to implement hierarchical clustering. The algorithm described in this recipe implements the<a id="id466" class="indexterm"/> <em>agglomerative</em> bottom-up approach<a id="id467" class="indexterm"/>. Each point is pre-emptively considered to be a cluster, and at each step the two closest clusters merge together. However, another approach to implement is top-down in a<a id="id468" class="indexterm"/> <em>divisive</em> approach <a id="id469" class="indexterm"/>where every point starts in one massive cluster that iteratively splits the clusters.</p><p>In this recipe, we begin by first assuming that every point is its own cluster. Then we take a step back and merge two of the nearest clusters. This step repeats until a desired convergence state is reached. In our example, we stop once we have exactly two clusters. The following diagram shows the three iterations of a hierarchical clustering algorithm:</p><div><img src="img/6331OS_08_03.jpg" alt="How it works…"/></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec271"/>There's more…</h2></div></div></div><p>Like most <a id="id470" class="indexterm"/>clustering algorithms, the choice of distance metric greatly affects the results. In this recipe, we assumed the Euclidean metric, but depending on the data, perhaps the distance metric should be the Manhattan distance or cosine similarity.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec272"/>See also</h2></div></div></div><p>For a non-hierarchical clustering algorithm, see the previous recipe on <em>Implementing the k-means clustering algorithm</em>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec101"/>Using a hierarchical clustering library </h1></div></div></div><p>We will group together a list <a id="id471" class="indexterm"/>of points using a hierarchical clustering approach. We will start by assuming that each point is its own cluster. The two closest clusters merge together and the algorithm repeats until the stopping criteria is met. In this algorithm, we will use a library to run hierarchical clustering until there are a specific number of clusters remaining.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec273"/>Getting ready</h2></div></div></div><p>Install the hierarchical clustering package using cabal as follows (documentation is available at <a class="ulink" href="http://hackage.haskell.org/package/hierarchical-clustering">http://hackage.haskell.org/package/hierarchical-clustering</a>):</p><div><pre class="programlisting">
<strong>$ cabal install hierarchical-clustering</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec274"/>How to do it…</h2></div></div></div><p>Insert the following code in a new file, which we call <code class="literal">Main.hs</code>:</p><div><ol class="orderedlist arabic"><li class="listitem">Import the required library:<div><pre class="programlisting">import Data.Clustering.Hierarchical</pre></div></li><li class="listitem">Define a Point data type:<div><pre class="programlisting">data Point = Point [Double] deriving Show</pre></div></li><li class="listitem">Define the Euclidian distance metric:<div><pre class="programlisting">dist :: Point -&gt; Point -&gt; Distance
dist (Point a) (Point b) = sqrt $ sum $ map (^2) $ 
                           zipWith (-) a b</pre></div></li><li class="listitem">Print out the clusters:<div><pre class="programlisting">printCluster :: Dendrogram Point -&gt; Double -&gt; IO ()

printCluster clusters cut = do
         let es = map elements $ clusters `cutAt` cut
         mapM_ print es</pre></div></li><li class="listitem">Test the clustering algorithm on some points:<div><pre class="programlisting">main = do
        let points = 
map Point [ [0,0], [1,0], [0,1], [1,1]
          , [7,5], [9,6], [8,7] ]
        let clusters = dendrogram SingleLinkage points dist
        printCluster clusters 2.0</pre></div></li><li class="listitem">Each of the three clusters are printed out as lists of points:<div><pre class="programlisting">[Point [0.0,1.0], Point [1.0,0.0], Point [0.0,0.0], Point [1.0,1.0]] 
[Point [7.0,5.0]] 
[Point [9.0,6.0], Point [8.0,7.0]]</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec275"/>How it works…</h2></div></div></div><p>The <code class="literal">dendogram</code> function has the type <code class="literal">Linkage -&gt; [a] -&gt; (a -&gt; a -&gt; Distance) -&gt; Dendogram a</code>. The linkage describes how distance is calculated. In this recipe, we use <code class="literal">SingleLinkage</code> as the first argument, which means that the distance between two clusters is the minimum distance between all their elements.</p><p>The second argument is the list of points, followed by a distance metric. The result of this function is a <a id="id472" class="indexterm"/>
<strong>dendogram</strong>, otherwise referred to as a hierarchical tree diagram. We use the defined <code class="literal">printCluster</code> function<a id="id473" class="indexterm"/> to display the clusters.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec276"/>There's more…</h2></div></div></div><p>The other types of linkage in this library include the following mentioned along with their description present on Hackage:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">SingleLinkage</code>: This is<a id="id474" class="indexterm"/> the minimum distance between two clusters.<div><img src="img/6331OS_08_05.jpg" alt="There's more…"/></div><div><blockquote class="blockquote"><p><em>"O(n^2) time and O(n) space, using the SLINK algorithm. This algorithm is optimal in both space and time and gives the same answer as the naive algorithm using a distance matrix."</em></p></blockquote></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">CompleteLinkage</code>: This <a id="id475" class="indexterm"/>is the maximum distance between two clusters.<div><img src="img/6331OS_08_06.jpg" alt="There's more…"/></div><div><blockquote class="blockquote"><p><em>"O(n^3) time and O(n^2) space, using the naive algorithm with a distance matrix. Use CLINK if you need more performance."</em></p></blockquote></div></li><li class="listitem" style="list-style-type: disc">Complete linkage with <strong>CLINK</strong><a id="id476" class="indexterm"/> is the same as the previous linkage type, except that it uses a faster but not always optimal algorithm.<div><blockquote class="blockquote"><p><em>"O(n^2) time and O(n) space, using the CLINK algorithm. Note that this algorithm doesn't always give the same answer as the naive algorithm using a distance matrix, but it's much faster."</em></p></blockquote></div></li><li class="listitem" style="list-style-type: disc">UPGMA <a id="id477" class="indexterm"/>is the average distance between the two clusters.<div><img src="img/6331OS_08_07.jpg" alt="There's more…"/></div><div><blockquote class="blockquote"><p><em>"O(n^3) time and O(n^2) space, using the naive algorithm with a distance matrix."</em></p></blockquote></div></li><li class="listitem" style="list-style-type: disc">And lastly, FakeAverageLinkage<a id="id478" class="indexterm"/> is similar to the previous UPGMA linkage but weighs both clusters equally in its calculations.<div><blockquote class="blockquote"><p><em>"O(n^3) time and O(n^2) space, using the naive algorithm with a distance matrix."</em></p></blockquote></div></li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec277"/>See also</h2></div></div></div><p>To use our own hierarchical clustering algorithm, see the previous recipe on <em>Implementing hierarchical clustering</em>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec102"/>Finding the number of clusters</h1></div></div></div><p>Sometimes, we do <a id="id479" class="indexterm"/>not know the number of clusters in a dataset, yet most clustering algorithms require this information a priori. One way to find the number of clusters is to run the clustering algorithm on all possible number of clusters and compute the average variance of the clusters. We can then graph the average variance for the number of clusters, and identify the number of clusters by finding the first fluctuation of the curve.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec278"/>Getting ready</h2></div></div></div><p>Review the k-means recipe titled <em>Implementing the k-means clustering algorithm</em>. We will be using the <code class="literal">kmeans</code><a id="id480" class="indexterm"/> and <a id="id481" class="indexterm"/>
<code class="literal">assign</code> functions defined in that recipe.</p><p>Install the Statistics package from cabal:</p><div><pre class="programlisting">
<strong>$ cabal install statistics</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec279"/>How to do it…</h2></div></div></div><p>Create a new<a id="id482" class="indexterm"/> file and insert the following code. We name this file <code class="literal">Main.hs</code>.</p><div><ol class="orderedlist arabic"><li class="listitem">Import the <code class="literal">variance</code> function<a id="id483" class="indexterm"/> and the helper <a id="id484" class="indexterm"/><code class="literal">fromList</code> function:<div><pre class="programlisting">import Statistics.Sample (variance)
import Data.Vector.Unboxed (fromList)</pre></div></li><li class="listitem">Compute the average of the variance of each cluster:<div><pre class="programlisting">avgVar points centroids = avg [variance . fromList $
  map (dist c) ps | (c, ps) &lt;- Map.assocs m]
  where m = assign centroids points
           avg xs = (sum xs) / (fromIntegral (length xs)) </pre></div></li><li class="listitem">In <code class="literal">main</code>, define a list of points. Notice how there appears to be three clusters:<div><pre class="programlisting">main = do
  let points = [ [0,0], [1,0], [0,1]
                       , [20,0], [21,0], [20,1]
                       , [40,5], [40,6], [40,8] ]</pre></div></li><li class="listitem">Get the average of the variance of each set of clusters:<div><pre class="programlisting">      let centroids = [ kmeans (take k points) points | 
                           k &lt;- [1..length points] ]
      let avgVars = map (avgVar points) centroids
      print avgVars</pre></div></li><li class="listitem">The output will be a list of numbers. Once plotted, we can see that the number of clusters is three, which occurs at the knee, or just before local maxima, of the curve as shown in the following image:<div><img src="img/6331OS_08_04.jpg" alt="How to do it…"/></div></li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec103"/>Clustering words by their lexemes </h1></div></div></div><p>Words that look <a id="id485" class="indexterm"/>alike can easily be clustered together. The clustering algorithm in the lexeme-clustering package<a id="id486" class="indexterm"/> is based on Janicki's research paper titled, "<em>A Lexeme-Clustering Algorithm for Unsupervised Learning of Morphology</em>". A direct link to this paper can be found through the following URL: <a class="ulink" href="http://skil.informatik.uni-leipzig.de/blog/wp-content/uploads/proceedings/2012/Janicki2012.37.pdf">http://skil.informatik.uni-leipzig.de/blog/wp-content/uploads/proceedings/2012/Janicki2012.37.pdf</a>.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec280"/>Getting ready</h2></div></div></div><p>An Internet connection is necessary for this recipe to download the package from GitHub.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec281"/>How to do it…</h2></div></div></div><p>Follow these steps to install and use the library:</p><div><ol class="orderedlist arabic"><li class="listitem">Obtain the<a id="id487" class="indexterm"/> lexeme-clustering library from GitHub. If Git is installed, enter the following command, otherwise download it from <a class="ulink" href="https://github.com/BinRoot/lexeme-clustering/archive/master.zip">https://github.com/BinRoot/lexeme-clustering/archive/master.zip</a>:<div><pre class="programlisting">
<strong>$ git clone https://github.com/BinRoot/lexeme-clustering</strong>
</pre></div></li><li class="listitem">Change into the library's directory:<div><pre class="programlisting">
<strong>$ cd lexeme-clustering/</strong>
</pre></div></li><li class="listitem">Install the package:<div><pre class="programlisting">
<strong>$ cabal install</strong>
</pre></div></li><li class="listitem">Create <a id="id488" class="indexterm"/>an input file with a different word on each line:<div><pre class="programlisting">
<strong>$ cat input.txt</strong>
<strong>mama</strong>
<strong>papa</strong>
<strong>sissy</strong>
<strong>bro</strong>
<strong>mother</strong>
<strong>father</strong>
<strong>grandfather</strong>
<strong>grandmother</strong>
<strong>uncle</strong>
<strong>mommy</strong>
<strong>daddy</strong>
<strong>ma</strong>
<strong>pa</strong>
<strong>mom</strong>
<strong>dad</strong>
<strong>sister</strong>
<strong>brother</strong>
</pre></div></li><li class="listitem">Run the lexeme-clustering algorithm on the input file:<div><pre class="programlisting">
<strong>$ dist/build/lexeme-clustering/lexeme-clustering input.txt</strong>
</pre></div></li><li class="listitem">The resulting output clusters are then displayed:<div><pre class="programlisting">
<strong># Clustering</strong>
<strong>bro, brother</strong>
<strong>dad, daddy</strong>
<strong>grandfather, grandmother</strong>
<strong>father, ma, mama, mom, mommy, mother, pa, papa</strong>
<strong>sissy, sister</strong>
<strong>uncle</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec282"/>How it works…</h2></div></div></div><p>The related words are clustered together by carefully looking at each word's <a id="id489" class="indexterm"/>
<strong>morpheme</strong>, or smallest meaningful component.</p><p>Here's a short excerpt from the abstract of the research paper of which this algorithm is based:</p><div><blockquote class="blockquote"><p><em>"Initially, a trie of words is built and each node in the trie is considered a candidate for stem. The suffixes, with which it occurs, are clustered according to mutual information in order to identify inflectional paradigms."</em></p></blockquote></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec283"/>See also</h2></div></div></div><p>For clustering points of data, see the previous algorithms on <em>Implementing the k-means clustering algorithm</em>, <em>Implementing hierarchical clustering</em>, and <em>Using a hierarchical clustering library</em>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec104"/>Classifying the parts of speech of words</h1></div></div></div><p>This recipe<a id="id490" class="indexterm"/> will demonstrate how to identify the parts of speech of each word in a sentence. We will be using a handy library <a id="id491" class="indexterm"/>called <strong>chatter</strong>, which contains very useful <strong>Natural Language Processing</strong> (<strong>NLP</strong>) tools. It can be obtained from Hackage at <a class="ulink" href="http://hackage.haskell.org/package/chatter">http://hackage.haskell.org/package/chatter</a>.</p><p>NLP is<a id="id492" class="indexterm"/> the study of human language embedded in a machine. Our naturally spoken or written language may seem obvious to us in our day-to-day lives, but producing meaning out of words is still a difficult task for computers.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec284"/>Getting ready</h2></div></div></div><p>Install the<a id="id493" class="indexterm"/> NLP library using cabal:</p><div><pre class="programlisting">
<strong>cabal install chatter</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec285"/>How to do it…</h2></div></div></div><p>In a new file, which we name <code class="literal">Main.hs</code>, enter the following source code:</p><div><ol class="orderedlist arabic"><li class="listitem">Import the parts of speech library and the pack function:<div><pre class="programlisting">import NLP.POS
import Data.Text (pack)</pre></div></li><li class="listitem">Obtain the default tagger provided by the library:<div><pre class="programlisting">main = do
tagger &lt;- defaultTagger</pre></div></li><li class="listitem">Feed the <code class="literal">tag</code> function a tagger and a text to see the corresponding parts of speech per each word:<div><pre class="programlisting">let text = pack "The best jokes have no punchline."
print $ tag tagger text</pre></div></li><li class="listitem">The output will be an association list of the word to its part of speech:<div><pre class="programlisting">
<strong>[[ ("The", Tag "at"), </strong>
<strong>    ("best", Tag "jjt"), </strong>
<strong>    ("jokes", Tag "nns"), </strong>
<strong>    ("have", Tag "hv"), </strong>
<strong>    ("no", Tag "at"), </strong>
<strong>    ("punchline",Tag "nn"), </strong>
<strong>    (".",Tag ".") ]]</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec286"/>How it works…</h2></div></div></div><p>A parts of speech <a id="id494" class="indexterm"/>tagger is <a id="id495" class="indexterm"/>trained from a corpus of text. In this example, we use the default tagger provided by the library, which trains on the corpus in the following directory of the package, <code class="literal">data/models/brown-train.model.gz</code>. This corpus is called the Brown University Standard Corpus of Present-Day American English, created in the 1960s.</p><p>Definitions of each of the abbreviations such as at, <code class="literal">jjt</code>, or <code class="literal">nns</code> can be found on <a class="ulink" href="http://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used">http://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec287"/>There's more…</h2></div></div></div><p>We can also train our own parts of speech taggers by loading a tagger from a file path, <code class="literal">loadTagger :: FilePath -&gt; IO POSTagger</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec288"/>See also</h2></div></div></div><p>To categorize words as something other than parts of speech, see the next recipe on <em>Identifying key words in a corpus of text</em>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec105"/>Identifying key words in a corpus of text</h1></div></div></div><p>One way to predict the<a id="id496" class="indexterm"/> topic of a paragraph or sentence is by identifying what the words mean. While the parts of speech give some insight about each word, they still don't reveal the connotation of that word. In this recipe, we will use a Haskell library to tag words by topics such as <code class="literal">PERSON</code>, <code class="literal">CITY</code>, <code class="literal">DATE</code>, and so on.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec289"/>Getting ready</h2></div></div></div><p>An Internet <a id="id497" class="indexterm"/>connection is necessary for this recipe to download the<a id="id498" class="indexterm"/> <code class="literal">sequor</code> package.</p><p>Install it from cabal:</p><div><pre class="programlisting">
<strong>$ cabal install sequor --prefix=`pwd`</strong>
</pre></div><p>Otherwise, follow these directions to install it manually:</p><div><ol class="orderedlist arabic"><li class="listitem">Obtain the latest version of the <a id="id499" class="indexterm"/>sequor library by opening up a browser and visiting the following URL: <a class="ulink" href="http://hackage.haskell.org/package/sequor">http://hackage.haskell.org/package/sequor</a>.</li><li class="listitem">Under the <strong>Downloads</strong> section, download the cabal source package.</li><li class="listitem">Extract the contents:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">On Windows, it is easiest to using<a id="id500" class="indexterm"/> 7-Zip, an easy-to-use file archiver. Install it on your machine by going to <a class="ulink" href="http://www.7-zip.org">http://www.7-zip.org</a>. Then using 7-Zip, extract the contents of the tarball.</li><li class="listitem" style="list-style-type: disc">On other operating systems, run the following command to extract the tarball. Replace the numbers in the following command to the correct version numbers of your download because a new version (that is, 0.7.3) may be out:<div><pre class="programlisting">
<strong>$ tar –zxvf sequor-0.7.2.tar.gz</strong>
</pre></div></li></ul></div></li><li class="listitem">Go into the directory:<div><pre class="programlisting">
<strong>$ cd sequor-0.7.2</strong>
</pre></div></li><li class="listitem">Make sure to read the <code class="literal">README</code> file:<div><pre class="programlisting">
<strong>$ cat README.*</strong>
</pre></div></li><li class="listitem">Install the library using the following Cabal command:<div><pre class="programlisting">
<strong>$ cabal install –-prefix=`pwd`</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec290"/>How to do it…</h2></div></div></div><p>We will set up an input file to feed into the program.</p><div><ol class="orderedlist arabic"><li class="listitem">Create an <code class="literal">input.txt</code> file using the CoNLL format, which requires one token per line, and sentences separated by a blank line:<div><pre class="programlisting">
<strong>$ cat input.txt</strong>
<strong>On</strong>
<strong>Tuesday</strong>
<strong>Richard</strong>
<strong>Stallman</strong>
<strong>will</strong>
<strong>visit</strong>
<strong>Charlottesville</strong>
<strong>,</strong>
<strong>Virginia</strong>
<strong>in</strong>
<strong>the</strong>
<strong>United </strong>
<strong>States</strong>
</pre></div></li><li class="listitem">Now <a id="id501" class="indexterm"/>run the word tagging on the input:<div><pre class="programlisting">
<strong>$ bin/seminer en &lt; input.txt &gt; output.txt</strong>
</pre></div></li><li class="listitem">The result is saved in the <code class="literal">output.txt</code> file. Open up the file and review the corresponding tags found:<div><pre class="programlisting">
<strong>$ cat output.txt</strong>
<strong>O</strong>
<strong>B-DATE</strong>
<strong>B-PERSON</strong>
<strong>I-PERSON</strong>
<strong>O</strong>
<strong>O</strong>
<strong>B-GPE:CITY</strong>
<strong>O</strong>
<strong>B-GPE:STATE_PROVINCE</strong>
<strong>O</strong>
<strong>O</strong>
<strong>B-GPE:COUNTRY</strong>
<strong>I-GPE:COUNTRY</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec291"/>How it works…</h2></div></div></div><p>The library uses Collins' sequence perceptron, based off a paper published in 2002 titled "<em>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</em>". His website (<a class="ulink" href="http://www.cs.columbia.edu/~mcollins/">http://www.cs.columbia.edu/~mcollins/</a>) contains comprehensive notes on designing the algorithm used in this recipe.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec292"/>See also</h2></div></div></div><p>To use an existing parts of speech tagger, see the previous recipe on <em>Classifying the parts of speech of words</em>. To train our own parts-of-speech tagger, see the next recipe on <em>Training a parts-of-speech tagger</em>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec106"/>Training a parts-of-speech tagger</h1></div></div></div><p>We will use <a id="id502" class="indexterm"/>a Haskell library, sequor, to train our own parts of speech tagger. Then we can use this newly trained model on our own input.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec293"/>Getting ready</h2></div></div></div><p>Please refer to the <em>Getting ready</em> section of the previous recipe.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec294"/>How to do it…</h2></div></div></div><p>In a new file, which we name <code class="literal">Main.hs</code>, enter the following source code:</p><div><ol class="orderedlist arabic"><li class="listitem">Use the <code class="literal">sequor</code> executable to train the parts of speech tagger:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The first argument to <code class="literal">sequor</code> will be <code class="literal">train</code>, to indicate that we are about to train a tagger</li><li class="listitem" style="list-style-type: disc">The next argument is the template-file, <code class="literal">data/all.features</code></li><li class="listitem" style="list-style-type: disc">Then we provide the train-file, <code class="literal">data/train.conll</code></li><li class="listitem" style="list-style-type: disc">The last file path we need to provide is the location of where to save the trained model</li><li class="listitem" style="list-style-type: disc">We can specify a learning rate using the <code class="literal">-rate</code> flag</li><li class="listitem" style="list-style-type: disc">The beam size can be modified using the <code class="literal">-beam</code> flag</li><li class="listitem" style="list-style-type: disc">Change the number of iterations using the <code class="literal">-iter</code> flag</li><li class="listitem" style="list-style-type: disc">Use hashing instead of a feature dictionary using the <code class="literal">-hash</code> flag</li><li class="listitem" style="list-style-type: disc">Provide a path to the held out data using the <code class="literal">-heldout</code> flag</li><li class="listitem" style="list-style-type: disc">An example of the sequor command in use is as follows:<div><pre class="programlisting">
<strong>$ ./bin/sequor train data/all.features data/train.conll \</strong>
<strong>model --rate 0.1 --beam 10 --iter 5 --hash \</strong>
<strong>--heldout data/devel.conll</strong>
</pre></div></li></ul></div></li><li class="listitem">Test out the trained model on a sample input:<div><pre class="programlisting">
<strong>$ ./bin/sequor predict model &lt; data/test.conll &gt; \</strong>
<strong>data/test.labels</strong>
</pre></div></li><li class="listitem">The first few lines of the output <code class="literal">test.labels</code> file will be:<div><pre class="programlisting">
<strong>B-NP</strong>
<strong>I-NP</strong>
<strong>B-PP</strong>
<strong>B-NP</strong>
<strong>I-NP</strong>
<strong>O</strong>
<strong>B-VP</strong>
<strong>B-NP</strong>
<strong>B-VP</strong>
<strong>B-NP</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec295"/>How it works…</h2></div></div></div><p>The library <a id="id503" class="indexterm"/>uses Collins' sequence perceptron, based off a paper published in 2002 titled "<em>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</em>". The Hackage documentation can be found on <a class="ulink" href="http://hackage.haskell.org/package/sequor">http://hackage.haskell.org/package/sequor</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec296"/>See also</h2></div></div></div><p>To use an existing parts of speech tagger, see the previous recipe on <em>Classifying the parts of speech of words</em>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec107"/>Implementing a decision tree classifier</h1></div></div></div><p>A decision tree<a id="id504" class="indexterm"/> is a model for classifying data effectively. Each child of a node in the tree represents a feature about the item we are classifying. Traversing down the tree to leaf nodes represent an item's classification. It's often desirable to create the smallest possible tree to represent a large sample of data.</p><p>In this recipe, we <a id="id505" class="indexterm"/>implement the ID3 decision tree algorithm<a id="id506" class="indexterm"/> in Haskell. It is one of the <a id="id507" class="indexterm"/>easiest to implement and produces useful results. However, ID3 does not guarantee an optimal solution, may be computationally inefficient compared to other algorithms, and only supports discrete data. While these issues can be addressed by a more complicated algorithm such as C4.5, the code in this recipe is enough to get up and running with a working decision tree.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec297"/>Getting ready</h2></div></div></div><p>Create a CSV file representing samples of data. The last column should be the classification. Name this file <code class="literal">input.csv</code>.</p><div><img src="img/6331OS_08_08.jpg" alt="Getting ready"/></div><p>The weather data<a id="id508" class="indexterm"/> is represented <a id="id509" class="indexterm"/>with four attributes, namely outlook, temperature, humidity, and wind. The last column represents whether it is a good idea to play outside.</p><p>Import the CSV helper library:</p><div><pre class="programlisting">
<strong>$ cabal install csv</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec298"/>How to do it…</h2></div></div></div><p>Insert this code into a new file, which we call <code class="literal">Main.hs</code>:</p><div><ol class="orderedlist arabic"><li class="listitem">Import the built-in libraries:<div><pre class="programlisting">import Data.List (nub, elemIndices)
import qualified Data.Map as M
import Data.Map (Map, (!))
import Data.List (transpose)
import Text.CSV</pre></div></li><li class="listitem">Define some type synonyms to better understand what data is being passed around:<div><pre class="programlisting">type Class = String
type Feature = String
type Entropy = Double
type DataSet = [([String], Class)]</pre></div></li><li class="listitem">Define the <a id="id510" class="indexterm"/>main function to read in the CSV file and handle any errors:<div><pre class="programlisting">main = do
  rawCSV &lt;- parseCSVFromFile "input.csv"
  either handleError doWork rawCSV

handleError = error "invalid file"</pre></div></li><li class="listitem">If the file <a id="id511" class="indexterm"/>was read successfully, remove any invalid CSV records and construct a decision tree out of it:<div><pre class="programlisting">doWork csv = do
  let removeInvalids = filter (\x -&gt; length x &gt; 1)
  let myData = map (\x -&gt; (init x, last x)) $ removeInvalids csv
  print $ dtree "root" myData</pre></div></li><li class="listitem">Define helper functions to break up the <code class="literal">DataSet</code> tuple into a list of samples or a list of classes:<div><pre class="programlisting">samples :: DataSet -&gt; [[String]]
samples d = map fst d


classes :: DataSet -&gt; [Class]
classes d = map snd d</pre></div></li><li class="listitem">Calculate the entropy of a list of values:<div><pre class="programlisting">entropy :: (Eq a) =&gt; [a] -&gt; Entropy

entropy xs = sum $ map (\x -&gt; prob x * info x) $ nub xs
  where prob x = (length' (elemIndices x xs)) / (length' xs)
        info x = negate $ logBase 2 (prob x)
        length' xs = fromIntegral $ length xs</pre></div></li><li class="listitem">Split an attribute by its features:<div><pre class="programlisting">splitAttr :: [(Feature, Class)] -&gt; Map Feature [Class]

splitAttr fc = foldl (\m (f,c) -&gt; M.insertWith (++) f [c] m)
               M.empty fc</pre></div></li><li class="listitem">Obtain each of the entropies from splitting up an attribute by its features:<div><pre class="programlisting">splitEntropy :: Map Feature [Class] -&gt; M.Map Feature Entropy

splitEntropy m = M.map entropy m</pre></div></li><li class="listitem">Compute <a id="id512" class="indexterm"/>the information gain from splitting up an attribute by its features:<div><pre class="programlisting">informationGain :: [Class] -&gt; [(Feature, Class)] -&gt; Double

informationGain s a = entropy s - newInformation
  where eMap = splitEntropy $ splitAttr a
        m = splitAttr a
        toDouble x = read x :: Double
        ratio x y = (fromIntegral x) / (fromIntegral y)
        sumE = M.map (\x -&gt; (fromIntegral.length) x / (fromIntegral.length) s) m
        newInformation = M.foldWithKey (\k a b -&gt; b + a*(eMap!k)) 
          0 sumE</pre></div></li><li class="listitem">Determine<a id="id513" class="indexterm"/> which attribute contributes the highest information gain:<div><pre class="programlisting">highestInformationGain :: DataSet -&gt; Int
highestInformationGain d = snd $ maximum $ 
  zip (map ((informationGain . classes) d) attrs) [0..]
  where attrs = map (attr d) [0..s-1]
        attr d n = map (\(xs,x) -&gt; (xs!!n,x)) d
        s = (length . fst . head) d</pre></div></li><li class="listitem">Define the data structure for a decision tree that we will soon construct:<div><pre class="programlisting">data DTree = DTree { feature :: String
                   , children :: [DTree] } 
           | Node String String
           deriving Show</pre></div></li><li class="listitem">Split up the dataset by the attribute that contributes the highest information gain:<div><pre class="programlisting">datatrees :: DataSet -&gt; Map String DataSet
datatrees d = 
  foldl (\m (x,n) -&gt; M.insertWith (++) (x!!i) [((x `dropAt` i), fst (cs!!n))] m)
    M.empty (zip (samples d) [0..])
  where i = highestInformationGain d
    dropAt xs i = let (a,b) = splitAt i xs in a ++ drop 1 b
        cs = zip (classes d) [0..]</pre></div></li><li class="listitem">Define a helper function to determine if all elements of a list are equal. We use this to check if further splitting of the dataset is necessary by checking if its classes are identical:<div><pre class="programlisting">allEqual :: Eq a =&gt; [a] -&gt; Bool
allEqual [] = True
allEqual [x] = True
allEqual (x:xs) = x == (head xs) &amp;&amp; allEqual xs</pre></div></li><li class="listitem">Construct <a id="id514" class="indexterm"/>the <a id="id515" class="indexterm"/>decision tree from a labeling and a dataset of samples:<div><pre class="programlisting">dtree :: String -&gt; DataSet -&gt; DTree

dtree f d 
  | allEqual (classes d) = Node f $ head (classes d)
  | otherwise = DTree f $ 
            M.foldWithKey (\k a b -&gt; b ++ [dtree k a] ) [] 
            (datatrees d)</pre></div></li><li class="listitem">Run the following code to see the tree printed out:<div><pre class="programlisting">
<strong>DTree { feature = "root"</strong>
<strong>      , children = [ DTree { feature = "Sunny"</strong>
<strong>                           , children = [ Node "Normal" "Yes"</strong>
<strong>                                        , Node "High" "No"</strong>
<strong>                                        ]</strong>
<strong>                   , DTree { feature = "Rain"</strong>
<strong>                           , children = [ Node "Weak" "Yes"</strong>
<strong>                                        , Node "Strong" "No" </strong>
<strong>                                        ] </strong>
<strong>                           }</strong>
<strong>                   , Node "Overcast" "Yes" </strong>
<strong>                   ] </strong>
<strong>      }</strong>
</pre></div></li></ol></div><p>It can be visualized using the following diagram:</p><div><img src="img/6331OS_08_09.jpg" alt="How to do it…"/></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec299"/>How it works…</h2></div></div></div><p>The ID3 algorithm <a id="id516" class="indexterm"/>uses the concept of Shannon's entropy to divide up a set of samples by the attribute that maximize the information gain. This process is recursively repeated until we're dealing with samples of the same classification or when we run out of attributes.</p><p>In the field of Information Theory, <strong>Entropy</strong><a id="id517" class="indexterm"/> is the measure of unpredictability. A fair coin has higher entropy than a biased coin. Entropy can be calculated by taking the expected value of the information content, where information content of a random variable X has the form — <em>ln(P(X))</em>. When the logarithm in the equation is to the base of 2, the units of entropy are called <em>bits</em>.</p><p>Information Gain <a id="id518" class="indexterm"/>is the change in entropy from the prior state to the new state. It has the equation <em>IG = H<sub>1</sub> – H<sub>2</sub></em>, where <em>H<sub>1</sub></em> is the original entropy of the sample. And <em>H<sub>2</sub></em> is the new entropy given an attribute to split.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec108"/>Implementing a k-Nearest Neighbors classifier</h1></div></div></div><p>One simple way to classify an item is to look at only its neighboring data. The k-Nearest Neighbors algorithm <a id="id519" class="indexterm"/>looks at k items located closest to the item in question. The item is then classified as the most common classification of its k neighbors. This heuristic has been very promising for a wide variety of classification tasks.</p><p>In this recipe, we will <a id="id520" class="indexterm"/>implement the k-Nearest Neighbors algorithm using a <a id="id521" class="indexterm"/>
<strong>k-d tree</strong> data structure, which is a binary tree with special properties that allow efficient representation of points in a k-dimensional space.</p><p>Imagine we have a web server for our hip new website. Every time someone requests a web page, our web server will fetch the file and present the page. However, bots can easily hammer a web server with thousands of requests, potentially causing a denial of service attack. In this recipe, we will classify whether a web request is being made by a human or a bot.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec300"/>Getting ready</h2></div></div></div><p>Install the <code class="literal">KdTree</code>, <code class="literal">CSV</code>, and <code class="literal">iproute</code> packages using cabal:</p><div><pre class="programlisting">
<strong>$ cabal install KdTree</strong>
<strong>$ cabal install CSV</strong>
<strong>$ cabal install iproute</strong>
</pre></div><p>Create a CSV file containing the IP addresses and number of seconds since last access. The last field of each CSV record should be the classification <em>Human</em> or <em>Bot</em>. We call our file <code class="literal">input.csv</code>.</p><div><img src="img/6331OS_08_10.jpg" alt="Getting ready"/></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec301"/>How to do it…</h2></div></div></div><p>After creating a new file called <code class="literal">Main.hs</code>, we perform the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Import the following packages:<div><pre class="programlisting">import Data.Trees.KdTree
import Data.IP (IPv4, fromIPv4)
import Text.CSV
import qualified Data.Map as M
import Data.Maybe (fromJust)</pre></div></li><li class="listitem">Convert <a id="id522" class="indexterm"/>an IPv4 address string into its 32-bit representation:<div><pre class="programlisting">ipToNum :: String -&gt; Double

ipToNum str = fromIntegral $ sum $ 
  zipWith (\a b -&gt; a * 256^b) ns [0..]
  where ns = reverse $ fromIPv4 (read str :: IPv4)</pre></div></li><li class="listitem">Parse data from a CSV file to obtain a list of points and their associated classifications:<div><pre class="programlisting">parse :: [Record] -&gt; [(Point3d, String)]

parse [] = []
parse xs = map pair (cleanList xs)
  where pair [ip, t, c] = 
          (Point3d (ipToNum ip) (read t) 0.0, c)
        cleanList = filter (\x -&gt; length x == 3)</pre></div></li><li class="listitem">Find the item in a list that occurs most often:<div><pre class="programlisting">maxFreq :: [String] -&gt; String

maxFreq xs = fst $ foldl myCompare ("", 0) freqs
  where freqs = M.toList $ M.fromListWith (+) 
                                         [(c, 1) | c &lt;- xs]
        myCompare (oldS, oldV) (s,v) = if v &gt; oldV
                                       then (s, v)
                                       else (oldS, oldV)</pre></div></li><li class="listitem">Classify a test point given the KdTree, the number of nearest neighbors to use, and the training set of points:<div><pre class="programlisting">test :: KdTree Point3d -&gt; Int -&gt; [(Point3d, String)] 
                       -&gt; Point3d -&gt; String

test kdtree k pairList p = maxFreq $ map classify neighbors
  where neighbors = kNearestNeighbors kdtree k p
        classify x = fromJust (lookup x pairList)</pre></div></li><li class="listitem">Define <code class="literal">main</code> to read a CSV file and process the data:<div><pre class="programlisting">main = do
  rawCSV &lt;- parseCSVFromFile "input.csv"
  either handleError doWork rawCSV</pre></div></li><li class="listitem">Handle an error if the CSV cannot be read properly:<div><pre class="programlisting">handleError = error "Invalid CSV file"</pre></div></li><li class="listitem">Otherwise create a KdTree from the CSV data and test out a couple of examples:<div><pre class="programlisting">doWork rawCSV = do
  let ps = parse rawCSV
  let kdtree = fromList (map fst ps)
  let examples = [ ["71.190.100.100", "2000", "?"]
                 , ["216.239.33.1", "1", "?"] ]
  let examplePts = map fst $ parse examples
  print $ map (test kdtree 2 ps) examplePts</pre></div></li><li class="listitem">Run the <a id="id523" class="indexterm"/>code to see the resulting classifications of the example points:<div><pre class="programlisting">
<strong>$ runhaskell Main.hs</strong>

<strong>["Human", "Bot"]</strong>
</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec302"/>How it works…</h2></div></div></div><p>The k-Nearest Neighbor algorithm<a id="id524" class="indexterm"/> looks at the k closest points from the training set and returns the most frequent classification between these k points. Since we are dealing with points, each of the coordinates should be orderable. Fortunately, an IP address has a faint sense of hierarchy that we can leverage. We convert an IP to its 32-bit number to obtain a useful ordering that we can treat as a coordinate of a point in space.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec109"/>Visualizing points using Graphics.EasyPlot</h1></div></div></div><p>Sometimes, it's<a id="id525" class="indexterm"/> convenient to simply visualize <a id="id526" class="indexterm"/>data points before clustering or classifying to inspect the data. This recipe will feed a list of points to a plotting library to easily see a diagram of the data.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec303"/>Getting ready</h2></div></div></div><p>Install easyplot from cabal:</p><div><pre class="programlisting">
<strong>$ cabal install easyplot</strong>
</pre></div><p>Create a CSV file containing two-dimensional points:</p><div><pre class="programlisting">
<strong>$ cat input.csv</strong>

<strong>1,2</strong>
<strong>2,3</strong>
<strong>3,1</strong>
<strong>4,5</strong>
<strong>5,3</strong>
<strong>6,1</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec304"/>How to do it…</h2></div></div></div><p>In a new file, <code class="literal">Main.hs</code>, follow these steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Import <a id="id527" class="indexterm"/>the required library to read in CSV data as well the library to plot points:<div><pre class="programlisting">import Text.CSV
import Graphics.EasyPlot</pre></div></li><li class="listitem">Create <a id="id528" class="indexterm"/>a helper function to convert a list of string records into a list of doubles. For example, we want to convert <code class="literal">[ "1.0,2.0", "3.5,4.5" ]</code> into <code class="literal">[ (1.0, 2.0), (3.5, 4.5) ]</code>:<div><pre class="programlisting">tupes :: [[String]] -&gt; [(Double, Double)]

tupes records = [ (read x, read y) | [x, y] &lt;- records ]</pre></div></li><li class="listitem">In <code class="literal">main</code>, parse the CSV file to be used later on:<div><pre class="programlisting">main = do 
  result &lt;- parseCSVFromFile "input.csv"</pre></div></li><li class="listitem">If the CSV file is valid, plot the points using the <code class="literal">plot :: TerminalType -&gt; a -&gt; IO Bool</code> function:<div><pre class="programlisting">  case result of
    Left err -&gt; putStrLn "Error reading CSV file"
    Right csv -&gt; do 
      plot X11 $ Data2D [Title "Plot"] [] (tupes csv)
        return ()</pre></div></li></ol></div></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec305"/>How it works…</h2></div></div></div><p>The first argument to <code class="literal">plot</code> tells gnuplot where its output should be displayed. For example, we use X11 to output to the X Window System on Linux. Depending on the computer, we can choose between different terminal types. The <a id="id529" class="indexterm"/>constructors for <code class="literal">TerminalType</code> are the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Aqua</code>: Output on Mac OS X (Aqua Terminal)</li><li class="listitem" style="list-style-type: disc"><code class="literal">Windows</code>: Output for MS Windows</li><li class="listitem" style="list-style-type: disc"><code class="literal">X11</code>: Output to the X Window System</li><li class="listitem" style="list-style-type: disc"><code class="literal">PS FilePath</code>: Output into a PostScript file</li><li class="listitem" style="list-style-type: disc"><code class="literal">EPS FilePath</code>: Output into an EPS file path</li><li class="listitem" style="list-style-type: disc"><code class="literal">PNG FilePath</code>: Output as Portable Network Graphic into a file</li><li class="listitem" style="list-style-type: disc"><code class="literal">PDF FilePath</code>: Output as Portable Document Format into a file</li><li class="listitem" style="list-style-type: disc"><code class="literal">SVG FilePath</code>: Output as Scalable Vector Graphic into a file</li><li class="listitem" style="list-style-type: disc"><code class="literal">GIF FilePath</code>: Output as Graphics Interchange Format into a file</li><li class="listitem" style="list-style-type: disc"><code class="literal">JPEG FilePath</code>: Output into a JPEG file</li><li class="listitem" style="list-style-type: disc"><code class="literal">Latex FilePath</code>: Output <a id="id530" class="indexterm"/>as LaTeX</li></ul></div><p>The second argument to plot is the graph, which may be a <code class="literal">Graph2D</code>, or <code class="literal">Graph3D</code>, or a list of these.</p></div></div></body></html>