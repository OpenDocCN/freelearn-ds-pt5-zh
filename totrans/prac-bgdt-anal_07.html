<html><head></head><body>
        

                            
                    <h1 class="header-title">An Introduction to Machine Learning Concepts</h1>
                
            
            
                
<p class="mce-root">Machine learning has become a commonplace topic in our day-to-day lives. The advancement in the field has been so dramatic that today, even cell phones incorporate advanced machine learning and artificial intelligence-related facilities, capable of responding and taking actions based on human instructions.</p>
<p>A subject that was once limited to university classrooms has transformed into a full-fledged industry, pervading our daily lives in ways we could not have envisioned even just a few years ago.</p>
<p>The aim of this chapter is to introduce the reader to the underpinnings of machine learning and explain the concepts in simple, lucid terms that will help readers become familiar with the core ideas in the subject. We'll start off with a high-level overview of machine learning, and explain the different categories and how to distinguish them. We'll explain some of the salient concepts in machine learning, such as data pre-processing, feature engineering, and variable importance. The next chapter will go into more detail regarding individual algorithms and theoretical machine learning.</p>
<p>We'll conclude with exercises that leverage real-world datasets to perform machine learning operations using R.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>What is machine learning?</li>
<li>The popular emergence</li>
<li>Machine learning, statistics, and artificial intelligence (AI)</li>
<li>Categories of machine learning</li>
<li>Core concepts in machine learning</li>
<li>Machine learning tutorial</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">What is machine learning?</h1>
                
            
            
                
<p><strong>Machine learning</strong> is not a new subject; it has existed in academia for well over 70 years as a formal discipline, but known by different names: statistics, and more generally mathematics, then <strong>artificial intelligence</strong> (<strong>AI</strong>), and today as machine learning. While the other related subject areas of statistics and AI are just as prevalent, machine learning has carved out a separate niche and become an independent discipline in and of itself.</p>
<p>In simple terms, machine learning involves predicting future events based on historical data. We see it manifested in our day-to-day lives and indeed we employ, knowingly or otherwise, principles of machine learning on a daily basis.</p>
<p>When we casually comment on whether a movie will succeed at the box office using our understanding of the popularity of the individuals in the lead roles, we are applying machine learning, albeit subconsciously. Our understanding of the characters in the lead roles has been shaped over years of watching movies where they appeared. And, when we make a determination of the success of a future movie featuring the same person, we are using historical information to make an assessment.</p>
<p>As another example, if we had data on temperature, humidity, and precipitation (rain) over a period of say, 12 months, can we use that information to predict whether it will rain today, given information on temperature and humidity?</p>
<p>This is akin to common regression problems found in statistics. But, machine learning involves applying a much higher level of rigor to the exercise to reach a conclusive decision based not only on theoretical calculations, but verification of the calculations hundreds or thousands of times using iterative methods before reaching a conclusion.</p>
<p>It should be noted and clarified here that the term <em>machine learning</em> relates to algorithms or programs that are executed typically on a computing device whose objective it is to predict outcomes. The algorithms build mathematical models that can then be used to make predictions. It is a common misconception that machine learning quite literally refers to a <em>machine</em> that is <em>learning</em>. The actual implication, as just explained, is much less dramatic.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The evolution of machine learning</h1>
                
            
            
                
<p>The timeline of machine learning, as available on Wikipedia (<a href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning" target="_blank">https://en.wikipedia.org/wiki/Timeline_of_machine_learning</a>), provides a succinct and insightful overview of the evolution of the field. The roots can be traced back to as early as the mid-1700s, when Thomas Bayes presented his paper on <em>inverse probability</em> at the Royal Society of London. Inverse probability, more commonly known today as probability distribution, deals with the problem of determining the state of a system given a prior set of events. For example, if a box contained milk chocolate and white chocolate, you took out a few at random, and received two milk and three white chocolates, can we infer how many of each chocolate there are in the box?</p>
<p>In other words, what can we infer about the unknown given a few points of data with which we can postulate a formal theory? Bayes' work was developed further into Bayes' Theorem by Pierre-Simon Laplace in his text, <em>Théorie Analytique des Probabilités</em>.</p>
<p>In the early 1900s, Andrey Markov's analysis of Pushkin's Poem, Eugeny Onegin, to determine the alliteration of consonants and vowels in Russian literature, led to the development of a technique known as Markov Chains, used today to model complex situations involving random events. Google's PageRank algorithm implements a form of Markov Chains.</p>
<p>The first formal application of machine learning, or more generally, AI, and its eventual emergence as a discipline, should be attributed to Alan Turing. He developed the Turing Test - a way to determine whether a machine is intelligent enough to mimic human behavior. Turing presented this in his paper, <em>Computing Machinery and Intelligence</em>, which starts out with the following:</p>
<p>I propose to consider the question, "Can machines think?" This should begin with definitions of the meaning of the terms "machine" and "think." The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words "machine" and "think" are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, "Can machines think?" is to be sought in a statistical survey such as a Gallup poll. But this is absurd. Instead of attempting such a definition I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.[...]</p>
<p>Later in the paper, Turing writes:</p>
<div><em>The original question, "Can machines think?" I believe to be too meaningless to deserve discussion. Nevertheless I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted. I believe further that no useful purpose is served by concealing these beliefs.</em></div>
<p>Turing's work on AI was followed by a series of seminal events in machine learning and AI. The first neural network was developed by Marvin Misky in 1951, Arthur Samuel began his work on the first machine learning programs that played checkers in 1952, and Rosenblatt invented the perceptron, a fundamental unit of neural networks, in 1957. Pioneers such as Leo Breiman, Jerome Friedman, Vladimir Vapnik and Alexey Chervonenkis, Geoff Hinton, and YannLeCun made significant contributions through the late 1990s to bring machine learning into the limelight. We are greatly indebted to their work and contributions, which have made machine learning stand out as a distinct area of research today.</p>
<p>In 1997, IBM's Deep Blue beat Kasparov and it immediately became a worldwide sensation. The ability of a machine to beat the world's top chess champion was no ordinary achievement. The event gave some much-needed credibility to machine learning as a formidable contender for the intelligent machines that Turing envisaged.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Factors that led to the success of machine learning</h1>
                
            
            
                
<p>Given machine learning, as a subject, has existed for many decades, it begs the question: why hadn't it become as popular as it is today much sooner? Indeed, the theories of complex machine learning algorithms such as neural networks were well known by the late 1990s, and the foundation had been established well before that in the theoretical realm.</p>
<p>There are a few factors that can be attributed to the success of machine learning:</p>
<ul>
<li><strong>The Internet</strong>: The web played a critical role in democratizing information and connecting people in an unprecedented way. It made the exchange of information simple in a way that could not have been achieved through the pre-existing methods of print media communication. Not only did the web transform and revolutionize the dissemination of information, it also opened up new opportunities. Google's PageRank, as mentioned earlier, was one of the first large-scale and highly visible successes in the application of statistical models to develop a highly successful web enterprise.</li>
<li><strong>Social media</strong>: While the web provided a platform for communication, it lacked a level of flexibility akin to how people interacted with one another in the real world. There was a noticeable, but understated, and arguably unexplored gap. Tools such as IRC and Usenet were the precursors to social network websites such as Myspace, which was one of the first web-based platforms intended to create personal networks. By early-mid 2000, Facebook had emerged as the leader in social networking. These platforms provided a unique opportunity to leverage the Internet to collect data at an individual level. Each user left a trail of messages, ripe for collection and analysis using Natural Language Processing and other techniques.</li>
<li><strong>Computing hardware</strong>: Hardware used for computers developed at an exponential rate. Machine learning algorithms are inherently compute and resource intensive, that is, they require powerful CPUs, fast disks, and high memory depending on the size of data. The invention of new ways to store data on <strong>solid state drives</strong> (<strong>SSDs</strong>) was a leap from the erstwhile process of storing on spinning hard drives. Faster access meant that data could be delivered to the CPU at a much faster rate and reduce the I/O bottleneck that has traditionally been a weak area in computing. Faster CPUs meant it was possible to perform hundreds and thousands of iterations demanded by machine learning algorithms in a timely manner. Finally, the demand led to the reduction in prices for computing resources, allowing more people to be able to afford buying computing hardware that was prohibitively expensive. Algorithms existed, but the resources were finally able to execute them in a reasonable time and cost.</li>
<li><strong>Programming languages and packages</strong>: Communities such as R and Python developers seized the opportunity, and individuals started releasing packages that exposed their work to a broader community of programmers. In particular, packages that provided machine learning algorithms became immediately popular and inspired other practitioners to release their individual code repositories, making platforms such as R a truly global collaborative effort. Today there are over 10,000 packages in R, up from 2000 in 2010.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Machine learning, statistics, and AI</h1>
                
            
            
                
<p>Machine learning is a term that has various synonyms - names that are the result of either marketing activities by corporates or just terms that have been used interchangeably. Although some may argue that they have different implications, they all ultimately refer to machine learning as a subject that facilitates the prediction of future events using historical information.</p>
<p>The commonly heard terms for machine learning include predictive analysis, predictive analytics, predictive modeling, and many others. As such, unless the entity that publishes material explaining their interpretation of the term and more specifically, how it is different, it is generally safe to assume that they are referring to machine learning. This is often a source of confusion among those new to the subject, largely due to the misuse and overuse of technical verbiage.</p>
<p>Statistics, on the other hand, is a distinct subject area that has been well known for over 200 years. The word is derived from the new Latin, <em>statisticum collegium</em> (council of state, in English) and the Italian word <em>statista</em>, meaning statesman or politician. You can visit <a href="https://en.wikipedia.org/wiki/History_of_statistics#Etymology" target="_blank">https://en.wikipedia.org/wiki/History_of_statistics#Etymology</a> for more details on this topic. Machine learning implements various statistical models, which due to the rigor of computation involved, is distinct from the branch of classical statistics.</p>
<p>AI is also closely related to machine learning, but is a much broader subject. It can be loosely defined as systems (software/hardware) that, in the presence of uncertainties, can arrive at a concrete decision in (usually) a responsible and socially aware manner to attain a target end objective. In other words, AI aims to produce actions by systematically processing a situation that involves both known and unknown (latent) factors.</p>
<p>AI conjures up images of smart and sometimes rebellious robots in sci-fi movies, just as much as it reminds us of intelligent systems, such as IBM Watson, that can parse complex questions and process ambiguous statements to find concrete answers.</p>
<p>Machine learning shares some of the same traits - the step-wise development of a model using training data, and measuring accuracy using test data. However, AI has existed for many decades and has been a familiar household term. Institutions in the US, such as Carnegie Mellon University, have led the way in establishing key principles and guidelines of AI.</p>
<p>The online resources/articles on AI versus machine learning do not seem to provide any conclusive answers on how they differ. However, the syllabus of AI courses at universities makes the differences very obvious. You can learn more about AI at <a href="https://cs.brown.edu/courses/csci1410/lectures.html" target="_blank">https://cs.brown.edu/courses/csci1410/lectures.html</a>.</p>
<p>AI refers to a vast array of study areas that involve:</p>
<ul>
<li><strong>Constrained optimization</strong>: Reach best possible results given a set of constraints or limitations in a given situation</li>
<li><strong>Game theory</strong>: For instance, zero-sum games, equilibrium, and others - taking a measured decision based on how the decision can affect future decisions and impact desired end goals</li>
<li><strong>Uncertainty/Bayes' rule</strong>: Given prior information, what is the likelihood of this happening given something else has already happened</li>
<li><strong>Planning</strong>: Formulating a plan of action = a set of paths (graph) to tackle a situation/reach an end goal</li>
<li><strong>Machine learning</strong>: The implementation (realization) of the preceding goals by using algorithms that are designed to handle uncertainties and imitate human reasoning. The machine learning algorithms generally used for AI include:
<ul>
<li>Neural networks/deep learning (find hidden factors)</li>
<li>Natural language processing (NLP) (understand context using tenor, linguistics, and such)</li>
<li>Visual object recognition</li>
<li>Probabilistic models (for example, Bayes' classifiers)</li>
<li>Markov decision processes (decisions for random events, for example, gambling)</li>
<li>Various other machine learning Algorithms (clustering, SVM)</li>
</ul>
</li>
<li><strong>Sociology</strong>: A study of how machine learning decisions affect society and take remedial steps to correct issues</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Categories of machine learning</h1>
                
            
            
                
<p>Arthur Samuel coined the term <strong>machine learning</strong> in 1959 while at IBM. A popular definition of machine learning is due to Arthur, who, it is believed, called machine learning <em>a field of computer science that gives computers the ability to learn without being explicitly programmed</em>.</p>
<p>Tom Mitchell, in 1998, added a more specific definition to machine learning and called it a, study of algorithms that improve their performance P at some task T with experience E.</p>
<p>A simple explanation would help to illustrate this concept. By now, most of us are familiar with the concept of spam in emails. Most email accounts also contain a separate folder known as <strong>Junk</strong>, <strong>Spam</strong>, or a related term. A cursory check of the folders will usually indicate the presence of several emails, many of which were presumably unsolicited and contain meaningless information.</p>
<p>The mere task of categorizing emails as spam and moving them to a folder involves the application of machine learning. Andrew Ng highlighted this elegantly in his popular MOOC course on machine learning.</p>
<p>In Mitchell's terms, the spam classification process involves:</p>
<ul>
<li><strong>Task T</strong>: Classifying emails as spam/not spam</li>
<li><strong>Performance P</strong>: Number of emails accurately identified as spam</li>
<li><strong>Experience E</strong>: The model is provided emails that have been marked as spam/not spam and uses that information to determine whether a new email is spam or not</li>
</ul>
<p>Broadly speaking, there are two distinct types of machine learning:</p>
<ul>
<li>Supervised machine learning</li>
<li>Unsupervised machine learning</li>
</ul>
<p>We shall discuss them in turn here.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Supervised and unsupervised machine learning</h1>
                
            
            
                
<p>Let us start with supervised machine learning first.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Supervised machine learning</h1>
                
            
            
                
<p><strong>Supervised machine learning</strong> refers to machine learning exercises that involve predicting outcomes with labelled data. Labelled data simply refers to the fact that the dataset we are using to make the predictions (as well as the outcome we will predict) has a definite value (irrespective of what it is). For instance, classifying emails as spam or not spam, predicting temperature, and identifying faces from images are all examples of supervised machine learning.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Vehicle Mileage, Number Recognition and other examples</h1>
                
            
            
                
<p>Given a dataset containing information on miles per gallon, number of cylinders, and such of various cars, can we predict what the value for miles per gallon would be if we only had the other values available?</p>
<p>In this case, our outcome is <kbd>mpg</kbd> and we are using the other variables of <kbd>cyl</kbd> (Cylinders), <kbd>hp</kbd> (Horsepower), <kbd>gear</kbd> (number of gears), and others to build a model that can then be applied against a dataset where the values for mpg are marked as <kbd>MISSING</kbd>. The model reads the information in these columns in the first five rows of the data and, based on that information, predicts what the value for <kbd>mpg</kbd> would be in the other rows, as shown in the following image:</p>
<div><img height="254" width="275" class=" image-border" src="img/5c1b160c-2b40-4223-9d7c-b9fb43c86b0d.png"/></div>
<p>The reason this is considered supervised is that in the course of building our machine learning model, we provided the model with information on what the outcome was. Other examples include:</p>
<ul>
<li><strong>Recognizing letters and numbers</strong>: In such cases, the input to the model are the images, say of letters and numbers, and the outcome is the alpha-numeric value shown on the image. Once the model is built, it can then be used against pictures to recognize and predict what numbers are shown in the picture. A simple example, but very powerful. Imagine if you were given 100,000 images of houses with house numbers. The manual way of identifying the house numbers would be to go through each image individually and write down the numbers. A machine learning model allows us to completely automate the entire operation. Instead of having to manually go through individual images, you could simply run the model against the images and get the results in a very short amount of time.</li>
<li><strong>Self-driving autonomous cars</strong>: The input to the algorithms are images where the objects in the image have been identified, for example, person, street sign, car, trees, shops, and other elements. The algorithm <em>learns</em> to recognize and differentiate among different elements once a sufficient number of images have been shown and thereafter given an unlabeled image, that is, an image where the objects have not been identified is able to recognize them individually. To be fair, this is a highly simplified explanation of a very complex topic, but the overall principle is the same.</li>
</ul>
<p>MNIST Dataset used for number recognition:</p>
<div><img height="158" width="229" src="img/18749693-441f-4359-8c23-45af6249b0c1.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Unsupervised machine learning</h1>
                
            
            
                
<p><strong>Unsupervised machine learning</strong> involves datasets that do not have labeled outcomes. Taking the example of predicting mpg values for cars, in an unsupervised exercise, our dataset would have looked as follows:</p>
<div><img height="235" width="251" class=" image-border" src="img/22338df9-3af6-4217-a4b9-be7b4556b7db.png"/></div>
<p>If all the outcomes are <em>missing</em>, it would be impossible to know what the values might have been. Recall that the primary premise of machine learning is to use historical information to make predictions on datasets whose outcome is not known. But, if the historical information itself does not have any identified outcomes, then it would not be possible to build a model. Without knowing any other information, the values of mpg in the table could be all 0 or all 100; it is not possible to tell, as we do not have any data point that will help lead us to the value.</p>
<p>This is where <em>unsupervised</em> machine learning gets applied. In this type of machine learning, we are not trying to predict outcomes. Rather, we are trying to determine which items are most similar to one another.</p>
<p>A common name for such an exercise is <em>clustering</em>, that is, we are attempting to find <em>clusters</em> or groups of records that are most similar to one another. Where can we use this information and what are some examples of unsupervised learning?</p>
<p>There are various news aggregators on the web - sites that do not themselves publish information, but collect information from other news sources. One such aggregator is Google News. If, say, we had to search for information on the last images taken by the satellite Cassini of Saturn, we could do a simple search for the phrase on Google News <a href="https://news.google.com/news/?gl=US&amp;ned=us&amp;hl=en" target="_blank">https://news.google.com/news/?gl=US&amp;amp;ned=us&amp;amp;hl=en</a>. An example is shown here:</p>
<div><img height="429" width="653" class=" image-border" src="img/4ad124ec-c378-451b-a50d-c4da5aab8847.png"/></div>
<p>Notice that there is a link for View all at the bottom of the news articles. Clicking the link will take you to a page with all the other related news articles. Surely, Google didn't manually classify the articles as belonging to the specific search term. In fact, Google doesn't know in advance what the user will search for. The search term could have well been <em>images of Saturn rings from space</em>.</p>
<p>So, how does Google know which articles belong to a specific search term? The answer lies in the application of clustering or principles of unsupervised learning. Unsupervised learning examines the attributes of a specific dataset in order to determine which articles are most similar to one another. To do this, the algorithm doesn't even need to know the contextual background.</p>
<p>Suppose you were given two sets of books with no covers, a set of books on gardening and a set of books on computer programming. Although you may not know the title of the book, it would be fairly easy to distinguish books on computers from books on gardening. One set of books would have an overwhelming number of terms related to computing, while the other would have an overwhelming number of terms related to plants. To make the distinction that there were two distinct categories of books would not be difficult just by virtue of the images in the books, even for a reader who, let's assume, is not aware of either computers or gardening.</p>
<p>Other examples of unsupervised machine learning include detection of malignant and non-malignant tumors, and gene sequencing.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Subdividing supervised machine learning</h1>
                
            
            
                
<p>Supervised machine learning can be further subdivided into exercises that involve either of the following:</p>
<ul>
<li><strong>Classification</strong></li>
<li><strong>Regression</strong></li>
</ul>
<p>The concepts are quite straightforward.</p>
<p>Classification involves a machine learning task that has a discrete outcome - a <strong>categorical</strong> outcome. All <strong>nouns</strong> are categorical variables, such as fruits, trees, color, and true/false.</p>
<p>The outcome variables in classification exercises are also known as <strong>discrete or categorical variables</strong>.</p>
<p>Some examples include:</p>
<ul>
<li>Identifying the fruit given size, weight, and shape</li>
<li>Identifying numbers given a set of images of numbers (as shown in the earlier chapter)</li>
<li>Identifying objects on the streets</li>
<li>Identifying playing cards as diamonds, spades, hearts and clubs</li>
<li>Identifying the class rank of a student based on the student's grade</li>
<li>The last one might not seem obvious, but a rank, that is, 1<sup>st</sup>, 2<sup>nd</sup>, 3<sup>rd</sup> denotes a fixed category. A student could rank, say, 1<sup>st</sup> or 2<sup>nd</sup>, but not have a rank of 1.5!</li>
</ul>
<p>Images of some atypical classification examples are shown below:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<div><img class=" image-border" src="img/06771d1b-3810-4a9b-a9d2-5f5f6c48be27.png"/></div>
<p>Classification of different types of fruits</p>
</td>
<td>
<div><img class=" image-border" src="img/02f51c3d-2e4f-46dc-b332-3cbac84c3bfe.png"/></div>
<p>Classification of playing cards: diamonds, spades, hearts, and clubs</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p><strong>Regression</strong>, on the other hand, involves calculating numeric outcomes. Any outcome on which you can perform numeric operations, such as addition, subtraction, multiplication, and division, would constitute a regression problem.</p>
<p>Examples of regression include:</p>
<ul>
<li>Predicting daily temperature</li>
<li>Calculating stock prices</li>
<li>Predicting the sales price of residential properties and others</li>
</ul>
<p>Images of some atypical regression examples are shown below. In both the cases, we are dealing with quantitative numeric data that is continuous. Hence, the outcome variables of regression are also known as <strong>quantitative or continuous variables</strong>.</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<div><img height="300" width="450" class="alignnone size-full wp-image-416 image-border" src="img/10902bef-e6e1-49ef-879d-212ac46fbea9.png"/></div>
<p>Calculating house prices</p>
</td>
<td>
<div><img height="243" width="410" class=" image-border" src="img/40718ef7-1629-42bc-868a-c3e136387bf9.png"/></div>
<p>Calculating stock prices using other market data</p>
</td>
</tr>
</tbody>
</table>
<div><br/>
Note that the concepts of classification or regression do not as such apply to unsupervised learning. Since there are no labels in unsupervised learning, there is no discrete classification or regression in the strict sense. That said, since unsupervised learning categories data into clusters, objects in a cluster are often said to belong to the same class (as other objects in the same cluster). This is akin to classification, except that it is created after-the-fact and no classes existed prior to the objects being classified into individual clusters.</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Common terminologies in machine learning</h1>
                
            
            
                
<p>In machine learning, you'll often hear the terms features, predictors, and dependent variables. They are all one and the same. They all refer to the variables that are used to predict an outcome. In our previous example of cars, the variables <strong>cyl</strong> (Cylinder), <strong>hp</strong> (Horsepower), <strong>wt</strong> (Weight), and <strong>gear</strong> (Gear) are the predictors and <strong>mpg</strong> (Miles Per Gallon) is the outcome.</p>
<p>In simpler terms, taking the example of a spreadsheet, the names of the columns are, in essence, known as features, predictors, and dependent variables. As an example, if we were given a dataset of toll booth charges and were tasked with predicting the amount charged based on the time of day and other factors, a hypothetical example could be as follows:</p>
<div><img height="132" width="348" class=" image-border" src="img/3fbda53b-25e6-41f7-996f-4c992c3bbee5.png"/></div>
<p>In this spreadsheet, the columns <strong>date</strong>, <strong>time</strong>, <strong>agency</strong>, <strong>type</strong>, <strong>prepaid</strong>, and <strong>rate</strong> are the features or predictors, whereas, the column <strong>amount</strong> is our outcome or dependent variable (what we are predicting).</p>
<p>The value of amount <em>depends</em> on the value of the other variables (which are thus known as <em>independent variables</em>).</p>
<p>Simple equations also reflect the obvious distinction, for example, in an equation, <em>y = a + b + c</em>, the <strong>left hand side</strong> (<strong>LHS</strong>) is the dependent/outcome variable and <em>a</em>, <em>b</em> and <em>c</em> are the features/predictors.</p>
<p>In summary:</p>
<div><img height="209" width="398" class=" image-border" src="img/09eb5136-5bcd-42a3-960d-e0cefa6602b0.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">The core concepts in machine learning</h1>
                
            
            
                
<p>There are many important concepts in machine learning; we'll go over some of the more common topics. Machine learning involves a multi-step process that starts with data acquisition, data mining, and eventually leads to building the predictive models.</p>
<p>The key aspects of the model-building process involve:</p>
<ul>
<li><strong>Data pre-processing</strong>: Pre-processing and feature selection (for example, centering and scaling, class imbalances, and variable importance)</li>
<li><strong>Train, test splits and cross-validation</strong>:
<ul>
<li>Creating the training set (say, 80 percent of the data)</li>
<li>Creating the test set (~ 20 percent of the data)</li>
<li>Performing cross-validation</li>
</ul>
</li>
<li><strong>Create model, get predictions</strong>:
<ul>
<li>Which algorithms should you try?</li>
<li>What accuracy measures are you trying to optimize?</li>
<li>What tuning parameters should you use?</li>
</ul>
</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Data management steps in machine learning</h1>
                
            
            
                
<p>Pre-processing, or more generally processing the data, is an integral part of most machine learning exercises. A dataset that you start out with is seldom going to be in the exact format against which you'll be building your machine learning models; it will invariably require a fair amount of cleansing in the majority of cases. In fact, data cleansing is often the most time-consuming part of the entire process. In this section, we will briefly highlight a few of the top data processing steps that you may encounter in practice.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Pre-processing and feature selection techniques</h1>
                
            
            
                
<p><strong>Data pre-processing</strong>, as the name implies, involves curating the data to make it suitable for machine learning exercises. There are various methods for pre-processing and a few of the more common ones have been illustrated here.</p>
<p>Note that data pre-processing should be performed as part of the cross-validation step, that is, pre-processing should not be done <em>before the fact</em>, but rather during the model-building process. This will be explained in more detail afterward.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Centering and scaling</h1>
                
            
            
                
<p>Applying center and scale function on numeric columns is often done in order to standardize data and remove the effect of large variations in the magnitude or differences of numbers. You may have encountered this in college or university courses where students would be graded on a standardized basis, or a curve.</p>
<p>For instance, say an exam paper was unusually difficult and half of all the students in a class of 10 students received scores below 60 - the passing rate set for the course. The professor can either a) make a determination that 50% of the students should re-take the course, or b) standardize the scores to find how students performed relative to one another.</p>
<p>Say the class scores were:</p>
<p>45,66,66,55,55,52,61,64,65,49</p>
<p>With the passing score set at 60, this implies that the students who scored 45, 55, 55, 52 and 49 will not successfully complete the course.</p>
<p>However, this might not be a truly accurate representation of their relative merits. The professor may alternatively choose to instead use a center-and-scale method, commonly known as standardization, which involves:</p>
<ul>
<li>Finding the mean of all the scores</li>
<li>Subtracting the mean from the scores</li>
<li>Dividing the result by the standard deviation of all the scores</li>
</ul>
<p>The operation is illustrated below for reference.</p>
<p>The mean of the scores is 57.8. Hence, subtracting 57.8 from each of the numbers produce the numbers shown in the second row. But, we are not done yet. We need to divide the numbers by the <em>standard deviation</em> of the scores to get the final standardized values:</p>
<div><img height="55" width="420" class=" image-border" src="img/4b4b2a16-5582-41f7-84df-f219eec1c0e6.png"/></div>
<p>Dividing by the <strong>SD</strong> (<strong>standard deviation</strong>) shows that there were only two students whose scores were below one standard deviation across the range of all the test scores. Hence, instead of five students who do not complete the course successfully based on the raw numbers, we can narrow it down to only two students.</p>
<p>Although this is a truly simple operation, it is not hard to see that it is very effective in smoothing out large variations in data.</p>
<p>Centering and scaling can be performed very easily in R using the scale command as shown here:</p>
<pre>&gt; scores &lt;- c(45,66,66,55,55,52,61,64,65,68) 
&gt; scale(scores) 
            [,1] 
 [1,] -1.9412062 
 [2,]  0.8319455 
 [3,]  0.8319455 
 [4,] -0.6206578 
 [5,] -0.6206578 
 [6,] -1.0168223 
 [7,]  0.1716713 
 [8,]  0.5678358 
 [9,]  0.6998907 
[10,]  1.0960552 
attr(,"scaled:center") 
[1] 59.7 
attr(,"scaled:scale") 
[1] 7.572611 </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">The near-zero variance function</h1>
                
            
            
                
<p>The near-zero variance, available in the <kbd>nearZeroVar</kbd> function in the <kbd>R package, caret</kbd>, is used to identify variables that have little or no variance. Consider a set of 10,000 numbers with only three distinct values. Such a variable may add very little value to an algorithm. In order to use the <kbd>nearZeroVar</kbd> function, first install the R package, caret, in RStudio (which we had set up <a href="5ca02405-8ab4-4274-8611-af003aab7c9f.xhtml" target="_blank">Chapter 3</a>, <em>The Analytics Toolkit</em>. The exact code to replicate the effect of using <kbd>nearZeroVar</kbd> is shown here:</p>
<pre>&gt; library(caret) 
Loading required package: lattice 
Loading required package: ggplot2 
Need help getting started? Try the cookbook for R: http://www.cookbook-r.com/Graphs/ 
 
&gt; repeated &lt;- c(rep(100,9999),10) # 9999 values are 100 and the last value is 10 
 
&gt;random&lt;- sample(100,10000,T) # 10,000 random values from 1 - 100 
 
&gt;data&lt;- data.frame(random = random, repeated = repeated) 
 
&gt;nearZeroVar(data) 
[1] 2 
 
&gt; names(data)[nearZeroVar(data)] 
[1] "repeated" </pre>
<p>As the example shows, the function was able to correctly detect the variable that met the criteria.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Removing correlated variables</h1>
                
            
            
                
<p>Correlated variables can produce results that over-emphasize the contribution of the variables. In regression exercises, this has the effect of increasing the value of R^2, and does not accurately represent the actual performance of the model. Although many classes of machine learning algorithms are resistant to the effects of correlated variables, it deserves some mention as it is a common topic in the discipline.</p>
<p>The premise of removing such variables is related to the fact that redundant variables do not add incremental value to a model. For instance, if a dataset contained height in inches and height in meters, these variables would have a near exact correlation of 1, and using one of them is just as good as using the other. Practical exercises that involve variables that we cannot judge intuitively, using methods of removing correlated variables, can greatly help in simplifying the model.</p>
<p>The following example illustrates the process of removing correlated variables. The dataset, <strong>Pima Indians Diabetes</strong>, contains vital statistics about the diet of Pima Indians and an outcome variable called <kbd>diabetes</kbd>.</p>
<p>During the course of the examples in successive chapters, we will refer to this dataset often. A high level overview of the meaning of the different columns in the dataset is as follows:</p>
<pre>pregnant Number of times pregnant 
glucose  Plasma glucose concentration (glucose tolerance test) 
pressure Diastolic blood pressure (mm Hg) 
triceps  Triceps skin fold thickness (mm) 
insulin  2-Hour serum insulin (mu U/ml) 
mass     Body mass index (weight in kg/(height in m)\^2) 
pedigree Diabetes pedigree function 
age            Age (years) 
diabetes Class variable (test for diabetes) </pre>
<p>We are interested in finding out if any of the variables, apart from diabetes (which is our outcome variable) are correlated. If so, it may be useful to remove the redundant variables.</p>
<p>Install the packages <kbd>mlbench</kbd> and <kbd>corrplot</kbd> in RStudio and execute the commands as shown here:</p>
<pre><strong>install.packages("mlbench") 
install.packages("corrplot") 
 
library(corrplot) 
library(mlbench) 
</strong><br/><strong>data (PimaIndiansDiabetes)
diab &lt;- PimaIndiansDiabetes 
 
</strong># To produce a correlogram 
<strong>corrplot(cor(diab[,-ncol(diab)]), method="color", type="upper")
 
</strong># To get the actual numbers 
<strong>corrplot(cor(diab[,-ncol(diab)]), method="number", type="upper")</strong></pre>
<p>The command will produce a plot as shown here using the <kbd>corrplot</kbd> package from <a href="http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram" target="_blank">http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram</a>:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<div><img class=" image-border" src="img/edccd408-6cf1-49de-b18d-a77282d496a8.png"/></div>
</td>
<td>
<p>&gt;<img class=" image-border" src="img/81bbdc2b-2af2-4adb-93c0-b35330a18bb5.png"/></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The darker the shade, the higher the correlation. In this case, it shows that age and pregnancy have a relatively high correlation. We can find the exact values by using <kbd>method="number"</kbd> as shown. You can also view the plot at <a href="http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram" target="_blank">http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram</a>.</p>
<p>We can also use functions such as the following to directly find the correlated variables without plotting the correlograms:</p>
<pre><strong>correlated_columns&lt;- findCorrelation(cor(diab[,-ncol(diab)]), cutoff = 0.5) 
correlated_columns</strong> </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Other common data transformations</h1>
                
            
            
                
<p>Several other data transformations are available and applicable to different situations. A summary of these transformations can be found at the documentation site for the <kbd>caret</kbd> package under <strong>Pre-Processing</strong> at <a href="https://topepo.github.io/caret/pre-processing.html" target="_blank">https://topepo.github.io/caret/pre-processing.html</a>.</p>
<p>The options available in the pre-process function of caret can be found from its help section, by running the command <kbd>?preProcess</kbd> in RStudio. The code for it is given here:</p>
<pre>Method 
 
a character vector specifying the type of processing. 
 
Possible values are "BoxCox", "YeoJohnson", "expoTrans", "center", "scale", "range", "knnImpute", "bagImpute", "medianImpute", "pca", "ica", "spatialSign", "corr", "zv", "nzv", and "conditionalX" (see Details below) </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Data sampling</h1>
                
            
            
                
<p>You may encounter datasets that have a high level of imbalanced outcome classes. For instance, if you were working with a dataset on a rare disease, with your outcome variable being true or false, due to the rarity of the occurrence, you may find that the number of observations marked as false (that is, the person did not have the rare disease) is much higher than the number of observations marked as true (that is, the person did have the rare disease).</p>
<p>Machine learning algorithms attempt to maximize performance, which in many cases could be the accuracy of the predictions. Say, in a sample of 1000 records, only 10 are marked as true and the rest of the <kbd>990</kbd> observations are false.</p>
<p>If someone were to randomly assign <em>all</em> observations as false, the accuracy rate would be:</p>
<pre>(990/1000) * 100 = 99% </pre>
<p>But, the objective of the exercise was to find the <em>individuals who had the rare disease</em>. We are already well aware that due to the nature of the disease, most individuals will not belong to the category.</p>
<p>Data sampling, in essence, is the process of <em>maximizing machine learning metrics such as specificity, sensitivity, precision, recall, and kappa</em>. These will be discussed at a later stage, but for the purposes of this section, we'll show some ways by which you can <em>sample</em> the data so as to produce a more evenly balanced dataset.</p>
<p>The R package, <kbd>caret</kbd>, includes several helpful functions to create a balanced distribution of the classes from an imbalanced dataset.</p>
<p>In these cases, we need to re-sample the data to get a better distribution of the classes in order to build a more effective model.</p>
<p>Some of the general methods include:</p>
<ul>
<li><strong>Up-sample</strong>: Increase instances of the class with lesser examples</li>
<li><strong>Down-sample</strong>: Reduce the instances of the class with higher examples</li>
<li><strong>Create synthetic examples</strong> (for example, <strong>SMOTE</strong> (<strong>Synthetic Minority Oversampling TechniquE</strong>))</li>
<li>Random oversampling (for example, (<strong>ROSE</strong>) <strong>Randomly OverSampling Examples</strong>)</li>
</ul>
<p class="mce-root">We will create a simulated dataset using the same data from the prior example where 95% of the rows will be marked as negative:</p>
<pre><strong>library(mlbench) 
library(caret) 
diab&lt;- PimaIndiansDiabetes</strong> 
 
<strong>diabsim&lt;- diab 
diabrows&lt;- nrow(diabsim) 
negrows&lt;- floor(.95 * diabrows) 
posrows&lt;- (diabrows - negrows) 
</strong> 
negrows 
[1] 729 
 
posrows 
[1] 39 
 
<strong>diabsim$diabetes[1:729]     &lt;- as.factor("neg")</strong>
<strong>diabsim$diabetes[-c(1:729)] &lt;- as.factor("pos")</strong>
<strong>table(diabsim$diabetes) 
</strong> 
neg. pos 
729  39 
 
# We observe that in this simulated dataset, we have 729 occurrences of positive outcome and 39 occurrences of negative outcome
 
# Method 1: Upsampling, i.e., increasing the number of observations marked as 'pos' (i.e., positive) 
 
<strong>upsampled_simdata&lt;- upSample(diabsim[,-ncol(diabsim)], diabsim$diabetes) 
table(upsampled_simdata$Class)</strong> 
 
negpos 
729 729 
 
# NOTE THAT THE OUTCOME IS CALLED AS 'Class' and not 'diabetes' 
# This is because of the use of the variable separately 
# We can always rename the column to revert to the original name 
 
# Method 2: Downsampling, i.e., reducing the number of observations marked as 'pos' (i.e., positive) 
 
<strong>downsampled_simdata&lt;- downSample(diabsim[,-ncol(diabsim)], diabsim$diabetes) 
</strong>
<strong>table(downsampled_simdata$Class) 
</strong> 
neg pos 
39  39 </pre>
<ul>
<li class="mce-root">The <strong>SMOTE</strong> (<strong>Synthetic Minority Over-sampling TechniquE</strong>) is a third method that, instead of plain vanilla up-/down-sampling, creates synthetic records from the nearest neighbors of the minority class. In our simulated dataset, it is obvious that <kbd>neg</kbd> is the minority class, that is, the class with the lowest number of occurrences.<br/>
The help file on the SMOTE function explains the concept succinctly:</li>
</ul>
<p>Unbalanced classification problems cause problems to many learning algorithms. These problems are characterized by the uneven proportion of cases that are available for each class of the problem.</p>
<p style="padding-left: 60px">SMOTE (Chawla et al., 2002) is a well-known algorithm to fight this problem. The general idea of this method is to artificially generate new examples of the minority class using the nearest neighbors of these cases. Furthermore, the majority class examples are also under-sampled, leading to a more balanced dataset:</p>
<pre style="padding-left: 60px"># Method 3: SMOTE 
# The function SMOTE is available in the R Package DMwR 
# In order to use it, we first need to install DmWR as follows 
 
<strong>install.packages ("DMwR") 
</strong> 
# Once the package has been installed, we will create a synthetic 
# Dataset in which we will increase the number of 'neg' records 
# Let us check once again the distribution of neg/pos in the dataset 
 
<strong>table(diabsim$diabetes) 
</strong> 
negpos 
729  39 
 
# Using SMOTE we can create synthetic cases of 'pos' as follows 
 
diabsyn&lt;- SMOTE(diabetes ~ ., diabsim, perc.over = 500, perc.under = 150) 
 
# perc.over = 500 means, increase the occurrence of the minority 
# class by 500%, i.e., 39 + 5*39 = 39 + 195 = 234 
 
# perc.under = 150 means, that for each new record generated for the 
# Minority class, we will generate 1.5 cases of the majority class 
# In this case, we created 195 new records (500% of 39) and hence 
# we will generate 150% of 195 records = 195 * 150% = 195 * 1.5 
# = 292.5, or 292 (rounded down) new records 
 
# We can verify this by running the table command against the newly 
# Created synthetic dataset, diabsyn 
 
<strong>table(diabsyn$diabetes) 
</strong> 
negpos 
292 234</pre>
<ul>
<li><strong>ROSE (Randomly OverSampling Examples)</strong>, the final method in this section, is available via the ROSE package in R. Like SMOTE, it is a method for generating synthetic samples. The help file for ROSE states the high-level use of the function as follows:</li>
</ul>
<p>Generation of synthetic data by Randomly Over Sampling Examples creates a sample of synthetic data by enlarging the features space of minority and majority class examples. Operationally, the new examples are drawn from a conditional kernel density estimate of the two classes, as described in Menardi and Torelli (2013).</p>
<pre><strong>install.packages("ROSE") 
library(ROSE)</strong> 
 
# Loaded ROSE 0.0-3 
<strong>set.seed(1) 
</strong> 
<strong>diabsyn2 &lt;- ROSE(diabetes ~ ., data=diabsim) 
</strong> 
<strong>table(diabsyn2$data$diabetes) 
</strong> 
# negpos 
# 395 373 </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Data imputation</h1>
                
            
            
                
<p>Sometimes, your data may have missing values. This could be due to errors in the data collection process, genuinely missing data, or any other reason, with the net result being that the information is not available. Real world examples of missing data can be found in surveys where the respondent did not answer a specific question on the survey.</p>
<p>You may have a dataset of, say, 1,000 records and 20 columns of which a certain column has 100 missing values. You may choose to discard this column altogether, but that also means discarding 90 percent of the information. You still have 19 other columns that have complete data. Another option is to simply exclude the column, but that means you cannot leverage the benefit afforded by the data that is available in the respective column.</p>
<p>Several methods exist for data imputation, that is, the process of filling in missing data. We do not know what the exact values are, but by looking at the other entries in the table, we may be able to make an educated and systematic assessment of what the values might be.</p>
<p>Some of the common methods in data imputation involve:</p>
<ul>
<li><strong>Mean, median, mode imputation</strong>: Substituting the missing values using the mean, median, or mode value for the column. This, however, has the disadvantage of increasing the correlation among the variables that are imputed, which might not be desirable for multivariate analysis.</li>
<li><strong>K-nearest neighbors imputation</strong>: kNN imputation is a process of using a machine learning approach (nearest-neighbors) in order to impute missing values. It works by finding k records that are most similar to the one that has missing values and calculates the weighted average using Euclidean distance relative to k records.</li>
<li><strong>Imputation using regression models</strong>: Regression methods use standard regression methods in R to predict the value of the missing variables. However, as noted in the respective section on Regression-based imputation on Wikipedia <a href="https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression" target="_blank">https://en.wikipedia.org/wiki/Imputation_(statistics)#Regression</a>, the problem (with regression imputation) is that the imputed data do not have an error term included in their estimation. Thus, the estimates fit perfectly along the regression line without any residual variance. This causes relationships to be over identified and suggests greater precision in the imputed values than is warranted.</li>
<li><strong>Hot-deck imputation</strong>: Another technique for filling missing values with observations from the dataset itself. This method, although very prevalent, does have a limitation in that, by assigning say, a single value, to a large range of missing values, it could add a significant bias in the observations and can produce misleading results.</li>
</ul>
<p>A short example has been provided here to demonstrate how imputation can be done using kNN Imputation. We simulate missing data by changing a large number of values to NA in the <kbd>PimaIndiansDiabetes</kbd> dataset.</p>
<p class="CDPAlignLeft CDPAlign">We make use of the following factors for the process:</p>
<ul>
<li>We use mean to fill in the NA values.</li>
<li>We use kNN imputation to fill in the missing values. We then compare how the two methods performed:</li>
</ul>
<pre><strong>library(DMwR) 
library(caret) 
 
diab&lt;- PimaIndiansDiabetes</strong> 
 
# In the dataset, the column mass represents the body mass index 
# Of the individuals represented in the corresponding row 
 
# mass: Body mass index (weight in kg/(height in m)\^2) 
 
# Creating a backup of the diabetes dataframe 
<strong>diabmiss_orig&lt;- diab 
</strong> 
# Creating a separate dataframe which we will modify 
<strong>diabmiss&lt;- diabmiss_orig 
</strong> 
# Saving the original values for body mass 
<strong>actual &lt;- diabmiss_orig$mass 
</strong> 
# Change 91 values of mass to NA in the dataset 
<strong>diabmiss$mass[10:100] &lt;- NA 
</strong> 
# Number of missing values in mass 
<strong>sum(is.na(diabmiss$mass)) 
</strong> 
# 91 
 
# View the missing values 
<strong>diabmiss[5:15,]</strong> </pre>
<p>We get the output as follows:</p>
<div><img height="172" width="426" class=" image-border" src="img/0dec61a7-716d-4f85-9804-2290ca5419ea.png"/></div>
<pre># Test with using the mean, we will set all the missing values 
# To the mean value for the column 
 
<strong>diabmiss$mass[is.na(diabmiss$mass)] &lt;- mean(diabmiss$mass,na.rm = TRUE) 
</strong> 
# Check the values that have been imputed 
<strong>data.frame(actual=actual[10:100], impute_with_mean=diabmiss$mass[10:100])</strong> </pre>
<p>The output of the preceding code is as follows:</p>
<div><img height="134" width="411" class=" image-border" src="img/6ad62a66-066c-4ce9-b69b-258304df178c.png"/></div>
<pre># Check the Root-Mean-Squared-Error for the entire column 
# Root Mean Squared Error provides an estimate for the 
# Difference between the actual and the predicted values 
# On 'average' 
 
<strong>diabmissdf&lt;- data.frame(actual=actual, impute_with_mean=diabmiss$mass) 
rmse1 &lt;- RMSE(diabmissdf$impute_with_mean,actual) 
rmse1</strong> 
 
# [1] 3.417476 
 
# We will re-run the exercise using knnImputation (from package DMwR) 
 
# Change the value of the records back to NA 
<strong>diabmiss&lt;- diabmiss_orig 
diabmiss$mass[10:100] &lt;- NA</strong> 
 
# Perform knnImputation 
<strong>diabknn&lt;- knnImputation(diabmiss,k=25) 
</strong> 
# Check the RMSE value for the knnImputation method 
<strong>rmse2 &lt;- RMSE(diabknn$mass,actual) 
rmse2</strong> 
 
# [1] 3.093827 
 
# Improvement using the knnImputation methods in percentage terms 
 
<strong>100 * (rmse1-rmse2)/rmse1 
</strong> 
[1] 22.20689 </pre>
<p>While it may not represent a dramatic change, it's still better than using a naïve approach such as using simply a mean or constant value.</p>
<p>There are several packages in R for data imputation. A few prominent ones are as follows:</p>
<ul>
<li><strong>Amelia II</strong>: Missing information in time-series data</li>
</ul>
<p style="padding-left: 90px"><a href="https://gking.harvard.edu/amelia" target="_blank">https://gking.harvard.edu/amelia</a></p>
<ul>
<li><strong>Hot-deck imputation with R package</strong>: HotDeckImputation and hot.deck</li>
</ul>
<p style="padding-left: 90px"><a href="https://cran.r-project.org/web/packages/HotDeckImputation/" target="_blank">https://cran.r-project.org/web/packages/HotDeckImputation/</a></p>
<p style="padding-left: 90px"><a href="https://cran.r-project.org/web/packages/hot.deck/" target="_blank">https://cran.r-project.org/web/packages/hot.deck/</a></p>
<ul>
<li><strong>Multivariate imputation (by Chained Equations)</strong></li>
</ul>
<p style="padding-left: 90px"><a href="https://cran.r-project.org/web/packages/mice/index.html" target="_blank">https://cran.r-project.org/web/packages/mice/index.html</a></p>
<ul>
<li><strong>Imputing values in a Bayesian framework with R package</strong>: mi</li>
</ul>
<p style="padding-left: 90px"><a href="https://cran.r-project.org/web/packages/mi/index.html" target="_blank">https://cran.r-project.org/web/packages/mi/index.html</a></p>


            

            
        
    

        

                            
                    <h1 class="header-title">The importance of variables</h1>
                
            
            
                
<p>During model-building exercises, datasets may have tens of variables. Not all of them may add value to the predictive model. It is not uncommon to reduce the dataset to include a subset of the variables and allow the machine learning programmer to devote more time toward fine-tuning the chosen variables and the model-building process. There is also a technical justification for reducing the number of variables in the dataset. Performing machine learning modeling on very large, that is, high dimensional datasets can be very compute-intensive, that is, it may require a significant amount of time, CPU, and RAM to perform the numerical operations. This not only makes the application of certain algorithms impractical, it also has the effect of causing unwarranted delays. Hence, the methodical selection of variables helps both in terms of analysis time as well as computational requirements for algorithmic analysis.</p>
<p><strong>Variable selection</strong> is also known as feature selection/attribute selection. Algorithms such as random forests and lasso regression implement variable selection as part of their algorithmic operations. But, variable selection can be done as a separate exercise.</p>
<p>The R package, <kbd>caret</kbd>, provides a very simple-to-use and intuitive interface for variable selection. As we haven't yet discussed the modeling process, we will learn how to find the important variables, and in the next chapter delve deeper into the subject.</p>
<p>We'll use a common, well-known algorithm called <kbd>RandomForest</kbd> that is used for building decision trees. The algorithm will be described in more detail in the next chapter, but the purpose of using it here is merely to show how variable selection can be performed. The example is illustrative of what the general process is.</p>
<p>We'll re-use the dataset we have been working with, that is, the <kbd>PimaIndiansDiabetes</kbd> data from the <kbd>mlbench</kbd> package. We haven't discussed the model training process yet, but it has been used here in order to derive the values for variable importance. The outcome variable in this case is diabetes and the other variables are used as the independent variables. In other words, can we predict if the person has diabetes using the data available:</p>
<pre><strong>diab&lt;- PimaIndiansDiabetes 
</strong> 
# We will use the createDataPartition function from caret to split 
# The data. The function produces a set of indices using which we 
# will create the corresponding training and test sets 
 
<strong>training_index&lt;- createDataPartition(diab$diabetes, p = 0.80, list = FALSE, times = 1) 
</strong> 
# Creating the training set 
<strong>diab_train&lt;- diab[training_index,] 
</strong> 
# Create the test set 
<strong>diab_test&lt;- diab[-training_index,] 
</strong> 
# Create the trainControl parameters for the model 
<strong>diab_control&lt;- trainControl("repeatedcv", number = 3, repeats = 2, classProbs = TRUE, summaryFunction = twoClassSummary)</strong> 
 
# Build the model 
<strong>rf_model&lt;- train(diabetes ~ ., data = diab_train, method = "rf", preProc = c("center", "scale"), tuneLength = 5, trControl = diab_control, metric = "ROC") 
</strong> 
# Find the Variable Importance 
<strong>varImp(rf_model) </strong><br/>rf variable importance 
 
         Overall 
glucose  100.000 
mass      52.669 
age       39.230 
pedigree  24.885 
pressure  12.619 
pregnant   6.919 
insulin    2.294 
triceps    0.000 
 
# This indicates that glucose levels, body mass index and age are the top 3 predictors of diabetes. 
 
# caret also includes several useful plot functions. We can visualize the variable importance using the command: 
 
<strong>plot(varImp(rf_model))</strong> </pre>
<p>The output of the preceding code is as shown below. It indicates that glucose, mass and age were the variables that contributed the most towards creating the model (to predict diabetes)</p>
<div><img height="372" width="448" class=" image-border" src="img/d6f247da-1244-4e39-b6e8-fd56c9607d9f.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">The train, test splits, and cross-validation concepts</h1>
                
            
            
                
<p>The train, test splits, and cross-validation sets are a fundamental concept in machine learning. This is one of the areas where a pure statistical approach differs materially from the machine learning approach. Whereas in a statistical modeling task, one may perform regressions, parametric/non-parametric tests, and apply other methods, in machine learning, the algorithmic approach is supplemented with an element of iterative assessment of the results being produced and subsequent improvisation of the model with each iteration.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Splitting the data into train and test sets</h1>
                
            
            
                
<p>Every machine learning modeling exercise begins with the process of data cleansing, as discussed earlier. The next step is to split the data into a train and test set. This is usually done by randomly selecting rows from the data that will be used to create the model. The rows that weren't selected would then be used to test the final model.</p>
<p>The usual split varies between 70-80 percent (training data versus test data). In an 80-20 split, 80% of the data would be used in order to create the model. The remaining 20% would be used to test the final model produced.</p>
<p>We applied this in the earlier section, but we can revisit the code once again. The <kbd>createDataPartition</kbd> function was used with the parameter <kbd>p = 0.80</kbd> in order to split the data. The <kbd>training_index</kbd> variable holds the training indices (of the <kbd>dataset</kbd>, <kbd>diab</kbd>) that we will use:</p>
<pre><strong>training_index&lt;- createDataPartition(diab$diabetes, p = 0.80, list = FALSE, times = 1) 
</strong> 
<strong>length(training_index) # Number of items that we will select for the train set 
</strong>[1] 615 
 
<strong>nrow(diab) # The total number of rows in the dataset 
</strong>[1] 768 
 
# Creating the training set, this is the data we will use to build our model 
<strong>diab_train&lt;- diab[training_index,] 
</strong> 
# Create the test set, this is the data against which we will test the performance of our model 
<strong>diab_test&lt;- diab[-training_index,]</strong> </pre>
<p>We do not have to necessarily use the <kbd>createDataPartition</kbd> function and instead, a random sample created using simple R commands as shown here will suffice:</p>
<pre># Create a set of random indices representing 80% of the data 
<strong>training_index2 &lt;- sample(nrow(diab),floor(0.80*nrow(diab))) 
</strong> 
# Check the size of the indices just created 
<strong>length(training_index2) 
</strong>[1] 614 
 
# Create the training set 
<strong>diab_train2 &lt;- diab[training_index2,] 
</strong> 
# Create the test set 
<strong>diab_test2 &lt;- diab[-training_index2]</strong> </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">The cross-validation parameter</h1>
                
            
            
                
<p>Cross-validation takes the train-test split concept to the next stage. The aim of the machine learning exercise is, in essence, to find what set of model parameters will provide the best performance. A model parameter indicates the arguments that the function (the model) takes. For example, for a decision tree model, parameters may include the number of levels deep the model should be built, number of splits, and so on. If, say, there are <em>n</em> different parameters, each having <em>k</em> different values, the total number of parameters would be <em>k</em>^<em>n</em>. We generally select a fixed set of combinations for each of the parameters and could easily end with 100-1000+ combinations. We will test the performance of the model (for example, accuracy in predicting the outcome correctly) for each of the parameters.</p>
<p>With a simple train-test split, say, if there were 500 combinations of parameters we had selected, we just need to run them against the training dataset and determine which one shows the optimal performance.</p>
<p>With cross-validation, we further split the training set into smaller subsets, for example, three- or five-fold is commonly used. If there are three folds, that is, we split the training set into three subsets, we keep aside one fold, say, Fold 2, and create a model using a set of parameters using Folds 1 and 3. We then test its accuracy against Fold 2. This step is repeated several times, with each iteration representing a unique set of folds on which the training-test process is being executed and accuracy measures are collected. Eventually, we would arrive at an optimal combination by selecting the parameters that showed the best overall performance.</p>
<p>The standard approach can be summarized as follows:</p>
<ol>
<li>Create an 80-20 train-test split</li>
<li>Execute your model(s) using different combinations of model parameters</li>
<li>Select the model parameters that show the best overall performance and create the final model</li>
<li>Apply the final model on the test set to see the results</li>
</ol>
<p>The cross-validation approach mandates that we should further split the training dataset into smaller subsets. These subsets are generally known as <strong>folds</strong> and collectively they are known as the <strong>k-folds</strong>, where <em>k</em> represents the number of splits:</p>
<ol>
<li>Create an 80-20 train-test split</li>
<li>Split the training set into k-folds, say, three folds</li>
<li>Set aside Fold 1 and build a model using Fold 2 and Fold 3</li>
<li>Test your model performance on Fold 1 (for example, the percentage of accurate results)</li>
<li>Set aside Fold 2 and build a model using Fold 1 and Fold 3</li>
<li>Test your model performance on Fold 2</li>
<li>Set aside Fold 3 and build a model using Fold 1 and Fold 2</li>
<li>Test your model performance on Fold 3</li>
<li>Take the average performance of the model across all three folds</li>
<li>Repeat Step 1 for <em>each set of model parameters</em></li>
<li>Select the model parameters that show the best overall performance and create the final model</li>
</ol>
<ol start="12">
<li>Apply the final model on the test set to see the results</li>
</ol>
<div><img height="311" width="402" class=" image-border" src="img/c300d7bd-53ec-426a-9b9d-71455234a676.png"/></div>
<p>This image illustrates the difference between using an approach without cross-validation and one with cross-validation. The cross-validation method is arguably more robust and involves a rigorous evaluation of the model. That said, it is often useful to attempt creating a model initially without cross-validation to get a sense of the kind of performance that may be expected. For example, if a model built with say 2-3 training-test splits shows a performance of say, 30% accuracy, it is unlikely that any other approach, including cross-validation would somehow make that 90%. In other words the standard approach helps to get a sense of the kind of performance that may be expected. As cross-validations can be quite compute-intensive and time consuming getting an initial feedback on performance helps in a preliminary analysis of the overall process.</p>
<p>The caret package in R provides a very user-friendly approach to building models using cross-validation. Recall that data pre-processing must be passed or made an integral part of the cross-validation process. So, say, we had to center and scale the dataset and perform a five-fold cross-validation, all we would have to do is define the type of sampling we'd like to use in caret's <kbd>trainControl</kbd> function.</p>
<p>Caret's webpage on <kbd>trainControl</kbd> provides a detailed overview of the functions along with worked-out examples at <a href="https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning" target="_blank">https://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning</a>.</p>
<p>We have used this approach in our earlier exercise where we built a model using <kbd>RandomForest</kbd> on the <kbd>PimaIndiansDiabetes</kbd> dataset. It is shown again here to indicate where the technique was used:</p>
<pre># Create the trainControl parameters for the model 
# The parameters indicate that a 3-Fold CV would be created 
# and that the process would be repeated 2 times (repeats) 
# The class probabilities in each run will be stored 
# And we'll use the twoClassSummary* function to measure the model 
# Performance 
<strong>diab_control&lt;- trainControl("repeatedcv", number = 3, repeats = 2, classProbs = TRUE, summaryFunction = twoClassSummary) 
</strong> 
# Build the model 
# We used the train function of caret to build the model 
# As part of the training process, we specified a tunelength** of 5 
# This parameter lets caret select a set of default model parameters 
# trControl = diab_control indicates that the model will be built 
# Using the cross-validation method specified in diab_control 
# Finally preProc = c("center", "scale") indicate that the data 
# Would be centered and scaled at each pass of the model iteration 
 
<strong>rf_model&lt;- train(diabetes ~ ., data = diab_train, method = "rf", preProc = c("center", "scale"), tuneLength = 5, trControl = diab_control, metric = "ROC")</strong> </pre>
<p>You can get a more detailed explanation of <kbd>summaryFunction</kbd> from <a href="https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf" target="_blank">https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf</a>.</p>
<p>The <kbd>summaryFunction</kbd> argument is used to pass in a function that takes the observed and predicted values and estimates some measure of performance. Two such functions are already included in the package: <kbd>defaultSummary</kbd> and <kbd>twoClassSummary</kbd>. The latter will compute measures specific to two-class problems, such as the area under the ROC curve, the sensitivity and specificity. Since the ROC curve is based on the predicted class probabilities (which are not computed automatically), another option is required. The <kbd>classProbs = TRUE</kbd> option is used to include these calculations.</p>
<p>Here is an explanation of <kbd>tuneLength</kbd> from the help file for the train function of <kbd>caret</kbd>.</p>
<p><kbd>tuneLength</kbd> is an integer denoting the amount of granularity in the tuning parameter grid. By default, this argument is the number of levels for each tuning parameter that should be generated by train. If <kbd>trainControl</kbd> has the option <kbd>search = random</kbd>, this is the maximum number of tuning parameter combinations that will be generated by the random search.</p>
<p>Note that if this argument is given it must be named.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating the model</h1>
                
            
            
                
<p>The final step after creating the model is to use the model against the test dataset to get the predictions. This is generally done using the <kbd>predict</kbd> function in R, with the first argument being the model that was created and the second argument being the dataset against which you'd like to get the predictions for.</p>
<p>Taking our example of the <kbd>PimaIndiansDiabetes</kbd> dataset, after the model has been built, we can get the predictions on the test dataset as follows:</p>
<pre># Install the R Package e1071, if you haven't already 
# By running install.packages("e1071") 
 
# Use the predict function and the rf_model that was previously built 
# To get the predictions on the test dataset 
# Note that we are not including the column diabetes in the test 
# dataset by using diab_test[,-ncol(diab_test)] 
 
<strong>predictions&lt;- predict(rf_model, diab_test[,-ncol(diab_test)]) 
</strong> 
# First few records predicted 
<strong>head(predictions)</strong><br/>[1] negnegpospospospos 
Levels: negpos 
 
# The confusion matrix allows us to see the number of true positives 
# False positives, True negatives and False negatives 
 
<strong>cf&lt;- confusionMatrix(predictions, diab_test$diabetes) 
cf</strong> 
 
# Confusion Matrix and Statistics 
#  
#        Reference 
# Prediction negpos 
#        neg  89  21 
#        pos  11  32 
#  
# Accuracy : 0.7908           
# 95% CI : (0.7178, 0.8523) 
# No Information Rate : 0.6536           
# P-Value [Acc&gt; NIR] : 0.0001499        
#  
# Kappa : 0.5167           
# Mcnemar's Test P-Value : 0.1116118        
#  
# Sensitivity : 0.8900           
# Specificity : 0.6038           
# PosPredValue : 0.8091           
# NegPredValue : 0.7442           
# Prevalence : 0.6536           
# Detection Rate : 0.5817           
# Detection Prevalence : 0.7190           
# Balanced Accuracy : 0.7469           
#  
# 'Positive' Class :neg </pre>
<p>Let's check what the confusion matrix tells us:</p>
<pre># This indicates that of the records that were marked negative (neg) 
# We predicted 89 of them as negative and 11 as positive (i.e., they 
# were negative but we incorrectly classified them as a positive 
 
# We correctly identified 32 positives but incorrectly classified 
# 21 positives as negative 
 
# 
#           Reference 
# Prediction neg  pos 
#        neg  89  21 
#        pos  11  32 
 
# The overall accuracy was 79% 
# This can be improved (significantly) by using more  
# Accuracy : 0.7908           
 
# We can plot the model using plot(rf_model) as follows 
<strong>plot(rf_model)</strong> </pre>
<p>The plot is as follows:</p>
<div><img height="264" width="317" class=" image-border" src="img/7d0a354a-6134-40f5-adc9-2a7a7932b0f5.png"/></div>
<pre># And finally we can also visualize our confusion matrix using the 
# inbuilt fourfoldplot function in R 
 
<strong>fourfoldplot(cf$table)</strong> </pre>
<p>We get the plot as follows:</p>
<div><img height="198" width="238" class=" image-border" src="img/08ce8ffe-fa37-435e-a852-bd95dd78a78d.png"/></div>
<p>Per the documentation of <em><kbd>fourfoldplot</kbd></em> [Source: <a href="https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html">https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/fourfoldplot.html</a>], an association (odds ratio different from 1) between the binary row and column variables is indicated by the tendency of diagonally opposite cells in one direction to differ in size from those in the other direction; color is used to show this direction. Confidence rings for the odds ratio allow a visual test of the null of no association; the rings for adjacent quadrants overlap if and only if the observed counts are consistent with the null hypothesis.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Leveraging multicore processing in the model</h1>
                
            
            
                
<p>The exercise in the previous section is repeated here using the PimaIndianDiabetes2 dataset instead. This dataset contains several missing values. As a result, we will first impute the missing values and then run the machine learning example.</p>
<p>The exercise has been repeated with some additional nuances, such as using multicore/parallel processing in order to make the cross-validations run faster.</p>
<p>To leverage multicore processing, install the package <kbd>doMC</kbd> using the following code:</p>
<pre><strong>Install.packages("doMC")  # Install package for multicore processing 
Install.packages("nnet") # Install package for neural networks in R</strong> </pre>
<p class="mce-root">Now we will run the program as shown in the code here:</p>
<pre># Load the library doMC 
<strong>library(doMC) 
</strong> 
# Register all cores 
<strong>registerDoMC(cores = 8) 
</strong> 
# Set seed to create a reproducible example 
<strong>set.seed(100) 
</strong> 
# Load the PimaIndiansDiabetes2 dataset 
<strong>data("PimaIndiansDiabetes2",package = 'mlbench') 
diab&lt;- PimaIndiansDiabetes2 
</strong> 
# This dataset, unlike PimaIndiansDiabetes has 652 missing values! 
<strong>&gt; sum(is.na(diab)) 
</strong>[1] 652 
 
# We will use knnImputation to fill in the missing values 
<strong>diab&lt;- knnImputation(diab) 
</strong> 
# Create the train-test set split 
<strong>training_index&lt;- createDataPartition(diab$diabetes, p = .8, list = FALSE, times = 1) 
</strong> 
# Create the training and test dataset 
<strong>diab_train&lt;- diab[training_index,] 
diab_test&lt;- diab[-training_index,] 
</strong> 
# We will use 10-Fold Cross Validations 
<strong>diab_control&lt;- trainControl("repeatedcv", number = 10, repeats = 3, search = "random", classProbs = TRUE) 
</strong> 
# Create the model using methodnnet (a Neural Network package in R) 
# Note that we have changed the metric here to "Accuracy" instead of # ROC 
<strong>nn_model&lt;- train(diabetes ~ ., data = diab_train, method = "nnet",   preProc = c("center", "scale"), trControl = diab_control, tuneLength = 10, metric = "Accuracy")</strong> 

<strong>predictions&lt;- predict(nn_model, diab_test[,-ncol(diab_test)]) </strong>
<strong>cf&lt;- confusionMatrix(predictions, diab_test$diabetes) 
cf</strong> 
 
# &gt;cf 
# Confusion Matrix and Statistics 
#  
#        Reference 
# Prediction negpos 
#        neg  89  19 
#        pos  11  34 
#  
# Accuracy : 0.8039           
# 95% CI : (0.7321, 0.8636) 
# No Information Rate : 0.6536           
# P-Value [Acc&gt; NIR] : 3.3e-05          
#  </pre>
<p>Even with 650+ missing values, our model was able to achieve an accuracy of 80%+.</p>
<p>It can certainly be improved, but as a baseline, it shows the kind of performance that can be expected of machine learning models.</p>
<p>In a case of a dichotomous outcome variable, a random guess would have had a 50% chance of being accurate. An accuracy of 80% is significantly higher than the accuracy we could have achieved using just guess-work:</p>
<pre><strong> plot(nn_model)</strong> </pre>
<p>The resulting plot is as follows:</p>
<div><img class=" image-border" src="img/e3f98e5b-e43e-4a7e-bad9-3242aed7141b.png"/></div>
<pre><strong>fourfoldplot(cf$table)</strong> </pre>
<p>The result is depicted in the following plot:</p>
<div><img height="191" width="213" class=" image-border" src="img/b2a4c384-7f1d-4f36-ac67-f15fd3df0f0d.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learnt about the basic fundamentals of Machine Learning, the different types such as Supervised and Unsupervised and major concepts such as data pre-processing, data imputation, managing imbalanced classes and other topics.</p>
<p>We also learnt about the key distinctions between terms that are being used interchangeably today, in particular the terms AI and Machine Learning. We learned that artificial intelligence deals with a vast array of topics, such as game theory, sociology, constrained optimizations, and machine learning; AI is much broader in scope relative to machine learning.</p>
<p>Machine learning facilitates AI; namely, machine learning algorithms are used to create systems that are <em>artificially intelligent</em>, but they differ in scope. A regression problem (finding the line of best fit given a set of points) can be considered a machine learning <em>algorithm</em>, but it is much less likely to be seen as an AI algorithm (conceptually, although it technically could be).</p>
<p>In the next chapter, we will look at some of the other concepts in Machine Learning such as Bias, Variance and Regularization. We will also read about a few important algorithms and learn how to apply them using machine learning packages in R.</p>


            

            
        
    </body></html>