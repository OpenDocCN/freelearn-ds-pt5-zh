<html><head></head><body>
        <section id="6A5N81-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Special RDD Operations</h1>
                
            
            <article>
                
<div class="book-info-bottom-author-body">"It's supposed to be automatic, but actually you have to push this button."</div>
<p class="cdpalignright">- John Brunner</p>
<p class="mce-root">In this chapter, you learn how RDDs can be tailored to different needs, and how these RDDs provide new functionalities (and dangers!) Moreover, we investigate other useful objects that Spark provides, such as broadcast variables and accumulators.<br class="title-page-name"/>
In a nutshell, the following topics will be covered throughout this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Types of RDDs</li>
<li class="mce-root1">Aggregations</li>
<li class="mce-root1">Partitioning and shuffling</li>
<li class="mce-root1">Broadcast variables</li>
<li class="mce-root1">Accumulators</li>
</ul>


            </article>

            
        </section>
    

        <section id="6B47Q1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Types of RDDs</h1>
                
            
            <article>
                
<p class="mce-root"><strong class="calibre1">Resilient Distributed Datasets</strong> (<strong class="calibre1">RDDs</strong>) are the fundamental object used in Apache Spark. RDDs are immutable collections representing datasets and have the inbuilt capability of reliability and failure recovery. By nature, RDDs create new RDDs upon any operation such as transformation or action. They also store the lineage, which is used to recover from failures. We have also seen in the previous chapter some details about how RDDs can be created and what kind of operations can be applied to RDDs.</p>
<p class="mce-root">The following is a simply <span>example of the RDD lineage:</span></p>
<div class="cdpaligncenter"><img class="image-border71" src="../images/00056.jpeg"/></div>
<p class="mce-root">Let's start looking at the simplest RDD again by creating a RDD from a sequence of numbers:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3,4,5,6))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.take(100)</strong><br class="title-page-name"/>res45: Array[Int] = Array(1, 2, 3, 4, 5, 6)
</pre>
<p class="mce-root">The preceding example shows RDD of integers and any operation done on the RDD results in another RDD. For example, if we multiply each element by <kbd class="calibre11">3</kbd>, the result is shown in the following snippet:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_two = rdd_one.map(i =&gt; i * 3)</strong><br class="title-page-name"/>rdd_two: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[29] at map at &lt;console&gt;:27<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.take(10)</strong><br class="title-page-name"/>res46: Array[Int] = Array(3, 6, 9, 12, 15, 18)
</pre>
<p class="mce-root">Let's do one more operation, adding <kbd class="calibre11">2</kbd> to each element and also print all three RDDs:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_three = rdd_two.map(i =&gt; i+2)</strong><br class="title-page-name"/>rdd_three: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[30] at map at &lt;console&gt;:29<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_three.take(10)</strong><br class="title-page-name"/>res47: Array[Int] = Array(5, 8, 11, 14, 17, 20)
</pre>
<p class="mce-root">An interesting thing to look at is the lineage of each RDD using the <kbd class="calibre11">toDebugString</kbd> function:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; rdd_one.toDebugString</strong><br class="title-page-name"/>res48: String = (8) ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:25 []<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.toDebugString</strong><br class="title-page-name"/>res49: String = (8) MapPartitionsRDD[29] at map at &lt;console&gt;:27 []<br class="title-page-name"/> | ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:25 []<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_three.toDebugString</strong><br class="title-page-name"/>res50: String = (8) MapPartitionsRDD[30] at map at &lt;console&gt;:29 []<br class="title-page-name"/> | MapPartitionsRDD[29] at map at &lt;console&gt;:27 []<br class="title-page-name"/> | ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:25 []
</pre>
<p class="mce-root">The following is the lineage shown in the Spark web UI:</p>
<div class="cdpaligncenter2"><img class="image-border72" src="../images/00064.jpeg"/></div>
<p class="mce-root">RDD does not need to be the same datatype as the first RDD (integer). The following is a RDD which writes a different datatype of a tuple of (string, integer).</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_four = rdd_three.map(i =&gt; ("str"+(i+2).toString, i-2))</strong><br class="title-page-name"/>rdd_four: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[33] at map at &lt;console&gt;:31<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_four.take(10)</strong><br class="title-page-name"/>res53: Array[(String, Int)] = Array((str7,3), (str10,6), (str13,9), (str16,12), (str19,15), (str22,18))
</pre>
<p class="mce-root">The following is a RDD of the <kbd class="calibre11">StatePopulation</kbd> file where each record is converted to <kbd class="calibre11">upperCase</kbd>.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val upperCaseRDD = statesPopulationRDD.map(_.toUpperCase)</strong><br class="title-page-name"/>upperCaseRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[69] at map at &lt;console&gt;:27<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; upperCaseRDD.take(10)</strong><br class="title-page-name"/>res86: Array[String] = Array(STATE,YEAR,POPULATION, ALABAMA,2010,4785492, ALASKA,2010,714031, ARIZONA,2010,6408312, ARKANSAS,2010,2921995, CALIFORNIA,2010,37332685, COLORADO,2010,5048644, DELAWARE,2010,899816, DISTRICT OF COLUMBIA,2010,605183, FLORIDA,2010,18849098)
</pre>
<p class="mce-root">The following is a diagram of the preceding transformation:</p>
<div class="cdpaligncenter"><img class="image-border73" src="../images/00156.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6C2OC1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Pair RDD</h1>
                
            
            <article>
                
<p class="mce-root">Pair RDDs are RDDs consisting of key-value tuples which suits many use cases such as aggregation, sorting, and joining data. The keys and values can be simple types such as integers and strings or more complex types such as case classes, arrays, lists, and other types of collections. The key-value based extensible data model offers many advantages and is the fundamental concept behind the MapReduce paradigm.</p>
<p class="mce-root">Creating a <kbd class="calibre11">PairRDD</kbd> can be done easily by applying transformation to any RDD to convert the RDD to an RDD of key-value pairs.</p>
<p class="mce-root">Let's read the <kbd class="calibre11">statesPopulation.csv</kbd> into an RDD using the <kbd class="calibre11">SparkContext</kbd>, which is available as <kbd class="calibre11">sc</kbd>.</p>
<p class="mce-root"><span>The following is an example of a basic RDD of the state population and how <kbd class="calibre11">PairRDD</kbd> looks like for the same RDD splitting the records into tuples (pairs) of state and population:<br class="title-page-name"/></span></p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val statesPopulationRDD = sc.textFile("statesPopulation.csv")<br class="title-page-name"/></strong>statesPopulationRDD: org.apache.spark.rdd.RDD[String] = statesPopulation.csv MapPartitionsRDD[47] at textFile at &lt;console&gt;:25<br class="title-page-name"/><strong class="calibre1"><br class="title-page-name"/>scala&gt; statesPopulationRDD.first</strong><br class="title-page-name"/>res4: String = State,Year,Population<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesPopulationRDD.take(5)</strong><br class="title-page-name"/>res5: Array[String] = Array(State,Year,Population, Alabama,2010,4785492, Alaska,2010,714031, Arizona,2010,6408312, Arkansas,2010,2921995)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val pairRDD = statesPopulationRDD.map(record =&gt; (record.split(",")(0), record.split(",")(2)))</strong><br class="title-page-name"/>pairRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[48] at map at &lt;console&gt;:27<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; pairRDD.take(10)</strong><br class="title-page-name"/>res59: Array[(String, String)] = Array((Alabama,4785492), (Alaska,714031), (Arizona,6408312), (Arkansas,2921995), (California,37332685), (Colorado,5048644), (Delaware,899816), (District of Columbia,605183), (Florida,18849098))
</pre>
<p class="mce-root">The following is a diagram of the preceding example showing how the RDD elements are converted to <kbd class="calibre11">(key - value)</kbd> pairs:</p>
<div class="cdpaligncenter"><img class="image-border74" src="../images/00341.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6D18U1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">DoubleRDD</h1>
                
            
            <article>
                
<p class="mce-root">DoubleRDD is an RDD consisting of a collection of double values. Due to this property, many statistical functions are available to use with the DoubleRDD.</p>
<p class="mce-root">The following are examples of DoubleRDD where we create an RDD from a sequence of double numbers:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1.0,2.0,3.0))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Double] = ParallelCollectionRDD[52] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.mean</strong><br class="title-page-name"/>res62: Double = 2.0<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.min</strong><br class="title-page-name"/>res63: Double = 1.0<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.max</strong><br class="title-page-name"/>res64: Double = 3.0<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.stdev</strong><br class="title-page-name"/>res65: Double = 0.816496580927726
</pre>
<p class="mce-root">The following is a diagram of the DoubleRDD and how you can run a <kbd class="calibre11">sum()</kbd> function on the DoubleRDD:</p>
<div class="cdpaligncenter"><img class="image-border75" src="../images/00371.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6DVPG1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">SequenceFileRDD</h1>
                
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">SequenceFileRDD</kbd> is created from a <kbd class="calibre11">SequenceFile</kbd> which is a format of files in the Hadoop File System. The <kbd class="calibre11">SequenceFile</kbd> can be compressed or uncompressed.</p>
<div class="packt_tip">Map Reduce processes can use SequenceFiles, which are pairs of Keys and Values. Key and Value are of Hadoop writable datatypes, such as Text, IntWritable, and so on.</div>
<p class="mce-root">The following is an example of a <kbd class="calibre11">SequenceFileRDD</kbd>, which shows how we can write and read <kbd class="calibre11">SequenceFile</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val pairRDD = statesPopulationRDD.map(record =&gt; (record.split(",")(0), record.split(",")(2)))</strong><br class="title-page-name"/>pairRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[60] at map at &lt;console&gt;:27<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; pairRDD.saveAsSequenceFile("seqfile")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val seqRDD = sc.sequenceFile[String, String]("seqfile")</strong><br class="title-page-name"/>seqRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[62] at sequenceFile at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; seqRDD.take(10)</strong><br class="title-page-name"/>res76: Array[(String, String)] = Array((State,Population), (Alabama,4785492), (Alaska,714031), (Arizona,6408312), (Arkansas,2921995), (California,37332685), (Colorado,5048644), (Delaware,899816), (District of Columbia,605183), (Florida,18849098))
</pre>
<p class="mce-root">The following is a diagram of <strong class="calibre1">SequenceFileRDD</strong> as seen in the preceding example:</p>
<div class="cdpaligncenter"><img class="image-border76" src="../images/00013.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6EUA21-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">CoGroupedRDD</h1>
                
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">CoGroupedRDD</kbd> is an RDD that cogroups its parents. Both parent RDDs have to be pairRDDs for this to work, as a cogroup essentially generates a pairRDD consisting of the common key and list of values from both parent RDDs. Take a look at the following code snippet:</p>
<pre class="calibre19">
<strong class="calibre1"><span><span>class</span> </span><span><span>CoGroupedRDD</span><span>[K]</span><span> extends RDD[(<span>K</span>, <span>Array</span>[<span>Iterable</span>[_]])]</span> </span></strong>
</pre>
<p class="mce-root">The following is an example of a CoGroupedRDD where we create a cogroup of two pairRDDs, one having pairs of State, Population and the other having pairs of State, Year:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val pairRDD = statesPopulationRDD.map(record =&gt; (record.split(",")(0), record.split(",")(2)))</strong><br class="title-page-name"/>pairRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[60] at map at &lt;console&gt;:27<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val pairRDD2 = statesPopulationRDD.map(record =&gt; (record.split(",")(0), record.split(",")(1)))</strong><br class="title-page-name"/>pairRDD2: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[66] at map at &lt;console&gt;:27<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val cogroupRDD = pairRDD.cogroup(pairRDD2)</strong><br class="title-page-name"/>cogroupRDD: org.apache.spark.rdd.RDD[(String, (Iterable[String], Iterable[String]))] = MapPartitionsRDD[68] at cogroup at &lt;console&gt;:31<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; cogroupRDD.take(10)</strong><br class="title-page-name"/>res82: Array[(String, (Iterable[String], Iterable[String]))] = Array((Montana,(CompactBuffer(990641, 997821, 1005196, 1014314, 1022867, 1032073, 1042520),CompactBuffer(2010, 2011, 2012, 2013, 2014, 2015, 2016))), (California,(CompactBuffer(37332685, 37676861, 38011074, 38335203, 38680810, 38993940, 39250017),CompactBuffer(2010, 2011, 2012, 2013, 2014, 2015, 2016))),
</pre>
<p class="mce-root">The following is a diagram of the cogroup of <strong class="calibre1">pairRDD</strong> and <strong class="calibre1">pairRDD2</strong> by creating pairs of values for each key:</p>
<div class="cdpaligncenter"><img class="image-border77" src="../images/00179.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6FSQK1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">ShuffledRDD</h1>
                
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">ShuffledRDD</kbd> shuffles the RDD elements by key so as to accumulate values for the same key on the same executor to allow an aggregation or combining logic. A very good example is to look at what happens when <kbd class="calibre11">reduceByKey()</kbd> is called on a PairRDD:</p>
<pre class="calibre19">
<span><span>class ShuffledRDD</span><span>[<span>K</span>, <span>V</span>, <span>C</span>]</span><span> extends RDD[(<span>K</span>, <span>C</span>)]</span> </span>
</pre>
<p class="mce-root">The following is a <kbd class="calibre11">reduceByKey</kbd> operation on the <kbd class="calibre11">pairRDD</kbd> to aggregate the records by the State:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val pairRDD = statesPopulationRDD.map(record =&gt; (record.split(",")(0), 1))</strong><br class="title-page-name"/>pairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[82] at map at &lt;console&gt;:27<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; pairRDD.take(5)</strong><br class="title-page-name"/>res101: Array[(String, Int)] = Array((State,1), (Alabama,1), (Alaska,1), (Arizona,1), (Arkansas,1))<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val shuffledRDD = pairRDD.reduceByKey(_+_)</strong><br class="title-page-name"/>shuffledRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[83] at reduceByKey at &lt;console&gt;:29<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; shuffledRDD.take(5)</strong><br class="title-page-name"/>res102: Array[(String, Int)] = Array((Montana,7), (California,7), (Washington,7), (Massachusetts,7), (Kentucky,7))
</pre>
<p class="mce-root">The following diagram, is an illustration of the shuffling by Key to send the records of the same Key(State) to the same partitions:</p>
<div class="cdpaligncenter"><img class="image-border78" src="../images/00024.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6GRB61-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">UnionRDD</h1>
                
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">UnionRDD</kbd> is the result of a union operation of two RDDs. Union simply creates an RDD with elements from both RDDs as shown in the following code snippet:</p>
<pre class="calibre19">
class UnionRDD[T: ClassTag]( sc: SparkContext, var rdds: Seq[RDD[T]]) extends RDD[T](sc, Nil)
</pre>
<p class="mce-root">The following code snippet is the API call to create a <kbd class="calibre11">UnionRDD</kbd> by combining the elements of the two RDDs:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[85] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rdd_two = sc.parallelize(Seq(4,5,6))</strong><br class="title-page-name"/>rdd_two: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[86] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[87] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.take(10)</strong><br class="title-page-name"/>res103: Array[Int] = Array(1, 2, 3)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rdd_two = sc.parallelize(Seq(4,5,6))</strong><br class="title-page-name"/>rdd_two: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[88] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_two.take(10)</strong><br class="title-page-name"/>res104: Array[Int] = Array(4, 5, 6)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val unionRDD = rdd_one.union(rdd_two)</strong><br class="title-page-name"/>unionRDD: org.apache.spark.rdd.RDD[Int] = UnionRDD[89] at union at &lt;console&gt;:29<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; unionRDD.take(10)</strong><br class="title-page-name"/>res105: Array[Int] = Array(1, 2, 3, 4, 5, 6)
</pre>
<p class="mce-root">The following diagram is an illustration of a union of two RDDs where the elements from both <strong class="calibre1">RDD 1</strong> and <strong class="calibre1">RDD 2</strong> are combined into a new RDD <strong class="calibre1">UnionRDD</strong>:</p>
<div class="cdpaligncenter"><img class="image-border79" src="../images/00305.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6HPRO1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">HadoopRDD</h1>
                
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">HadoopRDD</kbd> provides core functionality for reading data stored in HDFS using the MapReduce API from the Hadoop 1.x libraries. <kbd class="calibre11">HadoopRDD</kbd> is the default used and can be seen when loading data from any file system into an RDD:</p>
<pre class="calibre19">
<strong class="calibre1"><span><span>class HadoopRDD</span><span>[K, V]</span><span> extends RDD[(<span>K</span>, <span>V</span>)]</span></span></strong>
</pre>
<p class="mce-root">When loading the state population records from the CSV, the underlying base RDD is actually <kbd class="calibre11">HadoopRDD</kbd> as in the following code snippet:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val statesPopulationRDD = sc.textFile("statesPopulation.csv")</strong><br class="title-page-name"/>statesPopulationRDD: org.apache.spark.rdd.RDD[String] = statesPopulation.csv MapPartitionsRDD[93] at textFile at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesPopulationRDD.toDebugString</strong><br class="title-page-name"/>res110: String =<br class="title-page-name"/>(2) statesPopulation.csv MapPartitionsRDD[93] at textFile at &lt;console&gt;:25 []<br class="title-page-name"/> | statesPopulation.csv HadoopRDD[92] at textFile at &lt;console&gt;:25 []
</pre>
<p class="mce-root"><span>The following diagram is an illustration of a <strong class="calibre1">HadoopRDD</strong> created by loading a textfile from the file system into an RDD:</span></p>
<div class="cdpaligncenter"><img class="image-border80" src="../images/00032.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6IOCA1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">NewHadoopRDD</h1>
                
            
            <article>
                
<p class="mce-root"><span><kbd class="calibre11">NewHadoopRDD</kbd> provides core functionality for reading data stored in HDFS, HBase tables, Amazon S3</span> <span>using the new MapReduce API from Hadoop 2.x <kbd class="calibre11">libraries.NewHadoopRDD</kbd> can read from many different formats thus is used to interact with several external systems.</span></p>
<div class="packt_infobox">Prior to <kbd class="calibre22">NewHadoopRDD</kbd>, <kbd class="calibre22">HadoopRDD</kbd> was the only available option which used the old MapReduce API from Hadoop 1.x</div>
<pre class="calibre19">
<span><span><strong class="calibre1">class NewHadoopRDD[K, V](</strong><br class="title-page-name"/><strong class="calibre1"> sc : SparkContext,</strong><br class="title-page-name"/><strong class="calibre1"> inputFormatClass: Class[_ &lt;: InputFormat[K, V]],</strong><br class="title-page-name"/><strong class="calibre1"> keyClass: Class[K],</strong><br class="title-page-name"/><strong class="calibre1"> valueClass: Class[V],</strong><br class="title-page-name"/><strong class="calibre1"> @transient private val _conf: Configuration)</strong><br class="title-page-name"/><strong class="calibre1">extends RDD[(K, V)]</strong><br class="title-page-name"/></span></span>
</pre>
<p class="mce-root">As seen in the preceding code snippet, <kbd class="calibre11">NewHadoopRDD</kbd> takes an input format class, a key class, and a value class. Let's look at examples of <kbd class="calibre11">NewHadoopRDD</kbd>.</p>
<p class="mce-root">The simplest example is to use SparkContext's <span><kbd class="calibre11">wholeTextFiles</kbd> function to create <kbd class="calibre11">WholeTextFileRDD</kbd>. Now, <kbd class="calibre11">WholeTextFileRDD</kbd> actually extends <kbd class="calibre11">NewHadoopRDD</kbd> as shown in the following code snippet:</span></p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_whole = sc.wholeTextFiles("wiki1.txt")</strong><br class="title-page-name"/>rdd_whole: org.apache.spark.rdd.RDD[(String, String)] = wiki1.txt MapPartitionsRDD[3] at wholeTextFiles at &lt;console&gt;:31<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_whole.toDebugString</strong><br class="title-page-name"/>res9: String =<br class="title-page-name"/>(1) wiki1.txt MapPartitionsRDD[3] at wholeTextFiles at &lt;console&gt;:31 []<br class="title-page-name"/> | <strong class="calibre1">WholeTextFileRDD</strong>[2] at wholeTextFiles at &lt;console&gt;:31 []
</pre>
<p class="mce-root">Let's look at another example where we will use the function <span><kbd class="calibre11">newAPIHadoopFile</kbd> using the <kbd class="calibre11">SparkContext</kbd>:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">import org.apache.hadoop.io.Text</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">val newHadoopRDD = sc.newAPIHadoopFile("statesPopulation.csv", classOf[KeyValueTextInputFormat], classOf[Text],classOf[Text])</strong>
</pre>


            </article>

            
        </section>
    

        <section id="6JMSS1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Aggregations</h1>
                
            
            <article>
                
<p class="mce-root">Aggregation techniques allow you to combine the elements in the RDD in arbitrary ways to perform some computation. In fact, aggregation is the most important part of big data analytics. Without aggregation, we would not have any way to generate reports and analysis like <em class="calibre8">Top States by Population</em>, which seems to be a logical question asked when given a dataset of all State populations for the past 200 years. Another simpler example is that of a need to just count the number of elements in the RDD, which asks the executors to count the number of elements in each partition and send to the Driver, which then adds the subsets to compute the total number of elements in the RDD.</p>
<p class="mce-root">In this section, our primary focus is on the aggregation functions used to collect and combine data by key. As seen earlier in this chapter, a PairRDD is an RDD of (key - value) pairs where key and value are arbitrary and can be customized as per the use case.</p>
<p class="mce-root">In our example of state populations, a PairRDD could be the pairs of <kbd class="calibre11">&lt;State, &lt;Population, Year&gt;&gt;</kbd> which means <kbd class="calibre11">State</kbd> is taken as the key and the tuple <kbd class="calibre11">&lt;Population, Year&gt;</kbd> is considered the value. This way of breaking down the key and value can generate aggregations such as <em class="calibre8">Top Years by Population per State</em><span>. On the contrary, i</span>n case our aggregations are done around Year say <em class="calibre8">Top States by Population per Year</em>, we can use a <kbd class="calibre11">pairRDD</kbd> of pairs of <kbd class="calibre11">&lt;Year, &lt;State, Population&gt;&gt;</kbd>.</p>
<p class="mce-root">The following is the sample code to generate a <kbd class="calibre11">pairRDD</kbd> from the <kbd class="calibre11">StatePopulation</kbd> dataset both with <kbd class="calibre11">State</kbd> as the key as well as the <kbd class="calibre11">Year</kbd> as the key:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val statesPopulationRDD = sc.textFile("statesPopulation.csv")</strong><br class="title-page-name"/>statesPopulationRDD: org.apache.spark.rdd.RDD[String] = statesPopulation.csv MapPartitionsRDD[157] at textFile at &lt;console&gt;:26<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesPopulationRDD.take(5)</strong><br class="title-page-name"/>res226: Array[String] = Array(State,Year,Population, Alabama,2010,4785492, Alaska,2010,714031, Arizona,2010,6408312, Arkansas,2010,2921995)
</pre>
<p class="mce-root">Next, we can generate a <kbd class="calibre11">pairRDD</kbd> using <kbd class="calibre11">State</kbd> as the key and a tuple of <kbd class="calibre11">&lt;Year, Population&gt;</kbd> as the value as shown in the following code snippet:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val pairRDD = statesPopulationRDD.map(record =&gt; record.split(",")).map(t =&gt; (t(0), (t(1), t(2))))</strong><br class="title-page-name"/>pairRDD: org.apache.spark.rdd.RDD[(String, (String, String))] = MapPartitionsRDD[160] at map at &lt;console&gt;:28<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; pairRDD.take(5)</strong><br class="title-page-name"/>res228: Array[(String, (String, String))] = Array((State,(Year,Population)), (Alabama,(2010,4785492)), (Alaska,(2010,714031)), (Arizona,(2010,6408312)), (Arkansas,(2010,2921995)))
</pre>
<p class="mce-root"><span>As mentioned earlier, we can also generate a <kbd class="calibre11">PairRDD</kbd> using <kbd class="calibre11">Year</kbd> as the key and a tuple of <kbd class="calibre11">&lt;State, Population&gt;</kbd> as the value as shown in the following code snippet:</span></p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val pairRDD = statesPopulationRDD.map(record =&gt; record.split(",")).map(t =&gt; (t(1), (t(0), t(2))))</strong><br class="title-page-name"/>pairRDD: org.apache.spark.rdd.RDD[(String, (String, String))] = MapPartitionsRDD[162] at map at &lt;console&gt;:28<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; pairRDD.take(5)</strong><br class="title-page-name"/>res229: Array[(String, (String, String))] = Array((Year,(State,Population)), (2010,(Alabama,4785492)), (2010,(Alaska,714031)), (2010,(Arizona,6408312)), (2010,(Arkansas,2921995)))
</pre>
<p class="mce-root">We will now look into how we can use the common aggregation functions on the <kbd class="calibre11">pairRDD</kbd> of <kbd class="calibre11">&lt;State, &lt;Year, Population&gt;&gt;</kbd>:</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">groupByKey</kbd></li>
<li class="mce-root1"><kbd class="calibre11">reduceByKey</kbd></li>
<li class="mce-root1"><kbd class="calibre11">aggregateByKey</kbd></li>
<li class="mce-root1"><kbd class="calibre11">combineByKey</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section id="6KLDE1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">groupByKey</h1>
                
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">groupByKey</kbd> groups the values for each key in the RDD into a single sequence. <kbd class="calibre11">groupByKey</kbd> also allows controlling the partitioning of the resulting key-value pair RDD by passing a partitioner. By default, a <kbd class="calibre11">HashPartitioner</kbd> is used but a custom partitioner can be given as an argument. The ordering of elements within each group is not guaranteed, and may even differ each time the resulting RDD is evaluated.</p>
<div class="packt_tip"><kbd class="calibre22">groupByKey</kbd> is an expensive operation due to all the data shuffling needed. <span class="field"><kbd class="calibre22">reduceByKey</kbd> or</span> <kbd class="calibre22">aggregateByKey</kbd> provide much better performance. We will look at this later in this section.</div>
<p class="mce-root"><kbd class="calibre11">groupByKey</kbd> can be invoked either using a custom partitioner or just using the default <kbd class="calibre11">HashPartitioner</kbd> as shown in the following code snippet:</p>
<pre class="calibre19">
<strong class="calibre1">def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])] </strong>
</pre>
<div class="packt_infobox">As currently implemented, <kbd class="calibre22">groupByKey</kbd> must be able to hold all the key-value pairs for any key in memory. If a key has too many values, it can result in an <kbd class="calibre22">OutOfMemoryError</kbd>.</div>
<p class="mce-root"><kbd class="calibre11">groupByKey</kbd> works by sending all elements of the partitions to the partition based on the partitioner so that all pairs of (key - value) for the same key are collected in the same partition. Once this is done, the aggregation operation can be done easily.</p>
<p class="mce-root">Shown here is an illustration of what happens when <kbd class="calibre11">groupByKey</kbd> is called:</p>
<div class="cdpaligncenter"><img class="image-border81" src="../images/00036.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6LJU01-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">reduceByKey</h1>
                
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">groupByKey</kbd> involves a lot of shuffling and <kbd class="calibre11">reduceByKey</kbd> tends to improve the performance by not sending all elements of the <kbd class="calibre11">PairRDD</kbd> using shuffles, rather using a local combiner to first do some basic aggregations locally and then send the resultant elements as in <kbd class="calibre11">groupByKey</kbd>. This greatly reduces the data transferred, as we don't need to send everything over. <kbd class="calibre11">reduceBykey</kbd> works by merging the values for each key using an associative and commutative reduce function. Of course, first, this will<br class="title-page-name"/>
also perform the merging locally on each mapper before sending results to a reducer.</p>
<div class="packt_tip">If you are familiar with Hadoop MapReduce, this is very similar to a combiner in MapReduce programming.</div>
<p class="mce-root"><kbd class="calibre11"><span>reduceByKey</span></kbd> can be invoked either using a custom partitioner or just using the default HashPartitioner as shown in the following code snippet:</p>
<pre class="calibre19">
<strong class="calibre1">def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)]<br class="title-page-name"/><br class="title-page-name"/>def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)] <br class="title-page-name"/><br class="title-page-name"/>def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)] </strong>
</pre>
<p class="mce-root"><kbd class="calibre11"><span>reduceByKey</span></kbd> works by sending all elements of the partitions to the partition based on the <kbd class="calibre11">partitioner</kbd> so that all pairs of (key - value) for the same Key are collected in the same partition. But before the shuffle, local aggregation is also done reducing the data to be shuffled. Once this is done, the aggregation operation can be done easily in the final partition.</p>
<p class="mce-root">The following diagram is an illustration of what happens when <kbd class="calibre11">reduceBykey</kbd> is called:</p>
<div class="cdpaligncenter"><img class="image-border82" src="../images/00039.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6MIEI1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">aggregateByKey</h1>
                
            
            <article>
                
<p class="mce-root"><span><kbd class="calibre11">aggregateByKey</kbd> is quite similar to <kbd class="calibre11">reduceByKey</kbd>, except that <kbd class="calibre11">aggregateByKey</kbd> allows more flexibility and customization of how to aggregate within partitions and between partitions to allow much more sophisticated use cases such as generating a list of all <kbd class="calibre11">&lt;Year, Population&gt;</kbd> pairs as well as total population for each State in one function call.</span></p>
<p class="mce-root"><span><kbd class="calibre11">aggregateByKey</kbd> works by a</span>ggregating the values of each key, using given combine functions and a neutral initial/zero value.<br class="title-page-name"/>
This function can return a different result type, <kbd class="calibre11">U</kbd>, than the type of the values in this RDD <kbd class="calibre11">V</kbd>, which is the biggest difference. Thus, we need one operation for merging a <kbd class="calibre11">V</kbd> into a <kbd class="calibre11">U</kbd> and one operation for merging two <kbd class="calibre11">U</kbd>'s. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new <kbd class="calibre11">U</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">def aggregateByKey[U: ClassTag](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) =&gt; U,</strong><br class="title-page-name"/><strong class="calibre1"> combOp: (U, U) =&gt; U): RDD[(K, U)] </strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">def aggregateByKey[U: ClassTag](zeroValue: U, numPartitions: Int)(seqOp: (U, V) =&gt; U,</strong><br class="title-page-name"/><strong class="calibre1"> combOp: (U, U) =&gt; U): RDD[(K, U)] </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) =&gt; U,</strong><br class="title-page-name"/><strong class="calibre1"> combOp: (U, U) =&gt; U): RDD[(K, U)] </strong>
</pre>
<p class="mce-root"><kbd class="calibre11"><span>aggregateByKey</span></kbd> works by performing an aggregation within the partition operating on all elements of each partition and then applies another aggregation logic when combining the partitions themselves. Ultimately, all pairs of (key - value) for the same Key are collected in the same partition; however, the aggregation as to how it is done and the output generated is not fixed as in <kbd class="calibre11">groupByKey</kbd> and <kbd class="calibre11">reduceByKey</kbd>, but is more flexible and customizable when using <kbd class="calibre11">aggregateByKey</kbd>.</p>
<p class="mce-root">The following diagram is an illustration of what happens when <kbd class="calibre11"><span>aggregateByKey</span></kbd> is called. Instead of adding up the counts as in <kbd class="calibre11">groupByKey</kbd> and <kbd class="calibre11">reduceByKey</kbd>, here we are generating lists of values for each Key:</p>
<div class="cdpaligncenter"><img class="image-border83" src="../images/00043.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6NGV41-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">combineByKey</h1>
                
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">combineByKey</kbd> is very similar to <kbd class="calibre11">aggregateByKey</kbd>; in fact, <kbd class="calibre11">combineByKey</kbd> internally invokes <kbd class="calibre11">combineByKeyWithClassTag</kbd>, which is also invoked by <kbd class="calibre11">aggregateByKey</kbd>. As in <kbd class="calibre11">aggregateByKey</kbd>, the <kbd class="calibre11">combineByKey</kbd> also works by applying an operation within each partition and then between combiners.</p>
<p class="mce-root"><kbd class="calibre11">combineByKey</kbd> turns an <kbd class="calibre11">RDD[K,V]</kbd> into an <kbd class="calibre11">RDD[K,C]</kbd>, where <kbd class="calibre11">C</kbd> is a list of Vs collected or combined under the name key <kbd class="calibre11">K</kbd>.</p>
<p class="mce-root">There are three functions expected when you call combineByKey.</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">createCombiner</kbd>, which turns a <kbd class="calibre11">V</kbd> into <kbd class="calibre11">C</kbd>, which is a one element list</li>
<li class="mce-root1"><kbd class="calibre11">mergeValue</kbd> to merge a <kbd class="calibre11">V</kbd> into a <kbd class="calibre11">C</kbd> by appending the <kbd class="calibre11">V</kbd> to the end of the list</li>
<li class="mce-root1"><kbd class="calibre11">mergeCombiners</kbd> to combine two Cs into one</li>
</ul>
<div class="packt_tip">In <kbd class="calibre22">aggregateByKey</kbd>, the first argument is simply a zero value but in <kbd class="calibre22">combineByKey</kbd>, we provide the initial function which takes the current value as a parameter.</div>
<p class="mce-root"><kbd class="calibre11"><span>combineByKey</span></kbd> can be invoked either using a custom partitioner or just using the default HashPartitioner as shown in the following code snippet:</p>
<pre class="calibre19">
<strong class="calibre1">def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, numPartitions: Int): RDD[(K, C)]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null): RDD[(K, C)]</strong>
</pre>
<p class="mce-root"><kbd class="calibre11"><span>combineByKey</span></kbd> works by performing an aggregation within the partition operating on all elements of each partition and then applies another aggregation logic when combining the partitions themselves. Ultimately, all pairs of (key - value) for the same Key are collected in the same partition however the aggregation as to how it is done and the output generated is not fixed as in <kbd class="calibre11">groupByKey</kbd> and <kbd class="calibre11">reduceByKey</kbd>, but is more flexible and customizable when using <kbd class="calibre11">combineByKey</kbd>.</p>
<p class="mce-root">The following diagram is an illustration of what happens when <kbd class="calibre11">combineBykey</kbd> is called:</p>
<div class="cdpaligncenter"><img class="image-border84" src="../images/00045.jpeg"/></div>


            </article>

            
        </section>
    

        <section>

                            <header id="6OFFM2-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">Comparison of groupByKey, reduceByKey, combineByKey, and aggregateByKey</h1>
                
            
            <article>
                
<p class="mce-root">Let's consider the example of StatePopulation RDD generating a <kbd class="calibre11">pairRDD</kbd> of <kbd class="calibre11">&lt;State, &lt;Year, Population&gt;&gt;</kbd>.</p>
<p class="mce-root"><kbd class="calibre11">groupByKey</kbd> as seen in the preceding section will do <kbd class="calibre11">HashPartitioning</kbd> of the <kbd class="calibre11">PairRDD</kbd> by generating a hashcode of the keys and then shuffling the data to collect the values for each key in the same partition. This obviously results in too much shuffling.</p>
<p class="mce-root"><kbd class="calibre11">reduceByKey</kbd> improves upon <kbd class="calibre11">groupByKey</kbd> using a local combiner logic to minimize the data sent in a shuffle phase. The result will be the same as <kbd class="calibre11">groupByKey</kbd>, but will be much more performant.</p>
<p class="mce-root"><kbd class="calibre11">aggregateByKey</kbd> is very similar to <kbd class="calibre11">reduceByKey</kbd> in how it works but with one big difference, which makes it the most powerful one among the three. <kbd class="calibre11">aggregateBykey</kbd> does not need to operate on the same datatype and can do different aggregation within the partition and do a different aggregation between partitions.</p>
<p class="mce-root"><kbd class="calibre11">combineByKey</kbd> is very similar in performance to <kbd class="calibre11">aggregateByKey</kbd> except for the initial function to create the combiner.</p>
<p class="mce-root">The function to use depends on your use case but when in doubt just refer to this section on <em class="calibre8">Aggregation</em> to choose the right function for your use case. Also, pay close attention to the next section as <em class="calibre8">Partitioning and shuffling</em> are covered in that section.</p>
<p class="mce-root">The following is the code showing all four ways of calculating total population by state.</p>
<p class="mce-root"><strong class="calibre1">Step 1. Initialize the RDD:</strong></p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val statesPopulationRDD = sc.textFile("statesPopulation.csv").filter(_.split(",")(0) != "State") </strong><br class="title-page-name"/>statesPopulationRDD: org.apache.spark.rdd.RDD[String] = statesPopulation.csv MapPartitionsRDD[1] at textFile at &lt;console&gt;:24<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesPopulationRDD.take(10)</strong><br class="title-page-name"/>res27: Array[String] = Array(Alabama,2010,4785492, Alaska,2010,714031, Arizona,2010,6408312, Arkansas,2010,2921995, California,2010,37332685, Colorado,2010,5048644, Delaware,2010,899816, District of Columbia,2010,605183, Florida,2010,18849098, Georgia,2010,9713521)
</pre>
<p class="mce-root"><strong class="calibre1">Step 2. Convert to pair RDD:</strong></p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val pairRDD = statesPopulationRDD.map(record =&gt; record.split(",")).map(t =&gt; (t(0), (t(1).toInt, t(2).toInt)))</strong><br class="title-page-name"/>pairRDD: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[26] at map at &lt;console&gt;:26<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; pairRDD.take(10)</strong><br class="title-page-name"/>res15: Array[(String, (Int, Int))] = Array((Alabama,(2010,4785492)), (Alaska,(2010,714031)), (Arizona,(2010,6408312)), (Arkansas,(2010,2921995)), (California,(2010,37332685)), (Colorado,(2010,5048644)), (Delaware,(2010,899816)), (District of Columbia,(2010,605183)), (Florida,(2010,18849098)), (Georgia,(2010,9713521)))
</pre>
<p class="mce-root"><strong class="calibre1">Step 3. groupByKey - Grouping the values and then adding up populations:</strong></p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val groupedRDD = pairRDD.groupByKey.map(x =&gt; {var sum=0; x._2.foreach(sum += _._2); (x._1, sum)})</strong><br class="title-page-name"/>groupedRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[38] at map at &lt;console&gt;:28<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; groupedRDD.take(10)</strong><br class="title-page-name"/>res19: Array[(String, Int)] = Array((Montana,7105432), (California,268280590), (Washington,48931464), (Massachusetts,46888171), (Kentucky,30777934), (Pennsylvania,89376524), (Georgia,70021737), (Tennessee,45494345), (North Carolina,68914016), (Utah,20333580))
</pre>
<p class="mce-root"><strong class="calibre1">Step 4. reduceByKey - Reduce the values by key simply adding the populations:</strong></p>
<pre class="calibre19">
<br class="title-page-name"/><strong class="calibre1">scala&gt; val reduceRDD = pairRDD.reduceByKey((x, y) =&gt; (x._1, x._2+y._2)).map(x =&gt; (x._1, x._2._2))</strong><br class="title-page-name"/>reduceRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[46] at map at &lt;console&gt;:28<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; reduceRDD.take(10)</strong><br class="title-page-name"/>res26: Array[(String, Int)] = Array((Montana,7105432), (California,268280590), (Washington,48931464), (Massachusetts,46888171), (Kentucky,30777934), (Pennsylvania,89376524), (Georgia,70021737), (Tennessee,45494345), (North Carolina,68914016), (Utah,20333580))
</pre>
<p class="mce-root"><strong class="calibre1">Step 5. aggregateBykey - aggregate the populations under each key and adds them up:</strong></p>
<pre class="calibre19">
<strong class="calibre1">Initialize the array</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; val initialSet = 0</strong><br class="title-page-name"/>initialSet: Int = 0<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">provide function to add the populations within a partition</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; val addToSet = (s: Int, v: (Int, Int)) =&gt; s+ v._2</strong><br class="title-page-name"/>addToSet: (Int, (Int, Int)) =&gt; Int = &lt;function2&gt;<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">provide funtion to add populations between partitions</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; val mergePartitionSets = (p1: Int, p2: Int) =&gt; p1 + p2</strong><br class="title-page-name"/>mergePartitionSets: (Int, Int) =&gt; Int = &lt;function2&gt;<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val aggregatedRDD = pairRDD.aggregateByKey(initialSet)(addToSet, mergePartitionSets)</strong><br class="title-page-name"/>aggregatedRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[41] at aggregateByKey at &lt;console&gt;:34<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; aggregatedRDD.take(10)</strong><br class="title-page-name"/>res24: Array[(String, Int)] = Array((Montana,7105432), (California,268280590), (Washington,48931464), (Massachusetts,46888171), (Kentucky,30777934), (Pennsylvania,89376524), (Georgia,70021737), (Tennessee,45494345), (North Carolina,68914016), (Utah,20333580))
</pre>
<p class="mce-root"><strong class="calibre1">Step 6. combineByKey - combine within partitions and then merging combiners:</strong></p>
<pre class="calibre19">
<strong class="calibre1">createcombiner function</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; val createCombiner = (x:(Int,Int)) =&gt; x._2</strong><br class="title-page-name"/>createCombiner: ((Int, Int)) =&gt; Int = &lt;function1&gt;<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">function to add within partition</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; val mergeValues = (c:Int, x:(Int, Int)) =&gt; c +x._2</strong><br class="title-page-name"/>mergeValues: (Int, (Int, Int)) =&gt; Int = &lt;function2&gt;<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">function to merge combiners</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; val mergeCombiners = (c1:Int, c2:Int) =&gt; c1 + c2</strong><br class="title-page-name"/>mergeCombiners: (Int, Int) =&gt; Int = &lt;function2&gt;<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val combinedRDD = pairRDD.combineByKey(createCombiner, mergeValues, mergeCombiners)</strong><br class="title-page-name"/>combinedRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[42] at combineByKey at &lt;console&gt;:34<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; combinedRDD.take(10)</strong><br class="title-page-name"/>res25: Array[(String, Int)] = Array((Montana,7105432), (California,268280590), (Washington,48931464), (Massachusetts,46888171), (Kentucky,30777934), (Pennsylvania,89376524), (Georgia,70021737), (Tennessee,45494345), (North Carolina,68914016), (Utah,20333580))
</pre>
<p class="mce-root">As you see, all four aggregations result in the same output. It's just how they work that is different.</p>


            </article>

            
        </section>
    

        <section id="6PE081-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Partitioning and shuffling</h1>
                
            
            <article>
                
<p class="mce-root">We have seen how Apache Spark can handle distributed computing much better than Hadoop. We also saw the inner workings, mainly the fundamental data structure known as <strong class="calibre1">Resilient Distributed Dataset</strong> (<strong class="calibre1">RDD</strong>). RDDs are immutable collections representing datasets and have the inbuilt capability of reliability and failure recovery. RDDs operate on data not as a single blob of data, rather RDDs manage and operate data in partitions spread across the cluster. Hence, the concept of data partitioning is critical to the proper functioning of Apache Spark Jobs and can have a big effect on the performance as well as how the resources are utilized.</p>
<p class="mce-root">RDD consists of partitions of data and all operations are performed on the partitions of data in the RDD. Several operations like transformations are functions executed by an executor on the specific partition of data being operated on. However, not all operations can be done by just performing isolated operations on the partitions of data by the respective executors. Operations like aggregations (seen in the preceding section) require data to be moved across the cluster in a phase known as <strong class="calibre1">shuffling</strong>. In this section, we will look deeper into the concepts of partitioning and shuffling.</p>
<p class="mce-root">Let's start looking at a simple RDD of integers by executing the following code. Spark Context's <kbd class="calibre11">parallelize</kbd> function creates an RDD from the Sequence of integers. Then, using the <kbd class="calibre11">getNumPartitions()</kbd> function, we can get the number of partitions of this RDD.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[120] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.getNumPartitions</strong><br class="title-page-name"/>res202: Int = 8
</pre>
<p class="mce-root">The RDD can be visualized as shown in the following diagram, which shows the 8 partitions in the RDD:</p>
<div class="cdpaligncenter"><img class="image-border85" src="../images/00136.jpeg"/></div>
<p class="mce-root">The number of partitions is important because this number directly influences the number of tasks that will be running RDD transformations. If the number of partitions is too small, then we will use only a few CPUs/cores on a lot of data thus having a slower performance and leaving the cluster underutilized. On the other hand, if the number of partitions is too large then you will use more resources than you actually need and in a multi-tenant environment could be causing starvation of resources for other Jobs being run by you or others in your team.</p>


            </article>

            
        </section>
    

        <section id="6QCGQ1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Partitioners</h1>
                
            
            <article>
                
<p class="mce-root">Partitioning of RDDs is done by partitioners. Partitioners assign a partition index to the elements in the RDD. All elements in the same partition will have the same partition index.</p>
<p class="mce-root">Spark comes with two partitioners the <kbd class="calibre11">HashPartitioner</kbd> and the <kbd class="calibre11">RangePartitioner</kbd>. In addition to these, you can also implement a custom partitioner.</p>


            </article>

            
        </section>
    

        <section id="6RB1C1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">HashPartitioner</h1>
                
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">HashPartitioner</kbd> is the default partitioner in Spark and works by calculating a hash value for each key of the RDD elements. All the elements with the same hashcode end up in the same partition as shown in the following code snippet:</p>
<pre class="calibre19">
<strong class="calibre1">partitionIndex = hashcode(key) % numPartitions</strong>
</pre>
<p class="mce-root">The following is an example of the String <kbd class="calibre11">hashCode()</kbd> function and how we can generate <kbd class="calibre11">partitionIndex</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val str = "hello"</strong><br class="title-page-name"/>str: String = hello<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; str.hashCode</strong><br class="title-page-name"/>res206: Int = 99162322<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val numPartitions = 8</strong><br class="title-page-name"/>numPartitions: Int = 8<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val partitionIndex = str.hashCode % numPartitions</strong><br class="title-page-name"/>partitionIndex: Int = 2
</pre>
<div class="packt_tip">The default number of partitions is either from the Spark configuration parameter <kbd class="calibre22">spark.default.parallelism</kbd> or the number of cores in the cluster</div>
<p class="mce-root">The following diagram is an illustration of how hash partitioning works. We have an RDD with 3 elements <strong class="calibre1">a</strong>, <strong class="calibre1">b</strong>, and <strong class="calibre1">e</strong>. Using String hashcode we get the <kbd class="calibre11">partitionIndex</kbd> for each element based on the number of partitions set at 6:</p>
<div class="cdpaligncenter"><img class="image-border86" src="../images/00140.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6S9HU1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">RangePartitioner</h1>
                
            
            <article>
                
<p class="mce-root"><kbd class="calibre11">RangePartitioner</kbd> works by partitioning the RDD into roughly equal ranges. Since the range has to know the starting and ending keys for any partition, the RDD needs to be sorted first before a <kbd class="calibre11">RangePartitioner</kbd> can be used.</p>
<p class="mce-root"><kbd class="calibre11">RangePartitioning</kbd> first needs reasonable boundaries for the partitions based on the RDD and then create a function from key K to the <kbd class="calibre11">partitionIndex</kbd> where the element belongs. Finally, we need to repartition the RDD, based on the <kbd class="calibre11">RangePartitioner</kbd> to distribute the RDD elements correctly as per the ranges we determined.</p>
<p class="mce-root">The following is an example of how we can use <kbd class="calibre11">RangePartitioning</kbd> of a <kbd class="calibre11">PairRDD</kbd>. We also can see how the partitions changed after we repartition the RDD using a <kbd class="calibre11">RangePartitioner</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.RangePartitioner</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; val statesPopulationRDD = sc.textFile("statesPopulation.csv")</strong><br class="title-page-name"/>statesPopulationRDD: org.apache.spark.rdd.RDD[String] = statesPopulation.csv MapPartitionsRDD[135] at textFile at &lt;console&gt;:26<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val pairRDD = statesPopulationRDD.map(record =&gt; (record.split(",")(0), 1))</strong><br class="title-page-name"/>pairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[136] at map at &lt;console&gt;:28<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rangePartitioner = new RangePartitioner(5, pairRDD)</strong><br class="title-page-name"/>rangePartitioner: org.apache.spark.RangePartitioner[String,Int] = org.apache.spark.RangePartitioner@c0839f25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val rangePartitionedRDD = pairRDD.partitionBy(rangePartitioner)</strong><br class="title-page-name"/>rangePartitionedRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[130] at partitionBy at &lt;console&gt;:32<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; pairRDD.mapPartitionsWithIndex((i,x) =&gt; Iterator(""+i + ":"+x.length)).take(10)</strong><br class="title-page-name"/>res215: Array[String] = Array(0:177, 1:174)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rangePartitionedRDD.mapPartitionsWithIndex((i,x) =&gt; Iterator(""+i + ":"+x.length)).take(10)</strong><br class="title-page-name"/>res216: Array[String] = Array(0:70, 1:77, 2:70, 3:63, 4:71)
</pre>
<p class="mce-root">The following diagram is an illustration of the <kbd class="calibre11">RangePartitioner</kbd> as seen in the preceding example:</p>
<div class="cdpaligncenter"><img class="image-border87" src="../images/00143.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6T82G1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Shuffling</h1>
                
            
            <article>
                
<p class="mce-root">Whatever the partitioner used, many operations will cause a repartitioning of data across the partitions of an RDD. New partitions can be created or several <span>partitions</span> can be collapsed/coalesced. All the data movement necessary for the repartitioning is called <strong class="calibre1">shuffling,</strong> and this is an important concept to understand when writing a Spark Job. The shuffling can cause a lot of performance lag as the computations are no longer in memory on the same executor but rather the executors are exchanging data over the wire.</p>
<p class="mce-root">A good example is the example of <kbd class="calibre11">groupByKey()</kbd>, we saw earlier in the <em class="calibre8">Aggregations</em> section. Obviously, lot of data was flowing between executors to make sure all values for a key are collected onto the same executor to perform the <kbd class="calibre11">groupBy</kbd> operation.</p>
<p class="mce-root">Shuffling also determines the Spark Job execution process and influences how the Job is split into Stages. As we have seen in this chapter and the previous chapter, Spark holds a DAG of RDDs, which represent the lineage of the RDDs such that not only does Spark use the lineage to plan the execution of the job but also any loss of executors can be recovered from. When an RDD is undergoing a transformation, an attempt is made to make sure the operations are performed on the same node as the data. However, often we use join operations, reduce, group, or aggregate operations among others, which cause repartitioning intentionally or unintentionally. This shuffling in turn determines where a particular stage in the processing has ended and a new stage has begun.</p>
<p class="mce-root">The following diagram is an illustration of how a Spark Job is split into stages. This example shows a <kbd class="calibre11">pairRDD</kbd> being filtered, transformed using map before invoking <kbd class="calibre11">groupByKey</kbd> followed by one last transformation using <kbd class="calibre11">map()</kbd>:</p>
<div class="cdpaligncenter"><img class="image-border88" src="../images/00147.jpeg"/></div>
<p class="mce-root">The more shuffling we have, the more stages occur in the job execution affecting the performance. There are two key aspects which are used by Spark Driver to determine the stages. This is done by defining two types of dependencies of the RDDs, the narrow dependencies and the wide dependencies.</p>


            </article>

            
        </section>
    

        <section id="6U6J21-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Narrow Dependencies</h1>
                
            
            <article>
                
<p class="mce-root">When an RDD can be derived from another RDD using a simple one-to-one transformation such as a <kbd class="calibre11">filter()</kbd> function, <kbd class="calibre11">map()</kbd> function, <kbd class="calibre11">flatMap()</kbd> function, and so on, then the child RDD is said to depend on the parent RDD on a one-to-one basis. This dependency is known as narrow dependency as the data can be transformed on the same node as the one containing the original RDD/parent RDD partition without requiring any data transfer over the wire between other executors.</p>
<div class="packt_tip">Narrow dependencies are in the same stage of the job execution.</div>
<p class="mce-root">The following diagram is an illustration of how a narrow dependency transforms one RDD to another RDD, applying one-to-one transformation on the RDD elements:</p>
<div class="cdpaligncenter"><img class="image-border89" src="../images/00152.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="6V53K1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Wide Dependencies</h1>
                
            
            <article>
                
<p class="mce-root">When an RDD can be derived from one or more RDDs by transferring data over the wire or exchanging data to repartition or redistribute the data using functions, such as <kbd class="calibre11">aggregateByKey</kbd>, <kbd class="calibre11">reduceByKey</kbd> and so on, then the child RDD is said to depend on the parent RDDs participating in a shuffle operation. This dependency is known as a Wide dependency as the data cannot be transformed on the same node as the one containing the original RDD/parent RDD partition thus requiring data transfer over the wire between other executors.</p>
<div class="packt_tip">Wide dependencies introduce new stages in the job execution.</div>
<p class="mce-root"><span>The following diagram is an</span> <span>illustration</span> <span>of how wide dependency transforms one RDD to another RDD shuffling data between executors:</span></p>
<div class="cdpaligncenter"><img src="../images/00155.jpeg" class="calibre35"/></div>


            </article>

            
        </section>
    

        <section id="703K61-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Broadcast variables</h1>
                
            
            <article>
                
<p class="mce-root"><span>Broadcast variables are shared variables across all executors. Broadcast variables are created once in the Driver and then are read only on executors. While it is simple to understand simple datatypes broadcasted, such as an <kbd class="calibre11">Integer</kbd>, broadcast is much bigger than simple variables conceptually. Entire datasets can be broadcasted in a Spark cluster so that executors have access to the broadcasted data. All the tasks running within an executor all have access to the broadcast variables.</span></p>
<p class="mce-root">Broadcast uses various optimized methods to make the broadcasted data accessible to all executors. This is an important challenge to solve as if the size of the datasets broadcasted is significant, you cannot expect 100s or 1000s of executors to connect to the Driver and pull the dataset. Rather, the executors pull the data via HTTP connection and the more recent addition which is similar to BitTorrent where the dataset itself is distributed like a torrent amongst the cluster. This enables a much more scalable method to distribute the broadcasted variables to all executors rather than having each executor pull the data from the Driver one by one which can cause failures on the Driver when you have a lot of executors.</p>
<div class="packt_tip">The driver can only broadcast the data it has and you cannot broadcast RDDs by using references. This is because only Driver knows how to interpret RDDs and executors only know the particular partitions of data they are handling.</div>
<p class="mce-root">If you look deeper into how broadcast works, you will see that the mechanism works by first having the Driver divide the serialized object into small chunks and then stores those chunks in the BlockManager of the driver. When the code is serialized to be run on the executors, then each executor first attempts to fetch the object from its own internal BlockManager. If the broadcast variable was fetched before, it will find it and use it. However, if it does not exist, the executor then uses remote fetches to fetch the small chunks from the driver and/or other executors if available. Once it gets the chunks, it puts the chunks in its own BlockManager, ready for any other executors to fetch from. This prevents the driver from being the bottleneck in sending out multiple copies of the broadcast data (one per executor).</p>
<p class="mce-root">The following diagram is an illustration of how broadcast works in a Spark cluster:</p>
<div class="cdpaligncenter"><img class="image-border90" src="../images/00006.jpeg"/></div>
<p class="mce-root">Broadcast variables can be both created and destroyed too. We will look into the creation and destruction of broadcast variables. There is also a way to remove broadcasted variables from memory which we will also look at.</p>


            </article>

            
        </section>
    

        <section id="7124O1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Creating broadcast variables</h1>
                
            
            <article>
                
<p class="mce-root">Creating a broadcast variable can be done using the Spark Context's <kbd class="calibre11">broadcast()</kbd> function on any data of any data type provided that the data/variable is serializable.</p>
<p class="mce-root">Let's look at how we can broadcast an Integer variable and then use the broadcast variable inside a transformation operation executed on the executors:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[101] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val i = 5</strong><br class="title-page-name"/>i: Int = 5<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val bi = sc.broadcast(i)</strong><br class="title-page-name"/>bi: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(147)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; bi.value</strong><br class="title-page-name"/>res166: Int = 5<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.take(5)</strong><br class="title-page-name"/>res164: Array[Int] = Array(1, 2, 3)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.map(j =&gt; j + bi.value).take(5)</strong><br class="title-page-name"/>res165: Array[Int] = Array(6, 7, 8)
</pre>
<p class="mce-root">Broadcast variables can also be created on more than just primitive data types as shown in the next example where we will broadcast a <kbd class="calibre11">HashMap</kbd> from the Driver.</p>
<p class="mce-root">The following is a simple transformation of an integer RDD by multiplying each element with another integer by looking up the HashMap. The RDD of 1,2,3 is transformed to 1 X 2 , 2 X 3, 3 X 4 = 2,6,12 :</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[109] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val m = scala.collection.mutable.HashMap(1 -&gt; 2, 2 -&gt; 3, 3 -&gt; 4)</strong><br class="title-page-name"/>m: scala.collection.mutable.HashMap[Int,Int] = Map(2 -&gt; 3, 1 -&gt; 2, 3 -&gt; 4)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val bm = sc.broadcast(m)</strong><br class="title-page-name"/>bm: org.apache.spark.broadcast.Broadcast[scala.collection.mutable.HashMap[Int,Int]] = Broadcast(178)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.map(j =&gt; j * bm.value(j)).take(5)</strong><br class="title-page-name"/>res191: Array[Int] = Array(2, 6, 12)
</pre>


            </article>

            
        </section>
    

        <section id="720LA1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Cleaning broadcast variables</h1>
                
            
            <article>
                
<p class="mce-root">Broadcast variables do occupy memory on all executors and depending on the size of the data contained in the broadcasted variable, this could cause resource issues at some point. There is a way to remove broadcasted variables from the memory of all executors.</p>
<p class="mce-root">Calling <kbd class="calibre11">unpersist()</kbd> on a broadcast variable removed the data of the broadcast variable from the memory cache of all executors to free up resources. If the variable is used again, then the data is retransmitted to the executors in order for it to be used again. The Driver, however, holds onto the memory as if the Driver does not have the data, then broadcast variable is no longer valid.</p>
<div class="packt_tip">We look at destroying broadcast variables next.</div>
<p class="mce-root">The following is an example of how <kbd class="calibre11">unpersist()</kbd> can be invoked on a broadcast variable. After calling <kbd class="calibre11">unpersist</kbd> if we access the broadcast variable again, it works as usual but behind the scenes, the executors are pulling the data for the variable again.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[101] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val k = 5</strong><br class="title-page-name"/>k: Int = 5<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val bk = sc.broadcast(k)</strong><br class="title-page-name"/>bk: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(163)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.map(j =&gt; j + bk.value).take(5)</strong><br class="title-page-name"/>res184: Array[Int] = Array(6, 7, 8)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; bk.unpersist</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.map(j =&gt; j + bk.value).take(5)</strong><br class="title-page-name"/>res186: Array[Int] = Array(6, 7, 8)
</pre>


            </article>

            
        </section>
    

        <section id="72V5S1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Destroying broadcast variables</h1>
                
            
            <article>
                
<p class="mce-root">You can also destroy broadcast variables, completely removing them from all executors and the Driver too making them inaccessible. This can be quite helpful in managing the resources optimally across the cluster.</p>
<p class="mce-root">Calling <kbd class="calibre11">destroy()</kbd> on a broadcast variable destroys all data and metadata related to the specified broadcast variable. Once a broadcast variable has been destroyed, it cannot be used again and will have to be recreated all over again.</p>
<p class="mce-root">The following is an example of destroying broadcast variables:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rdd_one = sc.parallelize(Seq(1,2,3))</strong><br class="title-page-name"/>rdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[101] at parallelize at &lt;console&gt;:25<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val k = 5</strong><br class="title-page-name"/>k: Int = 5<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val bk = sc.broadcast(k)</strong><br class="title-page-name"/>bk: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(163)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; rdd_one.map(j =&gt; j + bk.value).take(5)</strong><br class="title-page-name"/>res184: Array[Int] = Array(6, 7, 8)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; bk.destroy</strong>
</pre>
<div class="packt_tip">If an attempt is made to use a destroyed broadcast variable, an exception is thrown</div>
<p class="mce-root">The following is an example of an attempt to reuse a destroyed broadcast variable:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; rdd_one.map(j =&gt; j + bk.value).take(5)</strong><br class="title-page-name"/>17/05/27 14:07:28 ERROR Utils: Exception encountered<br class="title-page-name"/>org.apache.spark.SparkException: Attempted to use Broadcast(163) after it was destroyed (destroy at &lt;console&gt;:30)<br class="title-page-name"/> at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144)<br class="title-page-name"/> at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$writeObject$1.apply$mcV$sp(TorrentBroadcast.scala:202)<br class="title-page-name"/> at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$wri
</pre>
<p class="mce-root">Thus, broadcast functionality can be use to greatly improve the flexibility and performance of Spark jobs.</p>


            </article>

            
        </section>
    

        <section>

                            <header id="73TME2-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">Accumulators</h1>
                
            
            <article>
                
<p class="mce-root">Accumulators are shared variables across executors typically used to add counters to your Spark program. If you have a Spark program and would like to know errors or total records processed or both, you can do it in two ways. One way is to add extra logic to just count errors or total records, which becomes complicated when handling all possible computations. The other way is to leave the logic and code flow fairly intact and add Accumulators.</p>
<div class="packt_tip">Accumulators can only be updated by adding to the value.</div>
<p class="mce-root">The following is an example of creating and using a long Accumulator using Spark Context and the <kbd class="calibre11">longAccumulator</kbd> function to initialize a newly created accumulator variable to zero. As the accumulator is used inside the map transformation, the Accumulator is incremented. At the end of the operation, the Accumulator holds a value of 351.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val acc1 = sc.longAccumulator("acc1")</strong><br class="title-page-name"/>acc1: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 10355, name: Some(acc1), value: 0)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val someRDD = statesPopulationRDD.map(x =&gt; {acc1.add(1); x})</strong><br class="title-page-name"/>someRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[99] at map at &lt;console&gt;:29<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; acc1.value</strong><br class="title-page-name"/>res156: Long = 0  /*there has been no action on the RDD so accumulator did not get incremented*/<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; someRDD.count</strong><br class="title-page-name"/>res157: Long = 351<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; acc1.value</strong><br class="title-page-name"/>res158: Long = 351<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; acc1</strong><br class="title-page-name"/>res145: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 10355, name: Some(acc1), value: 351)
</pre>
<p class="mce-root"><span>There are inbuilt accumulators which can be used for many use cases:</span></p>
<ul class="calibre9">
<li class="mce-root1"><span><kbd class="calibre11">LongAccumulator</kbd>: for computing sum, count, and average of 64-bit integers</span></li>
<li class="mce-root1"><span><kbd class="calibre11">DoubleAccumulator</kbd>: for computing sum, count, and averages for double precision floating numbers.</span></li>
<li class="mce-root1"><kbd class="calibre11"><span>CollectionAccumulator</span>[<span>T</span>]</kbd> : for collecting a list of elements</li>
</ul>
<p class="mce-root">All the preceding Accumulators are built on top of the <kbd class="calibre11">AccumulatorV2</kbd> class. By following the same logic, we can potentially build very complex and customized Accumulators to use in our project.</p>
<p class="mce-root">We can build a custom accumulator by extending the <kbd class="calibre11">AccumulatorV2</kbd> class. The following is an example showing the necessary functions to implement. <kbd class="calibre11">AccumulatorV2[Int, Int]</kbd> shown in the following code means that the Input and Output are both of Integer type:</p>
<pre class="calibre19">
<strong class="calibre1">class MyAccumulator extends AccumulatorV2[Int, Int] {<br class="title-page-name"/></strong>  //simple boolean check<br class="title-page-name"/><strong class="calibre1">  override def isZero: Boolean = ???<br class="title-page-name"/><br class="title-page-name"/> </strong> //function to copy one Accumulator and create another one<strong class="calibre1"><br class="title-page-name"/>  override def copy(): AccumulatorV2[Int, Int] = ???<br class="title-page-name"/><br class="title-page-name"/> </strong> //to reset the value<strong class="calibre1"><br class="title-page-name"/>  override def reset(): Unit = ???<br class="title-page-name"/><br class="title-page-name"/>  </strong>//function to add a value to the accumulator<strong class="calibre1"><br class="title-page-name"/>  override def add(v: Int): Unit = ???<br class="title-page-name"/><br class="title-page-name"/>  </strong>//logic to merge two accumulators<strong class="calibre1"><br class="title-page-name"/>  override def merge(other: AccumulatorV2[Int, Int]): Unit = ???<br class="title-page-name"/><br class="title-page-name"/>  </strong>//the function which returns the value of the accumulator<strong class="calibre1"><br class="title-page-name"/>  override def value: Int = ???<br class="title-page-name"/>}</strong>
</pre>
<p class="mce-root">Next, we will look at a practical example of a custom accumulator. Again, we shall use the <kbd class="calibre11">statesPopulation</kbd> CSV file for this. Our goal is to accumulate the sum of year and sum of population in a custom accumulator.</p>
<p class="mce-root"><strong class="calibre1">Step 1. Import the package containing the AccumulatorV2 class:</strong></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.util.AccumulatorV2</strong>
</pre>
<p class="mce-root"><strong class="calibre1">Step 2. Case class to contain the Year and Population:</strong></p>
<pre class="calibre19">
<strong class="calibre1">case class YearPopulation(year: Int, population: Long)</strong>
</pre>
<p class="mce-root"><strong class="calibre1">Step 3. StateAccumulator class extends AccumulatorV2:</strong></p>
<pre class="calibre19">
<br class="title-page-name"/><strong class="calibre1">class StateAccumulator extends AccumulatorV2[YearPopulation, YearPopulation] {</strong> <br class="title-page-name"/>      //declare the two variables one Int for year and Long for population<br class="title-page-name"/>      <strong class="calibre1">private var year = 0 </strong><br class="title-page-name"/><strong class="calibre1">      private var population:Long = 0L</strong><br class="title-page-name"/> <br class="title-page-name"/>      //return iszero if year and population are zero<br class="title-page-name"/>      <strong class="calibre1">override def isZero: Boolean = year == 0 &amp;&amp; population == 0L</strong><br class="title-page-name"/> <br class="title-page-name"/>      //copy accumulator and return a new accumulator<br class="title-page-name"/>     <strong class="calibre1"> override def copy(): StateAccumulator = {  </strong><br class="title-page-name"/><strong class="calibre1">           val newAcc = new StateAccumulator  </strong><br class="title-page-name"/><strong class="calibre1">           newAcc.year =     this.year  </strong><br class="title-page-name"/><strong class="calibre1">           newAcc.population = this.population  </strong><br class="title-page-name"/><strong class="calibre1">           newAcc </strong><br class="title-page-name"/><strong class="calibre1">       }</strong><br class="title-page-name"/><br class="title-page-name"/>       //reset the year and population to zero <br class="title-page-name"/>       <strong class="calibre1">override def reset(): Unit = { year = 0 ; population = 0L }</strong><br class="title-page-name"/> <br class="title-page-name"/>       //add a value to the accumulator<br class="title-page-name"/>       <strong class="calibre1">override def add(v: YearPopulation): Unit = { </strong><br class="title-page-name"/><strong class="calibre1">           year += v.year </strong><br class="title-page-name"/><strong class="calibre1">           population += v.population </strong><br class="title-page-name"/><strong class="calibre1">       }</strong><br class="title-page-name"/> <br class="title-page-name"/>       //merge two accumulators<br class="title-page-name"/>      <strong class="calibre1"> override def merge(other: AccumulatorV2[YearPopulation, YearPopulation]): Unit = {  </strong><br class="title-page-name"/><strong class="calibre1">           other match {               </strong><br class="title-page-name"/><strong class="calibre1">               case o: StateAccumulator =&gt; {     </strong><br class="title-page-name"/><strong class="calibre1">                       year += o.year </strong><br class="title-page-name"/><strong class="calibre1">                       population += o.population    </strong><br class="title-page-name"/><strong class="calibre1">               }    </strong><br class="title-page-name"/><strong class="calibre1">               case _ =&gt;   </strong><br class="title-page-name"/><strong class="calibre1">           } </strong><br class="title-page-name"/><strong class="calibre1">        }</strong><br class="title-page-name"/><br class="title-page-name"/> <br class="title-page-name"/>       //function called by Spark to access the value of accumulator<br class="title-page-name"/>      <strong class="calibre1"> override def value: YearPopulation = YearPopulation(year, population)</strong><br class="title-page-name"/><strong class="calibre1">}</strong>
</pre>
<p class="mce-root"><strong class="calibre1">Step 4. Create a new StateAccumulator and register the same with SparkContext:</strong></p>
<pre class="calibre19">
<strong class="calibre1">val statePopAcc = new StateAccumulator</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">sc.register(statePopAcc, "statePopAcc")</strong>
</pre>
<p class="mce-root"><strong class="calibre1">Step 5. Read the statesPopulation.csv as an RDD:</strong></p>
<pre class="calibre19">
<br class="title-page-name"/><strong class="calibre1">val statesPopulationRDD = sc.textFile("statesPopulation.csv").filter(_.split(",")(0) != "State")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesPopulationRDD.take(10)</strong><br class="title-page-name"/>res1: Array[String] = Array(Alabama,2010,4785492, Alaska,2010,714031, Arizona,2010,6408312, Arkansas,2010,2921995, California,2010,37332685, Colorado,2010,5048644, Delaware,2010,899816, District of Columbia,2010,605183, Florida,2010,18849098, Georgia,2010,9713521)
</pre>
<p class="mce-root"><strong class="calibre1">Step 6. Use the StateAccumulator:</strong></p>
<pre class="calibre19">
<strong class="calibre1">statesPopulationRDD.map(x =&gt; { </strong><br class="title-page-name"/><strong class="calibre1">     val toks = x.split(",") </strong><br class="title-page-name"/><strong class="calibre1">     val year = toks(1).toInt </strong><br class="title-page-name"/><strong class="calibre1">     val pop = toks(2).toLong </strong><br class="title-page-name"/><strong class="calibre1">     statePopAcc.add(YearPopulation(year, pop)) </strong><br class="title-page-name"/><strong class="calibre1">     x</strong><br class="title-page-name"/><strong class="calibre1">}).count</strong>
</pre>
<p class="mce-root"><strong class="calibre1">Step 7. Now, we can examine the value of the StateAccumulator:</strong></p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statePopAcc</strong><br class="title-page-name"/>res2: StateAccumulator = StateAccumulator(id: 0, name: Some(statePopAcc), value: YearPopulation(704550,2188669780))
</pre>
<p class="mce-root">In this section, we examined accumulators and how to build a custom accumulator. Thus, using the preceding illustrated example, you can create complex accumulators to meet your needs.</p>


            </article>

            
        </section>
    

        <section id="74S701-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="mce-root">In this chapter, we discussed the many types of RDDs, such as <kbd class="calibre11">shuffledRDD</kbd>, <kbd class="calibre11">pairRDD</kbd>, <kbd class="calibre11">sequenceFileRDD</kbd>, <kbd class="calibre11">HadoopRDD</kbd>, and so on. We also looked at the three main types of aggregations, <kbd class="calibre11">groupByKey</kbd>, <kbd class="calibre11">reduceByKey</kbd>, and <kbd class="calibre11">aggregateByKey</kbd>. We looked into how partitioning works and why it is important to have a proper plan around partitioning to increase the performance. We also looked at shuffling and the concepts of narrow and wide dependencies which are basic tenets of how Spark jobs are broken into stages. Finally, we looked at the important concepts of broadcast variables and accumulators.</p>
<p class="mce-root">The true power of the flexibility of RDDs makes it easy to adapt to most use cases and perform the necessary operations to accomplish the goal.</p>
<p class="mce-root">In the next chapter, we will switch gears to the higher layer of abstraction added to the RDDs as part of the Tungsten initiative known as DataFrames and Spark SQL and how it all comes together in the <a href="part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 8</a>, <em class="calibre8">Introduce a Little Structure – Spark SQL</em>.</p>


            </article>

            
        </section>
    </body></html>