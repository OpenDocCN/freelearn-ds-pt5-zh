- en: Stream Me Up, Scotty - Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*"I really like streaming services. It''s a great way for people to find your
    music"*'
  prefs: []
  type: TYPE_NORMAL
- en: '- Kygo'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about Spark Streaming and find out how we can
    take advantage of it to process streams of data using the Spark API. Moreover,
    in this chapter, we will learn various ways of processing real-time streams of
    data using a practical example to consume and process tweets from Twitter. In
    a nutshell, the following topics will be covered throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discretized streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateful/stateless transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpointing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interoperability with streaming platforms (Apache Kafka)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Brief introduction to streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today's world of interconnected devices and services, it is hard to even
    spend a few hours a day without our smartphone to check Facebook, or order an
    Uber ride, or tweet something about the burger you just bought, or check the latest
    news or sports updates on your favorite team. We depend on our phones and Internet,
    for a lot of things, whether it is to get work done, or just browse, or e-mail
    your friend. There is simply no way around this phenomenon, and the number and
    variety of applications and services will only grow over time.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the smart devices are everywhere, and they generate a lot of data
    all the time. This phenomenon, also broadly referred to as the Internet of Things,
    has changed the dynamics of data processing forever. Whenever you use any of the
    services or apps on your iPhone, or Droid or Windows phone, in some shape or form,
    real-time data processing is at work. Since so much is depending on the quality
    and value of the apps, there is a lot of emphasis on how the various startups
    and established companies are tackling the complex challenges of **SLAs** (**Service
    Level Agreements**), and usefulness and also the timeliness of the data.
  prefs: []
  type: TYPE_NORMAL
- en: One of the paradigms being researched and adopted by organisations and service
    providers is the building of very scalable, near real-time or real-time processing
    frameworks on a very cutting-edge platform or infrastructure. Everything must
    be fast and also reactive to changes and failures. You would not like it if your
    Facebook updated once every hour or if you received email only once a day; so,
    it is imperative that data flow, processing, and the usage are all as close to
    real time as possible. Many of the systems we are interested in monitoring or
    implementing generate a lot of data as an indefinite continuous stream of events.
  prefs: []
  type: TYPE_NORMAL
- en: As in any other data processing system, we have the same fundamental challenges
    of a collection of data, storage, and processing of data. However, the additional
    complexity is due to the real-time needs of the platform. In order to collect
    such indefinite streams of events and then subsequently process all such events
    in order to generate actionable insights, we need to use highly scalable specialized
    architectures to deal with tremendous rates of events. As such, many systems have
    been built over the decades starting from AMQ, RabbitMQ, Storm, Kafka, Spark,
    Flink, Gearpump, Apex, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Modern systems built to deal with such large amounts of streaming data come
    with very flexible and scalable technologies that are not only very efficient
    but also help realize the business goals much better than before. Using such technologies,
    it is possible to consume data from a variety of data sources and then use it
    in a variety of use cases almost immediately or at a later time as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Let us talk about what happens when you take out your smartphone and book an
    Uber ride to go to the airport. With a few touches on the smartphone screen, you're
    able to select a point, choose the credit card, make the payment, and book the
    ride. Once you're done with your transaction, you then get to monitor the progress
    of your car real-time on a map on your phone. As the car is making its way toward
    you, you're able to monitor exactly where the car is and you can also make a decision
    to pick up coffee at the local Starbucks while you're waiting for the car to pick
    you up.
  prefs: []
  type: TYPE_NORMAL
- en: You could also make informed decisions regarding the car and the subsequent
    trip to the airport by looking at the expected time of arrival of the car. If
    it looks like the car is going to take quite a bit of time picking you up, and
    if this poses a risk to the flight you are about to catch, you could cancel the
    ride and hop in a taxi that just happens to be nearby. Alternatively, if it so
    happens that the traffic situation is not going to let you reach the airport on
    time, thus posing a risk to the flight you are due to catch, then you also get
    to make a decision regarding rescheduling or canceling your flight.
  prefs: []
  type: TYPE_NORMAL
- en: Now in order to understand how such real-time streaming architectures work to
    provide such invaluable information, we need to understand the basic tenets of
    streaming architectures. On the one hand, it is very important for a real-time
    streaming architecture to be able to consume extreme amounts of data at very high
    rates while , on the other hand, also ensuring reasonable guarantees that the
    data that is getting ingested is also processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following images diagram shows a generic stream processing system with
    a producer putting events into a messaging system while a consumer is reading
    from the messaging system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00125.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Processing of real-time streaming data can be categorized into the following
    three essential paradigms:'
  prefs: []
  type: TYPE_NORMAL
- en: At least once processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At most once processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exactly once processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at what these three stream processing paradigms mean to our business
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: While exactly once processing of real-time events is the ultimate nirvana for
    us, it is very difficult to always achieve this goal in different scenarios. We
    have to compromise on the property of exactly once processing in cases where the
    benefit of such a guarantee is outweighed by the complexity of the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: At least once processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The at least once processing paradigm involves a mechanism to save the position
    of the last event received **only after** the event is actually processed and
    results persisted somewhere so that, if there is a failure and the consumer restarts,
    the consumer will read the old events again and process them. However, since there
    is no guarantee that the received events were not processed at all or partially
    processed, this causes a potential duplication of events as they are fetched again.
    This results in the behavior that events ate processed at least once.
  prefs: []
  type: TYPE_NORMAL
- en: At least once is ideally suitable for any application that involves updating
    some instantaneous ticker or gauge to show current values. Any cumulative sum,
    counter, or dependency on the accuracy of aggregations (`sum`, `groupBy`, and
    so on) does not fit the use case for such processing simply because duplicate
    events will cause incorrect results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequence of operations for the consumer are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Save results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save offsets
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shown in the following is an illustration of what happens if there are a failure
    and **consumer** restarts. Since the events have already been processed but the
    offsets have not saved, the consumer will read from the previous offsets saved,
    thus causing duplicates. Event 0 is processed twice in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00277.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: At most once processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The At most once processing paradigm involves a mechanism to save the position
    of the last event received before the event is actually processed and results
    persisted somewhere so that, if there is a failure and the consumer restarts,
    the consumer will not try to read the old events again. However, since there is
    no guarantee that the received events were all processed, this causes potential
    loss of events as they are never fetched again. This results in the behavior that
    the events are processed at most once or not processed at all.
  prefs: []
  type: TYPE_NORMAL
- en: At most once is ideally suitable for any application that involves updating
    some instantaneous ticker or gauge to show current values, as well as any cumulative
    sum, counter, or other aggregation, provided accuracy is not mandatory or the
    application needs absolutely all events. Any events lost will cause incorrect
    results or missing results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequence of operations for the consumer are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Save offsets
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shown in the following is an illustration of what happens if there are a failure
    and the **consumer** restarts. Since the events have not been processed but offsets
    are saved, the consumer will read from the saved offsets, causing a gap in events
    consumed. Event 0 is never processed in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00340.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Exactly once processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Exactly once processing paradigm is similar to the at least once paradigm,
    and involves a mechanism to save the position of the last event received only
    after the event has actually been processed and the results persisted somewhere
    so that, if there is a failure and the consumer restarts, the consumer will read
    the old events again and process them. However, since there is no guarantee that
    the received events were not processed at all or were partially processed, this
    causes a potential duplication of events as they are fetched again. However, unlike
    the at least once paradigm, the duplicate events are not processed and are dropped,
    thus resulting in the exactly once paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Exactly once processing paradigm is suitable for any application that involves
    accurate counters, aggregations, or which in general needs every event processed
    only once and also definitely once (without loss).
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequence of operations for the consumer are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Save results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save offsets
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is illustration shows what happens if there are a failure and
    the **consumer** restarts. Since the events have already been processed but offsets
    have not saved, the consumer will read from the previous offsets saved, thus causing
    duplicates. Event 0 is processed only once in the following figure because the
    **consumer** drops the duplicate event 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'How does the Exactly once paradigm drop duplicates? There are two techniques
    which can help here:'
  prefs: []
  type: TYPE_NORMAL
- en: Idempotent updates
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transactional updates
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark Streaming also implements structured streaming in Spark 2.0+, which support
    Exactly once processing out of the box. We will look at structured streaming later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Idempotent updates involve saving results based on some unique ID/key generated
    so that, if there is a duplicate, the generated unique ID/key will already be
    in the results (for instance, a database) so that the consumer can drop the duplicate
    without updating the results. This is complicated as it's not always possible
    or easy to generate unique keys. It also requires additional processing on the
    consumer end. Another point is that the database can be separate for results and
    offsets.
  prefs: []
  type: TYPE_NORMAL
- en: Transactional updates save results in batches that have a transaction beginning
    and a transaction commit phase within so that, when the commit occurs, we know
    that the events were processed successfully. Hence, when duplicate events are
    received, they can be dropped without updating results. This technique is much
    more complicated than the idempotent updates as now we need some transactional
    data store. Another point is that the database must be the same for results and
    offsets.
  prefs: []
  type: TYPE_NORMAL
- en: You should look into the use case you're trying to build and see if at least
    once processing, or At most once processing, can be reasonably wide and still
    achieve an acceptable level of performance and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We will be looking at the paradigms closely when we learn about Spark Streaming
    and how to use Spark Streaming and consume events from Apache Kafka in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Streaming is not the first streaming architecture to come into existence.
    Several technologies have existenced over time to deal with the real-time processing
    needs of various business use cases. Twitter Storm was one of the first popular
    stream processing technologies out there and was in used by many organizations
    fulfilling the needs of many businesses.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark comes with a streaming library, which has rapidly evolved to be
    the most widely used technology. Spark Streaming has some distinct advantages
    over the other technologies, the first and foremost being the tight integration
    between Spark Streaming APIs and the Spark core APIs making building a dual purpose
    real-time and batch analytical platform feasible and efficient than otherwise.
    Spark Streaming also integrates with Spark ML and Spark SQL, as well as GraphX,
    making it the most powerful stream processing technology that can serve many unique
    and complex use cases. In this section, we will look deeper into what Spark Streaming
    is all about.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on Spark Streaming, you can refer to [https://spark.apache.org/docs/2.1.0/streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/streaming-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming supports several input sources and can write results to several
    sinks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00004.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: While Flink, Heron (successor to Twitter Storm), Samza, and so on all handle
    events as they are collected with minimal latency, Spark Streaming consumes continuous
    streams of data and then processes the collected data in the form of micro-batches.
    The size of the micro-batch can be as low as 500 milliseconds but usually cannot
    go lower than that.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Apex, Gear pump, Flink, Samza, Heron, or other upcoming technologies
    compete with Spark Streaming in some use cases. If you need true event-by-event
    processing, then Spark Streaming is not the right fit for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: The way streaming works are by creating batches of events at regular time intervals
    as per configuration and delivering the micro-batches of data at every specified
    interval for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00011.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Just like `SparkContext,` Spark Streaming has a `StreamingContext`, which is
    the main entry point for the streaming job/application. `StreamingContext` is
    dependent on `SparkContext`. In fact, the `SparkContext` can be directly used
    in the streaming job. The `StreamingContext` is similar to the `SparkContext`,
    except that `StreamingContext` also requires the program to specify the time interval
    or duration of the batching interval, which can be in milliseconds or minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that `SparkContext` is the main point of entry, and the task scheduling
    and resource management is part of `SparkContext`, so `StreamingContext` reuses
    the logic.
  prefs: []
  type: TYPE_NORMAL
- en: StreamingContext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`StreamingContext` is the main entry point for streaming and essentially takes
    care of the streaming application, including checkpointing, transformations, and
    actions on DStreams of RDDs.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating StreamingContext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A new StreamingContext can be created in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `StreamingContext` using an existing `SparkContext` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `StreamingContext` by providing the configuration necessary for a
    new `SparkContext` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A third method is to use `getOrCreate()`, which is used to either recreate
    a `StreamingContext` from checkpoint data or to create a new `StreamingContext`.
    If checkpoint data exists in the provided `checkpointPath`, then `StreamingContext`
    will be recreated from the checkpoint data. If the data does not exist, then the
    `StreamingContext` will be created by calling the provided `creatingFunc`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Starting StreamingContext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `start()` method starts the execution of the streams defined using the
    `StreamingContext`. This essentially starts the entire streaming application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Stopping StreamingContext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stopping the `StreamingContext` stops all processing and you will have to recreate
    a new `StreamingContext` and invoke `start()` on it to restart the application.
    There are two APIs useful to stop a stream processing application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stop the execution of the streams immediately (do not wait for all received
    data to be processed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Stop the execution of the streams, with the option of ensuring that all received
    data has been processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Input streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several types of input streams such as `receiverStream` and `fileStream`
    that can be created using the `StreamingContext` as shown in the following subsections:'
  prefs: []
  type: TYPE_NORMAL
- en: receiverStream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Create an input stream with any arbitrary user implemented receiver. It can
    be customized to meet the use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Find more details at [http://spark.apache.org/docs/latest/streaming-custom-receivers.html](http://spark.apache.org/docs/latest/streaming-custom-receivers.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the API declaration for the `receiverStream`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: socketTextStream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This creates an input stream from TCP source `hostname:port`. Data is received
    using a TCP socket and the received bytes are interpreted as UTF8 encoded `\n`
    delimited lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: rawSocketStream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Create an input stream from network source `hostname:port`, where data is received
    as serialized blocks (serialized using the Spark's serializer) that can be directly
    pushed into the block manager without deserializing them. This is the most efficient
  prefs: []
  type: TYPE_NORMAL
- en: way to receive data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: fileStream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create an input stream that monitors a Hadoop-compatible filesystem for new
    files and reads them using the given key-value types and input format. Files must
    be written to the monitored directory by moving them from another location within
    the same filesystem. File names starting with a dot (`.`) are ignored, so this
    is an obvious choice for the moved file names in the monitored directory. Using
    an atomic file rename function call, the filename which starts with `.` can be
    now renamed to an actual usable filename so that `fileStream` can pick it up and
    let us process the file content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: textFileStream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create an input stream that monitors a Hadoop-compatible filesystem for new
    files and reads them as text files (using a key as `LongWritable`, value as Text,
    and input format as `TextInputFormat`). Files must be written to the monitored
    directory by moving them from another location within the same filesystem. File
    names starting with . are ignored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: binaryRecordsStream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create an input stream that monitors a Hadoop-compatible filesystem for new
    files and reads them as flat binary files, assuming a fixed length per record,
    generating one byte array per record. Files must be written to the monitored directory
    by moving them from another location within the same filesystem. File names starting
    with `.` are ignored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: queueStream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create an input stream from a queue of RDDs. In each batch, it will process
    either one or all of the RDDs returned by the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: textFileStream example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Shown in the following is a simple example of Spark Streaming using `textFileStream`.
    In this example, we create a `StreamingContext` from the spark-shell `SparkContext`
    (`sc`) and an interval of 10 seconds. This starts the `textFileStream`, which
    monitors the directory named **streamfiles** and processes any new file found
    in the directory. In this example, we are simply printing the number of elements
    in the RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: twitterStream example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us look at another example of how we can process tweets from Twitter using
    Spark Streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: First, open a terminal and change the directory to `spark-2.1.1-bin-hadoop2.7`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder `streamouts` under the `spark-2.1.1-bin-hadoop2.7` folder where
    you have spark installed. When the application runs, `streamouts` folder will
    have collected tweets to text files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the following jars into the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar](http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Launch spark-shell with the jars needed for Twitter integration specified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can write a sample code. Shown in the following is the code to test
    Twitter event processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You will see the `streamouts` folder contains several `tweets` output in text
    files. You can now open the directory `streamouts` and check that the files contain
    `tweets`.
  prefs: []
  type: TYPE_NORMAL
- en: Discretized streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Streaming is built on an abstraction called **Discretized Streams** referred,
    to as **DStreams**. A DStream is represented as a sequence of RDDs, with each
    RDD created at each time interval. The DStream can be processed in a similar fashion
    to regular RDDs using similar concepts such as a directed cyclic graph-based execution
    plan (Directed Acyclic Graph). Just like a regular RDD processing, the transformations
    and actions that are part of the execution plan are handled for the DStreams.
  prefs: []
  type: TYPE_NORMAL
- en: DStream essentially divides a never ending stream of data into smaller chunks
    known as micro-batches based on a time interval, materializing each individual
    micro-batch as a RDD which can then processed as a regular RDD. Each such micro-batch
    is processed independently and no state is maintained between micro-batches thus
    making the processing stateless by nature. Let's say the batch interval is 5 seconds,
    then while events are being consumed, real-time and a micro-batch are created
    at every 5-second interval and the micro-batch is handed over for further processing
    as an RDD. One of the main advantages of Spark Streaming is that the API calls
    used to process the micro-batch of events are very tightly integrated into the
    spark for APIs to provide seamless integration with the rest of the architecture.
    When a micro-batch is created, it gets turned into an RDD, which makes it a seamless
    process using spark APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `DStream` class looks like the following in the source code showing the
    most important variable, a `HashMap[Time, RDD]` pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Shown in the following is an illustration of a DStream comprising an RDD created
    every **T** seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the following example, a streaming context is created to create micro-batches
    every 5 seconds and to create an RDD, which is just like a Spark core API RDD.
    The RDDs in the DStream can be processed just like any other RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in building a streaming application are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a `StreamingContext` from the `SparkContext`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `DStream` from `StreamingContext`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide transformations and actions that can be applied to each RDD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the streaming application is started by calling `start()` on the `StreamingContext`.
    This starts the entire process of consuming and processing real-time events.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the Spark Streaming application has started, no further operations can
    be added. A stopped context cannot be restarted and you have to create a new streaming
    context if such a need arises.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown in the following is an example of how to create a simple streaming job
    accessing Twitter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `StreamingContext` from the `SparkContext`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `DStream` from `StreamingContext`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Provide transformations and actions that can be applied to each RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the streaming application is started by calling `start()` on the `StreamingContext`.
    This starts the entire process of consuming and processing real-time events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Created a `DStream` of type `ReceiverInputDStream`, which is defined as an
    abstract class for defining any `InputDStream` that has to start a receiver on
    worker nodes to receive external data. Here, we are receiving from Twitter stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run a transformation `flatMap()` on the `twitterStream`, you get a `FlatMappedDStream`,
    as shown in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformations on a DStream are similar to the transformations applicable to
    a Spark core RDD. Since DStream consists of RDDs, a transformation also applies
    to each RDD to generate a transformed RDD for the RDD, and then a transformed
    DStream is created. Every transformation creates a specific `DStream` derived
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the hierarchy of `DStream` classes starting from
    the parent `DStream` class. We can also see the different classes inheriting from
    the parent class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There are a lot of `DStream` classes purposely built for the functionality.
    Map transformations, window functions, reduce actions, and different types of
    input streams are all implemented using different class derived from `DStream`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown in the following is an illustration of a transformation on a base DStream
    to generate a filtered DStream. Similarly, any transformation is applicable to
    a DStream:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00382.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Refer to the following table for the types of transformations possible.
  prefs: []
  type: TYPE_NORMAL
- en: '| Transformation | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| `map(func)` | This applies the transformation function to each element of
    the DStream and returns a new DStream. |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMap(func)` | This is similar to map; however, just like RDD''s `flatMap`
    versus map, using `flatMap` operates on each element and applies `flatMap`, producing
    multiple output items per each input. |'
  prefs: []
  type: TYPE_TB
- en: '| `filter(func)` | This filters out the records of the DStream to return a
    new DStream. |'
  prefs: []
  type: TYPE_TB
- en: '| `repartition(numPartitions)` | This creates more or fewer partitions to redistribute
    the data to change the parallelism. |'
  prefs: []
  type: TYPE_TB
- en: '| `union(otherStream)` | This combines the elements in two source DStreams
    and returns a new DStream. |'
  prefs: []
  type: TYPE_TB
- en: '| `count()` | This returns a new DStream by counting the number of elements
    in each RDD of the source DStream. |'
  prefs: []
  type: TYPE_TB
- en: '| `reduce(func)` | This returns a new DStream by applying the `reduce` function
    on each element of the source DStream. |'
  prefs: []
  type: TYPE_TB
- en: '| `countByValue()` | This computes the frequency of each key and returns a
    new DStream of (key, long) pairs. |'
  prefs: []
  type: TYPE_TB
- en: '| `reduceByKey(func, [numTasks])` | This aggregates the data by key in the
    source DStream''s RDDs and returns a new DStream of (key, value) pairs. |'
  prefs: []
  type: TYPE_TB
- en: '| `join(otherStream, [numTasks])` | This joins two DStreams of *(K, V)* and
    *(K, W)* pairs and returns a new DStream of *(K, (V, W))* pairs combining the
    values from both DStreams. |'
  prefs: []
  type: TYPE_TB
- en: '| `cogroup(otherStream, [numTasks])` | `cogroup()`, when called on a DStream
    of *(K, V)* and *(K, W)* pairs, will return a new DStream of *(K, Seq[V], Seq[W])*
    tuples. |'
  prefs: []
  type: TYPE_TB
- en: '| `transform(func)` | This applies a transformation function on each RDD of
    the source DStream and returns a new DStream. |'
  prefs: []
  type: TYPE_TB
- en: '| `updateStateByKey(func)` | This updates the state for each key by applying
    the given function on the previous state of the key and the new values for the
    key. Typically, it used to maintain a state machine. |'
  prefs: []
  type: TYPE_TB
- en: Window operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark Streaming provides windowed processing, which allows you to apply transformations
    over a sliding window of events. The sliding window is created over an interval
    specified. Every time the window slides over a source DStream, the source RDDs,
    which fall within the window specification, are combined and operated upon to
    generate the windowed DStream. There are two parameters that need to be specified
    for the window:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Window length: This specifies the length in interval considered as the window**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sliding interval: This is the interval at which the window is created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The window length and the sliding interval must both be a multiple of the block
    interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown in the following is an illustration shows a DStream with a sliding window
    operation showing how the old window (dotted line rectangle) slides by one interval
    to the right into the new window (solid line rectangle):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Some of the common window operation are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| Transformation | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| `window(windowLength, slideInterval)` | This creates a window on the source
    DStream and returns the same as a new DStream. |'
  prefs: []
  type: TYPE_TB
- en: '| `countByWindow(windowLength, slideInterval)` | This returns count of elements
    in the DStream by applying a sliding window. |'
  prefs: []
  type: TYPE_TB
- en: '| `reduceByWindow(func, windowLength, slideInterval)` | This returns a new
    DStream by applying the reduce function on each element of the source DStream
    after creating a sliding window of length `windowLength`. |'
  prefs: []
  type: TYPE_TB
- en: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | This
    aggregates the data by key in the window applied to the source DStream''s RDDs
    and returns a new DStream of (key, value) pairs. The computation is provided by
    function `func`. |'
  prefs: []
  type: TYPE_TB
- en: '| `reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`
    | This aggregates the data by key in the window applied to the source DStream''s
    RDDs and returns a new DStream of (key, value) pairs. The key difference between
    the preceding function and this one is the `invFunc`, which provides the computation
    to be done at the beginning of the sliding window. |'
  prefs: []
  type: TYPE_TB
- en: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | This computes
    the frequency of each key and returns a new DStream of (key, long) pairs within
    the sliding window as specified. |'
  prefs: []
  type: TYPE_TB
- en: Let us look at the Twitter stream example in more detail. Our goal is to print
    the top five words used in tweets streamed every five seconds, using a window
    of length 15 seconds, sliding every 10 seconds. Hence, we can get the top five
    words in 15 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this code, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, open a terminal and change directory to `spark-2.1.1-bin-hadoop2.7`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder `streamouts` under the `spark-2.1.1-bin-hadoop2.7` folder where
    you have spark installed. When the application runs, the `streamouts` folder will
    have collected tweets to text files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the following jars into the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar](http://central.maven.org/maven2/org/apache/bahir/spark-streaming-twitter_2.11/2.1.0/spark-streaming-twitter_2.11-2.1.0.jar)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.6/twitter4j-core-4.0.6.jar)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar](http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.6/twitter4j-stream-4.0.6.jar)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Launch spark-shell with the jars needed for Twitter integration specified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can write the code. Shown in the following is the code used to test
    Twitter event processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is displayed on the console every 15 seconds and looks something
    like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Stateful/stateless transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As seen previously, Spark Streaming uses a concept of DStreams, which are essentially
    micro-batches of data created as RDDs. We also saw types of transformations that
    are possible on DStreams. The transformations on DStreams can be grouped into
    two types: **Stateless transformations** and **Stateful transformations.**'
  prefs: []
  type: TYPE_NORMAL
- en: In Stateless transformations, the processing of each micro-batch of data does
    not depend on the previous batches of data. Thus, this is a stateless transformation,
    with each batch doing its own processing independently of anything that occurred
    prior to this batch.
  prefs: []
  type: TYPE_NORMAL
- en: In Stateful transformations, the processing of each micro-batch of data depends
    on the previous batches of data either fully or partially. Thus, this is a stateful
    transformation, with each batch considering what happened prior to this batch
    and then using the information while computing the data in this batch.
  prefs: []
  type: TYPE_NORMAL
- en: Stateless transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stateless transformations transform one DStream to another by applying transformations
    to each of the RDDs within the DStream. Transformations such as `map()`, `flatMap()`,
    `union()`, `join()`, and `reduceByKey` are all examples of stateless transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown in the following is an illustration showing a `map()` transformation
    on `inputDStream` to generate a new `mapDstream`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00210.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Stateful transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stateful transformations operate on a DStream, but the computations depend on
    the previous state of processing. Operations such as `countByValueAndWindow`,
    `reduceByKeyAndWindow` , `mapWithState`, and `updateStateByKey` are all examples
    of stateful transformations. In fact, all window-based transformations are all
    stateful because, by the definition of window operations, we need to keep track
    of the window length and sliding interval of DStream.
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-time streaming applications are meant to be long running and resilient
    to failures of all sorts. Spark Streaming implements a checkpointing mechanism
    that maintains enough information to recover from failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of data that needs to be checkpointed:'
  prefs: []
  type: TYPE_NORMAL
- en: Metadata checkpointing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data checkpointing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Checkpointing can be enabled by calling `checkpoint()` function on the `StreamingContext`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Specifies the directory where the checkpoint data will be reliably stored.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this must be a fault-tolerant file system like HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once checkpoint directory is set, any DStream can be checkpointed into the
    directory based on a specified interval. Looking at the Twitter example, we can
    checkpoint each DStream every 10 seconds into the directory `checkpoints`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `checkpoints` directory looks something like the following after few seconds,
    showing the metadata as well as the RDDs and the `logfiles` are maintained as
    part of the checkpointing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00246.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Metadata checkpointing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Metadata checkpointing** saves information defining the streaming operations,
    which are represented by a **Directed Acyclic Graph** (**DAG**) to the HDFS. This
    can be used to recover the DAG, if there is a failure and the application is restarted.
    The driver restarts and reads the metadata from HDFS, and rebuilds the DAG and
    recovers all the operational state before the crash.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metadata includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Configuration**: the configuration that was used to create the streaming
    application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DStream operations**: the set of DStream operations that define the streaming
    application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incomplete batches**: batches whose jobs are queued but have not completed
    yet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data checkpointing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data checkpointing saves the actual RDDs to HDFS so that, if there is a failure
    of the Streaming application, the application can recover the checkpointed RDDs
    and continue from where it left off. While streaming application recovery is a
    good use case for the data checkpointing, checkpointing also helps in achieving
    better performance whenever some RDDs are lost because of cache cleanup or loss
    of an executor by instantiating the generated RDDs without a need to wait for
    all the parent RDDs in the lineage (DAG) to be recomputed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Checkpointing must be enabled for applications with any of the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Usage of stateful transformations**: If either `updateStateByKey` or `reduceByKeyAndWindow`
    (with inverse function) is used in the application, then the checkpoint directory
    must be provided to allow for periodic RDD checkpointing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recovering from failures of the driver running the application**: Metadata
    checkpoints are used to recover with progress information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your streaming application does not have the stateful transformations, then
    the application can be run without enabling checkpointing.
  prefs: []
  type: TYPE_NORMAL
- en: There might be loss of data received but not processed yet in your streaming
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Note that checkpointing of RDDs incurs the cost of saving each RDD to storage.
    This may cause an increase in the processing time of those batches where RDDs
    get checkpointed. Hence, the interval of checkpointing needs to be set carefully
    so as not to cause performance issues. At tiny batch sizes (say 1 second), checkpointing
    too frequently every tiny batch may significantly reduce operation throughput.
    Conversely, checkpointing too infrequently causes the lineage and task sizes to
    grow, which may cause processing delays as the amount of data to be persisted
    is large.
  prefs: []
  type: TYPE_NORMAL
- en: For stateful transformations that require RDD checkpointing, the default interval
    is a multiple of the batch interval that is at least 10 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: A checkpoint interval of 5 to 10 sliding intervals of a DStream is a good setting
    to start with.
  prefs: []
  type: TYPE_NORMAL
- en: Driver failure recovery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Driver failure recovery can be accomplished by using `StreamingContext.getOrCreate()`
    to either initialize `StreamingContext` from an existing checkpoint or to create
    a new StreamingContext.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two conditions for a streaming application when started are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When the program is being started for the first time, it needs to create a new
    `StreamingContext`, set up all the streams, and then call `start()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the program is being restarted after failure, it needs to initialize a
    `StreamingContext` from the checkpoint data in the checkpoint directory and then
    call `start()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will implement a function `createStreamContext()`, which creates the `StreamingContext`
    and sets up the various DStreams to parse the tweets and generate the top five
    tweet hashtags every 15 seconds using a window. But instead of calling `createStreamContext(`)
    and then calling `ssc.start()` , we will call `getOrCreate()` so that if the `checkpointDirectory`
    exists, then the context will be recreated from the checkpoint data. If the directory
    does not exist (the application is running for the first time), then the function
    `createStreamContext()` will be called to create a new context and set up the
    DStreams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Shown in the following is the code showing the definition of the function and
    how `getOrCreate()` can be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Interoperability with streaming platforms (Apache Kafka)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Streaming has very good integration with Apache Kafka, which is the most
    popular messaging platform currently. Kafka integration has several approaches,
    and the mechanism has evolved over time to improve the performance and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main approaches for integrating Spark Streaming with Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: Receiver-based approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct stream approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Receiver-based approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The receiver-based approach was the first integration between Spark and Kafka.
    In this approach, the driver starts receivers on the executors that pull data
    using high-level APIs, from Kafka brokers. Since receivers are pulling events
    from Kafka brokers, receivers update the offsets into Zookeeper, which is also
    used by Kafka cluster. The key aspect is the usage of a **WAL** (**Write Ahead
    Log**), which the receiver keeps writing to as it consumes data from Kafka. So,
    when there is a problem and executors or receivers are lost or restarted, the
    WAL can be used to recover the events and process them. Hence, this log-based
    design provides both durability and consistency.
  prefs: []
  type: TYPE_NORMAL
- en: Each receiver creates an input DStream of events from a Kafka topic while querying
    Zookeeper for the Kafka topics, brokers, offsets, and so on. After this, the discussion
    we had about DStreams in previous sections comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Long-running receivers make parallelism complicated as the workload is not going
    to be properly distributed as we scale the application. Dependence on HDFS is
    also a problem along with the duplication of write operations. As for the reliability
    needed for exactly once paradigm of processing, only the idempotent approach will
    work. The reason why a transactional approach, will not work in the receiver-based
    approach is that there is no way to access the offset ranges from the HDFS location
    or Zookeeper.
  prefs: []
  type: TYPE_NORMAL
- en: The receiver-based approach works with any messaging system, so it's more general
    purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create a receiver-based stream by invoking the `createStream()` API
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Shown in the following is an example of creating a receiver-based stream that
    pulls messages from Kafka brokers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Shown in the following is an illustration of how the driver launches receivers
    on executors to pull data from Kafka using the high-level API. The receivers pull
    the topic offset ranges from the Kafka Zookeeper cluster and then also update
    Zookeeper as they pull events from the brokers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Direct stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The direct stream based approach is the newer approach with respect to Kafka
    integration and works by using the driver to connect to the brokers directly and
    pull events. The key aspect is that using direct stream API, Spark tasks work
    on a 1:1 ratio when looking at spark partition to Kafka topic/partition. No dependency
    on HDFS or WAL makes it flexible. Also, since now we can have direct access to
    offsets, we can use idempotent or transactional approach for exactly once processing.
  prefs: []
  type: TYPE_NORMAL
- en: Create an input stream that directly pulls messages from Kafka brokers without
    using any receiver. This stream can guarantee that each message from Kafka is
    included in transformations exactly once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Properties of a direct stream are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**No receivers**: This stream does not use any receiver, but rather directly
    queries Kafka.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offsets**: This does not use Zookeeper to store offsets, and the consumed
    offsets are tracked by the stream itself. You can access the offsets used in each
    batch from the generated RDDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failure recovery**: To recover from driver failures, you have to enable checkpointing
    in the `StreamingContext`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**End-to-end semantics**: This stream ensures that every record is effectively
    received and transformed exactly once, but gives no guarantees on whether the
    transformed data are outputted exactly once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can create a direct stream by using KafkaUtils, `createDirectStream()`
    API as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Shown in the following is an example of a direct stream created to pull data
    from Kafka topics and create a DStream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The direct stream API can only be used with Kafka, so this is not a general
    purpose approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown in the following is an illustration of how the driver pulls offset information
    from Zookeeper and directs the executors to launch tasks to pull events from brokers
    based on the offset ranges prescribed by the driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Structured streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Structured streaming is new in Apache Spark 2.0+ and is now in GA from Spark
    2.2 release. You will see details in the next section along with examples of how
    to use structured streaming.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on the Kafka integration in structured streaming, refer to
    [https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of how to use Kafka source stream in structured streaming is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'An example of how to use Kafka source instead of source stream (in case you
    want more batch analytics approach) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Structured streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Structured streaming is a scalable and fault-tolerant stream processing engine
    built on top of Spark SQL engine. This brings stream processing and computations
    closer to batch processing, rather than the DStream paradigm and challenges involved
    with Spark streaming APIs at this time. The structured streaming engine takes
    care of several challenges like exactly-once stream processing, incremental updates
    to results of processing, aggregations, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The structured streaming API also provides the means to tackle a big challenge
    of Spark streaming, that is, Spark streaming processes incoming data in micro-batches
    and uses the received time as a means of splitting the data, thus not considering
    the actual event time of the data. The structured streaming allows you to specify
    such an event time in the data being received so that any late coming data is
    automatically handled.
  prefs: []
  type: TYPE_NORMAL
- en: The structured streaming is GA in Spark 2.2, and the APIs are marked GA. Refer
    to [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea behind structured streaming is to treat a live data stream as
    an unbounded table being appended to continuously as events are processed from
    the stream. You can then run computations and SQL queries on this unbounded table
    as you normally do on batch data. A Spark SQL query for instance will process
    the unbounded table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00348.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As the DStream keeps changing with time, more and more data will be processed
    to generate the results. Hence, the unbounded input table is used to generate
    a result table. The output or results table can be written to an external sink
    known as **Output**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Output** is what gets written out and can be defined in a different mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complete mode**: The entire updated result table will be written to the external
    storage. It is up to the storage connector to decide how to handle the writing
    of the entire table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Append mode**: Only any new rows appended to the result table since the last
    trigger will be written to the external storage. This is applicable only on the
    queries where existing rows in the result table are not expected to change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update mode**: Only the rows that were updated in the result table since
    the last trigger will be written to the external storage. Note that this is different
    from the complete mode in that this mode only outputs the rows that have changed
    since the last trigger. If the query doesn''t contain aggregations, it will be
    equivalent to Append mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shown in the following is an illustration of the output from the unbounded
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00001.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will show an example of creating a Structured streaming query by listening
    to input on localhost port 9999.
  prefs: []
  type: TYPE_NORMAL
- en: 'If using a Linux or Mac, it''s easy to start a simple server on port 9999:
    nc -lk 9999.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown in the following is an example where we start by creating an `inputStream`
    calling SparkSession''s `readStream` API and then extracting the words from the
    lines. Then we group the words and count the occurrences before finally writing
    the results to the output stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As you keep typing words in the terminal, the query keeps updating and generating
    results which are printed on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Handling Event-time and late data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Event time** is the time inside the data itself. Traditional Spark Streaming
    only handled time as the received time for the DStream purposes, but this is not
    enough for many applications where we need the event time. For example, if you
    want to get the number of times hashtag appears in a tweet every minute, then
    you should want to use the time when the data was generated, not when Spark receives
    the event. To get event time into the mix, it is very easy to do so in structured
    streaming by considering the event time as a column in the row/event. This allows
    window-based aggregations to be run using the event time rather than the received
    time. Furthermore, this model naturally handles data that has arrived later than
    expected based on its event time. Since Spark is updating the result table, it
    has full control over updating old aggregates when there is late data as well
    as cleaning up old aggregates to limit the size of intermediate state data. There
    is also support for watermarking event streams, which allows the user to specify
    the threshold of late data and allows the engine to accordingly clean up the old
    state.'
  prefs: []
  type: TYPE_NORMAL
- en: Watermarks enable the engine to track the current event times and determine
    whether the event needs to be processed or has been already processed by checking
    the threshold of how late data can be received. For instance, if the event time
    is denoted by `eventTime` and the threshold interval of late arriving data is
    `lateThreshold`, then by checking the difference between the `max(eventTime) -
    lateThreshold` and comparing with the specific window starting at time T, the
    engine can determine if the event can be considered for processing in this window
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shown in the following is an extension of the preceding example on structured
    streaming listening on port 9999\. Here we are enabling `Timestamp` as part of
    the input data so that we can do Window operations on the unbounded table to generate
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Fault tolerance semantics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Delivering **end-to-end exactly once semantics** was one of the key goals behind
    the design of Structured streaming, which implements the Structured streaming
    sources, the output sinks, and the execution engine to reliably track the exact
    progress of the processing so that it can handle any kind of failure by restarting
    and/or reprocessing. Every streaming source is assumed to have offsets (similar
    to Kafka offsets) to track the read position in the stream. The engine uses checkpointing
    and write ahead logs to record the offset range of the data being processed in
    each trigger. The streaming sinks are designed to be idempotent for handling reprocessing.
    Together, using replayable sources and idempotent sinks, Structured streaming
    can ensure end-to-end exactly once semantics under any failure.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that exactly once the paradigm is more complicated in traditional streaming
    using some external database or store to maintain the offsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structured streaming is still evolving and has several challenges to overcome
    before it can be widely used. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple streaming aggregations are not yet supported on streaming datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting and taking first *N* rows is not supported on streaming datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinct operations on streaming datasets are not supported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sorting operations are supported on streaming datasets only after an aggregation
    step is performed and that too exclusively when in complete output mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any kind of join operations between two streaming datasets are not yet supported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only a few types of sinks - file sink and for each sink are supported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the concepts of the stream processing systems,
    Spark streaming, DStreams of Apache Spark, what DStreams are, DAGs and lineages
    of DStreams, Transformations, and Actions. We also looked at window concept of
    stream processing. We also looked at a practical examples of consuming tweets
    from Twitter using Spark Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we looked at receiver-based and direct stream approaches of consuming
    data from Kafka. In the end, we also looked at the new structured streaming, which
    promises to solve many of the challenges such as fault tolerance and exactly once
    semantics on the stream. We also discussed how structured streaming also simplifies
    the integration with messaging systems such as Kafka or other messaging systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at graph processing and how it all works.
  prefs: []
  type: TYPE_NORMAL
