<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Integrating R and Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Integrating R and Hadoop</h1></div></div></div><p>From the first two chapters we got basic information on how to install the R and Hadoop tools. Also, we learned what the key features of Hadoop are and why they are integrated with R for Big Data solutions to business data problems. So with the integration of R and Hadoop we can forward data analytics to Big Data analytics. Both of these middleware are still getting improved for being used along with each other.</p><p>In <a class="link" href="ch02.html" title="Chapter 2. Writing Hadoop MapReduce Programs">Chapter 2</a>, <span class="emphasis"><em>Writing Hadoop MapReduce Programs</em></span>, we learned how to write a MapReduce program in Hadoop. In this chapter, we will learn to develop the MapReduce programs in R that run over the Hadoop cluster. This chapter will provide development tutorials on R and Hadoop with RHIPE and RHadoop. After installing R and Hadoop, we will see how R and Hadoop can be integrated using easy steps.</p><p>Before we start moving on to the installation, let's see what are the advantages of R and Hadoop integration within an organization. Since statisticians and data analysts frequently use the R tool for data exploration as well as data analytics, Hadoop integration is a big boon for processing large-size data. Similarly, data engineers who use Hadoop tools, such as system, to organize the data warehouse can perform such logical analytical operations to get informative insights that are actionable by integrating with R tool.</p><p>Therefore, the integration of such data-driven tools and technologies can build a powerful scalable system that has features of both of them.</p><div class="mediaobject"><img src="graphics/3282OS_03_01.jpg" alt="Integrating R and Hadoop"/></div><p>Three<a id="id264" class="indexterm"/> ways <a id="id265" class="indexterm"/>to link R and Hadoop are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">RHIPE</li><li class="listitem" style="list-style-type: disc">RHadoop</li><li class="listitem" style="list-style-type: disc">Hadoop streaming</li></ul></div><p>In this chapter, we will be learning integration and analytics with RHIPE and RHadoop. Hadoop streaming will be covered in <a class="link" href="ch04.html" title="Chapter 4. Using Hadoop Streaming with R">Chapter 4</a>, <span class="emphasis"><em>Using Hadoop Streaming with R</em></span>.</p><div class="section" title="Introducing RHIPE"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec29"/>Introducing RHIPE</h1></div></div></div><p><span class="strong"><strong>RHIPE</strong></span> stands for <span class="strong"><strong>R and Hadoop Integrated Programming Environment</strong></span>. As mentioned on <a class="ulink" href="http://www.datadr.org/">http://www.datadr.org/</a>, it means "in a moment" in Greek and is a merger of R<a id="id266" class="indexterm"/> and Hadoop. It was first developed by <span class="emphasis"><em>Saptarshi Guha</em></span> for his PhD thesis in the Department of Statistics at Purdue University in 2012. Currently this is carried out by the Department of Statistics team at Purdue University and other active Google discussion groups.</p><p>The RHIPE package uses the <a id="id267" class="indexterm"/>
<span class="strong"><strong>Divide and Recombine</strong></span> technique to perform data analytics over Big Data. In this technique, data is divided into subsets, computation is performed over those subsets by specific R analytics operations, and the output is combined. RHIPE has mainly been designed to accomplish <a id="id268" class="indexterm"/>two goals that are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Allowing you to perform in-depth analysis of large as well as small data.</li><li class="listitem" style="list-style-type: disc">Allowing users to perform the analytics operations within R using a lower-level language. RHIPE is <a id="id269" class="indexterm"/>designed with several functions that help perform <span class="strong"><strong>Hadoop Distribute File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>) as well as MapReduce operations using a simple R console.</li></ul></div><p>RHIPE is a <a id="id270" class="indexterm"/>lower-level interface as compared to HDFS and MapReduce operation. Use the latest supported version of RHIPE which is 0.73.1 as <code class="literal">Rhipe_0.73.1-2.tar.gz</code>.</p><div class="section" title="Installing RHIPE"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec33"/>Installing RHIPE</h2></div></div></div><p>As RHIPE is a <a id="id271" class="indexterm"/>connector of R and Hadoop, we need Hadoop and R installed <a id="id272" class="indexterm"/>on our machine or in our clusters in the following sequence:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Installing Hadoop.</li><li class="listitem">Installing R.</li><li class="listitem">Installing protocol buffers.</li><li class="listitem">Setting up environment variables.</li><li class="listitem">Installing rJava.</li><li class="listitem">Installing RHIPE.</li></ol></div><p>Let us begin with the installation.</p><div class="section" title="Installing Hadoop"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec14"/>Installing Hadoop</h3></div></div></div><p>As we are here <a id="id273" class="indexterm"/>to integrate R and Hadoop <a id="id274" class="indexterm"/>with the RHIPE package library, we need to install Hadoop<a id="id275" class="indexterm"/> on our machine. It will be arbitrary that it either be a single node or multinode installation depending on the size of the data to be analyzed.</p><p>As we have already learned how to install Hadoop in Ubuntu, we are not going to repeat the process here. If you haven't installed it yet, please refer to <a class="link" href="ch01.html" title="Chapter 1. Getting Ready to Use R and Hadoop">Chapter 1</a>, <span class="emphasis"><em>Getting Ready to Use R and Hadoop</em></span>, for guidance.</p></div><div class="section" title="Installing R"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec15"/>Installing R</h3></div></div></div><p>If we use a<a id="id276" class="indexterm"/> multinode Hadoop architecture, there<a id="id277" class="indexterm"/> are a number of TaskTracker nodes for executing the <a id="id278" class="indexterm"/>MapReduce job. So, we need to install R over all of these TaskTracker nodes. These TaskTracker nodes will start process over the data subsets <a id="id279" class="indexterm"/>with developed map and reduce logic <a id="id280" class="indexterm"/>with the<a id="id281" class="indexterm"/> consideration of key values.</p></div><div class="section" title="Installing protocol buffers"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec16"/>Installing protocol buffers</h3></div></div></div><p>Protocol <a id="id282" class="indexterm"/>buffers just serialize the <a id="id283" class="indexterm"/>data to make it platform independent, neutral, and robust (primarily used for structured data). Google uses the same protocol for data<a id="id284" class="indexterm"/> interchange. RHIPE depends on protocol buffers 2.4.1 for data serialization over the network.</p><p>This can be installed using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## For downloading the protocol buffer 2.4.1</strong></span>
<span class="strong"><strong>wget http://protobuf.googlecode.com/files/protobuf-2.4.1.tar.gz</strong></span>

<span class="strong"><strong>## To extracting the protocol buffer</strong></span>
<span class="strong"><strong>tar -xzf protobuf-2.4.1.tar.gz</strong></span>

<span class="strong"><strong>## To get in to the extracted protocol buffer directory</strong></span>
<span class="strong"><strong>cd protobuf-2.4.1</strong></span>

<span class="strong"><strong>## For making install the protocol buffer</strong></span>
<span class="strong"><strong>./configure # --prefix=...</strong></span>
<span class="strong"><strong>make</strong></span>
<span class="strong"><strong>make install</strong></span>
</pre></div></div><div class="section" title="Environment variables"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec17"/>Environment variables</h3></div></div></div><p>In order for RHIPE to compile and work correctly, it is better to ensure that the following environment variables are set appropriately:</p><p>For <a id="id285" class="indexterm"/>configuring <a id="id286" class="indexterm"/>the Hadoop libraries, we need to set two variables, <code class="literal">PKG_CONFIG_PATH</code> and <code class="literal">LD_LIBRARY_PATH</code>, to the <code class="literal">~./bashrc</code> file of <code class="literal">hduser</code> (Hadoop user) so that it can automatically be set when the user logs in to the system.</p><p>Here, <code class="literal">PKG_CONFIG_PATH</code> is an environment variable that holds the path of the <code class="literal">pkg-config</code> script for retrieving information about installed libraries in the system, and <code class="literal">LD_LIBRARY_PATH</code> is an environment variable that holds the path of native shared libraries.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export PKG_CONFIG_PATH = /usr/local/lib</strong></span>
<span class="strong"><strong>export LD_LIBRARY_PATH = /usr/local/lib</strong></span>
</pre></div><p>You can also set all these variables from your R console, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Sys.setenv(HADOOP_HOME="/usr/local/hadoop/")</strong></span>
<span class="strong"><strong>Sys.setenv(HADOOP_BIN="/usr/local/hadoop/bin")</strong></span>
<span class="strong"><strong>Sys.setenv(HADOOP_CONF_DIR="/usr/local/hadoop/conf")</strong></span>
</pre></div><p>Where <code class="literal">HADOOP_HOME</code> is used for specifying the location of the Hadoop directory, <code class="literal">HADOOP_BIN</code> is used for specifying the location of binary files of Hadoop, and <code class="literal">HADOOP_CONF_DIR</code> is used for specifying the configuration files of Hadoop.</p><p>Setting the variables is temporary and valid up to a particular R session. If we want to make <a id="id287" class="indexterm"/>this variable <a id="id288" class="indexterm"/>permanent, as initialized automatically when the R session initializes, we need to set these variables to the <code class="literal">/etc/R/Renviron</code> file as we set the environment variable in <code class="literal">.bashrc</code> of a specific user profile.</p></div><div class="section" title="The rJava package installation"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec18"/>The rJava package installation</h3></div></div></div><p>Since RHIPE is a Java package, it acts like a Java bridge between R and Hadoop. RHIPE serializes<a id="id289" class="indexterm"/> the input data to a <a id="id290" class="indexterm"/>Java type, which has to be serialized over the cluster. It needs<a id="id291" class="indexterm"/> a low-level interface to Java, which is provided by rJava. So, we will install rJava to enable the functioning of RHIPE.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## For installing the rJava Package will be used for calling java libraries from R.</strong></span>
<span class="strong"><strong>install.packages("rJava")</strong></span>
</pre></div></div><div class="section" title="Installing RHIPE"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec19"/>Installing RHIPE</h3></div></div></div><p>Now, it's<a id="id292" class="indexterm"/> time<a id="id293" class="indexterm"/> to install the RHIPE package from its repository.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## Downloading RHIPE package from RHIPE repository</strong></span>
<span class="strong"><strong>Wget http://ml.stat.purdue.edu/rhipebin/Rhipe_0.73.1-2.tar.gz</strong></span>

<span class="strong"><strong>## Installing the RHIPE package in R via CMD command</strong></span>
<span class="strong"><strong>R CMD INSTALL Rhipe_0.73.1.tar.gz</strong></span>
</pre></div><p>Now, we <a id="id294" class="indexterm"/>are ready with a RHIPE system for performing data analytics with R <a id="id295" class="indexterm"/>and Hadoop.</p></div></div><div class="section" title="Understanding the architecture of RHIPE"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec34"/>Understanding the architecture of RHIPE</h2></div></div></div><p>Let's understand <a id="id296" class="indexterm"/>the working of the RHIPE library package developed to <a id="id297" class="indexterm"/>integrate R and Hadoop for effective Big Data analytics.</p><div class="mediaobject"><img src="graphics/3282OS_03_02.jpg" alt="Understanding the architecture of RHIPE"/><div class="caption"><p>Components of RHIPE</p></div></div><p>There are a number of Hadoop components that will be used for data analytics operations with R and Hadoop.</p><p>The components of RHIPE are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>RClient</strong></span>: RClient <a id="id298" class="indexterm"/>is an R application that calls the <span class="strong"><strong>JobTracker</strong></span> to execute the job <a id="id299" class="indexterm"/>with an indication of several MapReduce job resources such as Mapper, Reducer, input format, output format, input file, output file, and other several parameters that can handle the MapReduce jobs with RClient.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>JobTracker</strong></span>: A <a id="id300" class="indexterm"/>JobTracker is the master <a id="id301" class="indexterm"/>node of the Hadoop MapReduce operations for initializing and monitoring the MapReduce jobs over the Hadoop cluster.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>TaskTracker</strong></span>: TaskTracker<a id="id302" class="indexterm"/> is a slave <a id="id303" class="indexterm"/>node in the Hadoop cluster. It executes the MapReduce jobs as per the orders given by JobTracker, retrieve the input data chunks, and run R-specific <code class="literal">Mapper</code> and <code class="literal">Reducer</code> over it. Finally, the output will be written on the HDFS directory.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>HDFS</strong></span>: HDFS<a id="id304" class="indexterm"/> is a filesystem distributed over <a id="id305" class="indexterm"/>Hadoop clusters with several data nodes. It provides data services for various data operations.</li></ul></div></div><div class="section" title="Understanding RHIPE samples"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec35"/>Understanding RHIPE samples</h2></div></div></div><p>In this section, we will create two RHIPE MapReduce examples. These two examples are defined with the basic utility of the Hadoop MapReduce job from a RHIPE package.</p><div class="section" title="RHIPE sample program (Map only)"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec20"/>RHIPE sample program (Map only)</h3></div></div></div><p>MapReduce<a id="id306" class="indexterm"/> problem definition: The goal of this MapReduce sample program is to test the RHIPE installation by using the <code class="literal">min</code> and <code class="literal">max</code> functions over numeric data with the Hadoop environment. Since this is a sample program, we have included only the Map phase, which will store its output in the HDFS directory.</p><p>To start the development with RHIPE, we need to initialize the RHIPE subsystem by loading the library and calling the <a id="id307" class="indexterm"/>
<code class="literal">rhinit()</code> method.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## Loading the RHIPE library</strong></span>
<span class="strong"><strong>library(Rhipe)</strong></span>

<span class="strong"><strong>## initializing the RHIPE subsystem, which is used for everything. RHIPE will not work if rhinit is not called.</strong></span>
<span class="strong"><strong>rhinit()</strong></span>
</pre></div><p>Input: We insert a numerical value rather than using a file as an input.</p><p>Map phase: The Map phase of this MapReduce program will call 10 different iterations and in all of those iterations, random numbers from 1 to 10 will be generated as per their iteration<a id="id308" class="indexterm"/> number. After that, the max and min values for that generated numbers will be calculated.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## Defining the Map phase</strong></span>

<span class="strong"><strong>Map(function(k,v){</strong></span>

<span class="strong"><strong>## for generating the random deviates</strong></span>
<span class="strong"><strong>  X  runif(v)</strong></span>

<span class="strong"><strong>## for emitting the key-value pairs with key – k and</strong></span>
<span class="strong"><strong>## value – min and max of generated random deviates.</strong></span>
<span class="strong"><strong>  rhcollect(k, c(Min=min(x),Max=max(x))</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>Output: Finally the output of the Map phase will be considered here as an output of this MapReduce job and it will be stored to HDFS at <code class="literal">/app/hadoop/RHIPE/</code>.</p><p>Defining the MapReduce job by the <a id="id309" class="indexterm"/>
<code class="literal">rhwatch()</code> method of the RHIPE package:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## Create and running a MapReduce job by following</strong></span>
<span class="strong"><strong>job = rhwatch(map=map,input=10,reduce=0,</strong></span>
<span class="strong"><strong>output="/app/Hadoop/RHIPE/test",jobname='test')</strong></span>
</pre></div><p>Reading the MapReduce output from HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## Read the results of job from HDFS</strong></span>
<span class="strong"><strong>result &lt;- rhread(job)</strong></span>
</pre></div><p>For displaying the result in a more readable form in the table format, use the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## Displaying the result</strong></span>
<span class="strong"><strong>outputdata  &lt;- do.call('rbind', lapply(result, "[[", 2))</strong></span>
</pre></div><p>Output:</p><div class="mediaobject"><img src="graphics/3282OS_03_03.jpg" alt="RHIPE sample program (Map only)"/></div></div><div class="section" title="Word count"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec21"/>Word count</h3></div></div></div><p>MapReduce problem definition: This RHIPE MapReduce program is defined for identifying<a id="id310" class="indexterm"/> the frequency of all of the words that are present in the provided input text files.</p><p>Also note that this is the same MapReduce problem as we saw in <a class="link" href="ch02.html" title="Chapter 2. Writing Hadoop MapReduce Programs">Chapter 2</a>, <span class="emphasis"><em>Writing Hadoop MapReduce Programs</em></span>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## Loading the RHIPE Library</strong></span>
<span class="strong"><strong>library(Rhipe)</strong></span>
</pre></div><p>Input: We will use the <code class="literal">CHANGES.txt</code> file, which comes with Hadoop distribution, and use it with this MapReduce algorithm. By using the following command, we will copy it to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>rhput("/usr/local/hadoop/CHANGES.txt","/RHIPE/input/")</strong></span>
</pre></div><p>Map phase: The Map phase contains the code for reading all the words from a file and assigning all of them to value <code class="literal">1</code>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## Defining the Map functionw_map&lt;-expression({ words_vector&lt;-unlist(strsplit(unlist(map.values)," ")) lapply(words_vector,function(i) rhcollect(i,1))</strong></span>
<span class="strong"><strong>})</strong></span>
</pre></div><p>Reduce phase: With this reducer task, we can calculate the total frequency of the words in the <a id="id311" class="indexterm"/>input text files.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## For reference, RHIPE provides a canned version</strong></span>
<span class="strong"><strong>Reduce = rhoptions()$templates$scalarsummer</strong></span>

<span class="strong"><strong>## Defining the Reduce functionw_reduce&lt;-expression(pre={total=0},reduce={total&lt;-sum(total,unlist(reduce.values))},post={rhcollect(reduce.key,total)})</strong></span>
</pre></div><p>Defining the MapReduce job object: After defining the word count mapper and reducer, we need to design the <code class="literal">driver</code> method that can execute this MapReduce job by calling <code class="literal">Mapper</code> and <code class="literal">Reducer</code> sequentially.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## defining and executing a MapReduce job object</strong></span>
<span class="strong"><strong>Job1 &lt;- rhwatch(map=w_map,reduce=w_reduce, ,input="/RHIPE/input/",output="/RHIPE/output/", jobname="word_count")</strong></span>
</pre></div><p>Reading the MapReduce output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## for reading the job output data from HDFS</strong></span>
<span class="strong"><strong>Output_data &lt;- rhread(Job1)results &lt;- data.frame(words=unlist(lapply(Output_data,"[[",1)), count =unlist(lapply(Output_data,"[[",2)))</strong></span>
</pre></div><p>The output of MapReduce job will be stored to <code class="literal">output_data</code>, we will convert this output into R supported dataframe format. The dataframe output will be stored to the <code class="literal">results</code> variable. For displaying the MapReduce output in the data frame the format will be as follows:</p><p>Output for <code class="literal">head (results)</code>:</p><div class="mediaobject"><img src="graphics/3282OS_03_04.jpg" alt="Word count"/></div><p>Output for <code class="literal">tail (results)</code>:</p><div class="mediaobject"><img src="graphics/3282OS_03_05.jpg" alt="Word count"/></div></div></div><div class="section" title="Understanding the RHIPE function reference"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec36"/>Understanding the RHIPE function reference</h2></div></div></div><p>RHIPE is specially designed for providing a lower-level interface over Hadoop. So R users with a RHIPE package can easily fire the Hadoop data operations over large datasets that are stored on HDFS, just like the <a id="id312" class="indexterm"/>
<code class="literal">print()</code> function called in R.</p><p>Now we will see all the possible functional uses of all methods that are available in RHIPE library. All these <a id="id313" class="indexterm"/>methods are with three categories: Initialization, <a id="id314" class="indexterm"/>HDFS, and MapReduce <a id="id315" class="indexterm"/>operations.</p><div class="section" title="Initialization"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec22"/>Initialization</h3></div></div></div><p>We use the<a id="id316" class="indexterm"/> following command for initialization:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">rhinit</code>: This<a id="id317" class="indexterm"/> is used to initialize the Rhipe <a id="id318" class="indexterm"/>subsystem.<p><code class="literal">rhinit(TRUE,TRUE)</code></p></li></ul></div></div><div class="section" title="HDFS"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec23"/>HDFS</h3></div></div></div><p>We use<a id="id319" class="indexterm"/> the following<a id="id320" class="indexterm"/> command for HDFS operations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">rhls</code>: This <a id="id321" class="indexterm"/>is used to retrieve all directories from HDFS.<p>Its syntax is <code class="literal">rhls(path)</code></p><p><code class="literal">rhls("/")</code></p><p>Output:</p><div class="mediaobject"><img src="graphics/3282OS_03_06.jpg" alt="HDFS"/></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.getwd</code>: This <a id="id322" class="indexterm"/>is used for acquiring the current <a id="id323" class="indexterm"/>working HDFS directory. Its syntax is <code class="literal">hdfs.getwd()</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.setwd</code>: This<a id="id324" class="indexterm"/> is used for setting up the current<a id="id325" class="indexterm"/> working HDFS directory. Its syntax is <code class="literal">hdfs.setwd("/RHIPE")</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">rhput</code>: This is <a id="id326" class="indexterm"/>used to copy a file from a local <a id="id327" class="indexterm"/>directory to HDFS. Its syntax is <code class="literal">rhput(src,dest)</code> and <code class="literal">rhput("/usr/local/hadoop/NOTICE.txt","/RHIPE/")</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhcp</code>: This is <a id="id328" class="indexterm"/>used to copy a file from one HDFS <a id="id329" class="indexterm"/>location to another HDFS location. Its syntax is <code class="literal">rhcp('/RHIPE/1/change.txt','/RHIPE/2/change.txt')</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhdel</code>: This is<a id="id330" class="indexterm"/> used to delete a directory/file from<a id="id331" class="indexterm"/> HDFS. Its syntax is <code class="literal">rhdel("/RHIPE/1")</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhget</code>: This<a id="id332" class="indexterm"/> is used to copy the HDFS file to a local <a id="id333" class="indexterm"/>directory. Its syntax is <code class="literal">rhget("/RHIPE/1/part-r-00000", "/usr/local/")</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rwrite</code>: This<a id="id334" class="indexterm"/> is used to write the R data to HDFS. its<a id="id335" class="indexterm"/> syntax is <code class="literal">rhwrite(list(1,2,3),"/tmp/x")</code>.</li></ul></div></div><div class="section" title="MapReduce"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec24"/>MapReduce</h3></div></div></div><p>We use the following commands for MapReduce operations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">rhwatch</code>: This<a id="id336" class="indexterm"/> is used to prepare, submit, and <a id="id337" class="indexterm"/>monitor MapReduce jobs.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># Syntax:</strong></span>
<span class="strong"><strong>rhwatch(map, reduce, combiner, input, output, mapred,partitioner,mapred, jobname)</strong></span>

<span class="strong"><strong>## to prepare and submit MapReduce job:</strong></span>

<span class="strong"><strong>z=rhwatch(map=map,reduce=0,input=5000,output="/tmp/sort",mapred=mapred,read=FALSE)</strong></span>

<span class="strong"><strong>results &lt;- rhread(z)</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">rhex</code>: This is<a id="id338" class="indexterm"/> used to execute the MapReduce <a id="id339" class="indexterm"/>job from over Hadoop cluster.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## Submit the job</strong></span>
<span class="strong"><strong>rhex(job)</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">rhjoin</code>: This is <a id="id340" class="indexterm"/>used to check whether the <a id="id341" class="indexterm"/>MapReduce job is completed or not. Its syntax is <code class="literal">rhjoin(job)</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhkill</code>: This<a id="id342" class="indexterm"/> is used to kill the running MapReduce job. Its <a id="id343" class="indexterm"/>syntax is <code class="literal">rhkill(job)</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhoptions</code>: This<a id="id344" class="indexterm"/> is used for getting or<a id="id345" class="indexterm"/> setting the RHIPE configuration options. Its syntax is <code class="literal">rhoptions()</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhstatus</code>: This <a id="id346" class="indexterm"/>is used to get the status <a id="id347" class="indexterm"/>of the RHIPE MapReduce job. Its syntax is <code class="literal">rhstatus(job)</code>.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>rhstatus(job, mon.sec = 5, autokill = TRUE,showErrors = TRUE, verbose = FALSE, handler = NULL)</strong></span>
</pre></div></li></ul></div></div></div></div></div>
<div class="section" title="Introducing RHadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec30"/>Introducing RHadoop</h1></div></div></div><p>RHadoop is a collection of three R packages for providing large data operations with an R environment. It was developed by Revolution Analytics, which is the leading commercial<a id="id348" class="indexterm"/> provider of software based on R. RHadoop is available with three main R packages: <code class="literal">rhdfs</code>, <code class="literal">rmr</code>, and <code class="literal">rhbase</code>. Each of them offers different Hadoop features.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">rhdfs</code> is an <a id="id349" class="indexterm"/>R interface for providing the HDFS usability from the R console. As Hadoop MapReduce programs write their output on <a id="id350" class="indexterm"/>HDFS, it is very easy to access them by calling the <code class="literal">rhdfs</code> methods. The R programmer can easily perform read and write operations on distributed data files. Basically, <code class="literal">rhdfs</code> package calls the HDFS API in backend to operate data sources stored on HDFS.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rmr</code> is an R <a id="id351" class="indexterm"/>interface for providing Hadoop MapReduce facility inside the R environment. So, the R programmer needs to just<a id="id352" class="indexterm"/> divide their application logic into the map and reduce phases and submit it with the <code class="literal">rmr</code> methods. After that, <code class="literal">rmr</code> calls the Hadoop streaming MapReduce API with several job parameters as input directory, output directory, mapper, reducer, and so on, to perform the R MapReduce job over Hadoop cluster.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhbase</code> is an <a id="id353" class="indexterm"/>R interface for operating the Hadoop HBase data source stored at the distributed network via a Thrift server. The <code class="literal">rhbase</code> package<a id="id354" class="indexterm"/> is designed with several methods for initialization and read/write and table manipulation operations.</li></ul></div><p>Here it's not necessary to install all of the three RHadoop packages to run the Hadoop MapReduce operations with R and Hadoop. If we have stored our input data source at the HBase data source, we need to install <code class="literal">rhbase</code>; else we require <code class="literal">rhdfs</code> and <code class="literal">rmr</code> packages. As Hadoop is most popular for its two main features, Hadoop MapReduce and HDFS, both of these features will be used within the R console with the help of RHadoop <code class="literal">rhdfs</code> and <code class="literal">rmr</code> packages. These packages are enough to run Hadoop MapReduce from R. Basically, <code class="literal">rhdfs</code> provides HDFS data operations while <code class="literal">rmr</code> provides MapReduce execution operations.</p><p>RHadoop also includes<a id="id355" class="indexterm"/> another package called <code class="literal">quick check</code>, which is <a id="id356" class="indexterm"/>designed for debugging the developed MapReduce job defined by the <code class="literal">rmr</code> package.</p><p>In the next section, we will see their architectural relationships as well as their installation steps.</p><div class="section" title="Understanding the architecture of RHadoop"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec37"/>Understanding the architecture of RHadoop</h2></div></div></div><p>Since<a id="id357" class="indexterm"/> Hadoop is highly popular because of HDFS and MapReduce, Revolution <a id="id358" class="indexterm"/>Analytics has developed separate R packages, namely, <code class="literal">rhdfs</code>, <code class="literal">rmr</code>, and <code class="literal">rhbase</code>. The architecture of RHadoop is shown in the following diagram:</p><div class="mediaobject"><img src="graphics/3282OS_03_07.jpg" alt="Understanding the architecture of RHadoop"/><div class="caption"><p>RHadoop Ecosystem</p></div></div></div><div class="section" title="Installing RHadoop"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec38"/>Installing RHadoop</h2></div></div></div><p>In this section, we <a id="id359" class="indexterm"/>will learn some installation tricks for the three<a id="id360" class="indexterm"/> RHadoop packages including their prerequisites.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>R and Hadoop installation</strong></span>: As we are going to use an R and Hadoop integrated<a id="id361" class="indexterm"/> environment, we <a id="id362" class="indexterm"/>need Hadoop as well as R installed on our machine. If you haven't installed yet, see <a class="link" href="ch01.html" title="Chapter 1. Getting Ready to Use R and Hadoop">Chapter 1</a>, <span class="emphasis"><em>Getting Ready to Use R and Hadoop</em></span>. As we know, if we<a id="id363" class="indexterm"/> have too much data, we need to scale our <a id="id364" class="indexterm"/>cluster by increasing the number of nodes. Based on this, to get RHadoop installed on our system we need Hadoop with either a<a id="id365" class="indexterm"/> single node <a id="id366" class="indexterm"/>or multimode<a id="id367" class="indexterm"/> installation <a id="id368" class="indexterm"/>as per the size of our data.<p>RHadoop is already tested with several Hadoop distributions provided by Cloudera, Hortonworks, and MapR.</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Installing the R packages</strong></span>: We <a id="id369" class="indexterm"/>nee<a id="id370" class="indexterm"/>d several R packages to be installed that help it <a id="id371" class="indexterm"/>to connect R with Hadoop<a id="id372" class="indexterm"/>. The list of the packages is as follows:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">rJava</li><li class="listitem" style="list-style-type: disc">RJSONIO</li><li class="listitem" style="list-style-type: disc">itertools</li><li class="listitem" style="list-style-type: disc">digest</li><li class="listitem" style="list-style-type: disc">Rcpp</li><li class="listitem" style="list-style-type: disc">httr</li><li class="listitem" style="list-style-type: disc">functional</li><li class="listitem" style="list-style-type: disc">devtools</li><li class="listitem" style="list-style-type: disc">plyr</li><li class="listitem" style="list-style-type: disc">reshape2</li></ul></div><p>We can install them by calling the execution of the following R command in the R console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>install.packages( c('rJava','RJSONIO', 'itertools', 'digest','Rcpp','httr','functional','devtools', 'plyr','reshape2'))</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Setting environment variables</strong></span>: We<a id="id373" class="indexterm"/> can set this via the R console using<a id="id374" class="indexterm"/> the following code:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## Setting HADOOP_CMD</strong></span>
<span class="strong"><strong>Sys.setenv(HADOOP_CMD="/usr/local/hadoop/bin/hadoop")</strong></span>

<span class="strong"><strong>## Setting up HADOOP_STREAMING</strong></span>
<span class="strong"><strong>Sys.setenv(HADOOP_STREAMING="/usr/local/hadoop/contrib/streaming/hadoop-streaming-1.0.3.jar")</strong></span>
</pre></div><p>or, we can also set the R console via the command line as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export HADOOP_CMD=/usr/local/Hadoop</strong></span>
<span class="strong"><strong>export HADOOP_STREAMING=/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.1.1.jar</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc">Installing<a id="id375" class="indexterm"/> RHadoop [<code class="literal">rhdfs</code>, <code class="literal">rmr</code>, <code class="literal">rhbase</code>]<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download <a id="id376" class="indexterm"/>RHadoop packages <a id="id377" class="indexterm"/>from GitHub repository of Revolution Analytics: <a class="ulink" href="https://github.com/RevolutionAnalytics/RHadoop">https://github.com/RevolutionAnalytics/RHadoop</a>.<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">rmr</code>: [<code class="literal">rmr-2.2.2.tar.gz</code>]</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhdfs</code>: [<code class="literal">rhdfs-1.6.0.tar.gz</code>]</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhbase</code>: [<code class="literal">rhbase-1.2.0.tar.gz</code>]</li></ul></div></li><li class="listitem">Installing packages.<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For rmr we use:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>R CMD INSTALL rmr-2.2.2.tar.gz</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc">For <code class="literal">rhdfs</code> we use:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>R CMD INSTALL rmr-2.2.2.tar.gz</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc">For <code class="literal">rhbase</code> we use:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>R CMD INSTALL rhbase-1.2.0.tar.gz</strong></span>
</pre></div></li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip14"/>Tip</h3><p>To install rhbase, we<a id="id378" class="indexterm"/> need to have HBase and Zookeeper installed on our Hadoop cluster.</p></div></div></li></ol></div></li></ul></div><div class="section" title="Understanding RHadoop examples"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec39"/>Understanding RHadoop examples</h3></div></div></div><p>Once we complete<a id="id379" class="indexterm"/> the installation of RHadoop, we can test the setup by running the MapReduce job with the <code class="literal">rmr2</code> and <code class="literal">rhdfs</code> libraries in the RHadoop sample program as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>## loading the libraries</strong></span>
<span class="strong"><strong>library(rhdfs')</strong></span>
<span class="strong"><strong>library('rmr2')</strong></span>

<span class="strong"><strong>## initializing the RHadoop</strong></span>
<span class="strong"><strong>hdfs.init()</strong></span>

<span class="strong"><strong># defining the input data</strong></span>
<span class="strong"><strong>small.ints = to.dfs(1:10)</strong></span>

<span class="strong"><strong>## Defining the MapReduce job</strong></span>
<span class="strong"><strong>mapreduce(</strong></span>
<span class="strong"><strong># defining input parameters as small.ints hdfs object, map parameter as function to calculate the min and max for generated random deviates.
  input = small.ints, 
  map = function(k, v)
  {
lapply(seq_along(v), function(r){
  
  x &lt;- runif(v[[r]])
    keyval(r,c(max(x),min(x))) 
  })})
</strong></span>
</pre></div><p>After running<a id="id380" class="indexterm"/> these lines, simply pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>Enter</em></span> will execute this MapReduce program. If it succeeds, the last line will appear as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/3282OS_03_08.jpg" alt="Understanding RHadoop examples"/></div><p>Where characters of that last line indicate the output location of the MapReduce job.</p><p>To read the <a id="id381" class="indexterm"/>result of the executed MapReduce job, copy the output location, as provided in the last line, and pass it to the <code class="literal">from.dfs()</code> function of <code class="literal">rhdfs</code>.</p><div class="mediaobject"><img src="graphics/3282OS_03_09.jpg" alt="Understanding RHadoop examples"/></div><p>Where the first column of the previous output indicates the max value, and the second one the min value.</p><div class="section" title="Word count"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec25"/>Word count</h4></div></div></div><p>MapReduce <a id="id382" class="indexterm"/>problem definition: This RHadoop MapReduce program is defined for identifying the frequency of all the words that are present in the provided input text files.</p><p>Also, note that this is the same MapReduce problem as we learned in the previous section about RHIPE in <a class="link" href="ch02.html" title="Chapter 2. Writing Hadoop MapReduce Programs">Chapter 2</a>, <span class="emphasis"><em>Writing Hadoop MapReduce Programs</em></span>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wordcount = function(input, output = NULL, pattern = " "){</strong></span>
</pre></div><p>Map phase: This <code class="literal">map</code> function will read the text file line by line and split them by spaces. This map phase will assign <code class="literal">1</code> as a value to all the words that are caught by the mapper.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wc.map = function(., lines) {</strong></span>
<span class="strong"><strong>  keyval(</strong></span>
<span class="strong"><strong>  unlist(</strong></span>
<span class="strong"><strong>  strsplit(</strong></span>
<span class="strong"><strong>  x = lines,</strong></span>
<span class="strong"><strong>  split = pattern)),</strong></span>
<span class="strong"><strong>  1)}</strong></span>
</pre></div><p>Reduce phase: Reduce phase will calculate the total frequency of all the words by performing<a id="id383" class="indexterm"/> sum operations over words with the same keys.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wc.reduce = function(word, counts ) {</strong></span>
<span class="strong"><strong>  keyval(word, sum(counts))}</strong></span>
</pre></div><p>Defining the MapReduce job: After defining the word count mapper and reducer, we need to create the <code class="literal">driver</code> method that starts the execution of MapReduce.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># To execute the defined Mapper and Reducer functions</strong></span>
<span class="strong"><strong># by specifying the input, output, map, reduce and input.format as parameters.</strong></span>

<span class="strong"><strong># Syntax:</strong></span>
<span class="strong"><strong># mapreduce(input, output, input.format, map,reduce,</strong></span>
<span class="strong"><strong># combine)</strong></span>

<span class="strong"><strong>mapreduce(input = input ,</strong></span>
<span class="strong"><strong>  output = output,</strong></span>
<span class="strong"><strong>  input.format = "text",</strong></span>
<span class="strong"><strong>  map = wc.map,</strong></span>
<span class="strong"><strong>  reduce = wc.reduce,</strong></span>
<span class="strong"><strong>  combine = T)}</strong></span>
</pre></div><p>Executing the MapReduce job: We will execute the RHadoop MapReduce job by passing the input data location as a parameter for the <code class="literal">wordcount</code> function.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wordcount('/RHadoop/1/')</strong></span>
</pre></div><p>Exploring the <code class="literal">wordcount</code> output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>from.dfs("/tmp/RtmpRMIXzb/file2bda5e10e25f")</strong></span>
</pre></div></div></div><div class="section" title="Understanding the RHadoop function reference"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec40"/>Understanding the RHadoop function reference</h3></div></div></div><p>RHadoop has <a id="id384" class="indexterm"/>three different packages, which are in<a id="id385" class="indexterm"/> terms of HDFS, MapReduce, and HBase operations, to perform operations over the data.</p><p>Here we will see how to use the <code class="literal">rmr</code> and <code class="literal">rhdfs</code> package functions:</p><div class="section" title="The hdfs package"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec26"/>The hdfs package</h4></div></div></div><p>The <a id="id386" class="indexterm"/>categorized functions are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Initialization<a id="id387" class="indexterm"/><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.init</code>: This<a id="id388" class="indexterm"/> is used to initialize the <code class="literal">rhdfs</code> package. Its syntax is <code class="literal">hdfs.init()</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.defaults</code>: This is used to retrieve and set the <code class="literal">rhdfs</code> defaults. Its <a id="id389" class="indexterm"/>syntax is <code class="literal">hdfs.defaults()</code>.</li></ul></div><p>To retrieve the <code class="literal">hdfs</code> configuration defaults, refer to the following screenshot:</p><div class="mediaobject"><img src="graphics/3282OS_03_10.jpg" alt="The hdfs package"/></div></li><li class="listitem" style="list-style-type: disc">File manipulation<a id="id390" class="indexterm"/><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.put</code>: This<a id="id391" class="indexterm"/> is used to copy files from <a id="id392" class="indexterm"/>the local filesystem to the HDFS filesystem.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.put('/usr/local/hadoop/README.txt','/RHadoop/1/')</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.copy</code>: This <a id="id393" class="indexterm"/>is used to copy files from the HDFS directory to the local filesystem.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.put('/RHadoop/1/','/RHadoop/2/')</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.move</code>: This <a id="id394" class="indexterm"/>is used to move a file from one HDFS directory to another HDFS directory.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.move('/RHadoop/1/README.txt','/RHadoop/2/')</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.rename</code>: This <a id="id395" class="indexterm"/>is used to rename the file stored at HDFS from R.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.rename('/RHadoop/README.txt','/RHadoop/README1.txt')</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.delete</code>: This <a id="id396" class="indexterm"/>is used to delete the HDFS file or directory from R.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.delete("/RHadoop")</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.rm</code>: This <a id="id397" class="indexterm"/>is used to delete the HDFS file or directory from R.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.rm("/RHadoop")</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.chmod</code>: This <a id="id398" class="indexterm"/>is used to change permissions of some files.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.chmod('/RHadoop', permissions= '777')</strong></span>
</pre></div></li></ul></div></li></ul></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">File read/write:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.file</code>: This<a id="id399" class="indexterm"/> is used to initialize the file to be<a id="id400" class="indexterm"/> used for read/write <a id="id401" class="indexterm"/>operation.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>f = hdfs.file("/RHadoop/2/README.txt","r",buffersize=104857600)</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.write</code>: This <a id="id402" class="indexterm"/>is used to write in to the file stored at HDFS via streaming.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>f = hdfs.file("/RHadoop/2/README.txt","r",buffersize=104857600)</strong></span>
<span class="strong"><strong>hdfs.write(object,con,hsync=FALSE)</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.close</code>: This is<a id="id403" class="indexterm"/> used to close the stream when a file operation is complete. It will close the stream and will not allow further file operations.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.close(f)</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.read</code>: This<a id="id404" class="indexterm"/> is used to read from binary files on the HDFS directory. This will use the stream for the deserialization of the data.<p>f = hdfs.file("/RHadoop/2/README.txt","r",buffersize=104857600)</p><p>m = hdfs.read(f)</p><p>c = rawToChar(m)</p><p>print(c)</p></li></ul></div></li><li class="listitem" style="list-style-type: disc">Directory operation<a id="id405" class="indexterm"/>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.dircreate</code> or <code class="literal">hdfs.mkdir</code><a id="id406" class="indexterm"/>: Both these functions will be used for<a id="id407" class="indexterm"/> creating a directory over the HDFS<a id="id408" class="indexterm"/> filesystem.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.mkdir("/RHadoop/2/")</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.rm</code><a id="id409" class="indexterm"/> or  <code class="literal">hdfs.rmr</code> or <code class="literal">hdfs.delete</code> - to delete the directory or file from HDFS.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.rm("/RHadoop/2/")</strong></span>
</pre></div></li></ul></div></li><li class="listitem" style="list-style-type: disc">Utility<a id="id410" class="indexterm"/>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.ls</code>: This<a id="id411" class="indexterm"/> is <a id="id412" class="indexterm"/>used to list the directory from HDFS.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Hdfs.ls('/')</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/3282OS_03_11.jpg" alt="The hdfs package"/></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs.file.info</code>: This is used to get meta information about the file <a id="id413" class="indexterm"/>stored at HDFS.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs.file.info("/RHadoop")</strong></span>
</pre></div></li></ul></div><div class="mediaobject"><img src="graphics/3282OS_03_12.jpg" alt="The hdfs package"/></div></li></ul></div></div><div class="section" title="The rmr package"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec27"/>The rmr package</h4></div></div></div><p>The <a id="id414" class="indexterm"/>categories of the functions are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For storing and retrieving data<a id="id415" class="indexterm"/>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">to.dfs</code>: This<a id="id416" class="indexterm"/> is used to write R objects from or to the filesystem.<p>small.ints = to.dfs(1:10)</p></li><li class="listitem" style="list-style-type: disc"><code class="literal">from.dfs</code>: This <a id="id417" class="indexterm"/>is used to read the R objects from the HDFS filesystem that are in the binary encrypted format.<p>from.dfs('/tmp/RtmpRMIXzb/file2bda3fa07850')</p></li></ul></div></li><li class="listitem" style="list-style-type: disc">For MapReduce<a id="id418" class="indexterm"/>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">mapreduce</code>: This<a id="id419" class="indexterm"/> is used for defining and executing the MapReduce job.<p>mapreduce(input, output, map, reduce, combine, input.fromat, output.format, verbose)</p></li><li class="listitem" style="list-style-type: disc"><code class="literal">keyval</code>: This <a id="id420" class="indexterm"/>is used to create and extract key-value pairs.<p>keyval(key, val)</p></li></ul></div></li></ul></div></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec31"/>Summary</h1></div></div></div><p>Since RHadoop is considered as matured, we will consider it while performing data analytics in further chapters. In <a class="link" href="ch05.html" title="Chapter 5. Learning Data Analytics with R and Hadoop">Chapter 5</a>, <span class="emphasis"><em>Learning Data Analytics with R and Hadoop</em></span> and <a class="link" href="ch06.html" title="Chapter 6. Understanding Big Data Analysis with Machine Learning">Chapter 6</a>, <span class="emphasis"><em>Understanding Big Data Analysis with Machine Learning</em></span>, we will dive into some Big Data analytics techniques as well as see how real world problems can be solved with RHadoop. So far we have learned how to write the MapReduce program with R and Hadoop using RHIPE and RHadoop. In the next chapter, we will see how to write the Hadoop MapReduce program with Hadoop streaming utility and also with Hadoop streaming R packages.</p></div></body></html>