- en: Regularization for Database Improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the idea of statistical regularization to
    improve data models in an effort to help comprehend what statistical regularization
    is, why it is important as well as to feel comfortable with the various statistical
    regularization methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve organized information into the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Statistical regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using data to understand statistical regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving data or a data model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using R for statistical regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, what is statistical regularization?
  prefs: []
  type: TYPE_NORMAL
- en: With regularization, whether we are speaking about mathematics, statistics,
    or machine learning, we are essentially talking about a process of adding additional
    information in order to solve a problem.
  prefs: []
  type: TYPE_NORMAL
- en: The term **regularization** has been described as an abstract concept of management of complex
    systems(according to a set of rules or accepted concepts). These rules will define
    how one can add or modify values in order to satisfy a requirement or solve a
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Does adding or modifying values mean changing data? (More about this will be
    studied later in this chapter.)
  prefs: []
  type: TYPE_NORMAL
- en: Various statistical regularization methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Within the statistical community, the most popular statistical regularization
    methods may include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Ridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lasso
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least angles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ridge regression is a statistical technique that is used when analyzing regression
    data or models that suffer from a condition known as **multicollinearity**. When
    multicollinearity occurs, estimates may be unbiased but their variances are usually
    large and far from the true value. This technique adds a degree of bias to the
    regression estimates to reduce standard errors (to produce estimates that are
    more dependable).
  prefs: []
  type: TYPE_NORMAL
- en: Multicollinearity is a condition within statistics in which a predictor (variable)
    in multiple regression models can be linearly predicted from the others with a
    significant accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Least absolute shrinkage and selection operator** (**Lasso**) is a statistical
    technique that performs both variable selection and regularization in an effort
    to enhance prediction accuracies within a model.'
  prefs: []
  type: TYPE_NORMAL
- en: The process of choosing or selecting variables within a statistical model results,
    obviously, in reducing the number of variables, which is also referred to as variable
    shrinkage.
  prefs: []
  type: TYPE_NORMAL
- en: Least angles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Least Angle Regression** (**LARS**) is a statistical technique used by data
    scientists when dealing with high-dimensional data. If there is speculation that
    a response variable is determined by a particular subset of predictors, then the
    LARS technique can help with determining which variables to include in the regression
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: Opportunities for regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, when would you, a data scientist, consider using any type of regularization
    method?
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, the truth is that there is no absolute rule that dictates the use of
    regularization; however, there are certain indicators to observe that should cause
    you to consider regularization, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: If your data contains a high variable count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is a low ratio of the number of observations to the number of variables
    in your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 6](8a0b3272-dfa0-46ca-9e90-f6050f2007cd.xhtml), *Database Progression
    to Database Regression* (on statistical regression), we reviewed some sample data
    consisting of consulting project results. In that example, we explored the relationship
    between the total hours billed to the project, the total project management hours
    spent on the project, and the project's supposed profitability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking closer at that same data, perhaps we may now see additional variables,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of consultants assigned to the project full time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of consultants assigned to the project part-time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of sub-contractors assigned to the project (full time or part time)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of customer resources assigned to the project full time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of customer resources assigned to the project part-time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of local resources assigned to the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Years of experience with the projects core technology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total project management hours
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total development hours
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hourly bill rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total hours invoiced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of technologies used in the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project style (time and materials, not to exceed, or staff augment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we can see more than twelve possible independent or predictor variables—certainly
    a manageable number—especially given that the number of observations (records)
    in the file is over 100 (the ratio of variables to observations is about 12%).
  prefs: []
  type: TYPE_NORMAL
- en: An independent variable (or experimental or predictor variable) is a variable
    that is being manipulated in a model to observe the effect on a dependent variable,
    or an outcome variable.
  prefs: []
  type: TYPE_NORMAL
- en: When a data scientist speaks of high variable counts, they are really referring
    to an excessive number or, if the number of variables is approaching the number
    of observations, (not so in this example) but suppose we had more than 50 possible
    predictor variables in our data of only 100 observations? This is what can be
    referred to as an overly complex model and warrants consideration of using a common
    regulation method.
  prefs: []
  type: TYPE_NORMAL
- en: What constitutes as overly complex is often a subject for debate and often differs
    based on the data and objectives of the statistical model.
  prefs: []
  type: TYPE_NORMAL
- en: Experience shows us that when a model is excessively complex, a model may fit
    but have a poor predicting performance (which is ultimately the goal). When this
    occurs, a data scientist will recognize overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization is the statistical technique used by data scientists to avoid
    or address this overfitting problem. The idea behind regularization is that models
    that overfit the data are complex statistical models that have, for example, too
    many parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other known opportunities for the use of regulation include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Instances involving high collinearity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a project objective is a sparse solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accounting for variables grouping in high-dimensional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collinearity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **collinearity** describes a statistical situation when a selected
    predictor variable can be linearly predicted from the others with a considerable
    degree of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Linear prediction is a procedure where future values of a variable are estimated
    based on a linear function of previous samples.
  prefs: []
  type: TYPE_NORMAL
- en: This typically allows very small changes to the data to produce unreliable results
    regarding individual predictor variables. That is, multiple regression models
    with collinear predictors can indicate how well the entire bundle of predictors
    predicts the outcome variable, but it may not give valid results about any individual
    predictor, or about which predictors are redundant with respect to others.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **sparse solution** or **approximation** is a sparse vector that approximately
    solves an arrangement of equations. Techniques to find sparse approximations have
    found a wide use in applications such as image processing and document analysis.
  prefs: []
  type: TYPE_NORMAL
- en: You should recall that a vector is a sequence of data points of the same basic
    type. Members of a vector are officially called **components**.
  prefs: []
  type: TYPE_NORMAL
- en: High-dimensional data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**High-dimensional statistics** is the study of data where the number of dimensions
    is higher than the dimensions considered in the classical **multivariate analysis **(**MVA**).'
  prefs: []
  type: TYPE_NORMAL
- en: In statistical studies, a **multivariate random variable** (or **random vector**)
    is a list of variables, each of whose value is unknown. MVA is defined as the
    study of this occasion.
  prefs: []
  type: TYPE_NORMAL
- en: High-dimensional statistics relies on the theory of random vectors. In many
    applications, the dimension of the data vectors may be larger than the sample
    size.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Classification** is the process of identifying to which of a set of categories
    or groups a new observation belongs, on the basis of a training set of data containing
    observations (or instances) whose category membership is known.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization is a common statistical technique used to address the mentioned
    (as well as other) scenarios. In the next section, we'll look at some simple examples
    of each of these.
  prefs: []
  type: TYPE_NORMAL
- en: Using data to understand statistical regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Variable selection is an imperative process within the field of statistics as
    it tries to make models simpler to understand, easier to train, and free of misassociations--by
    eliminating variables unrelated to the output.
  prefs: []
  type: TYPE_NORMAL
- en: This (variable selection) is one possible approach to dealing with the problem
    of overfitting. In general, we don't expect a model to completely fit our data;
    in fact, the problem of overfitting often means that it may be disadvantageous
    to our predictive model's accuracy on unseen data if we fit our training or test
    data too well.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than using variable selection, the process of regularization is an alternative
    approach to reducing the number of variables in the data in order to deal with
    the issue of overfitting and is essentially a process of introducing an intentional
    bias or constraint in a training of a model that (hopefully) prevents our coefficients
    from exhibiting very high variances.
  prefs: []
  type: TYPE_NORMAL
- en: When the number of parameters (in a population) is deemed very large—particularly
    compared to the number of available observations—linear regression tends to allow
    small changes in a few of the observations to cause the coefficients to change
    drastically (or, as we already put it, exhibit very high variances).
  prefs: []
  type: TYPE_NORMAL
- en: '**Ridge regression** is a statistical method that introduces a controlled bias
    (through or using a constraint) to the model''s regression estimates but is effective
    at reducing the model''s variance as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression is sometimes referred to within the data scientist community
    as a penalized regression technique.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of different R functions and packages that implement ridge
    regression, such as `lm.ridge()` from the `MASS` package and `ridge()` from the
    `genridge` package.
  prefs: []
  type: TYPE_NORMAL
- en: You might be familiar with the `MASS` R package but perhaps not `genridge`.
    The `genridge` package introduces generalizations of the standard univariate ridge
    trace plot used in ridge regression and related methods and is worthy of additional
    investigation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 6](8a0b3272-dfa0-46ca-9e90-f6050f2007cd.xhtml), *Database Progression
    to Database Regression* we proposed an example where we created a linear regression
    model on data from a consulting company''s project results in an effort to predict
    a project''s profitability. We used the R function: `lm()`, which takes in two
    main arguments: `formula` (an object of class formula) and `data` (typically a
    `data.frame`), as shown in the following R code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this chapter, we are going to work with the `lm.ridge()` function in an attempt
    to acceptably fit the preceding linear model using ridge regression. The preceding
    code generated a linear model using our R object named `MyData`, using the `ProjectManagment`
    variable to predict `Profit`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `lm.ridge` function uses the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The arguments are included here for later reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '`formula`: This a formula expression as for regression models, of the form
    `response ~ predictors`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data`: This is an optional data frame in which to interpret the variables
    occurring in the formula'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset`: This is an expression saying which subset of the rows of the data
    should be used in the fit. All observations are included by default'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`na.action`: This a function to filter missing data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lambda`: This is a scalar or vector of ridge constants'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model`: Should the model frame be returned?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x`: Should the design matrix be returned?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y`: Should the response be returned?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`contrasts`: A list of contrasts to be used for some or all of the factor terms
    in the formula'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term `lambda` (here, a parameter in the `lm.ridge` function) is typically
    defined as a comparison of a group means on a combination of dependent variables.
  prefs: []
  type: TYPE_NORMAL
- en: To set up our next example, let's recall that our project data had a ratio of
    variables to observations of 12%. Suppose we've been provided with a new data
    file, one which has only 50 observations. Now our ratio of variables to observations
    goes up to 24%.
  prefs: []
  type: TYPE_NORMAL
- en: What about a file with only 12 observations? Further, suppose we are told that
    management believes that these 12 observations are based upon the key, high-visibility
    projects and therefore are unwilling to provide a bigger population to the data
    scientist (at least at this time)? Is it even worthwhile to model this data? Would
    the results be valuable in any way?
  prefs: []
  type: TYPE_NORMAL
- en: In general terms, it is said that the more the variables present in a regression
    model, the more flexible a model is considered to be, or that it will become.
    It is very likely that a model of this type will be able to achieve a low error
    by fitting random fluctuations in the training data but the outcome or results
    won't represent the true, underlying distribution of the variables within the
    data and in other words, performance will, therefore, be poor when the model is
    run on future data drawn from the same distribution. (Management would not be
    happy if our predictions for project profitability were based upon flawed logic!)
  prefs: []
  type: TYPE_NORMAL
- en: Given the preceding scenario, how should a data scientist proceed? Well, it
    is certainly possible to fit good models when there are more variables than data
    points, but it must be done very carefully.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule, when the data contains more variables than observations, the results
    may seemingly yield acceptable performance, but as we already mentioned, the solution
    may achieve favorable results or even zero error on the training data. Such a
    model would certainly overfit on actual data because it's too flexible for the
    amount of training data. (This condition is called **ill-posed** or **underdetermined**.)
  prefs: []
  type: TYPE_NORMAL
- en: This problem is most often addressed by carefully setting limitations or imposing
    constraints on the parameters, either explicitly or via a logical process. The
    model then becomes a trade-off between fitting the data well and satisfying these
    set limits or constraints. Ridge regression constraints or penalizes data parameters
    and can yield better predictive performance by limiting the model's flexibility,
    thereby reducing the tendency to overfit.
  prefs: []
  type: TYPE_NORMAL
- en: However, simply setting limits or imposing constraints doesn't imply that the
    resulting solution will be good or acceptable. Constraints will only produce good
    solutions when they're actually suited to the problem or objective at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get back to the `lm.ridge` function we mentioned earlier in the section.
    A little different from the use of the `lm` function, we can see the difference
    in the following use case examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical to most examples you''ll find, we can utilize the `runif` and `rnom`
    R functions to generate some random number datasets (to be used for illustration),
    we can see the difference between executing `lm` and `lm.ridge`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As we know what we want to explore (for example, estimating the parameters
    in a linear regression model), we can take liberties with creating the testing
    data. The following is an example of R code that generates a linear regression
    model using our three made-up variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the generated output from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14a32907-43d6-400a-b36a-e6a0ef9f84dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's move on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our same made-up example data and similar thinking, we can use the R
    function `lm.ridge` to attempt to fit our linear model using ridge regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output generated (note the difference in output generated
    by the `summary` function):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b40c4f50-dc12-4ffe-acf7-87ea1a6da8f5.png)'
  prefs: []
  type: TYPE_IMG
- en: You'll find that the `summary` function does not yield the same output on a
    linear regression model as it does on a model using the ridge regression method.
    However, there are a variety of packages available to produce sufficient output
    on ridge regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Improving data or a data model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are various parameters which are used for improving data or data model.
    In this section, we will be studying about a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/161afca5-347a-4159-8d3b-8154f2c3cf63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are much other acceptable or at least well-known methods or approaches
    that a data scientist may employ in an attempt to improve on a statistical model
    (other than regularization) and it''s worth spending some time mentioning a few
    of the most popular:'
  prefs: []
  type: TYPE_NORMAL
- en: Simplification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variation of coefficients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Casual inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back to regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first may be just plain common sense. A simple model is just plain easier
    to interpret and understand. Algorithms run more efficiently on a simpler model,
    allowing the data scientist the luxury of higher iterations as well as more time
    to evaluate the outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind, though, that a more complicated model is somewhat more believable,
    so beware of over-simplification. The approach to finding the right mix between
    complex and simple can be worked both ways; by starting simple and adding complexities
    or, more commonly, by starting complex and removing things out of the model, testing,
    and evaluating and then repeating, until successfully understanding the (fitting)
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Relevance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This one also seems obvious as well. In other words, don't waste time on statistical
    noise. Using common statistical regression packages, you will have visuals (such
    as quantile-quantile plots, influence diagrams, box plots, and so on) to pour
    over and understand. Spending time on removing irrelevancies from a model or data
    will pay dividends. The trick is to be able to identify what is relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Speed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The faster a data scientist can fit models, the more models (and data) can be
    evaluated and understood (the ultimate goal!). The ways and means of model optimization
    can be costly--either in time or expertise--and can focus on the model or the
    data, or both.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is something that can have a substantial effect on a model, but is not
    without risk. Transformation of variables can create models that could make sense
    (and can then be fit and compared to data) and that includes all relevant information,
    but if done irrationally, may introduce bias and imply incorrect outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Variation of coefficients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing coefficients to determine whether a coefficient should vary by group,
    and the estimated scale of variation, is a feasible approach to model improvement.
    Very small varying coefficients (across categories) have the propensity to be
    dropped out of consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Casual inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may be tempted to set up a single large regression to answer several causal
    questions that exist in a model or data; however, in observational settings (including
    experiments in which certain conditions of interest are observational), this approach
    risks bias. The bottom line here is, don't assume anything about any perceived
    relationships (or coefficients), especially don't assume that a coefficient can
    be interpreted causally. However, a casual inference can be effective (where appropriate)
    as a method used to improve a statistical model.
  prefs: []
  type: TYPE_NORMAL
- en: Back to regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting to the point--the theme of regularization is an attempt to improve an
    outcome or performance of a statistical model or method. In other words, to improve
    the process of learning (from data of course) through direct and indirect observation.
  prefs: []
  type: TYPE_NORMAL
- en: The process of attempting to gain knowledge or learn from a finite dataset (also
    known as **empirical learning**) is said to be an **underdetermined problem**,
    because in general, it is an attempt to infer a function `x {\displaystyle x}`,
    given only some examples of data observations.
  prefs: []
  type: TYPE_NORMAL
- en: Another possible method of improving a statistical model is to use **additive
    smoothing** (also known as **Laplacian smoothing**) during the training of a model.
    This is a form of regularization and it works by adding a fixed number to all
    the counts of feature and class combinations during model training.
  prefs: []
  type: TYPE_NORMAL
- en: It is a popular opinion that additive smoothing is more effective than other
    probability smoothing methods in several retrieval tasks such as language model-based
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization fundamentally works to introduce additional information, or an
    intentional bias, or constraint in a training procedure—preventing coefficients
    from taking large values—in order to solve an ill-posed problem. This is a method
    that attempts to shrink coefficients, also known as a **shrinkage method**. The
    information introduced tends to be in the form of a penalty for complexity, such
    as restrictions for smoothness or bounds on the vector space norm. In other words,
    regularization A does what it implies, it regulates how or how much you can change
    a parameter within a statistical model or its data. Yes, that is right, you can
    change the actual data!
  prefs: []
  type: TYPE_NORMAL
- en: When is it justifiable to change the values of your data?
  prefs: []
  type: TYPE_NORMAL
- en: The statistical community respects that the theoretical justification for regularization
    might be that it attempts to impose the belief that among competing hypotheses,
    the one with the fewest assumptions will be the most effective (and therefore
    should be the one selected and used). This belief is rigorously known as **Occam's
    razor** (or the law of parsimony).
  prefs: []
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Should one always (Of course, we are referring to those situations that are
    identified as we discussed in this chapter's section, *Opportunities for regulation*.)
    attempt to institute a regulation method on a statistical model? Will it always
    improve a model or data population?
  prefs: []
  type: TYPE_NORMAL
- en: Before considering an answer to this question, remember that regularization
    does not improve the performance on the dataset that the algorithm initially used
    to learn the model parameters (feature weights). However, it can improve the generalization
    performance (the performance on new, unseen data, which is what you are looking
    for).
  prefs: []
  type: TYPE_NORMAL
- en: Think of using regularization in a statistical model as the adding of bias as
    a countermeasure to overfitting; on the other hand, though, adding too much bias
    almost always results in underfitting and the model will perform badly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer: Regularization doesn''t always work and may cause a model to perform
    poorly (perhaps even worse than before!). S. Raschka, Author of Python Machine
    Learning, makes an interesting comment:'
  prefs: []
  type: TYPE_NORMAL
- en: In intuitive terms, you can think of regularization as a penalty against the
    complexity (of a model). Increasing the regularization strength penalizes large
    weight coefficients. Therefore, your goal is to prevent your model from picking
    up peculiarities or noise and to generalize well to new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Using R for statistical regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of different functions and packages that implement ridge
    regression, such as `lm.ridge()` from the `MASS` package and `ridge()` from the
    `genridge` package. For the lasso, there is also the `lars` package. Here, in
    this chapter, we are going to use R's `glmnet()` function (from the `glmnet` package)
    due to it being well-documented and having a consistent and friendly interface.
  prefs: []
  type: TYPE_NORMAL
- en: The key to working with regularization is to determine an appropriate `lambda`
    value to use. The approach that the `glmnet()` function uses is to use a grid
    of different `lambda` values, training a regression model for each value. Then,
    one can either pick a value manually or use a technique to estimate the best `lambda`.
  prefs: []
  type: TYPE_NORMAL
- en: You can specify the sequence of the values to try (via the `lambda` parameter);
    otherwise, a default sequence with 100 values will be used.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first parameter to the `glmnet()` function must be a matrix of features
    (which we can create using the R function, `model.matrix()`). The second parameter
    is a vector with the output variable. Finally, the `alpha` parameter is a switch
    between ridge regression (0) and lasso (1). The following code sets up for our
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `model.matrix` R function creates a matrix by expanding factors to a set
    of summary variables (depending on the contrasts) and expanding interactions similarly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code that we used to set up the data to be used in this example
    (specifically, `length = 250`) provided a sequence of 250 values. This means that
    (in the preceding code) actually trained 250 ridge regression models and another
    250 lasso models!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can review the value of the `lambda` attribute (of the `cars_models_ridge`
    object) that is produced by `glmnet()` and then apply the `coef()` function to
    this object to retrieve the corresponding coefficients for the 100^(th) model,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can use the R `plot()` function to obtain a plot showing how the
    values of the coefficients change as the logarithm values change.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following code, it is very helpful to show the corresponding
    plot for ridge regression and lasso side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the plot graphic generated by the preceding R code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/966624e3-2d01-46cd-a46d-bf473ac56cc6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the R code to generate the `lasso` plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce26b845-7e47-48f0-839c-b26deda890ed.png)'
  prefs: []
  type: TYPE_IMG
- en: The significant difference between the preceding two graphs is that `lasso`
    forces many coefficients to fall to zero exactly, whereas, in ridge regression,
    they tend to drop off smoothly and only become zero altogether at extreme values.
    Note the values on the top horizontal axis of both of the graphs, which show the
    number of non-zero coefficients as values vary.
  prefs: []
  type: TYPE_NORMAL
- en: Along with applying regularization to minimize the issue of overfitting, the
    `lasso` function is often used to perform feature selection as a feature with
    a zero coefficient would not be included in the model.
  prefs: []
  type: TYPE_NORMAL
- en: As a part of the `glmnet` package, the `predict()` function operates in a variety
    of contexts. We can, for example, determine the **coefficient variance** (**CV**)
    percentages (the strength and direction of a linear relationship between two variables)
    of a model for a `lambda` value that was not in our original list.
  prefs: []
  type: TYPE_NORMAL
- en: Predict is a generic function for predictions from the results of various model
    fitting functions.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try using `predict` on our lasso model (created earlier).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write the following R code on our previously created lasso model, `cars_models_lasso`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding output, you can see that `lasso` has not forced any coefficients
    to zero, in this case, suggesting that none should be removed (and therefore remain
    as features in the model) from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided an explanation of statistical regularization and
    then used sample data in an example to illustrate and better understand statistical
    regularization. Later, we had a discussion of various methods on how to improve
    (the performance of) data or a data model with regulation. Finally, we saw how
    well the R language supports the concepts and methods of regulation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're looking to cover the idea of data model assessment
    and using statistics for assessment. We'll compare the concepts of data assessment
    and data quality assurance, and finally, apply the idea of statistical assessment
    to data using R.
  prefs: []
  type: TYPE_NORMAL
