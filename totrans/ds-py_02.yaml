- en: Chapter 2. Python and Jupyter Notebooks to Power your Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"The Best Line of Code is the One You Didn''t Have to Write!"'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – *Unknown*
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, I gave a developer''s perspective on data science
    based on real experience and discussed three strategic pillars required for successful
    deployment with in the enterprise: data, services, and tools. I also discussed
    the idea that data science is not only the sole purview of data scientists, but
    rather a team sport with a special role for developers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, I''ll introduce a solution—based on Jupyter Notebooks, Python,
    and the PixieDust open source library—that focuses on three simple goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Democratizing data science by lowering the barrier to entry for non-data scientists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing collaboration between developers and data scientists
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making it easier to operationalize data science analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This solution only focuses on the tools pillar and not on data and services, which
    should be implemented independently, although we''ll cover some of it when discussing
    the sample applications starting in [Chapter 6](ch06.xhtml "Chapter 6. Analytics
    Study: AI and Image Recognition with TensorFlow"), *Analytics Study: AI and Image
    Recognition with TensorFlow*.'
  prefs: []
  type: TYPE_NORMAL
- en: Why choose Python?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like many developers, when it came to building data-intensive projects, using
    Python wasn't my first choice. To be honest, having worked with Java for so many
    years, Scala seemed much more attractive to me at first, even though the learning
    curve was pretty steep. Scala is a very powerful language that elegantly combines
    object-oriented and functional programming, which is sorely lacking in Java (at least until
    Java 8 started to introduce lambda expressions).
  prefs: []
  type: TYPE_NORMAL
- en: Scala also provides a very concise syntax that translates into fewer lines of
    code, higher productivity, and ultimately fewer bugs. This comes in very handy,
    especially when a large part of your work is to manipulate data. Another reason
    for liking Scala is the better API coverage when using big data frameworks such
    as Apache Spark, which are themselves written in Scala. There are also plenty
    of other good reasons to prefer Scala, such as it's a strong typed system and
    its interoperability with Java, online documentation, and high performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for a developer like myself who is starting to get involved in data science,
    Scala would seem like a more natural choice, but yet, spoiler alert, we ended
    up focusing on Python instead. There are multiple reasons for this choice:'
  prefs: []
  type: TYPE_NORMAL
- en: Python, as a language, has a lot going on for itself too. It is a dynamic programming
    language with similar benefits to Scala, such as functional programming, and concise
    syntax, among others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python has seen, over the last few years, a meteoric rise among data scientists,
    overtaking longtime rival R as the overall preferred language for data science,
    as demonstrated by a quick search for the terms `Python Data Science`, `Python
    Machine Learning`, `R Data Science`, and `R Machine Learning` on Google Trends:![Why
    choose Python?](img/B09699_02_01.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interest trends for 2017
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In a virtuous circle, Python's rising popularity fuels a vast and growing ecosystem
    of wide-ranging libraries that can be easily imported into your projects using
    the pip Python package installer. Data scientists now have access to many powerful
    open source Python libraries for data manipulation, data visualization, statistics, mathematics,
    machine learning, and natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Even beginners can quickly build a machine learning classifier using the popular
    scikit-learn package ([http://scikit-learn.org](http://scikit-learn.org)) without
    being a machine learning expert, or quickly plot rich charts using Matplotlib
    ([https://matplotlib.org](https://matplotlib.org)) or Bokeh ([https://bokeh.pydata.org](https://bokeh.pydata.org)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, Python has also emerged as one of the top languages for developers
    as shown in this IEEE Spectrum 2017 survey ([https://spectrum.ieee.org/computing/software/the-2017-top-programming-languages](https://spectrum.ieee.org/computing/software/the-2017-top-programming-languages)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why choose Python?](img/B09699_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Usage statistics by programming languages
  prefs: []
  type: TYPE_NORMAL
- en: 'This trend is also confirmed on GitHub where Python is now number three in
    the total number of repositories, just behind Java and JavaScript:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why choose Python?](img/B09699_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: GitHub repositories statistics by programming language
  prefs: []
  type: TYPE_NORMAL
- en: The preceding chart shows some interesting statistics, demonstrating how active
    the Python developer community is. Python - related repositories that are active
    on GitHub are the third biggest in size, with similarly healthy total code pushes
    and opened issues per repository.
  prefs: []
  type: TYPE_NORMAL
- en: Python has also become ubiquitous on the web, powering numerous high-profile
    websites with web development frameworks, such as Django ([https://www.djangoproject.com](https://www.djangoproject.com)),
    Tornado ([http://www.tornadoweb.org](http://www.tornadoweb.org)) and TurboGears
    ([http://turbogears.org](http://turbogears.org)). More recently, there are signs
    that Python is also making its way into the field of cloud services with all major
    Cloud providers including it in some capacity in their offerings.
  prefs: []
  type: TYPE_NORMAL
- en: Python obviously has a bright future in the field of data science, especially
    when used in conjunction with powerful tools such as Jupyter Notebooks, which
    have become very popular in the data scientist community. The value proposition
    of Notebooks is that they are very easy to create and perfect for quickly running
    experiments. In addition, Notebooks support multiple high-fidelity serialization
    formats that can capture instructions, code, and results, which can then very
    easily be shared with other data scientists on the team or as open source for
    everyone to use. For example, we're seeing an explosion of Jupyter Notebooks being
    shared on GitHub, numbering in excess of 2.5 million and counting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the result of a GitHub search for any file with
    the extension `.ipynb,` which is the most popular format for serialized Jupyter
    Notebooks (JSON format):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why choose Python?](img/B09699_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Search results for Jupyter Notebooks on GitHub
  prefs: []
  type: TYPE_NORMAL
- en: This is great, but Jupyter Notebooks are too often thought of as data scientist
    tools only. We'll see in the coming chapters that they can be much more and that
    they can also help all types of teams solve data problems. For example, they can
    help business analysts quickly load and visualize a dataset, enable developers
    to work with data scientists directly within a Notebook to leverage their analytics
    and build powerful dashboards, or allow DevOps to effortlessly deploy these dashboards
    into scalable, enterprise-ready microservices that can run as standalone web applications
    or embeddable components. It is based on this vision of bringing the tools of
    data science to non-data scientists that the PixieDust open source project was
    created.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing PixieDust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Fun fact**'
  prefs: []
  type: TYPE_NORMAL
- en: I am often asked how I came up with the name PixieDust, for which I answer that
    I simply wanted to make Notebook simple, as in magical, for non-data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: 'PixieDust ([https://github.com/ibm-watson-data-lab/pixiedust](https://github.com/ibm-watson-data-lab/pixiedust))
    is an open-source project composed primarily of three components designed to address the
    three goals stated at the beginning of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A helper Python library for Jupyter Notebooks that provides simple APIs to load
    data from various sources into popular frameworks, such as pandas and Apache Spark
    DataFrame, and then to visualize and explore the dataset interactively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple Python-based programming model that enables developers to "productize"
    the analytics directly into the Notebook by creating powerful dashboards called
    PixieApps. As we'll see in the next chapters, PixieApps are different from traditional
    **BI** (short for, **Business Intelligence**) dashboards because developers can
    directly use HTML and CSS to create an arbitrary complex layout. In addition,
    they can embed in their business logic access to any variable, class, or function
    created in the Notebook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A secure microservice web server called PixieGateway that can run PixieApps
    as standalone web applications or as components that can be embedded into any
    website. PixieApps can easily be deployed from the Jupyter Notebook using a graphical
    wizard and without requiring any code changes. In addition, PixieGateway supports
    the sharing of any charts created by PixieDust as embeddable web pages, allowing
    data scientists to easily communicate results outside of the Notebook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is important to note that the PixieDust `display()` API primarily supports
    two popular data processing frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pandas** ([https://pandas.pydata.org](https://pandas.pydata.org)): By far
    the most popular Python data analysis package, pandas provides two main data structures:
    DataFrame for manipulating two-dimensional table-like datasets, and Series for
    one-dimensional column-like datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, only pandas DataFrames are supported by PixieDust `display()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Apache Spark DataFrame** ([https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)):
    This is a high-level data structure for manipulating distributed datasets across
    a Spark Cluster. Spark DataFrames are built on top of the lower-level **RDD**
    (short for, **Resilient Distributed Dataset**) with the added functionality that
    it supports SQL queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another less commonly used format supported by PixieDust `display()` is an
    array of JSON objects. In this case, PixieDust will use the values to build the
    rows and keys are used as columns, for example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In addition, PixieDust is highly extensible both at the data handling and rendering
    level. For example, you can add new data types to be rendered by the visualization
    framework or if you want to leverage a plotting library you particularly like,
    you can easily add it to the list of renderers supported by PixieDust (see the
    next chapters for more details).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also find that PixieDust contains a few extra utilities related to
    Apache Spark, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PackageManager**: This lets you install Spark packages inside a Python Notebook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scala Bridge**: This lets you use Scala directly in a Python Notebook using
    the `%%scala` magic. Variables are automatically transferred from Python to Scala and
    vice versa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark Job Progress Monitor**: Track the status of any Spark job by showing
    a progress bar directly in the cell output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we dive into each of the three PixieDust components, it would be a good
    idea to get access to a Jupyter Notebook, either by signing up to a hosted solution
    on the cloud (for example, Watson Studio at [https://datascience.ibm.com](https://datascience.ibm.com))
    or installing a development version on your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can install the Notebook server locally by following the instructions here:
    [http://jupyter.readthedocs.io/en/latest/install.html](http://jupyter.readthedocs.io/en/latest/install.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the Notebook server locally, simply run the following command from
    a Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The Notebook home page will automatically open in a browser. There are many
    configuration options to control how the Notebook server is launched. These options
    can be added to the command line or persisted in the Notebook configuration file.
    If you want to experiment with all the possible configuration options, you can
    generate a configuration file using the `--generate-config` option as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following Python file, `<home_directory>/.jupyter/jupyter_notebook_config.py`,
    which contains a set of auto-documented options that have been disabled. For example,
    if you don''t want to have the browser automatically opened when the Jupyter Notebook
    starts, locate the line that contains the `sc.NotebookApp.open_browser` variable,
    uncomment it, and set it to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: After making that change, simply save the `jupyter_notebook_config.py` file
    and restart the Notebook server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to install the PixieDust library using the `pip` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Notebook itself, enter the following command in a cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: The exclamation point syntax is specific to Jupyter Notebook and
    denotes that the rest of the command will be executed as a system command. For
    example, you could use `!ls` to list all the files and directories that are under
    the current working directory.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell using the **Cell** | **Run Cells** menu or the **Run** icon on
    the toolbar. You can also use the following keyboard shortcuts to run a cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Ctrl* + *Enter*: Run and keep the current cell selected'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Shift* + *Enter*: Run and select the next cell'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alt* + *Enter*: Run and create new empty cell just below'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Restart the kernel to make sure the `pixiedust` library is correctly loaded
    into the kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the results after installing `pixiedust` for
    the first time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing PixieDust](img/B09699_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Installing the PixieDust library on a Jupyter Notebook
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I strongly recommend using Anaconda ([https://anaconda.org](https://anaconda.org)),
    which provides excellent Python package management capabilities. If, like me,
    you like to experiment with different versions of Python and libraries dependencies,
    I suggest you use Anaconda virtual environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'They are lightweight Python sandboxes that are very easy to create and activate
    (see [https://conda.io/docs/user-guide/tasks/manage-environments.html](https://conda.io/docs/user-guide/tasks/manage-environments.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new environment: `conda create --name env_name`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'List all environments: `conda env list`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activate an environment: `source activate env_name`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I also recommend that, optionally, you get familiar with the source code, which
    is available at [https://github.com/ibm-watson-data-lab/pixiedust](https://github.com/ibm-watson-data-lab/pixiedust)
    and [https://github.com/ibm-watson-data-lab/pixiegateway](https://github.com/ibm-watson-data-lab/pixiegateway).
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to explore the PixieDust APIs starting with `sampleData()`
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: SampleData – a simple API for loading data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Loading data into a Notebook is one of the most repetitive tasks a data scientist
    can do, yet depending on the framework or data source being used, writing the code can
    be difficult and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a concrete example of trying to load a CSV file from an open data
    site (say [https://data.cityofnewyork.us](https://data.cityofnewyork.us)) into
    both a pandas and Apache Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: Going forward, all the code is assumed to run in a Jupyter Notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For pandas, the code is pretty straightforward as it provides an API to directly
    load from URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The last statement, calling `building_df,` will print its contents in the output
    cell. This is possible without a print because Jupyter is interpreting the last
    statement of a cell calling a variable as a directive to print it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SampleData – a simple API for loading data](img/B09699_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The default output of a pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for Apache Spark, we need to first download the data into a file then
    use the Spark CSV connector to load it into a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is slightly different since `building_df` is now a Spark DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SampleData – a simple API for loading data](img/B09699_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Default output of a Spark DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: Even though this code is not that big, it has to be repeated every time and,
    most likely, will require spending the time to do a Google search to remember
    the correct syntax. The data may also be in a different format, for example, JSON,
    which will require calling different APIs both for pandas and Spark. The data
    may also not be well-formed and can contain a bad line in a CSV file or have a
    wrong JSON syntax. All these issues are unfortunately not rare and contribute
    to the 80/20 rule of data science, which states that data scientists spends on
    average 80% of their time acquiring, cleaning, and loading data and only 20% doing
    the actual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'PixieDust provides a simple `sampleData` API to help improve the situation.
    When called with no parameters, it displays a list of pre-curated datasets ready
    for analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SampleData – a simple API for loading data](img/B09699_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: PixieDust built-in datasets
  prefs: []
  type: TYPE_NORMAL
- en: The list of prebuilt curated datasets can be customized to fit the organization,
    which is a good step toward our *data* pillar, as described in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The user can then simply call the `sampleData` API again with the ID of the
    prebuilt dataset and get a Spark DataFrame if the Spark framework in the Jupyter
    Kernel is available or fall back to a pandas DataFrame if not.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we call `sampleData()` on a Notebook connected with
    Spark. We also call `enableSparkJobProgressMonitor()` to display real-time information
    about the Spark jobs involved in the operation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: Spark jobs are processes that run on a particular node in the Spark
    cluster with a specific subset of the data. In the case of loading a large amount
    data from a data source, each Spark job is given a specific subset to work on
    (the actual size depends on the number of nodes in the cluster and the size of
    the overall data), running in parallel with the other jobs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a separate cell, we run the following code to enable the Spark Job Progress
    Monitor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then invoke `sampleData` to load the `cars` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SampleData – a simple API for loading data](img/B09699_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Loading a built-in dataset with PixieDust sampleData API
  prefs: []
  type: TYPE_NORMAL
- en: 'The user can also pass an arbitrary URL that points to a downloadable file;
    PixieDust currently supports JSON and CSV files. In this case, PixieDust will
    automatically download the file, cache it in a temporary area, detect the format,
    and load it into a Spark or pandas DataFrame depending on whether Spark is available
    in the Notebook. Note that the user can also force loading into pandas even if
    Spark is available using the `forcePandas` keyword argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `sampleData()` API is also smart enough to recognize URLs that point to
    compressed files of the ZIP and GZ types. In this case, it will automatically
    unpack the raw binary data and load the file included in the archive. For ZIP
    files, it looks at the first file in the archive and, for GZ files, it simply
    decompresses the content as GZ files are not archives and do not contain multiple
    files. The `sampleData()` API will then load the DataFrame from the decompressed
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can directly load borough information from a ZIP file provided
    by the London open data website and display the results as a pie chart using the
    `display()` API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows (assuming that your Notebook is connected to Spark,
    otherwise a pandas DataFrame will be loaded):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then call `display()` on the `london_info` DataFrame, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We select **Pie Chart** in the Chart menu and in the **Options** dialog, we
    drag and drop the `Area name` column in the **Keys** area and the `Crime rates
    per thousand population 2014/15` in the **Values** area, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SampleData – a simple API for loading data](img/B09699_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Chart options for visualizing the london_info DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking on the **OK** button in the **Options** dialog, we get the following
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SampleData – a simple API for loading data](img/B09699_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Pie chart created from a URL pointing at a compressed file
  prefs: []
  type: TYPE_NORMAL
- en: Many times, you have found a great dataset, but the file contains errors or
    the data that's important to you is in the wrong format or buried in some unstructured
    text that needs to be extracted into its own column. This process is also known
    as **data wrangling** and can be very time-consuming. In the next section, we
    will look at an extension to PixieDust called `pixiedust_rosie` that provides
    a `wrangle_data` method, which helps with this process.
  prefs: []
  type: TYPE_NORMAL
- en: Wrangling data with pixiedust_rosie
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working in a controlled experiment is, most of the time, not the same as working
    in the real world. By this I mean that, during development, we usually pick (or
    I should say manufacture) a sample dataset that is designed to behave; it has
    the right format, it complies with the schema specification, no data is missing,
    and so on. The goal is to focus on verifying the hypotheses and build the algorithms,
    and not so much on data cleansing, which can be very painful and time-consuming.
    However, there is an undeniable benefit to get data that is as close to the real
    thing as early as possible in the development process. To help with this task,
    I worked with two IBM colleagues, Jamie Jennings and Terry Antony, who volunteered
    to build an extension to PixieDust called `pixiedust_rosie`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This Python package implements a simple `wrangle_data()` method to automate
    the cleansing of raw data. The `pixiedust_rosie` package currently supports CSV
    and JSON, but more formats will be added in the future. The underlying data processing
    engine uses the **Rosie Pattern Language** (**RPL**) open source component, which
    is a regular expressions engine designed to be simpler to use for developers,
    more performant, and scalable to big data. You can find more information about
    Rosie here: [http://rosie-lang.org](http://rosie-lang.org).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, you need to install the `pixiedust_rosie` package using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `pixiedust_rosie` package has a dependency on `pixiedust` and `rosie,` which will
    be automatically downloaded if not already installed on the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `wrangle_data()` method is very similar to the `sampleData()` API. When
    called with no parameters, it will show you the list of pre-curated datasets,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wrangling data with pixiedust_rosie](img/B09699_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: List of pre-curated datasets available for wrangle_data()
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also invoke it with the ID of a pre-curated dataset or a URL link,
    for example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we invoke `wrangle_data()` on a CSV file referenced by
    the `url` variable. The function starts by downloading the file in the local filesystem
    and performs an automated data classification on a subset of the data, to infer
    the data schema. A schema editor PixieApp is then launched, which provides a set
    of wizard screens to let the user configure the schema. For example, the user
    will be able to drop and rename columns and, more importantly, destructure existing
    columns into new columns by providing Rosie patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wrangling data with pixiedust_rosie](img/B09699_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: wrangle_data() workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'The first screen of the `wrangle_data()` wizard shows the schema that has been inferred
    by the Rosie data classifier as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wrangling data with pixiedust_rosie](img/B09699_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The wrangle_data() schema editor
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding schema widget shows the column names, `Rosie Type` (advanced type
    representation specific to Rosie) and `Column Type` (map to the supported pandas
    types). Each row also contains three action buttons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Delete column**: This removes the columns from the schema. This column will
    not appear in the final pandas DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rename column**: This changes the name of the column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transform column**: This transforms a column by destructuring it into new
    columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At any time, the user is able to preview the data (shown in the preceding SampleData widget)
    to validate that the schema configuration is behaving as intended.
  prefs: []
  type: TYPE_NORMAL
- en: When the user clicks on the transform column button, a new screen is shown that
    lets the user specify patterns for building new columns. In some cases, the data
    classifier will be able to automatically detect the patterns, in which case, a
    button will be added to ask the user whether the suggestions should be applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the **Transform Selected Column** screen with
    automated suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wrangling data with pixiedust_rosie](img/B09699_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Transform column screen
  prefs: []
  type: TYPE_NORMAL
- en: 'This screen shows four widgets with the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Rosie Pattern input is where you can enter a custom Rosie Pattern that represents
    the data for this column. You then use the **Extract Variables** button to tell
    the schema editor which part of the pattern should be extracted into a new column
    (more on that is explained soon).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's a help widget that provides a link to the RPL documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's a preview of the data for the current column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's a preview of the data with the Rosie Pattern applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When the user clicks on the **Extract Variables** button, the widget is updated
    as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wrangling data with pixiedust_rosie](img/B09699_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Extracting Rosie variables into columns
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the user has the option to edit the definition and then click
    on the **Create Columns** button to add the new columns to the schema. The **Sample
    of New Column(s)** widget is then updated to show a preview of what the data would
    look like. An error is shown in this widget if the pattern definition contains
    bad syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wrangling data with pixiedust_rosie](img/B09699_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Preview of new columns after applying pattern definitions
  prefs: []
  type: TYPE_NORMAL
- en: 'When the user clicks on the **Commit Columns** button, the main schema editor
    screen is displayed again with the new columns added, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wrangling data with pixiedust_rosie](img/B09699_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Schema editor with new columns
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is to click on the **Finish** button to apply the schema definition
    to the raw file and create a pandas DataFrame that will be available as a variable
    in the Notebook. At this point, the user is presented with a dialog box that contains
    a default variable name that can be edited, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wrangling data with pixiedust_rosie](img/B09699_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Edit the variable name for the Result Pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking on the **Finish** button, `pixiedust_rosie` goes over the entire
    dataset, applying the schema definition. When done, it creates a new cell just
    below the current one with a generated code that invokes the `display()` API on
    the newly generated pandas DataFrame, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding cell will let you explore and visualize the new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The `wrangle_data()` capability we've explored in this section is a first step
    toward helping data scientists spend less time cleaning the data and more time
    analyzing it. In the next section, we will discuss how to help data scientists
    with data exploration and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Display – a simple interactive API for data visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data visualization is another very important data science task that is indispensable
    for exploring and forming hypotheses. Fortunately, the Python ecosystem has a
    lot of powerful libraries dedicated to data visualization, such as these popular
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Matplotlib: [http://matplotlib.org](http://matplotlib.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Seaborn: [https://seaborn.pydata.org](https://seaborn.pydata.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bokeh: [http://bokeh.pydata.org](http://bokeh.pydata.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brunel: [https://brunelvis.org](https://brunelvis.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, similar to data loading and cleaning, using these libraries in a Notebook
    can be difficult and time-consuming. Each of these libraries come with their own
    programming model and APIs are not always easy to learn and use, especially if
    you are not an experienced developer. Another issue is that these libraries do
    not have a high-level interface to commonly used data processing frameworks such
    as pandas (except maybe Matplotlib) or Apache Spark and, as a result, a lot of
    data preparation is needed before plotting the data.
  prefs: []
  type: TYPE_NORMAL
- en: To help with this problem, PixieDust provides a simple `display()` API that
    enables Jupyter Notebook users to plot data using an interactive graphical interface
    and without any required coding. This API doesn't actually create charts but does
    all the heavy lifting of preparing the data before delegating to a renderer by
    calling its APIs according to the user selection.
  prefs: []
  type: TYPE_NORMAL
- en: The `display()` API supports multiple data structures (pandas, Spark, and JSON)
    as well as multiple renderers (Matplotlib, Seaborn, Bokeh, and Brunel).
  prefs: []
  type: TYPE_NORMAL
- en: 'As an illustration, let''s use the built-in car performance dataset and start
    visualizing the data by calling the `display()` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The first time the command is called on the cell, a tabular view is displayed
    and, as the user navigates through the menus, selected options are stored in the
    cell metadata as JSON so they can be used again the next time the cell is running.
    The output layout for all the visualizations follows the same pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: There's an extensible top-level menu for switching between charts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's a download menu for downloading the file in the local machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's a filter toggle button that lets users refine their exploration by filtering
    the data. We'll discuss the filter capability in the *Filtering* section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's a Expand/Collapse Pixiedust Output button for collapsing/expanding the output
    content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's an **Options** button that invokes a dialog box with configurations
    specific to the current visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's a **Share** button that lets you publish the visualization on the web.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: This button can only be used if you have deployed a PixieGateway,
    which we''ll discuss in detail in [Chapter 4](ch04.xhtml "Chapter 4. Publish your
    Data Analysis to the Web - the PixieApp Tool"), *Publish your Data Analysis to
    the Web - the PixieApp Tool*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There's a contextual set of options on the right-hand side of the visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's the main visualization area.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Display – a simple interactive API for data visualization](img/B09699_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Visualization output layout for the table renderer
  prefs: []
  type: TYPE_NORMAL
- en: 'To start creating a chart, first select the appropriate type in the menu. Out
    of the box, PixieDust supports six types of charts: **Bar Chart**, **Line Chart**,
    **Scatter Plot**, **Pie Chart**, **Map**, and **Histogram**. As we''ll see in
    [Chapter 5](ch05.xhtml "Chapter 5. Python and PixieDust Best Practices and Advanced
    Concepts"), *Python and PixieDust Best Practices and Advanced Concepts*, PixieDust
    also provides APIs to let you customize these menus by adding new ones or adding
    options to existing ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Display – a simple interactive API for data visualization](img/B09699_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: PixieDust Charts menu
  prefs: []
  type: TYPE_NORMAL
- en: The first time a chart menu is called, an options dialog will be displayed to
    configure a set of basic configuration options, such as what to use for the *X*
    and *Y* axes, the type of aggregation, and many more. To save you time, the dialog
    will be prepopulated with the data schema that PixieDust automatically introspected
    from the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will create a bar chart showing the average mileage
    consumption by horsepower:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Display – a simple interactive API for data visualization](img/B09699_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Bar chart dialog options
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking **OK** will display the interactive interface in the cell output area:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Display – a simple interactive API for data visualization](img/B09699_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Bar chart visualization
  prefs: []
  type: TYPE_NORMAL
- en: 'The canvas shows the chart in the center area and some contextual options on
    the side relevant to the type of chart selected. For example, we can select the
    field **origin** in the **Cluster By** combobox to show a breakdown by country
    of origin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Display – a simple interactive API for data visualization](img/B09699_02_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Clustered bar chart visualization
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, PixieDust `display()` doesn't actually create the chart,
    rather it prepares the data based on the selected options and does the heavy lifting
    of calling the APIs of a renderer engine, with the correct parameters. The goal
    behind this design is for each chart type to support multiple renderers without
    any extra coding, providing as much freedom of exploration to the user as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Out of the box, PixieDust supports the following renderers provided that the
    corresponding libraries are installed. For those that are not installed, a warning
    will be generated in the PixieDust log and the corresponding renderer will not
    be displayed in the menu. We'll cover in detail the PixieDust log in [Chapter
    5](ch05.xhtml "Chapter 5. Python and PixieDust Best Practices and Advanced Concepts"),
    *Python and PixieDust Best Practices and Advanced Concepts*.
  prefs: []
  type: TYPE_NORMAL
- en: Matplotlib ([https://matplotlib.org](https://matplotlib.org))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seaborn ([https://seaborn.pydata.org](https://seaborn.pydata.org))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This library needs to be installed using: `!pip install seaborn.`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Bokeh ([https://bokeh.pydata.org](https://bokeh.pydata.org))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This library needs to be installed using: `!pip install bokeh.`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Brunel ([https://brunelvis.org](https://brunelvis.org))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This library needs to be installed using: `!pip install brunel.`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Google Map ([https://developers.google.com/maps](https://developers.google.com/maps))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapbox ([https://www.mapbox.com](https://www.mapbox.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: Google Map and Mapbox require an API key that you can obtain on their
    respective sites.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can switch between renderers using the **Renderer** combobox. For example,
    if we want more interactivity to explore the chart (such as zooming and panning),
    we can use the Bokeh renderer instead of Matplotlib, which gives us only a static
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Display – a simple interactive API for data visualization](img/B09699_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Cluster bar chart using the Bokeh renderer
  prefs: []
  type: TYPE_NORMAL
- en: Another chart type worth mentioning is Map, which is interesting when your data contains
    geospatial information, such as longitude, latitude, or country/state information.
    PixieDust supports multiple types of geo-mapping rendering engines including the
    popular Mapbox engine.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before using the Mapbox renderer, it is recommended to get an API key from
    the Mapbox site at this location: ([https://www.mapbox.com/help/how-access-tokens-work](https://www.mapbox.com/help/how-access-tokens-work)).
    However, if you don''t have one, a default key will be provided by PixieDust.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a Map chart, let''s use the *Million-dollar home sales in NE Mass*
    dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'First, select **Map** in the chart drop-down button, then in the options dialog,
    select `LONGITUDE` and `LATITUDE` as the keys and enter the Mapbox access token
    in the provided input. You can add multiples fields in the **Values** area, and
    they will be displayed as tooltips on the map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Display – a simple interactive API for data visualization](img/B09699_02_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Options dialog for Mapbox charts
  prefs: []
  type: TYPE_NORMAL
- en: 'When clicking the **OK** button, you''ll get an interactive map that you can
    customize using the style (simple, choropleth, or density map), color, and basemap (light, satellite,
    dark, and outdoors) options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Display – a simple interactive API for data visualization](img/B09699_02_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Interactive Mapbox visualization
  prefs: []
  type: TYPE_NORMAL
- en: 'Each chart type has its own set of contextual options, which are self-explanatory,
    and I encourage you at this point to play with each and every one of them. If
    you find issues or have enhancement ideas, you can always create a new issue on
    GitHub at [https://github.com/ibm-watson-data-lab/pixiedust/issues](https://github.com/ibm-watson-data-lab/pixiedust/issues)
    or, better yet, submit a pull request with your code changes (there''s more information
    on how to do that here: [https://help.github.com/articles/creating-a-pull-request](https://help.github.com/articles/creating-a-pull-request)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid reconfiguring the chart every time the cell runs, PixieDust stores
    the chart options as a JSON object in the cell metadata, which is eventually saved
    in the Notebook. You can manually inspect this data by selecting the **View**
    | **Cell Toolbar** | **Edit Metadata** menu, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Display – a simple interactive API for data visualization](img/B09699_02_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Show Edit Metadata button
  prefs: []
  type: TYPE_NORMAL
- en: 'An **Edit Metadata** button will be shown at the top of the cell, which, when
    clicked on, displays the PixieDust configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Display – a simple interactive API for data visualization](img/B09699_02_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Edit Cell Metadata dialog
  prefs: []
  type: TYPE_NORMAL
- en: This JSON configuration will be important when we discuss PixieApps in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To better explore data, PixieDust also provides a built-in, simple graphical
    interface that lets you quickly filter the data being visualized. You can quickly
    invoke the filter by clicking on the filter toggle button in the top-level menu.
    To keep things simple, the filter only supports building predicates based on one
    column only, which is sufficient in most cases to validate simple hypotheses (based
    on feedback, this feature may be enhanced in the future to support multiple predicates).
    The filter UI will automatically let you select the column to filter on and, based
    on its type, will show different options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numerical type**: The user can select a mathematical comparator and enter
    a value for the operand. For convenience, the UI will also show statistical values
    related to the chosen column, which can be used when picking the operand value:![Filtering](img/B09699_02_30.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter on the mpg numerical column of the cars data set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**String type**: The user can enter an expression to match the column value,
    which can be either a regular expression or a plain string. For convenience, the UI also
    shows basic help on how to build a regular expression:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Filtering](img/B09699_02_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Filter on the name String type of the cars dataset
  prefs: []
  type: TYPE_NORMAL
- en: When clicking on the **Apply** button, the current visualization is updated
    to reflect the filter configuration. It is important to note that the filter applies
    to the whole cell and not only to the current visualization. Therefore, it will
    continue to apply when switching between chart types. The filter configuration
    is also saved in the cell metadata, so it will be preserved when saving the Notebook
    and rerunning the cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following screenshot visualizes the `cars` dataset as a bar
    chart showing only the rows with `mpg` greater than `23,` which, according to
    the statistics box, is the mean for the dataset, and clustered by years. In the
    options dialog, we select the `mpg` column as the key and `origin` as the value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Filtering](img/B09699_02_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Filtered bar chart for the cars dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, in this section, we''ve discussed how PixieDust can help with
    three difficult and time-consuming data science tasks: data loading, data wrangling,
    and data visualization. Next, we are going to see how PixieDust can help increase
    collaboration between data scientists and developers.'
  prefs: []
  type: TYPE_NORMAL
- en: Bridging the gap between developers and data scientists with PixieApps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Solving hard data problems is only part of the mission given to data science
    teams. They also need to make sure that data science results get properly operationalized
    to deliver business value to the organization. Operationalizing data analytics
    is very much use case - dependent. It could mean, for example, creating a dashboard
    that synthesizes insights for decision makers or integrating a machine learning
    model, such as a recommendation engine, into a web application.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, this is where data science meets software engineering (or as
    some would say, *where the rubber meets the road*). Sustained collaboration between
    the teams—instead of a one-time handoff—is key to a successful completion of the
    task. More often than not, they also have to grapple with different languages
    and platforms, leading to significant code rewrites by the software engineering
    team.
  prefs: []
  type: TYPE_NORMAL
- en: We experienced it firsthand in our *Sentiment analysis of Twitter hashtags*
    project when we needed to build a real-time dashboard to visualize the results.
    The data analytics was written in Python using pandas, Apache Spark, and a few
    plotting libraries such as Matplotlib and Bokeh, while the dashboard was written
    in Node.js ([https://nodejs.org](https://nodejs.org)) and D3 ([https://d3js.org](https://d3js.org)).
  prefs: []
  type: TYPE_NORMAL
- en: We also needed to build a data interface between the analytics and the dashboard
    and, since we needed the system to be real-time, we chose to use Apache Kafka
    to stream events formatted with the analytics results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram generalizes an approach that I call the **hand-off pattern**
    where the data science team builds the analytics and deploys the results in a
    data interface layer. The results are then consumed by the application. The data
    layer is usually handled by the data engineer, which is one of the roles we discussed
    in [Chapter 1](ch01.xhtml "Chapter 1. Programming and Data Science – A New Toolset"),
    *Programming and Data Science – A New Toolset*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bridging the gap between developers and data scientists with PixieApps](img/B09699_02_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hand-off between data science and engineering
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this hand-off pattern is that it is not conducive to rapid
    iteration. Any changes in the data layer need to be synchronized with the software
    engineering team to avoid breaking the application. The idea behind PixieApps
    is to build the application while staying as close as possible to the data science
    environment, which is, in our case, the Jupyter Notebook. With this approach,
    the analytics are directly called from the PixieApp, which runs embedded in the
    Jupyter Notebook, hence making it easy for data scientists and developers to collaborate
    and iterate to make rapid improvements.
  prefs: []
  type: TYPE_NORMAL
- en: PixieApp defines a simple programming model for building single-page applications with
    direct access to the IPython Notebook Kernel (which is the Python backend process
    running the Notebook code). In essence, a PixieApp is a Python class that encapsulates
    both the presentation and business logic. The presentation is composed of a set
    of special methods called routes that return an arbitrary HTML fragment. Each
    PixieApp has a default route that returns the HTML fragment for the starting page.
    Developers can use custom HTML attributes to invoke other routes and dynamically
    update all or part of the page. A route may, for example, invoke a machine learning
    algorithm created from within the Notebook or generate a chart using the PixieDust
    display framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the high-level architecture of how PixieApps interact with
    the Jupyter Notebook client frontend and the IPython Kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bridging the gap between developers and data scientists with PixieApps](img/B09699_02_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: PixieApp interaction with the Jupyter Kernel
  prefs: []
  type: TYPE_NORMAL
- en: 'As a preview of what a PixieApp looks like, here''s a *hello world* sample
    application that has one button showing a bar chart for the cars DataFrame we
    created in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'When the preceding code runs in a Notebook cell, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bridging the gap between developers and data scientists with PixieApps](img/B09699_02_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hello World PixieApp
  prefs: []
  type: TYPE_NORMAL
- en: You probably have a lot of questions about the preceding code, but don't worry.
    In the next chapters, we'll cover all the PixieApp technical details, including
    how to use them in end-to-end pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture for operationalizing data science analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we saw how PixieApps combined with the PixieDust display
    framework offer an easy way to build powerful dashboards that connect directly
    with your data analytics, allowing for rapid iterations between the algorithms
    and the user interface. This is great for rapid prototyping, but Notebooks are
    not suitable to be used in a production environment where the target persona is
    the line of business user. One obvious solution would be to rewrite the PixieApp
    using a traditional three tiers web application architecture, for example, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: React ([https://reactjs.org](https://reactjs.org)) for the presentation layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node.js for the web layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data access library targeted at the web analytics layer for machine learning
    scoring or running any other analytic jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, this would provide only a marginal improvement over the existing process,
    which would consist only, in this case, of the ability to do iterative implementation
    with the PixieApp.
  prefs: []
  type: TYPE_NORMAL
- en: A much better solution would be to directly deploy and run PixieApps as web
    applications, including the analytics in the surrounding Notebook and, while we're at
    it, without any code change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this model, Jupyter Notebooks would become the central tool for a simplified
    development life cycle, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture for operationalizing data science analytics](img/B09699_02_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data science pipeline development life cycle
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists use a Python Notebook to load, enrich, and analyze data and
    create analytics (machine learning models, statistics, and so on)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the same Notebook, developers create a PixieApp to operationalize these
    analytics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once ready, developers publish the PixieApp as a web application, where it can
    be easily consumed interactively by line-of-business users without the need to
    access Notebooks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'PixieDust provides an implementation of this solution with the PixieGateway
    component. PixieGateway is a web application server responsible for loading and running
    PixieApps. It is built on top of the Jupyter Kernel Gateway ([https://github.com/jupyter/kernel_gateway](https://github.com/jupyter/kernel_gateway)),
    which itself is built on top of the Tornado web framework, and therefore follows
    an architecture as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture for operationalizing data science analytics](img/B09699_02_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: PixieGateway architecture diagram
  prefs: []
  type: TYPE_NORMAL
- en: The PixieApp is published into the PixieGateway server directly from the Notebook
    and a URL is generated. Behind the scene, PixieGateway allocates a Jupyter Kernel
    to run the PixieApp. Based on configuration, the PixieApp could share the kernel
    instance with other apps or have a dedicated kernel based on needs. The PixieGateway
    middleware can scale horizontally by managing the lifecycle of multiple kernels
    instances, which themselves can either be local to the server or remote on a cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: Remote kernels must be Jupyter Kernel Gateways.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using the publishing wizard, the user can optionally define security for the application.
    Multiple options are available including Basic Authentication, OAuth 2.0, and
    Bearer Token.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The line of business users accesses the app from their browser using the URL from
    step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PixieGateway provides a comprehensive admin console for managing the server
    including configuring the applications, configuring and monitoring kernels, access
    to the logs for troubleshooting, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PixieGateway manages sessions for each active user and dispatches requests
    to the appropriate kernels for execution using the IPython messaging protocol
    ([http://jupyter-client.readthedocs.io/en/latest/messaging.html](http://jupyter-client.readthedocs.io/en/latest/messaging.html))
    over WebSocket or ZeroMQ depending on whether the Kernel is local or remote.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When productizing your analytics, this solution provides a major improvement
    over the classic three-tier web application architecture because it collapses
    the web and the data tier into one **web analytics tiers,** as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture for operationalizing data science analytics](img/B09699_02_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Comparison between classic three tiers and PixieGateway web architecture
  prefs: []
  type: TYPE_NORMAL
- en: In the classic three-tier architecture, developers have to maintain multiple
    REST endpoints that invoke the analytics in the data tier and massage the data
    to comply with the presentation tier requirements for correctly displaying the
    data. As a result, a lot of engineering has to be added to these endpoints, increasing
    the cost of development and code maintenance. In contrast, in the PixieGateway
    two-tier architecture, developers do not have to worry about creating endpoints
    because the server is responsible for dispatching the requests to the appropriate
    kernel using built-in generic endpoints. Explained another way, the PixieApp Python
    methods automatically become endpoints for the presentation tier without any code
    change. This model is conducive to rapid iterations since any change in the Python
    code is directly reflected in the application after republishing.
  prefs: []
  type: TYPE_NORMAL
- en: PixieApps are great to rapidly build single-page applications and dashboards.
    However, you may also want to generate simpler one-page reports and share them
    with your users. To that end, PixieGateway also lets you share charts generated
    by the `display()` API using the **Share** button, resulting in a URL linking
    to a web page containing the chart. In turn, a user can embed the chart into a
    website or a blog post by copying and pasting the code generated for the page.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: We''ll cover PixieGateway in details in [Chapter 4](ch04.xhtml "Chapter 4. Publish
    your Data Analysis to the Web - the PixieApp Tool"), *Publish your Data Analysis
    to the Web - the PixieApp Tool*, including how to install a new instance both
    locally and on the cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate this capability, let''s use the cars DataFrame created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture for operationalizing data science analytics](img/B09699_02_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Share Chart dialog
  prefs: []
  type: TYPE_NORMAL
- en: 'If sharing is successful, then the next page will show the generated URL and
    the code snippet to embed into a web application or blog post:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture for operationalizing data science analytics](img/B09699_02_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Confirmation of a shared chart
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the link will take you to the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture for operationalizing data science analytics](img/B09699_02_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Display chart as a web page
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed the reasons why our data science tooling strategy
    was centered around Python and Jupyter Notebook. We also introduced the PixieDust
    capabilities that improve user productivity with features such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data loading and cleaning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization and exploration without any coding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A simple programming model based on HTML and CSS, called PixieApp, for building
    tools and dashboards that interact directly with the Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A point and click mechanism to publish charts and PixieApp directly to the web
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we'll do a deep dive on the PixieApp programming model,
    discussing every aspect of the APIs with numerous code samples.
  prefs: []
  type: TYPE_NORMAL
