["```py\npip install nltk\n```", "```py\n!pip install nltk\n```", "```py\npip install spacy\n```", "```py\npython -m spacy download en\n```", "```py\n!pip install spacy\n!python -m spacy download en\n```", "```py\n# Input text\nparagraph=\"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n\n# Converting paragraph in lowercase \nprint(paragraph.lower()) \n```", "```py\ntaj mahal is one of the beautiful monuments. it is one of the wonders of the world. it was built by shah jahan in 1631 in memory of his third beloved wife mumtaj mahal.\n```", "```py\n# Loading NLTK module\nimport nltk\n\n# downloading punkt\nnltk.download('punkt')\n\n# downloading stopwords\nnltk.download('stopwords')\n\n# downloading wordnet\nnltk.download('wordnet')\n\n# downloading average_perception_tagger\nnltk.download('averaged_perceptron_tagger')\n```", "```py\n# Sentence Tokenization\nfrom nltk.tokenize import sent_tokenize\n\nparagraph=\"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n\ntokenized_sentences=sent_tokenize(paragraph)\nprint(tokenized_sentences)\n```", "```py\n['Taj Mahal is one of the beautiful monument.', 'It is one of the wonders of the world.', 'It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.']\n```", "```py\n# Import spacy\nimport spacy\n\n# Loading english language model \nnlp = spacy.load(\"en\")\n\n# Build the nlp pipe using 'sentencizer'\nsent_pipe = nlp.create_pipe('sentencizer')\n\n# Append the sentencizer pipe to the nlp pipeline\nnlp.add_pipe(sent_pipe)\nparagraph = \"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n\n# Create nlp Object to handle linguistic annotations in a documents.\nnlp_doc = nlp(paragraph)\n\n# Generate list of tokenized sentence\ntokenized_sentences = []\nfor sentence in nlp_doc.sents:\n    tokenized_sentences.append(sentence.text)\nprint(tokenized_sentences)\n```", "```py\n['Taj Mahal is one of the beautiful monument.', 'It is one of the wonders of the world.', 'It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.']\n```", "```py\n# Import nltk word_tokenize method\nfrom nltk.tokenize import word_tokenize\n\n# Split paragraph into words\ntokenized_words=word_tokenize(paragraph)\nprint(tokenized_words)\n```", "```py\n['Taj', 'Mahal', 'is', 'one', 'of', 'the', 'beautiful', 'monument', '.', 'It', 'is', 'one', 'of', 'the', 'wonders', 'of', 'the', 'world', '.', 'It', 'was', 'built', 'by', 'Shah', 'Jahan', 'in', '1631', 'in', 'memory', 'of', 'his', 'third', 'beloved', 'wife', 'Mumtaj', 'Mahal', '.']\n```", "```py\n# Import spacy\nimport spacy\n\n# Loading english language model \nnlp = spacy.load(\"en\")\n\nparagraph = \"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n\n# Create nlp Object to handle linguistic annotations in a documents.\nmy_doc = nlp(paragraph)\n\n# tokenize paragraph into words\ntokenized_words = []\nfor token in my_doc:\n    tokenized_words.append(token.text)\n\nprint(tokenized_words)\n```", "```py\n['Taj', 'Mahal', 'is', 'one', 'of', 'the', 'beautiful', 'monument', '.', 'It', 'is', 'one', 'of', 'the', 'wonders', 'of', 'the', 'world', '.', 'It', 'was', 'built', 'by', 'Shah', 'Jahan', 'in', '1631', 'in', 'memory', 'of', 'his', 'third', 'beloved', 'wife', 'Mumtaj', 'Mahal', '.']\n```", "```py\n# Import frequency distribution\nfrom nltk.probability import FreqDist\n\n# Find frequency distribution of paragraph\nfdist = FreqDist(tokenized_words)\n\n# Check top 5 common words\nfdist.most_common(5)\n```", "```py\n[('of', 4), ('the', 3), ('.', 3), ('Mahal', 2), ('is', 2)]\n```", "```py\n# Import matplotlib\nimport matplotlib.pyplot as plt\n\n# Plot Frequency Distribution\nfdist.plot(20, cumulative=False)\nplt.show()\n\n```", "```py\n# import the nltk stopwords\nfrom nltk.corpus import stopwords\n\n# Load english stopwords list\nstopwords_set=set(stopwords.words(\"english\"))\n\n# Removing stopwords from text\nfiltered_word_list=[]\nfor word in tokenized_words:\n    # filter stopwords\n    if word not in stopwords_set:\n        filtered_word_list.append(word)\n\n# print tokenized words\nprint(\"Tokenized Word List:\", tokenized_words)\n\n# print filtered words\nprint(\"Filtered Word List:\", filtered_word_list)\n```", "```py\nTokenized Word List: ['Taj', 'Mahal', 'is', 'one', 'of', 'the', 'beautiful', 'monuments', '.', 'It', 'is', 'one', 'of', 'the', 'wonders', 'of', 'the', 'world', '.', 'It', 'was', 'built', 'by', 'Shah', 'Jahan', 'in', '1631', 'in', 'memory', 'of', 'his', 'third', 'beloved', 'wife', 'Mumtaj', 'Mahal', '.']\n\nFiltered Word List: ['Taj', 'Mahal', 'one', 'beautiful', 'monuments', '.', 'It', 'one', 'wonders', 'world', '.', 'It', 'built', 'Shah', 'Jahan', '1631', 'memory', 'third', 'beloved', 'wife', 'Mumtaj', 'Mahal', '.']\n```", "```py\n# Import spacy\nimport spacy\n\n# Loading english language model \nnlp = spacy.load(\"en\")\n\n# text paragraph\nparagraph = \"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n\n# Create nlp Object to handle linguistic annotations in a documents.\nmy_doc = nlp(paragraph)\n\n# Removing stopwords from text\nfiltered_token_list = []\nfor token in my_doc:\n     # filter stopwords\n     if token.is_stop==False:\n         filtered_token_list.append(token)\n\nprint(\"Filtered Word List:\",filtered_token_list)\n```", "```py\nFiltered Sentence: [Taj, Mahal, beautiful, monument, ., wonders, world, ., built, Shah, Jahan, 1631, memory, beloved, wife, Mumtaj, Mahal, .]\n```", "```py\n# Import Lemmatizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n# Import Porter Stemmer\nfrom nltk.stem.porter import PorterStemmer\n\n# Create lemmatizer object\nlemmatizer = WordNetLemmatizer()\n\n# Create stemmer object\nstemmer = PorterStemmer()\n\n# take a sample word\nsample_word = \"crying\"\nprint(\"Lemmatized Sample Word:\", lemmatizer.lemmatize(sample_word, \"v\"))\n\nprint(\"Stemmed Sample Word:\", stemmer.stem(sample_word))\n```", "```py\nLemmatized Sample Word: cry\nStemmed Sample Word: cri\n```", "```py\n# Import english language model\nimport spacy\n\n# Loading english language model \nnlp = spacy.load(\"en\")\n\n# Create nlp Object to handle linguistic annotations in documents.\nwords = nlp(\"cry cries crying\")\n\n# Find lemmatized word\nfor w in words:\n    print('Original Word: ', w.text)\n    print('Lemmatized Word: ',w.lemma_)\n```", "```py\nOriginal Word:  cry\nLemmatized Word:  cry\nOriginal Word:  cries\nLemmatized Word:  cry\nOriginal Word:  crying\nLemmatized Word:  cry\n```", "```py\n# import Word Tokenizer and PoS Tagger\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\n\n# Sample sentence\nsentence = \"Taj Mahal is one of the beautiful monument.\"\n\n# Tokenize the sentence\nsent_tokens = word_tokenize(sentence)\n\n# Create PoS tags\nsent_pos = pos_tag(sent_tokens)\n\n# Print tokens with PoS\nprint(sent_pos)\n```", "```py\n[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('beautiful', 'JJ'), ('monument', 'NN'), ('.', '.')]\n```", "```py\n# Import spacy\nimport spacy\n\n# Loading small english language model \nnlp = spacy.load(\"en_core_web_sm\")\n\n# Create nlp Object to handle linguistic annotations in a documents.\nsentence = nlp(u\"Taj Mahal is one of the beautiful monument.\")\n\nfor token in sentence:\n  print(token.text, token.pos_)\n```", "```py\nTaj PROPN \nMahal PROPN \nis VERB \none NUM \nof ADP \nthe DET \nbeautiful ADJ \nmonument NOUN\n. PUNCT\n```", "```py\n# Import spacy\nimport spacy\n\n# Load English model for tokenizer, tagger, parser, and NER\nnlp = spacy.load('en')\n\n# Sample paragraph\nparagraph = \"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n\n# Create nlp Object to handle linguistic annotations in documents.\ndocs=nlp(paragraph)\nentities=[(i.text, i.label_) for i in docs.ents]\nprint(entities)\n```", "```py\n[('Taj Mahal', 'PERSON'), ('Shah Jahan', 'PERSON'), ('1631', 'DATE'), ('third', 'ORDINAL'), ('Mumtaj Mahal', 'PERSON')]\n```", "```py\n# Import display for visualizing the Entities\nfrom spacy import displacy\n\n# Visualize the entities using render function\ndisplacy.render(docs, style = \"ent\",jupyter = True)\n\n```", "```py\n# Import spacy\nimport spacy\n\n# Load English model for tokenizer, tagger, parser, and NER\nnlp = spacy.load('en')\n\n# Create nlp Object to handle linguistic annotations in a documents.\ndocs=nlp(sentence)\n\n# Visualize the using render function\ndisplacy.render(docs, style=\"dep\", jupyter= True, options={'distance': 150})\n```", "```py\npip install wordcloud\n```", "```py\nconda install -c conda-forge wordcloud\n```", "```py\n# importing all necessary modules\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\nimport matplotlib.pyplot as plt\n\nstopword_list = set(STOPWORDS)\n\nparagraph=\"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world. It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n```", "```py\nword_cloud = WordCloud(width = 550, height = 550,\nbackground_color ='white',\nstopwords = stopword_list,\nmin_font_size = 10).generate(paragraph)\n```", "```py\n0.# Visualize the WordCloud Plot\n# Set wordcloud figure size\nplt.figure(figsize = (8, 6))\n\n# Show image\nplt.imshow(word_cloud)\n\n# Remove Axis\nplt.axis(\"off\")\n\n# show plot\nplt.show()\n```", "```py\n# Import libraries\nimport pandas as pd\n\n# read the dataset\ndf=pd.read_csv('amazon_alexa.tsv', sep='\\t')\n\n# Show top 5-records\ndf.head()\n```", "```py\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Count plot\nsns.countplot(x='feedback', data=df)\n\n# Set X-axis and Y-axis labels\nplt.xlabel('Sentiment Score')\nplt.ylabel('Number of Records')\n\n# Show the plot using show() function\nplt.show()\n```", "```py\n# Import CountVectorizer and RegexTokenizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Create Regex tokenizer for removing special symbols and numeric values\nregex_tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n\n# Initialize CountVectorizer object\ncount_vectorizer = CountVectorizer(lowercase=True,\nstop_words='english',\nngram_range = (1,1),\ntokenizer = regex_tokenizer.tokenize)\n\n# Fit and transform the dataset\ncount_vectors = count_vectorizer.fit_transform( df['verified_reviews'])\n```", "```py\n# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Partition data into training and testing set\nfeature_train, feature_test, target_train, target_test = train_test_split(count_vectors, df['feedback'], test_size=0.3, random_state=1)\n```", "```py\n# import logistic regression scikit-learn model\nfrom sklearn.linear_model import LogisticRegression\n\n# Create logistic regression model object\nlogreg = LogisticRegression(solver='lbfgs')\n\n# fit the model with data\nlogreg.fit(feature_train,target_train)\n\n# Forecast the target variable for given test dataset\npredictions = logreg.predict(feature_test)\n```", "```py\n# Import metrics module for performance evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n# Assess model performance using accuracy measure\nprint(\"Logistic Regression Model Accuracy:\",accuracy_score(target_test, predictions))\n\n# Calculate model precision\nprint(\"Logistic Regression Model Precision:\",precision_score(target_test, predictions))\n\n# Calculate model recall\nprint(\"Logistic Regression Model Recall:\",recall_score(target_test, predictions))\n\n# Calculate model f1 score\nprint(\"Logistic Regression Model F1-Score:\",f1_score(target_test, predictions))\n```", "```py\nLogistic Regression Model Accuracy: 0.9428571428571428\nLogistic Regression Model Precision: 0.952433628318584\nLogistic Regression Model Recall: 0.9873853211009175\nLogistic Regression Model F1-Score: 0.9695945945945945\n```", "```py\n# Import libraries\nimport pandas as pd\n\n# read the dataset\ndf=pd.read_csv('amazon_alexa.tsv', sep='\\t')\n\n# Show top 5-records\ndf.head()\n```", "```py\n# Import TfidfVectorizer and RegexTokenizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create Regex tokenizer for removing special symbols and numeric values\nregex_tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n\n# Initialize TfidfVectorizer object\ntfidf = TfidfVectorizer(lowercase=True, stop_words ='english',ngram_range = (1,1),tokenizer = regex_tokenizer.tokenize)\n\n# Fit and transform the dataset\ntext_tfidf = tfidf.fit_transform(df['verified_reviews'])\n```", "```py\n# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Partition data into training and testing set\nfrom sklearn.model_selection import train_test_split\n\nfeature_train, feature_test, target_train, target_test = train_test_split(text_tfidf, df['feedback'], test_size=0.3, random_state=1)\n\n```", "```py\n# import logistic regression scikit-learn model\nfrom sklearn.linear_model import LogisticRegression\n\n# instantiate the model\nlogreg = LogisticRegression(solver='lbfgs')\n\n# fit the model with data\nlogreg.fit(feature_train,target_train)\n\n# Forecast the target variable for given test dataset\npredictions = logreg.predict(feature_test)\n```", "```py\n# Import metrics module for performance evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n# Assess model performance using accuracy measure\nprint(\"Logistic Regression Model Accuracy:\",accuracy_score(target_test, predictions))\n\n# Calculate model precision\nprint(\"Logistic Regression Model Precision:\",precision_score(target_test, predictions))\n\n# Calculate model recall\nprint(\"Logistic Regression Model Recall:\",recall_score(target_test, predictions))\n\n# Calculate model f1 score\nprint(\"Logistic Regression Model F1-Score:\",f1_score(target_test, predictions))\n\n```", "```py\nLogistic Regression Model Accuracy: 0.9238095238095239\nLogistic Regression Model Precision: 0.923728813559322\nLogistic Regression Model Recall: 1.0\nLogistic Regression Model F1-Score: 0.960352422907489\n\n```", "```py\n# Import spacy\nimport spacy\n\n# Load English model for tokenizer, tagger, parser, and NER\nnlp = spacy.load('en')\n\n# Create documents\ndoc1 = nlp(u'I love pets.')\ndoc2 = nlp(u'I hate pets')\n\n# Find similarity\nprint(doc1.similarity(doc2))\n```", "```py\n0.724494176985974\n<ipython-input-32-f157deaa344d>:12: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n```", "```py\ndef jaccard_similarity(sent1, sent2):\n    \"\"\"Find text similarity using jaccard similarity\"\"\"\n    # Tokenize sentences\n    token1 = set(sent1.split())\n    token2 = set(sent2.split())\n\n    # intersection between tokens of two sentences    \n    intersection_tokens = token1.intersection(token2)\n\n    # Union between tokens of two sentences\n    union_tokens=token1.union(token2)\n\n    # Cosine Similarity\n    sim_= float(len(intersection_tokens) / len(union_tokens))\n    return sim_\n\njaccard_similarity('I love pets.','I hate pets.')\n```", "```py\n0.5\n```", "```py\n# Let's import text feature extraction TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndocs=['I love pets.','I hate pets.']\n\n# Initialize TfidfVectorizer object\ntfidf= TfidfVectorizer()\n\n# Fit and transform the given data\ntfidf_vector = tfidf.fit_transform(docs)\n\n# Import cosine_similarity metrics\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# compute similarity using cosine similarity\ncos_sim=cosine_similarity(tfidf_vector, tfidf_vector)\n\nprint(cos_sim)\n```", "```py\n[[1\\. 0.33609693]\n[0.33609693 1\\. ]]\n```"]