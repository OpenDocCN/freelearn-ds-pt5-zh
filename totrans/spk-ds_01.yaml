- en: Chapter 1.  Big Data and Data Science – An Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Big data is definitely a big deal!* It promises a wealth of opportunities
    by deriving hidden insights in huge data silos and by opening new avenues to excel
    in business. Leveraging **big data** through advanced analytics techniques has
    become a no-brainer for organizations to create and maintain their competitive
    advantage.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explains what big data is all about, the various challenges with
    big data analysis and how **Apache Spark** pitches in as the de facto standard
    to address computational challenges and also serves as a data science platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Big data overview - what is all the fuss about?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges with big data analytics - why was it so difficult?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolution of big data analytics - the data analytics trend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark for data analytics - the solution to big data challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spark stack - all that makes it up for a complete big data solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much has already been spoken and written about what big data is, but there is
    no specific standard as such to clearly define it. It is actually a relative term
    to some extent. Whether small or big, your data can be leveraged only if you can
    analyze it properly. To make some sense out of your data, the right set of analysis
    techniques is needed and selecting the right tools and techniques is of utmost
    importance in data analytics. However, when the data itself becomes a part of
    the problem and the computational challenges need to be addressed prior to performing
    data analysis, it becomes a big data problem.
  prefs: []
  type: TYPE_NORMAL
- en: A revolution took place in the World Wide Web, also referred to as Web 2.0,
    which changed the way people used the Internet. Static web pages became interactive
    websites and started collecting more and more data. Technological advancements
    in cloud computing, social media, and mobile computing created an explosion of
    data. Every digital device started emitting data and many other sources started
    driving the data deluge. The dataflow from every nook and corner generated varieties
    of voluminous data, at speed! The formation of big data in this fashion was a
    natural phenomenon, because this is how the World Wide Web had evolved and no
    explicit efforts were involved in specifics. This is about the past! If you consider
    the change that is happening now, and is going to happen in future, the volume
    and speed of data generation is beyond what one can anticipate. I am propelled
    to make such a statement because every device is getting smarter these days, thanks
    to the **Internet of Things** (**IoT**).
  prefs: []
  type: TYPE_NORMAL
- en: The IT trend was such that the technological advancements also facilitated the
    data explosion. Data storage had experienced a paradigm shift with the advent
    of cheaper clusters of online storage pools and the availability of commodity
    hardware with bare minimal price. Storing data from disparate sources in its native
    form in a single data lake was rapidly gaining over carefully designed data marts
    and data warehouses. Usage patterns also shifted from rigid schema-driven, RDBMS-based
    approaches to schema-less, continuously available **NoSQL** data-store-driven
    solutions. As a result, the rate of data creation, whether structured, semi-structured,
    or unstructured, started accelerating like never before.
  prefs: []
  type: TYPE_NORMAL
- en: Organizations are very much convinced that not only can specific business questions
    be answered by leveraging big data; it also brings in opportunities to cover the
    uncovered possibilities in businesses and address the uncertainties associated
    with this. So, apart from the natural data influx, organizations started devising
    strategies to generate more and more data to maintain their competitive advantages
    and to be future ready. Here, an example would help to understand this better.
    Imagine sensors are installed on the machines of a manufacturing plant which are
    constantly emitting data, and hence the status of the machine parts, and a company
    is able to predict when the machine is going to fail. It lets the company prevent
    a failure or damage and avoid unplanned downtime, saving a lot of money.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with big data analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are broadly two types of formidable challenges in the analysis of big
    data. The first challenge is the requirement for a massive computation platform,
    and once it is in place, the second challenge is to analyze and make sense out
    of huge data at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Computational challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the increase in data, the storage requirement for big data also grew more
    and more. Data management became a cumbersome task. The latency involved in accessing
    the disk storage due to the seek time became the major bottleneck even though
    the processing speed of the processor and the frequency of RAM were up to the
    mark.
  prefs: []
  type: TYPE_NORMAL
- en: Fetching structured and unstructured data from across the gamut of business
    applications and data silos, consolidating them, and processing them to find useful
    business insights was challenging. There were only a few applications that could
    address any one area, or just a few areas of diversified business requirement.
    However, integrating those applications to address most of the business requirements
    in a unified way only increased the complexity.
  prefs: []
  type: TYPE_NORMAL
- en: To address these challenges, people turned to the distributed computing framework
    with distributed file system, for example, Hadoop and **Hadoop Distributed File
    System** (**HDFS**). This could eliminate the latency due to disk I/O, as the
    data could be read in parallel across the cluster of machines.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed computing technologies had existed for decades before, but gained
    more prominence only after the importance of big data was realized in the industry.
    So, technology platforms such as Hadoop and HDFS or Amazon S3 became the industry
    standard. On top of Hadoop, many other solutions such as Pig, Hive, Sqoop, and
    others were developed to address different kinds of industry requirements such
    as storage, **Extract, Transform, and Load** (**ETL**), and data integration to
    make Hadoop a unified platform.
  prefs: []
  type: TYPE_NORMAL
- en: Analytical challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analyzing data to find some hidden insights has always been challenging because
    of the additional intricacies involved in dealing with huge datasets. The traditional
    BI and OLAP solutions could not address most of the challenges that arose due
    to big data. As an example, if there were multiple dimensions to a dataset, say
    100, it got really difficult to compare these variables with one another to draw
    a conclusion because there would be around 100C2 combinations for it. Such cases
    required statistical techniques such as *correlation* and the like to find the
    hidden patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Though there were statistical solutions to many problems, it got really difficult
    for data scientists or analytics professionals to slice and dice the data to find
    intelligent insights unless they loaded the entire dataset into a **DataFrame**
    in memory. The major roadblock was that most of the general-purpose algorithms
    for statistical analysis and machine learning were single-threaded and written
    at a time when datasets were usually not so huge and could fit in the RAM on a
    single computer. Those algorithms written in R or Python were no longer very useful
    in their native form to be deployed on a distributed computing environment because
    of the limitation of in-memory computation.
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, statisticians and computer scientists had to work
    together to rewrite most of the algorithms that would work well in a distributed
    computing environment. Consequently, a library called **Mahout** for machine learning
    algorithms was developed on Hadoop for parallel processing. It had most of the
    common algorithms that were being used most often in the industry. Similar initiatives
    were taken for other distributed computing frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of big data analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section outlined how the computational and data analytics challenges
    were addressed for big data requirements. It was possible because of the convergence
    of several related trends such as low-cost commodity hardware, accessibility to
    big data, and improved data analytics techniques. Hadoop became a cornerstone
    in many large, distributed data processing infrastructures.
  prefs: []
  type: TYPE_NORMAL
- en: However, people soon started realizing the limitations of Hadoop. Hadoop solutions
    were best suited for only specific types of big data requirements such as ETL;
    it gained popularity for such requirements only.
  prefs: []
  type: TYPE_NORMAL
- en: There were scenarios when data engineers or analysts had to perform ad hoc queries
    on the data sets for interactive data analysis. Every time they ran a query on
    Hadoop, the data was read from the disk (HDFS-read) and loaded into the memory
    - which was a costly affair. Effectively, jobs were running at the speed of I/O
    transfers over the network and cluster of disks, instead of the speed of CPU and
    RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a pictorial representation of the scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evolution of big data analytics](img/image_01_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One more case where Hadoop's MapReduce model could not fit in well was with
    machine learning algorithms that were iterative in nature. Hadoop MapReduce was
    underperforming, with huge latency in iterative computation. Since MapReduce had
    a restricted programming model with forbidden communication between Map and Reduce
    workers, the intermediate results needed to be stored in a stable storage. So,
    those were pushed on to the HDFS, which in turn writes into the instead of saving
    in RAM and then loading back in the memory for the subsequent iteration, similarly
    for the rest of the iterations. The number of disk I/O was dependent on the number
    of iterations involved in an algorithm and this was topped with the serialization
    and deserialization overhead while saving and loading the data. Overall, it was
    computationally expensive and could not get the level of popularity compared to
    what was expected of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a pictorial representation of this scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evolution of big data analytics](img/image_01_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To address this, tailor-made solutions were developed, for example, Google's
    Pregel, which was an iterative graph processing algorithm and was optimized for
    inter-process communication and in-memory storage for the intermediate results
    to make it run faster. Similarly, many other solutions were developed or redesigned
    that would best suit some of the specific needs that the algorithms used were
    designed for.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of redesigning all the algorithms, a general-purpose engine was needed
    that could be leveraged by most of the algorithms for in-memory computation on
    a distributed computing platform. It was also expected that such a design would
    result in faster execution of iterative computation and ad hoc data analysis.
    This is how the Spark project paved its way out at the AMPLab at UC Berkeley.
  prefs: []
  type: TYPE_NORMAL
- en: Spark for data analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Soon after the Spark project was successful in the AMP labs, it was made open
    source in 2010 and transferred to the Apache Software Foundation in 2013\. It
    is currently being led by Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark offers many distinct advantages over other distributed computing platforms,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: A faster execution platform for both iterative machine learning and interactive
    data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single stack for batch processing, SQL queries, real-time stream processing,
    graph processing, and complex data analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides high-level API to develop a diverse range of distributed applications
    by hiding the complexities of distributed programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seamless support for various data sources such as RDBMS, HBase, Cassandra, Parquet,
    MongoDB, HDFS, Amazon S3, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Spark for data analytics](img/image_01_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is a pictorial representation of in-memory data sharing for iterative
    algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark for data analytics](img/image_01_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Spark hides the complexities in writing the core MapReduce jobs and provides
    most of the functionalities through simple function calls. Because of its simplicity,
    it is able to cater to wider and bigger audience groups such as data scientists,
    data engineers, statisticians, and R/Python/Scala/Java developers.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark architecture broadly consists of a data storage layer, management
    framework, and API. It is designed to work on top of an HDFS filesystem, and thereby
    leverages the existing ecosystem. Deployment could be as a standalone server or
    on distributed computing frameworks such as Apache Mesos or YARN. An API is provided
    for Scala, the language in which Spark is written, along with Java, R and Python.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark is a general-purpose cluster computing system that empowers other higher-level
    components to leverage its core engine. It is interoperable with Apache Hadoop,
    in the sense that it can read and write data from/to HDFS and can also integrate
    with other storage systems that are supported by the Hadoop API.
  prefs: []
  type: TYPE_NORMAL
- en: While it allows building other higher-level applications on top of it, it already
    has a few components built on top that are tightly integrated with its core engine
    to take advantage of the future enhancements at the core. These applications come
    bundled with Spark to cover the broader sets of requirements in the industry.
    Most of the real-world applications need to be integrated across projects to solve
    specific business problems that usually have a set of requirements. This is eased
    out with Apache Spark as it allows its higher level components to be seamlessly
    integrated, such as libraries in a development project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, with Spark''s built-in support for Scala, Java, R and Python, a broader
    range of developers and data engineers are able to leverage the entire Spark stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Spark stack](img/image_01_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Spark core
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spark core, in a way, is similar to the kernel of an operating system. It
    is the general execution engine, which is fast as well as fault tolerant. The
    entire Spark ecosystem is built on top of this core engine. It is mainly designed
    to do job scheduling, task distribution, and monitoring of jobs across worker
    nodes. It is also responsible for memory management, interacting with various
    heterogeneous storage systems, and various other operations.
  prefs: []
  type: TYPE_NORMAL
- en: The primary building block of Spark core is the **Resilient Distributed Dataset**
    (**RDD**), which is an immutable, fault-tolerant collection of elements. Spark
    can create RDDs from a variety of data sources such as HDFS, local filesystems,
    Amazon S3, other RDDs, NoSQL data stores such as Cassandra, and so on. They are
    resilient in the sense that they automatically rebuild on failure. RDDs are built
    through lazy parallel transformations. They may be cached and partitioned, and
    may or may not be materialized.
  prefs: []
  type: TYPE_NORMAL
- en: The entire Spark core engine may be viewed as a set of simple operations on
    distributed datasets. All the scheduling and execution of jobs in Spark is done
    based on the methods associated with each RDD. Also, the methods associated with
    each RDD define their own ways of distributed in-memory computation.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This module of Spark is designed to query, analyze, and perform operations on
    structured data. This is a very important component in the entire Spark stack
    because of the fact that most of the organizational data is structured, though
    unstructured data is growing rapidly. Acting as a distributed query engine, it
    enables Hadoop Hive queries to run up to 100 times faster on it without any modification.
    Apart from Hive, it also supports Apache Parquet, an efficient columnar storage,
    JSON, and other structured data formats. Spark SQL enables running SQL queries
    along with complex programs written in Python, Scala, and Java.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL provides a distributed programming abstraction called **DataFrames**,
    referred to as SchemaRDD before, which had fewer functions associated with it.
    DataFrames are distributed collections of named columns, analogous to SQL tables
    or Python's Pandas DataFrames. They can be constructed with a variety of data
    sources that have schemas with them such as Hive, Parquet, JSON, other RDBMS sources,
    and also from Spark RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL can be used for ETL processing across different formats and then running
    ad hoc analysis. Spark SQL comes with an optimizer framework called Catalyst that
    can transform SQL queries for better efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Spark streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The processing window for the enterprise data is becoming shorter than ever.
    To address the real-time processing requirement of the industry, this component
    of Spark was designed, which is fault tolerant as well as scalable. Spark enables
    real-time data analytics on live streams of data by supporting data analysis,
    machine learning, and graph processing on them.
  prefs: []
  type: TYPE_NORMAL
- en: It provides an API called **Discretised Stream** (**DStream**) to manipulate
    the live streams of data. The live streams of data are sliced up into small batches
    of, say, *x* seconds. Spark treats each batch as an RDD and processes them as
    basic RDD operations. DStreams can be created out of live streams of data from
    HDFS, Kafka, Flume, or any other source which can stream data on the TCP socket.
    By applying some higher-level operations on DStreams, other DStreams can be produced.
  prefs: []
  type: TYPE_NORMAL
- en: The final result of Spark streaming can either be written back to the various
    data stores supported by Spark or can be pushed to any dashboard for visualization.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLlib is the built-in machine learning library in the Spark stack. This was
    introduced in Spark 0.8\. Its goal is to make machine learning scalable and easy.
    Developers can seamlessly use Spark SQL, Spark Streaming, and GraphX in their
    programming language of choice, be it Java, Python, or Scala. MLlib provides the
    necessary functions to perform various statistical analyses such as correlations,
    sampling, hypothesis testing, and so on. This component also has a broad coverage
    of applications and algorithms in classification, regression, collaborative filtering,
    clustering, and decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: The machine learning workflow involves collecting and preprocessing data, building
    and deploying the model, evaluating the results, and refining the model. In the
    real world, the preprocessing steps take up significant effort. These are typically
    multi-stage workflows involving expensive intermediate read/write operations.
    Often, these processing steps may be performed multiple times over a period of
    time. A new concept called **ML Pipelines** was introduced to streamline these
    preprocessing steps. A Pipeline is a sequence of transformations where the output
    of one stage is the input of another, forming a chain. The ML Pipeline leverages
    Spark and MLlib and enables developers to define reusable sequences of transformations.
  prefs: []
  type: TYPE_NORMAL
- en: GraphX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GraphX is a thin-layered unified graph analytics framework on Spark. It was
    designed to be a general-purpose distributed dataflow framework in place of specialized
    graph processing frameworks. It is fault tolerant and also exploits in-memory
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: '**GraphX** is an embedded graph processing API for manipulating graphs (for
    example, social networks) and to do graph parallel computation (for example, Google''s
    Pregel). It combines the advantages of both graph-parallel and data-parallel systems
    on the Spark stack to unify exploratory data analysis, iterative graph computation,
    and ETL processing. It extends the RDD abstraction to introduce the **Resilient
    Distributed Graph** (**RDG**), which is a directed graph with properties associated
    to each of its vertices and edges.'
  prefs: []
  type: TYPE_NORMAL
- en: GraphX includes a decently large collection of graph algorithms, such as PageRank,
    K-Core, Triangle Count, LDA, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: SparkR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The SparkR project was started to integrate the statistical analysis and machine
    learning capability of R with the scalability of Spark. It addressed the limitation
    of R, which was its ability to process as much data as fitted in the memory of
    a single machine. R programs can now scale in a distributed setting through SparkR.
  prefs: []
  type: TYPE_NORMAL
- en: SparkR is actually an R Package that provides an R shell to leverage Spark's
    distributed computing engine. With R's rich set of built-in packages for data
    analytics, data scientists can analyze large datasets interactively at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we briefly covered what big data is all about. We then discussed
    the computational and analytical challenges involved in big data analytics. Later,
    we looked at how the analytics space in the context of big data has evolved over
    a period of time and what the trend has been. We also covered how Spark addressed
    most of the big data analytics challenges and became a general-purpose unified
    analytics platform for data science as well as parallel computation. At the end
    of this chapter, we just gave you a heads-up on the Spark stack and its components.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the Spark programming model. We will
    take a deep dive into the basic building block of Spark, which is the RDD. Also,
    we will learn how to program with the RDD API on Scala and Python.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Spark overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/spark/about](https://databricks.com/spark/about)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Spark architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
