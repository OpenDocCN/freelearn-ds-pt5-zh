["```py\n[['None', 'Cold', 'No'], ['None', 'Warm', 'No'], ['Small', 'Cold', 'No'], ['Small', 'Warm', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Warm', 'Yes']]\n```", "```py\n[['None', 'Warm', 'No'], ['None', 'Warm', 'No'], ['Small', 'Cold', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No']]\n```", "```py\n[['Good', 'Warm', 'Yes'], ['None', 'Warm', 'No'], ['Good', 'Cold', 'No'], ['None', 'Cold', 'No'], ['None', 'Warm', 'No'], ['Small', 'Warm', 'No']]\n```", "```py\nTree 0:\n    Root\n    ├── [swimming_suit=Small]\n    │ └── [swim=No]\n    ├── [swimming_suit=None]\n    │ └── [swim=No]\n    └── [swimming_suit=Good]\n      └── [swim=No]\nTree 1:\n    Root\n    ├── [swimming_suit=Small]\n    │ └── [swim=No]\n    ├── [swimming_suit=None]\n    │ ├── [water_temperature=Cold]\n    │ │ └── [swim=No]\n    │ └──[water_temperature=Warm]\n    │   └── [swim=No]\n    └── [swimming_suit=Good]\n      ├──  [water_temperature=Cold] \n      │ └── [swim=No]\n      └──  [water_temperature=Warm]\n        └── [swim=Yes]\nThe total number of trees in the random forest=2.\nThe maximum number of the variables considered at the node is m=3.\n```", "```py\n# source_code/4/random_forest.py import mathimport randomimport sys\nsys.path.append('../common')\nimport common # noqa\nimport decision_tree # noqa\nfrom common import printfv # noqa\n\n#Random forest construction\ndef sample_with_replacement(population, size):\n    sample = []\n    for i in range(0, size):\n        sample.append(population[random.randint(0, len(population) - 1)])\n    return sample\n\ndef construct_random_forest(verbose, heading, complete_data,\n enquired_column, m, tree_count):\n    printfv(2, verbose, \"*** Random Forest construction ***\\n\")\n    printfv(2, verbose, \"We construct a random forest that will \" +\n            \"consist of %d random decision trees.\\n\", tree_count)\n    random_forest = []\n    for i in range(0, tree_count):\n        printfv(2, verbose, \"\\nConstruction of a random \" +\n                \"decision tree number %d:\\n\", i)\n        random_forest.append(construct_random_decision_tree(\n            verbose, heading, complete_data, enquired_column, m))\n    printfv(2, verbose, \"\\nTherefore we have completed the \" +\n            \"construction of the random forest consisting of %d \" +\n            \"random decision trees.\\n\", tree_count)\n    return random_forest\n\ndef construct_random_decision_tree(verbose, heading, complete_data,\n enquired_column, m):\n    sample = sample_with_replacement(complete_data, len(complete_data))\n    printfv(2, verbose, \"We are given %d features as the input data. \" +\n            \"Out of these, we choose randomly %d features with the \" +\n            \"replacement that we will use for the construction of \" +\n            \"this particular random decision tree:\\n\" +\n            str(sample) + \"\\n\", len(complete_data),\n            len(complete_data))\n# The function construct_general_tree from the module decision_tree\n# is written in the implementation section in the previous chapter\n# on decision trees.\n    return decision_tree.construct_general_tree(verbose, heading,\n                                                sample,\n                                                enquired_column, m)\n\n# M is the given number of the decision variables, i.e. properties\n# of one feature.\ndef choose_m(verbose, M):\n    m = int(min(M, math.ceil(2 * math.sqrt(M))))\n    printfv(2, verbose, \"We are given M=\" + str(M) +\n            \" variables according to which a feature can be \" +\n            \"classified. \")\n    printfv(3, verbose, \"In random forest algorithm we usually do \" +\n            \"not use all \" + str(M) + \" variables to form tree \" +\n            \"branches at each node. \")\n    printfv(3, verbose, \"We use only m variables out of M. \")\n    printfv(3, verbose, \"So we choose m such that m is less than or \" +\n            \"equal to M. \")\n    printfv(3, verbose, \"The greater m is, a stronger classifier an \" +\n            \"individual tree constructed is. However, it is more \" +\n            \"susceptible to a bias as more of the data is considered. \" +\n            \"Since we in the end use multiple trees, even if each may \" +\n            \"be a weak classifier, their combined classification \" +\n            \"accuracy is strong. Therefore as we want to reduce a \" +\n            \"bias in a random forest, we may want to consider to \" +\n            \"choose a parameter m to be slightly less than M.\\n\")\n    printfv(2, verbose, \"Thus we choose the maximum number of the \" +\n            \"variables considered at the node to be \" +\n            \"m=min(M,math.ceil(2*math.sqrt(M)))\" +\n            \"=min(M,math.ceil(2*math.sqrt(%d)))=%d.\\n\", M, m)\n    return m\n\n#Classification\ndef display_classification(verbose, random_forest, heading,\n enquired_column, incomplete_data):\n    if len(incomplete_data) == 0:\n        printfv(0, verbose, \"No data to classify.\\n\")\n    else:\n        for incomplete_feature in incomplete_data:\n            printfv(0, verbose, \"\\nFeature: \" +\n                    str(incomplete_feature) + \"\\n\")\n            display_classification_for_feature(\n                verbose, random_forest, heading,\n                enquired_column, incomplete_feature)\n\ndef display_classification_for_feature(verbose, random_forest, heading,\n enquired_column, feature):\n    classification = {}\n    for i in range(0, len(random_forest)):\n        group = decision_tree.classify_by_tree(\n            random_forest[i], heading, enquired_column, feature)\n        common.dic_inc(classification, group)\n        printfv(0, verbose, \"Tree \" + str(i) +\n                \" votes for the class: \" + str(group) + \"\\n\")\n    printfv(0, verbose, \"The class with the maximum number of votes \" +\n            \"is '\" + str(common.dic_key_max_count(classification)) +\n            \"'. Thus the constructed random forest classifies the \" +\n            \"feature \" + str(feature) + \" into the class '\" +\n            str(common.dic_key_max_count(classification)) + \"'.\\n\")\n\n# Program start\ncsv_file_name = sys.argv[1]\ntree_count = int(sys.argv[2])\nverbose = int(sys.argv[3])\n\n(heading, complete_data, incomplete_data,\n enquired_column) = common.csv_file_to_ordered_data(csv_file_name)\nm = choose_m(verbose, len(heading))\nrandom_forest = construct_random_forest(\n    verbose, heading, complete_data, enquired_column, m, tree_count)\ndisplay_forest(verbose, random_forest)\ndisplay_classification(verbose, random_forest, heading,\n                       enquired_column, incomplete_data)\n```", "```py\n# source_code/4/swim.csv\nswimming_suit,water_temperature,swim\nNone,Cold,No\nNone,Warm,No\nSmall,Cold,No\nSmall,Warm,No\nGood,Cold,No\nGood,Warm,Yes\nGood,Cold,?\n```", "```py\n$ python random_forest.py swim.csv 2 3 > swim.out\n```", "```py\n[['Cold', 'Strong', 'Cloudy', 'No'], ['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'None', 'Sunny',\n'Yes'], ['Hot', 'None', 'Sunny', 'No'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Warm', 'Breeze',\n'Sunny', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'None', 'Sunny', 'Yes'], ['Hot', 'Strong', 'Cloudy', 'Yes'], ['Warm', 'None', 'Cloudy', 'Yes']]\n```", "```py\n[['Warm', 'Strong', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'None', 'Sunny', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Warm', 'Strong', 'Cloudy', 'No'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Warm', 'Breeze', 'Sunny', 'Yes']]\n```", "```py\nTree 0:\nRoot\n├── [Temperature=Cold]\n│ ├── [Wind=None]\n│ │ └── [Play=Yes]\n│ └── [Wind=Breeze]\n│   └── [Play=No]\n├── [Temperature=Warm]\n│ ├── [Wind=Breeze]\n│ │ └── [Play=Yes]\n│ └── [Wind=Strong]\n│   └── [Play=No]\n└── [Temperature=Hot]\n  └── [Play=Yes]\n\nTree 1:\nRoot\n├── [Wind=Breeze]\n│ └── [Play=No]\n├── [Wind=None]\n│ ├── [Temperature=Cold]\n│ │ └── [Play=Yes]\n│ ├── [Temperature=Warm]\n│ │ ├── [Sunshine=Sunny]\n│ │ │ └── [Play=Yes]\n│ │ └── [Sunshine=Cloudy]\n│ │   └── [Play=Yes]\n│ └── [Temperature=Hot]\n│   └── [Play=No]\n└── [Wind=Strong]\n    ├── [Temperature=Cold]\n    │ └── [Play=No]\n    └── [Temperature=Warm]\n        └── [Play=No]\n\nTree 2: \n    Root\n    ├── [Wind=Strong]\n    │ └── [Play=No]\n    ├── [Wind=None]\n    │ ├── [Temperature=Cold]\n    │ │ └── [Play=Yes]\n    │ └── [Temperature=Warm]\n    │   └── [Play=Yes]\n    └── [Wind=Breeze]\n      ├── [Temperature=Hot]\n      │ └── [Play=Yes]\n      └── [Temperature=Warm]\n        └── [Play=Yes]\n\nTree 3:\n    Root\n    ├── [Temperature=Cold]\n    │ └── [Play=No]\n    ├── [Temperature=Warm]\n    │ ├──[Wind=Strong]\n    │ │ └──[Play=No]\n    │ ├── [Wind=None]\n    │ │ └── [Play=Yes]\n    │ └──[Wind=Breeze]\n    │   └── [Play=Yes]\n    └── [Temperature=Hot]\n      ├── [Wind=Strong]\n      │ └── [Play=Yes]\n      └── [Wind=Breeze]\n        └── [Play=Yes]\nThe total number of trees in the random forest=4.\nThe maximum number of the variables considered at the node is m=4.\n```", "```py\n# source_code/4/chess.csv Temperature,Wind,Sunshine,Play  \nCold,Strong,Cloudy,No  \nWarm,Strong,Cloudy,No  \nWarm,None,Sunny,Yes  \nHot,None,Sunny,No  \nHot,Breeze,Cloudy,Yes  \nWarm,Breeze,Sunny,Yes  \nCold,Breeze,Cloudy,No  \nCold,None,Sunny,Yes  \nHot,Strong,Cloudy,Yes  \nWarm,None,Cloudy,Yes  \nWarm,Strong,Sunny,? \n```", "```py\n$ python random_forest.py chess.csv 4 2 > chess.out\n```", "```py\n# source_code/4/shopping.csv Temperature,Rain,Shopping  \nCold,None,Yes  \nWarm,None,No  \nCold,Strong,Yes  \nCold,None,No  \nWarm,Strong,No  \nWarm,None,Yes \nCold,None,? \n```", "```py\n$ python random_forest.py shopping.csv 20 0\n***Classification***\nFeature: ['Cold', 'None', '?'] \nTree 0 votes for the class: Yes \nTree 1 votes for the class: No \nTree 2 votes for the class: No \nTree 3 votes for the class: No \nTree 4 votes for the class: No \nTree 5 votes for the class: Yes \nTree 6 votes for the class: Yes \nTree 7 votes for the class: Yes \nTree 8 votes for the class: No \nTree 9 votes for the class: Yes \nTree 10 votes for the class: Yes \nTree 11 votes for the class: Yes \nTree 12 votes for the class: Yes \nTree 13 votes for the class: Yes \nTree 14 votes for the class: Yes \nTree 15 votes for the class: Yes \nTree 16 votes for the class: Yes \nTree 17 votes for the class: No \nTree 18 votes for the class: No \nTree 19 votes for the class: No\nThe class with the maximum number of votes is 'Yes'. Thus the constructed random forest classifies the feature ['Cold', 'None', '?'] into the class 'Yes'.\n\n```", "```py\nsource_code/4/chess_with_seasons.csv Temperature,Wind,Season,Play  \nCold,Strong,Winter,No  \nWarm,Strong,Autumn,No  \nWarm,None,Summer,Yes  \nHot,None,Spring,No  \nHot,Breeze,Autumn,Yes  \nWarm,Breeze,Spring,Yes  \nCold,Breeze,Winter,No  \nCold,None,Spring,Yes  \nHot,Strong,Summer,Yes  \nWarm,None,Autumn,Yes  \nWarm,Strong,Spring,? \n```", "```py\n$ python random_forest.py chess_with_seasons.csv 4 2 > chess_with_seasons.out\n```", "```py\nTree 0:\n    Root\n    ├── [Wind=None]\n    │ ├── [Temperature=Cold]\n    │ │ └── [Play=Yes]\n    │ └── [Temperature=Warm]\n    │   ├── [Season=Autumn]\n    │   │ └── [Play=Yes]\n    │   └── [Season=Summer]\n    │     └── [Play=Yes]\n    └── [Wind=Strong]\n      ├── [Temperature=Cold]\n      │ └── [Play=No]\n      └── [Temperature=Warm]\n        └── [Play=No]\nTree 1:\n    Root\n    ├── [Season=Autumn]\n    │ ├──[Wind=Strong]\n    │ │ └──[Play=No]\n    │ ├── [Wind=None]\n    │ │ └── [Play=Yes]\n    │ └──[Wind=Breeze]\n    │   └── [Play=Yes]\n    ├── [Season=Summer]\n    │   └── [Play=Yes]\n    ├── [Season=Winter]\n    │   └── [Play=No]\n    └── [Season=Spring]\n      ├── [Temperature=Cold]\n      │ └── [Play=Yes]\n      └── [Temperature=Warm]\n        └── [Play=Yes]\n\nTree 2:\n    Root\n    ├── [Season=Autumn]\n    │ ├── [Temperature=Hot]\n    │ │ └── [Play=Yes]\n    │ └── [Temperature=Warm]\n    │   └── [Play=No]\n    ├── [Season=Spring]\n    │ ├── [Temperature=Cold]\n    │ │ └── [Play=Yes]\n    │ └── [Temperature=Warm]\n    │   └── [Play=Yes]\n    ├── [Season=Winter]\n    │ └── [Play=No]\n    └── [Season=Summer]\n      ├── [Temperature=Hot]\n      │ └── [Play=Yes]\n      └── [Temperature=Warm]\n        └── [Play=Yes]\nTree 3:\n    Root\n    ├── [Season=Autumn]\n    │ ├──[Wind=Breeze]\n    │ │ └── [Play=Yes]\n    │ ├── [Wind=None]\n    │ │ └── [Play=Yes]\n    │ └──[Wind=Strong]\n    │   └── [Play=No]\n    ├── [Season=Spring]\n    │ ├── [Temperature=Cold]\n    │ │ └── [Play=Yes]\n    │ └── [Temperature=Warm]\n    │   └── [Play=Yes]\n    ├── [Season=Winter]\n    │ └── [Play=No]\n    └── [Season=Summer]\n      └── [Play=Yes]\nThe total number of trees in the random forest=4.\nThe maximum number of the variables considered at the node is m=4.\n\nClassication\nFeature: ['Warm', 'Strong', 'Spring', '?']\nTree 0 votes for the class: No\nTree 1 votes for the class: Yes\nTree 2 votes for the class: Yes\nTree 3 votes for the class: Yes\nThe class with the maximum number of votes is 'Yes'. Thus the constructed random forest classifies the feature ['Warm', 'Strong', 'Spring', '?'] into the class 'Yes'.\n```"]