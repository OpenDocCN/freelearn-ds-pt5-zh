["```py\n StreamingContext(sparkContext: SparkContext, batchDuration: Duration) scala> val ssc = new StreamingContext(sc, Seconds(10))\n\n```", "```py\n StreamingContext(conf: SparkConf, batchDuration: Duration) scala> val conf = new SparkConf().setMaster(\"local[1]\")\n                                       .setAppName(\"TextStreams\")\n      scala> val ssc = new StreamingContext(conf, Seconds(10))\n\n```", "```py\n        def getOrCreate(\n          checkpointPath: String,\n          creatingFunc: () => StreamingContext,\n          hadoopConf: Configuration = SparkHadoopUtil.get.conf,\n          createOnError: Boolean = false\n        ): StreamingContext\n\n```", "```py\ndef start(): Unit \n\nscala> ssc.start()\n\n```", "```py\ndef stop(stopSparkContext: Boolean) scala> ssc.stop(false)\n\n```", "```py\ndef stop(stopSparkContext: Boolean, stopGracefully: Boolean) scala> ssc.stop(true, true)\n\n```", "```py\n def receiverStream[T: ClassTag](receiver: Receiver[T]): ReceiverInputDStream[T]\n\n```", "```py\ndef socketTextStream(hostname: String, port: Int,\n storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2):\n    ReceiverInputDStream[String]\n\n```", "```py\ndef rawSocketStream[T: ClassTag](hostname: String, port: Int,\n storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2):\n    ReceiverInputDStream[T]\n\n```", "```py\ndef fileStream[K: ClassTag, V: ClassTag, F <: NewInputFormat[K, V]: ClassTag] (directory: String): InputDStream[(K, V)]\n\n```", "```py\ndef textFileStream(directory: String): DStream[String]\n\n```", "```py\ndef binaryRecordsStream(directory: String, recordLength: Int): DStream[Array[Byte]]\n\n```", "```py\ndef queueStream[T: ClassTag](queue: Queue[RDD[T]], oneAtATime: Boolean = true): InputDStream[T]\n\n```", "```py\nscala> import org.apache.spark._\nscala> import org.apache.spark.streaming._\n\nscala> val ssc = new StreamingContext(sc, Seconds(10))\n\nscala> val filestream = ssc.textFileStream(\"streamfiles\")\n\nscala> filestream.foreachRDD(rdd => {println(rdd.count())})\n\nscala> ssc.start\n\n```", "```py\n ./bin/spark-shell --jars twitter4j-stream-4.0.6.jar,\n                               twitter4j-core-4.0.6.jar,\n                               spark-streaming-twitter_2.11-2.1.0.jar\n\n```", "```py\n        import org.apache.spark._\n        import org.apache.spark.streaming._\n        import org.apache.spark.streaming.Twitter._\n        import twitter4j.auth.OAuthAuthorization\n        import twitter4j.conf.ConfigurationBuilder\n\n        //you can replace the next 4 settings with your own Twitter\n              account settings.\n        System.setProperty(\"twitter4j.oauth.consumerKey\",\n                           \"8wVysSpBc0LGzbwKMRh8hldSm\") \n        System.setProperty(\"twitter4j.oauth.consumerSecret\",\n                  \"FpV5MUDWliR6sInqIYIdkKMQEKaAUHdGJkEb4MVhDkh7dXtXPZ\") \n        System.setProperty(\"twitter4j.oauth.accessToken\",\n                  \"817207925756358656-yR0JR92VBdA2rBbgJaF7PYREbiV8VZq\") \n        System.setProperty(\"twitter4j.oauth.accessTokenSecret\",\n                  \"JsiVkUItwWCGyOLQEtnRpEhbXyZS9jNSzcMtycn68aBaS\")\n\n        val ssc = new StreamingContext(sc, Seconds(10))\n\n        val twitterStream = TwitterUtils.createStream(ssc, None)\n\n        twitterStream.saveAsTextFiles(\"streamouts/tweets\", \"txt\")\n        ssc.start()\n\n        //wait for 30 seconds\n\n        ss.stop(false)\n\n```", "```py\nclass DStream[T: ClassTag] (var ssc: StreamingContext)\n\n//hashmap of RDDs in the DStream\nvar generatedRDDs = new HashMap[Time, RDD[T]]()\n\n```", "```py\n scala> val ssc = new StreamingContext(sc, Seconds(5))\n      ssc: org.apache.spark.streaming.StreamingContext = \n org.apache.spark.streaming.StreamingContext@8ea5756\n\n```", "```py\n scala> val twitterStream = TwitterUtils.createStream(ssc, None)\n      twitterStream: org.apache.spark.streaming.dstream\n .ReceiverInputDStream[twitter4j.Status] = \n org.apache.spark.streaming.Twitter.TwitterInputDStream@46219d14\n\n```", "```py\n val aggStream = twitterStream\n .flatMap(x => x.getText.split(\" \")).filter(_.startsWith(\"#\"))\n .map(x => (x, 1))\n .reduceByKey(_ + _)\n\n```", "```py\n ssc.start()      //to stop just call stop on the StreamingContext\n ssc.stop(false)\n\n```", "```py\n        class InputDStream[T: ClassTag](_ssc: StreamingContext) extends\n                                        DStream[T](_ssc)\n\n        class ReceiverInputDStream[T: ClassTag](_ssc: StreamingContext)\n                                  extends InputDStream[T](_ssc)\n\n```", "```py\n scala> val wordStream = twitterStream.flatMap(x => x.getText()\n                                                          .split(\" \"))\n      wordStream: org.apache.spark.streaming.dstream.DStream[String] = \n org.apache.spark.streaming.dstream.FlatMappedDStream@1ed2dbd5\n\n```", "```py\n ./bin/spark-shell --jars twitter4j-stream-4.0.6.jar,\n                               twitter4j-core-4.0.6.jar,\n                               spark-streaming-twitter_2.11-2.1.0.jar\n\n```", "```py\n        import org.apache.log4j.Logger\n        import org.apache.log4j.Level\n        Logger.getLogger(\"org\").setLevel(Level.OFF)\n\n       import java.util.Date\n       import org.apache.spark._\n       import org.apache.spark.streaming._\n       import org.apache.spark.streaming.Twitter._\n       import twitter4j.auth.OAuthAuthorization\n       import twitter4j.conf.ConfigurationBuilder\n\n       System.setProperty(\"twitter4j.oauth.consumerKey\",\n                          \"8wVysSpBc0LGzbwKMRh8hldSm\")\n       System.setProperty(\"twitter4j.oauth.consumerSecret\",\n                  \"FpV5MUDWliR6sInqIYIdkKMQEKaAUHdGJkEb4MVhDkh7dXtXPZ\")\n       System.setProperty(\"twitter4j.oauth.accessToken\",\n                  \"817207925756358656-yR0JR92VBdA2rBbgJaF7PYREbiV8VZq\")\n       System.setProperty(\"twitter4j.oauth.accessTokenSecret\",\n                  \"JsiVkUItwWCGyOLQEtnRpEhbXyZS9jNSzcMtycn68aBaS\")\n\n       val ssc = new StreamingContext(sc, Seconds(5))\n\n       val twitterStream = TwitterUtils.createStream(ssc, None)\n\n       val aggStream = twitterStream\n             .flatMap(x => x.getText.split(\" \"))\n             .filter(_.startsWith(\"#\"))\n             .map(x => (x, 1))\n             .reduceByKeyAndWindow(_ + _, _ - _, Seconds(15),\n                                   Seconds(10), 5)\n\n       ssc.checkpoint(\"checkpoints\")\n       aggStream.checkpoint(Seconds(10))\n\n       aggStream.foreachRDD((rdd, time) => {\n         val count = rdd.count()\n\n         if (count > 0) {\n           val dt = new Date(time.milliseconds)\n           println(s\"\\n\\n$dt rddCount = $count\\nTop 5 words\\n\")\n           val top5 = rdd.sortBy(_._2, ascending = false).take(5)\n           top5.foreach {\n             case (word, count) =>\n             println(s\"[$word] - $count\")\n           }\n         }\n       })\n\n       ssc.start\n\n       //wait 60 seconds\n       ss.stop(false)\n\n```", "```py\n Mon May 29 02:44:50 EDT 2017 rddCount = 1453\n Top 5 words\n\n [#RT] - 64\n [#de] - 24\n [#a] - 15\n [#to] - 15\n [#the] - 13\n\n Mon May 29 02:45:00 EDT 2017 rddCount = 3312\n Top 5 words\n\n [#RT] - 161\n [#df] - 47\n [#a] - 35\n [#the] - 29\n [#to] - 29\n\n```", "```py\ndef checkpoint(directory: String)\n\n```", "```py\nval ssc = new StreamingContext(sc, Seconds(5))\n\nval twitterStream = TwitterUtils.createStream(ssc, None)\n\nval wordStream = twitterStream.flatMap(x => x.getText().split(\" \"))\n\nval aggStream = twitterStream\n .flatMap(x => x.getText.split(\" \")).filter(_.startsWith(\"#\"))\n .map(x => (x, 1))\n .reduceByKeyAndWindow(_ + _, _ - _, Seconds(15), Seconds(10), 5)\n\nssc.checkpoint(\"checkpoints\")\n\naggStream.checkpoint(Seconds(10))\n\nwordStream.checkpoint(Seconds(10))\n\n```", "```py\nval ssc = StreamingContext.getOrCreate(checkpointDirectory,\n                                       createStreamContext _)\n\n```", "```py\nval checkpointDirectory = \"checkpoints\"\n\n// Function to create and setup a new StreamingContext\ndef createStreamContext(): StreamingContext = {\n  val ssc = new StreamingContext(sc, Seconds(5))\n\n  val twitterStream = TwitterUtils.createStream(ssc, None)\n\n  val wordStream = twitterStream.flatMap(x => x.getText().split(\" \"))\n\n  val aggStream = twitterStream\n    .flatMap(x => x.getText.split(\" \")).filter(_.startsWith(\"#\"))\n    .map(x => (x, 1))\n    .reduceByKeyAndWindow(_ + _, _ - _, Seconds(15), Seconds(10), 5)\n\n  ssc.checkpoint(checkpointDirectory)\n\n  aggStream.checkpoint(Seconds(10))\n\n  wordStream.checkpoint(Seconds(10))\n\n  aggStream.foreachRDD((rdd, time) => {\n    val count = rdd.count()\n\n    if (count > 0) {\n      val dt = new Date(time.milliseconds)\n      println(s\"\\n\\n$dt rddCount = $count\\nTop 5 words\\n\")\n      val top10 = rdd.sortBy(_._2, ascending = false).take(5)\n      top10.foreach {\n        case (word, count) => println(s\"[$word] - $count\")\n      }\n    }\n  })\n  ssc\n}\n\n// Get StreamingContext from checkpoint data or create a new one\nval ssc = StreamingContext.getOrCreate(checkpointDirectory, createStreamContext _)\n\n```", "```py\ndef createStream(\n ssc: StreamingContext, // StreamingContext object\n zkQuorum: String, //Zookeeper quorum (hostname:port,hostname:port,..)\n groupId: String, //The group id for this consumer\n topics: Map[String, Int], //Map of (topic_name to numPartitions) to\n                  consume. Each partition is consumed in its own thread\n storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2 \n  Storage level to use for storing the received objects\n  (default: StorageLevel.MEMORY_AND_DISK_SER_2)\n): ReceiverInputDStream[(String, String)] //DStream of (Kafka message key, Kafka message value)\n\n```", "```py\nval topicMap = topics.split(\",\").map((_, numThreads.toInt)).toMap\nval lines = KafkaUtils.createStream(ssc, zkQuorum, group,\n                                    topicMap).map(_._2)\n\n```", "```py\ndef createDirectStream[\n K: ClassTag, //K type of Kafka message key\n V: ClassTag, //V type of Kafka message value\n KD <: Decoder[K]: ClassTag, //KD type of Kafka message key decoder\n VD <: Decoder[V]: ClassTag, //VD type of Kafka message value decoder\n R: ClassTag //R type returned by messageHandler\n](\n ssc: StreamingContext, //StreamingContext object\n KafkaParams: Map[String, String], \n  /*\n  KafkaParams Kafka <a  href=\"http://Kafka.apache.org/documentation.html#configuration\">\n  configuration parameters</a>. Requires \"metadata.broker.list\" or   \"bootstrap.servers\"\nto be set with Kafka broker(s) (NOT zookeeper servers) specified in\n  host1:port1,host2:port2 form.\n  */\n fromOffsets: Map[TopicAndPartition, Long], //fromOffsets Per- topic/partition Kafka offsets defining the (inclusive) starting point of the stream\n messageHandler: MessageAndMetadata[K, V] => R //messageHandler Function for translating each message and metadata into the desired type\n): InputDStream[R] //DStream of R\n\n```", "```py\nval topicsSet = topics.split(\",\").toSet\nval KafkaParams : Map[String, String] =\n        Map(\"metadata.broker.list\" -> brokers,\n            \"group.id\" -> groupid )\n\nval rawDstream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, KafkaParams, topicsSet)\n\n```", "```py\nval ds1 = spark\n .readStream\n .format(\"Kafka\")\n .option(\"Kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n .option(\"subscribe\", \"topic1\")\n .load()\n\nds1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n .as[(String, String)]\n\n```", "```py\nval ds1 = spark\n .read\n .format(\"Kafka\")\n .option(\"Kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n .option(\"subscribe\", \"topic1\")\n .load()\n\nds1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n .as[(String, String)]\n\n```", "```py\n//create stream reading from localhost 9999\nval inputLines = spark.readStream\n .format(\"socket\")\n .option(\"host\", \"localhost\")\n .option(\"port\", 9999)\n .load()\ninputLines: org.apache.spark.sql.DataFrame = [value: string]\n\n// Split the inputLines into words\nval words = inputLines.as[String].flatMap(_.split(\" \"))\nwords: org.apache.spark.sql.Dataset[String] = [value: string]\n\n// Generate running word count\nval wordCounts = words.groupBy(\"value\").count()\nwordCounts: org.apache.spark.sql.DataFrame = [value: string, count: bigint]\n\nval query = wordCounts.writeStream\n .outputMode(\"complete\")\n .format(\"console\")\nquery: org.apache.spark.sql.streaming.DataStreamWriter[org.apache.spark.sql.Row] = org.apache.spark.sql.streaming.DataStreamWriter@4823f4d0\n\nquery.start()\n\n```", "```py\nscala> -------------------------------------------\nBatch: 0\n-------------------------------------------\n+-----+-----+\n|value|count|\n+-----+-----+\n| dog| 1|\n+-----+-----+\n\n-------------------------------------------\nBatch: 1\n-------------------------------------------\n+-----+-----+\n|value|count|\n+-----+-----+\n| dog| 1|\n| cat| 1|\n+-----+-----+\n\nscala> -------------------------------------------\nBatch: 2\n-------------------------------------------\n+-----+-----+\n|value|count|\n+-----+-----+\n| dog| 2|\n| cat| 1|\n+-----+-----+\n\n```", "```py\nimport java.sql.Timestamp import org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._ // Create DataFrame representing the stream of input lines from connection to host:port\nval inputLines = spark.readStream\n .format(\"socket\")\n .option(\"host\", \"localhost\")\n .option(\"port\", 9999)\n .option(\"includeTimestamp\", true)\n .load() // Split the lines into words, retaining timestamps\nval words = inputLines.as[(String, Timestamp)].flatMap(line =>\n line._1.split(\" \").map(word => (word, line._2))\n).toDF(\"word\", \"timestamp\") // Group the data by window and word and compute the count of each group\nval windowedCounts = words.withWatermark(\"timestamp\", \"10 seconds\")\n.groupBy(\n window($\"timestamp\", \"10 seconds\", \"10 seconds\"), $\"word\"\n).count().orderBy(\"window\") // Start running the query that prints the windowed word counts to the console\nval query = windowedCounts.writeStream\n .outputMode(\"complete\")\n .format(\"console\")\n .option(\"truncate\", \"false\")\n\nquery.start()\nquery.awaitTermination()\n\n```"]