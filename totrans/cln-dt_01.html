<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;Why Do You Need Clean Data?" id="aid-DB7S1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Why Do You Need Clean Data?</h1></div></div></div><p>Big data, data mining, machine learning, and visualization—it seems like data is at the center of everything great happening in computing lately. From statisticians to software developers to graphic designers, everyone is suddenly interested in data science. The confluence of cheap hardware, better processing and visualization tools, and massive amounts of freely available data means that we can now discover trends and make predictions more accurately and more easily than ever before.</p><p>What you might <span class="emphasis"><em>not</em></span> have heard, though, is that all of these data science hopes and dreams are predicated on the fact that data is messy. Usually, data has to be moved, compressed, cleaned, chopped, sliced, diced, and subjected to any number of other transformations before it is ready to <a id="id0" class="indexterm"/>be used in the algorithms or visualizations that we think of as the heart of <span class="strong"><strong>data science</strong></span>.</p><p>In this chapter, we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">A simple six-step process you can follow for data science, including cleaning</li><li class="listitem">Helpful guidelines to communicate how you cleaned your data</li><li class="listitem">Some tools that you might find helpful for data cleaning</li><li class="listitem">An introductory example that shows how data cleaning fits into the overall data science process</li></ul></div><div class="section" title="A fresh perspective"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/>A fresh perspective</h1></div></div></div><p>We recently read that The New York Times called data<a id="id1" class="indexterm"/> cleaning <span class="strong"><strong>janitor work</strong></span> and said that 80 percent of a data scientist's time will be spent doing this kind<a id="id2" class="indexterm"/> of cleaning. As we can see in the following figure, despite its importance, data cleaning has not really captured the public imagination in the same way as big data, data mining, or machine learning:</p><div class="mediaobject"><img src="../Images/image00228.jpeg" alt="A fresh perspective"/></div><p style="clear:both; height: 1em;"> </p><p>Who can blame us for not wanting to gather in droves to talk about how fun and super-cool janitor work is? Well, unfortunately—and this is true for actual housekeeping chores as well—we would all be a lot better off if we just got the job done rather than ignoring it, complaining<a id="id3" class="indexterm"/> about it, and giving it various demeaning names.</p><p>Not convinced yet? Consider a different metaphor instead, you are not a data janitor; you are a data chef. Imagine you have been handed a market basket overflowing with the most gorgeous heirloom vegetables you have ever seen, each one handpicked at the peak of freshness and sustainably produced on an organic farm. The tomatoes are perfectly succulent, the lettuce is crisp, and the peppers are bright and firm. You are excited to begin cooking, but you look around and the kitchen is filthy, the pots and pans have baked-on, caked-on who-knows-what, and, as for tools, you have nothing but a rusty knife and a soggy towel. The sink is broken and you just saw a beetle crawl out from under that formerly beautiful lettuce.</p><p>Even a beginner chef knows you should not cook in a place like this. At the very least, you will destroy that perfectly good delicious basket of goodies you have been given. And at worst, you will make people sick. Plus, cooking like this is not even fun, and it would take all day to chop the veggies with an old rusty knife.</p><p>Just as you would in a kitchen, it's definitely worth spending time cleaning and preparing your data science workspace, your tools, and your raw materials upfront. The old computer programming adage from the 1960s—garbage in, garbage out—is also true with data science.</p></div></div>
<div class="section" title="The data science process" id="aid-E9OE1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>The data science process</h1></div></div></div><p>How <a id="id4" class="indexterm"/>does cleaning fit into the rest of the job of data science? Well, the short answer is that it's a critical piece, and it directly affects the processes that come before and after it.</p><p>The longer answer relies on describing the data science process in six steps, as shown in the following lists. Data cleaning is right in the middle, at step 3. But rather than thinking of these steps as a linear, start-to-finish framework, we will revisit the steps as needed several times over the course of a project in more of an iterative manner. It is also worth pointing out that not every project will have all the steps; for example, sometimes, we do not have a collection step or a visualization step. It really depends on the particular needs of the project:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first step is to come up with the problem statement. Identify what the problem you are trying to solve is.</li><li class="listitem">The next step is data collection and storage. Where did the data come from that is helping you answer this question? Where did you store it and in what format?</li><li class="listitem">Then comes data cleaning. Did you change the data at all? Did you delete anything? How did you prepare it for the analysis and mining step next?</li><li class="listitem">The next step is data analysis and machine learning. What kind of processing did you do to the data? What transformations? What algorithms did you use? What formulas did you apply? What machine learning algorithms did you use? In what order?</li><li class="listitem">Representation and visualization is the next step. How do you show the results of your work? This can be one or more tables, drawings, graphs, charts, network diagrams, word clouds, maps, and so on. Is this the best visualization to represent the data? What alternatives did you consider?</li><li class="listitem">The last step is problem resolution. What is the answer to the question or problem you posed in step 1? What limitations do you have on your results? Were there parts of the question that you could not answer with this method? What could you have done differently? What are the next steps?</li></ol><div style="height:10px; width: 1px"/></div><p>It makes sense that data cleaning needs to happen before you attempt the analysis / mining / machine learning or visualization steps. Although, remember that, as this is an iterative process, we may revisit cleaning several times during the course of a project. Also, the type of mining or analysis we are going to do will often drive the way we clean the data. We consider cleaning to include a variety of tasks that may be dictated by the analysis method chosen, for example, swapping file formats, changing character encodings, or parsing out pieces of data to operate on.</p><p>Data cleaning is also going to be very closely tied to the collection and storage step (step 2). This <a id="id5" class="indexterm"/>means that you may have to collect raw data, store it, clean it, store the cleaned data again, collect some more, clean that, combine it with the previous data, clean it again, store it, and so on. As such, it is going to be very important to remember what you did and be able to repeat that process again if needed or tell someone else what you did.</p></div>
<div class="section" title="Communicating about data cleaning" id="aid-F8901"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Communicating about data cleaning</h1></div></div></div><p>As the six-step process is organized around a story arc that starts with a question and ends with the resolution of the question, it works nicely as a reporting framework. If you decide to use the six-step framework as a way to report on your data science process, you will find that you get to write about your cleaning in the third step.</p><p>But even if <a id="id6" class="indexterm"/>you do not document your data science process in a formal report, you will find that it is extremely helpful to keep careful notes of what you did to the data and in what order.</p><p>Remember that even for the smallest, lowest-stakes project, you are always working for an audience of at least two people: you now and you 6 months from now. Believe me when I tell you that the you-in-six-months isn't going to remember what the you-of-today did to clean your data, much less why you did it or how to do it again!</p><p>The simplest solution for this is to just keep a <span class="strong"><strong>log</strong></span> of what you did. The log should include links, screenshots, or copy and pastes of the specific commands you ran as well as a short explanation for why you did this. The following example shows a log for a very small text mining project with embedded links to the output files at each stage as well as links to the cleaning scripts. Don't worry if you are unfamiliar with some of the technologies mentioned in this log. This <a id="id7" class="indexterm"/>example shows you what a log<a id="id8" class="indexterm"/> might look like:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We wrote a SQL query to retrieve a list of every item and its description.</li><li class="listitem">In order to conduct a term frequency analysis in Python, we needed data in a specific JSON format. We constructed a PHP script that looped through the results of our query, placing its results in a JSON file (version 1).</li><li class="listitem">This file had some formatting errors, such as unescaped quotation marks and embedded HTML tags. These errors were corrected with a second PHP script, which when run, printed this cleaned JSON file (version 2).</li></ol><div style="height:10px; width: 1px"/></div><p>Note that our log tries to explain what we did and why we did it. It is short and can include links when possible.</p><p>There are many more sophisticated solutions to communicate about data cleaning should you choose to use them, for <a id="id9" class="indexterm"/>example, if you are familiar with <span class="strong"><strong>version control systems</strong></span> such <a id="id10" class="indexterm"/>as <span class="strong"><strong>Git</strong></span> or Subversion, which are usually used to manage a software project, you can probably conceive how to extend them to keep track of your data cleaning. Whatever system you choose, even if it is a simple log, the most important <a id="id11" class="indexterm"/>thing is to actually use it. So, pick something that will encourage its use and not impede your progress.</p></div>
<div class="section" title="Our data cleaning environment" id="aid-G6PI1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Our data cleaning environment</h1></div></div></div><p>The approach to data cleaning we are using in this book is a general-purpose, widely applicable one. It does not require or assume that you have any high-end specialty single-vendor database or data analysis products (in fact, these vendors and products may have their own cleaning routines or methods). I have designed the cleaning tutorials in this book around common, everyday issues that you might encounter when using real-world datasets. I have designed the book around real-world data that anyone can access. I'll show you how to clean data using open source, general-purpose software and technologies <a id="id12" class="indexterm"/>that are easy to get and are commonly used in the workplace.</p><p>Here are some of the tools and technologies that you should be ready to use:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">For nearly<a id="id13" class="indexterm"/> every chapter, we will use a terminal window and its command-line interface, such as the Terminal program on Mac OSX or bash on a Linux system. In Windows, some commands will be able to be run using Windows Command Prompt, but other commands may require the use of a more full-featured Windows command-line program, such as CygWin.</li><li class="listitem">For nearly every chapter, we will use a text editor or programmer's editor, such as Text Wrangler on a Mac, vi or emacs in Linux, or Notepad++ or Sublime Editor on Windows.</li><li class="listitem">For most chapters, we will need a Python 2.7 client, such as Enthought Canopy, and we will need enough permissions to install packages. Many of the examples will work with Python 3, but some will not, so if you already have that, you may wish to create an alternate 2.7 installation.</li><li class="listitem">For <a class="link" title="Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors" href="part0024.xhtml#aid-MSDG2">Chapter 3</a>, <span class="emphasis"><em>Workhorses of Clean Data – Spreadsheets and Text Editors</em></span>, we will need a spreadsheet program (we will focus on Microsoft Excel and Google Spreadsheets).</li><li class="listitem">For <a class="link" title="Chapter 7. RDBMS Cleaning Techniques" href="part0045.xhtml#aid-1AT9A1">Chapter 7</a>, <span class="emphasis"><em>RDBMS Cleaning Techniques</em></span>, we will need a working MySQL installation and the client software to access it.</li></ul></div></div>
<div class="section" title="An introductory example"><div class="titlepage" id="aid-H5A42"><div><div><h1 class="title"><a id="ch01lvl1sec12"/>An introductory example</h1></div></div></div><p>To get started, let's sharpen our chef's knife with a small example that integrates the six-step framework and illustrates how to tackle a few simple cleaning issues. This example uses the <a id="id14" class="indexterm"/>publicly available Enron e-mail dataset. This is a very famous dataset consisting of e-mail messages sent to, from, and between employees working at the now-defunct Enron Corporation. As part of the U.S. Government investigation into accounting fraud at Enron, the e-mails became part of the public record and are now downloadable by anyone. Researchers in a variety of domains have found the e-mails helpful for studying workplace communication, social networks, and more.</p><div class="note" title="Note"><h3 class="title"><a id="note02"/>Note</h3><p>You can read more<a id="id15" class="indexterm"/> about Enron and the financial scandal that led to its demise on its Wikipedia page at <a class="ulink" href="http://en.wikipedia.org/wiki/Enron">http://en.wikipedia.org/wiki/Enron</a>, and you can read <a id="id16" class="indexterm"/>about the Enron e-mail corpus itself on its separate page at <a class="ulink" href="http://en.wikipedia.org/wiki/Enron_Corpus">http://en.wikipedia.org/wiki/Enron_Corpus</a>.</p></div><p>In this example, we will implement the six-step framework on a simple data science question. Suppose we want to reveal trends and patterns in e-mail usage over time within Enron Corporation. Let's start by counting messages that were sent to/from Enron employees by date. We will then show the counts visually on a graph over time.</p><p>First, we need to download the MySQL Enron corpus using the instructions at <a class="ulink" href="http://www.ahschulz.de/enron-email-data/">http://www.ahschulz.de/enron-email-data/</a>. Another (backup) source for this file is <a class="ulink" href="https://www.cs.purdue.edu/homes/jpfeiff/enron.html">https://www.cs.purdue.edu/homes/jpfeiff/enron.html</a>. Following these instructions, we will <a id="id17" class="indexterm"/>need to import the data into a new database scheme called <span class="strong"><strong>Enron</strong></span> on a MySQL server. The data is now ready to be queried using either the MySQL command-line interface or using a web-based tool such as PHPMyAdmin.</p><p>Our first count query is shown as follows:</p><div class="informalexample"><pre class="programlisting">SELECT date(date) AS dateSent, count(mid) AS numMsg
FROM message
GROUP BY dateSent
ORDER BY dateSent;</pre></div><p>Right away, we notice that numerous e-mails have incorrect dates, for example, there are a number of dates that seem to predate or postdate the existence of the corporation (for example, 1979) or that were from years that were illogical (for example, 0001 or 2044). E-mail is old but not <span class="emphasis"><em>that</em></span> old!</p><p>The following table shows an excerpt of a few of the weird lines (the complete result set is about 1300 rows long) All of these dates are formatted correctly; however, some of the dates are definitely wrong:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>dateSent</p>
</th><th valign="bottom">
<p>numMsg</p>
</th></tr></thead><tbody><tr><td valign="top">
<p><code class="literal">0002-03-05</code></p>
</td><td valign="top">
<p><code class="literal">1</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">0002-03-07</code></p>
</td><td valign="top">
<p><code class="literal">3</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">0002-03-08</code></p>
</td><td valign="top">
<p><code class="literal">2</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">0002-03-12</code></p>
</td><td valign="top">
<p><code class="literal">1</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1979-12-31</code></p>
</td><td valign="top">
<p><code class="literal">6</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1997-01-01</code></p>
</td><td valign="top">
<p><code class="literal">1</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1998-01-04</code></p>
</td><td valign="top">
<p><code class="literal">1</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1998-01-05</code></p>
</td><td valign="top">
<p><code class="literal">1</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1998-10-30</code></p>
</td><td valign="top">
<p><code class="literal">3</code></p>
</td></tr></tbody></table></div><p>These bad dates are most likely due to misconfigured e-mail clients. At this point, we have three choices for what to do:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Do nothing</strong></span>: Maybe we can just ignore the bad data and get away with building the line graph anyway. But, as the lowest bad date was from the year 0001 and the highest was from the year 2044, we can imagine our line graph with the 1300 tick marks on the time axis, each showing a count of 1 or 2. This graph does not sound very appealing or informative, so doing nothing will not work.</li><li class="listitem"><span class="strong"><strong>Fix the data</strong></span>: We could try to figure out what the correct date for each bad message was and produce a corrected dataset that we can then use to build our graph.</li><li class="listitem"><span class="strong"><strong>Throw out the affected e-mails</strong></span>: We can just make an informed decision to discard any e-mail that has a date that falls outside a predetermined window.</li></ul></div><p>In order to <a id="id18" class="indexterm"/>decide between options 2 and 3, we will need to count how many messages will be affected using only a 1999-2002 window. We can use the following SQL:</p><div class="informalexample"><pre class="programlisting">SELECT count(*) FROM message
WHERE year(date) &lt; 1998 or year(date) &gt; 2002;
Result: 325</pre></div><p>325 messages with bad dates may initially seem like a lot, but then again, they are only about 1 percent of the entire dataset. Depending on our goals, we might decide to fix these dates manually, but let's assume here that we do not mind losing 1 percent of the messages. We can proceed cautiously toward option 3, throwing out the affected e-mails. Here is the amended query:</p><div class="informalexample"><pre class="programlisting">SELECT date(date) AS dateSent, count(mid) AS numMsg
FROM message
WHERE year(date) BETWEEN 1998 AND 2002
GROUP BY dateSent
ORDER BY dateSent;</pre></div><p>The cleaned data now consists of 1,211 rows, each with a count. Here is an excerpt of the new dataset:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>dateSent</p>
</th><th valign="bottom">
<p>numMsg</p>
</th></tr></thead><tbody><tr><td valign="top">
<p><code class="literal">1998-01-04</code></p>
</td><td valign="top">
<p><code class="literal">1</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1998-01-05</code></p>
</td><td valign="top">
<p><code class="literal">1</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1998-10-30</code></p>
</td><td valign="top">
<p><code class="literal">3</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1998-11-02</code></p>
</td><td valign="top">
<p><code class="literal">1</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1998-11-03</code></p>
</td><td valign="top">
<p><code class="literal">1</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1998-11-04</code></p>
</td><td valign="top">
<p><code class="literal">4</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1998-11-05</code></p>
</td><td valign="top">
<p><code class="literal">1</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">1998-11-13</code></p>
</td><td valign="top">
<p><code class="literal">2</code></p>
</td></tr></tbody></table></div><p>In this <a id="id19" class="indexterm"/>example, it looks like there are two questionable dates in January 1998 and no other messages until October, at which point, the messages start coming in more regularly. This seems weird, and it also points to another issue, is it important that we have every date on the <span class="emphasis"><em>x</em></span> axis, even if there were no e-mails sent that day?</p><p>If we answer yes, it is important to show every date, even those with 0 counts; this may mean going through another round of cleaning in order to produce rows that show the date with a zero.</p><p>But then again, maybe we can be more strategic about this. Whether we need to have zero values in our raw data actually depends on what tool we are using to create the graph and what type of graph it is, for example, Google Spreadsheets will build a line or bar graph that can automatically detect that there are missing dates on the <span class="emphasis"><em>x</em></span> axis and will fill in zero values even if they are not given in the initial dataset. In our data, these zero values would be the mysterious missing dates from most of 1998.</p><p>The next three figures show each of these tools and how they handle zero values on a date axis. Note the long zero tails at the beginning and end of the Google Spreadsheets representation of the data shown here:</p><div class="mediaobject"><img src="../Images/image00229.jpeg" alt="An introductory example"/><div class="caption"><p>Google Spreadsheets automatically fills in any missing days with a zero.</p></div></div><p style="clear:both; height: 1em;"> </p><p>The<a id="id20" class="indexterm"/> D3 JavaScript visualization library will do the same, filling in zero values for missing dates in a range by default, as shown in the next graph.</p><div class="note" title="Note"><h3 class="title"><a id="tip02"/>Tip</h3><p>For a simple D3 line graph example, take a look at this tutorial: <a class="ulink" href="http://bl.ocks.org/mbostock/3883245">http://bl.ocks.org/mbostock/3883245</a>.</p></div><div class="mediaobject"><img src="../Images/image00230.jpeg" alt="An introductory example"/><div class="caption"><p>D3 automatically fills in any missing days with a zero.</p></div></div><p style="clear:both; height: 1em;"> </p><p>Excel also has identical date-filling behavior in its default line graph, as shown here:</p><div class="mediaobject"><img src="../Images/image00231.jpeg" alt="An introductory example"/><div class="caption"><p>Excel automatically fills in any missing days with a zero.</p></div></div><p style="clear:both; height: 1em;"> </p><p>Next, we <a id="id21" class="indexterm"/>need to consider whether, by allowing zero values for dates, we are also making our <span class="emphasis"><em>x</em></span> axis substantially longer (my count query yielded 1211 rows, but there are a total of 1822 days in the range specified, which is 1998-2002). Maybe showing zero count days might not work; if the graph is so crowded, we cannot see the gaps anyway.</p><p>To compare, we can quickly run the same data into Google Spreadsheets (you can do this in Excel or D3 too), but this time, we will only select our count column to build the graph, thereby forcing Google Spreadsheets to <span class="emphasis"><em>not</em></span> show dates on the <span class="emphasis"><em>x</em></span> axis. The result is the true shape of only the data that came from the database count query with no zero count days filled in. The long tails are gone, but the overall shape of the important part of the graph (the middle) remains the same:</p><div class="mediaobject"><img src="../Images/image00232.jpeg" alt="An introductory example"/><div class="caption"><p>The graph now shows only dates with one or more message.</p></div></div><p style="clear:both; height: 1em;"> </p><p>Lucky for us, the shape of the data is similar, save for a shorter head and tail on the graph. Based on this comparison, and based on what we plan to do with the data (remember that all we wanted to do was create a simple line graph), we can feel good about our decision to move forward without specifically creating a dataset showing zero count days.</p><p>When all is <a id="id22" class="indexterm"/>said and done, the line graphs reveal that Enron had several significant peaks in e-mail traffic. The largest peaks and heaviest traffic occurred in the October and November of 2001, when the scandal broke. The two smaller peaks occurred around June 26-27 of 2001 and December 12-13 of 2000, when similar newsworthy events involving Enron transpired (one involving the California energy crisis and another involving a leadership change at the company).</p><p>If you get excited by data analysis, you probably have all sorts of cool ideas for what to do next with this data. And now that you have cleaned data, it will make your analysis tasks easier, hopefully!</p></div>
<div class="section" title="Summary" id="aid-I3QM1"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Summary</h1></div></div></div><p>After all that work, it looks like The New York Times was right. As you can see from this simple exercise, data cleaning indeed comprises about 80 percent of the effort of answering even a tiny data-oriented question (in this case, talking through the rationale and choices for data cleaning took 700 words out of the 900-word case study). Data cleaning really is a pivotal part of the data science process, and it involves understanding technical issues and also requires us to make some value judgments. As part of data cleaning, we even had to take into account the desired outcomes of both the analysis and visualization steps even though we had not really completed them yet.</p><p>After considering the role of data cleaning as presented in this chapter, it becomes even more obvious how improvements in our cleaning effectiveness could quickly add up to substantial time savings.</p><p>The next chapter will describe a few of the fundamentals that will be required for any "data chef" who wants to move into a bigger, better "kitchen", including file formats, data types, and character encodings.</p></div></body></html>