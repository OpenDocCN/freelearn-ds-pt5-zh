<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting the Likelihood of Marketing Engagement</h1>
                </header>
            
            <article>
                
<p>In this chapter, we are going to expand the knowledge we gained from the previous chapter and the customer analytics exercise we conducted in <a href="72e8f4ee-7f95-4acc-928d-d33c9fc31bd6.xhtml" target="_blank">Chapter 7</a>, <em>Exploratory Analysis for Customer Behavior</em>. For successful and more intelligent marketing strategies, we cannot stop at analyzing customer data. With the advanced technology in data science and machine learning, we can now make intelligent guesses and estimates on customers' future behaviors, such as what types of customers are more likely to engage with marketing efforts, the amount of purchases that customers are likely to make, or which customers are likely to churn. These predictions or intelligent guesses that are built based on historical customer data can help you improve your marketing performance and further tailor your marketing strategies for different target audiences. In this chapter, w<span>e are going to learn how we can utilize data science and machine learning to predict future outcomes and how this can help your future marketing efforts.</span></p>
<p>I<span>n this chapte</span><span>r, we will cover the following topics:</span></p>
<ul>
<li>Predictive analytics in marketing</li>
<li>Evaluating classification models</li>
<li>Predicting the likelihood of marketing engagement with Python</li>
<li><span>Predicting the likelihood of marketing engagement</span> with R</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predictive analytics in marketing</h1>
                </header>
            
            <article>
                
<p><strong>Predictive analytics</strong> is a process of analyzing and extracting information from historical data to identify patterns and make predictions about future outcomes. Numerous statistical and machine learning models are typically used to find the relationship between the attributes or features in the dataset and the target variable or behavior that you would like to predict. Predictive analytics can be utilized and applied in many different industries.</p>
<p>For example, it is often used in the financial industry for fraud detection, where machine learning models are trained to detect and prevent potential fraudulent transactions. The healthcare industry can also benefit from predictive analytics to help physicians in their decision-making processes. Furthermore, there are various parts of marketing that can also benefit from predictive analytics, such as customer acquisition, customer retention, and up-selling and cross-selling, to name a few.</p>
<p>In predictive analytics, broadly speaking, there are two types of problems:</p>
<ul>
<li><span><strong>Classification problems</strong></span>: A classification problem is where there is a set of categories an observation can belong to. For example, predicting whether a customer is going to open a marketing email or not is a classification problem. There are only two possible outcomes—opening the marketing email or not opening the email. </li>
<li><span><strong>Regression problems</strong></span>:<span> A r</span>egression problem, on the other hand, is where the outcome can take on any range of real numbers. For example, predicting customer lifetime value is a regression problem. One customer can have a lifetime value of $0 and another customer can have a lifetime value of $10,000. This type of problem, where the outcome can take continuous values, is called a regression problem. </li>
</ul>
<p>In this chapter, we are going to focus on one of the common classification problems in the marketing industry—predicting the likelihood of customer engagement. In the following chapter, <a href="9b3d36ba-d690-491c-9a6f-b8c00f59cfb4.xhtml" target="_blank">Chapter 9</a>, <em>Customer Lifetime Value</em>, we are going to tackle one of the frequently appearing regression problems within the marketing industry.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applications of predictive analytics in marketing</h1>
                </header>
            
            <article>
                
<p>As briefly mentioned previously, there are numerous ways of applying and utilizing predictive analytics in marketing. In this section, we are going to discuss four popular use cases of predictive analytics in marketing:</p>
<ul>
<li><strong>Likelihood of engagement</strong>: Predictive analytics can help marketers forecast the likelihood of customer engagements with their marketing strategies. For example, if your marketing happens a lot in the email space, you can utilize predictive analytics to forecast which customers have a high likelihood of opening your marketing emails and custom-tailor your marketing strategies to those high-likelihood customers to maximize your marketing results. For another example, if you are displaying advertisements on social media, predictive analytics can help you identify certain types of customers that are likely to click on the ads.</li>
<li><strong>Customer lifetime value</strong>: Predictive analytics can help you forecast the expected lifetime values of your customers. Using historical transactional data, predictive analytics can help you identify high-value customers within your customer base. With these predictions, you and your firm can focus more on building healthy relationships with those high-value customers. We are going to discuss in more detail how to build predictive models for customer lifetime value forecasts in the following chapter.</li>
<li><strong>Recommending the right products and contents</strong>: As we have already discussed in <a href="d3ba7047-2873-4b03-9a44-4c1d55b84178.xhtml" target="_blank">Chapter 6</a>, <em>Recommending the Right Products</em>, we can use data science and machine learning to predict which customers are likely to purchase products or view contents. Using these predictions, you can improve customer conversion rates by recommending the right products and contents for individual customers.</li>
<li><strong>Customer acquisition and retention</strong>: Predictive analytics has also been heavily used for customer acquisition and retention. Based on the profile data you gathered about your prospects or leads and the historical data of your existing customers, you can apply predictive analytics to identify high-quality leads or rank the leads by their likelihood of being converted into active customers. On the other hand, you can use the customer churn data and the historical data of your existing customers to develop predictive models to forecast which customers are likely to leave or unsubscribe from your products. We are going to discuss in more detail applying predictive analytics for customer retention in <a href="3d5c7798-6874-40e9-b7e9-6fe39592cc2a.xhtml" target="_blank">Chapter 11</a>, <em>Retaining Customers</em>.</li>
</ul>
<p>On top of these four common use cases of predictive analytics in marketing, there are many other ways you can utilize predictive analytics for your marketing strategies. You should get creative on how and where to use predictive analytics for your future marketing strategies.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating classification models</h1>
                </header>
            
            <article>
                
<p>When developing predictive models, it is important to know how to evaluate those models. In this section, we are going to discuss five different ways to evaluate the performance of classification models. The first metric that can be used to measure prediction performance is <strong>accuracy</strong>. Accuracy is simply the percentage of correct predictions out of all predictions, as shown in the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/84ed1d1c-4cfa-42b1-bbc9-378337f43d0a.png" style="width:20.75em;height:2.58em;"/></p>
<p>The second metric that is commonly used for classification problems is <strong>precision</strong>. Precision is defined as the number of true positives divided by the total number of true positives and false positives. True positives are cases where the model correctly predicted as positive, while false positives are cases where the model was predicted as positive, but the true label was negative. The formula looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ae4af65b-cc35-401a-938e-d5da51b56c4d.png" style="width:8.33em;height:1.83em;"/></p>
<p>Along with precision, <strong>recall</strong> is also commonly used to evaluate the performances of classification models. Recall is defined as the number of true positives divided by the number of true positives plus false negatives. False negatives are cases where the model was predicted as negative, but the true label was positive. Recall can be thought of as a measure of how much of the positive cases are retrieved or found by the model. The formula looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7aef9390-c65f-4cf2-91f2-2c2f5a1d4a58.png" style="width:8.67em;height:2.25em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The final two metrics we are going to discuss are the <strong>receiver operating characteristic</strong> (<strong>ROC</strong>) curve and the <strong>area under the curve</strong> (<strong>AUC</strong>). The ROC curve shows how true positive rates and false positive rates change at different thresholds. The AUC is simply the total area under the ROC curve. The AUC ranges from 0 to 1 and a higher AUC number suggests better model performance. A random classifier has an AUC of 0.5, so any classifier with an AUC higher than 0.5 suggests that the model performs better than random predictions. A typical ROC curve looks as in the following:<br/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8b7e03bd-8eec-49a7-8c05-138ca8798c0a.png" style="width:40.50em;height:29.50em;"/></p>
<p>In the following programming exercise, we are going to use<span> these five metrics that we have just discussed to evaluate the performance of the model we build in Python and R. Let's now dive into building machine learning models to predict the likelihood of marketing engagement!</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting the likelihood of marketing engagement with Python</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to discuss how to build predictive models using machine learning algorithms in Python. More specifically, we will learn how to build a predictive model using the random forest algorithm, as well as how to tune the random forest model and evaluate the performance of the model. We will be mainly using the<span> </span><kbd>pandas</kbd><span>,</span><span> </span><kbd>matplotlib</kbd><span>, and <kbd>scikit-learn</kbd> </span>packages to analyze, visualize, and build machine learning models that predict the likelihood of customer marketing engagement. <span>For those readers who would like to use R instead of Python for this exercise, you can skip to the next section.</span></p>
<p><span>For this exercise, we will be using one of the publicly available datasets from</span> IBM, whic<span>h can be found at this link: <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/">https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/</a></span><span>. You can follow this link and download the data that is available in CSV format, named <kbd>WA_Fn UseC_ Marketing Customer Value Analysis.csv</kbd>. Once you have downloaded this data, you can load it into your Jupyter notebook by running the following command:</span></p>
<pre>import pandas as pd<br/><br/>df = pd.read_csv('../data/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')</pre>
<p>The <kbd>df</kbd> <span>DataFrame </span>looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1251 image-border" src="assets/63a2f76d-29f3-443a-a327-34ab8b596ffd.png" style="width:162.50em;height:62.83em;"/></p>
<p>As you might have noticed, this is the same dataset that we used in the previous chapter, where we conducted customer analytics. With the knowledge we gained about this dataset from the previous chapter, we are going to first prepare our data by encoding the target variable and other categorical variables that we are going to use as features for our machine learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variable encoding</h1>
                </header>
            
            <article>
                
<p>In order to build machine learning models using the <kbd>scikit-learn</kbd> package in Python, all the features in the dataset need to have numerical values. However, in the dataset, we have numerous columns that have non-numerical values. For example, the target variable, <kbd>Response</kbd>, which is what we are going to try to predict with machine learning models, is non-numeric. It contains two string values—<kbd>Yes</kbd> and <kbd>No</kbd>. We will need to encode this <kbd>Response</kbd> <span>target variable </span>with numerical values in order to be able to build machine learning models. For another example, the column <kbd>Gender</kbd>, which we can use as one of the features for our predictive model, also does not have numerical values. It contains two string values—<kbd>F</kbd> for female and <kbd>M</kbd> for male. In this section, we are going to discuss how we can encode these non-numeric columns so that we can use them as features for machine learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Response variable encoding</h1>
                </header>
            
            <article>
                
<p>The first thing we are going to do is encode the response variable <kbd>Response</kbd>. We are going to encode <kbd>Yes</kbd> values with <kbd>1</kbd>s and <kbd>No</kbd> values with <kbd>0</kbd>s. Take a look at the following code:</p>
<pre>df['Engaged'] = df['Response'].apply(lambda x: 1 if x == 'Yes' else 0)</pre>
<p>As you can see from this code, we are using the <kbd>apply</kbd> function of the <kbd>pandas</kbd> <kbd>DataFrame</kbd> to apply our <kbd>lambda</kbd> function on the <kbd>Response</kbd> column, so that it encodes <kbd>Yes</kbd> values with <kbd>1</kbd> and <kbd>No</kbd> values with <kbd>0</kbd>. We then store these encoded values in a newly-created column, <kbd>Engaged</kbd>. In order to get the overall response or engagement rate using this newly-created column, you can use the following code:</p>
<pre>tdf['Engaged'].mean()</pre>
<p>The overall engagement rate looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1252 image-border" src="assets/90aabaf8-3052-4f9f-a47f-57daf049fbe0.png" style="width:21.42em;height:5.00em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Categorical variable encoding</h1>
                </header>
            
            <article>
                
<p>If you look closely at the data, the following variables are categorical variables:</p>
<pre>columns_to_encode = [<br/>    'Sales Channel', 'Vehicle Size', 'Vehicle Class', 'Policy', 'Policy Type', <br/>    'EmploymentStatus', 'Marital Status', 'Education', 'Coverage'<br/>]</pre>
<p>These variables have a set of different values they can take and those values do not necessarily have orders that differentiate one from another.</p>
<p>If you recall from <a href="a9f09970-4826-46d0-8bfd-5796702c5629.xhtml" target="_blank">Chapter 4</a>, <em>From Engagement to Conversion</em>, there is more than one way to encode categorical variables. In this chapter, the method we are going to use is to create dummy variables for each category of individual categorical variables, using the <kbd>get_dummies</kbd> function in the <kbd>pandas</kbd> package. Take a look at the following code:</p>
<pre>categorical_features = []<br/>for col in columns_to_encode:<br/>   encoded_df = pd.get_dummies(df[col])<br/>    encoded_df.columns = [col.replace(' ', '.') + '.' + x for x in encoded_df.columns]<br/>    <br/>    categorical_features += list(encoded_df.columns)<br/>    <br/>    df = pd.concat([df, encoded_df], axis=1)</pre>
<p>As you can see from this code snippet, we are iterating through the list of column names of categorical variables, defined in <kbd>columns_to_encode</kbd>. Then, for each column, we are using the <kbd>get_dummies</kbd> function in the <kbd>pandas</kbd> package to build dummy variables. In order to make things clear and cause less confusion, we are renaming the columns of the newly-created and encoded <kbd>DataFrame</kbd> <kbd>encoded_df</kbd>, where each column contains information about the original column name and the category it represents. As an example, for the  <kbd>Sale Channel</kbd> column, the newly-created DataFrame <kbd>encoded_df</kbd> will look as in the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cea74d25-63a1-4b68-b9be-1041261dea91.png" style="width:37.50em;height:17.92em;"/></p>
<p>As you can see from this example, each column of this new <kbd>DataFrame</kbd> represents each category in the original <kbd>Sales Channel</kbd> <span>column </span>and the values are one-hot encoded, meaning it assigns a value of <kbd>1</kbd> if the given record belongs to the given category, and <kbd>0</kbd> otherwise.</p>
<p>Once we have created dummy variables for the given column, we then store the newly-created columns into a variable named <kbd>categorical_features</kbd>. Lastly, we concatenate this newly-created <kbd>DataFrame</kbd> to the original <kbd>DataFrame</kbd>, by using the <kbd>concat</kbd> function of the <kbd>pandas</kbd> package. One of the parameters in the concat function, <kbd>axis=1</kbd>, tells <kbd>pandas</kbd> to concatenate the two <kbd>DataFrames</kbd> by the column.</p>
<p>By now, we have successfully encoded all the categorical variables except <kbd>Gender</kbd>. Since we do not need to create two dummy variables for the <kbd>Gender</kbd> column, as there can only be two genders, we are going to create one variable that contains information about the gender of a given record. Take a look at the following code:</p>
<pre>df['Is.Female'] = df['Gender'].apply(lambda x: 1 if x == 'F' else 0)<br/><br/>categorical_features.append('Is.Female')</pre>
<p>As you can see from this code, we are creating a new column named <kbd>Is.Female</kbd>. We are using the apply function of the <kbd>pandas</kbd> <kbd>DataFrame</kbd> and encoding all females with the value of <kbd>1</kbd> and all males with the value of <kbd>0</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building predictive models</h1>
                </header>
            
            <article>
                
<p>We are almost ready to start building and training machine learning models to predict customer responses or engagements. There are a few things to clean up in our data. Take a look at the following code:</p>
<pre>all_features = continuous_features + categorical_features<br/>response = 'Engaged'<br/><br/>sample_df = df[all_features + [response]]<br/>sample_df.columns = [x.replace(' ', '.') for x in sample_df.columns]<br/>all_features = [x.replace(' ', '.') for x in all_features]</pre>
<p>As you can see from this code, we are creating a new <kbd>DataFrame</kbd> <kbd>sample_df</kbd>, which contains all the features, <kbd>all_features</kbd>, and the response variable, <kbd>response</kbd>. Then, we are cleaning up the column and feature names by replacing all the spaces in the names with dots. After these cleanups, DataFrame <kbd>sample_df</kbd> now looks as in the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/eb370c76-2e57-4457-a963-d7d7e9995c9f.png"/></p>
<p>Now that we have a sample set that we can train and test our machine learning models with, let's split this sample set into two subsets—one for training the models and another for testing and evaluating the trained models. The Python machine learning package, <kbd>scikit-learn</kbd>, has a function that splits a given sample set into train and test sets. Take a look at the following code:</p>
<pre>from sklearn.model_selection import train_test_split<br/><br/>x_train, x_test, y_train, y_test = train_test_split(sample_df[all_features], sample_df[response], test_size=0.3)</pre>
<p class="mce-root"/>
<p>In the <kbd>model_selection</kbd> module of the <kbd>scikit-learn</kbd> package, there is a function named <kbd>train_test_split</kbd>. This function takes the sample set and the desired breakdown between train and test set sizes as input parameters and returns train and test sets that are randomly split. As you can see from this code snippet, we are using <kbd>70%</kbd> of the sample set for training and the remaining <kbd>30%</kbd> for testing. The following shows the breakdowns of train and test sets from the sample set:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1255 image-border" src="assets/b4bae11b-4c12-4301-a00b-a4d6616a5795.png" style="width:25.67em;height:15.25em;"/></p>
<p>As you can see here, there are a total of <kbd>9,134</kbd> records in <kbd>sample_df</kbd>, <kbd>6,393</kbd> records in <kbd>x_train</kbd>, and <kbd>2,741</kbd> records in <kbd>x_test</kbd>, meaning that roughly <kbd>70%</kbd> of the sample set went into the train set and the remaining <kbd>30%</kbd> of the sample set went into the test set. We will be using these train and test sets for building and evaluating models in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forest model</h1>
                </header>
            
            <article>
                
<p>With the data that we have prepared so far, we are going to build a predictive model, using a random forest algorithm, which predicts whether a customer is going to respond or engage with the marketing campaign. In Python's <kbd>scikit-learn</kbd> package, the random forest algorithm is implemented in the <kbd>ensemble</kbd> module and you can import the random forest class using the following code:</p>
<pre>from sklearn.ensemble import RandomForestClassifier</pre>
<p>You can create a random forest classifier using the following code:</p>
<pre>rf_model = RandomForestClassifier()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>However, there are many hyperparameters you can tune for random forest models. Hyperparameters<span> are the parameters you define before you train a machine learning model. For example, in the case of a random forest algorithm, you can define the number of trees you want in your random forest model. As another example, you can define the maximum depth of each tree in the forest, so that you can limit how big each tree in the forest can grow.</span></p>
<p><span>There are numerous hyperparameters you can define in <kbd>scikit-learn</kbd>'s <kbd>RandomForestClassifier</kbd> class. We will take a look at the following few examples of hyperparameters:</span></p>
<ul>
<li><kbd>n_estimators</kbd>: This defines the number of trees you want to build in the forest. Generally speaking, more trees mean better performance results. However, the amount of performance gain for each additional tree decreases as the number of trees in the forest increases. Since having more trees in a forest means higher cost in computations for training additional trees, you should try to find the balance and stop adding trees when the computational cost from training additional trees outweighs the performance gain.</li>
<li><kbd>max_depth</kbd>: This parameter defines the maximum depth of individual trees. The larger the depth is, the more information your tree can capture from the train set, meaning larger trees learn the train set better than smaller trees. However, the larger the tree grows, the more likely it is going to overfit the train set. This means that the trained tree performs and predicts well within the train set, but predicts poorly in the dataset that it has not seen before. In order to avoid overfitting, we would want to limit the depth of the tree to a point where it does not overfit to the train set, but predicts the outcomes well enough.</li>
<li><kbd>min_samples_split</kbd>: This defines the minimum number of data points required to split a node of the tree. For example, if you defined <kbd>min_samples_split</kbd> to be <kbd>50</kbd>, but the node only has <kbd>40</kbd> records, then it will not split the node any further. On the other hand, if the node has more than the predefined minimum number of samples, then it will split the node into child nodes. Similar to the <kbd>max_depth</kbd> hyperparameter, this helps you manage the amount of overfitting happening in the tree.</li>
<li><kbd>max_features</kbd><span>: This defines the maximum number of features to be considered for splitting a node. This parameter creates the <em>randomness</em> in random forest models. Given the maximum number of features to be considered for a split, the random forest algorithm randomly chooses a subset of the features up to the maximum number and decides how to split a given node of a tree. This helps each tree of a random forest model to learn different information from the train set. When these trees that have learned the train set with slightly different set of features are bagged or ensembled all together, then the resulting forest will become more accurate and robust in its predictions.</span></li>
</ul>
<p>For a more detailed description and information on other hyperparameters, you can refer to their official documentation, which can be found at the following link: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>.<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a random forest model</h1>
                </header>
            
            <article>
                
<p>Training a random forest model using <kbd>scikit-learn</kbd> is simple. Take a look at the following code:</p>
<pre>rf_model = RandomForestClassifier(<br/>    n_estimators=200,<br/>    max_depth=5<br/>)<br/><br/>rf_model.fit(X=x_train, y=y_train)</pre>
<p class="mce-root">Using the <kbd>RandomforestClasifier</kbd> class in the <kbd>scikit-learn</kbd> package's <kbd>ensemble</kbd> module, you first need to create a <kbd><span>RandomforestClasifier</span></kbd> object with the hyperparameters. For illustration purposes, we are instructing the model to build <kbd>200</kbd> trees, where each tree can only grow up to the depth of <kbd>5</kbd>. Then, you can train this model with the <kbd>fit</kbd> function, which takes two parameters, <kbd>X</kbd> and <kbd>y</kbd>, where <kbd>X</kbd> is for the training samples and <kbd>y</kbd> is for the training labels or target values.</p>
<p>When you run this code, you will see an output that looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1257 image-border" src="assets/eb29841c-0a3c-423b-8568-410014d5215e.png" style="width:38.92em;height:15.83em;"/></p>
<p>Once a random forest model is trained or fitted, the model object contains a lot of useful information. One of the useful attributes you can extract from a trained <kbd>scikit-learn</kbd> random forest model is the information about individual trees in the forest. Using the <kbd>estimators_</kbd><span> attribute,</span> you can retrieve the individual trees that are built within the forest. Take a look at the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1258 image-border" src="assets/08d11a03-fa14-417c-aa6e-c7f2b7750427.png" style="width:41.58em;height:28.00em;"/></p>
<p>As you can see from this output, the <kbd>estimators_</kbd> attribute returns a list of sub-estimators, which are decision trees. With this information, you can simulate what each of these sub-estimators predicts for each input. For example, the following code shows how you can get the predictions from the first sub-estimator in the forest:</p>
<pre>rf_model.estimators_[0].predict(x_test)</pre>
<p>The following output shows some of the predictions from the first five sub-estimators:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1259 image-border" src="assets/db72f840-2ef0-4e2f-a4c0-8526c150a6c1.png" style="width:34.58em;height:21.92em;"/></p>
<p>As you can see from this output, different trees predict differently for each record of the test set. This is because each tree is trained with different subsets of features that are randomly selected. Let's take a quick look at the predictions of these individual sub-estimators. The first tree predicts the <kbd>6th</kbd> record in the test set to be a class of <kbd>1</kbd> and the rest to be a class of <kbd>0</kbd>. On the other hand, the second tree predicts that the first 10 records of the test set to be a class of <kbd>0</kbd>. Using this information, you can see how the final predictions from the random forest model are formed from these individual sub-estimators or trees.</p>
<p>Other useful information that we can gain from the trained <kbd>RandomForestClassifier</kbd> object is the feature importances, with which we can understand the importance or the impact of each feature on the final predictions. You can get the feature importances for each feature using the following code:</p>
<pre>rf_model.feature_importances_</pre>
<p>The output of this code looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1260 image-border" src="assets/1b9b3774-34bf-4bb3-8431-5be4b5e0c88f.png" style="width:32.33em;height:12.50em;"/></p>
<p>In order to associate these feature importances with the corresponding features, you can use the following code:</p>
<pre>feature_importance_df = pd.DataFrame(list(zip(rf_model.feature_importances_, all_features)))<br/>feature_importance_df.columns = ['feature.importance', 'feature']</pre>
<p>The result looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9482caaa-0e7e-418d-a0fc-eaa2ca7edc89.png" style="width:43.17em;height:28.83em;"/></p>
<p class="mce-root"/>
<p>As you can see from this output, the <kbd>EmploymentStatus.Retired</kbd> <span>feature </span>seems to be the most important factor in making the final prediction and the <kbd>Income</kbd>, <kbd>Total.Claim.Amount</kbd>, and <kbd>Customer.Lifetime.Value</kbd> <span>features </span>follow as the second, third, and fourth most important features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating a classification model</h1>
                </header>
            
            <article>
                
<p>Earlier in this chapter, we have discussed five different ways to look at the performance of a classification model. In this section, we are going to learn how we can compute and visualize the metrics for evaluating a classification model in Python using the random forest model we have just built.</p>
<p>The first three metrics that we are going to look at are accuracy, precision, and recall. Python's <kbd>scikit-learn</kbd> package has implemented functions for these three metrics. You can import these functions using the following line of code:</p>
<pre>from sklearn.metrics import accuracy_score, precision_score, recall_score</pre>
<p>As you can see from this code snippet, the <kbd>metrics</kbd> module of the <kbd>scikit-learn</kbd> package has an <kbd>accuracy_score</kbd> function for calculating the accuracy of a model, a <kbd>precision_score</kbd> function for the precision, and a <kbd>recall_score</kbd> function for the recall. </p>
<p>Before we go ahead and evaluate the model performance, we will need the model prediction results. In order to have the random forest model we have built in the previous section to make predictions on a dataset, we can simply use the <kbd>predict</kbd> function of the model. Take a look at the following code:</p>
<pre>in_sample_preds = rf_model.predict(x_train)<br/>out_sample_preds = rf_model.predict(x_test)</pre>
<p>With these prediction results, we are going to evaluate how well our random forest model performs in the train and test sets. The following code shows how we can use the <kbd>accuracy_score</kbd>, <kbd>precision_score</kbd>, and <kbd>recall_score</kbd> functions in the <kbd>scikit-learn</kbd> package:</p>
<pre># accuracy<br/>accuracy_score(actual, predictions)<br/><br/># precision <br/>precision_score(actual, predictions)<br/><br/># recall <br/>recall_score(actual, predictions)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>As you can see from this code, the <kbd>accuracy_score</kbd>, <kbd>precision_score</kbd>, and <kbd>recall_score</kbd> functions all take two parameters—truth labels and predicted labels. Take a look at the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1262 image-border" src="assets/d6d2fa84-f2c8-4adc-95d2-47d878b3792f.png" style="width:133.17em;height:75.67em;"/></p>
<p>This output gives us a brief overview of how well our model performs at predicting the responses. For the train set, the accuracy of the overall prediction was <kbd>0.8724</kbd>, meaning the model prediction was correct for about <kbd>87%</kbd> of the time. For the test set, the accuracy of the overall prediction was <kbd>0.8818</kbd>, which is roughly on the same line with the prediction accuracy within the train set. You can also see that the precision for in-sample and out-of-sample predictions were <kbd>0.9919</kbd> and <kbd>0.9423</kbd> respectively, and the recalls were <kbd>0.1311</kbd> and <kbd>0.1324</kbd>. Due to the randomness and the different hyperparameters you might have used, you can get different results.</p>
<p>The next set of metrics we are going to look at are the ROC curve and the AUC. The <kbd>metrics</kbd> module in the <kbd>scikit-learn</kbd> package has handy functions for the ROC curve and the AUC. Take a look at the following line of code:</p>
<pre>from sklearn.metrics import roc_curve, auc</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The <kbd>roc_curve</kbd> <span>function </span>in the metrics module of the <kbd>scikit-learn</kbd> package computes the ROC, and the <kbd>auc</kbd> <span>function </span>computes the AUC. In order to compute the ROC and AUC using these functions, we need to first get the prediction probabilities from our random forest model. The following code shows how we can get the random forest model's prediction probabilities for both the train and test sets:</p>
<pre>in_sample_preds = rf_model.predict_proba(x_train)[:,1]<br/>out_sample_preds = rf_model.predict_proba(x_test)[:,1]</pre>
<p>As you can see from this code, we are using the <kbd>predict_proba</kbd> function of the random forest model, <kbd>rf_model</kbd>. This function outputs the predicted probabilities of the given record belonging to each class. Since we only have two possible classes in our case, <kbd>0</kbd> for no responses and <kbd>1</kbd> for responses, the output of the <kbd>predict_proba</kbd> function has two columns, where the first column represents the predicted probability of a negative class, meaning no response for each record, and the second column represents the predicted probability of a positive class, meaning a response for each record. Since we are only interested in the likelihood of responding to the marketing effort, we can take the second column for the predicted probabilities of the positive class.</p>
<p>With these predicted probabilities of the positive class for both the train and test sets, we can now compute the ROC curve and AUC. Let's first take a look at how we can compute the ROC curve using the <kbd>roc_curve</kbd> function in the following code:</p>
<pre>in_sample_fpr, in_sample_tpr, in_sample_thresholds = roc_curve(y_train, in_sample_preds)<br/>out_sample_fpr, out_sample_tpr, out_sample_thresholds = roc_curve(y_test, out_sample_preds)</pre>
<p>As you can see from this code snippet, the <kbd>roc_curve</kbd> function takes two parameters—observed labels and predicted probabilities. This function returns three variables, <kbd>fpr</kbd>, <kbd>tpr</kbd>, and <kbd>thresholds</kbd>. The <kbd>fpr</kbd> values represent the false positive rates for each given threshold and the <kbd>tpr</kbd> values represent the true positive rates for each given threshold. The <kbd>thresholds</kbd> values represent the actual thresholds at which <kbd>fpr</kbd> and <kbd>tpr</kbd> are measured.</p>
<p>Next, with these <kbd>fpr</kbd> and <kbd>tpr</kbd> values, we can compute the AUC using the following code:</p>
<pre>in_sample_roc_auc = auc(in_sample_fpr, in_sample_tpr)<br/>out_sample_roc_auc = auc(out_sample_fpr, out_sample_tpr)<br/><br/>print('In-Sample AUC: %0.4f' % in_sample_roc_auc)<br/>print('Out-Sample AUC: %0.4f' % out_sample_roc_auc)</pre>
<p class="mce-root"/>
<p>As you can see from this code, the <kbd>auc</kbd> function takes two parameters—<kbd>fpr</kbd> and <kbd>tpr</kbd>. Using the previously calculated <kbd>fpr</kbd> and <kbd>tpr</kbd> values from the <kbd>roc_curve</kbd> function, we can easily compute the AUC numbers for both the train and test sets. The output looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1263 image-border" src="assets/0bd94f4b-6d28-4250-a692-262db25c71ae.png" style="width:38.92em;height:10.42em;"/></p>
<p>Depending on the hyperparameters and the randomness within the random forest algorithm, your AUC numbers can look different from these examples. However, in our case, the in-sample train set AUC was <kbd>0.8745</kbd> and the out-of-sample test set AUC was <kbd>0.8425</kbd>. If you see a big gap between these two numbers, it is a sign of overfitting and you should try to address it by pruning the trees in the forest by tuning the hyperparameters, such as the maximum depth and minimum number of samples to split.</p>
<p>The last thing we are going to look at for evaluating machine learning models is the actual ROC curve. With the output of the <kbd>roc_curve</kbd> function, we can plot the actual ROC curves using the <kbd>matplotlib</kbd> package. Take a look at the following code:</p>
<pre>plt.figure(figsize=(10,7))<br/><br/>plt.plot(<br/>    out_sample_fpr, out_sample_tpr, color='darkorange', label='Out-Sample ROC curve (area = %0.4f)' % in_sample_roc_auc<br/>)<br/>plt.plot(<br/>    in_sample_fpr, in_sample_tpr, color='navy', label='In-Sample ROC curve (area = %0.4f)' % out_sample_roc_auc<br/>)<br/>plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')<br/>plt.grid()<br/>plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.05])<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.title('RandomForest Model ROC Curve')<br/>plt.legend(loc="lower right")<br/><br/>plt.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>As you can see from this code, we are plotting three line plots—one for the out-of-sample test set ROC curve, another for the in-sample train set ROC curve, and lastly one for a straight line for the benchmark. The result looks as in the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1264 image-border" src="assets/0a2dedf6-0a8c-4f20-909b-6d231c502508.png" style="width:41.58em;height:30.33em;"/></p>
<p>As you can see from this plot, it is easier to see and compare the overall performance of the model between the train and test sets with ROC curves. The larger the gap between the in-sample ROC curve and the out-of-sample ROC curve, the more the model is overfitting to the train set and fails to generalize the findings for unforeseen data.</p>
<div class="packt_infobox">The full code for this Python exercise can be found at the following link: <a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.8/python/PredictingEngagement.ipynb">https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.8/python/PredictingEngagement.ipynb</a></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting the likelihood of marketing engagement with R</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to discuss how to build predictive models using machine learning algorithms in R. More specifically, we will learn how to build a predictive model using a random forest algorithm, as well as how to tune the random forest model, and evaluate the performance of the model. We will be mainly using the <kbd><span>caTools</span></kbd>, <kbd>ROCR</kbd>, and <kbd>randomForest</kbd> packages to evaluate, visualize, and build machine learning models that predict the likelihood of customer marketing engagement. For those readers who would like to use Python instead of R for this exercise, you can refer to the previous section.</p>
<p><span>For this exercise, we will be using one of the publicly available datasets from </span><strong>IBM</strong><span>, which can be found at this link: <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/">https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/</a></span><span>. You can follow this link and download the data that is available in CSV format, named <kbd>WA_Fn UseC_ Marketing Customer Value Analysis.csv</kbd>. Once you have downloaded this data, you can load it into your RStudio by running the following command:</span></p>
<pre>#### 1. Load Data ####<br/>df &lt;- read.csv(<br/>  file="~/Documents/data-science-for-marketing/ch.8/data/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv", <br/>  header=TRUE<br/>)</pre>
<p><kbd>df</kbd> looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1265 image-border" src="assets/5ae79a64-9bbd-424d-9259-21bdec131954.png" style="width:162.50em;height:56.83em;"/></p>
<p class="mce-root"/>
<p>As you might have noticed, this is the same dataset that we used in the previous chapter, where we conducted customer analytics. With the knowledge we gained about this dataset from the previous chapter, we are going to first prepare our data by encoding the target variable and other categorical variables that we are going to use as features for our machine learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variable encoding</h1>
                </header>
            
            <article>
                
<p>In order to build machine learning models in R, all the features in the dataset need to have numerical values. However, in the dataset we have numerous columns that have non-numerical values. For example, the target variable,<span> </span><kbd>Response</kbd>, which is what we are going to try to predict with our machine learning models, is non-numeric. It contains two string values—<kbd>Yes</kbd><span> </span>or<span> </span><kbd>No</kbd>. We will need to encode this <kbd>Response</kbd> <span>target variable</span><span> </span>with numerical values in order to be able to build machine learning models. For another example, the <kbd>Gender</kbd> column, which we can use as one of the features for our predictive model, also does not have numerical values. It contains two string values—<kbd>F</kbd><span> </span>for female and<span> </span><kbd>M</kbd><span> </span>for male. In this section, we are going to discuss how we can encode these non-numeric columns, so that we can use them as features in machine learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Response variable encoding</h1>
                </header>
            
            <article>
                
<p>The first thing we are going to do is encode the response variable,<span> </span><kbd>Response</kbd>. We are going to encode<span> </span><kbd>Yes</kbd><span> </span>values with<span> </span><kbd>1</kbd>s and<span> </span><kbd>No</kbd><span> </span>values with<span> </span><kbd>0</kbd>s. Take a look at the following code:</p>
<pre>## 2.1. Response Variable: Response<br/>df$Engaged &lt;- as.integer(df$Response) - 1</pre>
<p>As you can see from this code, we are simply casting the values of the <kbd>Response</kbd> column to integer values using the <kbd>as.integer</kbd> function. The reason why we are subtracting by <kbd>1</kbd> is because it encodes values into <kbd>1</kbd> for <kbd>No</kbd> and <kbd>2</kbd> for <kbd>Yes</kbd>, instead of <kbd>0</kbd> for <kbd>No</kbd> and <kbd>1</kbd> for <kbd>Yes</kbd>, as we wanted. We then store these encoded values in a newly-created column,<span> </span><kbd>Engaged</kbd>. In order to get the overall response or engagement rate using this newly-created column, you can use the following code:</p>
<pre>mean(df$Engaged)</pre>
<p>The overall engagement rate looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1266 image-border" src="assets/aa44411e-2bd6-4206-938f-11a79d3908ec.png" style="width:10.42em;height:2.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Categorical variable encoding</h1>
                </header>
            
            <article>
                
<p>If you look closely at the data, the following columns are categorical variables in our dataset:</p>
<pre>## 2.2. Categorical Features<br/><br/>categoricalVars = c(<br/>  'Sales.Channel', 'Vehicle.Size', 'Vehicle.Class', 'Policy', 'Policy.Type',<br/>  'EmploymentStatus', 'Marital.Status', 'Education', 'Coverage', 'Gender'<br/>)</pre>
<p>These variables have a set of different values they can take and these values do not necessarily have orders that differentiate one from another.</p>
<p>If you recall from <a href="a9f09970-4826-46d0-8bfd-5796702c5629.xhtml" target="_blank">Chapter 4</a>, <em>From Engagement to Conversion</em>, we discussed how we can create factor variables for such categorical variables in R. In this chapter, the method we are going to use is to create dummy variables for each category of individual categorical variables, using the <kbd>model.matrix</kbd><span> </span>function in R. Take a look at the following code:</p>
<pre>encodedDF &lt;- model.matrix(~.-1, df[categoricalVars])</pre>
<p>As you can see from this code, it is simple to create dummy variables for categorical variables in R. All you need to do is to apply the <kbd>model.matrix</kbd> function on the R <kbd>DataFrame</kbd>'s categorical variable columns. If you look closely at the code, you will notice the <kbd>~.-1</kbd> <span>formula that </span>we are using here. Without this formula, the <kbd>model.matrix</kbd> function will create an unnecessary column named <kbd>Intercept</kbd> in the output matrix. In order to avoid having this unnecessary column, we can use the formula in this code example. The first few columns of the newly-created<span> </span><kbd>DataFrame</kbd><span> </span><kbd>encodedDf</kbd> now look as in the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1267 image-border" src="assets/b03f57a2-e6e4-43cd-9918-6b10293bf4b2.png" style="width:43.33em;height:17.00em;"/></p>
<p>As you can see from this output, each column of this new<span> </span><kbd>DataFrame</kbd><span> </span>represents each category in the original column. For example, the first column, <kbd>Sales.ChannelAgent</kbd>, is encoded with <kbd>1</kbd> if the given record or customer was reached out by a sales agent and <kbd>0</kbd> otherwise. For another example, the fifth column, <kbd>Vehicle.SizeMedsize</kbd>, is encoded with <kbd>1</kbd> if the given record or customer has medium-size vehicles and <kbd>0</kbd> otherwise.</p>
<p>Now that we have successfully encoded all the categorical variables with numerical values, we need to append the continuous variables to this newly-created <kbd>DataFrame</kbd>, <kbd>encodedDF</kbd>. Take a look at the following code:</p>
<pre>## 2.3. Continuous Features<br/>continuousFeatures &lt;- c(<br/>  'Customer.Lifetime.Value', 'Income', 'Monthly.Premium.Auto',<br/>  'Months.Since.Last.Claim', 'Months.Since.Policy.Inception',<br/>  'Number.of.Open.Complaints', 'Number.of.Policies', 'Total.Claim.Amount'<br/>)<br/><br/>encodedDF &lt;- cbind(encodedDF, df[continuousFeatures])</pre>
<p>As you can see from this code, we are using the <kbd>cbind</kbd> R function, which combines two DataFrames by columns. We are combining the previously-created DataFrame <kbd>encodedDF</kbd>, which contains all the encoded categorical variables with the DataFrame with continuous variables. Then, we are storing this combined <kbd>DataFrame</kbd> back to the <kbd>encodedDF</kbd> variable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building predictive models</h1>
                </header>
            
            <article>
                
<p>We are almost ready to start building and training machine learning models to predict customer responses or engagements. There is one thing we need to do before we start training a random forest model. We need to split the sample set, the <kbd>encodedDF</kbd> variable, into two subsets—one for training the models and another for testing and evaluating the trained models. The <kbd>caTools</kbd> <span>R package</span><span> </span>has a handy function that splits a given sample set into train and test sets. If you do not have this library installed in your R environment, you can install it using the following command:</p>
<pre>install.packages('caTools')</pre>
<p>Now, take a look at the following code on how to split the sample set into training and testing:</p>
<pre>library(caTools)<br/><br/>sample &lt;- sample.split(df$Customer, SplitRatio = .7)<br/><br/>trainX &lt;- as.matrix(subset(encodedDF, sample == TRUE))<br/>trainY &lt;- as.double(as.matrix(subset(df$Engaged, sample == TRUE)))<br/><br/>testX &lt;- as.matrix(subset(encodedDF, sample == FALSE))<br/>testY &lt;- as.double(as.matrix(subset(df$Engaged, sample == FALSE)))</pre>
<p>Let's take a closer look at this code. The <kbd>sample.split</kbd> function in the <kbd>caTools</kbd> package lets us split the dataset into a proportion we would like. As you can see from this code, we defined <kbd>SplitRatio</kbd> to be <kbd>0.7</kbd>, which means we are taking <kbd>70%</kbd> of the sample set as a training set and the remaining <kbd>30%</kbd> of the <kbd>sample</kbd> set as a test set. The resulting variable, sample, now has an array of Boolean values, <kbd>TRUE</kbd> or <kbd>FALSE</kbd>, where <kbd>70%</kbd> of the arrays are <kbd>TRUE</kbd> and the remaining <kbd>30%</kbd> are <kbd>FALSE</kbd>. </p>
<p>With this data, we can create train and test sets. As you can see from the code, we are using the <kbd>subset</kbd> function in R to create train and test sets. First, we take those records that correspond to <kbd>TRUE</kbd> values in the <kbd>sample</kbd> <span>variable </span>as the train set. Then, we take those records whose indexes correspond to <kbd>FALSE</kbd> values in the <kbd>sample</kbd> <span>variable </span>as the test set. The following shows the breakdown of train and test sets from the sample set:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1268 image-border" src="assets/9b3580ee-87ad-4281-9f57-cc83a7323b88.png" style="width:6.58em;height:7.17em;"/></p>
<p class="mce-root"/>
<p>As you can see here, there are a total of<span> </span><kbd>9,134</kbd><span> </span>records in<span> </span><kbd>encodedDF</kbd>,<span> </span><kbd>6,393</kbd><span> </span>records in<span> </span><kbd>trainX</kbd>, and<span> </span><kbd>2,741</kbd><span> </span>records in<span> </span><kbd>testX</kbd><span>, </span>meaning that roughly<span> </span><kbd>70%</kbd><span> </span>of the sample set went into the the train set and the remaining <kbd>30%</kbd> of the sample set went into the test set. We will be using these train and test sets for building and evaluating models in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forest model</h1>
                </header>
            
            <article>
                
<p>With the data that we have prepared so far, we are going to build a predictive model using a random forest algorithm, which predicts whether a customer is going to respond or engage with the marketing campaign. We are going to use the <kbd>randomForest</kbd> R library. If you do not have this library installed in your R environment, you can install it using the following command:</p>
<pre>install.packages('randomForest')</pre>
<p>Once you have this package installed, you can use the following code to build a random forest model:</p>
<pre>library(randomForest)<br/><br/>rfModel &lt;- randomForest(x=trainX, y=factor(trainY))</pre>
<p>However, there are many hyperparameters you can tune for random forest models. Hyperparameters<span> are the parameters you define before you train a machine learning model. For example, in the case of a random forest algorithm, you can define the number of trees you want in your random forest model. As another example, you can define the maximum number of terminal nodes for each tree in the forest, so that you can limit how big each tree in the forest can grow.</span></p>
<p>There are numerous hyperparameters you can define and fine-tune. We will take a look at a few of these hyperparameters:</p>
<ul>
<li><strong>ntree:</strong> This defines the number of trees you want to build in the forest. Generally speaking, more trees mean better performance results. However, the amount of performance gain for each additional tree decreases as the number of trees in the forest increases. Since having more trees in a forest means higher cost in computations for training additional trees, you should try to find the balance and stop adding trees when the computational cost from training additional trees outweighs the performance gain.</li>
<li><strong><span>sampsize</span></strong>: This parameter defines the size of the sample to draw for training each tree. This introduces randomness in the forest, while training a random forest model. Having a high sample size results in a less random forest and has a higher chance of overfitting. This means that the trained tree performs and predicts well within the train set, but predicts poorly in the dataset that it has not seen before. Decreasing the sample size can help you avoid overfitting, but the performance of your model usually decreases as you decrease the sample size.</li>
<li><span><strong>nodesize</strong>: This parameter defines the minimum size of the terminal nodes, which means how many samples each terminal node needs to have at the very least. The larger this number is, the smaller the tree can grow. As you increase this number, you can mitigate the overfitting issues, but at the expense of the model performance.</span></li>
<li><span><strong>maxnodes</strong>: This parameter defines the maximum number of terminal nodes each tree in the forest can have. If you do not set this number, the algorithm is going to grow the tree to the fullest. This can result in overfitting the train set. Reducing the maximum number of terminal nodes can help you overcome overfitting issues.</span></li>
</ul>
<p>For a more detailed description and information on other hyperparameters, you can refer to the official documentation that can be found at the following link: <a href="https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest">https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a random forest model</h1>
                </header>
            
            <article>
                
<p>Training a random forest model using the<span> </span><kbd>randomForest</kbd><span> package </span>is simple. Take a look at the following code:</p>
<pre>rfModel &lt;- randomForest(x=trainX, y=factor(trainY), ntree=200, maxnodes=24)</pre>
<p class="mce-root">Using the <kbd>randomForest</kbd> function in the <kbd>randomForest</kbd> package, you can easily train a random forest model. You just need to supply the train set to the function. For illustration purposes, we are instructing the model to build <kbd>200</kbd> trees, where each tree can only grow up to <kbd>24</kbd> terminal nodes. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When you run this code, your model object will look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1269 image-border" src="assets/9c563e81-5c46-48cd-a297-3f7600443bca.png" style="width:61.17em;height:33.25em;"/></p>
<p>Once a random forest model is trained or fitted, the model object contains a lot of useful information. One of the useful attributes you can extract from a trained<span> </span>random forest model is the information about individual trees in the forest. Using the <kbd>getTree</kbd> function, you can retrieve how the individual trees are built within the forest. Take a look at the following example:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1270 image-border" src="assets/d829463f-5d3e-40ad-9886-e52700eec057.png" style="width:28.83em;height:20.50em;"/></p>
<p>Here we are looking at the information about the first tree in the forest. This gives us some information about the structure of the tree. The <kbd>left daughter</kbd> and <kbd>right daughter</kbd> <span>columns </span>tell us the location of this node in the given tree. The <kbd>status</kbd> <span>column </span>tells us whether the node is terminal (<kbd>-1</kbd>) or not (<kbd>1</kbd>). The <kbd>prediction</kbd> <span>column </span>tells us the prediction from this node.</p>
<p>Other information we can get from the fitted random forest model is the prediction from each tree in the forest. Take a look at the following code:</p>
<pre>predict(rfModel, trainX, predict.all=TRUE)</pre>
<p>By using the <kbd>predict.all=TRUE</kbd> flag, the <kbd>prediction</kbd> function returns the predictions from each tree in the forest. Take a look at the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1271 image-border" src="assets/09bc2153-ddd4-4d9c-bcd7-64668f852017.png" style="width:139.75em;height:20.33em;"/></p>
<p class="mce-root"/>
<p>This output is showing the first 20 trees' predictions for the first five records in the train set. As you can see from this output, the <kbd>10<sup>th</sup></kbd> tree in the forest predicted the <kbd>5<sup>th</sup></kbd> record in the train set to be a class of <kbd>1</kbd>, but all the other 19 trees predicted the <kbd>5<sup>th</sup></kbd> record in the train set to be a class of <kbd>0</kbd>. As you can see from this output, different trees predict differently for each record of the test set. This is because each tree is trained with different subsets of features that are randomly selected. Using this information, you can see how the final predictions from the random forest model are formed from these individual sub-estimators or trees.</p>
<p>Other useful information that we can gain from a trained <kbd>randomForest</kbd><span> </span>object is the feature importances, with which we can understand the importance or the impact of each feature on the final predictions. You can get the feature importances for each feature using the following code:</p>
<pre># - Feature Importances<br/>importance(rfModel)</pre>
<p>Part of the output of this code looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1272 image-border" src="assets/14bab6a6-f5a2-4ff3-9c5c-b40bb0a74b48.png" style="width:24.83em;height:33.33em;"/></p>
<p>As you can see from this output, the <kbd>EmploymentStatusRetired</kbd> <span>feature</span><span> </span>seems to be the most important factor in making the final prediction and the <kbd>Income</kbd>,<span> </span><kbd>Total.Claim.Amount</kbd>, and<span> </span><kbd>Customer.Lifetime.Value</kbd> <span>features</span><span> </span>follow as the second, third, and fourth most important features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating a classification model</h1>
                </header>
            
            <article>
                
<p>Earlier in this chapter, we discussed five different ways to look at the performance of a classification model. In this section, we are going to learn how we can compute and visualize the metrics for evaluating a classification model in R using the random forest model we have just built.</p>
<p>The first three metrics that we are going to look at are accuracy, precision, and recall. Before we go ahead and evaluate the model performance, we will need the model prediction results. In order to have the random forest model we have built in the previous section make predictions on a dataset, we can simply use the<span> </span><kbd>predict</kbd><span> function</span>. Take a look at the following code:</p>
<pre>inSamplePreds &lt;- as.double(predict(rfModel, trainX)) - 1<br/>outSamplePreds &lt;- as.double(predict(rfModel, testX)) - 1</pre>
<p>With these prediction results, we are going to evaluate how well our random forest model performs in the train and test sets. The following code shows how we can compute accuracy, precision, and recall in R:</p>
<pre># - Accuracy<br/>accuracy &lt;- mean(testY == outSamplePreds)<br/><br/># - Precision<br/>precision &lt;- sum(outSamplePreds &amp; testY) / sum(outSamplePreds)<br/><br/># - Recall<br/>recall &lt;- sum(outSamplePreds &amp; testY) / sum(testY)</pre>
<p>Using this method, we can compare the in-sample train set <kbd>accuracy</kbd>, <kbd>precision</kbd>, and <kbd>recall</kbd> against the out-of-sample test set's <kbd>accuracy</kbd>, <kbd>precision</kbd>, and <kbd>recall</kbd>. Take a look at the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1273 image-border" src="assets/cc2daec3-a359-482b-b2d6-c4f83f2803b4.png" style="width:39.33em;height:26.00em;"/></p>
<p>This output gives us a brief overview of how well our model performs at predicting the responses. For the train set, the accuracy of the overall prediction was<span> </span><kbd>0.8756</kbd>, meaning the model prediction was correct for about<span> </span><kbd>88%</kbd><span> </span>of the time. For the test set, the accuracy of the overall prediction was<span> </span><kbd>0.8636</kbd>. You can also find that the precisions for in-sample and out-of-sample predictions were<span> </span><kbd>0.9717</kbd><span> </span>and<span> </span><kbd>0.8980</kbd><span> </span>respectively, and the recalls were<span> </span><kbd>0.1151</kbd><span> </span>and<span> </span><kbd>0.1065</kbd>. Due to the randomness and the different hyperparameters you might have used, you might get different results.</p>
<p>The next set of metrics we are going to look at are the ROC curve and the AUC. We are going to use the <kbd>ROCR</kbd> R package. If you do not have this package installed in your R environment, you can install it using the following command:</p>
<pre>install.packages('ROCR')</pre>
<p>Take a look at the following code for the ROC curve and the AUC number first:</p>
<pre>library(ROCR)<br/><br/>inSamplePredProbs &lt;- as.double(predict(rfModel, trainX, type='prob')[,2])<br/>outSamplePredProbs &lt;- as.double(predict(rfModel, testX, type='prob')[,2])<br/><br/>pred &lt;- prediction(outSamplePredProbs, testY)<br/>perf &lt;- performance(pred, measure = "tpr", x.measure = "fpr") <br/>auc &lt;- performance(pred, measure='auc')@y.values[[1]]<br/><br/>plot(<br/>  perf, <br/>  main=sprintf('Random Forest Model ROC Curve (AUC: %0.2f)', auc), <br/>  col='darkorange', <br/>  lwd=2<br/>) + grid()<br/>abline(a = 0, b = 1, col='darkgray', lty=3, lwd=2)</pre>
<p>The first thing we need to do is to get the predicted probabilities from the model we have built. Using the <kbd>predict</kbd> function and the <kbd>type='prob'</kbd> flag, we can get the predicted probabilities from the random forest model. Then, we are using the <kbd>prediction</kbd> function in the <kbd>ROCR</kbd> package. This function computes the number of true positives and false positives at different probability cutoffs that we need for the ROC curve. Using the output of the <kbd>prediction</kbd> function, we can then get the true positive rates and false positive rates at different probability cutoffs with the <kbd>performance</kbd> function in the <kbd>ROCR</kbd> package. Lastly, in order to get the AUC number, we can use the same <kbd>performance</kbd> function with a different flag, <kbd>measure='auc'</kbd>. </p>
<p>With this data, we can now plot the ROC curve. Using the <kbd>plot</kbd> function and the <kbd>perf</kbd> variable, which is the output of the <kbd>performance</kbd> function, we can plot the ROC curve. The plot looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1274 image-border" src="assets/6e8169d1-ce56-4e05-b2af-e80266d6f90a.png" style="width:29.25em;height:25.33em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As you can see from this plot, the AUC of our random forest model was <kbd>0.76</kbd>. Compared to the benchmark straight line, which represents the random line, the model performs much better, and this shows that the model predictions are much better than random predictions.</p>
<div class="packt_infobox">The full code for this R exercise can be found at the following link: <a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.8/R/PredictingEngagement.R">https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.8/R/PredictingEngagement.R</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed predictive analytics and its applications in marketing. We first discussed what predictive analytics is and how it is used in various other industries, such as in the financial and healthcare industries. Then we discussed four common use cases of predictive analytics in marketing—likelihood of engagement, customer lifetime value, recommending the right products and contents, and customer acquisition and retention. There can be numerous other use cases of predictive analytics in marketing, so we recommend you keep up with the latest news on how predictive analytics can be used in marketing industries. We then discussed five different ways to evaluate the performances of predictive models—accuracy, precision, recall, the ROC curve, and the AUC.</p>
<p>In the following chapter, we are going to expand our knowledge of predictive analytics. We are going to discuss the concept and importance of measuring customer lifetime value, as well as building machine learning models for customer lifetime value predictions.</p>


            </article>

            
        </section>
    </body></html>