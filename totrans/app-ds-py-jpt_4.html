<html><head></head><body><div><div><p class="hidden">&gt;</p>
		</div>
		<div><h1 id="_idParaDest-69"><a id="_idTextAnchor071"/>Appendix A</h1>
		</div>
		<div><h2><a id="_idTextAnchor072"/>About</h2>
			<p>This section is included to assist the students to perform the activities present in the book. It includes detailed steps that are to be performed by the students to complete and achieve the objectives of the activity.</p>
		</div>
		<div><h2 id="_idParaDest-70"><a id="_idTextAnchor073"/>Chapter 1: Jupyter Fundamentals</h2>
			<h3 id="_idParaDest-71"><a id="_idTextAnchor074"/>Activity 1: Building a Third-Order Polynomial Model</h3>
			<ol>
				<li>Scroll to the empty cells at the bottom of <code>Subtopic C</code> in your Jupyter Notebook. </li>
				<li>These will be found beneath the linear-model <code>Activity</code> heading.<h4>Note</h4><p class="callout">You should fill these empty cells in with code as we complete the activity. You may need to insert new cells as these become filled up; please do so as needed.</p></li>
				<li>We will first pull out our dependent feature from and target variable from <code>df.</code> using the following:<pre>y = df['MEDV'].values
x = df['LSTAT'].values.reshape(-1,1)</pre><p>This is identical to what we did earlier for the linear model.</p></li>
				<li>Verify what <code>x</code> looks like by printing the first few samples with <code>print(x[:3])</code>:<div><img src="img/C13018_01_491.jpg" alt="Figure 1.49: Printing first three values of x using print()" width="921" height="238"/></div><h6>Figure 1.49: Printing first three values of x using print()</h6><p>Notice how each element in the array is itself an array with length 1. This is what <code>reshape(-1,1)</code> does, and it is the form expected by scikit-learn.</p></li>
				<li>Transform <code>x</code> into "polynomial features" by importing the appropriate transformation tool from scikit-learn and instantiating the third-degree polynomial feature transformer:<pre>from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=3)</pre><p>The rationale for this step may not be immediately obvious but will be explained shortly.</p></li>
				<li>Transform the <code>x</code>) by running the <code>fit_transform</code> method. Build the polynomial feature set by running the following code:<pre>x_poly = poly.fit_transform(x)</pre><p>Here, we have used the instance of the transformer feature to transform the LSTAT variable.</p></li>
				<li>Verify what <code>x_poly</code> looks like by printing the first few samples with <code>print(x_poly[:3])</code>.<div><img src="img/C13018_01_501.jpg" alt="Figure 1.50: Printing first three values of x_poly using print()" width="1251" height="250"/></div><h6>Figure 1.50: Printing first three values of x_poly using print()</h6><p>Unlike <em class="italics">x</em>, the arrays in each row now have length 4, where the values have been calculated as <em class="italics">xº</em>, <em class="italics">x¹</em>, <em class="italics">x²</em> and <em class="italics">x³</em>.</p><p>We are now going to use this data to fit a linear model. Labeling the features as <em class="italics">a</em>, <em class="italics">b</em>, <em class="italics">c</em>, and <em class="italics">d</em>, we will calculate the coefficients αₒ, α1, α2, and α3 and of the linear model:</p><div><img src="img/C13018_01_50_a.jpg" alt="" width="1638" height="92"/></div><p>We can plug in the definitions of a, b, c, and d, to get the following polynomial model, where the coefficients are the same as the previous ones:</p><div><img src="img/C13018_01_50_b.jpg" alt="" width="1464" height="94"/></div></li>
				<li>Import the <code>LinearRegression</code> class and build our linear classification model the same way as done while calculating the MSE. Run the following:<pre>from sklearn.linear_model import LinearRegression clf = 
LinearRegression()
clf.fit(x_poly, y)</pre></li>
				<li>Extract the coefficients and print the polynomial model using the following code:<pre>a_0 = clf.intercept_ + clf.coef_[0] # intercept a_1, a_2, a_3 = clf.coef_[1:]       # other coefficients
msg = 'model: y = {:.3f} + {:.3f}x + {:.3f}x^2 + {:.3f}x^3'\        .format(a_0, a_1, a_2, a_3)print(msg)</pre><div><img src="img/C13018_01_511.jpg" alt="Figure 1.51: Extracting coefficients and printing the polynomial model" width="1635" height="188"/></div><h6>Figure 1.51: Extracting coefficients and printing the polynomial model</h6><p>To get the actual model intercept, we have to add the <code>intercept_</code> and <code>coef_</code></p><p><code>[0]</code> attributes. The higher-order coefficients are then given by the remaining values of <code>coef_</code>.</p></li>
				<li>Determine the predicted values for each sample and calculate the residuals by running the following code:<pre>y_pred = clf.predict(x_poly) resid_MEDV = y - y_pred</pre></li>
				<li>Print some of the residual values by running <code>print(resid_MEDV[:10])</code>:<div><img src="img/C13018_01_521.jpg" alt="Figure 1.52: Printing residual values" width="1073" height="134"/></div><h6>Figure 1.52: Printing residual values</h6><p>We'll plot these soon to compare with the linear model residuals, but first we will calculate the MSE.</p></li>
				<li>Run the following code to print the MSE for the third-order polynomial model:<pre>from sklearn.metrics import mean_squared_error error = mean_squared_error(y, y_pred) print('mse = {:.2f}'.format(error))</pre><div><img src="img/C13018_01_531.jpg" alt="Figure 1.53: Calculating the mean squared error" width="1340" height="138"/></div><h6>Figure 1.53: Calculating the mean squared error</h6><p>As can be seen, the <strong class="bold">MSE</strong> is significantly less for the polynomial model compared to the linear model (which was 38.5). This error metric can be converted to an average error in dollars by taking the square root. Doing this for the polynomial model, we find the average error for the median house value is only $5,300.</p><p>Now, we'll visualize the model by plotting the polynomial line of best fit along with the data.</p></li>
				<li>Plot the polynomial model along with the samples by running the following:<pre>fig, ax = plt.subplots() # Plot the samples
ax.scatter(x.flatten(), y, alpha=0.6)
# Plot the polynomial model
x_ = np.linspace(2, 38, 50).reshape(-1, 1) x_poly = poly.fit_transform(x_)
y_ = clf.predict(x_poly)
ax.plot(x_, y_, color='red', alpha=0.8) ax.set_xlabel('LSTAT'); ax.set_ylabel('MEDV');</pre><div><img src="img/C13018_01_541.jpg" alt="Figure 1.54: Plotting the polynomial model for MEDV and LSTAT" width="1538" height="858"/></div><h6>Figure 1.54: Plotting the polynomial model for MEDV and LSTAT</h6><p>Here, we are plotting the red curve by calculating the polynomial model predictions on an array of <code>x</code> values. The array of <code>x</code> values was created using <code>np.linspace</code>, resulting in 50 values arranged evenly between 2 and 38.</p><p>Now, we'll plot the corresponding residuals. Whereas we used Seaborn for this earlier, we'll have to do it manually to show results for a scikit-learn model. Since we already calculated the residuals earlier, as reference by the <code>resid_MEDV</code> variable, we simply need to plot this list of values on a scatter chart.</p></li>
				<li>Plot the residuals by running the following:<pre>fig, ax = plt.subplots(figsize=(5, 7)) ax.scatter(x, resid_MEDV, alpha=0.6) ax.set_xlabel('LSTAT')
ax.set_ylabel('MEDV Residual $(y-\hat{y})$') plt.axhline(0, color='black', ls='dotted');</pre></li>
			</ol>
			<div><div><img src="img/C13018_01_551.jpg" alt="Figure 1.55: Plotting the residuals for LSTAT and MEDV" width="1493" height="1100"/>
				</div>
			</div>
			<h6>Figure 1.55: Plotting the residuals for LSTAT and MEDV</h6>
			<p>Compared to the linear model <strong class="bold">LSTAT</strong> residual plot, the polynomial model residuals appear to be more closely clustered around <em class="italics">y - </em>ŷ<em class="italics"> = 0.</em> Note that y is the sample <strong class="bold">MEDV</strong> and ŷ is the predicted value. There are still clear patterns, such as the cluster near <em class="italics">x = 7</em> and <em class="italics">y = -7</em> that indicates suboptimal modeling.</p>
			<p>Having successfully modeled the data using a polynomial model, let's finish up this chapter by looking at categorical features. In particular, we are going to build a set of categorical features and use them to explore the dataset in more detail.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor075"/>Chapter 2: Data Cleaning and Advanced Machine</h2>
			<h3 id="_idParaDest-73"><a id="_idTextAnchor076"/>Activity 2: Preparing to Train a Predictive Model for the Employee-Retention Problem</h3>
			<ol>
				<li value="1">Scroll to the <code>Activity A</code> section of the<code> lesson-2-workbook.ipynb</code> notebook file.</li>
				<li>Check the head of the table by running the following code:<pre>%%bash
head ../data/hr-analytics/hr_data.csv</pre><p>Judging by the output, convince yourself that it looks to be in standard CSV format. For CSV files, we should be able to simply load the data with pd.read_csv.</p></li>
				<li>Load the data with Pandas by running <code>df = pd.read_csv('../data/hr- analytics/hr_data.csv')</code>. Write it out yourself and use tab completion to help type the file path.</li>
				<li>Inspect the columns by printing <code>df.columns</code> and make sure the data has loaded as expected by printing the DataFrame <code>head</code> and <code>tail</code> with <code>df.head()</code> and <code>df.tail()</code>:<div><img src="img/C13018_02_46.jpg" alt="Figure 2.46: Output for inspecting head and tail of columns" width="1800" height="1013"/></div><h6>Figure 2.46: Output for inspecting head and tail of columns</h6><p>We can see that it appears to have loaded correctly. Based on the tail index values, there are nearly 15,000 rows; let's make sure we didn't miss any.</p></li>
				<li>Check the number of rows (including the header) in the CSV file with the following code:<pre>with open('../data/hr-analytics/hr_data.csv') as f: print(len(f.read().splitlines()))</pre><div><img src="img/C13018_02_47.jpg" alt="Figure 2.46: Output after checking for number of rows" width="1423" height="226"/></div><h6>Figure 2.47: Output after checking for number of rows</h6></li>
				<li>Compare this result to <code>len(df)</code> to make sure you've loaded all the data:<div><img src="img/C13018_02_48.jpg" alt="Figure 2.48: Output after checking for number of sample uploaded" width="1786" height="198"/></div><h6>Figure 2.48: Output after checking for number of sample uploaded</h6><p>Now that our client's data has been properly loaded, let's think about how we can use predictive analytics to find insights into why their employees are leaving.</p><p>Let's run through the first steps for creating a predictive analytics plan:</p><p>Look at the available data: You've already done this by looking at the columns, datatypes, and the number of samples.</p><p>Determine the business needs: The client has clearly expressed their needs: reduce the number of employees who leave.</p><p>Assess the data for suitability: Let's try to determine a plan that can help satisfy the client's needs, given the provided data</p><p>Recall, as mentioned earlier, that effective analytics techniques lead to impactful business decisions. With that in mind, if we were able to predict how likely an employee is to quit, the business could selectively target those employees for special treatment. For example, their salary could be raised or their number of projects reduced. Furthermore, the impact of these changes could be estimated using the model!</p><p>To assess the validity of this plan, let's think about our data. Each row represents an employee who either works for the company or has left, as labeled by the column named left. We can therefore train a model to predict this target, given a set of features.</p></li>
				<li>Assess the target variable. Check the distribution and number of missing entries by running the following code:<pre>df.left.value_counts().plot('barh') print(df.left.isnull().sum()</pre><div><img src="img/C13018_02_49.jpg" alt="Figure 2.49: Distribution of the target variables" width="1414" height="582"/></div><h6>Figure 2.49: Distribution of the target variables</h6><p>Here's the output of the second code line:</p><div><img src="img/C13018_02_50.jpg" alt="Figure 2.50: Output to check missing data points" width="577" height="79"/></div><h6>Figure 2.50: Output to check missing data points</h6><p>About three-quarters of the samples are employees who have not left. The group that has left make up the other quarter of the samples. This tells us we are dealing with an imbalanced classification problem, which means we'll have to take special measures to account for each class when calculating accuracies. We also see that none of the target variables are missing (no <code>NaN</code> values).</p><p>Now, we'll assess the features:</p></li>
				<li>Print the datatype of each by executing <code>df.dtypes</code>. Observe how we have a mix of continuous and discrete features:<div><img src="img/C13018_02_51.jpg" alt="Figure 2.51: Printing data types for verification" width="1244" height="586"/></div><h6>Figure 2.51: Printing data types for verification</h6></li>
				<li>Display the feature distributions by running the following code:<pre>for f in df.columns: try:
fig = plt.figure()
…
…
print('-'*30)</pre><h4>Note</h4><pre></div><h6>Figure 2.52: Distribution of all features: satisfaction_level and last_evaluation</h6><div><img src="img/C13018_02_53.jpg" alt="Figure 2.52: Distribution of all remaining features" width="1587" height="1244"/></div><h6>Figure 2.53: Distribution of all remaining features</h6><div><img src="img/C13018_02_54.jpg" alt="Figure 2.54: Distribution for the variable promotion_last_5years" width="1330" height="630"/></div><h6>Figu<a id="_idTextAnchor077"/>re 2.54: Distribution for the variable promotion_last_5years</h6><p>For many features, we see a wide distribution over the possible values, indicating a good variety in the feature spaces. This is encouraging; features that are strongly grouped around a small range of values may not be very informative for the model. This is the case for <code>promotion_last_5years</code>, where we see that the vast majority of samples are <code>0</code>.</pre><p>The next thing we need to do is remove any <code>NaN</code> values from the dataset.</p></li>
				<li>Check how many <code>NaN</code> values are in each column by running the following code:<pre>df.isnull().sum() / len(df) * 100</pre><div><img src="img/C13018_02_55.jpg" alt="Figure 2.55: Verification for the number of NaN values" width="1483" height="624"/></div><h6>Figure 2.55: Verification for the number of NaN values</h6><p>We can see there are about 2.5% missing for <code>average_montly_hours</code>, 1% missing for <code>time_spend_company</code>, and 98% missing for <code>is_smoker!</code> Let's use a couple of different strategies that you've learned to handle these.</p></li>
				<li>Drop the <code>is_smoker</code> column as there is barely any information in this metric. Do this by running: <code>del df['is_smoker']</code>.</li>
				<li>Fill the <code>NaN</code> values in the <code>time_spend_company</code> column. This can be done with the following code:<pre>fill_value = df.time_spend_company.median()
df.time_spend_company = df.time_spend_company.fillna(fill_ value)</pre><p>The final column to deal with is <code>average_montly_hours</code>. We could do something similar and use the median or rounded mean as the integer fill value. Instead though, let's try to take advantage of its relationship with another variable. This may allow us to fill the missing data more accurately.</p></li>
				<li>Make a boxplot of <code>average_montly_hours</code> segmented by <code>number_project</code>. This can be done by running the following code:<pre>sns.boxplot(x='number_project', y='average_montly_hours', data=df)</pre><div><img src="img/C13018_02_56.jpg" alt="Figure 2.56: Boxplot for average_monthly_hours and number_project" width="1800" height="844"/></div><h6>Figure 2.56: Boxplot for average_monthly_hours and number_project</h6><p>We can see how the number of projects is correlated with <code>average_ monthly_hours</code>, a result that is hardly surprising. We'll exploit this relationship by filling in the <code>NaN</code> values of <code>average_montly_hours</code> differently, depending on the number of projects for that sample.</p><p>Specifically, we'll use the mean of each group.</p></li>
				<li> Calculate the mean of each group by running the following code:<pre>mean_per_project = df.groupby('number_project')\
.average_montly_hours.mean() mean_per_project = dict(mean_per_project) print(mean_per_project)</pre><div><img src="img/C13018_02_57.jpg" alt="Figure 2.57: Calculation of mean values for average_monthly_hours" width="1112" height="514"/></div><h6>Figure 2.57: Calculation of mean values for average_monthly_hours</h6><p>We can then map this onto the <code>number_project</code> column and pass the resulting series object as the argument to <code>fillna</code>.</p></li>
				<li>Fill the <code>NaN</code> values in <code>average_montly_hours</code> by executing the following code:<pre>fill_values = df.number_project.map(mean_per_project)
df.average_montly_hours = df.average_montly_hours. fillna(fill_values)</pre></li>
				<li>Confirm that <code>df</code> has no more <code>NaN</code> values by running the following assertion test. If it does not raise an error, then you have successfully removed the <code>NaNs</code> from the table:<pre>assert df.isnull().sum().sum() == 0</pre><h4>Note</h4><p class="callout">We pass <code>index=False</code> so that the index is not written to file. In this case, the index is a set of integers spanning from 0 to the DataFrame length, and it therefore tells us nothing important.</p></li>
				<li>Transform the string and Boolean fields into integer representations. In particular, we'll manually convert the target variable <code>left</code> from <code>yes</code> and <code>no</code> to <code>1</code> and <code>0</code> and build the one-hot encoded features. Do this by running the following code:<pre>df.left = df.left.map({'no': 0, 'yes': 1}) df = pd.get_dummies(df)</pre></li>
				<li>Print <code>df.columns</code> to show the fields:<div><img src="img/C13018_02_58.jpg" alt="Figure 2.58: A screenshot of the different fields in the dataframe" width="1646" height="406"/></div><h6>Figure 2.58: A screenshot of the different fields in the dataframe</h6><p>We can see that department and salary have been split into various binary features.</p><p>The final step to prepare our data for machine learning is scaling the features, but for various reasons (for example, some models do not require scaling), we'll do it as part of the model-training workflow in the next activity.</p></li>
				<li>We have completed the data preprocessing and are ready to move on to training models! Let's save our preprocessed data by running the following code:<pre>df.to_csv('../data/hr-analytics/hr_data_processed.csv', index=False)</pre></li>
			</ol>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor078"/>Chapter 3: Web Scraping and Interactive Visualizations</h2>
			<h3 id="_idParaDest-75"><a id="_idTextAnchor079"/>Activity 3: Web Scraping with Jupyter Notebooks</h3>
			<ol>
				<li value="1">For this page, the data can be scraped using the following code snippet:<pre>data = []
for i, row in enumerate(soup.find_all('tr')): row_data = row.find_all('td')
try:
d1, d2, d3 = row_data[1], row_data[5], row_data[6] d1 = d1.find('a').text
d2 = float(d2.text)
d3 = d3.find_all('span')[1].text.replace('+', '') data.append([d1, d2, d3])
except:
print('Ignoring row {}'.format(i)</pre></li>
				<li>In the <code>lesson-3-workbook.ipynb</code> Jupyter Notebook, scroll to <code>Activity A: Web scraping with Python</code>.</li>
				<li>Set the <code>url</code> variable and load an IFrame of our page in the notebook by running the following code:<pre><a href="http://www.worldometers.info/world-population/">url = 'http://www.worldometers.info/world-population/</a> population-by-country/'
IFrame(url, height=300, width=800)</pre><p>The page should load in the notebook. Scrolling down, we can see the <strong class="bold">Countries in the world by population</strong> heading and the table of values beneath it. We'll scrape the first three columns from this table to get the countries, populations, and yearly population changes.</p></li>
				<li>Close the IFrame by selecting the cell and clicking <strong class="bold">Current Outputs</strong> | <strong class="bold">Clear</strong> from the <strong class="bold">Cell</strong> menu in the Jupyter Notebook.</li>
				<li>Request the page and load it as a <code>BeautifulSoup</code> object by running the following code:<pre>page = requests.get(url)
soup = BeautifulSoup(page.content, 'html.parser')</pre><p>We feed the page content to the <code>BeautifulSoup</code> constructor. Recall that previously, we used <code>page.text</code> here instead. The difference is that <code>page.content</code> returns the raw binary response content, whereas <code>page.text</code> returns the UTF-8 decoded content. It's usually best practice to pass the bytes object and let <code>BeautifulSoup</code> decode it, rather than doing it with Requests using <code>page.text</code>.</p></li>
				<li>Print the H1 for the page by running the following code:<pre>soup.find_all('h1')
&gt;&gt; [&lt;h1&gt;Countries in the world by population (2017)&lt;/h1&gt;]</pre><p>We'll scrape the table by searching for <code>&lt;th&gt;</code>, <code>&lt;tr&gt;</code>, and <code>&lt;td&gt;</code> elements, as in the previous exercise.</p></li>
				<li>Get and print the table headings by running the following code:<pre>table_headers = soup.find_all('th') table_headers
&gt;&gt; [&lt;th&gt;#&lt;/th&gt;,
&lt;th&gt;Country (or dependency)&lt;/th&gt;,
&lt;th&gt;Population&lt;br/&gt; (2017)&lt;/th&gt;,
&lt;th&gt;Yearly&lt;br/&gt; Change&lt;/th&gt;,
&lt;th&gt;Net&lt;br/&gt; Change&lt;/th&gt;,
&lt;th&gt;Density&lt;br/&gt; (P/Km²)&lt;/th&gt;,
&lt;th&gt;Land Area&lt;br/&gt; (Km²)&lt;/th&gt;,
&lt;th&gt;Migrants&lt;br/&gt; (net)&lt;/th&gt;,
&lt;th&gt;Fert.&lt;br/&gt; Rate&lt;/th&gt;,
&lt;th&gt;Med.&lt;br/&gt; Age&lt;/th&gt;,
&lt;th&gt;Urban&lt;br/&gt; Pop %&lt;/th&gt;,
&lt;th&gt;World&lt;br/&gt; Share&lt;/th&gt;]</pre></li>
				<li>We are only interested in the first three columns. Select these and parse the text with the following code: <pre>table_headers = table_headers[1:4] table_headers = [t.text.replace('\n', '') for t in table_ headers]</pre><p>After selecting the subset of table headers we want, we parse the text content from each and remove any newline characters.</p><p>Now, we'll get the data. Following the same prescription as the previous exercise, we'll test how to parse the data for a sample row.</p></li>
				<li>Get the data for a sample row by running the following code:<pre>row_number = 2
row_data = soup.find_all('tr')[row_number]\
.find_all('td') </pre></li>
				<li>How many columns of data do we have? Print the length of <code>row_data</code> by running <code>print(len(row_data))</code>.</li>
				<li>Print the first elements by running <code>print(row_data[:4])</code>:<pre>&gt;&gt; [&lt;td&gt;2&lt;/td&gt;,
&lt;td style="font-weight: bold; font-size:15px; text-align:left"&gt;&lt;a href="/world-population/india- population/"&gt;India&lt;/a&gt;&lt;/td&gt;,
&lt;td style="font-weight: bold;"&gt;1,339,180,127&lt;/td&gt;,
&lt;td&gt;1.13 %&lt;/td&gt;]</pre><p>It's pretty obvious that we want to select list indices 1, 2, and 3. The first data value can be ignored, as it's simply the index.</p></li>
				<li>Select the data elements we're interested in parsing by running the following code:<pre>d1, d2, d3 = row_data[1:4]</pre></li>
				<li>Looking at the <code>row_data</code> output, we can find out how to correctly parse the data. We'll want to select the content of the <code>&lt;a&gt;</code> element in the first data element, and then simply get the text from the others. Test these assumptions by running the following code:<pre>print(d1.find('a').text) print(d2.text) print(d3.text)
&gt;&gt; India
&gt;&gt; 1,339,180,127
&gt;&gt; 1.13 %</pre><p>Excellent! This looks to be working well. Now, we're ready to scrape the entire table.</p></li>
				<li>Scrape and parse the table data by running the following code:<pre>data = []
for i, row in enumerate(soup.find_all('tr')): try:
d1, d2, d3 = row.find_all('td')[1:4] d1 = d1.find('a').text
d2 = d2.text d3 = d3.text
data.append([d1, d2, d3]) except:
print('Error parsing row {}'.format(i))
&gt;&gt; Error parsing row 0</pre><p>This is quite similar to before, where we try to parse the text and skip the row if there's some error.</p></li>
				<li>Print the head of the scraped data by running <code>print(data[:10])</code>:<pre>&gt;&gt; [['China', '1,409,517,397', '0.43 %'],
['India', '1,339,180,127', '1.13 %'],
['U.S.', '324,459,463', '0.71 %'],
['Indonesia', '263,991,379', '1.10 %'],
['Brazil', '209,288,278', '0.79 %'],
['Pakistan', '197,015,955', '1.97 %'],
['Nigeria', '190,886,311', '2.63 %'],
['Bangladesh', '164,669,751', '1.05 %'],
['Russia', '143,989,754', '0.02 %'],
['Mexico', '129,163,276', '1.27 %']]</pre><p>It looks like we have managed to scrape the data! Notice how similar the process was for this table compared to the Wikipedia one, even though this web page is completely different. Of course, it will not always be the case that data is contained within a table, but regardless, we can usually use <code>find_all</code> as the primary method for parsing.</p></li>
				<li>Finally, save the data to a CSV file for later use. Do this by running the following code:<pre>f_path = '../data/countries/populations.csv' with open(f_path, 'w') as f:
f.write('{};{};{}\n'.format(*table_headers)) for d in data:
f.write('{};{};{}\n'.format(*d))</pre></li>
			</ol>
			<h3 id="_idParaDest-76"><a id="_idTextAnchor080"/>Activity 4: Exploring Data with Interactive Visualizations</h3>
			<ol>
				<li value="1">In the <code>lesson-3-workbook.ipynb</code> file, scroll to the <code>Activity B: Interactive visualizations with Bokeh</code> section.</li>
				<li>Load the previously scraped, merged, and cleaned web page data by running the following code:<pre>df = pd.read_csv('../data/countries/merged.csv')
df['Date of last change'] = pd.to_datetime(df['Date of last change'])</pre></li>
				<li>Recall what the data looks like by displaying the DataFrame:<div><img src="img/C13018_03_18.jpg" alt="Figure 3.18: Output of the data within DataFrame" width="849" height="435"/></div><h6>Figure 3.18: Output of the data within DataFrame</h6><p>Whereas in the previous exercise we were interested in learning how Bokeh worked, now we are interested in what this data looks like. In order to explore this dataset, we are going to use interactive visualizations.</p></li>
				<li>Draw a scatter plot of the population as a function of the interest rate by running the following code:<pre>source = ColumnDataSource(data=dict( x=df['Interest rate'], y=df['Population'], desc=df['Country'],
))
hover = HoverTool(tooltips=[ ('Country', '@desc'),
('Interest Rate (%)', '@x'), ('Population', '@y')
])
tools = [hover, PanTool(), BoxZoomTool(), WheelZoomTool(), ResetTool()]
p = figure(tools=tools,
x_axis_label='Interest Rate (%)', y_axis_label='Population')
p.circle('x', 'y', size=10, alpha=0.5, source=source) show(p)</pre><div><img src="img/C13018_03_19.jpg" alt="Figure 3.19: Scatter plot of population and interest rate&#13;&#10;" width="1800" height="892"/></div><h6>Figure 3.19: Scatter plot of population and interest rate</h6><p>This is quite similar to the final examples we looked at when introducing Bokeh in the previous exercise. We set up a customized data source with the <em class="italics">x</em> and <em class="italics">y </em>coordinates for each point, along with the country name. This country name is passed to the Hover Tool, so that it's visible when hovering the mouse over the dot. We pass this tool to the figure, along with a set of other useful tools.</p></li>
				<li>In the data, we see some clear outliers with high populations. Hover over these to see what they are:<div><img src="img/C13018_03_20.jpg" alt="Figure 3.20: Labels obtained by hovering over data points&#13;&#10;" width="502" height="176"/></div><h6>Figure 3.20: Labels obtained by hovering over data points</h6><p>We see they belong to India and China. These countries have fairly average interest rates. Let's focus on the rest of the points by using the Box Zoom tool to modify the view window size.</p></li>
				<li>Select the Box Zoom tool and alter the viewing window to better see the majority of the data:<div><img src="img/C13018_03_21.jpg" alt="Figure 3.21: The Box Zoom tool&#13;&#10;" width="1800" height="386"/></div><h6>Figure 3.21: The Box Zoom tool</h6><div><img src="img/C13018_03_22.jpg" alt="Figure 3.22: Scatter plot with majority of the data points within the box&#13;&#10;" width="1800" height="833"/></div><h6>Figure 3.22: Scatter plot with majority of the data points within the box</h6><p>Explore the points and see how the interest rates compare for various countries. What are the countries with the highest interest rates?:</p><div><img src="img/C13018_03_23.jpg" alt="Figure 3.23: Hovering over data points to view detailed data&#13;&#10;" width="657" height="288"/></div><h6>Figure 3.23: Hovering over data points to view detailed data</h6></li>
				<li>Some of the lower population countries appear to have negative interest rates. Select the <strong class="bold">Wheel Zoom </strong>tool and use it to zoom in on this region. Use the <strong class="bold">Pan </strong>tool to re-center the plot, if needed, so that the negative interest rate samples are in view. Hover over some of these and see what countries they correspond to:<div><img src="img/C13018_03_24.jpg" alt="Figure 3.24: Screen shot of the Wheel Zoom tool&#13;&#10;" width="1176" height="240"/></div><h6>Figure 3.24: Screen shot of the Wheel Zoom tool</h6><div><img src="img/C13018_03_25.jpg" alt="Figure 3.25: Data points of negative interest rates countries&#13;&#10;" width="1418" height="614"/></div><h6>Figure 3.25: Data points of negative interest rates countries</h6><p>Let's re-plot this, adding a color based on the date of last interest rate change. This will be useful to search for relations between the date of last change and the interest rate or population size.</p></li>
				<li>Add a <strong class="bold">Year of last change</strong> column to the DataFrame by running the following code:<pre>def get_year(x):
year = x.strftime('%Y')
if year in ['2018', '2017', '2016']:
return year else: return 'Other'
df['Year of last change'] = df['Date of last change']. apply(get_year)</pre></li>
				<li>Create a map to group the last change date into color categories by running the following code:<pre>year_to_color = { '2018': 'black',
'2017': 'blue',
'2016': 'orange',
'Other':'red'
}</pre><p>Once mapped to the <strong class="bold">Year of last change</strong> column, this will assign values to colors based on the available categories: 2018, 2017, 2016, and Other. The colors here are standard strings, but they could alternatively by represented by hexadecimal codes.</p></li>
				<li>Create the colored visualization by running the following code:<pre>source = ColumnDataSource(data=dict( x=df['Interest rate'],
...
...
fill_color='colors', line_color='black', legend='label')
show(p)</pre><h4>Note</h4><p class="callout">For the complete code, refer to the following: <a href="https://bit.ly/2Si3K04">https://bit.ly/2Si3K04</a></p><div><img src="img/C13018_03_26.jpg" alt="Figure 3.26: Visualization obtained after assigning values to colors&#13;&#10;" width="1800" height="1013"/></div><h6>Figure 3.26: Visualization obtained after assigning values to colors</h6><p>There are some technical details that are important here. First of all, we add the colors and labels for each point to the <code>ColumnDataSource</code>. These are then referenced when plotting the circles by setting the <code>fill_color</code> and legend arguments.</p></li>
				<li>Looking for patterns, zoom in on the lower population countries:<div><img src="img/C13018_03_27.jpg" alt="Figure 3.27: A zoomed in view of the lower population countries&#13;&#10;" width="1296" height="591"/></div><h6>Figure 3.27: A zoomed in view of the lower population countries</h6><p>We can see how the dark dots are more prevalent to the right-hand side of the plot. This indicates that countries that have higher interest rates are more likely to have been recently updated.</p><p>The one data column we have not yet looked at is the year-over-year change in population. Let's visualize this compared to the interest rate and see if there is any trend. We'll also enhance the plot by setting the circle size based on the country population.</p></li>
				<li>Plot the interest rate as a function of the year-over-year population change by running the following code:<pre>source = ColumnDataSource(data=dict( x=df['Yearly Change'],
...
...
p.circle('x', 'y', size=10, alpha=0.5, source=source, radius='radii')
show(p)</pre><div><img src="img/C13018_03_28.jpg" alt="Figure 3.28: Plotting interest rate as a function of YoY population change&#13;&#10;" width="1800" height="947"/></div><h6>Figure 3.28: Plotting interest rate as a function of YoY population change</h6><p>Here, we use the square root of the population for the radii, making sure to also scale down the result to a good size for the visualization.</p><p>We see a strong correlation between the year-over-year population change and the interest rate. This correlation is especially strong when we take the population sizes into account, by looking primarily at the bigger circles. Let's add a line of best fit to the plot to illustrate this correlation.</p><p>We'll use scikit-learn to create the line of best fit, using the country populations (as visualized in the preceding plot) as weights.</p></li>
				<li>Determine the line of best fit for the previously plotted relationship by running the following code:<pre>from sklearn.linear_model import LinearRegression X = df['Yearly Change'].values.reshape(-1, 1)
y = df['Interest rate'].values
weights = np.sqrt(df['Population'])/1e5
lm = LinearRegression()
lm.fit(X, y, sample_weight=weights)
lm_x = np.linspace(X.flatten().min(), X.flatten().max(), 50)
lm_y = lm.predict(lm_x.reshape(-1, 1))</pre><p>The scikit-learn code should be familiar from earlier in this book. As promised, we are using the transformed populations, as seen in the previous plot, as the weights. The line of best fit is then calculated by predicting the linear model values for a range of x values.</p><p>To plot the line, we can reuse the preceding code, adding an extra call to the <code>line</code> module in Bokeh. We'll also have to set a new data source for this line.</p></li>
				<li>Re-plot the preceding figure, adding a line of best fit, by running the following code:<pre>source = ColumnDataSource(data=dict( x=df['Yearly Change'], y=df['Interest rate'],
...
...
p.line('x', 'y', line_width=2, line_color='red', source=lm_source)
show(p)</pre><div><img src="img/C13018_03_29.jpg" alt="Figure 3.29: Adding a best fit line to the plot of YoY population change and interest rates&#13;&#10;" width="1800" height="1034"/></div><h6>Figure 3.29: Adding a best fit line to the plot of YoY population change and interest rates</h6><p>For the line source, <code>lm_source</code>, we include <code>N/A</code> as the country name and population, as these are not applicable values for the line of best fit. As can be seen by hovering over the line, they indeed appear in the tooltip.</p><p>The interactive nature of this visualization gives us a unique opportunity to explore outliers in this dataset, for example, the tiny dot in the lower-right corner.</p></li>
				<li>Explore the plot by using the zoom tools and hovering over interesting samples. Note the following:<p>Ukraine has an unusually high interest rate, given the low year-over-year population change:</p></li>
			</ol>
			<div><div><img src="img/C13018_03_30.jpg" alt="Figure 3.30: Using the Zoom tool to explore the data for Ukraine&#13;&#10;" width="1800" height="862"/>
				</div>
			</div>
			<h6>Figure 3.30: Using the Zoom tool to explore the data for Ukraine</h6>
			<p>The small country of Bahrain has an unusually low interest rate, given the high year-over-year population change:</p>
			<div><div><img src="img/C13018_03_31.jpg" alt="Figure 3.31: Using the Zoom tool to explore the data for Bahrain" width="1800" height="841"/>
				</div>
			</div>
			<h6>Figure 3.31: Using the Zoom tool to explore the data for Bahrain</h6>
		</div>
		<div><div></div>
		</div>
	</div>
<div><nav id="toc" epub:type="toc">
		<h2>Contents</h2>
			<ol>
				<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-1">Preface</a>
					<ol>
						<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-2">About the Book</a>
							<ol>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-3">About the Author</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-4">Objectives</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-5">Audience</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-6">Approach</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-7">Minimum Hardware Requirements</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-8">Software Requirements</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-9">Installation and Setup</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-10">Installing Anaconda</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-11">Updating Jupyter and Installing Dependencies</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-12">Additional Resources</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-13">Conventions</a></li>
							</ol>
						</li>
					</ol>
				</li>
				<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-14">Jupyter Fundamentals</a>
					<ol>
						<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-15">Introduction</a></li>
						<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-16">Basic Functionality and Features</a>
							<ol>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-17">What is a Jupyter Notebook and Why is it Useful?</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-18">Navigating the Platform</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-19">Exercise 1: Introducing Jupyter Notebooks</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-20">Jupyter Features</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-21">Exercise 2: Implementing Jupyter's Most Useful Features</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-22">Converting a Jupyter Notebook to a Python Script</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-23">Python Libraries</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-24">Exercise 3: Importing the External Libraries and Setting Up the Plotting Environment</a></li>
							</ol>
						</li>
						<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-25">Our First Analysis - The Boston Housing Dataset</a>
							<ol>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-26">Loading the Data into Jupyter Using a Pandas DataFrame</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-27">Exercise 4: Loading the Boston Housing Dataset</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-28">Data Exploration</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-29">Exercise 5: Analyzing the Boston Housing Dataset</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-30">Introduction to Predictive Analytics with Jupyter Notebooks</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-31">Exercise 6: Applying Linear Models With Seaborn and Scikit-learn</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-32">Activity 1: Building a Third-Order Polynomial Model</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-33">Using Categorical Features for Segmentation Analysis</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-34">Exercise 7: Creating Categorical Fields From Continuous Variables and Make Segmented Visualizations</a></li>
							</ol>
						</li>
						<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-35">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-36">Data Cleaning and Advanced Machine Learning</a>
					<ol>
						<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-37">Introduction</a></li>
						<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-38">Preparing to Train a Predictive Model</a>
							<ol>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-39">Determining a Plan for Predictive Analytics</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-40">Exercise 8: Explore Data Preprocessing Tools and Methods</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-41">Activity 2: Preparing to Train a Predictive Model for the Employee-Retention Problem</a></li>
							</ol>
						</li>
						<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-42">Training Classification Models</a>
							<ol>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-43">Introduction to Classification Algorithms</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-44">Exercise 9: Training Two-Feature Classification Models With Scikit-learn</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-45">The plot_decision_regions Function</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-46">Exercise 10: Training K-nearest Neighbors for Our Model</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-47">Exercise 11: Training a Random Forest</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-48">Assessing Models With K-fold Cross-Validation and Validation Curves</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-49">Exercise 12: Using K-fold Cross Validation and Validation Curves in Python With Scikit-learn</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-50">Dimensionality Reduction Techniques</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-51">Exercise 13: Training a Predictive Model for the Employee Retention Problem</a></li>
							</ol>
						</li>
						<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-52">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-53">Web Scraping and Interactive Visualizations</a>
					<ol>
						<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-54">Introduction</a></li>
						<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-55">Scraping Web Page Data</a>
							<ol>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-56">Introduction to HTTP Requests</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-57">Making HTTP Requests in the Jupyter Notebook</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-58">Exercise 14: Handling HTTP Requests With Python in a Jupyter Notebook</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-59">Parsing HTML in the Jupyter Notebook</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-60">Exercise 15: Parsing HTML With Python in a Jupyter Notebook</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-61">Activity 3: Web Scraping With Jupyter Notebooks</a></li>
							</ol>
						</li>
						<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-62">Interactive Visualizations</a>
							<ol>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-63">Building a DataFrame to Store and Organize Data</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-64">Exercise 16: Building and Merging Pandas DataFrames</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-65">Introduction to Bokeh</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-66">Exercise 17: Introduction to Interactive Visualization With Bokeh</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-67">Activity 4: Exploring Data with Interactive Visualizations</a></li>
							</ol>
						</li>
						<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-68">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-69">Appendix A</a>
					<ol>
						<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-70">Chapter 1: Jupyter Fundamentals</a>
							<ol>
								<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-71">Activity 1: Building a Third-Order Polynomial Model</a></li>
							</ol>
						</li>
						<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-72">Chapter 2: Data Cleaning and Advanced Machine</a>
							<ol>
								<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-73">Activity 2: Preparing to Train a Predictive Model for the Employee-Retention Problem</a></li>
							</ol>
						</li>
						<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-74">Chapter 3: Web Scraping and Interactive Visualizations</a>
							<ol>
								<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-75">Activity 3: Web Scraping with Jupyter Notebooks</a></li>
								<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-76">Activity 4: Exploring Data with Interactive Visualizations</a></li>
							</ol>
						</li>
					</ol>
				</li>
			</ol>
		</nav>
		<nav epub:type="landmarks">
		<h2>Landmarks</h2>
			<ol>
				<li><a epub:type="cover" href="Images/cover.xhtml">Cover</a></li>
				<li><a epub:type="toc" href="C13018_Credits_comm_Final.xhtml#_idContainer004">Table of Contents</a></li>
			</ol>
		</nav>
	</div></body></html>