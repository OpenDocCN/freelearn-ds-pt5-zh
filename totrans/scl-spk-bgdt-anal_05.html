<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Tackle Big Data – Spark Comes to the Party</h1>
                
            
            
                
<p>An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem.</p>
<p>- John Tukey</p>
<p class="mce-root">In this chapter, you learn about data analysis and big data; we see the challenges that big data provides and how they are dealt with. You will learn about distributed computing and the approach suggested by functional programming; we introduce Google's MapReduce, Apache Hadoop, and finally Apache Spark and see how they embrace this approach and these techniques.</p>
<p class="mce-root">In a nutshell, the following topics will be covered throughout this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Introduction to data analytics</li>
<li class="mce-root1">Introduction to big data</li>
<li class="mce-root1">Distributed computing using Apache Hadoop</li>
<li class="mce-root1">Here comes Apache Spark</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Introduction to data analytics</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre1">Data analytics</strong> is the process of applying qualitative and quantitative techniques when examining data with the goal of providing valuable insights. Using various techniques and concepts, data analytics can provide the means to explore the data <strong class="calibre1">Exploratory Data Analysis</strong> (<strong class="calibre1">EDA</strong>) as well as draw conclusions about the data <strong class="calibre1">Confirmatory Data Analysis</strong> (<strong class="calibre1">CDA</strong>). EDA and CDA are fundamental concepts of data analytics, and it is important to understand the difference between the two.</p>
<p class="mce-root">EDA involves methodologies, tools, and techniques used to explore data with the intention of finding patterns in the data and relationships between various elements of the data. CDA involves methodologies, tools, and techniques used to provide an insight or conclusion on a specific question based on a hypothesis and statistical techniques or simple observation of the data.</p>
<p class="mce-root">A quick example to understand these ideas is that of a grocery store, which has asked you to give them ways to improve sales and customer satisfaction as well as keep the cost of operations low.</p>
<p class="mce-root">The following is a grocery store with aisles of various products:</p>
<div><img class="image-border32" src="img/00107.jpeg"/></div>
<p class="mce-root">Assume that all sales at the grocery store are stored in some database and that you have access to the data for the last 3 months. Typically, businesses store data for years as you need sufficient data over a period of time to establish any hypothesis or observe any patterns. In this example, our goal is to perform better placement of products in various aisles based on how customers are buying the products. One hypothesis is that customers often buy products, that are both at eye level and also close together. For instance, if Milk is on one corner of the store and Yogurt is in other corner of the store, some customers might just choose either Milk or Yogurt and just leave the store, causing a loss of business. More adverse affects might result in customers choosing another store where products are better placed because if the feeling that <em class="calibre8">things are hard to find at this store</em>. Once that feeling sets in, it also percolates to friends and family eventually causing a bad social presence. This phenomenon is not uncommon in the real world causing some businesses to succeed while others fail while both seem to be very similar in products and prices.</p>
<p class="mce-root">There are many ways to approach this problem starting from customer surveys to professional statisticians to machine learning scientists. Our approach will be to understand what we can from just the sales transactions alone.</p>
<p class="mce-root">The following is an example of what the transactions might look like:</p>
<div><img class="image-border33" src="img/00111.jpeg"/></div>
<p class="mce-root">The following are the steps you could follow as part of EDA:</p>
<ol class="calibre14">
<li value="1" class="mce-root1">Calculate <em class="calibre8">Average number of products bought per day = Total of all products sold in a day / Total number of receipts for the</em> <em class="calibre8">day</em>.</li>
<li value="2" class="mce-root1">Repeat the preceding step for last 1 week, month, and quarter.</li>
</ol>
<p class="mce-root"> </p>
<ol start="3" class="calibre14">
<li value="3" class="mce-root1">Try to understand if there is a difference between weekends and weekdays and also time of the day (morning, noon, and evening)</li>
<li value="4" class="mce-root1">For each product, create a list of all other products to see which products are usually bought together (same receipt)</li>
<li value="5" class="mce-root1">Repeat the preceding step for 1 day, 1 week, month, and quarter.</li>
<li value="6" class="mce-root1">Try to determine which products should be placed closer together by the number of transactions (sorted in descending order).</li>
</ol>
<p class="mce-root">Once we have completed the preceding 6 steps, we can try to reach some conclusions for CDA.</p>
<p class="mce-root">Let's assume this is the output we get:</p>
<table class="calibre28">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Item</strong></td>
<td class="calibre7"><strong class="calibre1">Day Of Week</strong></td>
<td class="calibre7"><strong class="calibre1">Quantity</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">Milk</td>
<td class="calibre7">Sunday</td>
<td class="calibre7">1244</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Bread</td>
<td class="calibre7">Monday</td>
<td class="calibre7">245</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Milk</td>
<td class="calibre7">Monday</td>
<td class="calibre7">190</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">In this case, we could state that <strong class="calibre1">Milk</strong> is bought more on <em class="calibre8">weekends</em> so its better to increase the quantity and variety of Milk products over weekends. Take a look at the following table:</p>
<table class="calibre29">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Item1</strong></td>
<td class="calibre7"><strong class="calibre1">Item2</strong></td>
<td class="calibre7"><strong class="calibre1">Quantity</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">Milk</td>
<td class="calibre7">Eggs</td>
<td class="calibre7">360</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Bread</td>
<td class="calibre7">Cheese</td>
<td class="calibre7">335</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Onions</td>
<td class="calibre7">Tomatoes</td>
<td class="calibre7">310</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">In this case, we could state that <strong class="calibre1">Milk</strong> and <strong class="calibre1">Eggs</strong> are bought by <em class="calibre8">more</em> customers in one purchase followed by <strong class="calibre1">Bread</strong> and <strong class="calibre1">Cheese.</strong> So, we could recommend that the store realigns the aisles and shelves to move <strong class="calibre1">Milk</strong> and <strong class="calibre1">Eggs</strong> <em class="calibre8">closer</em> to each other.</p>
<p class="mce-root">The two conclusions we have are:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Milk</strong> is bought more on <em class="calibre8">weekends,</em> so it's better to increase the quantity and variety of Milk products over weekends.</li>
<li class="mce-root1"><strong class="calibre1">Milk</strong> and <strong class="calibre1">Eggs</strong> are bought by <em class="calibre8">more</em> customers in one purchase followed by <strong class="calibre1">Bread</strong> and <strong class="calibre1">Cheese.</strong> So, we could recommend that the store realigns the aisles and shelves to move <strong class="calibre1">Milk</strong> and <strong class="calibre1">Eggs</strong> <em class="calibre8">closer</em> to each other.</li>
</ul>
<p>Conclusions are usually tracked over a period of time to evaluate the gains. If there is no significant impact on sales even after adopting the preceding two recommendations for 6 months, we simply invested in the recommendations which are not able to give you a good Return On Investment (ROI).</p>
<p class="mce-root">Similarly, you can also perform some analysis with respect to the Profit margin and pricing optimizations. This is why you will typically see a single item costing more than the average of multiple numbers of the same item bought. Buy one Shampoo for $7 or two bottles of Shampoo for $12.</p>
<p>Think about other aspects you can explore and recommend for the grocery store. For example, can you guess which products to position near checkout registers just based on fact that these have no affinity toward any particular product--chewing gum, magazines, and so on.</p>
<p class="mce-root">Data analytics initiatives support a wide variety of business uses. For example, banks and credit card companies analyze withdrawal and spending patterns to prevent fraud and identity theft. Advertising companies analyze website traffic to identify prospects with a high likelihood of conversion to a customer. Department stores analyze customer data to figure out if better discounts will help boost sales. Cell Phone operators can figure out pricing strategies. Cable companies are constantly looking for customers who are likely to churn unless given some offer or promotional rate to retain their customer. Hospitals and pharmaceutical companies analyze data to come up with better products and detect problems with prescription drugs or measure the performance of prescription drugs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Inside the data analytics process</h1>
                
            
            
                
<p class="mce-root">Data analytics applications involve more than just analyzing data. Before any analytics can be planned, there is also a need to invest time and effort in collecting, integrating, and preparing data, checking the quality of the data and then developing, testing, and revising analytical methodologies. Once data is deemed ready, data analysts and scientists can explore and analyze the data using statistical methods such as SAS or machine learning models using Spark ML. The data itself is prepared by data engineering teams and the data quality team checks the data collected. Data governance becomes a factor too to ensure the proper collection and protection of the data. Another not commonly known role is that of a Data Steward who specializes in understanding data to the byte, exactly where it is coming from, all transformations that occur, and what the business really needs from the column or field of data.</p>
<p>Various entities in the business might be dealing with addresses differently, <strong class="calibre27">123 N Main St</strong> as opposed to <strong class="calibre27">123 North Main Street.</strong> But, our analytics depends on getting the correct address field; otherwise both the addresses mentioned above will be considered different and our analytics will not have the same accuracy.</p>
<p class="mce-root">The analytics process starts with data collection based on what the analysts might need from the data warehouse, collecting all sorts of data in the organization (Sales, Marketing, Employee, Payroll, HR, and so on). Data stewards and the Governance team are important here to make sure the right data is collected and that any information deemed confidential or private is not accidentally exported out even if the end users are all employees.</p>
<p>Social Security Numbers or full addresses might not be a good idea to include in analytics as this can cause a lot of problems to the organization.</p>
<p class="mce-root">Data quality processes must be established to make sure the data being collected and engineered is correct and will match the needs of the data scientists. At this stage, the main goal is to find and fix data quality problems that could affect the accuracy of analytical needs. Common techniques are profiling the data and cleansing the data to make sure that the information in a dataset is consistent, and also that any errors and duplicate records are removed.</p>
<p class="mce-root">Data from disparate source systems may need to be combined, transformed, and normalized using various data engineering techniques, such as distributed computing or MapReduce programming, Stream processing, or SQL queries, and then stored on Amazon S3, Hadoop cluster, NAS, or SAN storage devices or a traditional data warehouse such as Teradata. Data preparation or engineering work involves techniques to manipulate and organize the data for the planned analytics use.</p>
<p class="mce-root">Once we have the data prepared and checked for quality, and it is available for the Data scientists or analysts to use, the actual analytical work starts. A Data scientist can now build an analytical model using predictive modeling tools and languages such as SAS, Python, R, Scala, Spark, H2O, and so on. The model is initially run against a partial dataset to test its accuracy in the <em class="calibre8">training phase</em>. Several iterations of the training phase are common and expected in any analytical project. After adjustments at the model level, or sometimes going all the way to the Data Steward to get or fix some data being collected or prepared, the model output tends to get better and better. Finally, a stable state is reached when further tuning does not change the outcome noticeably; at this time, we can think of the model as being ready for production usage.</p>
<p class="mce-root">Now, the model can be run in production mode against the full dataset and generate outcomes or results based on how we trained the model. The choices made in building the analysis, either statistical or machine learning, directly affect the quality and the purpose of the model. You cannot look at the sales from groceries and figure out if Asians buy more milk than Mexicans as that needs additional elements from demographical data. Similarly, if our analysis was focused on customer experience (returns or exchanges of products) then it is based on different techniques and models than if we are trying to focus on revenue or up-sell customers.</p>
<p>You will see various machine learning techniques in later chapters.</p>
<p class="mce-root">Analytical applications can thus be realized using several disciplines, teams, and skillsets. Analytical applications can be used to generate reports all the way to automatically triggering business actions. For example, you can simply create daily sales reports to be emailed out to all managers every day at 8 a.m. in the morning. But, you can also integrate with Business process management applications or some custom stock trading application to take action, such as buying, selling, or alerting on activities in the stock market. You can also think of taking in news articles or social media information to further influence the decisions to be made.</p>
<p class="mce-root">Data visualization is an important piece of data analytics and it's hard to understand numbers when you are looking at a lot of metrics and calculation. Rather, there is an increasing dependence on <strong class="calibre1">Business Intelligence</strong> (<strong class="calibre1">BI</strong>) tools, such as Tableau, QlikView, and so on, to explore and analyze data. Of course, large-scale visualization such as showing all Uber cars in the country or heat maps showing the water supply in New York City requires more custom applications or specialized tools to be built.</p>
<p class="mce-root">Managing and analyzing data has always been a challenge across many organizations of different sizes across all industries. Businesses have always struggled to find a pragmatic approach to capturing information about their customers, products, and services. When the company only had a handful of customers who bought a few of their items, it was not that difficult. It was not as big a challenge. But over time, companies in the markets started growing. Things have become more complicated. Now, we have branding Information and social media. We have things that are sold and bought over the Internet. We need to come up with different solutions. Web development, organizations, pricing, social networks, and segmentations; there's a lot of different data that we're dealing with that brings a lot more complexity when it comes to dealing, managing, organizing, and trying to gain some insight from the data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Introduction to big data</h1>
                
            
            
                
<p class="mce-root">As seen in the preceding section, data analytics incorporates techniques, tools, and methodologies to explore and analyze data to produce quantifiable outcomes for the business. The outcome could be a simple choice of a color to paint the storefront or more complicated predictions of customer behavior. As businesses grow, more and more varieties of analytics are coming into the picture. In 1980s or 1990s , all we could get was what was available in a SQL Data Warehouse; nowadays a lot of external factors are all playing an important role in influencing the way businesses run.</p>
<p>Twitter, Facebook, Amazon, Verizon, Macy's, and Whole Foods are all companies that run their business using data analytics and base many of the decisions on it. Think about what kind of data they are collecting, how much data they might be collecting, and then how they might be using the data.</p>
<p class="mce-root">Let's look at our grocery store example seen earlier. What if the store starts expanding its business to set up 100s of stores. Naturally, the sales transactions will have to be collected and stored on a scale that is 100s of times more than the single store. But then, no business works independently any more. There is a lot of information out there starting from local news, tweets, yelp reviews, customer complaints, survey activities, competition from other stores, changing demographics, or the economy of the local area, and so on. All such additional data can help in better understanding customer behavior and revenue models.</p>
<p class="mce-root">For example, if we see increasing negative sentiment regarding the store parking facility, then we could analyze this and take corrective action such as validated parking or negotiating with the city public transportation department to provide more frequent trains or buses for better reach.</p>
<p class="mce-root">Such increasing quantity and a variety of data while provides better analytics also poses challenges to the business IT organization trying to store, process, and analyze all the data. It is, in fact, not uncommon to see TBs of data.</p>
<p>Every day, we create more than 2 quintillion bytes of data (2 Exa Bytes), and it is estimated that more than 90% of the data has been generated in the last few years alone.<br class="calibre23"/>
<strong class="calibre27">1 KB = 1024 Bytes</strong><br class="calibre23"/>
<strong class="calibre27">1 MB = 1024 KB</strong><br class="calibre23"/>
<strong class="calibre27">1 GB = 1024 MB</strong><br class="calibre23"/>
<strong class="calibre27">1 TB = 1024 GB ~ 1,000,000 MB</strong><br class="calibre23"/>
<strong class="calibre27">1 PB = 1024 TB ~ 1,000,000 GB ~ 1,000,000,000 MB</strong><br class="calibre23"/>
<strong class="calibre27">1 EB = 1024 PB ~ 1,000,000 TB ~ 1,000,000,000 GB ~ 1,000,000,000,000 MB</strong></p>
<p class="mce-root">Such large amounts of data since the 1990s, and the need to understand and make sense of the data, gave rise to the term <em class="calibre8">big data</em>.</p>
<p>The term big data, which spans computer science and statistics/econometrics, probably originated in the lunch-table conversations at Silicon Graphics in the mid-1990s, in which John Mashey figured prominently.</p>
<p class="mce-root">In 2001, Doug Laney, then an analyst at consultancy Meta Group Inc (which got acquired by Gartner) introduced the idea of 3Vs (variety, velocity, and volume). Now, we refer to 4 Vs instead of 3Vs with the addition of Veracity of data to the 3Vs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">4 Vs of big data</h1>
                
            
            
                
<p class="mce-root">The following are the 4 Vs of big data used to describe the properties of big data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Variety of Data</h1>
                
            
            
                
<p class="mce-root">Data can be from weather sensors, car sensors, census data, Facebook updates, tweets, transactions, sales, and marketing. The data format is both structured and unstructured as well. Data types can also be different; binary, text, JSON, and XML.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Velocity of Data</h1>
                
            
            
                
<p class="mce-root">Data can be obtained from a data warehouse, batch mode file archives, near real-time updates, or instantaneous real-time updates from the Uber ride you just booked.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Volume of Data</h1>
                
            
            
                
<p class="mce-root">Data can be collected and stored for an hour, a day, a month, a year, or 10 years. The size of data is growing to 100s of TBs for many companies.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Veracity of Data</h1>
                
            
            
                
<p class="mce-root">Data can be analyzed for actionable insights, but with so much data of all types being analyzed from across data sources, it is very difficult to ensure correctness and proof of accuracy.</p>
<p class="mce-root">The following are the 4 Vs of big data:</p>
<div><img class="image-border34" src="img/00115.jpeg"/></div>
<p class="mce-root">To make sense of all the data and apply data analytics to big data, we need to expand the concept of data analytics to operate at a much larger scale dealing with the 4 Vs of big data. This changes not only the tools, technologies, and methodologies used in analyzing data, but also the way we even approach the problem. If a SQL database was used for data in a business in 1999, now to handle the data for the same business we will need a distributed SQL database scalable and adaptable to the nuances of the big data space.</p>
<p class="mce-root">Big data analytics applications often include data from both internal systems and external sources, such as weather data or demographic data on consumers compiled by third-party information services providers. In addition, streaming analytics applications are becoming common in big data environments, as users look to do real-time analytics on data fed into Hadoop systems through Spark's Spark streaming module or other open source stream processing engines, such as Flink and Storm.</p>
<p class="mce-root">Early big data systems were mostly deployed on-premises particularly in large organizations that were collecting, organizing, and analyzing massive amounts of data. But cloud platform vendors, such as <strong class="calibre1">Amazon Web Services</strong> (<strong class="calibre1">AWS</strong>) and Microsoft, have made it easier to set up and manage Hadoop clusters in the cloud, as have Hadoop suppliers such as Cloudera and Hortonworks, which support their distributions of the big data framework on the AWS and Microsoft Azure clouds. Users can now spin up clusters in the cloud, run them for as long as needed, and then take them offline, with usage-based pricing that doesn't require ongoing software licenses.</p>
<p class="mce-root">Potential pitfalls that can trip up organizations on big data analytics initiatives include a lack of internal analytics skills and the high cost of hiring experienced data scientists and data engineers to fill the gaps.</p>
<p class="mce-root">The amount of data that's typically involved, and its variety, can cause data management issues in areas including data quality, consistency, and governance; also, data silos can result from the use of different platforms and data stores in a big data architecture. In addition, integrating Hadoop, Spark, and other big data tools into a cohesive architecture that meets an organization's big data analytics needs is a challenging proposition for many IT and analytics teams, which have to identify the right mix of technologies and then put the pieces together.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Distributed computing using Apache Hadoop</h1>
                
            
            
                
<p class="mce-root">Our world is filled with devices starting from the smart refrigerator, smart watch, phone, tablet, laptops, kiosks at the airport, ATM dispensing cash to you, and many many more. We are able to do things we could not even imagine just a few years ago. Instagram, Snapchat, Gmail, Facebook, Twitter, and Pinterest are a few of the applications we are now so used to; it is difficult to imagine a day without access to such applications.</p>
<p class="mce-root">With the advent of Cloud computing, using a few clicks we are able to launch 100s if not, 1000s of machines in AWS, Azure (Microsoft), or Google Cloud among others and use immense resources to realize our business goals of all sorts.</p>
<p class="mce-root">Cloud computing has introduced us to the concepts of IaaS, PaaS, and SaaS, which gives us the ability to build and operate scalable infrastructures serving all types of use cases and business needs.</p>
<div><strong class="calibre27">IaaS</strong> (<strong class="calibre27">Infrastructure as a Service</strong>) - Reliable-managed hardware is provided without the need for a Data center, power cords, Airconditioning, and so on.<br class="calibre23"/>
<strong class="calibre27">PaaS</strong> (<strong class="calibre27">Platform as a Service</strong>) - On top of IaaS, managed platforms such as Windows, Linux , Databases and so on are provided.<br class="calibre23"/>
<strong class="calibre27">SaaS</strong> (<strong class="calibre27">Software as a Service</strong>) - On top of SaaS, managed services such as SalesForce, <a href="https://www.kayak.co.in/?ispredir=true" class="calibre21">Kayak.com</a> and so on are provided to everyone.</div>
<p class="mce-root">Behind the scenes is the world of highly scalable distributed computing, which makes it possible to store and process PB (PetaBytes) of data.</p>
<p>1 ExaByte = 1024 PetaBytes (50 Million Blue Ray Movies)<br class="calibre23"/>
1 PetaByte = 1024 Tera Bytes (50,000 Blue Ray Movies)<br class="calibre23"/>
1 TeraByte = 1024 Giga Bytes (50 Blue Ray Movies)<br class="calibre23"/>
Average size of 1 Blue Ray Disc for a Movie is ~ 20 GB</p>
<p class="mce-root">Now, the paradigm of Distributed Computing is not really a genuinely new topic and has been pursued in some shape or form over decades primarily at research facilities as well as by a few commercial product companies. <strong class="calibre1">Massively Parallel Processing</strong> (<strong class="calibre1">MPP</strong>) is a paradigm that was in use decades ago in several areas such as Oceanography, Earthquake monitoring, and Space exploration. Several companies such as Teradata also implemented MPP platforms and provided commercial products and applications. Eventually, tech companies such as Google and Amazon among others pushed the niche area of scalable distributed computing to a new stage of evolution, which eventually led to the creation of Apache Spark by Berkeley University.</p>
<p class="mce-root">Google published a paper on <strong class="calibre1">Map Reduce</strong> (<strong class="calibre1">MR</strong>) as well as <strong class="calibre1">Google File System</strong> (<strong class="calibre1">GFS</strong>), which brought the principles of distributed computing to everyone. Of course, due credit needs to be given to Doug Cutting, who made it possible by implementing the concepts given in the Google white papers and introducing the world to Hadoop.</p>
<p class="mce-root">The Apache Hadoop Framework is an open source software framework written in Java. The two main areas provided by the framework are storage and processing. For Storage, the Apache Hadoop Framework uses <strong class="calibre1">Hadoop Distributed File System</strong> (<strong class="calibre1">HDFS</strong>), which is based on the Google File System paper released on October 2003. For processing or computing, the framework depends on MapReduce, which is based on a Google paper on MR released in December 2004.</p>
<div><br class="calibre23"/>
The MapReduce framework evolved from V1 (based on Job Tracker and Task Tracker) to V2 (based on YARN).</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Hadoop Distributed File System (HDFS)</h1>
                
            
            
                
<p class="mce-root">HDFS is a software-based filesystem implemented in Java and sits on top of the native file system. The main concept behind HDFS is that it divides a file into blocks (typically 128 MB) instead of dealing with a file as a whole. This allowed many features such as distribution, replication, failure recovery, and more importantly distributed processing of the blocks using multiple machines.</p>
<p>Block sizes can be 64 MB, 128 MB, 256 MB, or 512 MB, whatever suits the purpose. For a 1 GB file with 128 MB blocks, there will be 1024 MB / 128 MB = 8 blocks. If you consider replication factor of 3, this makes it 24 blocks.</p>
<p class="mce-root">HDFS provides a distributed storage system with fault tolerance and failure recovery. HDFS has two main components: name node and data node(s). Name node contains all the metadata of all content of the file system. Data nodes connect to the Name Node and rely on the name node for all metadata information regarding the content in the file system. If the name node does not know any information, data node will not be able to serve it to any client who wants to read/write to the HDFS.</p>
<p class="mce-root">The following is the HDFS architecture:</p>
<div><img class="image-border35" src="img/00120.jpeg"/></div>
<p class="mce-root">NameNode and DataNode are JVM processes so any machine that supports Java can run the NameNode or the DataNode process. There is only one NameNode (the second NameNode will be there too if you count the HA deployment) but 100s if not 1000s of DataNodes.</p>
<p>It is not advisable to have 1000s of DataNodes because all operations from all the DataNodes will tend to overwhelm the NameNode in a real production environment with a lot of data-intensive applications.</p>
<p class="mce-root">The existence of a single NameNode in a cluster greatly simplifies the architecture of the system. The NameNode is the arbitrator and repository for all HDFS metadata and any client, that wants to read/write data first contacts the NameNode for the metadata information. The data never flows directly through the NameNode, which allows 100s of DataNodes (PBs of data) to be managed by 1 NameNode.</p>
<p class="mce-root">HDFS supports a traditional hierarchical file organization with directories and files similar to most other filesystems. You can create, move, and delete files, and directories. The NameNode maintains the filesystem namespace and records all changes and the state of the filesystem. An application can specify the number of replicas of a file that should be maintained by HDFS and this information is also stored by the NameNode.</p>
<p class="mce-root">HDFS is designed to reliably store very large files in a distributed manner across machines in a large cluster of data nodes. To deal with replication, fault tolerance, as well as distributed computing, HDFS stores each file as a sequence of blocks.</p>
<p class="mce-root">The NameNode makes all decisions regarding the replication of blocks. This is mainly dependent on a Block report from each of the DataNodes in the cluster received periodically at a heart beat interval. A block report contains a list of all blocks on a DataNode, which the NameNode then stores in its metadata repository.</p>
<p class="mce-root">The NameNode stores all metadata in memory and serves all requests from clients reading from/writing to HDFS. However, since this is the master node maintaining all the metadata about the HDFS, it is critical to maintain consistent and reliable metadata information. If this information is lost, the content on the HDFS cannot be accessed.</p>
<p class="mce-root">For this purpose, HDFS NameNode uses a transaction log called the EditLog, which persistently records every change that occurs to the metadata of the filesystem. Creating a new file updates EditLog, so does moving a file or renaming a file, or deleting a file. The entire filesystem namespace, including the mapping of blocks to files and filesystem properties, is stored in a file called the <kbd class="calibre11">FsImage</kbd>. The <strong class="calibre1">NameNode</strong> keeps everything in memory as well. When a NameNode starts up, it loads the EditLog and the <kbd class="calibre11">FsImage</kbd> initializes itself to set up the HDFS.</p>
<p class="mce-root">The DataNodes, however, have no idea about the HDFS, purely relying on the blocks of data stored. DataNodes rely entirely on the NameNode to perform any operations. Even when a client wants to connect to read a file or write to a file, it's the NameNode that tells the client where to connect to.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">HDFS High Availability</h1>
                
            
            
                
<p class="mce-root">HDFS is a Master-Slave cluster with the NameNode as the master and the 100s, if not 1000s of DataNodes as slaves, managed by the master node. This introduces a <strong class="calibre1">Single Point of Failure</strong> (<strong class="calibre1">SPOF</strong>) in the cluster as if the Master NameNode goes down for some reason, the entire cluster is going to be unusable. HDFS 1.0 supports an additional Master Node known as the <strong class="calibre1">Secondary NameNode</strong> to help with recovery of the cluster. This is done by maintaining a copy of all the metadata of the filesystem and is by no means a Highly Available System requiring manual interventions and maintenance work. HDFS 2.0 takes this to the next level by adding support for full <strong class="calibre1">High Availability</strong> (<strong class="calibre1">HA</strong>).</p>
<p class="mce-root">HA works by having two Name Nodes in an active-passive mode such that one Name Node is active and other is passive. When the primary NameNode has a failure, the passive Name Node will take over the role of the Master Node.</p>
<p class="mce-root">The following diagram shows how the active-passive pair of NameNodes will be deployed:</p>
<div><img class="image-border36" src="img/00123.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">HDFS Federation</h1>
                
            
            
                
<p class="mce-root">HDFS Federation is a way of using multiple name nodes to spread the filesystem namespace over. Unlike the first HDFS versions, which simply managed entire clusters using a single NameNode, which does not scale that well as the size of the cluster grows, HDFS Federation can support significantly larger clusters and horizontally scales the NameNode or name service using multiple federated name nodes. Take a look at the following diagram:</p>
<div><img class="image-border37" src="img/00127.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">HDFS Snapshot</h1>
                
            
            
                
<p class="mce-root">Hadoop 2.0 also added a new capability: taking a snapshot (read-only copy and copy-on-write) of the filesystem (data blocks) stored on the data nodes. Using Snapshots, you can take a copy of directories seamlessly using the NameNode's metadata of the data blocks. Snapshot creation is instantaneous and doesn't require interference with other regular HDFS operations.</p>
<p class="mce-root">The following is an illustration of how snapshot works on specific directories:</p>
<div><img class="image-border38" src="img/00131.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">HDFS Read</h1>
                
            
            
                
<p class="mce-root">Client connects to the NameNode and ask about a file using the name of the file. NameNode looks up the block locations for the file and returns the same to the client. The client can then connect to the DataNodes and read the blocks needed. NameNode does not participate in the data transfer.</p>
<p class="mce-root">The following is the flow of a read request from a client. First, the client gets the locations and then pulls the blocks from the DataNodes. If a DataNode fails in the middle, then the client gets the replica of the block from another DataNode.</p>
<p class="mce-root"> </p>
<div><img class="image-border39" src="img/00264.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">HDFS Write</h1>
                
            
            
                
<p class="mce-root">The client connects to the NameNode and asks the NameNode to let it write to the HDFS. The NameNode looks up information and plans the blocks, the Data Nodes to be used to store the blocks, and the replication strategy to be used. The NameNode does not handle any data and only tells the client where to write. Once the first DataNode receives the block, based on the replication strategy, the NameNode tells the first DataNode where else to replicate. So, the DataNode that is received from client sends the block over to the second DataNode (where the copy of the block is supposed to be written to) and then the second DataNode sends it to a third DataNode (if replication-factor is 3).</p>
<p class="mce-root">The following is the flow of a write request from a client. First, the client gets the locations and then writes to the first DataNode. The DataNode that receives the block replicates the block to the DataNodes that should hold the replica copy of the block. This happens for all the blocks being written to from the client. If a DataNode fails in the middle, then the block gets replicated to another DataNode as determined by the NameNode.</p>
<div><img class="image-border40" src="img/00267.jpeg"/></div>
<p class="mce-root">So far, we have seen how HDFS provides a distributed filesystem using blocks, the NameNode, and DataNodes. Once data is stored at a PB scale, it is also important to actually process the data to serve the various use cases of the business.</p>
<p class="mce-root">MapReduce framework was created in the Hadoop framework to perform distributed computation. We will look at this further in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">MapReduce framework</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre1">MapReduce</strong> (<strong class="calibre1">MR</strong>) framework enables you to write distributed applications to process large amounts of data from a filesystem such as HDFS in a reliable and fault-tolerant manner. When you want to use the MapReduce Framework to process data, it works through the creation of a job, which then runs on the framework to perform the tasks needed.</p>
<p class="mce-root">A MapReduce job usually works by splitting the input data across worker nodes running <strong class="calibre1">Mapper</strong> tasks in a parallel manner. At this time, any failures that happen either at the HDFS level or the failure of a Mapper task are handled automatically to be fault-tolerant. Once the Mappers are completed, the results are copied over the network to other machines running <strong class="calibre1">Reducer</strong> tasks.</p>
<p class="mce-root">An easy way to understand this concept is to imagine that you and your friends want to sort out piles of fruit into boxes. For that, you want to assign each person the task of going through one raw basket of fruit (all mixed up) and separate out the fruit into various boxes. Each person then does the same with this basket of fruit.</p>
<p class="mce-root">In the end, you end up with a lot of boxes of fruit from all your friends. Then, you can assign a group to put the same kind of fruit together in a box, weight the box, and seal the box for shipping.</p>
<p class="mce-root">The following depicts the idea of taking fruit baskets and sorting the fruit by the type of fruit:</p>
<div><img class="image-border41" src="img/00270.jpeg"/></div>
<p class="mce-root">MapReduce framework consists of a single resource manager and multiple node managers (usually Node Managers coexist with the data nodes of HDFS). When an application wants to run, the client launches the application master, which then negotiates with the resource manager to get resources in the cluster in form of containers.</p>
<p>A container represents CPUs (cores) and memory allocated on a single node to be used to run tasks and processes. Containers are supervised by the node manager and scheduled by the resource manager.<br class="calibre23"/>
Examples of containers:<br class="calibre23"/>
1 core + 4 GB RAM<br class="calibre23"/>
2 cores + 6 GB RAM<br class="calibre23"/>
4 cores + 20 GB RAM</p>
<p class="mce-root">Some Containers are assigned to be Mappers and other to be Reducers; all this is coordinated by the application master in conjunction with the resource manager. This framework is called <strong class="calibre1">Yet Another Resource Negotiator</strong> (<strong class="calibre1">YARN</strong>)</p>
<p class="mce-root">The following is a depiction of YARN:</p>
<div><img class="image-border42" src="img/00276.jpeg"/></div>
<p class="mce-root">A classic example showing the MapReduce framework at work is the word count example. The following are the various stages of processing the input data, first splitting the input across multiple worker nodes and then finally generating the output counts of words:</p>
<div><img class="image-border43" src="img/00279.jpeg"/></div>
<p class="mce-root">Though MapReduce framework is very successful all across the world and has been adopted by most companies, it does run into issues mainly because of the way it processes data. Several technologies have come into existence to try and make MapReduce easier to use such as Hive and Pig but the complexity remains.</p>
<p class="mce-root">Hadoop MapReduce has several limitations such as:</p>
<ul class="calibre9">
<li class="mce-root1">Performance bottlenecks due to disk-based processing</li>
<li class="mce-root1">Batch processing doesn't serve all needs</li>
<li class="mce-root1">Programming can be verbose and complex</li>
<li class="mce-root1">Scheduling of the tasks is slow as there is not much reuse of resources</li>
<li class="mce-root1">No good way to do real-time event processing</li>
<li class="mce-root1">Machine learning takes too long as usually ML involves iterative processing and MR is too slow for this</li>
</ul>
<p>Hive was created by Facebook as a SQL-like interface to MR. Pig was created by Yahoo with a scripting interface to MR. Moreover, several enhancements such as Tez (Hortonworks) and LLAP (Hive2.x) are in use, which makes use of in-memory optimizations to circumvent the limitations of MapReduce.</p>
<p class="mce-root">In the next section, we will look at Apache Spark, which has already solved some of the limitations of Hadoop technologies.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Here comes Apache Spark</h1>
                
            
            
                
<p class="mce-root">Apache Spark is a unified distributed computing engine across different workloads and platforms. Spark can connect to different platforms and process different data workloads using a variety of paradigms such as Spark streaming, Spark ML, Spark SQL, and Spark GraphX.</p>
<p class="mce-root">Apache Spark is a fast in-memory data processing engine with elegant and expressive development APIs to allow data workers to efficiently execute streaming machine learning or SQL workloads that require fast interactive access to data sets. Apache Spark consists of Spark core and a set of libraries. The core is the distributed execution engine and the Java, Scala, and Python APIs offer a platform for distributed application development. Additional libraries built on top of the core allow workloads for streaming, SQL, Graph processing, and machine learning. Spark ML, for instance, is designed for data science and its abstraction makes data science easier.</p>
<p class="mce-root">Spark provides real-time streaming, queries, machine learning, and graph processing. Before Apache Spark, we had to use different technologies for different types of workloads, one for batch analytics, one for interactive queries, one for real-time streaming processing and another for machine learning algorithms. However, Apache Spark can do all of these just using Apache Spark instead of using multiple technologies that are not always integrated.</p>
<div><br class="calibre23"/>
Using Apache Spark, all types of workload can be processed and Spark also supports Scala, Java, R, and Python as a means of writing client programs.</div>
<p class="mce-root">Apache Spark is an open-source distributed computing engine which has key advantages over the MapReduce paradigm:</p>
<ul class="calibre9">
<li class="mce-root1">Uses in-memory processing as much as possible</li>
<li class="mce-root1">General purpose engine to be used for batch, real-time workloads</li>
<li class="mce-root1">Compatible with YARN and also Mesos</li>
<li class="mce-root1">Integrates well with HBase, Cassandra, MongoDB, HDFS, Amazon S3, and other file systems and data sources</li>
</ul>
<p class="mce-root">Spark was created in Berkeley back in 2009 and was a result of the project to build Mesos, a cluster management framework to support different kinds of cluster computing systems. Take a look at the following table:</p>
<table class="wikitable">
<tbody class="calibre5">
<tr class="calibre6">
<th class="calibre30">Version</th>
<th class="calibre30">Release date</th>
<th class="calibre30">Milestones</th>
</tr>
<tr class="calibre6">
<td class="calibre7">0.5</td>
<td class="calibre7">2012-10-07</td>
<td class="calibre7">First available version for non-production usage</td>
</tr>
<tr class="calibre6">
<td class="calibre7">0.6</td>
<td class="calibre7">2013-02-07</td>
<td class="calibre7">Point release with various changes</td>
</tr>
<tr class="calibre6">
<td class="calibre7">0.7</td>
<td class="calibre7">2013-07-16</td>
<td class="calibre7">Point release with various changes</td>
</tr>
<tr class="calibre6">
<td class="calibre7">0.8</td>
<td class="calibre7">2013-12-19</td>
<td class="calibre7">Point release with various changes</td>
</tr>
<tr class="calibre6">
<td class="calibre7">0.9</td>
<td class="calibre7">2014-07-23</td>
<td class="calibre7">Point release with various changes</td>
</tr>
<tr class="calibre6">
<td class="calibre7">1.0</td>
<td class="calibre7">2014-08-05</td>
<td class="calibre7">First production ready, backward-compatible release. Spark Batch, Streaming, Shark, MLLib, GraphX</td>
</tr>
<tr class="calibre6">
<td class="calibre7">1.1</td>
<td class="calibre7">2014-11-26</td>
<td class="calibre7">Point release with various changes</td>
</tr>
<tr class="calibre6">
<td class="calibre7">1.2</td>
<td class="calibre7">2015-04-17</td>
<td class="calibre7">Structured Data, SchemaRDD (subsequently evolved into DataFrames)</td>
</tr>
<tr class="calibre6">
<td class="calibre7">1.3</td>
<td class="calibre7">2015-04-17</td>
<td class="calibre7">API to provide a unified API to read from structured and semi-structured sources</td>
</tr>
<tr class="calibre6">
<td class="calibre7">1.4</td>
<td class="calibre7">2015-07-15</td>
<td class="calibre7">SparkR, DataFrame API, Tungsten improvements</td>
</tr>
<tr class="calibre6">
<td class="calibre7">1.5</td>
<td class="calibre7">2015-11-09</td>
<td class="calibre7">Point release with various changes</td>
</tr>
<tr class="calibre6">
<td class="calibre7">1.6</td>
<td class="calibre7">2016-11-07</td>
<td class="calibre7">Dataset DSL introduced</td>
</tr>
<tr class="calibre6">
<td class="calibre7">2.0</td>
<td class="calibre7">2016-11-14</td>
<td class="calibre7">
<p class="mce-root">DataFrames and Datasets API as fundamental layer for ML, Structured Streaming,</p>
<p class="mce-root">SparkR improvements.</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">2.1</td>
<td class="calibre7">2017-05-02</td>
<td class="calibre7">Event time watermarks, ML, GraphX improvements</td>
</tr>
</tbody>
</table>
<p>2.2 has been released 2017-07-11 which has several improvements especially Structured Streaming which is now GA.</p>
<p class="mce-root">Spark is a platform for distributed computing that has several features:</p>
<ul class="calibre9">
<li class="mce-root1">Transparently processes data on multiple nodes via a simple API</li>
<li class="mce-root1">Resiliently handles failures</li>
<li class="mce-root1">Spills data to disk as necessary though predominantly uses memory</li>
<li class="mce-root1">Java, Scala, Python, R, and SQL APIs are supported</li>
<li class="mce-root1">The same Spark code can run standalone, in Hadoop YARN, Mesos, and the cloud</li>
</ul>
<div><br class="calibre23"/>
Scala features such as implicits, higher-order functions, structured types, and so on allow us to easily build DSL's and integrate them with the language.</div>
<p class="mce-root">Apache Spark does not provide a Storage layer and relies on HDFS or Amazon S3 and so on. Hence, even if Apache Hadoop technologies are replaced with Apache Spark, HDFS is still needed to provide a reliable storage layer.</p>
<p>Apache Kudu provides an alternative to HDFS and there is already integration between Apache Spark and Kudu Storage layer, further decoupling Apache Spark and the Hadoop ecosystem.</p>
<p class="mce-root">Hadoop and Apache Spark are both popular big data frameworks, but they don't really serve the same purposes. While Hadoop provides distributed storage and a MapReduce distributed computing framework, Spark on the other hand is a data processing framework that operates on the distributed data storage provided by other technologies.</p>
<p class="mce-root">Spark is generally a lot faster than MapReduce because of the way it processes data. MapReduce operates on splits using Disk operations, Spark operates on the dataset much more efficiently than MapReduce, with the main reason behind the performance improvement in Apache Spark being the efficient off-heap in-memory processing rather than solely relying on disk-based computations.</p>
<p>MapReduce's processing style can be sufficient if you were data operations and reporting requirements are mostly static and it is okay to use batch processing for your purposes, but if you need to do analytics on streaming data or your processing requirements need multistage processing logic, you will probably want to want to go with Spark.</p>
<p class="mce-root">There are three layers in the Spark stack. The bottom layer is the cluster manager, which can be standalone, YARN, or Mesos.</p>
<div><br class="calibre23"/>
Using local mode, you don't need a cluster manager to process.</div>
<p class="mce-root">In the middle, above the cluster manager, is the layer of Spark core, which provides all the underlying APIs to perform task scheduling and interacting with storage.</p>
<p class="mce-root">At the top are modules that run on top of Spark core such as Spark SQL to provide interactive queries, Spark streaming for real-time analytics, Spark ML for machine learning, and Spark GraphX for graph processing.</p>
<p class="mce-root">The three layers are as follows:</p>
<div><img class="image-border44" src="img/00283.jpeg"/></div>
<p class="mce-root">As seen in the preceding diagram, the various libraries such as Spark SQL, Spark streaming, Spark ML, and GraphX all sit on top of Spark core, which is the middle layer. The bottom layer shows the various cluster manager options.</p>
<p class="mce-root">Let's now look at each of the component briefly:</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Spark core</h1>
                
            
            
                
<p class="mce-root">Spark core is the underlying general execution engine for the Spark platform that all other functionality is built upon. Spark core contains basic Spark functionalities required for running jobs and needed by other components. It provides in-memory computing and referencing datasets in external storage systems, the most important being the <strong class="calibre1">Resilient Distributed Dataset</strong> (<strong class="calibre1">RDD</strong>).</p>
<p class="mce-root">In addition, Spark core contains logic for accessing various filesystems, such as HDFS, Amazon S3, HBase, Cassandra, relational databases, and so on. Spark core also provides fundamental functions to support networking, security, scheduling, and data shuffling to build a high scalable, fault-tolerant platform for distributed computing.</p>
<p>We cover Spark core in detail in <a href="part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c" class="calibre21">Chapter 6</a>, <em class="calibre25">Start Working with Spark - REPL</em> and RDDs and <a href="part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c" class="calibre21">Chapter 7</a>, <em class="calibre25">Special RDD Operations</em>.</p>
<p class="mce-root">DataFrames and datasets built on top of RDDs and introduced with Spark SQL are becoming the norm now over RDDs in many use cases. RDDs are still more flexible in terms of handling totally unstructured data, but in future datasets, API might eventually become the core API.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Spark SQL</h1>
                
            
            
                
<p class="mce-root">Spark SQL is a component on top of Spark core that introduces a new data abstraction called <strong class="calibre1">SchemaRDD</strong>, which provides support for structured and semi-structured data. Spark SQL provides functions for manipulating large sets of distributed, structured data using an SQL subset supported by Spark and Hive QL. Spark SQL simplifies the handling of structured data through DataFrames and datasets at a much more performant level as part of the Tungsten initative. Spark SQL also supports reading and writing data to and from various structured formats and data sources, files, parquet, orc, relational databases, Hive, HDFS, S3, and so on. Spark SQL provides a query optimization framework called <strong class="calibre1">Catalyst</strong> to optimize all operations to boost the speed (compared to RDDs Spark SQL is several times faster). Spark SQL also includes a Thrift server, which can be used by external systems to query data through Spark SQL using classic JDBC and ODBC protocols.</p>
<div><br class="calibre23"/>
We cover Spark SQL in detail in <a href="part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c" class="calibre21">Chapter 8</a>, <em class="calibre25">Introduce a Little Structure - Spark SQL</em>.</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Spark streaming</h1>
                
            
            
                
<p class="mce-root">Spark streaming leverages Spark core's fast scheduling capability to perform streaming analytics by ingesting real-time streaming data from various sources such as HDFS, Kafka, Flume, Twitter, ZeroMQ, Kinesis, and so on. Spark streaming uses micro-batches of data to process the data in chunks and, uses a concept known as DStreams, Spark streaming can operate on the RDDs, applying transformations and actions as regular RDDs in the Spark core API. Spark streaming operations can recover from failure automatically using various techniques. Spark streaming can be combined with other Spark components in a single program, unifying real-time processing with machine learning, SQL, and graph operations.</p>
<div><br class="calibre23"/>
We cover Spark streaming in detail in the <a href="part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c" class="calibre21">Chapter 9</a>, <em class="calibre25">Stream Me Up, Scotty - Spark Streaming</em>.</div>
<p class="mce-root">In addition, the new Structured Streaming API makes Spark streaming programs more similar to Spark batch programs and also allows real-time querying on top of streaming data, which is complicated with the Spark streaming library before Spark 2.0+.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Spark GraphX</h1>
                
            
            
                
<p class="mce-root">GraphX is a distributed graph processing framework on top of Spark. Graphs are data structures comprising vertices and the edges connecting them. GraphX provides functions for building graphs, represented as Graph RDDs. It provides an API for expressing graph computation that can model user-defined graphs by using the Pregel abstraction API. It also provides an optimized runtime for this abstraction. GraphX also contains implementations of the most important algorithms of graph theory, such as page rank, connected components, shortest paths, SVD++, and others.</p>
<div><br class="calibre23"/>
We cover Spark Graphx in detail in <a href="part0326.html#9MSNC1-21aec46d8593429cacea59dbdcd64e1c" class="calibre21">Chapter 10</a>, <em class="calibre25">Everything is Connected - GraphX</em>.</div>
<p class="mce-root">A newer module known as GraphFrames is in development, which makes it easier to do Graph processing using DataFrame-based Graphs. GraphX is to RDDs what GraphFrames are to DataFrames/datasets. Also, this is currently separate from GraphX and is expected to support all the functionality of GraphX in the future, when there might be a switch over to GraphFrames.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Spark ML</h1>
                
            
            
                
<p class="mce-root">MLlib is a distributed machine learning framework above Spark core and handles machine-learning models used for transforming datasets in the form of RDDs. Spark MLlib is a library of machine-learning algorithms providing various algorithms such as logistic regression, Naive Bayes classification, <strong class="calibre1">Support Vector Machines</strong> (<strong class="calibre1">SVMs</strong>), decision trees, random forests, linear regression, <strong class="calibre1">Alternating Least Squares</strong> (<strong class="calibre1">ALS</strong>), and k-means clustering. Spark ML integrates very well with Spark core, Spark streaming, Spark SQL, and GraphX to provide a truly integrated platform where data can be real-time or batch.</p>
<div><br class="calibre23"/>
We cover Spark ML in detail in <a href="part0170.html#523VK1-21aec46d8593429cacea59dbdcd64e1c" class="calibre21">Chapter 11</a>, <em class="calibre25">Learning Machine Learning - Spark MLlib and ML</em>.</div>
<p class="mce-root">In addition, PySpark and SparkR are also available as means to interact with Spark clusters and use the Python and R APIs. Python and R integrations truly open up Spark to a population of Data scientists and Machine learning modelers as the most common languages used by Data scientists in general are Python and R. This is the reason why Spark supports Python integration and also R integration, so as to avoid the costly process of learning a new language of Scala. Another reason is that there might be a lot of existing code written in Python and R, and if we can leverage some of the code, that will improve the productivity of the teams rather than building everything again from scratch.</p>
<p class="mce-root">There is increasing popularity for, and usage of, notebook technologies such as Jupyter and Zeppelin, which make it significantly easier to interact with Spark in general, but particularly very useful in Spark ML where a lot of hypotheses and analysis are expected.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">PySpark</h1>
                
            
            
                
<p class="mce-root">PySpark uses Python-based <kbd class="calibre11">SparkContext</kbd> and Python scripts as tasks and then uses sockets and pipes to executed processes to communicate between Java-based Spark clusters and Python scripts. PySpark also uses <kbd class="calibre11">Py4J</kbd>, which is a popular library integrated within PySpark that lets Python interface dynamically with Java-based RDDs.</p>
<div><br class="calibre23"/>
Python must be installed on all worker nodes running the Spark executors.</div>
<p class="mce-root">The following is how PySpark works by communicating between Java processed and Python scripts:</p>
<div><img class="image-border45" src="img/00286.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">SparkR</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">SparkR</kbd> is an R package that provides a light-weight frontend to use Apache Spark from R. SparkR provides a distributed data frame implementation that supports operations such as selection, filtering, aggregation, and so on. SparkR also supports distributed machine learning using MLlib. SparkR uses R-based <kbd class="calibre11">SparkContext</kbd> and R scripts as tasks and then uses JNI and pipes to executed processes to communicate between Java-based Spark clusters and R scripts.</p>
<div><br class="calibre23"/>
R must be installed on all worker nodes running the Spark executors.</div>
<p class="mce-root">The following is how SparkR works by communicating between Java processed and R scripts:</p>
<div><img class="image-border46" src="img/00289.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="mce-root">We explored the evolution of the Hadoop and MapReduce frameworks and discussed YARN, HDFS concepts, HDFS Reads and Writes, and key features as well as challenges. Then, we discussed the evolution of Apache Spark, why Apache Spark was created in the first place, and the value it can bring to the challenges of big data analytics and processing.</p>
<p class="mce-root">Finally, we also took a peek at the various components in Apache Spark, namely, Spark core, Spark SQL, Spark streaming, Spark GraphX, and Spark ML as well as PySpark and SparkR as a means of integrating Python and R language code with Apache Spark.</p>
<p class="mce-root">Now that we have seen big data analytics, the space and the evolution of the Hadoop Distributed computing platform, and the eventual development of Apache Spark along with a high-level overview of how Apache Spark might solve some of the challenges, we are ready to start learning Spark and how to use it in our use cases.</p>
<p class="mce-root">In the next chapter, we will delve more deeply into Apache Spark and start to look under the hood of how it all works in <a href="part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 6﻿</a>, <em class="calibre8">Start Working with Spark - REPL and RDDs</em>.</p>


            

            
        
    </body></html>