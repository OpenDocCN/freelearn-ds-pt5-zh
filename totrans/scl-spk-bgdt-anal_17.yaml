- en: Time to Go to ClusterLand - Deploying Spark on a Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"I see the moon like a clipped piece of silver. Like gilded bees, the stars
    cluster around her"'
  prefs: []
  type: TYPE_NORMAL
- en: '- Oscar Wilde'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapters, we have seen how to develop practical applications
    using different Spark APIs. However, in this chapter, we will see how Spark works
    in a cluster mode with its underlying architecture. Finally, we will see how to
    deploy a full Spark application on a cluster. In a nutshell, the following topics
    will be cover throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark architecture in a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark ecosystem and cluster management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on a standalone cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on a Mesos cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on YARN cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-based deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Spark on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark architecture in a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop-based **MapReduce** framework has been widely used for the last few years;
    however, it has some issues with I/O, algorithmic complexity, low-latency streaming
    jobs, and fully disk-based operation. Hadoop provides the **Hadoop Distributed
    File System** (**HDFS**) for efficient computing and storing big data cheaply,
    but you can only do the computations with a high-latency batch model or static
    data using the Hadoop-based MapReduce framework. The main big data paradigm that
    Spark has brought for us is the introduction of in-memory computing and caching
    abstraction. This makes Spark ideal for large-scale data processing and enables
    the computing nodes to perform multiple operations by accessing the same input
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Spark's **Resilient Distributed Dataset** (**RDD**) model can do everything
    that the MapReduce paradigm can, and even more. Nevertheless, Spark can perform
    iterative computations on your dataset at scale. This option helps to execute
    machine learning, general purpose data processing, graph analytics, and **Structured
    Query Language** (**SQL**) algorithms much faster with or without depending upon
    Hadoop. Therefore, reviving the Spark ecosystem is a demand at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Enough knowing about Spark's beauties and features. At this point, reviving
    the Spark ecosystem is your demand to know how does Spark work.
  prefs: []
  type: TYPE_NORMAL
- en: Spark ecosystem in brief
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To provide you with more advanced and additional big data processing capabilities,
    your Spark jobs can be running on top of Hadoop-based (aka YARN) or Mesos-based
    clusters. On the other hand, the core APIs in Spark, which is written in Scala,
    enable you to develop your Spark application using several programming languages
    such as Java, Scala, Python, and R. Spark provides several libraries that are
    part of the Spark ecosystems for additional capabilities for general purpose data
    processing and analytics, graph processing, large-scale structured SQL, and **machine
    learning** (**ML**) areas. The Spark ecosystem consists of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00161.jpeg)**Figure 1:** Spark ecosystem (up to Spark 2.1.0)'
  prefs: []
  type: TYPE_IMG
- en: 'The core engine of Spark is written in Scala but supports different languages
    to develop your Spark application, such as R, Java, Python, and Scala. The main
    components/APIs in the Spark core engine are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SparkSQL**: This helps in seamlessly mix SQL queries with Spark programs
    so that you can query structured data inside Spark programs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Spark Streaming**: This is for large-scale streaming application development
    that provides seamless integration of Spark with other streaming data sources
    such as Kafka, Flink, and Twitter.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**SparkMLlib** and **SparKML**: These are for RDD and dataset/DataFrame-based
    machine learning and pipeline creation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**GraphX**: This is for large-scale graph computation and processing to make
    your graph data object fully connected.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**SparkR**: R on Spark helps in basic statistical computations and machine
    learning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we have already stated, it is very much possible to combine these APIs seamlessly
    to develop large-scale machine learning and data analytics applications. Moreover,
    Spark jobs can be submitted and executed through cluster managers such as Hadoop
    YARN, Mesos, and standalone, or in the cloud by accessing data storage and sources
    such as HDFS, Cassandra, HBase, Amazon S3, or even RDBMS. However, to the full
    facility of Spark, we need to deploy our Spark application on a computing cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a distributed and parallel processing system and it also provides
    in-memory computing capabilities. This type of computing paradigm needs an associated
    storage system so that you can deploy your application on top of a big data cluster.
    To make this happen, you will have to use distributed storage systems such as
    HDFS, S3, HBase, and Hive. For moving data, you will be needing other technologies
    such as Sqoop, Kinesis, Twitter, Flume, and Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you can configure a small Hadoop cluster very easily. You only
    need to have a single master and multiple worker nodes. In your Hadoop cluster,
    generally, a master node consists of **NameNodes**, **DataNodes**, **JobTracker**,
    and **TaskTracker**. A worker node, on the other hand, can be configured so that
    it works both as a DataNode and as a TaskTracker.
  prefs: []
  type: TYPE_NORMAL
- en: 'For security reasons, most of the big data cluster might set up behind a network
    firewall so that the complexity caused by the firewall can be overcome or at least
    reduced by the computing nodes. Otherwise, computing nodes cannot be accessed
    from outside of the network, that is, extranet. The following figure shows a simplified
    big data cluster that is commonly used in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00379.jpeg)**Figure 2:** A general architecture for big data processing
    with JVM'
  prefs: []
  type: TYPE_NORMAL
- en: The above picture shows a cluster consisting of five computing nodes. Here each
    node has a dedicated executor JVM, one per CPU core, and the Spark Driver JVM
    sitting outside the cluster. The disk is directly attached to the nodes using
    the **JBOD** (**Just a bunch of disks**) approach. Very large files are partitioned
    over the disks, and a virtual file system such as HDFS makes these chunks available
    as one large virtual file. The following simplified component model shows the
    driver JVM sitting outside the cluster. It talks to the cluster manager (see **Figure
    4**) in order to obtain permission to schedule tasks on the worker nodes because
    the cluster manager keeps track on resource allocation of all processes running
    on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have developed your Spark application using Scala or Java, it means
    that your job is a JVM-based process. For your JVM-based process, you can simply
    configure the Java heap space by specifying the following two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**-Xmx**: **T**his one specifies the upper limit of your Java heap space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**-Xms**: This one is the lower limit of the Java heap space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you have sumitted a Spark job, heap memory need to be allocated for your
    Spark jobs. The following figure provides some insights on how:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00113.jpeg)**Figure 3:** JVM memory management'
  prefs: []
  type: TYPE_NORMAL
- en: As demonstrated in the preceding figure, Spark starts a Spark job with 512 MB
    of JVM heap space. However, for an uninterrupted processing of your Spark job
    and to avoid the **Out of Memory** (**OOM**) error, Spark allows the computing
    nodes to utilize only up to 90% of the heap (that is, ~461 MB), which is eventually
    increased or decreased by controlling the `spark.storage.safetyFraction` parameter
    in Spark environment. To be more realistic, the JVM can be seen as a concatenation
    of **Storage** (60% of the Java heap), 20% of the heap for execution (aka **Shuffle**),
    and the rest of the 20% for other storage.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, Spark is a cluster computing tool that tries to utilize both in-memory
    and disk-based computing and allows users to store some data in memory. In reality,
    Spark utilizes the main memory only for its LRU cache. For uninterrupted caching
    mechanism, a little amount of memory is required to be reserved for the application
    specific data processing. Informally, this is around 60% of the Java heap space
    controlled by the `spark.memory.fraction`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, if you would like to see or calculate how much application specific
    data you can cache in memory in your Spark application, you can just sum up all
    the heap sizes usages by all the executors and multiply it by the `safetyFraction`
    and `spark.memory.fraction`. In practice, 54% of the total heap size (276.48 MB)
    you can allow Spark computing nodes to be used. Now the shuffle memory is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The default values for `spark.shuffle.safetyFraction` and `spark.shuffle.memoryFraction`
    are 80% and 20%, respectively. Therefore, in practical, you can use up to *0.8*0.2
    = 16%* of the JVM heap for the shuffle. Finally, unroll memory is the amount of
    the main memory (in a computing node) that can be utilized by the unroll processes.
    The calculation goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The above is around 11% of the heap *(0.2*0.6*0.9 = 10.8~11%)*, that is, 56.32
    MB of the Java heap space.
  prefs: []
  type: TYPE_NORMAL
- en: More detailed discussion can be found at [http://spark.apache.org/docs/latest/configuration.html.](http://spark.apache.org/docs/latest/configuration.html)
  prefs: []
  type: TYPE_NORMAL
- en: As we will see later, there exist a variety of different cluster managers, some
    of them also capable of managing other Hadoop workloads or even non-Hadoop applications
    in parallel to the Spark executors. Note that the executor and driver have bidirectional
    communication all the time, so network wise they should also be sitting close
    together.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00167.jpeg)**Figure 4:** Driver, master, and worker architecture in
    Spark for cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Spark uses the driver (aka the driver program), master, and worker architecture
    (aka host, slave, or computing nodes). The driver program (or machine) talks to
    a single coordinator called master node. The master node actually manages all
    the workers (aka the slave or computing nodes) in which several executors run
    in parallel in a cluster. It is to be noted that the master is also a computing
    node having large memory, storage, OS, and underlying computing resources. Conceptually,
    this architecture can be shown in **Figure 4**. More details will be discussed
    later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: In a real cluster mode, the cluster manager (aka the resource manager) manages
    all the resources of computing nodes in a cluster. Generally, firewalls, while
    adding security to the cluster, also increase the complexity. Ports between system
    components need to be opened up so that they can talk to each other. For instance,
    Zookeeper is used by many components for configuration. Apache Kafka, which is
    a subscribing messaging system, uses Zookeeper for configuring its topics, groups,
    consumers, and producers. So, client ports to Zookeeper, potentially across the
    firewall, need to be open.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the allocation of systems to cluster nodes needs to be considered.
    For instance, if Apache Spark uses Flume or Kafka, then in-memory channels will
    be used. Apache Spark should not be competing with other Apache components for
    memory usage. Depending upon your data flows and memory usage, it might be necessary
    to have the Spark, Hadoop, Zookeeper, Flume, and other tools on distinct cluster
    nodes. Alternatively, resource managers such as YARN, Mesos, or Docker, for instance,
    can be used to tackle this problem as well. In standard Hadoop environments, most
    likely YARN is there anyway.
  prefs: []
  type: TYPE_NORMAL
- en: The computing nodes that act as workers, or Spark master, will need greater
    resources than the cluster processing nodes within the firewall. When many Hadoop
    ecosystem components are deployed on the cluster, all of them will need extra
    memory on the master server. You should monitor worker nodes for resource usage
    and adjust in terms of resources and/or application location as necessary. YARN,
    for instance, is taking care of this.
  prefs: []
  type: TYPE_NORMAL
- en: This section has briefly set the scene for the big data cluster in terms of
    Apache Spark, Hadoop, and other tools. However, how might the Apache Spark cluster
    itself, within the big data cluster, be configured? For instance, it is possible
    to have many types of Spark cluster manager. The next section will examine this
    and describe each type of Apache Spark cluster manager.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark context can be defined through the Spark configuration object (that
    is, `SparkConf`) and a Spark URL. First, the purpose of the Spark context is to
    connect the Spark cluster manager in which your Spark jobs will be running. The
    cluster or resource manager then allocates the required resources across the computing
    nodes for your application. The second task of the cluster manager is to allocate
    the executors across the cluster worker nodes so that your Spark jobs get executed.
    Third, the resource manager also copies the driver program (aka the application
    JAR file, R code, or Python script) to the computing nodes. Finally, the computing
    tasks are assigned to the computing nodes by the resource manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following subsections describe the possible Apache Spark cluster manager
    options available with the current Spark version (that is, Spark 2.1.0 during
    the writing of this book). To know about the resource management by a resource
    manager (aka the cluster manager), the following shows how YARN manages all its
    underlying computing resources. However, this is same for any cluster manager
    (for example, Mesos or YARN) you use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00380.jpeg)**Figure 5:** Resource management using YARN'
  prefs: []
  type: TYPE_NORMAL
- en: A detailed discussion can be found at [http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types](http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types).
  prefs: []
  type: TYPE_NORMAL
- en: Pseudocluster mode (aka Spark local)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you already know, Spark jobs can be run in local mode. This is sometimes
    called pseudocluster mode of execution. This is also nondistributed and single
    JVM-based deployment mode where Spark issues all the execution components, for
    example, driver program, executor, LocalSchedulerBackend, and master, into your
    single JVM. This is the only mode where the driver itself is used as an executor.
    The following figure shows the high-level architecture of the local mode for submitting
    your Spark jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00214.jpeg)**Figure 6:** High-level architecture of local mode for
    Spark jobs (source: [https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-local.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-local.html))'
  prefs: []
  type: TYPE_IMG
- en: Is it too surprising? No, I guess, since you can achieve some short of parallelism
    as well, where the default parallelism is the number of threads (aka Core used)
    as specified in the master URL, that is, local [4] for 4 cores/threads and `local
    [*]` for all the available threads. We will discuss this topic later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Standalone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By specifying a Spark configuration local URL, it is possible to have the application
    run locally. By specifying *local[n]*, it is possible to have Spark use *n* threads
    to run the application locally. This is a useful development and test option because
    you can also test some sort of parallelization scenarios but keep all log files
    on a single machine. The standalone mode uses a basic cluster manager that is
    supplied with Apache Spark. The spark master URL will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, `<hostname>` is the name of the host on which the Spark master is running.
    I have specified 7077 as the port, which is the default value, but it is configurable.
    This simple cluster manager currently only supports **FIFO** (**first in first
    out**) scheduling. You can contrive to allow concurrent application scheduling
    by setting the resource configuration options for each application. For example,
    `spark.core.max` is used to share the processor cores between applications. A
    more detail discussion will be carried out later this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Apache YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the Spark master value is set as YARN-cluster, then the application can be
    submitted to the cluster and then terminated. The cluster will take care of allocating
    resources and running tasks. However, if the application master is submitted as
    YARN-client, then the application stays alive during the life cycle of processing
    and requests resources from YARN. These are applicable at a larger scale, when
    integrating with Hadoop YARN. A step-by-step guideline will be provided later
    in this chapter to configure a single-node YARN cluster for launching your Spark
    jobs needing minimal resources.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Mesos is an open source system for resource sharing across a cluster.
    It allows multiple frameworks to share a cluster by managing and scheduling resources.
    It is a cluster manager, which provides isolation using Linux containers, allowing
    multiple systems such as Hadoop, Spark, Kafka, Storm, and more to share a cluster
    safely. This is a master-slave based system using Zookeeper for configuration
    management. This way you can scalae up your Spark jobs to thousands of nodes.
    For a single master node Mesos cluster, the Spark master URL will be in the following
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The consequence of a Spark job submission by specifically using Mesos can be
    shown visually in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00075.jpeg)**Figure 7:** Mesos in action (image source: [https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-architecture.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-architecture.html))'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding figure, where `<hostname>` is the hostname of the Mesos master
    server, and the port is defined as 5050, which is the default Mesos master port
    (this is configurable). If there are multiple Mesos master servers in a large-scale
    high availability Mesos cluster, then the Spark master URL would look like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So, the election of the Mesos master server will be controlled by Zookeeper.
    The `<hostname>` will be the name of a host in the Zookeeper quorum. Also, the
    port number 2181 is the default master port for Zookeeper.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-based deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three different abstraction levels in the cloud computing paradigm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Infrastructure as a Service** (aka **IaaS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Platform as a Service** (aka **PaaS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software as a Service** (aka **SaaS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IaaS provides the computing infrastructure through empty virtual machines for
    your software running as SaaS. This is also true for the Apache Spark on OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of OpenStack is that it can be used among multiple different cloud
    providers, since it is an open standard and is also based on open source. You
    even can use OpenStack in a local data center, and transparently and dynamically
    move workloads between local, dedicated, and public cloud data centers.
  prefs: []
  type: TYPE_NORMAL
- en: PaaS, in contrast, takes away from you the burden of installing and operating
    an Apache Spark cluster because this is provided as a Service. In other words,
    you can think it as a layer like what your OS does.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, you can even Dockerize your Spark application and deploy on the cloud
    platform independent manner. However, there is an ongoing discussion whether Docker
    is IaaS or PaaS, but in our opinion, this is just a form of a lightweight preinstalled
    virtual machine, so more on the IaaS.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, SaaS is an application layer provided and managed by cloud computing
    paradigm. To be frank, you won't see or have to worry about the first two layers
    (IaaS and PaaS).
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud, Amazon AWS, Digital Ocean, and Microsoft Azure are good examples
    of cloud computing services that provide these three layers as services. We will
    show an example of how to deploy your Spark cluster on top of Cloud using Amazon
    AWS later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Spark application on a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss how to deploy Spark jobs on a computing cluster.
    We will see how to deploy clusters in three deploy modes: standalone, YARN, and
    Mesos. The following figure summarizes terms that are needed to refer to cluster
    concepts in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00258.jpeg)**Figure 8:** Terms that are needed to refer to cluster
    concepts (source: http://spark.apache.org/docs/latest/cluster-overview.html#glossary)'
  prefs: []
  type: TYPE_IMG
- en: However, before diving onto deeper, we need to know how to submit a Spark job
    in general.
  prefs: []
  type: TYPE_NORMAL
- en: Submitting Spark jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once a Spark application is bundled as either a jar file (written in Scala
    or Java) or a Python file, it can be submitted using the Spark-submit script located
    under the bin directory in Spark distribution (aka `$SPARK_HOME/bin`). According
    to the API documentation provided in Spark website ([http://spark.apache.org/docs/latest/submitting-applications.html](http://spark.apache.org/docs/latest/submitting-applications.html)),
    the script takes care of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the classpath of `JAVA_HOME`, `SCALA_HOME` with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the all the dependencies required to execute the jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing different cluster managers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, deploying models that Spark supports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a nutshell, Spark job submission syntax is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `[options]` can be: `--conf <configuration_parameters> --class <main-class>
    --master <master-url> --deploy-mode <deploy-mode> ... # other options`'
  prefs: []
  type: TYPE_NORMAL
- en: '`<main-class>` is the name of the main class name. This is practically the
    entry point for our Spark application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--conf` signifies all the used Spark parameters and configuration property.
    The format of a configuration property is a key=value format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<master-url>` specifies the master URL for the cluster (for example, `spark://HOST_NAME:PORT`*)*
    for connecting to the master of the Spark standalone cluster, `local` for running
    your Spark jobs locally. By default, it allows you using only one worker thread
    with no parallelism. The `local [k]` can be used for running your Spark job locally
    with *K* worker threads. It is to be noted that K is the number of cores on your
    machine. Finally, if you specify the master with `local[*]` for running Spark
    job locally, you are giving the permission to the `spark-submit` script to utilize
    all the worker threads (logical cores) on your machine have. Finally, you can
    specify the master as `mesos://IP_ADDRESS:PORT` for connecting to the available
    Mesos cluster. Alternatively, you could specify using `yarn` to run your Spark
    jobs on a YARN-based cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For other options on Master URL, please refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00183.jpeg)**Figure 9:** Details about the master URLs supported by
    Spark\'
  prefs: []
  type: TYPE_NORMAL
- en: '`<deploy-mode>` you have to specify this if you want to deploy your driver
    on the worker nodes (cluster) or locally as an external client (client). Four
    (4) modes are supported: local, standalone, YARN, and Mesos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<app-jar>` is the JAR file you build with with dependencies. Just pass the
    JAR file while submitting your jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<python-file>` is the application main source code written using Python. Just
    pass the `.py` file while submitting your jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[app-arguments]` could be input or output argument specified by an application
    developer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While submitting the Spark jobs using the spark-submit script, you can specify
    the main jar of the Spark application (and other related JARS included) using
    the `--jars` option. All the JARS will then be transferred to the cluster. URLs
    supplied after `--jars` must be separated by commas.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you specify the jar using the URLs, it is a good practice to separate
    the JARS using commas after `--jars`. Spark uses the following URL scheme to allow
    different strategies for disseminating JARS:'
  prefs: []
  type: TYPE_NORMAL
- en: '**file:** Specifies the absolute paths and `file:/`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hdfs****:**, **http****:**, **https:**, **ftp****:** JARS or any other files
    will be pull-down from the URLs/URIs you specified as expected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**local:** A URI starting with `local:/` can be used to point local jar files
    on each computing node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is to be noted that dependent JARs, R codes, Python scripts, or any other
    associated data files need to be copied or replicated to the working directory
    for each SparkContext on the computing nodes. This sometimes creates a significant
    overhead and needs a pretty large amount of disk space. The disk usages increase
    over time. Therefore, at a certain period of time, unused data objects or associated
    code files need to be cleaned up. This is, however, quite easy with YARN. YARN
    handles the cleanup periodically and can be handled automatically. For example,
    with the Spark standalone mode, automatic cleanup can be configured with the `spark.worker.cleanup.appDataTtl`
    property while submitting the Spark jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Computationally, the Spark is designed such that during the job submission (using
    `spark-submit` script), default Spark config values can be loaded and propagate
    to Spark applications from a property file. Master node will read the specified
    options from the configuration file named `spark-default.conf`. The exact path
    is `SPARK_HOME/conf/spark-defaults.conf` in your Spark distribution directory.
    However, if you specify all the parameters in the command line, this will get
    higher priority and will be used accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Running Spark jobs locally and in standalone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The examples are shown [Chapter 13](part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c),
    *My Name is Bayes, Naive Bayes*, and can be made scalable for even larger dataset
    to solve different purposes. You can package all these three clustering algorithms
    with all the required dependencies and submit them as Spark job in the cluster.
    If you don't know how to make a package and create jar files out of the Scala
    class, you can bundle your application with all the dependencies using SBT or
    Maven.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Spark documentation at [http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management](http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management),
    both the SBT and Maven have assembly plugins for packaging your Spark application
    as a fat jar. If your application is already bundled with all the dependencies,
    use the following lines of code to submit your Spark job of k-means clustering,
    for example (use similar syntax for other classes), for Saratoga NY Homes dataset.
    For submitting and running a Spark job locally, run the following command on 8
    cores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, `com.chapter15.KMeansDemo` is the main class file written
    in Scala. Local [8] is the master URL utilizing eight cores of your machine. `KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar`
    is the application JAR file we just generated by Maven project; `Saratoga_NY_Homes.txt`
    is the input text file for the Saratoga NY Homes dataset. If the application executed
    successfully, you will find the message including the output in the following
    figure (abridged):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00015.gif)**Figure 10:** Spark job output on terminal [local mode]'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's dive into the cluster setup in standalone mode. To install Spark
    standalone mode, you should place prebuilt versions of Spark with each release
    on each node on the cluster. Alternatively, you can build it yourself and use
    it according to the instruction at [http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure the environment as a Spark standalone mode, you will have to provide
    the prebuilt versions of Spark with the desired version to each node on the cluster.
    Alternatively, you can build it yourself and use it according to the instruction
    at [http://spark.apache.org/docs/latest/building-spark.html](http://spark.apache.org/docs/latest/building-spark.html).
    Now we will see how to start a standalone cluster manually. You can start a standalone
    master by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once started, you should observe the following logs on terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You should be able to access Spark web UI at `http://localhost:8080` by default.
    Observe the following UI as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00321.jpeg)**Figure 11:** Spark master as standalone'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can change the port number by editing the following parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `SPARK_HOME/sbin/start-master.sh`, just change the port number and then
    apply the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can restart the Spark master to effect the preceding change.
    However, you will have to make a similar change in the `SPARK_HOME/sbin/start-slave.sh`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see here, there are no active workers associated with the master
    node. Now to create a slave node (aka a worker node or computing node), create
    workers and connect them to the master using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon successful completion of the preceding command, you should observe the
    following logs on terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have one of your worker nodes started, you can look at its status
    on the Spark web UI at `http://localhost:8081`. However, if you start another
    worker node, you can access it''s status in the consecutive ports (that is, 8082,
    8083, and so on). You should also see the new node listed there, along with its
    number of CPUs and memory, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00055.jpeg)**Figure 12:** Spark worker as standalone'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you refresh `http://localhost:8080`, you should see that one worker
    node that is associated with your master node has been added, as shown in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00104.jpeg)**Figure 13:** Spark master has now one worker node as standalone'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, as shown in the following figure, these are all the configuration
    options that can be passed to the master and worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00195.jpeg)**Figure 14:** Configuration options that can be passed
    to the master and worker nodes (source: [http://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually](http://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually))'
  prefs: []
  type: TYPE_IMG
- en: 'Now one of your master node and a worker node are reading and active. Finally,
    you can submit the same Spark job as standalone rather than local mode using the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Once the job started, access Spark web UI at `http://localhost:80810` for master
    and `http://localhost:8081` for the worker, you can see the progress of your job
    as discussed in [Chapter 14](part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c),
    *Time to Put Some Order - Cluster Your Data with Spark MLlib*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize this section, we would like to redirect you to the following image
    (that is, **Figure 15**) that shows the usages of the following shell scripts
    for launching or stopping your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00295.jpeg)**Figure 15:** The usages of the shell scripts for launching
    or stopping your cluster\'
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As already discussed, the Apache Hadoop YARN has to main components: a scheduler
    and an applications manager, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00250.jpeg)**Figure 16:** Apache Hadoop YARN architecture (blue: system
    components; yellow and pink: two applications running)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that using the scheduler and the applications manager, the following two
    deploy modes can be configured to launch your Spark jobs on a YARN-based cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster mode**: In the cluster mode, the Spark driver works within the master
    process of an application managed by YARN''s application manager. Even the client
    can be terminated or disconnected away when the application has been initiated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Client mode**: In this mode, the Spark driver runs inside the client process.
    After that, Spark master is used only for requesting computing resources for the
    computing nodes from YARN (YARN resource manager).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the Spark standalone and Mesos modes, the URL of the master (that is, address)
    needs to be specified in the `--master` parameter. However, in the YARN mode,
    the address of the resource manager is read from the Hadoop configuration file
    in your Hadoop setting. Consequently, the `--master` parameter is `yarn`. Before
    submitting our Spark jobs, we, however, you need to set up your YARN cluster.
    The next subsection shows a step-by-step of doing so.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a single-node YARN cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this subsection, we will see how to set up your YARN cluster before running
    your Spark jobs on YARN cluster. There are several steps so keep patience and
    do the following step-by-step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Downloading Apache Hadoop'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Download the latest distribution from the Hadoop website ([http://hadoop.apache.org/](http://hadoop.apache.org/)).
    I used the latest stable version 2.7.3 on Ubuntu 14.04 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create and extract the package in `/opt/yarn` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Setting the JAVA_HOME'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the section of Java setup in [Chapter 1](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduction to Scala*, for details and apply the same changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Creating users and groups'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following `yarn`, `hdfs`, and `mapred` user accounts for `hadoop` group
    can be created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: Creating data and log directories'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run your Spark jobs using Hadoop, it needs to have the data and the log
    directories with various permissions. You can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you need to create the log directory where YARN is installed and then set
    the owner and group as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: Configuring core-site.xml'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two properties (that is, `fs.default.name` and `hadoop.http.staticuser.user`)
    need to be set to the `etc/hadoop/core-site.xml` file. Just copy the following
    lines of codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 6: Configuring hdfs-site.xml'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Five properties (that is, `dfs.replication` , `dfs.namenode.name.dir` , `fs.checkpoint.dir`
    , `fs.checkpoint.edits.dir`, and `dfs.datanode.data.dir`) need to be set to the
    `etc/hadoop/ hdfs-site.xml` file. Just copy the following lines of codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 7: Configuring mapred-site.xml'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One property (that is, `mapreduce.framework.name`) needs to be set to the `etc/hadoop/
    mapred-site.xml` file. First, copy and replace the original template file to the
    `mapred-site.xml` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, just copy the following lines of codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 8: Configuring yarn-site.xml'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two properties (that is, `yarn.nodemanager.aux-services` and `yarn.nodemanager.aux-services.mapreduce.shuffle.class`)
    need to be set to the `etc/hadoop/yarn-site.xml` file. Just copy the following
    lines of codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 9: Setting Java heap space'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run your Spark job on Hadoop-based YARN cluster, you need to specify enough
    heap space for the JVM. You need to edit the `etc/hadoop/hadoop-env.sh` file.
    Enable the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you also need to edit the `mapred-env.sh` file with the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, make sure that you have edited `yarn-env.sh` to make the changes permanent
    for Hadoop YARN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 10: Formatting HDFS'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to start your HDFS NameNode, Hadoop needs to initialize the directory
    where it will store or persist its data for tracking all the metadata for your
    file system. The formatting will destroy everything and sets up a new file system.
    Then it uses the values of the parameters set on `dfs.namenode.name.dir` in `etc/hadoop/hdfs-site.xml`.
    For doing the format, at first, move to the `bin` directory and execute the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If the preceding command executed successfully, you should see the following
    on your Ubuntu terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 11: Starting the HDFS'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the `bin` directory in step 10, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon successful execution of the preceding command, you should see the following
    on your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To start the `secondarynamenode` and the `datanode`, you should use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You should receive the following message on your terminal if the preceding
    commands succeed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then use the following command to start the data node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You should receive the following message on your terminal if the preceding
    commands succeed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now make sure that, you check all the services related to those nodes are running
    use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'You should observe something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 12: Starting YARN'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For working with YARN, one `resourcemanager` and one node manager have to be
    started as the user yarn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'You should receive the following message on your terminal if the preceding
    commands succeed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Tehn execute the following command to start the node manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You should receive the following message on your terminal if the preceding
    commands succeed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to make sure that every services in those nodes are running, you
    should use the `$jsp` command. Moreover, if you want to stop your resource manager
    or `nodemanager`, use the following `g` commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 13: Verifying on the web UI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Access `http://localhost:50070` to view the status of the NameNode, and access
    `http://localhost:8088` for the resource manager on your browser.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding steps show how to configure a Hadoop-based YARN cluster with only
    a few nodes. However, if you want to configure your Hadoop-based YARN clusters
    ranging from a few nodes to extremely large clusters with thousands of nodes,
    refer to [https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html).
  prefs: []
  type: TYPE_NORMAL
- en: Submitting Spark jobs on YARN cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that our YARN cluster with the minimum requirement (for executing a small
    Spark job to be frank) is ready, to launch a Spark application in a cluster mode
    of YARN, you can use the following submit command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'For running our `KMeansDemo`, it should be done like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The preceding `submit` command starts a YARN cluster mode with the default application
    master. Then `KMeansDemo` will be running as a child thread of the application
    master. For the status updates and for displaying them in the console, the client
    will periodically poll the application master. When your application (that is,
    `KMeansDemo` in our case) has finished its execution, the client will be exited.
  prefs: []
  type: TYPE_NORMAL
- en: "Upon submission of your job, you might want to see the progress using the Spark\
    \ web UI or Spark history server. Moreover, you should refer to [Chapter 18](part0550.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c),\uFEFF\
    \ *Testing and Debugging Spark*) to know how to analyze driver and executor logs."
  prefs: []
  type: TYPE_NORMAL
- en: 'To launch a Spark application in a client mode, you should use the earlier
    command, except that you will have to replace the cluster with the client. For
    those who want to work with Spark shell, use the following in client mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Advance job submissions in a YARN cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you opt for the more advanced way of submitting Spark jobs to be computed
    in your YARN cluster, you can specify additional parameters. For example, if you
    want to enable the dynamic resource allocation, make the `spark.dynamicAllocation.enabled`
    parameter true. However, to do so, you also need to specify `minExecutors`, `maxExecutors`,
    and `initialExecutors` as explained in the following. On the other hand, if you
    want to enable the shuffling service, set `spark.shuffle.service.enabled` as `true`.
    Finally, you could also try specifying how many executor instances will be running
    using the `spark.executor.instances` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to make the preceding discussion more concrete, you can refer to the following
    submission command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: However, the consequence of the preceding job submission script is complex and
    sometimes nondeterministic. From my previous experience, if you increase the number
    of partitions from code and the number of executors, then the app will finish
    faster, which is okay. But if you increase only the executor-cores, the finish
    time is the same. However, you might expect the time to be lower than initial
    time. Second, if you launch the preceding code twice, you might expect both jobs
    to finish in say 60 seconds, but this also might not happen. Often, both jobs
    might finish after 120 seconds instead. This is a bit weird, isn't it? However,
    here goes the explanation that would help you understand this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have 16 cores and 8 GB memory on your machine. Now, if you use four
    executors with one core each, what will happen? Well, when you use an executor,
    Spark reserves it from YARN and YARN allocates the number of cores (for example,
    one in our case) and the memory required. The memory is required more than you
    asked for actually for faster processing. If you ask for 1 GB, it will, in fact,
    allocate almost 1.5 GB with 500 MB overhead. In addition, it will probably allocate
    an executor for the driver with probably 1024 MB memory usage (that is, 1 GB).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it doesn't matter how much memory your Spark job wants but how much
    it reserves. In the preceding example, it will not take 50 MB of the test but
    around 1.5 GB (including the overhead) per executor. We will discuss how to configure
    Spark cluster on AWS later this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Mesos master usually replaces the Spark master as the cluster manager (aka
    the resource manager) when using Mesos. Now, when a driver program creates a Spark
    job and starts assigning the related tasks for scheduling, Mesos determines which
    computing nodes handle which tasks. We assume that you have already configured
    and installed Mesos on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, following links may be helpful to install Mesos on your machine.
    [http://blog.madhukaraphatak.com/mesos-single-node-setup-ubuntu/,](http://blog.madhukaraphatak.com/mesos-single-node-setup-ubuntu/)
    [https://mesos.apache.org/gettingstarted/.](https://mesos.apache.org/gettingstarted/)
  prefs: []
  type: TYPE_NORMAL
- en: Depending upon hardware configuration, it takes a while. On my machine (Ubuntu
    14.04 64-bit, with Core i7 and 32 GB of RAM), it took 1 hour to complete the build.
  prefs: []
  type: TYPE_NORMAL
- en: To submit and compute your Spark jobs by utilizing the Mesos cluster mode, make
    sure to check that the Spark binary packages are available in a place accessible
    by Mesos. Additionally, make sure that your Spark driver program can be configured
    in such a way that it is automatically connected to Mesos. The second option is
    installing Spark in the same location as the Mesos slave nodes. Then, you will
    have to configure the `spark.mesos.executor.home` parameter to point the location
    of Spark distribution. It is to be noted that the default location that could
    point is the `SPARK_HOME`.
  prefs: []
  type: TYPE_NORMAL
- en: When Mesos executes a Spark job on a Mesos worker node (aka computing node)
    for the first time, the Spark binary packages have to be available on that worker
    node. This will ensure that the Spark Mesos executor is running in the backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark binary packages can be hosted to Hadoop to make them accessible:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Having the URIs/URLs (including HTTP) via `http://`,
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Using the Amazon S3 via `s3n://`,
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Using the HDFS via `hdfs://`.
  prefs: []
  type: TYPE_NORMAL
- en: If you set the `HADOOP_CONF_DIR` environment variable, the parameter is usually
    set as `hdfs://...`; otherwise `file://`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can specify the Master URLs for Mesos as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mesos://host:5050` for a single-master Mesos cluster, and `mesos://zk://host1:2181,host2:2181,host3:2181/mesos`
    for a multimaster Mesos cluster controlled by the ZooKeeper.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a more detailed discussion, please refer to [http://spark.apache.org/docs/latest/running-on-mesos.html](http://spark.apache.org/docs/latest/running-on-mesos.html).
  prefs: []
  type: TYPE_NORMAL
- en: Client mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this mode, the Mesos framework works in such a way that the Spark job is
    launched on the client machine directly. It then waits for the computed results,
    also called the driver output. To interact properly with the Mesos, the driver,
    however, expects that there are some application-specific configurations specified
    in `SPARK_HOME/conf/spark-env.sh` . To make this happened, modify the `spark-env.sh.template`
    file at `$SPARK_HOME /conf`, and before using this client mode, in your `spark-env.sh`,
    set the following environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This path is typically `/usr/local /lib/libmesos.so` on Ubuntu. On the other
    hand, on macOS X, the same library is called `libmesos.dylib` instead of `libmesos.so`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when submitting and starting a Spark application to be executed on the
    cluster, you will have to pass the Mesos `:// HOST:PORT` as the master URL. This
    is usually done while creating the `SparkContext` in your Spark application development
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The second option of doing so is using the `spark-submit` script and configure
    `spark.executor.uri` in the `SPARK_HOME/conf/spark-defaults.conf` file. When running
    a shell, the `spark.executor.uri` parameter is inherited from `SPARK_EXECUTOR_URI`,
    so it does not need to be redundantly passed in as a system property. Just use
    the following command to access the client mode from your Spark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Cluster mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark on Mesos also supports cluster mode. If the driver is already launched
    Spark job (on a cluster) and the computation is also finished, client can access
    the result (of driver) from the Mesos Web UI. If you have started `MesosClusterDispatcher`
    in your cluster through the `SPARK_HOME/sbin/start-mesos-dispatcher.sh` script,
    you can use the cluster mode.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the condition is that you have to pass the Mesos master URL (for example,
    `mesos://host:5050`) while creating the `SparkContext` in your Spark application.
    Starting the Mesos in the cluster mode also starts the `MesosClusterDispatcher`
    as a daemon running on your host machine.
  prefs: []
  type: TYPE_NORMAL
- en: To gain a more flexible and advanced execution of your Spark jobs, you can also
    use the **Marathon**. The advantageous thing about using the Marathon is that
    you can run the `MesosClusterDispatcher` with Marathon. If you do that, make sure
    that the `MesosClusterDispatcher` is running in the foreground.
  prefs: []
  type: TYPE_NORMAL
- en: '**Marathon** is a framework for Mesos that is designed to launch long-running
    applications, and in Mesosphere, it serves as a replacement for a traditional
    init system. It has many features that simplify running applications in a clustered
    environment, such as high-availability, node constraints, application health checks,
    an API for scriptability and service discovery, and an easy-to-use web user interface.
    It adds its scaling and self-healing capabilities to the Mesosphere feature set.
    Marathon can be used to start other Mesos frameworks, and it can also launch any
    process that can be started in the regular shell. As it is designed for long-running
    applications, it will ensure that applications it has launched will continue running,
    even if the slave node(s) they are running on fails. For more information on using
    Marathon with the Mesosphere, refer to the GitHub page at [https://github.com/mesosphere/marathon](https://github.com/mesosphere/marathon).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To be more specific, from the client, you can submit a Spark job to your Mesos
    cluster by using the `spark-submit` script and specifying the master URL to the
    URL of the `MesosClusterDispatcher` (for example, `mesos://dispatcher:7077`).
    It goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'You can view driver statuses on the Spark cluster web UI. For example, use
    the following job submission command for doing so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that JARS or Python files that are passed to Spark-submit should be URIs
    reachable by Mesos slaves, as the Spark driver doesn''t automatically upload local
    jars. Finally, Spark can run over Mesos in two modes: *coarse-grained* (default)
    and *fine-grained* (deprecated). For more details, please refer to [http://spark.apache.org/docs/latest/running-on-mesos.html](http://spark.apache.org/docs/latest/running-on-mesos.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a cluster mode, the Spark driver runs on a different machine, that is, driver,
    master, and computing nodes are different machines. Therefore, if you try adding
    JARS using `SparkContext.addJar`, this will not work. To avoid this issue, make
    sure that the jar files on the client are also available to `SparkContext.addJar`,
    using the `--jars` option in the launch command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Deploying on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we illustrated how to submit spark jobs in local, standalone,
    or deploy mode (YARN and Mesos). Here, we are going to show how to run spark application
    in real cluster mode on AWS EC2\. To make our application running on spark cluster
    mode and for better scalability, we consider the **Amazon Elastic Compute Cloud**
    (**EC2**) services as IaaS or **Platform as a Service** (**PaaS**). For pricing
    and related information, please refer to [https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Key pair and access key configuration'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We assume that you have EC2 accounts already created. Well! The first requirement
    is to create EC2 key pairs and AWS access keys. The EC2 key pair is the private
    key that you need when you will make a secure connection through SSH to your EC2
    server or instances. For making the key, you have to go through AWS console at
    [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair).
    Please refer to the following figure that shows the key-pair creation page for
    an EC2 account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00237.jpeg)**Figure 17:** AWS key-pair generation window'
  prefs: []
  type: TYPE_NORMAL
- en: 'Name it `aws_key_pair.pem` once you have downloaded it and save it on your
    local machine. Then ensure the permission by executing the following command (you
    should store this file in a secure location for security purpose, say `/usr/local/key`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Now what you need are the AWS access keys and the credentials of your account.
    These are needed if you want to submit your Spark job to computing nodes from
    your local machine using the `spark-ec2` script. To generate and download the
    keys, login to your AWS IAM services at [http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey](http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey).
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon the completion of download (that is, `/usr/local/key`), you need to set
    two environment variables in your local machine. Just execute following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Configuring Spark cluster on EC2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to Spark 1.6.3 release, Spark distribution (that is, `/SPARK_HOME/ec2`) provides
    a shell script called **spark-ec2** for launching Spark Cluster in EC2 instances
    from your local machine. This eventually helps in launching, managing, and shutting
    down the Spark Cluster that you will be using on AWS. However, since Spark 2.x,
    the same script was moved to AMPLab so that it would be easier to fix bugs and
    maintain the script itself separately.
  prefs: []
  type: TYPE_NORMAL
- en: The script can be accessed and used from the GitHub repo at [https://github.com/amplab/spark-ec2](https://github.com/amplab/spark-ec2).
  prefs: []
  type: TYPE_NORMAL
- en: Starting and using a cluster on AWS will cost money. Therefore, it is always
    a good practice to stop or destroy a cluster when the computation is done. Otherwise,
    it will incur additional cost to you. For more about AWS pricing, please refer
    to [https://aws.amazon.com/ec2/pricing/](https://aws.amazon.com/ec2/pricing/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You also need to create an IAM Instance profile for your Amazon EC2 instances
    (Console). For details, refer to [http://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-create-iam-instance-profile.html](https://github.com/amplab/spark-ec2).
    For simplicity, let''s download the script and place it under a directory `ec2`
    in Spark home (`$SPARK_HOME/ec2`). Once you execute the following command to launch
    a new instance, it sets up Spark, HDFS, and other dependencies on the cluster
    automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We believe that these parameters are self-explanatory. Alternatively, for more
    details, please refer to [https://github.com/amplab/spark-ec2#readme](https://github.com/amplab/spark-ec2#readme).
  prefs: []
  type: TYPE_NORMAL
- en: '**If you already have a Hadoop cluster and want to deploy spark on it:** If
    you are using Hadoop-YARN (or even Apache Mesos), running a spark job is relatively
    easier. Even if you don''t use either, Spark can run in standalone mode. Spark
    runs a driver program, which, in turn, invokes spark executors. This means that
    you need to tell Spark the nodes where you want your spark daemons to run (in
    terms of master/slave). In your `spark/conf` directory, you can see a file `slaves`.
    Update it to mention all the machines you want to use. You can set up spark from
    source or use a binary from the website. You always should use the **Fully Qualified
    Domain Names** (**FQDN**) for all your nodes, and make sure that each of those
    machines are passwordless SSH accessible from your master node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that you have already created and configured an instance profile. Now
    you are ready to launch the EC2 cluster. For our case, it would be something like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows your Spark home on AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00063.jpeg)**Figure 18:** Cluster home on AWS'
  prefs: []
  type: TYPE_NORMAL
- en: After the successful completion, spark cluster will be instantiated with two
    workers (slaves) nodes on your EC2 account. This task, however, sometimes might
    take half an hour approximately, depending on your Internet speed and hardware
    configuration. Therefore, you'd love to have a coffee break. Upon successful competition
    of the cluster setup, you will get the URL of the Spark cluster on the terminal.
    To make sure if the cluster is really running, check `https://<master-hostname>:8080`
    on your browser, where the `master-hostname` is the URL you receive on the terminal.
    If every think was okay, you will find your cluster running; see cluster home
    in **Figure 18**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Running Spark jobs on the AWS cluster'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now you master and worker nodes are active and running. This means that you
    can submit your Spark job to them for computing. However, before that, you need
    to log in the remote nodes using SSH. For doing so, execute the following command
    to SSH remote Spark cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'For our case, it should be something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now copy your application, that is, JAR file (or python/R script) to the remote
    instance (that is, `ec2-52-48-119-121.eu-west-1.compute.amazonaws.com` in our
    case) by executing the following command (in a new terminal):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you need to copy your data (`/usr/local/data/Saratoga_NY_Homes.txt`, in
    our case) to the same remote instance by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Note that if you have already configured HDFS on your remote machine and put
    your code/data file, you don't need to copy the JAR and data files to the slaves;
    the master will do it automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well done! You are almost done! Now, finally, you will have to submit your
    Spark job to be computed by the slaves or worker nodes. To do so, just execute
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Place your input file under `file:///input.txt` if HDFS is not set on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have already put your data on HDFS, you should issue the submit command
    something like following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Upon successful completion of the job computation, you are supposed to see the
    status and related statistics of your job at port 8080.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Pausing, restarting, and terminating the Spark cluster'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When your computation is done, it is better to stop your cluster to avoid additional
    cost. To stop your clusters, execute the following commands from your local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'For our case, it would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'To restart the cluster later on, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'For our case, it will be something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to terminate your Spark cluster on AWS we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, it would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Spot instances are great for reducing AWS costs, sometimes cutting instance
    costs by a whole order of magnitude. A step-by-step guideline using this facility
    can be accessed at [http://blog.insightdatalabs.com/spark-cluster-step-by-step/](http://blog.insightdatalabs.com/spark-cluster-step-by-step/).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it's difficult to move large dataset, say 1 TB of raw data file.
    In that case, and if you want your application to scale up even more for large-scale
    datasets, the fastest way of doing so is loading them from Amazon S3 or EBS device
    to HDFS on your nodes and specifying the data file path using `hdfs://`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data files or any other files (data, jars, scripts, and so on) can be hosted
    on HDFS to make them highly accessible:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Having the URIs/URLs (including HTTP) via `http://`
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Using the Amazon S3 via `s3n://`
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Using the HDFS via `hdfs://`
  prefs: []
  type: TYPE_NORMAL
- en: If you set `HADOOP_CONF_DIR` environment variable, the parameter is usually
    set as `hdfs://...`; otherwise `file://`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how Spark works in a cluster mode with its underlying
    architecture. You also saw how to deploy a full Spark application on a cluster.
    You saw how to deploy cluster for running Spark application in different cluster
    modes such as local, standalone, YARN, and Mesos. Finally, you saw how to configure
    Spark cluster on AWS using EC2 script. We believe that this chapter will help
    you to gain some good understanding of Spark. Nevertheless, due to page limitation,
    we could not cover many APIs and their underlying functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: If you face any issues, please don't forget to report this to Spark user mailing
    list at `user@spark.apache.org`. Before doing so, make sure that you have subscribed
    to it. In the next chapter, you will see how to test and debug Spark applications.
  prefs: []
  type: TYPE_NORMAL
