<html><head></head><body><div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Database Classification using Support Vector Machines</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this chapter, we will explore <strong class="calibre7">Support Vector Machines</strong> (<strong class="calibre7">SVMs</strong>), identify various applications for their use, and walk through an example of using a simple SVM to classify data in a database.</p>
<p class="calibre4">In this chapter, we will again organize the topics into the following chief areas:</p>
<ul class="calibre18">
<li class="calibre19">Database classification</li>
<li class="calibre19">Definition and purpose of SVMs</li>
<li class="calibre19">Common SVM applications</li>
<li class="calibre19">Using R and an SVM to classify data in a database</li>
</ul>
<p class="calibre4">Let's start this chapter with some dialogue around the idea of general or generic data classification.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Database classification</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">As we've said throughout this book, if the reader is a data or database developer, or has a similar background, the reader will most likely have heard of and be familiar with and comprehend the process of data modeling. This can be (at a high level, perhaps) described as the effort of analyzing and understanding the makeup and details of some data. Then, this data is organized or classified with the objective being that it can be easily understood and consumed by a community of users, either named (those already <span class="calibre14">identified, such as financial analysts in an organization) or</span> anonymous<span class="calibre14"> (such as internet consumers). The following image shows the data classification as per requirements:</span></p>
<div class="packt_figure"><img class="alignnone32" src="Images/023409d2-a36f-4311-9c03-3475dab957a6.png"/></div>
<div class="packt_infobox">Anonymous access is the most common system access control method, at least, when it comes to websites.</div>
<p class="calibre4">As part of almost any data modeling development project, one might be asked to create a class diagram. This (class) diagram details how to split data (or a database) into discrete objects, how those objects relate to each other, and any known interfaces or connections that they may have. Each class in a class diagram can hold both data and its purpose.</p>
<p class="calibre4">So, data classification is the process of sorting and categorizing data into various types, forms, or any other distinct classes.</p>
<p class="calibre4">Data classification enables the separation and cataloguing of data according to established requirements for the data, for numerous business or other objectives, so that a data plan or data model can be developed:</p>
<div class="packt_figure"><img class="alignnone33" src="Images/09afdc24-eafc-4d77-9583-92b911a30e22.png"/></div>
<p class="calibre4">Perhaps a real-world illustration, one which a database or data developer would recognize or be somewhat familiar with, might be the exercise of classifying or reclassifying financial accounts for reporting.</p>
<p class="calibre4">For example, you might find that a typical, recurring exercise during a financial reporting project (or perhaps, during a typical accounting period close) is performing a detailed review of each account, as well as consolidation of the accounts being used, to report an organization's financial results or performance.</p>
<p class="calibre4">To the accounting department, the term <em class="calibre21">classification</em> (or reclassification) is frequently used to define moving an amount from one <strong class="calibre7">general ledger</strong> (<strong class="calibre7">GL</strong>) account to another. For example, if an expense (for example, tax preparation) was charged to marketing supplies instead of administrative supplies, the correcting entry might be read:</p>
<p class="calibre4"><em class="calibre21">To reclassify from marketing supplies to administrative supplies</em>.</p>
<p class="calibre4">Here, from an accounting department perspective, we are speaking about an actual transaction to deduct an amount from one GL account and add that (same) amount to another. From a data developer perspective, there would be no account transaction made, but rather a programming change or process would be run that perhaps removes a particular account from one parent (or consolidation of a number of accounts) to another. This is usually referred to as updating the chart of accounts reporting hierarchy, or <strong class="calibre7">c</strong><span class="calibre14"><strong class="calibre7">hart of accounts</strong> (</span><strong class="calibre7">COA</strong>) maintenance.</p>
<p class="calibre4">Will we discover that these same concepts still apply in the field of statistics?</p>
<p class="calibre4">In the following sections of this chapter, we'll answer that question.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Data classification in statistics</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Statistical data classification is defined by some data scientists or statisticians as:</p>
<p class="calibre4"><em class="calibre21">The division of data into meaningful categories for analysis.</em></p>
<p class="calibre4">The database developer reading this should identify with that:</p>
<div class="packt_figure"><img class="image-border49" src="Images/3b7038bc-2721-4550-9fef-3c0582632f47.png"/></div>
<p class="calibre4">In data science and statistics, classification is defined as identifying to which categories (sometimes called sub-populations) a new observation should be included, on the basis of a training set of data containing observations (or instances) whose category membership has been validated.</p>
<div class="packt_infobox">Data scientists routinely apply statistical formulas to data automatically, allowing for processing big data in preparation for statistical analysis.</div>
<p class="calibre4">Typically, a population of unorganized or unclassified data (which is often referred to as raw data) collected in real situations and arranged indiscriminately does not provide a data scientist with any clear understanding, only hunches based upon poorly formed opinions.</p>
<p class="calibre4">The process of data classification attempts to locate similarities within the data and condenses the data by dropping out unnecessary details or noise (which we discussed in <a href="d114349e-1538-42d5-b9a6-939081494991.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">Chapter 10</a>, <em class="calibre21">Boosting your Database</em>). Classification facilitates the comparison between different sets of data, clearly showing the different points of agreement and disagreement.</p>
<p class="calibre4">Data classification can also help in determining baselines for the use of data.</p>
<p class="calibre4">Classification empowers the data scientist to study the relationship between several characteristics and make further statistical treatment like tabulation and so on.</p>
<div class="packt_tip">A very good example of benefitting from statistical data classification is an annual population census where people in a population are classified according to sex, age, marital status, and so on.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Guidelines for classifying data</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The purpose of a guideline (or policy) is to establish a framework for performing a task. There exist many schools of thought on the best way or most effective method for statistical classification.</p>
<p class="calibre4">In statistical data classification, a data scientist will classify data by assigning limits or boundaries. These are most often called <strong class="calibre7">class-limits</strong>. The group between any two class-limits is termed a <strong class="calibre7">class</strong> or <strong class="calibre7">class-interval</strong>.</p>
<p class="calibre4">In your statistical travels, you may have also come across the term <em class="calibre21">decision boundary</em>. Data scientists often use different or similar terms for referring to a concept. In statistical-classification problems concerning two classes, they may use the terms <strong class="calibre7">decision</strong> <strong class="calibre7">boundary</strong> or <strong class="calibre7">decision</strong> <strong class="calibre7">surface</strong> to define a hypersurface (a hypersurface is a generalization of the concept of the hyperplane) that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class, and all those on the other side as belonging to the other class.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Common guidelines</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Guidelines for classification will most often adhere to the following:</p>
<ul class="calibre18">
<li class="calibre19">
<p class="calibre33">There should not be any doubt about a definition of the classes</p>
</li>
<li class="calibre19">
<p class="calibre33">All the classes should preferably have equal width or length</p>
</li>
<li class="calibre19">
<p class="calibre33">The class-limits should be selected in such a way that no value of the item in the raw data coincides with the value of the limit</p>
</li>
<li class="calibre19">
<p class="calibre33">The number of classes should preferably be between 10 and 20, that is, neither too large nor too small</p>
</li>
<li class="calibre19">
<p class="calibre33">The classes should be exhaustive, that is, each value of the raw data should be included in them</p>
</li>
<li class="calibre19">
<p class="calibre33">The classes should be mutually exclusive and non-overlapping, that is, each item of the raw data should fit only into one class</p>
</li>
<li class="calibre19">
<p class="calibre33">The classification must be suitable for the object of inquiry</p>
</li>
<li class="calibre19">
<p class="calibre33">The classification should be flexible and items included in each class must be consistent</p>
</li>
<li class="calibre19">
<p class="calibre33">Width of class-interval is determined by first fixing the number of class-intervals and then dividing the total range by that number</p>
</li>
</ul>
<p class="calibre4">Guidelines are applied to whichever guideline(s) or policies a data scientist uses for classification. Those guidelines or policies must apply to all of the data being observed within a statistical project from start to completion.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Definitions</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">A few key terms that a data scientist must be aware of since they may affect how classification is approached, are:</p>
<ul class="calibre18">
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Confidential data</strong> typically represents any data classified as restricted and is often used interchangeably with sensitive data</p>
</li>
<li class="calibre19">
<p class="calibre33">A <strong class="calibre7">Data Steward</strong> is usually a senior-level resource whose responsibilities include the overseeing of the life cycle of one or more sets of data to be classified</p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">A Hyperplane</strong> subspace; an n-dimensional Euclidean space is a flat, n to a one-dimensional subset of that space that divides the space into two disconnected parts</p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Feature selection</strong> is the act of selecting a subset of relevant features (variables, predictors) for use in the construction of a statistical model</p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Feature space</strong> simply refers to the c<span class="calibre14">ollections of features that are used to characterize data</span></p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Statistical classification</strong> is the problem of identifying to which set of categories (sub-populations), a new observation should be a member of</p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Soft margin classification</strong> is a classification where the data scientist allows some error while defining or creating a classifier</p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Institutional</strong> <strong class="calibre7">data</strong> is defined as all data, maintained or licensed, by a university</p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Margin classifier</strong> is a classifier which is able to provide an accompanying distance from the decision boundary for each example within the data</p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Non-public information</strong> is any information that is classified as private or restricted according to a guideline, policy, or law</p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Sensitive data</strong> typically represents data classified as restricted and is frequently used interchangeably with confidential data</p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Property descriptor</strong>;<strong class="calibre7"> </strong>sometimes a classification system will employ descriptors to convey the reasons for a given classification; for example, middle age or urban are familiar descriptors</p>
</li>
<li class="calibre19">
<p class="calibre33"><strong class="calibre7">Interoperability</strong>; when different applications use different descriptors and have different rules about which properties are required and which are optional (for a particular classification), how does that support interoperability?</p>
</li>
</ul>
<div class="packt_infobox">The reader should spend some time researching the many related terms related to statistical data classification—there are numerous other data defining terms within the field of statistics.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Definition and purpose of an SVM</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">I support Vector Machines, do you?</p>
<div class="packt_figure"><img class="image-border50" src="Images/fa53bba4-b572-4d3c-b670-fc639011578c.png"/></div>
<p class="calibre4">In the field of machine learning, SVMs are similarly recognized as support vector networks and are defined as supervised learning models with accompanying learning algorithms that analyze data used for classification.</p>
<p class="calibre4">An important note about SVMs is that they are all about the ability to successfully perform pattern recognition. In other words, SVMs promote the ability to extend patterns found in data that are:</p>
<p class="calibre4"><em class="calibre21">Not linearly separable by transformations of original data to map into new space.</em></p>
<p class="calibre4">Again, everything you will find and come to know about SVMs will align with the idea that an SVM is a supervised machine learning algorithm which is most often used for classification or regression problems in statistics.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">The trick</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">You will hear most data scientists today refer to the trick or the SVM Trick; what they are referring to is that support vector machine algorithms use a method referred to as the <strong class="calibre7">kernel trick</strong>.</p>
<p class="calibre4">The kernel trick transforms data and then, based on those transformations it performs, works to discover an optimal boundary (remember, we defined classification boundaries earlier in this chapter) between the possible outputs or data points.</p>
<div class="packt_infobox">In statistics, kernel methods are a kind of algorithm which is used for pattern analysis. The overall mission of performing pattern analysis is to find and study general types of relationships in data.</div>
<p class="calibre4">Basically, the transformations that an SVM performs are some tremendously complex data transformations (on your data) which then efficiently figure out how to separate the data based on the labels or outputs (sometimes thought of as data scientist defined features) the data scientist has previously defined.</p>
<p class="calibre4">Another key (and thought-provoking) point to be aware of when it comes to support vector machines is that the data scientist doesn't have to worry about preserving the original dimensionality of the data when the SVM performs its transformations.</p>
<p class="calibre4">Consequently, the kernel trick takes the raw data, some previously defined features, performs its transformations, and produces the output you might not recognize--sort of like uncovering a great labyrinth!</p>
<p class="calibre4">The following is from <a href="http://www.yaksis.com/posts/why-use-svm.html" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">www.yaksis.com/posts/why-use-svm.html</a>, <em class="calibre21">Why use SVM?</em>:</p>
<p class="calibre4"><em class="calibre21">"You start with this harmless looking vector of data and after putting it through the kernel trick, it's unraveled and compounded itself until it's now a much larger set of data that can't be understood by looking at a spreadsheet.</em></p>
<p class="calibre4"><em class="calibre21">But here lies the magic, in expanding the dataset there are now more obvious boundaries between your classes and the SVM algorithm is able to compute a much more optimal hyperplane."</em></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Feature space and cheap computations</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">A huge part of understanding how SVMs work is understanding the trick which we've just mentioned. The trick, as we said, uses kernel methods (which use kernel functions) and are able to perform well in a high-dimensional feature space.</p>
<p class="calibre4">A feature space is an n-dimensional space where your data variables live. Kernel functions are able to operate within this high-dimensional space without having to compute the coordinates of the data within that space, but rather by merely computing the inner products between the images of all pairs of data in the feature space.</p>
<p class="calibre4">This kernel trick can then process the data quicker and more efficiently than if it had to explicitly compute the coordinates. This is known as being <strong class="calibre7">computationally inexpensive</strong>.</p>
<p class="calibre4">For the reader who is a data or database developer, since database views can hide complexity; imagine perhaps comparing the concept of dividing data within a database into several compiled views (rather than running queries on all of the data as a single source) to how vectors are defined and used by SVMs.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Drawing the line</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">A great way to describe the functional (or the practical) steps that a Support Vector Machine carries out in an effort to classify data might be to imagine that the SVMs are continuously endeavoring to find the line that best separates two classes of data points:</p>
<div class="packt_figure"><img class="image-border51" src="Images/f38c6c4b-15de-4689-8086-94d71ee47297.png"/></div>
<p class="calibre4">Here, the best line is defined as the line that results in the largest margin between the two classifications. The points that lie on this margin are the support vectors.</p>
<p class="calibre4">The great thing about acknowledging these (support) vectors is that we can then formulate the problem of finding the maximum-margin hyperplane (the line that best separates the two classes) as an optimization problem that only considers the support vectors that the SVM has established.</p>
<p class="calibre4">This means that the SVM processing can ignore the vast majority of the data, making the classification process go much faster.</p>
<p class="calibre4">More importantly, by presenting the problem in terms of the support vectors (the so-called <strong class="calibre7">dual form</strong>), we can apply the kernel trick we defined earlier in this chapter to effectively transform the support vector machine into a non-linear classifier.</p>
<div class="packt_infobox">Linear methods can only solve problems that are linearly separable (usually through a hyperplane). Non-linear methods characteristically include applying some type of transformation to your input dataset.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">More than classification</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Another key point is that SVMs can, in addition to classification, also perform regression.</p>
<p class="calibre4">An SVM using a non-linear kernel algorithm means that the boundary that the algorithm calculates doesn't have to be a straight line; this means that the SVM can capture even more complex relationships between the data points without the data scientist having to do all those difficult transformations by hand.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Downside</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">So, can we find a disadvantage or downside to using support vector machines?</p>
<p class="calibre4">Yes, sometimes the training time required by the SVM is much lengthier and can be much more computationally intensive (which we touched on with the idea of computational cost earlier in this chapter).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Reference resources</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">A very understandable book on the topic of SVMs is <em class="calibre21">An Introduction to Support Vector Machines and Other Kernel-based Learning Methods</em> by NelloChristiani and John Shawe-Taylor.</p>
<p class="calibre4">Another respectable reference that offers an insightful association between SVMs and a related type of neural network known as a <strong class="calibre7">Radial Basis Function Network</strong> is <em class="calibre21">Neural Networks and Learning Machines</em> by Simon Haykin, which we also referenced in <em class="calibre21">Chapter 5</em>, <em class="calibre21">Neural Networks</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Predicting credit scores</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In the remaining sections of this chapter, we'll try to review an SVMs example based on R thinking like a data developer.</p>
<div class="packt_infobox">This example is available online and referenced in detail in <em class="calibre20">Chapter 6</em> of the book, <em class="calibre20">Mastering Predictive Analytics with R, Second Edition</em>.<br class="calibre2"/>
<a href="https://www.packtpub.com/big-data-and-business-intelligence/mastering-predictive-analytics-r-second-edition" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">https://www.packtpub.com/big-data-and-business-intelligence/mastering-predictive-analytics-r-second-edition</a></div>
<p class="calibre4">If you're a data developer, you might not be aware that the following website offers a very good resource for test datasets at <a href="http://archive.ics.uci.edu/ml/index.php" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">http://archive.ics.uci.edu/ml/index.php</a></p>
<p class="calibre4">This example uses the particular dataset named <em class="calibre21">Statlog (German Credit Data) Data Set,</em> which can be found at the site under <a href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)</a></p>
<p class="calibre4">The website is shown as follows:</p>
<div class="packt_figure"><img class="image-border52" src="Images/f0490a06-ef25-46ae-a400-f3e6b0273e80.png"/></div>
<p class="calibre4">This dataset is data from the field of banking and finance. The observations in the dataset are loan applications made by individuals at a bank.</p>
<p class="calibre4">The objective of this data is to determine whether a load application constitutes a high credit risk.</p>
<p class="calibre4">A data or database developer will most likely observe that rather than downloading a dataset, one would create a query to extract the data as records or transactions, rather than instances (or observations).</p>
<div class="packt_infobox">Most likely, if extracting this data from a database, it will not be as easy as merely querying a single table because the loan applications will likely be in the form of a transaction, pulling data points from multiple database tables.</div>
<p class="calibre4">Other points a data developer should consider are:</p>
<ul class="calibre18">
<li class="calibre19">Dataset characteristics—field names</li>
<li class="calibre19">Attribute characteristics—datatypes</li>
<li class="calibre19">Date donated—the last update or add date</li>
</ul>
<p class="calibre4">The website<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"> https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/</a> provides two key files; one is the actual data, and one is the data schema:</p>
<div class="packt_figure"><img class="image-border53" src="Images/3adc97ec-25ef-4939-b926-1ed0f78b8e23.png"/></div>
<p class="calibre4">The data schema is in the format of an MS Word document (<kbd class="calibre22">german.doc</kbd>) and is used to create the following matrix:</p>
<table class="calibre9">
<tbody class="calibre10">
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13"><strong class="calibre7">Column name</strong></p>
</td>
<td class="calibre12">
<p class="calibre13"><strong class="calibre7">Type</strong></p>
</td>
<td class="calibre12">
<p class="calibre13"><strong class="calibre7">Definition</strong></p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">checking</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">The status of the existing checking account</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">duration</p>
</td>
<td class="calibre12">
<p class="calibre13">Numerical</p>
</td>
<td class="calibre12">
<p class="calibre13">The duration of months</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">creditHistory</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">The applicant's credit history</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">purpose</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">The purpose of the loan</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">credit</p>
</td>
<td class="calibre12">
<p class="calibre13">Numerical</p>
</td>
<td class="calibre12">
<p class="calibre13">The credit amount</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">savings</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">Savings account/bonds</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">employment</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">Present employment since</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">installmentRate</p>
</td>
<td class="calibre12">
<p class="calibre13">Numerical</p>
</td>
<td class="calibre12">
<p class="calibre13">The instalment rate (as a percentage of disposable income)</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">personal</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">Personal status and gender</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">debtors</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">Other debtors/guarantors</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">presentResidence</p>
</td>
<td class="calibre12">
<p class="calibre13">Numerical</p>
</td>
<td class="calibre12">
<p class="calibre13">Present residence since</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">property</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">The type of property</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">age</p>
</td>
<td class="calibre12">
<p class="calibre13">Numerical</p>
</td>
<td class="calibre12">
<p class="calibre13">The applicant's age in years</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">otherPlans</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">Other instalment plans</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">housing</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">The applicant's housing situation</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">existingBankCredits</p>
</td>
<td class="calibre12">
<p class="calibre13">Numerical</p>
</td>
<td class="calibre12">
<p class="calibre13">The number of existing credits at this bank</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">job</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">The applicant's job situation</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">dependents</p>
</td>
<td class="calibre12">
<p class="calibre13">Numerical</p>
</td>
<td class="calibre12">
<p class="calibre13">The number of dependents</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">telephone</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">The status of the applicant's telephone</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">foreign</p>
</td>
<td class="calibre12">
<p class="calibre13">Categorical</p>
</td>
<td class="calibre12">
<p class="calibre13">Foreign worker</p>
</td>
</tr>
<tr class="calibre16">
<td class="calibre12">
<p class="calibre13">risk</p>
</td>
<td class="calibre12">
<p class="calibre13">Binary</p>
</td>
<td class="calibre12">
<p class="calibre13">Credit risk (1 = good, 2 = bad)</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre4"> </p>
<p class="calibre4">The raw data looks like this:</p>
<div class="packt_figure"><img class="alignnone34" src="Images/a98b7f80-e769-4c5c-bb55-7cdf69dfa935.png"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Using R and an SVM to classify data in a database</h1>
                </header>
            
            <article class="calibre2">
                
<p class="cdpaligncenter1">Now that we understand the data, we can continue with this particular statistical example.</p>
<p class="calibre4">First, the data scientist will need to load the data into an R data frame object. This example is calling it <kbd class="calibre22">german_raw.</kbd></p>
<pre class="calibre29"># --- load the data 
german_raw&lt;- read.table("german.data", quote = "\"") </pre>
<p class="calibre4">The next step is to provide column names that match our data schema table, shown in the preceding:</p>
<pre class="calibre29">names(german_raw) &lt;- c("checking", "duration", "creditHistory", <br class="calibre2"/> "purpose", "credit", "savings", "employment", "installmentRate", <br class="calibre2"/> "personal", "debtors", "presentResidence", "property", "age", <br class="calibre2"/> "otherPlans", "housing", "existingBankCredits", "job", <br class="calibre2"/> "dependents", "telephone", "foreign", "risk") </pre>
<p class="calibre4">Note from the data schema (the table describing the features in the data) that we have a lot of categorical features to deal with. For this reason, a data scientist could employ the R <kbd class="calibre22">dummyVars()</kbd> function (which can be used to create a full set of dummy variables) to create dummy binary variables for use. In addition, he or she would record the <kbd class="calibre22">risk</kbd> variable, the output, as a factor with:</p>
<ul class="calibre18">
<li class="calibre19">level 0 = good credit</li>
<li class="calibre19">level 1 = bad credit</li>
</ul>
<pre class="calibre29">library(caret) 
dummies &lt;- dummyVars(risk ~ ., data = german_raw) 
german&lt;- data.frame(predict(dummies, newdata = german_raw),  
                       risk = factor((german_raw$risk - 1))) 
dim(german) 
[1] 1000   62 </pre>
<p class="calibre4">As a result of the preceding work, one would have an R data frame object with 61 features (because several of the categorical input features had numerous levels).</p>
<p class="calibre4">Next, the data scientist would partition or split the data into two subsets:</p>
<ul class="calibre18">
<li class="calibre19">Training dataset</li>
<li class="calibre19">Test dataset</li>
</ul>
<p class="calibre4">This split can be accomplished using the following R statements:</p>
<pre class="calibre29">set.seed(977) 
german_sampling_vector&lt;- createDataPartition(german$risk,  
                                      p = 0.80, list = FALSE) 
german_train&lt;- german[german_sampling_vector,] 
german_test&lt;- german[-german_sampling_vector,] </pre>
<p class="calibre4">For the data developer, there are similar approaches to take, such as perhaps using the from clause option TABLESAMPLE. With the TAMPLESAMPLE option, you are able to get a sample set of data from a table without having to read through the entire table or having to assign temporary random values to each row of data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Moving on</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">One particularity of this dataset that is mentioned on the (previously mentioned) website is that the data comes from a scenario where the two different types of errors defined have different costs associated with them.</p>
<p class="calibre4">Specifically, the cost of misclassifying a high-risk customer as a low-risk customer is five times more expensive for the bank than misclassifying a low-risk customer as a high-risk customer. This is understandable, as in the first case, the bank stands to lose a lot of money from a loan it gives out that cannot be repaid, whereas in the second case, the bank misses out on an opportunity to give out a loan that will yield interest for the bank.</p>
<div class="packt_infobox">This is a practical example where predictive analytics have a direct effect on an organization bottom line.</div>
<p class="calibre4">The <kbd class="calibre22">svm()</kbd> R function has a <kbd class="calibre22">class.weights</kbd> parameter, which is then used to specify the cost of misclassifying (an observation to each class). This is how the data scientist incorporates the asymmetric error cost information into the model.</p>
<p class="calibre4">First, a vector of class weights is created, noting the need to specify names that correspond to the output factor levels.</p>
<p class="calibre4">Then, the data scientist uses the R <kbd class="calibre22">tune()</kbd> function to train various SVM models with a radial kernel:</p>
<pre class="calibre29">class_weights&lt;- c(1, 5) 
names(class_weights) &lt;- c("0", "1") 
class_weights 
0 1  
1 5 
 
set.seed(2423) 
german_radial_tune&lt;- tune(svm,risk ~ ., data = german_train,  
  kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 10, 100),  
  gamma = c(0.01, 0.05, 0.1, 0.5, 1)), class.weights = class_weights) 
german_radial_tune$best.parameters 
   cost gamma 
9  10  0.05 
 
german_radial_tune$best.performance 
[1] 0.26 </pre>
<p class="calibre4">The suggested best model here has the cost as 10 and gamma as 0.05 and achieves a 74 percent training accuracy.</p>
<p class="calibre4">Next, we see how the model performs on the test dataset:</p>
<pre class="calibre29">german_model&lt;- german_radial_tune$best.model 
test_predictions&lt;- predict(german_model, german_test[,1:61]) 
 mean(test_predictions == german_test[,62]) 
[1] 0.735 
 
table(predicted = test_predictions, actual = german_test[,62]) 
         actual 
predicted   0   1 
        0 134  47 
        1   6  13 </pre>
<p class="calibre4">The performance on the test set is 73.5 percent and very close to what was seen in the training of the model. As expected, the model tends to make many more errors that misclassify a low-risk customer as a high-risk customer.</p>
<p class="calibre4">Unsurprisingly, this takes a toll on the overall classification accuracy, which just computes the ratio of correctly classified observations to the overall number of observations. In fact, were we to remove this cost imbalance, we would actually select a different set of parameters for our model and our performance, from the perspective of the unbiased classification accuracy, which would be better:</p>
<pre class="calibre29">set.seed(2423) 
german_radial_tune_unbiased&lt;- tune(svm,risk ~ .,  
  data = german_train, kernel = "radial", ranges = list(  
cost = c(0.01, 0.1, 1, 10, 100), gamma = c(0.01, 0.05, 0.1, 0.5, 1))) 
german_radial_tune_unbiased$best.parameters 
  cost gamma 
3    1  0.01 
german_radial_tune_unbiased$best.performance 
[1] 0.23875 </pre>
<p class="calibre4">Of course, this last model will tend to make a greater number of costly misclassifications of high-risk customers as low-risk customers, which we know is very undesirable. We'll conclude this section with two final thoughts. Firstly, we have used relatively small ranges for the <kbd class="calibre22">gamma</kbd> and <kbd class="calibre22">cost</kbd> parameters.</p>
<p class="calibre4">Originally, when this example was presented, it was left as "an exercise for the reader" to rerun the analysis with a greater spread of values for these two in order to see whether we can get even better performance, which would most likely result in longer training times.</p>
<p class="calibre4">Secondly, that particular dataset is quite challenging in that its baseline accuracy is actually 70 percent. This is because 70 percent of the customers in the data are low-risk customers (the two output classes are not balanced).</p>
<p class="calibre4">Whew!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this chapter, we defined the idea of database and data classification using some examples that a data developer may find familiar. Next, we introduced statistical data classification and compared that concept with the former.</p>
<p class="calibre4">Classification guidelines were offered along with several important, relevant terms.</p>
<p class="calibre4">Finally, we spoke of support vector machines, how they work, and the advantages they offer the data scientist.</p>
<p class="calibre4">In the next chapter, we aim to provide an explanation of the types of machine learning and illustrate to the developer how to use machine learning processes to understand database mappings and identify patterns within the data.</p>


            </article>

            
        </section>
    </div>



  </body></html>