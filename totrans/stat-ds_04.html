<html><head></head><body><div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Data Mining and the Database Developer</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">This chapter introduces the data developer to mining (not to be confused with querying) data, providing an understanding of exactly what data mining is and why it is an integral part of data science.</p>
<p class="calibre4">We'll provide working examples to help the reader feel comfortable using R for the most common statistical data mining methods: dimensional reduction, frequent patterns, and sequences.</p>
<p class="calibre4">In this chapter, we've broken things into the following topics:</p>
<ul class="calibre18">
<li class="calibre19">Definition and purpose of data mining</li>
<li class="calibre19">Preparing the developer for data mining rather than data querying</li>
<li class="calibre19">Using R for dimensional reduction, frequent patterns, and sequence mining</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Data mining</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">It is always prudent to start explaining things with a high-level definition.</p>
<p class="calibre4">Data mining can be explained simply as assembling information concerning a particular topic or belief in an understandable (and further useable) format. Keep in mind though that the information assembled is not the data itself (as with data querying) but information from the data (more on this later in this chapter).</p>
<p class="calibre4">Data mining should also not be confused with analytics, information extraction, or data analysis. Also, it can be manual or by hand, a semi-automatic, or automatic process. When working with new data, it will typically be a manual process that the data scientist will perform. Later, when working with newer versions of the same data (source), it may become automated to some level or degree.</p>
<p class="calibre4">Data mining is the probing carried out by a data scientist to find previously unknown information within the data, such as:</p>
<ul class="calibre18">
<li class="calibre19">Patterns, such as groups of data records, known as <strong class="calibre3">clusters</strong></li>
<li class="calibre19">Unusual records, known as <strong class="calibre3">anomalies</strong></li>
<li class="calibre19">Dependencies in the form of association rules or sequential patterns</li>
</ul>
<p class="calibre4">This new information (or insights) can be thought of as a kind of data summary and can be used in further analysis or, for example, in machine learning and predictive analytics. For example, with data mining, a data scientist might identify various groups that can then be used to obtain more accurate prediction results by a decision support system.</p>
<div class="packt_infobox">Data developers can liken the insights derived from data mining to descriptive or structural metadata, which is understood within the industry as data that defines data.</div>
<p class="calibre4">Once the probing is completed and the data scientist has mined the information, that information must then be transformed into a comprehensible and usable structure, given the objectives of the effort.</p>
<div class="packt_infobox">Data collection, data preparation, results from interpretation and visualization, and reporting is not part of data mining.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Common techniques</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Some of the most common and widely accepted and used data mining (statistical) analysis methods are explained in the following sub-sections.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Visualization</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Visualization of averages, measures of variation, counts, percentages, cross-tabbing, and simple correlations help the data scientists in understanding the structure of the data. This is also referred to as <strong class="calibre7">data profiling</strong>.</p>
<div class="packt_infobox">Area, temporal, multidimensional, and hierarchical are typical, commonly used, and easily understood formats for data visualization.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Cluster analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Cluster analysis is used by the data scientists to place data variables into defined collections (that is, clusters) as a way of summarizing the data. Clusters should be both internally homogeneous (the variables are like one another) as well as externally heterogeneous (the variables are not like members of other clusters).</p>
<div class="packt_infobox">Hierarchical agglomerative, partitioning, and model-based are all very common methods of cluster analysis.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Correlation analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Correlation analysis is a method where the data scientists measure the relationship between two data variables. This results in something called a <strong class="calibre7">correlation coefficient</strong>, which shows if changes to one variable (the independent variable) will result in changes to the other (the dependent variable).</p>
<div class="packt_infobox">Common correlation approaches are positive/negative and linear and non-linear.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Discriminant analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Discriminant analysis is used when there is no obvious natural ordering of groups to determine if a data variable is a member. With this method, there are predetermined groups with specific scores or measures that are used in the classification or grouping of the data variables process.</p>
<div class="packt_infobox"><strong class="calibre3">Linear discriminant analysis</strong> (<strong class="calibre3">LDA</strong>) is one of the most common approaches to discriminate analysis where the data scientist attempts to find a linear combination of features that characterize or separates a data variable (into groups).</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Factor analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Factor analysis is useful for understanding the reasons for the associations (amongst a group of data variables). The main goal is to try and reduce the number of variables and to detect structure in the relationships among them (this method also results in an overall data reduction).</p>
<div class="packt_infobox">Types of factor analysis used by data scientists are the principal component, common, image, alpha, and factor regression.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Regression analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Regression analysis uses the relationship between two or more quantitative variables so that one variable (dependent variable) can be predicted from the other(s) (independent variables).</p>
<div class="packt_infobox">There are many kinds of regression analysis, including simple linear, multiple linear, curvilinear, and multiple curvilinear, as well as logistic regression models.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Logistic analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">A logistic analysis is a method that is used when the response variable is binary or qualitative and attempts to find a best fitting equation using a maximum likelihood method to maximize the probability of obtaining the observed results given the fitted regression coefficients.</p>
<div class="packt_infobox">Some of the common flavours that logistic regression comes in include simple, multiple, polytomous, and Poisson logistic regression models.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Purpose</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Through the practice of data mining, a data scientist can achieve (the aforementioned) goal of deriving actionable information from long information or data.</p>
<p class="calibre4">Some have said that the objective of data mining is to discover structure inside unstructured (data). For example, you might use data mining to identify customer segments to design a promotion targeting high-value customers or an inventory control plan to ensure short product shelf life.</p>
<p class="calibre4">One might confuse data querying with data mining. But if we consider the example of generating a plan for controlling inventory at a newly opened home improvement store, then by simply querying sales transactions to determine the fastest selling products from the past few months (from other store locations), we might not be successful. Mining demographical information might, however, yield better results, as we might identify valid novel, potentially useful, and understandable correlations and patterns in the data, which can then be used to predict local consumer purchasing. In other words, the objective or purpose of data mining is not reporting but uncovering.</p>
<p class="calibre4">In the next section, we'll take a closer look at the differences between data mining and data querying.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Mining versus querying</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Data querying is the process of asking specific, structured questions of data in search of a specific answer, while data mining is the process of sifting through data to identify patterns and relationships using statistical algorithms.</p>
<p class="calibre4">The following matrix may help the data developers in gaining an understanding of the differences between data querying and data mining:</p>
<table class="calibre9">
<tbody class="calibre10">
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13"><strong class="calibre7">Example</strong></p>
</td>
<td class="calibre12">
<p class="calibre13"><strong class="calibre7">Data querying or mining</strong></p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">What was the total number of technical books sold last month worldwide?</p>
</td>
<td class="calibre12">
<p class="calibre13">Data querying</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">What factors affected the type of technical books sold worldwide last month?</p>
</td>
<td class="calibre12">
<p class="calibre13">Data mining</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">How many different technologies' technical books were sold last quarter?</p>
</td>
<td class="calibre12">
<p class="calibre13">Data querying</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">Which technologies were purchased as part of a set?</p>
</td>
<td class="calibre12">
<p class="calibre13">Data mining</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">Does a technology tend to be purchased in hardcopy or electronic version?</p>
</td>
<td class="calibre12">
<p class="calibre13">Data mining</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">Which technical books have repeat customers?</p>
</td>
<td class="calibre12">
<p class="calibre13">Data mining</p>
</td>
</tr>
<tr class="calibre16">
<td class="calibre12">
<p class="calibre13">Which is the most sold technical book overall?</p>
</td>
<td class="calibre12">
<p class="calibre13">Data querying</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre4">Once again, data querying is about reporting the results of events while data mining is the process of identifying relationships that may help to understand which factors affected the outcome of those events, or they may be used to predict future outcomes of similar events.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Choosing R for data mining</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Although there are many good options to choose from, R is a language and environment that has a short learning curve, is very flexible in nature, and is also very focused on statistical computing, making it great for manipulating, cleaning, summarizing, producing probability statistics, and so on (as well as actually creating visualizations with your data); thus, it's a great choice for the exercises data mining.</p>
<p class="calibre4">In addition, here are a few more reasons to learn and use R for data mining projects:</p>
<ul class="calibre18">
<li class="calibre19">R is used by a large number of academic statisticians, so it's a tool that is not going away</li>
<li class="calibre19">R is pretty much platform independent; what you develop will run almost anywhere</li>
<li class="calibre19">R has awesome help resources. Just google it and you'll see!</li>
</ul>
<p class="calibre4">To illustrate, we'll explore a few practical data mining examples using the R programming language throughout the rest of this chapter.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Visualizations</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">To begin, let's look at creating a simple visualization of our data, using R. In this use case scenario, we have data collected from a theoretical hospital, whereupon admission, patient medical history information is collected through an online survey. Information is also added to a patient's file as treatment is provided. The file includes many fields, including:</p>
<ul class="calibre18">
<li class="calibre19">Basic descriptive data for the patient, such as sex, date of birth, height, weight, blood type, and so on</li>
<li class="calibre19">Vital statistics, such as blood pressure, heart rate, and so on</li>
<li class="calibre19">Medical history, such as number of hospital visits, surgeries, major illnesses or conditions, is currently under a doctor's care, and so on</li>
<li class="calibre19">Demographical statistics, such as occupation, home state, educational background, and so on</li>
<li class="calibre19">Some additional information is also collected in the file to develop patient characteristics and habits, such as the number of times the patient included beef, pork, and fowl in his or her weekly diet, if he or she typically uses a butter replacement product, and so on</li>
</ul>
<p class="calibre4">Assuming we have been given no further information about the data (other than a brief field name list and the knowledge that the data is captured by hospital personnel upon patient admission), the next step would be to perform some data mining, that is identifying or grouping data and perhaps locating relationships between the variables.</p>
<p class="calibre4">To get started, we can read out hospital survey data into an R data frame and then use two available R functions to reveal information about our file:</p>
<div class="packt_figure"><img class="alignnone" src="Images/950b0084-7b4d-4367-949b-3883f928494a.png"/></div>
<p class="calibre4">The code shown here reads our text file (named <kbd class="calibre22">Chapter4.txt</kbd>) into an R data frame (also named <kbd class="calibre22">chapter4</kbd>) and then uses the functions <kbd class="calibre22">dim</kbd> and <kbd class="calibre22">names</kbd>. The <kbd class="calibre22">dim</kbd> function shows us the file's data structure (there are <kbd class="calibre22">5994</kbd> records or cases in this file and <kbd class="calibre22">107</kbd> data points or variables, as shown in the screenshot we just saw). The <kbd class="calibre22">names</kbd> function simply lists all the field or variable names in our file (partially shown in the screenshot we just saw).</p>
<div class="packt_infobox">R function attributes and <kbd class="calibre22">str</kbd> are also some very useful R data mining functions and would be worth the readers' time to investigate and experiment with further.</div>
<p class="calibre4">Initially, a data scientist might begin by looking through the field names for some ideas to start with; perhaps the common groups, such as sex, age, and state (insured is also a pretty interesting attribute these days!).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Current smokers</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Often, a data scientist has an objective in mind when performing data mining. So in this example, let's suppose we are interested in grouping patients who are smokers into age groups. Using the variable <kbd class="calibre22">current_smoker</kbd>, we can use the R table function and run the following code:</p>
<pre class="calibre29">table(chapter4["current_smoker"]) </pre>
<p class="calibre4">This yields the following information:</p>
<div class="packt_figure"><img class="alignnone1" src="Images/31da38d7-af84-4551-8b2a-015d5a9b1d39.png"/></div>
<p class="calibre4">From the results shown here, it seems like we have more non-smokers (<kbd class="calibre22">5466</kbd>) than smokers (<kbd class="calibre22">528</kbd>), at least in this file or population.</p>
<p class="calibre4">Next, what we'd like to see (that is, visualize) is the smoker patients in our population organized into (or by) age groups. To do this, a logical next step as a data scientist would be to understand the <kbd class="calibre22">range</kbd> of values present in the <kbd class="calibre22">age</kbd> variable. In other words, part of our data mining effort will be to see the age of the youngest patient, as well as the age of the oldest patient, within our population. Rather than having to slice and dice the data to find this information, we can use the R range function, as shown in the following screenshot:</p>
<div class="packt_figure"><img class="alignnone2" src="Images/63fac424-9842-4140-be51-90b52d3dac33.png"/></div>
<p class="calibre4">From these results, the data scientist can now see that we have cases with patient ages ranging from 1 to 99 years! Another good idea would be to visualize the frequency of the ages of our patients. The data scientist might want to again use the R table function to create a histogram:</p>
<pre class="calibre29">hist(table(Chapter4["age"]))</pre>
<p class="calibre4">The output of the preceding code is:</p>
<div class="packt_figure"><img class="alignnone3" src="Images/876062c3-b0f2-4597-9d7c-f9ce1fe997df.png"/></div>
<p class="calibre4">This R code will generate the following visualization, which provides, even more, visibility to our patients' ages:</p>
<div class="packt_figure"><img class="alignnone4" src="Images/19b94e0e-09b7-4773-bdac-4cd0b846e07f.png"/></div>
<p class="calibre4">Another interesting bit of information is density estimation. Without much effort, we can nest the three R functions, <kbd class="calibre22">plot</kbd>, <kbd class="calibre22">density</kbd>, and <kbd class="calibre22">table</kbd>, to create another visual of patient age.</p>
<p class="calibre4">We can run the following code:</p>
<div class="packt_figure"><img class="alignnone5" src="Images/66d1d045-173a-4da2-84d4-132131283177.png"/></div>
<p class="calibre4">This will generate the following visualization:</p>
<div class="packt_figure"><img class="alignnone6" src="Images/a607895f-588a-421b-995d-d3d159b5078a.png"/></div>
<p class="calibre4">Given all this new-found knowledge, perhaps the data scientist will want to go ahead and group our cases into six distinct age groups:</p>
<ul class="calibre18">
<li class="calibre19">Under 22</li>
<li class="calibre19">22 to 34</li>
<li class="calibre19">35 to 44</li>
<li class="calibre19">45 to 54</li>
<li class="calibre19">55 to 64</li>
<li class="calibre19">65 and older</li>
</ul>
<div class="packt_infobox">To begin speaking the language of the data scientist, the data developer should start using the word cases rather than records (in the file) and population rather than the file.</div>
<p class="calibre4">The following R program code groups our cases into current smokers by their recorded age and then creates a simple pie chart to visualize the results:</p>
<pre class="calibre29"># -- read our data into a data frame object 
Chapter4&lt;-read.csv('c:/chapter4/Chapter4.txt') 
 
# -- initialize holders for counting cases 
a1 &lt;-0;a2 &lt;-0;a3 &lt;-0;a4 &lt;-0;a5 &lt;-0;a6 &lt;-0 
# -- read through the cases and count smokers by age group 
for(i in 2:nrow(Chapter4)) 
{ 
if (as.numeric(Chapter4[i,"age"]) &lt; 22 &amp; Chapter4[i,"current_smoker"]=="Yes") {a1 &lt;- a1 + 1} 
if (as.numeric(Chapter4[i,"age"]) &gt; 21 &amp; as.numeric(Chapter4[i,"age"]) &lt; 35 &amp; Chapter4[i,"current_smoker"]=="Yes") {a2 &lt;- a2 + 1} 
if (as.numeric(Chapter4[i,"age"]) &gt; 34 &amp; as.numeric(Chapter4[i,"age"]) &lt; 45 &amp; Chapter4[i,"current_smoker"]=="Yes") {a3 &lt;- a3 + 1} 
if (as.numeric(Chapter4[i,"age"]) &gt; 44 &amp; as.numeric(Chapter4[i,"age"]) &lt; 55 &amp; Chapter4[i,"current_smoker"]=="Yes") {a4 &lt;- a4 + 1} 
if (as.numeric(Chapter4[i,"age"]) &gt; 54 &amp; as.numeric(Chapter4[i,"age"]) &lt; 65 &amp; Chapter4[i,"current_smoker"]=="Yes") {a5 &lt;- a5 + 1} 
if (as.numeric(Chapter4[i,"age"]) &gt; 64) {a6 &lt;- a6 + 1} 
} 
 
# -- build a pie chart 
slices &lt;- c(a1, a2, a3, a4, a5, a6) 
lbls &lt;- c("under 21", "22-34","35-44","45-54","55-64", "65 &amp; over") 
 
# -- create the actual visualization 
pie(slices, labels = lbls, main="Smokers by Age Range") </pre>
<p class="calibre4">The following is our simple pie chart generated by using the R <kbd class="calibre22">pie</kbd> function:</p>
<div class="packt_figure"><img class="alignnone7" src="Images/845383f3-5fb7-4068-b846-c48a87672a67.png"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Missing values</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Critical to the outcome of any analysis is the availability of data.</p>
<p class="calibre4">Suppose there are cases within our population that have values that are missing. You may want to ignore (or omit) those cases in your analysis. Rather than spending time writing code to deal with these cases, you can use the handy R generic function <kbd class="calibre22">na</kbd>. The <kbd class="calibre22">na.omit</kbd> function evaluates each case in your file and if it is missing any values for any of the variables, it drops that case automatically.</p>
<p class="calibre4">The following example shows the use of the R functions <kbd class="calibre22">na.omit</kbd> and <kbd class="calibre22">nrow</kbd> on a file with missing values:</p>
<div class="packt_figure"><img class="alignnone8" src="Images/87268f9b-8acb-49d2-a27a-671ee5bc0956.png"/></div>
<p class="calibre4">Note the row (case) count before and after the use of <kbd class="calibre22">na.omit</kbd> (five records were dropped).</p>
<div class="packt_infobox">I've overwritten the object <kbd class="calibre22">Chapter4</kbd> with the updated file; in reality, it is a good habit to create a new object so that you have an audit of your data before and after any processing.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">A cluster analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this next example, the data scientist wants to take a closer look at our cases, but only those that are smokers. So, with R, let's first create a subset of our original cases, including only those cases that are current smokers. As we did in the previous example, after we create our subset (named <kbd class="calibre22">mysub</kbd>), we will use the R <kbd class="calibre22">nrow</kbd> function to verify the number of records in our new population, so that we can get an idea of the number of cases in our new population:</p>
<pre class="calibre29"># --- create a subset of smokers only cases 
mysub &lt;- subset(Chapter4,Chapter4["current_smoker"]=="Yes") 
 
# --- confirm the row count 
nrow(mysub) </pre>
<p class="calibre4"><span class="calibre14">The output of the preceding code is:</span></p>
<div class="packt_figure"><img class="alignnone9" src="Images/d05eac96-f095-4229-98cf-502e1c5fefc9.png"/></div>
<p class="calibre4">As per the output we just saw, there are still over 500 cases in our new population. So, as a data scientist, we make the decision to pull a random sample of our cases (which we will then perform a cluster analysis on) and then, again, validate the number of records in our newest population.</p>
<p class="calibre4">We can use the R <kbd class="calibre22">sample</kbd> command to create a sample of just 30 cases:</p>
<pre class="calibre29"># --- create a random sample of 30 smokers 
mysample &lt;- mysub[sample(1:nrow(mysub), 30, 
   replace=FALSE),] 
# --- confirm the row count in our random case sample 
nrow(mysample) </pre>
<p class="calibre4"><span class="calibre14">The output of the preceding code is:</span></p>
<div class="packt_figure"><img class="alignnone10" src="Images/9fd427d2-4235-486e-9f54-b6b0c9f402f6.png"/></div>
<p class="calibre4">Finally, our data scientist feels that he now has a small enough sampling of our cases that is easy enough to work with, so let's go ahead and perform a cluster analysis with it. As mentioned earlier in this chapter, hierarchical agglomerative is one of the most popular cluster analysis techniques, and so we will use it with our random sample of cases.</p>
<p class="calibre4">We can perform the hierarchical agglomerative cluster analysis of our random sample of cases using a combination of the R's <kbd class="calibre22">dist</kbd> and <kbd class="calibre22">hclust</kbd> functions. The <kbd class="calibre22">dist</kbd> function calculates a distance matrix for your dataset, giving the Euclidean distance between any two observations. The <kbd class="calibre22">hclust</kbd> function performs hierarchical clustering on that distance matrix.</p>
<p class="calibre4">In conclusion, the easiest way to review and understand the results of a hierarchical cluster analysis is through visualization of the results. This visualization is known as a <strong class="calibre7">dendrogram</strong> (a tree diagram frequently used to illustrate the arrangement of the clusters), so we'll add that code as well:</p>
<pre class="calibre29"># -- perform the hierarchical cluster analysis 
smokerclust&lt;-hclust(dist(mysample)) 
 
# -- create results in a dendrogram 
plot(smokerclust)</pre>
<p class="calibre4"><span class="calibre14">The output of the preceding code is:</span></p>
<div class="packt_figure"><img class="alignnone11" src="Images/48739aaa-fd9f-4b8a-b6fe-1c04ef942a5b.png"/></div>
<p class="calibre4">This code sample creates the following visualization:</p>
<div class="packt_figure"><img class="alignnone12" src="Images/9ddf3744-4d87-44f5-a1f1-ae718bbb2ab1.png"/></div>
<div class="packt_infobox">R offers a long list of options for creating rich visualizations based upon data and statistics. It is important for a data scientist to be familiar with these options and, perhaps more importantly, understand which type of visualization best fits the objective of the analysis.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Dimensional reduction</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Clustering is intended to group data variables that are found to be interrelated, based on observations of their attributes' values. However, given a scenario with a large number of attributes, the data scientist will find that some of the attributes will usually not be meaningful for a given cluster. In the example, we used earlier in this chapter (dealing with patient cases), we could have found this situation. Recall that we performed a hierarchical cluster analysis on smokers only. Those cases include many attributes, such as, sex, age, weight, height, no_hospital_visits, heartrate, state, relationship, Insurance blood type, blood_pressure, education, date of birth, current_drinker, currently_on_medications, known_allergies, currently_under_doctors_care, ever_operated_on, occupation, heart_attack, rheumatic_fever, heart_murmur, diseases_of_the_arteries, and so on.</p>
<div class="packt_infobox">As a data scientist, you can use the R function <kbd class="calibre22">names</kbd>, as we did earlier in this chapter, to see the complete list of attributes.</div>
<p class="calibre4">Dimensional reduction is a process where the data scientist attempts to reduce or limit the number of attributes, or dimensions, within a case. This is referred to as reducing the number of random variables under consideration, but what that translates to is simply removing columns from a data file, based upon scientific theory.</p>
<p class="calibre4">Currently accepted and commonly used approaches to eliminating dimensions include:</p>
<ul class="calibre18">
<li class="calibre19">Missing data: If a variable (column) has many cases (records) with no values, it is not going to add much value; therefore, the column can be removed.</li>
<li class="calibre19">Remember, earlier in this chapter, we used the R function <kbd class="calibre22">na.omit</kbd>. This function comes in handy for removing entire cases; however, with dimensional reduction, we want to omit the entire variable for all cases.</li>
<li class="calibre19">Little variance: Like a variable having a great number of missing values, variables with little variation do not add value and can be removed.</li>
<li class="calibre19">High correlation: Data columns with very similar trends are also likely to carry very similar information. In this case, only one of them is needed.</li>
<li class="calibre19">Decision trees: It is a technique that may require a bit more work. It is an approach to dimensionality reduction where the data scientist generates a set of decision trees against a target attribute and then uses each attribute's usage statistics to find the most informative features (or columns). The columns with the lowest statistics may be dropped.</li>
<li class="calibre19"><strong class="calibre3">Principal component analysis</strong> (<strong class="calibre3">PCA</strong>): It is a process that transforms variables in a dataset into a new set of variables called <strong class="calibre3">principal components</strong>. The components are ordered by the variables' possible variance and only those with the highest variance are kept.</li>
<li class="calibre19">Backward elimination and forward construction: These techniques involve focusing on one or more variables, and sequentially removing or adding one additional variable at a time and observing the effect. Backward elimination measures effect with a tolerable error rate, while forward construction measures by the effect on performance.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Calculating statistical significance</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Let's now look at a simple example of using a data variables calculated variance to determine if it should be removed from an analysis.</p>
<p class="calibre4">Again, using our same patient cases example that we've used throughout this chapter, we can use the R function <kbd class="calibre22">var</kbd> to determine the statistical significance of the variables within our population.</p>
<div class="packt_infobox">The R function <kbd class="calibre22">var</kbd> only works with numeric values.</div>
<p class="calibre4">In the next code, we use the R <kbd class="calibre22">var</kbd> function to calculate the variance of the variable named:</p>
<pre class="calibre29">"no_servings_per_week_skim_milk". </pre>
<p class="calibre4">We can see that it has a low variance percentage (it doesn't vary often or is not found to have many different values, case by case):</p>
<div class="packt_figure"><img class="alignnone13" src="Images/75c4e61f-31b4-4065-b509-fb90f93a9c70.png"/></div>
<p class="calibre4">If we look at the calculated variance percentage of yet another variable, this one named: <kbd class="calibre22">No_servings_per_week_regular_or_diet_soda</kbd>, we see that it has a higher calculated variance (than the previous variable):</p>
<div class="packt_figure"><img class="alignnone14" src="Images/361f94a8-c1d3-454e-83d2-682bd8f1d3c2.png"/></div>
<p class="calibre4">And finally, if we look at a third variable, this one named <kbd class="calibre22">No_servings_per_week_water</kbd>, we get a third calculated variance:</p>
<div class="packt_figure"><img class="alignnone15" src="Images/4cb8fc1e-7e8f-4461-8ea9-dca10adf3eaf.png"/></div>
<p class="calibre4">From these individual variance calculations, we can see how statistically significant each of these variables may be in our case analysis:</p>
<table class="calibre9">
<tbody class="calibre10">
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13"><strong class="calibre7">Data Variable</strong></p>
</td>
<td class="calibre12">
<p class="calibre13"><strong class="calibre7">Calculated Variance</strong></p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13"><kbd class="calibre22">No_servings_per_week_skim_milk</kbd></p>
</td>
<td class="calibre12">
<p class="calibre13">.003160316</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13"><kbd class="calibre22">No_servings_per_week_regular_or_diet_soda</kbd></p>
</td>
<td class="calibre12">
<p class="calibre13">8.505655</p>
</td>
</tr>
<tr class="calibre16">
<td class="calibre12">
<p class="calibre13"><kbd class="calibre22">No_servings_per_week_water</kbd></p>
</td>
<td class="calibre12">
<p class="calibre13">24.10477</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre4">The data variable named <kbd class="calibre22">No_servings_per_week_skim_milk</kbd> could be certainly eliminated from the analysis, and depending upon our data scientists tolerance levels, possibly the data variable named <kbd class="calibre22">No_servings_per_week_regular_or_diet_soda</kbd> could be eliminated from our analysis as well.</p>
<p class="calibre4">Using simple R functions, we can visualize our calculated variance data for a better understanding:</p>
<div class="packt_figure"><img class="alignnone16" src="Images/a95c2d43-5c75-44d5-ac74-b9dad173e743.png"/></div>
<p class="calibre4">So, we generate the following visualization:</p>
<div class="packt_figure"><img class="alignnone17" src="Images/6ba20498-03cb-40ec-9a11-647e77adff0f.png"/></div>
<div class="packt_infobox">When we eliminate a variable, it is eliminated from all cases within our population.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Frequent patterning</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">To gain an understanding of statistical patterning, let us begin with thinking about what happens when an urban area is threatened by severe weather and potentially hazardous traveling—all the local stores sell out of bread, milk, and eggs!</p>
<p class="calibre4">Patterning (which is a subfield of data mining) is the process of looking through data in an effort to identify previously unknown but potentially useful patterns consisting of frequently co-occurring events (such as the stormy weather event triggering the sale of bread, milk, and eggs) or objects (such as the products bread, milk, and eggs being typically purchased together or bundled together in the same shopping cart).</p>
<p class="calibre4">Pattern mining is the process that consists of using or developing custom pattern mining logic. This logic might be applied to various types of data sources (such as transaction and sequence databases, streams, strings, spatial data, graphs, and so on) in an effort to look for various types of patterns.</p>
<p class="calibre4">At a higher level, data scientists look for:</p>
<ul class="calibre18">
<li class="calibre19">Interesting patterns</li>
<li class="calibre19">Frequent patterns</li>
<li class="calibre19">Rare patterns</li>
<li class="calibre19">Patterns with a high confidence</li>
<li class="calibre19">The top patterns, and others</li>
</ul>
<p class="calibre4">Some of the more specific types of patterns that may exist in data include:</p>
<ul class="calibre18">
<li class="calibre19"><strong class="calibre3">Subgraphs</strong>: The discovery of an interesting graph(s) within a graph or in a set of graphs</li>
<li class="calibre19"><strong class="calibre3">Direct and indirect associations</strong>: Identifying couplings or dependencies between objects or events; either implicit or explicit defined</li>
<li class="calibre19"><strong class="calibre3">Trends</strong>: This is also sometimes known as trend analysis, and is the practice of collecting seemingly unrelated information and attempting to spot a pattern</li>
<li class="calibre19"><strong class="calibre3">Periodic patterns</strong>: This is defined as a trend or change in the character of an element, either across a period or group</li>
<li class="calibre19"><strong class="calibre3">Sequential rules</strong>: This is an add-on to sequential pattern mining, taking into account the probability that an identified pattern will be followed</li>
<li class="calibre19"><strong class="calibre3">Lattices</strong>: A partially ordered set in which every two elements have a unique least upper bound and a unique greatest lower bound</li>
<li class="calibre19"><strong class="calibre3">Sequential patterns</strong>: A subsequence that appears in several sequences of a data</li>
<li class="calibre19"><strong class="calibre3">High-utility patterns</strong>: High utility patterns are those patterns that have been determined to have higher, greater, or equal to a threshold value</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Frequent item-setting</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Building on the idea in the previous section (of finding frequent patterns) is frequent item-setting. To the data developer, by far the most applicable patterning concept is that of frequent item-setting or finding items that are found to be frequently a member of a group or set.</p>
<p class="calibre4">Using our stormy weather example from earlier in this chapter, one can envision the process of searching through a file or database of sales transactions, looking for the occasion (that is, the event) where milk, bread, and eggs were purchased together as one sale (or one set of products).</p>
<p class="calibre4">Frequent item setting also involves determining a minsup or minimum supported threshold to be used within an analysis. What this means is that the data scientist will determine the minimum occurrence of items that constitute a set.</p>
<p class="calibre4">Again, going back to our stormy weather example, if the data scientist sets a minsup of two to be used, then sales, where just two of the member products exist, would be considered a set or pattern.</p>
<p class="calibre4">Let us consider the following sales transactions:</p>
<table class="calibre9">
<tbody class="calibre10">
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13"><strong class="calibre7">Sales ID</strong></p>
</td>
<td class="calibre12">
<p class="calibre13"><strong class="calibre7">Items Purchased</strong></p>
</td>
<td class="calibre12">
<p class="calibre13"><strong class="calibre7">Qualifies (as a Frequent Item set)</strong></p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">Sale 1</p>
</td>
<td class="calibre12">
<p class="calibre13">Milk, Bread, Eggs</p>
</td>
<td class="calibre12">
<p class="calibre13">Yes</p>
</td>
</tr>
<tr class="calibre11">
<td class="calibre12">
<p class="calibre13">Sale 2</p>
</td>
<td class="calibre12">
<p class="calibre13">Milk, Potatoes</p>
</td>
<td class="calibre12">
<p class="calibre13">No</p>
</td>
</tr>
<tr class="calibre15">
<td class="calibre12">
<p class="calibre13">Sale 3</p>
</td>
<td class="calibre12">
<p class="calibre13">Bread, Eggs, Tea</p>
</td>
<td class="calibre12">
<p class="calibre13">Yes</p>
</td>
</tr>
<tr class="calibre30">
<td class="calibre12">
<p class="calibre13">Sale 4</p>
</td>
<td class="calibre12">
<p class="calibre13">Eggs, Orange Juice</p>
</td>
<td class="calibre12">
<p class="calibre13">No</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_infobox">The most known algorithm for pattern mining, without a doubt, is Apriori, designed to be applied to a transaction database to discover patterns in transactions made by customers in stores. This algorithm takes as input a minsup threshold set by the user and a transaction database containing a set of transactions and outputs all the frequent item-sets.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Sequence mining</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Sequence mining evolves the preceding concepts even further. This is a process that the data scientist uses to discover a set of patterns that are shared among objects but which also have between them a specific order.</p>
<p class="calibre4">With sequence mining, we acknowledge that there are sequence rules associated with identified sequences. These rules define the pattern's objects and order. A sequence can have multiple rules. The support of a sequence rule can be calculated or determined by the data scientist by the number of sequences containing the rule divided by the total number of sequences. The confidence of a sequence rule will be the number of sequences containing the rule divided by the number of sequences containing its antecedent.</p>
<p class="calibre4">Overall, the objective of sequential rule mining is to discover all sequential rules having a support and confidence no less than two thresholds, given by the user named minsup and minconf.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this chapter, we provided a universal definition for data mining, listed the most common techniques used by data scientists, and stated the overall objective of the efforts. Data mining was also compared to data querying and, using R, various working examples were given to illustrate certain key techniques. Finally, the concepts of dimensional reduction, frequent patterning, and sequence mining were explored.</p>
<p class="calibre4">The next chapter will be a hands-on introduction to statistical analysis of data through the eyes of a data developer, providing instructions for describing the nature of data, exploring relationships presented in data, creating a summarization model from data, proving the validly of a data model, and employing predictive analytics on a data developed model.</p>


            </article>

            
        </section>
    </div>



  </body></html>