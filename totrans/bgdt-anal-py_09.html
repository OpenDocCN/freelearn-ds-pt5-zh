<html><head></head><body>
		<div class="Content" id="_idContainer180">
			<h1 id="_idParaDest-203"><em class="italics"><a id="_idTextAnchor230"/>Appendix</em></h1>
		</div>
		<div>
			<div class="Content" id="_idContainer181">
			</div>
		</div>
		<div class="Content" id="_idContainer182">
			<h2>About</h2>
			<p>This section is included to assist the students in performing the activities in the book. It includes detailed steps that are to be performed by the students to achieve the objectives of the activities.</p>
		</div>
		<div class="Content" id="_idContainer236">
			<h2 id="_idParaDest-204"><a id="_idTextAnchor231"/>Chapter 01: The Python Data Science Stack</h2>
			<h3 id="_idParaDest-205"><a id="_idTextAnchor232"/>Activity 1: IPython and Jupyter</h3>
			<ol>
				<li>Open the <strong class="inline">python_script_student.py</strong> file in a text editor, copy the contents to a notebook in IPython, and execute the operations.</li>
				<li>Copy and paste the code from the Python script into a Jupyter notebook:<p class="snippet">import numpy as np</p><p class="snippet">def square_plus(x, c):</p><p class="snippet">    return np.power(x, 2) + c</p></li>
				<li>Now, update the values of the <strong class="inline">x</strong> and <strong class="inline">c</strong> variables. Then, change the definition of the function:<p class="snippet">x = 10</p><p class="snippet">c = 100</p><p class="snippet">result = square_plus(x, c)</p><p class="snippet">print(result)</p><p>The output is as follows:</p><p class="snippet">200</p></li>
			</ol>
			<h3 id="_idParaDest-206"><a id="_idTextAnchor233"/>Activity 2: Working with Data Problems</h3>
			<ol>
				<li value="1">Import pandas and NumPy library:<p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p></li>
				<li>Read the RadNet dataset from the U.S. Environmental Protection Agency, available from the Socrata project:<p class="snippet">url = "https://opendata.socrata.com/api/views/cf4r-dfwe/rows.csv?accessType=DOWNLOAD"</p><p class="snippet">df = pd.read_csv(url)</p></li>
				<li>Create a list with numeric columns for radionuclides in the RadNet dataset:<p class="snippet">columns = df.columns</p><p class="snippet">id_cols = ['State', 'Location', "Date Posted", 'Date Collected', 'Sample Type', 'Unit']</p><p class="snippet">columns = list(set(columns) - set(id_cols))</p><p class="snippet">columns</p></li>
				<li>Use the <strong class="inline">apply</strong> method on one column, with a <strong class="inline">lambda</strong> function that compares the <strong class="inline">Non-detect</strong> string:<p class="snippet">df['Cs-134'] = df['Cs-134'].apply(lambda x: np.nan if x == "Non-detect" else x)</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer183"><img alt="Figure 1.19: DataFrame after applying the lambda function" src="image/C12913_01_19.jpg"/></div><h6>Figure 1.19: DataFrame after applying the lambda function</h6></li>
				<li>Replace the text values with <strong class="inline">NaN</strong> in one column with <strong class="inline">np.nan</strong>:<p class="snippet">df.loc[:, columns] = df.loc[:, columns].applymap(lambda x: np.nan if x == 'Non-detect' else x)</p><p class="snippet">df.loc[:, columns] = df.loc[:, columns].applymap(lambda x: np.nan if x == 'ND' else x)</p></li>
				<li>Use the same lambda comparison and use the <strong class="inline">applymap</strong> method on several columns at the same time, using the list created in the first step:<p class="snippet">df.loc[:, ['State', 'Location', 'Sample Type', 'Unit']] = df.loc[:, ['State', 'Location', 'Sample Type', 'Unit']].applymap(lambda x: x.strip())</p></li>
				<li>Create a list of the remaining columns that are not numeric:<p class="snippet">df.dtypes</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer184"><img alt="Figure 1.20: List of columns and their type" src="image/C12913_01_20.jpg"/></div><h6>Figure 1.20: List of columns and their type</h6></li>
				<li>Convert the DataFrame objects into floats using the <strong class="inline">to_numeric</strong> function:<p class="snippet">df['Date Posted'] = pd.to_datetime(df['Date Posted'])</p><p class="snippet">df['Date Collected'] = pd.to_datetime(df['Date Collected'])</p><p class="snippet">for col in columns:</p><p class="snippet">    df[col] = pd.to_numeric(df[col])</p><p class="snippet">df.dtypes</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer185"><img alt="Figure 1.21: List of columns and their type" src="image/C12913_01_21.jpg"/></div><h6>Figure 1.21: List of columns and their type</h6></li>
				<li>Using the selection and filtering methods, verify that the names of the string columns don't have any spaces:<p class="snippet">df['Date Posted'] = pd.to_datetime(df['Date Posted'])</p><p class="snippet">df['Date Collected'] = pd.to_datetime(df['Date Collected'])</p><p class="snippet">for col in columns:</p><p class="snippet">    df[col] = pd.to_numeric(df[col])</p><p class="snippet">df.dtypes</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer186">
					<img alt="Figure 1.22: DataFrame after applying the selection and filtering method" src="image/C12913_01_22.jpg"/>
				</div>
			</div>
			<h6>Figure 1.22: DataFrame after applying the selection and filtering method</h6>
			<h3 id="_idParaDest-207"><a id="_idTextAnchor234"/>Activity 3: Plotting Data with Pandas</h3>
			<ol>
				<li value="1">Use the RadNet DataFrame that we have been working with.</li>
				<li>Fix all the data type problems, as we saw before.</li>
				<li>Create a plot with a filter per <strong class="inline">Location</strong>, selecting the city of <strong class="inline">San Bernardino</strong>, and one radionuclide, with the <em class="italics">x</em>-axis set to the <strong class="inline">date</strong> and the <em class="italics">y</em>-axis with radionuclide <strong class="inline">I-131</strong>:<p class="snippet">df.loc[df.Location == 'San Bernardino'].plot(x='Date Collected', y='I-131')</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer187"><img alt="" src="image/C12913_01_23.jpg"/></div><h6>Figure 1.23: Plot of Date collected vs I-131</h6></li>
				<li>Create a scatter plot with the concentration of two related radionuclides, <strong class="inline">I-131</strong> and <strong class="inline">I-132</strong>:<p class="snippet">fig, ax = plt.subplots()</p><p class="snippet">ax.scatter(x=df['I-131'], y=df['I-132'])</p><p class="snippet">_ = ax.set(</p><p class="snippet">    xlabel='I-131',</p><p class="snippet">    ylabel='I-132',</p><p class="snippet">    title='Comparison between concentrations of I-131 and I-132'</p><p class="snippet">)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer188">
					<img alt="Figure 1.24: Plot of concentration of I-131 and I-132" src="image/C12913_01_24.jpg"/>
				</div>
			</div>
			<h6>Figure 1.24: Plot of concentration of I-131 and I-132</h6>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor235"/>Chapter 02: Statistical Visualizations Using Matplotlib and Seaborn</h2>
			<h3 id="_idParaDest-209"><a id="_idTextAnchor236"/>Activity 4: Line Graphs with the Object-Oriented API and Pandas DataFrames</h3>
			<ol>
				<li value="1">Import the required libraries in the Jupyter notebook and read the dataset from the Auto-MPG dataset repository:<p class="snippet">%matplotlib inline</p><p class="snippet">import matplotlib as mpl</p><p class="snippet">import matplotlib.pyplot as plt import numpy as np</p><p class="snippet">import pandas as pd</p><p class="snippet">url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"</p><p class="snippet">df = pd.read_csv(url)</p></li>
				<li>Provide the column names to simplify the dataset, as illustrated here:<p class="snippet">column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']</p></li>
				<li>Now read the new dataset with column names and display it:<p class="snippet">df = pd.read_csv(url, names= column_names, delim_whitespace=True)</p><p class="snippet">df.head()</p><p>The plot is as follows:</p><div class="IMG---Figure" id="_idContainer189"><img alt="Figure 2.29: The auto-mpg DataFrame" src="image/C12913_02_13.jpg"/></div><h6>Figure 2.29: The auto-mpg DataFrame</h6></li>
				<li>Convert the <strong class="inline">horsepower</strong> and <strong class="inline">year</strong> data types to float and integer using the following command:<p class="snippet">df.loc[df.horsepower == '?', 'horsepower'] = np.nan</p><p class="snippet">df['horsepower'] = pd.to_numeric(df['horsepower'])</p><p class="snippet">df['full_date'] = pd.to_datetime(df.year, format='%y')</p><p class="snippet">df['year'] = df['full_date'].dt.year</p></li>
				<li>Let's display the data types:<p class="snippet">df.dtypes</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer190"><img alt="Figure 2.30: The data types" src="image/C12913_02_30.jpg"/></div><h6>Figure 2.30: The data types</h6></li>
				<li>Now plot the average <strong class="inline">horsepower</strong> per <strong class="inline">year</strong> using the following command:<p class="snippet">df.groupby('year')['horsepower'].mean().plot()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer191">
					<img alt="" src="image/C12913_02_31.jpg"/>
				</div>
			</div>
			<h6>Figure 2.31: Line plot</h6>
			<h3 id="_idParaDest-210"><a id="_idTextAnchor237"/>Activity 5: Understanding Relationships of Variables Using Scatter Plots</h3>
			<ol>
				<li value="1">Import the required libraries into the Jupyter notebook and read the dataset from the Auto-MPG dataset repository:<p class="snippet">%matplotlib inline</p><p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib as mpl</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import seaborn as sns</p><p class="snippet">url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"</p><p class="snippet">df = pd.read_csv(url)</p></li>
				<li>Provide the column names to simplify the dataset as illustrated here:<p class="snippet">column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']</p></li>
				<li>Now read the new dataset with column names and display it:<p class="snippet">df = pd.read_csv(url, names= column_names, delim_whitespace=True)</p><p class="snippet">df.head()</p><p>The plot is as follows:</p><div class="IMG---Figure" id="_idContainer192"><img alt="Figure 2.32: Auto-mpg DataFrame" src="image/C12913_02_13.jpg"/></div><h6>Figure 2.32: Auto-mpg DataFrame</h6></li>
				<li>Now plot the scatter plot using the <strong class="inline">scatter</strong> method:<p class="snippet">fig, ax = plt.subplots()</p><p class="snippet">ax.scatter(x = df['horsepower'], y=df['weight'])</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer193">
					<img alt="Figure 2.33: Scatter plot using the scatter method" src="image/Image54488.jpg"/>
				</div>
			</div>
			<h6>Figure 2.33: Scatter plot using the scatter method</h6>
			<h3 id="_idParaDest-211"><a id="_idTextAnchor238"/>Activity 6: Exporting a Graph to a File on Disk</h3>
			<ol>
				<li value="1">Import the required libraries in the Jupyter notebook and read the dataset from the Auto-MPG dataset repository:<p class="snippet">%matplotlib inline</p><p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib as mpl</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import seaborn as sns</p><p class="snippet">url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"</p><p class="snippet">df = pd.read_csv(url)</p></li>
				<li>Provide the column names to simplify the dataset, as illustrated here:<p class="snippet">column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']</p></li>
				<li>Now read the new dataset with column names and display it:<p class="snippet">df = pd.read_csv(url, names= column_names, delim_whitespace=True)</p></li>
				<li>Create a bar plot using the following command:<p class="snippet">fig, ax = plt.subplots()</p><p class="snippet">df.weight.plot(kind='hist', ax=ax)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer194"><img alt="Figure 2.34: Bar plot" src="image/C12913_02_34.jpg"/></div><h6>Figure 2.34: Bar plot</h6></li>
				<li>Export it to a PNG file using the <strong class="inline">savefig</strong> function:<p class="snippet">fig.savefig('weight_hist.png')</p></li>
			</ol>
			<h3 id="_idParaDest-212"><a id="_idTextAnchor239"/>Activity 7: Complete Plot Design</h3>
			<ol>
				<li value="1">Import the required libraries in the Jupyter notebook and read the dataset from the Auto-MPG dataset repository:<p class="snippet">%matplotlib inline</p><p class="snippet">import pandas as pd</p><p class="snippet">import numpy as np</p><p class="snippet">import matplotlib as mpl</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import seaborn as sns</p><p class="snippet">url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"</p><p class="snippet">df = pd.read_csv(url)</p></li>
				<li>Provide the column names to simplify the dataset, as illustrated here:<p class="snippet">column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']</p></li>
				<li>Now read the new dataset with column names and display it:<p class="snippet">df = pd.read_csv(url, names= column_names, delim_whitespace=True)</p></li>
				<li>Perform GroupBy on <strong class="inline">year</strong> and <strong class="inline">cylinders</strong>, and unset the option to use them as indexes:<p class="snippet">df_g = df.groupby(['year', 'cylinders'], as_index=False)</p></li>
				<li>Calculate the average miles per gallon over the grouping and set <strong class="inline">year</strong> as index:<p class="snippet">df_g = df_g.mpg.mean()</p></li>
				<li>Set year as the DataFrame index:<p class="snippet">df_g = df_g.set_index(df_g.year)</p></li>
				<li>Create the figure and axes using the object-oriented API:<p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">fig, axes = plt.subplots()</p></li>
				<li>Group the <strong class="inline">df_g</strong> dataset by <strong class="inline">cylinders</strong> and plot the miles per gallon variable using the axes created with size (<strong class="inline">10</strong>,<strong class="inline">8</strong>):<p class="snippet">df = df.convert_objects(convert_numeric=True)</p><p class="snippet">df_g = df.groupby(['year', 'cylinders'], as_index=False).horsepower.mean()</p><p class="snippet">df_g = df_g.set_index(df_g.year)</p></li>
				<li>Set the <strong class="bold">title</strong>, <strong class="bold">x</strong> label, and <strong class="bold">y</strong> label on the axes:<p class="snippet">fig, axes = plt.subplots()</p><p class="snippet">df_g.groupby('cylinders').horsepower.plot(axes=axes, figsize=(12,10))</p><p class="snippet">_ = axes.set(</p><p class="snippet">    title="Average car power per year",</p><p class="snippet">    xlabel="Year",</p><p class="snippet">    ylabel="Power (horsepower)"</p><p class="snippet">    </p><p class="snippet">)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer195"><img alt="" src="image/Image54506.jpg"/></div><h6>Figure 2.35: Line plot for average car power per year (without legends)</h6></li>
				<li>Include legends, as follows:<p class="snippet">axes.legend(title='Cylinders', fancybox=True)</p><div class="IMG---Figure" id="_idContainer196"><img alt="" src="image/Image54515.jpg"/></div><h6>Figure 2.36: Line plot for average car power per year (with legends)</h6></li>
				<li>Save the figure to disk as a PNG file:<p class="snippet">fig.savefig('mpg_cylinder_year.png')</p></li>
			</ol>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor240"/>Chapter 03: Working with Big Data Frameworks</h2>
			<h3 id="_idParaDest-214"><a id="_idTextAnchor241"/>Activity 8: Parsing Text</h3>
			<ol>
				<li value="1">Read the text files into the Spark object using the <strong class="inline">text</strong> method:<p class="snippet">rdd_df = spark.read.text("/localdata/myfile.txt").rdd</p><p>To parse the file that we are reading, we will use lambda functions and Spark operations such as <strong class="inline">map</strong>, <strong class="inline">flatMap</strong>, and <strong class="inline">reduceByKey</strong>. <strong class="inline">flatmap</strong> applies a function to all elements of an RDD, flattens the results, and returns the transformed RDD. <strong class="inline">reduceByKey</strong> merges the values based on the given key, combining the values. With these functions, we can count the number of lines and words in the text.</p></li>
				<li>Extract the <strong class="inline">lines</strong> from the text using the following command:<p class="snippet">lines = rdd_df.map(lambda line: line[0])</p></li>
				<li>This splits each line in the file as an entry in the list. To check the result, you can use the <strong class="inline">collect</strong> method, which gathers all data back to the driver process:<p class="snippet">lines.collect()</p></li>
				<li>Now, let's count the number of lines, using the <strong class="inline">count</strong> method:<p class="snippet">lines.count()</p><h4>Note</h4><p class="callout">Be careful when using the <strong class="inline">collect</strong> method! If the DataFrame or RDD being collected is larger than the memory of the local driver, Spark will throw an error.</p></li>
				<li>Now, let's first split each line into words, breaking it by the space around it, and combining all elements, removing words in uppercase:<p class="snippet">splits = lines.flatMap(lambda x: x.split(' '))</p><p class="snippet">lower_splits = splits.map(lambda x: x.lower())</p></li>
				<li>Let's also remove the <em class="italics">stop words</em>. We could use a more consistent stop words list from <strong class="bold">NLTK</strong>, but for now, we will row our own:<p class="snippet">stop_words = ['of', 'a', 'and', 'to']</p></li>
				<li>Use the following command to remove the stop words from our token list:<p class="snippet">tokens = lower_splits.filter(lambda x: x and x not in stop_words)</p><p>We can now process our token list and count the unique words. The idea is to generate a list of tuples, where the first element is the <strong class="inline">token</strong> and the second element is the <strong class="inline">count</strong> of that particular token.</p></li>
				<li>First, let's <strong class="inline">map</strong> our token to a list:<p class="snippet">token_list = tokens.map(lambda x: [x, 1])</p></li>
				<li>Use the <strong class="inline">reduceByKey</strong> operation, which will apply the operation to each of the lists:<p class="snippet">count = token_list.reduceByKey(add).sortBy(lambda x: x[1], ascending=False)</p><p class="snippet">count.collect()</p></li>
			</ol>
			<p>Remember, collect all data back to the driver node! Always check whether there is enough memory by using tools such as <strong class="inline">top</strong> and <strong class="inline">htop</strong>.</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor242"/>Chapter 04: Diving Deeper with Spark</h2>
			<h3 id="_idParaDest-216"><a id="_idTextAnchor243"/>Activity 9: Getting Started with Spark DataFrames</h3>
			<p>If you are using Google Collab to run the Jupyter notebook, add these lines to ensure you have set the environment:</p>
			<p class="snippet">!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null</p>
			<p class="snippet">!wget -q http://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz</p>
			<p class="snippet">!tar xf spark-2.4.0-bin-hadoop2.7.tgz</p>
			<p class="snippet">!pip install -q findspark</p>
			<p class="snippet">import os</p>
			<p class="snippet">os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"</p>
			<p class="snippet">os.environ["SPARK_HOME"] = "/content/spark-2.4.2-bin-hadoop2.7"</p>
			<p>Install findspark if not installed using the following command:</p>
			<p class="snippet">pip install -q findspark</p>
			<ol>
				<li value="1">To create a sample DataFrame by manually specifying the schema, importing findspark module to connect Jupyter with Spark:<p class="snippet">import findspark</p><p class="snippet">findspark.init()</p><p class="snippet">import pyspark</p><p class="snippet">import os</p></li>
				<li>Create the <strong class="inline">SparkContext</strong> and <strong class="inline">SQLContext</strong> using the following command:<p class="snippet">sc = pyspark.SparkContext()</p><p class="snippet">from pyspark.sql import SQLContext</p><p class="snippet">sqlc = SQLContext(sc)</p><p class="snippet">from pyspark.sql import *</p><p class="snippet">na_schema = Row("Name","Subject","Marks")</p><p class="snippet">row1 = na_schema("Ankit", "Science",95)</p><p class="snippet">row2 = na_schema("Ankit", "Maths", 86)</p><p class="snippet">row3 = na_schema("Preity", "Maths", 92)</p><p class="snippet">na_list = [row1, row2, row3]</p><p class="snippet">df_na = sqlc.createDataFrame(na_list)</p><p class="snippet">type(df_na)</p><p>The output is as follows:</p><p class="snippet">pyspark.sql.dataframe.DataFrame</p></li>
				<li>Check the DataFrame using the following command:<p class="snippet">df_na.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer197"><img alt="Figure 4.29: Sample DataFrame" src="image/C12913_04_29.jpg"/></div><h6>Figure 4.29: Sample DataFrame</h6></li>
				<li>Create a sample DataFrame from an existing RDD. First creating RDD as illustrated here:<p class="snippet">data = [("Ankit","Science",95),("Preity","Maths",86),("Ankit","Maths",86)]</p><p class="snippet">data_rdd = sc.parallelize(data)</p><p class="snippet">type(data_rdd)</p><p>The output is as follows:</p><p class="snippet">pyspark.rdd.RDD</p></li>
				<li>Converting RDD to DataFrame using the following command:<p class="snippet">data_df = sqlc.createDataFrame(data_rdd)</p><p class="snippet">data_df.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer198"><img alt="Figure 4.30: RDD to DataFrame" src="image/Image54539.jpg"/></div><h6>Figure 4.30: RDD to DataFrame</h6></li>
				<li>Create a sample DataFrame by reading the data from a CSV file:<p class="snippet">df = sqlc.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('mtcars.csv')</p><p class="snippet">type(df)</p><p>The output is as follows:</p><p class="snippet">pyspark.sql.dataframe.DataFrame</p></li>
				<li>Print first seven rows of the DataFrame:<p class="snippet">df.show(7)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer199"><img alt="" src="image/C12913_04_31.jpg"/></div><h6>Figure 4.31: First seven rows of the DataFrame</h6></li>
				<li>Print the schema of the DataFrame:<p class="snippet">df.printSchema()</p></li>
				<li>The output is as follows:<div class="IMG---Figure" id="_idContainer200"><img alt="" src="image/C12913_04_32.jpg"/></div><h6>Figure 4.32: Schema of the DataFrame</h6></li>
				<li>Print the number of columns and rows in DataFrame:<p class="snippet">print('number of rows:'+ str(df.count()))</p><p class="snippet">print('number of columns:'+ str(len(df.columns)))</p><p>The output is as follows:</p><p class="snippet">number of rows:32</p><p class="snippet">number of columns:11</p></li>
				<li>Print the summary statistics of DataFrame and any two individual columns:<p class="snippet">df.describe().show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer201"><img alt="" src="image/C12913_04_33.jpg"/></div><h6>Figure 4.33: Summary statistics of DataFrame</h6><p>Print the summary of any two columns:</p><p class="snippet">df.describe(['mpg','cyl']).show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer202"><img alt="" src="image/C12913_04_34.jpg"/></div><h6>Figure 4.34: Summary statistics of mpg and cyl columns</h6></li>
				<li>Write first seen rows of the sample DataFrame in a CSV file:<p class="snippet">df_p = df.toPandas()</p><p class="snippet">df_p.head(7).to_csv("mtcars_head.csv")</p></li>
			</ol>
			<h3 id="_idParaDest-217"><a id="_idTextAnchor244"/>Activity 10: Data Manipulation with Spark DataFrames</h3>
			<ol>
				<li value="1">Install the packages as illustrated here:<p class="snippet">!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null</p><p class="snippet">!wget -q http://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz</p><p class="snippet">!tar xf spark-2.4.0-bin-hadoop2.7.tgz</p><p class="snippet">!pip install -q findspark</p><p class="snippet">import os</p><p class="snippet">os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"</p><p class="snippet">os.environ["SPARK_HOME"] = "/content/spark-2.4.0-bin-hadoop2.7"</p></li>
				<li>Then, import the <strong class="inline">findspark</strong> module to connect the Jupyter with Spark use the following command:<p class="snippet">import findspark</p><p class="snippet">findspark.init()</p><p class="snippet">import pyspark</p><p class="snippet">import os</p></li>
				<li>Now, create the <strong class="inline">SparkContext</strong> and <strong class="inline">SQLContext</strong> as illustrated here:<p class="snippet">sc = pyspark.SparkContext()</p><p class="snippet">from pyspark.sql import SQLContext</p><p class="snippet">sqlc = SQLContext(sc)</p></li>
				<li>Create a DataFrame in Spark as illustrated here:<p class="snippet">df = sqlc.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('mtcars.csv')</p><p class="snippet">df.show(4)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer203"><img alt="" src="image/C12913_04_35.jpg"/></div><h6>Figure 4.35: DataFrame in Spark</h6></li>
				<li>Rename any five columns of DataFrame using the following command:<p class="snippet">data = df</p><p class="snippet">new_names = ['mpg_new', 'cyl_new', 'disp_new', 'hp_new', 'drat_new']</p><p class="snippet">for i,z in zip(data.columns[0:5],new_names):</p><p class="snippet">    data = data.withColumnRenamed(str(i),str(z))</p><p class="snippet">    </p><p class="snippet">data.columns</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer204"><img alt="" src="image/C12913_04_36.jpg"/></div><h6>Figure 4.36: Columns of DataFrame</h6></li>
				<li>Select any two numeric and one categorical column from the DataFrame:<p class="snippet">data = df.select(['cyl','mpg','hp'])</p><p class="snippet">data.show(5)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer205"><img alt="" src="image/C12913_04_37.jpg"/></div><h6>Figure 4.37: Two numeric and one categorical column from the DataFrame</h6></li>
				<li>Count the number of distinct categories in the categorical variable:<p class="snippet">data.select('cyl').distinct().count() #3</p></li>
				<li>Create two new columns in DataFrame by summing up and multiplying together the two numerical columns:<p class="snippet">data = data.withColumn('colsum',(df['mpg'] + df['hp']))</p><p class="snippet">data = data.withColumn('colproduct',(df['mpg'] * df['hp']))</p><p class="snippet">data.show(5)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer206"><img alt="" src="image/C12913_04_38.jpg"/></div><h6>Figure 4.38: New columns in DataFrame</h6></li>
				<li>Drop both the original numerical columns:<p class="snippet">data = data.drop('mpg','hp')</p><p class="snippet">data.show(5)</p><div class="IMG---Figure" id="_idContainer207"><img alt="" src="image/C12913_04_39.jpg"/></div><h6>Figure 4.39: New columns in DataFrame after dropping</h6></li>
				<li>Sort the data by categorical column:<p class="snippet">data = data.orderBy(data.cyl)</p><p class="snippet">data.show(5)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer208"><img alt="" src="image/C12913_04_40.jpg"/></div><h6>Figure 4.40: Sort data by categorical columns</h6></li>
				<li>Calculate the <strong class="inline">mean</strong> of the summation column for each distinct category in the <strong class="inline">categorical</strong> variable:<p class="snippet">data.groupby('cyl').agg({'colsum':'mean'}).show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer209"><img alt="" src="image/C12913_04_41.jpg"/></div><h6>Figure 4.41: Mean of the summation column</h6></li>
				<li>Filter the rows with values greater than the <strong class="inline">mean</strong> of all the <strong class="inline">mean</strong> values calculated in the previous step:<p class="snippet">data.count()#15</p><p class="snippet">cyl_avg = data.groupby('cyl').agg({'colsum':'mean'})</p><p class="snippet">avg = cyl_avg.agg({'avg(colsum)':'mean'}).toPandas().iloc[0,0]</p><p class="snippet">data = data.filter(data.colsum &gt; avg)</p><p class="snippet">data.count()</p><p class="snippet">data.show(5)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer210"><img alt="" src="image/C12913_04_42.jpg"/></div><h6>Figure 4.42: Mean of all the mean values calculated of the summation column</h6></li>
				<li>De-duplicate the resultant DataFrame to make sure it has all unique records:<p class="snippet">data = data.dropDuplicates()</p><p class="snippet">data.count()</p><p>The output is <strong class="inline">15</strong>.</p></li>
			</ol>
			<h3 id="_idParaDest-218"><a id="_idTextAnchor245"/>Activity 11: Graphs in Spark</h3>
			<ol>
				<li value="1">Import the required Python libraries in the Jupyter Notebook:<p class="snippet">import pandas as pd</p><p class="snippet">import os</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import seaborn as sns</p><p class="snippet">%matplotlib inline</p></li>
				<li>Read and show the data from the CSV file using the following command:<p class="snippet">df = pd.read_csv('mtcars.csv')</p><p class="snippet">df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer211"><img alt="" src="image/C12913_04_43.jpg"/></div><h6>Figure 4.43: Auto-mpg DataFrame</h6></li>
				<li>Visualize the discrete frequency distribution of any continuous numeric variable from your dataset using a histogram:<p class="snippet">plt.hist(df['mpg'], bins=20)</p><p class="snippet">plt.ylabel('Frequency')</p><p class="snippet">plt.xlabel('Values')</p><p class="snippet">plt.title('Frequency distribution of mpg')</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer212"><img alt="Figure 4.44: Discrete frequency distribution histogram" src="image/C12913_04_44.jpg"/></div><h6>Figure 4.44: Discrete frequency distribution histogram</h6></li>
				<li>Visualize the percentage share of the categories in the dataset using a pie chart:<p class="snippet">## Calculate count of records for each gear</p><p class="snippet">data = pd.DataFrame([[3,4,5],df['gear'].value_counts().tolist()]).T</p><p class="snippet">data.columns = ['gear','gear_counts']</p><p class="snippet">## Visualising percentage contribution of each gear in data using pie chart</p><p class="snippet">plt.pie(data.gear_counts, labels=data.gear, startangle=90, autopct='%.1f%%')</p><p class="snippet">plt.title('Percentage contribution of each gear')</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer213"><img alt="" src="image/C12913_04_45.jpg"/></div><h6>Figure 4.45: Percentage share of the categories using pie chart</h6></li>
				<li>Plot the distribution of a continuous variable across the categories of a categorical variable using a boxplot:<p class="snippet">sns.boxplot(x = 'gear', y = 'disp', data = df)</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer214"><img alt="Figure 4.46: Distribution of a continuous using boxplot" src="image/C12913_04_46.jpg"/></div><h6>Figure 4.46: Distribution of a continuous using boxplot</h6></li>
				<li>Visualize the values of a continuous numeric variable using a line chart:<p class="snippet">data = df[['hp']]</p><p class="snippet">data.plot(linestyle='-')</p><p class="snippet">plt.title('Line Chart for hp')</p><p class="snippet">plt.ylabel('Values')</p><p class="snippet">plt.xlabel('Row number')</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer215"><img alt="Figure 4.47: Continuous numeric variable using a line chart" src="image/Image54696.jpg"/></div><h6>Figure 4.47: Continuous numeric variable using a line chart</h6></li>
				<li>Plot the values of multiple continuous numeric variables on the same line chart:<p class="snippet">data = df[['hp','disp', 'mpg']]</p><p class="snippet">data.plot(linestyle='-')</p><p class="snippet">plt.title('Line Chart for hp, disp &amp; mpg')</p><p class="snippet">plt.ylabel('Values')</p><p class="snippet">plt.xlabel('Row number')</p><p class="snippet">plt.show()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer216">
					<img alt="Figure 4.48: Multiple continuous numeric variables" src="image/C12913_04_48.jpg"/>
				</div>
			</div>
			<h6>Figure 4.48: Multiple continuous numeric variables</h6>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor246"/>Chapter 05: Missing Value Handling and Correlation Analysis in Spark</h2>
			<h3 id="_idParaDest-220"><a id="_idTextAnchor247"/>Activity 12: Missing Value Handling and Correlation Analysis with PySpark DataFrames</h3>
			<ol>
				<li value="1">Import the required libraries and modules in the Jupyter notebook, as illustrated here:<p class="snippet">import findspark</p><p class="snippet">findspark.init()</p><p class="snippet">import pyspark</p><p class="snippet">import random</p></li>
				<li>Set up the <strong class="inline">SparkContext</strong> with the help of the following command in the Jupyter notebook:<p class="snippet">sc = pyspark.SparkContext(appName = "chapter5")</p></li>
				<li>Similarly, set up the <strong class="inline">SQLContext</strong> in the notebook:<p class="snippet">from pyspark.sql import SQLContext</p><p class="snippet">sqlc = SQLContext(sc)</p></li>
				<li>Now, read the CSV data into a Spark object using the following command:<p class="snippet">df = sqlc.read.format('com.databricks.spark.csv').options(header = 'true', inferschema = 'true').load('iris.csv')</p><p class="snippet">df.show(5)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer217"><img alt="Figure 5.14: Iris DataFrame, reading the CSV data into a Spark object" src="image/C12913_05_14.jpg"/></div><h6>Figure 5.14: Iris DataFrame, reading the CSV data into a Spark object</h6></li>
				<li>Fill in the missing values in the <strong class="inline">Sepallength</strong> column with the column's mean.</li>
				<li>First, calculate the mean of the <strong class="inline">Sepallength</strong> column using the following command:<p class="snippet">from pyspark.sql.functions import mean</p><p class="snippet">avg_sl = df.select(mean('Sepallength')).toPandas()['avg(Sepallength)']</p></li>
				<li>Now, impute the missing values in the <strong class="inline">Sepallength</strong> column with the column's mean, as illustrated here:<p class="snippet">y = df</p><p class="snippet">y = y.na.fill(float(avg_sl),['Sepallength'])</p><p class="snippet">y.describe().show(1)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer218"><img alt="Figure 5.15: Iris DataFrame" src="image/C12913_05_15.jpg"/></div><h6>Figure 5.15: Iris DataFrame</h6></li>
				<li>Compute the correlation matrix for the dataset. Make sure to import the required modules, as shown here:<p class="snippet">from pyspark.mllib.stat import Statistics</p><p class="snippet">import pandas as pd</p></li>
				<li>Now, fill the missing values in the DataFrame before computing the correlation:<p class="snippet">z = y.fillna(1)</p></li>
				<li>Next, remove the <strong class="inline">String</strong> columns from the PySpark DataFrame, as illustrated here:<p class="snippet">a = z.drop('Species') </p><p class="snippet">features = a.rdd.map(lambda row: row[0:])</p></li>
				<li>Now, compute the correlation matrix in Spark:<p class="snippet">correlation_matrix = Statistics.corr(features, method="pearson")</p></li>
				<li>Next, convert the correlation matrix into a pandas DataFrame using the following command:<p class="snippet">correlation_df = pd.DataFrame(correlation_matrix)</p><p class="snippet">correlation_df.index, correlation_df.columns = a.columns, a.columns</p><p class="snippet">correlation_df</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer219"><img alt="Figure 5.16: Convert the correlation matrix into a pandas DataFrame" src="image/Image54735.jpg"/></div><h6>Figure 5.16: Convert the correlation matrix into a pandas DataFrame</h6></li>
				<li>Plot the variable pairs showing strong positive correlation and fit a linear line on them.</li>
				<li>First, load the data from the Spark DataFrame into a pandas DataFrame:<p class="snippet">import pandas as pd</p><p class="snippet">dat = y.toPandas()</p><p class="snippet">type(dat)</p><p>The output is as follows:</p><p class="snippet">pandas.core.frame.DataFrame</p></li>
				<li>Next, load the required modules and plotting data using the following commands:<p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import seaborn as sns</p><p class="snippet">%matplotlib inline</p><p class="snippet">sns.lmplot(x = "Sepallength", y = "Petallength", data = dat)</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer220"><img alt="Figure 5.17: Seaborn plot for x = “Sepallength”, y = “Petallength”" src="image/Image54744.jpg"/></div><h6>Figure 5.17: Seaborn plot for x = "Sepallength", y = "Petallength"</h6></li>
				<li>Plot the graph so that <strong class="inline">x</strong> equals <strong class="inline">Sepallength</strong>, and <strong class="inline">y</strong> equals <strong class="inline">Petalwidth</strong>:<p class="snippet">import seaborn as sns</p><p class="snippet">sns.lmplot(x = "Sepallength", y = "Petalwidth", data = dat)</p><p class="snippet">plt.show()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer221"><img alt="Figure 5.18: Seaborn plot for x = “Sepallength”, y = “Petalwidth”" src="image/Image54754.jpg"/></div><h6>Figure 5.18: Seaborn plot for x = "Sepallength", y = "Petalwidth"</h6></li>
				<li>Plot the graph so that <strong class="inline">x</strong> equals <strong class="inline">Petalwidth</strong> and <strong class="inline">y</strong> equals <strong class="inline">Petalwidth</strong>:<p class="snippet">sns.lmplot(x = "Petallength", y = "Petalwidth", data = dat)</p><p class="snippet">plt.show()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer222">
					<img alt="Figure 5.19: Seaborn plot for x = “Petallength”, y = “Petalwidth”" src="image/C12913_05_19.jpg"/>
				</div>
			</div>
			<h6>Figure 5.19: Seaborn plot for x = "Petallength", y = "Petalwidth"</h6>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor248"/>Chapter 6: Business Process Definition and Exploratory Data Analysis</h2>
			<h3 id="_idParaDest-222"><a id="_idTextAnchor249"/>Activity 13: Carry Out Mapping to Gaussian Distribution of Numeric Features from the Given Data</h3>
			<ol>
				<li value="1">Download the <strong class="inline">bank.csv</strong>. Now, use the following commands to read the data from it:<p class="snippet">import numpy as np</p><p class="snippet">import pandas as pd</p><p class="snippet">import seaborn as sns</p><p class="snippet">import time</p><p class="snippet">import re</p><p class="snippet">import os</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">sns.set(style="ticks")</p><p class="snippet"># import libraries required for preprocessing</p><p class="snippet">import sklearn as sk</p><p class="snippet">from scipy import stats</p><p class="snippet">from sklearn import preprocessing</p><p class="snippet"># set the working directory to the following</p><p class="snippet">os.chdir("/Users/svk/Desktop/packt_exercises")</p><p class="snippet"># read the downloaded input data (marketing data)</p><p class="snippet">df = pd.read_csv('bank.csv', sep=';')</p></li>
				<li>Identify the numeric data from the DataFrame. The data can be categorized according to its type, such as categorical, numeric (float, integer), date, and so on. We identify numeric data here because we can only carry out normalization on numeric data:<p class="snippet">numeric_df = df._get_numeric_data()</p><p class="snippet">numeric_df.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer223"><img alt="Figure 6.12: DataFrame" src="image/C12913_06_12.jpg"/></div><h6>Figure 6.12: DataFrame</h6></li>
				<li>Carry out a normality test and identify the features that have a non-normal distribution:<p class="snippet">numeric_df_array = np.array(numeric_df) # converting to numpy arrays for more efficient computation</p><p class="snippet">loop_c = -1</p><p class="snippet">col_for_normalization = list()</p><p class="snippet">for column in numeric_df_array.T:</p><p class="snippet">    loop_c+=1</p><p class="snippet">    x = column</p><p class="snippet">    k2, p = stats.normaltest(x) </p><p class="snippet">    alpha = 0.001</p><p class="snippet">    print("p = {:g}".format(p))</p><p class="snippet">        </p><p class="snippet">    # rules for printing the normality output</p><p class="snippet">    if p &lt; alpha:</p><p class="snippet">        test_result = "non_normal_distr"</p><p class="snippet">        col_for_normalization.append((loop_c)) # applicable if yeo-johnson is used</p><p class="snippet">        </p><p class="snippet">        #if min(x) &gt; 0: # applicable if box-cox is used</p><p class="snippet">            #col_for_normalization.append((loop_c)) # applicable if box-cox is used</p><p class="snippet">        print("The null hypothesis can be rejected: non-normal distribution")</p><p class="snippet">        </p><p class="snippet">    else:</p><p class="snippet">        test_result = "normal_distr"</p><p class="snippet">        print("The null hypothesis cannot be rejected: normal distribution")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer224"><img alt="" src="image/C12913_06_13.jpg"/></div><h6>Figure 6.13: Normality test and identify the features</h6><h4>Note</h4><p class="callout">The normality test conducted here is based on D'Agostino and Pearson's test (<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html</a>), which combines skew and kurtosis to identify how close the distribution of the features is to a Gaussian distribution. In this test, if the p-value is less than the set alpha value, then the null hypothesis is rejected, and the feature does not have a normal distribution. Here, we look into each column using a loop function and identify the distribution of each feature.</p></li>
				<li>Plot the probability density of the features to visually analyze their distribution:<p class="snippet">columns_to_normalize = numeric_df[numeric_df.columns[col_for_normalization]]</p><p class="snippet">names_col = list(columns_to_normalize)</p><p class="snippet"># density plots of the features to check the normality</p><p class="snippet">columns_to_normalize.plot.kde(bw_method=3)</p><p>The density plot of the features to check the normality is as follows:</p><div class="IMG---Figure" id="_idContainer225"><img alt="Figure 6.14: Plot of features" src="image/C12913_06_14.jpg"/></div><h6>Figure 6.14: Plot of features</h6><h4>Note</h4><p class="callout">Multiple variables' density plots are shown in the previous graph. The distribution of the features in the graph can be seen with a high positive kurtosis, which is not a normal distribution.</p></li>
				<li>Prepare the power transformation model and carry out transformations on the identified features to convert them to normal distribution based on the <strong class="inline">box-cox</strong> or <strong class="inline">yeo-johnson</strong> method:<p class="snippet">pt = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True, copy=True)</p><p class="snippet">normalized_columns = pt.fit_transform(columns_to_normalize)</p><p class="snippet">normalized_columns = pd.DataFrame(normalized_columns, columns=names_col)</p><p>In the previous commands, we prepare the power transformation model and apply it to the data of selected features.</p></li>
				<li>Plot the probability density of the features again after the transformations to visually analyze the distribution of the features:<p class="snippet">normalized_columns.plot.kde(bw_method=3)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer226">
					<img alt="Figure 6.15: Plot of features" src="image/C12913_06_15.jpg"/>
				</div>
			</div>
			<h6>Figure 6.15: Plot of features</h6>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor250"/>Chapter 07: Reproducibility in Big Data Analysis</h2>
			<h3 id="_idParaDest-224"><a id="_idTextAnchor251"/>Activity 14: Test normality of data attributes (columns) and carry out Gaussian normalization of non-normally distributed attributes</h3>
			<ol>
				<li value="1">Import the required libraries and packages in the Jupyter notebook:<p class="snippet">import numpy as np</p><p class="snippet">import pandas as pd</p><p class="snippet">import seaborn as sns</p><p class="snippet">import time</p><p class="snippet">import re</p><p class="snippet">import os</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">sns.set(style="ticks")</p></li>
				<li>Now, import the libraries required for preprocessing:<p class="snippet">import sklearn as sk</p><p class="snippet">from scipy import stats</p><p class="snippet">from sklearn import preprocessing</p></li>
				<li>Set the working directory using the following command:<p class="snippet">os.chdir("/Users/svk/Desktop/packt_exercises")</p></li>
				<li>Now, import the dataset into the Spark object:<p class="snippet">df = pd.read_csv('bank.csv', sep=';')</p></li>
				<li>Identify the target variable in the data:<p class="snippet">DV = 'y'</p><p class="snippet">df[DV]= df[DV].astype('category')</p><p class="snippet">df[DV] = df[DV].cat.codes</p></li>
				<li>Generate training and testing data using the following command:<p class="snippet">msk = np.random.rand(len(df)) &lt; 0.8</p><p class="snippet">train = df[msk]</p><p class="snippet">test = df[~msk]</p></li>
				<li>Create the <strong class="inline">Y</strong> and <strong class="inline">X</strong> data, as illustrated here:<p class="snippet"># selecting the target variable (dependent variable) as y</p><p class="snippet">y_train = train[DV]</p></li>
				<li>Drop the <strong class="inline">DV</strong> or <strong class="inline">y</strong> using the <strong class="inline">drop</strong> command:<p class="snippet">train = train.drop(columns=[DV])</p><p class="snippet">train.head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer227"><img alt="Figure 7.22: Bank dataset" src="image/C12913_07_22.jpg"/></div><h6>Figure 7.22: Bank dataset</h6></li>
				<li>Segment the data numerically and categorically and perform distribution transformation on the numeric data:<p class="snippet">numeric_df = train._get_numeric_data()</p><p>Perform data preprocessing on the data.</p></li>
				<li>Now, create a <strong class="inline">loop</strong> to identify the columns with a non-normal distribution using the following command (converting to NumPy arrays for more efficient computation):<p class="snippet">numeric_df_array = np.array(numeric_df)</p><p class="snippet">loop_c = -1</p><p class="snippet">col_for_normalization = list()</p><p class="snippet">for column in numeric_df_array.T:</p><p class="snippet">    loop_c+=1</p><p class="snippet">    x = column</p><p class="snippet">    k2, p = stats.normaltest(x) </p><p class="snippet">    alpha = 0.001</p><p class="snippet">    print("p = {:g}".format(p))</p><p class="snippet">        </p><p class="snippet">    # rules for printing the normality output</p><p class="snippet">    if p &lt; alpha:</p><p class="snippet">        test_result = "non_normal_distr"</p><p class="snippet">        col_for_normalization.append((loop_c)) # applicable if yeo-johnson is used</p><p class="snippet">        </p><p class="snippet">        #if min(x) &gt; 0: # applicable if box-cox is used</p><p class="snippet">            #col_for_normalization.append((loop_c)) # applicable if box-cox is used</p><p class="snippet">        print("The null hypothesis can be rejected: non-normal distribution")</p><p class="snippet">        </p><p class="snippet">    else:</p><p class="snippet">        test_result = "normal_distr"</p><p class="snippet">        print("The null hypothesis cannot be rejected: normal distribution")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer228"><img alt="Figure 7.23: Identifying the columns with a non-linear distribution" src="image/C12913_07_23.jpg"/></div><h6>Figure 7.23: Identifying the columns with a non-linear distribution</h6></li>
				<li>Create a <strong class="inline">PowerTransformer</strong> based transformation (<strong class="inline">box-cox</strong>):<p class="snippet">pt = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True, copy=True)</p><h4>Note</h4><p class="callout"><strong class="inline">box-cox</strong> can handle only positive values.</p></li>
				<li>Apply the power transformation model on the data. Select the columns to normalize:<p class="snippet">columns_to_normalize = numeric_df[numeric_df.columns[col_for_normalization]]</p><p class="snippet">names_col = list(columns_to_normalize)</p></li>
				<li>Create a density plot to check the normality:<p class="snippet">columns_to_normalize.plot.kde(bw_method=3)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer229"><img alt="Figure 7.24: Density plot to check the normality" src="image/Image54824.jpg"/></div><h6>Figure 7.24: Density plot to check the normality</h6></li>
				<li>Now, transform the columns to a normal distribution using the following command:<p class="snippet">normalized_columns = pt.fit_transform(columns_to_normalize)</p><p class="snippet">normalized_columns = pd.DataFrame(normalized_columns, columns=names_col)</p></li>
				<li>Again, create a density plot to check the normality:<p class="snippet">normalized_columns.plot.kde(bw_method=3)</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer230"><img alt="Figure 7.25: Another density plot to check the normality" src="image/Image54838.jpg"/></div><h6>Figure 7.25: Another density plot to check the normality</h6></li>
				<li>Use a <strong class="inline">loop</strong> to identify the columns with non-normal distribution on the transformed data:<p class="snippet">numeric_df_array = np.array(normalized_columns) </p><p class="snippet">loop_c = -1</p><p class="snippet">for column in numeric_df_array.T:</p><p class="snippet">    loop_c+=1</p><p class="snippet">    x = column</p><p class="snippet">    k2, p = stats.normaltest(x) </p><p class="snippet">    alpha = 0.001</p><p class="snippet">    print("p = {:g}".format(p))</p><p class="snippet">        </p><p class="snippet">    # rules for printing the normality output</p><p class="snippet">    if p &lt; alpha:</p><p class="snippet">        test_result = "non_normal_distr"</p><p class="snippet">        print("The null hypothesis can be rejected: non-normal distribution")</p><p class="snippet">        </p><p class="snippet">    else:</p><p class="snippet">        test_result = "normal_distr"</p><p class="snippet">        print("The null hypothesis cannot be rejected: normal distribution")</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer231"><img alt="Figure 7.26: Power transformation model to data" src="image/Image54847.jpg"/></div><h6>Figure 7.26: Power transformation model to data</h6></li>
				<li>Bind the normalized and non-normalized columns. Select the columns not to normalize:<p class="snippet">columns_to_notnormalize = numeric_df</p><p class="snippet">columns_to_notnormalize.drop(columns_to_notnormalize.columns[col_for_normalization], axis=1, inplace=True)</p></li>
				<li>Use the following command to bind both the non-normalized and normalized columns:<p class="snippet">numeric_df_normalized = pd.concat([columns_to_notnormalize.reset_index(drop=True), normalized_columns], axis=1)</p><p class="snippet">numeric_df_normalized</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer232">
					<img alt="Figure 7.27: Non-normalized and normalized columns" src="image/C12913_07_27.jpg"/>
				</div>
			</div>
			<h6>Figure 7.27: Non-normalized and normalized columns</h6>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor252"/>Chapter 08: Creating a Full Analysis Report</h2>
			<h3 id="_idParaDest-226"><a id="_idTextAnchor253"/>Activity 15: Generating Visualization Using Plotly</h3>
			<ol>
				<li value="1">Import all the required libraries and packages into the Jupyter notebook. Make sure to read the data from <strong class="inline">bank.csv</strong> into the Spark DataFrame.</li>
				<li>Import the libraries for Plotly, as illustrated here:<p class="snippet">import plotly.graph_objs as go</p><p class="snippet">from plotly.plotly import iplot</p><p class="snippet">import plotly as py</p></li>
				<li>Now, for visualization in Plotly, we need to initiate an offline session. Use the following command (requires version &gt;= 1.9.0):<p class="snippet">from plotly import __version__</p><p class="snippet">from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot</p><p class="snippet">print(__version__)</p></li>
				<li>Now Plotly is initiated offline. Use the following command to start a Plotly notebook:<p class="snippet">import plotly.plotly as py</p><p class="snippet">import plotly.graph_objs as go</p><p class="snippet">init_notebook_mode(connected=True)</p><p>After starting the Plotly notebook, we can use Plotly to generate many types of graphs, such as a bar graph, a boxplot, or a scatter plot, and convert the entire output into a user interface or an app that is supported by Python's Flask framework.</p></li>
				<li>Now, plot each graph using Plotly:<p>Bar graph:</p><p class="snippet">df = pd.read_csv('bank.csv', sep=';')</p><p class="snippet">data = [go.Bar(x=df.y,</p><p class="snippet">            y=df.balance)]</p><p class="snippet">py.iplot(data)</p><p>The bar graph is as follows:</p><div class="IMG---Figure" id="_idContainer233"><img alt="" src="image/C12913_08_18.jpg"/></div></li>
			</ol>
			<h6>Figure 8.18: Bar graph</h6>
			<p>Scatter plot:</p>
			<p class="snippet">py.iplot([go.Histogram2dContour(x=df.balance, y=df.age, contours=dict(coloring='heatmap')),</p>
			<p class="snippet">       go.Scatter(x=df.balance, y=df.age, mode='markers', marker=dict(color='red', size=8, opacity=0.3))], show_link=False)</p>
			<p>The scatter plot is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer234">
					<img alt="Figure 8.19: Scatter plot" src="image/C12913_08_19.jpg"/>
				</div>
			</div>
			<h6>Figure 8.19: Scatter plot</h6>
			<p>Boxplot:</p>
			<p class="snippet">plot1 = go.Box(</p>
			<p class="snippet">    y=df.age,</p>
			<p class="snippet">    name = 'age of the customers',</p>
			<p class="snippet">    marker = dict(</p>
			<p class="snippet">        color = 'rgb(12, 12, 140)',</p>
			<p class="snippet">    )</p>
			<p class="snippet">)</p>
			<p class="snippet">py.iplot([plot1])</p>
			<p>The boxplot is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer235">
					<img alt="Figure 8.20: Boxplot" src="image/C12913_08_20.jpg"/>
				</div>
			</div>
			<h6>Figure 8.20: Boxplot</h6>
		</div>
	</body></html>