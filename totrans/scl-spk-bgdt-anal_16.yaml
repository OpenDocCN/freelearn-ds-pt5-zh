- en: Spark Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*"Harpists spend 90 percent of their lives tuning their harps and 10 percent
    playing out of tune."*'
  prefs: []
  type: TYPE_NORMAL
- en: '- Igor Stravinsky'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will dig deeper into Apache Spark internals and see that
    while Spark is great in making us feel like we are using just another Scala collection,
    we don''t have to forget that Spark actually runs in a distributed system. Therefore,
    some extra care should be taken. In a nutshell, the following topics will be covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Spark jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common mistakes in Spark app development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring Spark jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark provides web UI for monitoring all the jobs running or completed on computing
    nodes (drivers or executors). In this section, we will discuss in brief how to
    monitor Spark jobs using Spark web UI with appropriate examples. We will see how
    to monitor the progress of jobs (including submitted, queued, and running jobs).
    All the tabs in the Spark web UI will be discussed briefly. Finally, we will discuss
    the logging procedure in Spark for better tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Spark web interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The web UI (also known as Spark UI) is the web interface for running Spark applications
    to monitor the execution of jobs on a web browser such as Firefox or Google Chrome.
    When a SparkContext launches, a web UI that displays useful information about
    the application gets started on port 4040 in standalone mode. The Spark web UI
    is available in different ways depending on whether the application is still running
    or has finished its execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, you can use the web UI after the application has finished its execution
    by persisting all the events using `EventLoggingListener`. The `EventLoggingListener`,
    however, cannot work alone, and the incorporation of the Spark history server
    is required. Combining these two features, the following facilities can be achieved:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of scheduler stages and tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summary of RDD sizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about the running executors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can access the UI at `http://<driver-node>:4040` in a web browser. For example,
    a Spark job submitted and running as a standalone mode can be accessed at `http://localhost:4040`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if multiple SparkContexts are running on the same host, they will
    bind to successive ports beginning with 4040, 4041, 4042, and so on. By default,
    this information will be available for the duration of your Spark application
    only. This means that when your Spark job finishes its execution, the binding
    will no longer be valid or accessible.
  prefs: []
  type: TYPE_NORMAL
- en: As long as the job is running, stages can be observed on Spark UI. However,
    to view the web UI after the job has finished the execution, you could try setting
    `spark.eventLog.enabled` as true before submitting your Spark jobs. This forces
    Spark to log all the events to be displayed in the UI that are already persisted
    on storage such as local filesystem or HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we saw how to submit a Spark job to a cluster. Let''s
    reuse one of the commands for submitting the k-means clustering, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you submit the job using the preceding command, you will not be able to
    see the status of the jobs that have finished the execution, so to make the changes
    permanent, use the following two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: By setting the preceding two configuration variables, we asked the Spark driver
    to make the event logging enabled to be saved at `file:///home/username/log`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, with the following changes, your submitting command will be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00201.jpeg)**Figure 1:** Spark web UI'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding screenshot, Spark web UI provides the following tabs:'
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is to be noted that all the features may not be visible at once as they are
    lazily created on demand, for example, while running a streaming job.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending upon the SparkContext, the Jobs tab shows the status of all the Spark
    jobs in a Spark application. When you access the Jobs tab on the Spark UI using
    a web browser at `http://localhost:4040` (for standalone mode), you should observe
    the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: This shows the active user who has submitted the Spark job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Total Uptime: This shows the total uptime for the jobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scheduling Mode: In most cases, it is first-in-first-out (aka FIFO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Active Jobs: This shows the number of active jobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Completed Jobs: This shows the number of completed jobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Event Timeline: This shows the timeline of a job that has completed its execution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Internally, the Jobs tab is represented by the `JobsTab` class, which is a
    custom SparkUI tab with the jobs prefix. The Jobs tab uses `JobProgressListener`
    to access statistics about the Spark jobs to display the above information on
    the page. Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00135.jpeg)**Figure 2:** The jobs tab in the Spark web UI'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you further expand the Active Jobs option in the Jobs tab, you will be able
    to see the execution plan, status, number of completed stages, and the job ID
    of that particular job as DAG Visualization, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00185.jpeg)**Figure 3:** The DAG visualization for task in the Spark
    web UI (abridged)'
  prefs: []
  type: TYPE_IMG
- en: When a user enters the code in the Spark console (for example, Spark shell or
    using Spark submit), Spark Core creates an operator graph. This is basically what
    happens when a user executes an action (for example, reduce, collect, count, first,
    take, countByKey, saveAsTextFile) or transformation (for example, map, flatMap,
    filter, mapPartitions, sample, union, intersection, distinct) on an RDD (which
    are immutable objects) at a particular node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00187.jpeg)**Figure 4:** DAG scheduler transforming RDD lineage into
    stage DAG'
  prefs: []
  type: TYPE_NORMAL
- en: During the transformation or action, **Directed Acyclic Graph** (**DAG**) information
    is used to restore the node to last transformation and actions (refer to *Figure
    4* and *Figure 5* for a clearer picture) to maintain the data resiliency. Finally,
    the graph is submitted to a DAG scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: How does Spark compute the DAG from the RDD and subsequently execute the task?
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, when any action is called on the RDD, Spark creates the DAG
    and submits it to the DAG scheduler. The DAG scheduler divides operators into
    stages of tasks. A stage comprises tasks based on partitions of the input data.
    The DAG scheduler pipelines operators together. For example, many map operators
    can be scheduled in a single stage. The final result of a DAG scheduler is a set
    of stages. The stages are passed on to the task scheduler. The task scheduler
    launches tasks through the cluster manager (Spark Standalone/YARN/Mesos). The
    task scheduler doesn't know about the dependencies of the stages. The worker executes
    the tasks on the stage.
  prefs: []
  type: TYPE_NORMAL
- en: The DAG scheduler then keeps track of which RDDs the stage outputs materialized
    from. It then finds a minimal schedule to run jobs and divides the related operators
    into stages of tasks. Based on partitions of the input data, a stage comprises
    multiple tasks. Then, operators are pipelined together with the DAG scheduler.
    Practically, more than one map or reduce operator (for example) can be scheduled
    in a single stage.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00191.jpeg)**Figure 5:** Executing action leads to new ResultStage
    and ActiveJob in DAGScheduler'
  prefs: []
  type: TYPE_NORMAL
- en: Two fundamental concepts in DAG scheduler are jobs and stages. Thus, it has
    to track them through internal registries and counters. Technically speaking,
    DAG scheduler is a part of SparkContext's initialization that works exclusively
    on the driver (immediately after the task scheduler and scheduler backend are
    ready). DAG scheduler is responsible for three major tasks in Spark execution.
    It computes an execution DAG, that is, DAG of stages, for a job. It determines
    the preferred node to run each task on and handles failures due to shuffle output
    files being lost.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00193.jpeg)**Figure 6:** DAGScheduler as created by SparkContext with
    other services'
  prefs: []
  type: TYPE_NORMAL
- en: The final result of a DAG scheduler is a set of stages. Therefore, most of the
    statistics and the status of the job can be seen using this visualization, for
    example, execution plan, status, number of completed stages, and the job ID of
    that particular job.
  prefs: []
  type: TYPE_NORMAL
- en: Stages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Stages tab in Spark UI shows the current status of all stages of all jobs
    in a Spark application, including two optional pages for the tasks and statistics
    for a stage and pool details. Note that this information is available only when
    the application works in a fair scheduling mode. You should be able to access
    the Stages tab at `http://localhost:4040/stages`. Note that when there are no
    jobs submitted, the tab shows nothing but the title. The Stages tab shows the
    stages in a Spark application. The following stages can be seen in this tab:'
  prefs: []
  type: TYPE_NORMAL
- en: Active Stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pending Stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completed Stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, when you submit a Spark job locally, you should be able to see
    the following status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00265.jpeg)**Figure 7:** The stages for all jobs in the Spark web UI'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, there's only one stage that is an active stage. However, in the
    upcoming chapters, we will be able to observe other stages when we will submit
    our Spark jobs to AWS EC2 clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further dig down to the summary of the completed jobs, click on any link
    contained in the Description column and you should find the related statistics
    on execution time as metrics. An approximate time of min, median, 25th percentile,
    75th percentile, and max for the metrics can also be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00378.jpeg)**Figure 8:** The summary for completed jobs on the Spark
    web UI'
  prefs: []
  type: TYPE_NORMAL
- en: Your case might be different as I have executed and submitted only two jobs
    for demonstration purposes during the writing of this book. You can see other
    statistics on the executors as well. For my case, I submitted these jobs in the
    standalone mode by utilizing 8 cores and 32 GB of RAM. In addition to these, information
    related to the executor, such as ID, IP address with the associated port number,
    task completion time, number of tasks (including number of failed tasks, killed
    tasks, and succeeded tasks), and input size of the dataset per records are shown.
  prefs: []
  type: TYPE_NORMAL
- en: The other section in the image shows other information related to these two
    tasks, for example, index, ID, attempts, status, locality level, host information,
    launch time, duration, **Garbage Collection** (**GC**) time, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Storage tab shows the size and memory use for each RDD, DataFrame, or Dataset.
    You should be able to see the storage-related information of RDDs, DataFrames,
    or Datasets. The following figure shows storage metadata such as RDD name, storage
    level, the number of cache partitions, the percentage of a fraction of the data
    that was cached, and the size of the RDD in the main memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00229.jpeg)**Figure 9:** Storage tab shows space consumed by an RDD
    in disk'
  prefs: []
  type: TYPE_NORMAL
- en: Note that if the RDD cannot be cached in the main memory, disk space will be
    used instead. A more detailed discussion will be carried out in a later section
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00067.jpeg)**Figure 10:** Data distribution and the storage used by
    the RDD in disk'
  prefs: []
  type: TYPE_NORMAL
- en: Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Environment tab shows the environmental variables that are currently set
    on your machine (that is, driver). More specifically, runtime information such
    as Java Home, Java Version, and Scala Version can be seen under Runtime Information.
    Spark properties such as Spark application ID, app name, and driver host information,
    driver port, executor ID, master URL, and the schedule mode can be seen. Furthermore,
    other system-related properties and job properties such as AWT toolkit version,
    file encoding type (for example, UTF-8), and file encoding package information
    (for example, sun.io) can be seen under System Properties.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00249.jpeg)**Figure 11:** Environment tab on Spark web UI'
  prefs: []
  type: TYPE_NORMAL
- en: Executors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Executors tab uses `ExecutorsListener` to collect information about executors
    for a Spark application. An executor is a distributed agent that is responsible
    for executing tasks. Executors are instantiated in different ways. For example,
    they can be instantiated when `CoarseGrainedExecutorBackend` receives `RegisteredExecutor`
    message for Spark Standalone and YARN. The second case is when a Spark job is
    submitted to Mesos. The Mesos''s `MesosExecutorBackend` gets registered. The third
    case is when you run your Spark jobs locally, that is, `LocalEndpoint` is created.
    An executor typically runs for the entire lifetime of a Spark application, which
    is called static allocation of executors, although you can also opt in for dynamic
    allocation. The executor backends exclusively manage all the executors in a computing
    node or clusters. An executor reports heartbeat and partial metrics for active
    tasks to the **HeartbeatReceiver** RPC endpoint on the driver periodically and
    the results are sent to the driver. They also provide in-memory storage for RDDs
    that are cached by user programs through block manager. Refer to the following
    figure for a clearer idea on this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.jpeg)**Figure 12:** Spark driver instantiates an executor that
    is responsible for HeartbeatReceiver''s Heartbeat message handler'
  prefs: []
  type: TYPE_NORMAL
- en: 'When an executor starts, it first registers with the driver and communicates
    directly to execute tasks, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00009.jpeg)**Figure 13:** Launching tasks on executor using TaskRunners'
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to access the Executors tab at `http://localhost:4040/executors`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00084.jpeg)**Figure 14:** Executor tab on Spark web UI'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding figure, Executor ID, Address, Status, RDD Blocks,
    Storage Memory, Disk Used, Cores, Active Tasks, Failed Tasks, Complete Tasks,
    Total Tasks, Task Time (GC Time), Input, Shuffle Read, Shuffle Write, and Thread
    Dump about the executor can be seen.
  prefs: []
  type: TYPE_NORMAL
- en: SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SQL tab in the Spark UI displays all the accumulator values per operator.
    You should be able to access the SQL tab at `http://localhost:4040/SQL/`. It displays
    all the SQL query executions and underlying information by default. However, the
    SQL tab displays the details of the SQL query execution only after a query has
    been selected.
  prefs: []
  type: TYPE_NORMAL
- en: A detailed discussion on SQL is out of the scope of this chapter. Interested
    readers should refer to [http://spark.apache.org/docs/latest/sql-programming-guide.html#sql](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)
    for more on how to submit an SQL query and see its result output.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Spark application using web UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When a Spark job is submitted for execution, a web application UI is launched
    that displays useful information about the application. An event timeline displays
    the relative ordering and interleaving of application events. The timeline view
    is available on three levels: across all jobs, within one job, and within one
    stage. The timeline also shows executor allocation and deallocation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00325.jpeg)**Figure 15:** Spark jobs executed as DAG on Spark web UI'
  prefs: []
  type: TYPE_NORMAL
- en: Observing the running and completed Spark jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To access and observe the running and the completed Spark jobs, open `http://spark_driver_host:4040`
    in a web browser. Note that you will have to replace `spark_driver_host` with
    an IP address or hostname accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if multiple SparkContexts are running on the same host, they will
    bind to successive ports beginning with 4040, 4041, 4042, and so on. By default,
    this information will be available for the duration of your Spark application
    only. This means that when your Spark job finishes its execution, the binding
    will no longer be valid or accessible.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to access the active jobs that are still executing, click on the Active
    Jobs link and you will see the related information of those jobs. On the other
    hand, to access the status of the completed jobs, click on Completed Jobs and
    you will see the information as DAG style as discussed in the preceding section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00129.jpeg)**Figure 16:** Observing the running and completed Spark
    jobs'
  prefs: []
  type: TYPE_NORMAL
- en: You can achieve these by clicking on the job description link under the Active
    Jobs or Completed Jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark applications using logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Seeing the information about all running Spark applications depends on which
    cluster manager you are using. You should follow these instructions while debugging
    your Spark application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark Standalone**: Go to the Spark master UI at `http://master:18080`. The
    master and each worker show cluster and the related job statistics. In addition,
    a detailed log output for each job is also written to the working directory of
    each worker. We will discuss how to enable the logging manually using the `log4j`
    with Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YARN**: If your cluster manager is YARN, and suppose that you are running
    your Spark jobs on the Cloudera (or any other YARN-based platform), then go to
    the YARN applications page in the Cloudera Manager Admin Console. Now, to debug
    Spark applications running on YARN, view the logs for the Node Manager role. To
    make this happen, open the log event viewer and then filter the event stream to
    choose a time window and log level and to display the Node Manager source. You
    can access logs through the command as well. The format of the command is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the following are the valid commands for these IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that the user IDs are different. However, this is only true if `yarn.log-aggregation-enable`
    is true in `yarn-site.xml` and the application has already finished the execution.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with log4j with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark uses `log4j` for its own logging. All the operations that happen backend
    get logged to the Spark shell console (which is already configured to the underlying
    storage). Spark provides a template of `log4j` as a property file, and we can
    extend and modify that file for logging in Spark. Move to the `SPARK_HOME/conf`
    directory and you should see the `log4j.properties.template` file. This could
    help us as the starting point for our own logging system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create our own custom logging system while running a Spark job.
    When you are done, rename the file as `log4j.properties` and put it under the
    same directory (that is, project tree). A sample snapshot of the file can be seen
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00112.jpeg)**Figure 17:** A snap of the log4j.properties file'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, everything goes to console and file. However, if you want to bypass
    all the noiser logs to a system file located at, say, `/var/log/sparkU.log`, then
    you can set these properties in the `log4j.properties` file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Basically, we want to hide all logs Spark generates so that we don't have to
    deal with them in the shell. We redirect them to be logged in the filesystem.
    On the other hand, we want our own logs to be logged in the shell and a separate
    file so that they don't get mixed up with the ones from Spark. From here, we will
    point Splunk to the files where our own logs are, which in this particular case
    is `/var/log/sparkU.log`*.*
  prefs: []
  type: TYPE_NORMAL
- en: Then the`log4j.properties` file is picked up by Spark when the application starts,
    so we don't have to do anything aside from placing it in the mentioned location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see how we can create our own logging system. Look at the following
    code and try to understand what is happening here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code conceptually logs only the warning message. It first prints
    the warning message and then creates an RDD by parallelizing numbers from 1 to
    100,000\. Once the RDD job is finished, it prints another warning log. However,
    there is a problem we haven't noticed yet with the earlier code segment.
  prefs: []
  type: TYPE_NORMAL
- en: 'One drawback of the `org.apache.log4j.Logger` class is that it is not serializable
    (refer to the optimization technique section for more details), which implies
    that we cannot use it inside a *closure* while doing operations on some parts
    of the Spark API. For example, if you try to execute the following code, you should
    experience an exception that says Task not serializable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To solve this problem is also easy; just declare the Scala object with `extends
    Serializable` and now the code looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So what is happening in the preceding code is that the closure can't be neatly
    distributed to all partitions since it can't close on the logger; hence, the whole
    instance of type `MyMapper` is distributed to all partitions; once this is done,
    each partition creates a new logger and uses it for logging.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the following is the complete code that helps us to get rid of
    this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We will discuss the built-in logging of Spark in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Spark configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of ways to configure your Spark jobs. In this section, we
    will discuss these ways. More specifically, according to Spark 2.x release, there
    are three locations to configure the system:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark properties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed previously, Spark properties control most of the application-specific
    parameters and can be set using a `SparkConf` object of Spark. Alternatively,
    these parameters can be set through the Java system properties. `SparkConf` allows
    you to configure some of the common properties as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'An application can be configured to use a number of available cores on your
    machine. For example, we could initialize an application with two threads as follows.
    Note that we run with `local [2]`, meaning two threads, which represents minimal
    parallelism and using `local [*]`, which utilizes all the available cores in your
    machine. Alternatively, you can specify the number of executors while submitting
    Spark jobs with the following spark-submit script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There might be some special cases where you need to load Spark properties dynamically
    when required. You can do this while submitting a Spark job through the spark-submit
    script. More specifically, you may want to avoid hardcoding certain configurations
    in `SparkConf`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark precedence:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark has the following precedence on the submitted jobs: configs coming from
    a config file have the lowest priority. The configs coming from the actual code
    have higher priority with respect to configs coming from a config file, and configs
    coming from the CLI through the Spark-submit script have higher priority.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you want to run your application with different masters, executors,
    or different amounts of memory, Spark allows you to simply create an empty configuration
    object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can provide the configuration for your Spark job at runtime as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`SPARK_HOME/bin/spark-submit` will also read configuration options from `SPARK_HOME
    /conf/spark-defaults.conf`, in which each line consists of a key and a value separated
    by whitespace. An example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Values that are specified as flags in the properties file will be passed to
    the application and merged with those ones specified through `SparkConf`. Finally,
    as discussed earlier, the application web UI at `http://<driver>:4040` lists all
    the Spark properties under the Environment tab.
  prefs: []
  type: TYPE_NORMAL
- en: Environmental variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Environment variables can be used to set the setting in the computing nodes
    or machine settings. For example, IP address can be set through the `conf/spark-env.sh`
    script on each computing node. The following table lists the name and the functionality
    of the environmental variables that need to be set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00116.gif)**Figure 18:** Environmental variables and their meaning'
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, logging can be configured through the `log4j.properties` file under
    your Spark application tree, as discussed in the preceding section. Spark uses
    log4j for logging. There are several valid logging levels supported by log4j with
    Spark; they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Log Level** | **Usages** |'
  prefs: []
  type: TYPE_TB
- en: '| OFF | This is the most specific, which allows no logging at all |'
  prefs: []
  type: TYPE_TB
- en: '| FATAL | This is the most specific one that shows fatal errors with little
    data |'
  prefs: []
  type: TYPE_TB
- en: '| ERROR | This shows only the general errors |'
  prefs: []
  type: TYPE_TB
- en: '| WARN | This shows warnings that are recommended to be fixed but not mandatory
    |'
  prefs: []
  type: TYPE_TB
- en: '| INFO | This shows information required for your Spark job |'
  prefs: []
  type: TYPE_TB
- en: '| DEBUG | While debugging, those logs will be printed |'
  prefs: []
  type: TYPE_TB
- en: '| TRACE | This provides the least specific error trace with a lot of data |'
  prefs: []
  type: TYPE_TB
- en: '| ALL | Least specific message with all data |'
  prefs: []
  type: TYPE_TB
- en: '**Table 1:** Log level with log4j and Spark'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can set up the default logging for Spark shell in `conf/log4j.properties`.
    In standalone Spark applications or while in a Spark Shell session, use `conf/log4j.properties.template`
    as a starting point. In an earlier section of this chapter, we suggested you put
    the `log4j.properties` file under your project directory while working on an IDE-based
    environment like Eclipse. However, to disable logging completely, you should use
    the following `conf/log4j.properties.template` as `log4j.properties` . Just set
    the `log4j.logger.org` flags as OFF, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will discuss some common mistakes made by the developer
    or programmer while developing and submitting Spark jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Common mistakes in Spark app development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Common mistakes that happen often are application failure, a slow job that gets
    stuck due to numerous factors, mistakes in the aggregation, actions or transformations,
    an exception in the main thread and, of course, **Out Of Memory** (**OOM**).
  prefs: []
  type: TYPE_NORMAL
- en: Application failure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the time, application failure happens because one or more stages fail
    eventually. As discussed earlier in this chapter, Spark jobs comprise several
    stages. Stages aren''t executed independently: for instance, a processing stage
    can''t take place before the relevant input-reading stage. So, suppose that stage
    1 executes successfully but stage 2 fails to execute, the whole application fails
    eventually. This can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00119.jpeg)**Figure 19:** Two stages in a typical Spark job'
  prefs: []
  type: TYPE_NORMAL
- en: 'To show an example, suppose you have the following three RDD operations as
    stages. The same can be visualized as shown in *Figure 20*, *Figure 21*, and *Figure
    22*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00157.jpeg)**Figure 20:** Stage 1 for rdd1'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Conceptually, this can be shown in *Figure 21*, which first parses the data
    using the `hadoopFile()` method, groups it using the `groupByKey()` method, and
    finally, maps it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.jpeg)**Figure 21:** Stage 2 for rdd2'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Conceptually, this can be shown in *Figure 22*, which first parses the data,
    joins it, and finally, maps it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00144.jpeg)**Figure 22:** Stage 3 for rdd3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can perform an aggregation function, for example, collect, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Well! You have developed a Spark job consisting of three stages. Conceptually,
    this can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00041.jpeg)**Figure 23:** three stages for the rdd3.collect() operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if one of the stages fails, your job will fail eventually. As a result,
    the final `rdd3.collect()` statement will throw an exception about stage failure.
    Moreover, you may have issues with the following four factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Mistakes in the aggregation operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exceptions in the main thread
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OOP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class not found exception while submitting jobs using the `spark-submit` script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misconception about some API/methods in Spark core library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get rid of the aforementioned issues, our general suggestion is to ensure
    that you have not made any mistakes while performing any map, flatMap, or aggregate
    operations. Second, ensure that there are no flaws in the main method while developing
    your application with Java or Scala. Sometimes you don''t see any syntax error
    in your code, but it''s important that you have developed some small test cases
    for your application. Most common exceptions that occur in the main method are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`java.lang.noclassdeffounderror`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.nullpointerexception`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.arrayindexoutofboundsexception`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.stackoverflowerror`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.lang.classnotfoundexception`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`java.util.inputmismatchexception`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These exceptions can be avoided with the careful coding of your Spark application.
    Alternatively, use Eclipse's (or any other IDEs) code debugging features extensively
    to get rid of the semantic error to avoid the exception. For the third problem,
    that is, OOM, it's a very common problem. It is to be noted that Spark requires
    at least 8 GB of main memory, with sufficient disk space available for the standalone
    mode. On the other hand, to get the full cluster computing facilities, this requirement
    is often high.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a JAR file including all the dependencies to execute Spark jobs is
    of paramount importance. Many practitioners use Google's Guava; it is included
    in most distributions, yet it doesn't guarantee backward compatibility. This means
    that sometimes your Spark job won't find a Guava class even if you explicitly
    provided it; this happens because one of the two versions of the Guava libraries
    takes precedence over the other, and this version might not include a required
    class. In order to overcome this issue, you usually resort to shading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that you have set the Java heap space with –Xmx parameter with a
    sufficiently large value if you''re coding using IntelliJ, Vim, Eclipse, Notepad,
    and so on. While working with cluster mode, you should specify the executor memory
    while submitting Spark jobs using the Spark-submit script. Suppose you have a
    CSV to be parsed and do some predictive analytics using a random forest classifier,
    you might need to specify the right amount of memory, say 20 GB, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Even if you receive the OOM error, you can increase this amount to, say, 32
    GB or more. Since random forest is computationally intensive, requiring larger
    memory, this is just an example of random forest. You might experience similar
    issues while just parsing your data. Even a particular stage may fail due to this
    OOM error. Therefore, make sure that you are aware of this error.
  prefs: []
  type: TYPE_NORMAL
- en: For the `class not found exception`, make sure that you have included your main
    class in the resulting JAR file. The JAR file should be prepared with all the
    dependencies to execute your Spark job on the cluster nodes. We will provide a
    step-by-step JAR preparation guideline in [Chapter 17](part0511.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c),
    *Time to Go to ClusterLand - Deploying Spark on a Cluster.*
  prefs: []
  type: TYPE_NORMAL
- en: For the last issue, we can provide some examples of some misconceptions about
    Spark Core library. For example, when you use the `wholeTextFiles` method to prepare
    RDDs or DataFrames from multiple files, Spark does not run in parallel; in cluster
    mode for YARN, it may run out of memory sometimes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once, I experienced an issue where, at first, I copied six files in my S3 storage
    to HDFS. Then, I tried to create an RDD, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, I tried to process those files line by line using a UDF. When I looked
    at my computing nodes, I saw that only one executor was running per file. However,
    I then got an error message saying that YARN had run out of memory. Why so? The
    reasons are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of `wholeTextFiles` is to have only one executor for each file to be
    processed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you use `.gz` files, for example, you will have only one executor per file,
    maximum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow jobs or unresponsiveness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, if the SparkContext cannot connect to a Spark standalone master,
    then the driver may display errors such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: At other times, the driver is able to connect to the master node but the master
    is unable to communicate back to the driver. Then, multiple attempts to connect
    are made even though the driver will report that it could not connect to the Master's
    log directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, you might often experience very slow performance and progress
    in your Spark jobs. This happens because your driver program is not that fast
    to compute your jobs. As discussed earlier, sometimes a particular stage may take
    a longer time than usual because there might be a shuffle, map, join, or aggregation
    operation involved. Even if the computer is running out of disk storage or main
    memory, you may experience these issues. For example, if your master node does
    not respond or you experience unresponsiveness from the computing nodes for a
    certain period of time, you might think that your Spark job has halted and become
    stagnant at a certain stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00224.jpeg)**Figure 24:** An example log for executor/driver unresponsiveness'
  prefs: []
  type: TYPE_NORMAL
- en: 'Potential solutions could be several, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check to make sure that workers and drivers are correctly configured to connect
    to the Spark master on the exact address listed in the Spark master web UI/logs.
    Then, explicitly supply the Spark cluster''s master URL when starting your Spark
    shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Set `SPARK_LOCAL_IP` to a cluster-addressable hostname for the driver, master,
    and worker processes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sometimes, we experience some issues due to hardware failure. For example, if
    the filesystem in a computing node closes unexpectedly, that is, an I/O exception,
    your Spark job will eventually fail too. This is obvious because your Spark job
    cannot write the resulting RDDs or data to store to the local filesystem or HDFS.
    This also implies that DAG operations cannot be performed due to the stage failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, this I/O exception occurs due to an underlying disk failure or other
    hardware failures. This often provides logs, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00048.jpeg)**Figure 25:** An example filesystem closed'
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, you often experience slow job computing performance because your
    Java GC is somewhat busy with, or cannot do, the GC fast. For example, the following
    figure shows that for task 0, it took 10 hours to finish the GC! I experienced
    this issue in 2014, when I was new to Spark. Control of these types of issues,
    however, is not in our hands. Therefore, our recommendation is that you should
    make the JVM free and try submitting the jobs again.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00352.jpeg)**Figure 26:** An example where GC stalled in between'
  prefs: []
  type: TYPE_NORMAL
- en: The fourth factor could be the slow response or slow job performance is due
    to the lack of data serialization. This will be discussed in the next section.
    The fifth factor could be the memory leak in the code that will tend to make your
    application consume more memory, leaving the files or logical devices open. Therefore,
    make sure that there is no option that tends to be a memory leak. For example,
    it is a good practice to finish your Spark application by calling `sc.stop()`
    or `spark.stop()`. This will make sure that one SparkContext is still open and
    active. Otherwise, you might get unwanted exceptions or issues. The sixth issue
    is that we often keep too many open files, and this sometimes creates `FileNotFoundException`
    in the shuffle or merge stage.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several aspects of tuning Spark applications toward better optimization
    techniques. In this section, we will discuss how we can further optimize our Spark
    applications by applying data serialization by tuning the main memory with better
    memory management. We can also optimize performance by tuning the data structure
    in your Scala code while developing Spark applications. The storage, on the other
    hand, can be maintained well by utilizing serialized RDD storage.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important aspects is garbage collection, and it's tuning if
    you have written your Spark application using Java or Scala. We will look at how
    we can also tune this for optimized performance. For distributed environment-
    and cluster-based system, a level of parallelism and data locality has to be ensured.
    Moreover, performance could further be improved by using broadcast variables.
  prefs: []
  type: TYPE_NORMAL
- en: Data serialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serialization is an important tuning for performance improvement and optimization
    in any distributed computing environment. Spark is not an exception, but Spark
    jobs are often data and computing extensive. Therefore, if your data objects are
    not in a good format, then you first need to convert them into serialized data
    objects. This demands a large number of bytes of your memory. Eventually, the
    whole process will slow down the entire processing and computation drastically.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, you often experience a slow response from the computing nodes.
    This means that we sometimes fail to make 100% utilization of the computing resources.
    It is true that Spark tries to keep a balance between convenience and performance.
    This also implies that data serialization should be the first step in Spark tuning
    for better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark provides two options for data serialization: Java serialization and Kryo
    serialization libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Java serialization:** Spark serializes objects using Java''s `ObjectOutputStream`
    framework. You handle the serialization by creating any class that implements
    `java.io.Serializable`. Java serialization is very flexible but often quite slow,
    which is not suitable for large data object serialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kryo serialization:** You can also use Kryo library to serialize your data
    objects more quickly. Compared to Java serialization, Kryo serialization is much
    faster, with 10x speedup and is compact than that of Java. However, it has one
    issue, that is, it does not support all the serializable types, but you need to
    require your classes to be registered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can start using Kryo by initializing your Spark job with a `SparkConf`
    and calling `conf.set(spark.serializer, org.apache.spark.serializer.KryoSerializer)`.
    To register your own custom classes with Kryo, use the `registerKryoClasses` method,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If your objects are large, you may also need to increase the `spark.kryoserializer.buffer`
    config. This value needs to be large enough to hold the largest object you serialize.
    Finally, if you don't register your custom classes, Kryo still works; however,
    the full class name with each object needs to be stored, which is wasteful indeed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the logging subsection at the end of the monitoring Spark jobs
    section, the logging and computing can be optimized using the `Kryo` serialization.
    At first, just create the `MyMapper` class as a normal class (that is, without
    any serialization), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s register this class as a `Kyro` serialization class and then set
    the `Kyro` serialization as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s all you need. The full source code of this example is given in the
    following. You should be able to run and observe the same output, but an optimized
    one as compared to the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Well done! Now let's have a quick look at how to tune the memory. We will look
    at some advanced strategies to make sure the efficient use of the main memory
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Memory tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss some advanced strategies that can be used by
    users like you to make sure that an efficient use of memory is carried out while
    executing your Spark jobs. More specifically, we will show how to calculate the
    memory usages of your objects. We will suggest some advanced ways to improve it
    by optimizing your data structures or by converting your data objects in a serialized
    format using Kryo or Java serializer. Finally, we will look at how to tune Spark's
    Java heap size, cache size, and the Java garbage collector.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three considerations in tuning memory usage:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The amount of memory used by your objects: You may even want your entire dataset
    to fit in the memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost of accessing those objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The overhead of garbage collection: If you have a high turnover in terms of
    objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although Java objects are fast enough to access, they can easily consume a factor
    of 2 to 5x more space than the actual (aka raw) data in their original fields.
    For example, each distinct Java object has 16 bytes of overhead with an object
    header. A Java string, for example, has almost 40 bytes of extra overhead over
    the raw string. Furthermore, Java collection classes like `Set`, `List`, `Queue`,
    `ArrayList`, `Vector`, `LinkedList`, `PriorityQueue`, `HashSet`, `LinkedHashSet`,
    `TreeSet`, and so on, are also used. The linked data structures, on the other
    hand, are too complex, occupying too much extra space since there is a wrapper
    object for each entry in the data structure. Finally, the collections of primitive
    types frequently store them in the memory as boxed objects, such as `java.lang.Double`
    and `java.lang.Integer`.
  prefs: []
  type: TYPE_NORMAL
- en: Memory usage and management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Memory usages by your Spark application and underlying computing nodes can be
    categorized as execution and storage. Execution memory is used during the computation
    in merge, shuffles, joins, sorts, and aggregations. On the other hand, storage
    memory is used for caching and propagating internal data across the cluster. In
    short, this is due to the large amount of I/O across the network.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, Spark caches network data locally. While working with Spark iteratively
    or interactively, caching or persistence are optimization techniques in Spark.
    These two help in saving interim partial results so that they can be reused in
    subsequent stages. Then these interim results (as RDDs) can be kept in memory
    (default) or more solid storage, such as disk, and/or replicated. Furthermore,
    RDDs can be cached using cache operations too. They can also be persisted using
    a persist operation. The difference between cache and persist operations is purely
    syntactic. The cache is a synonym of persisting or persists (`MEMORY_ONLY`), that
    is, cache is merely persisted with the default storage level `MEMORY_ONLY`.
  prefs: []
  type: TYPE_NORMAL
- en: If you go under the Storage tab in your Spark web UI, you should observe the
    memory/storage used by an RDD, DataFrame, or Dataset object, as shown in *Figure
    10*. Although there are two relevant configurations for tuning memory in Spark,
    users do not need to readjust them. The reason is that the default values set
    in the configuration files are enough for your requirements and workloads.
  prefs: []
  type: TYPE_NORMAL
- en: spark.memory.fraction is the size of the unified region as a fraction of (JVM
    heap space - 300 MB) (default 0.6). The rest of the space (40%) is reserved for
    user data structures, internal metadata in Spark, and safeguarding against OOM
    errors in case of sparse and unusually large records. On the other hand, `spark.memory.storageFraction`
    expresses the size of R storage space as a fraction of the unified region (default
    is 0.5). The default value of this parameter is 50% of Java heap space, that is,
    300 MB.
  prefs: []
  type: TYPE_NORMAL
- en: A more detailed discussion on memory usage and storage is given in [Chapter
    15](part0458.html#DKP1K1-21aec46d8593429cacea59dbdcd64e1c), *Text Analytics Using
    Spark ML*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, one question might arise in your mind: which storage level to choose?
    To answer this question, Spark storage levels provide you with different trade-offs
    between memory usage and CPU efficiency. If your RDDs fit comfortably with the
    default storage level (MEMORY_ONLY), let your Spark driver or master go with it.
    This is the most memory-efficient option, allowing operations on the RDDs to run
    as fast as possible. You should let it go with this, because this is the most
    memory-efficient option. This also allows numerous operations on the RDDs to be
    done as fast as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: If your RDDs do not fit the main memory, that is, if `MEMORY_ONLY` does not
    work out, you should try using `MEMORY_ONLY_SER`. It is strongly recommended to
    not spill your RDDs to disk unless your **UDF** (aka **user-defined function**
    that you have defined for processing your dataset) is too expensive. This also
    applies if your UDF filters a large amount of the data during the execution stages.
    In other cases, recomputing a partition, that is, repartition, may be faster for
    reading data objects from disk. Finally, if you want fast fault recovery, use
    the replicated storage levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, there are the following StorageLevels available and supported in
    Spark 2.x: (number _2 in the name denotes 2 replicas):'
  prefs: []
  type: TYPE_NORMAL
- en: '`DISK_ONLY`: This is for disk-based operation for RDDs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DISK_ONLY_2`: This is for disk-based operation for RDDs for 2 replicas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_ONLY`: This is the default for cache operation in memory for RDDs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_ONLY_2`: This is the default for cache operation in memory for RDDs
    with 2 replicas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_ONLY_SER`: If your RDDs do not fit the main memory, that is, if `MEMORY_ONLY`
    does not work out, this option particularly helps in storing data objects in a
    serialized form'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_ONLY_SER_2`: If your RDDs do not fit the main memory, that is, if `MEMORY_ONLY`
    does not work out with 2 replicas, this option also helps in storing data objects
    in a serialized form'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_AND_DISK`: Memory and disk (aka combined) based RDD persistence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_AND_DISK_2`: Memory and disk (aka combined) based RDD persistence with
    2 replicas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_AND_DISK_SER`: If `MEMORY_AND_DISK` does not work, it can be used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_AND_DISK_SER_2`: If `MEMORY_AND_DISK` does not work with 2 replicas,
    this option can be used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OFF_HEAP`: Does not allow writing into Java heap space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that cache is a synonym of persist (`MEMORY_ONLY`). This means that cache
    is solely persist with the default storage level, that is, `MEMORY_ONLY`. Detailed
    information can be found at [https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-StorageLevel.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-StorageLevel.html).
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first way to reduce extra memory usage is to avoid some features in the
    Java data structure that impose extra overheads. For example, pointer-based data
    structures and wrapper objects contribute to nontrivial overheads. To tune your
    source code with a better data structure, we provide some suggestions here, which
    can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: First, design your data structures such that you use arrays of objects and primitive
    types more. Thus, this also suggests using standard Java or Scala collection classes
    like `Set`, `List`, `Queue`, `ArrayList`, `Vector`, `LinkedList`, `PriorityQueue`,
    `HashSet`, `LinkedHashSet`, and `TreeSet` more frequently.
  prefs: []
  type: TYPE_NORMAL
- en: Second, when possible, avoid using nested structures with a lot of small objects
    and pointers so that your source code becomes more optimized and concise. Third,
    when possible, consider using numeric IDs and sometimes using enumeration objects
    rather than using strings for keys. This is recommended because, as we have already
    stated, a single Java string object creates an extra overhead of 40 bytes. Finally,
    if you have less than 32 GB of main memory (that is, RAM), set the JVM flag `-XX:+UseCompressedOops`
    to make pointers 4 bytes instead of 8.
  prefs: []
  type: TYPE_NORMAL
- en: The earlier option can be set in the `SPARK_HOME/conf/spark-env.sh.template`.
    Just rename the file as `spark-env.sh` and set the value straight away!
  prefs: []
  type: TYPE_NORMAL
- en: Serialized RDD storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed already, despite other types of memory tuning, when your objects
    are too large to fit in the main memory or disk efficiently, a simpler and better
    way of reducing memory usage is storing them in a serialized form.
  prefs: []
  type: TYPE_NORMAL
- en: This can be done using the serialized storage levels in the RDD persistence
    API, such as `MEMORY_ONLY_SER`. For more information, refer to the previous section
    on memory management and start exploring available options.
  prefs: []
  type: TYPE_NORMAL
- en: If you specify using `MEMORY_ONLY_SER`, Spark will then store each RDD partition
    as one large byte array. However, the only downside of this approach is that it
    can slow down data access times. This is reasonable and obvious too; fairly speaking,
    there's no way to avoid it since each object needs to deserialize on the fly back
    while reusing.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed previously, we highly recommend using Kryo serialization instead
    of Java serialization to make data access a bit faster.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although it is not a major problem in your Java or Scala programs that just
    read an RDD sequentially or randomly once and then execute numerous operations
    on it, **Java Virtual Machine** (**JVM**) GC can be problematic and complex if
    you have a large amount of data objects w.r.t RDDs stored in your driver program.
    When the JVM needs to remove obsolete and unused objects from the old objects
    to make space for the newer ones, it is mandatory to identify them and remove
    them from the memory eventually. However, this is a costly operation in terms
    of processing time and storage. You might be wondering that the cost of GC is
    proportional to the number of Java objects stored in your main memory. Therefore,
    we strongly suggest you tune your data structure. Also, having fewer objects stored
    in your memory is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in GC tuning is collecting the related statistics on how frequently
    garbage collection by JVM occurs on your machine. The second statistic needed
    in this regard is the amount of time spent on GC by JVM on your machine or computing
    nodes. This can be achieved by adding `-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps`
    to the Java options in your IDE, such as Eclipse, in the JVM startup arguments
    and specifying a name and location for our GC log file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00213.jpeg)**Figure 27:** Setting GC verbose on Eclipse'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can specify `verbose:gc` while submitting your Spark jobs
    using the Spark-submit script, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In short, when specifying GC options for Spark, you must determine where you
    want the GC options specified, on the executors or on the driver. When you submit
    your jobs, specify `--driver-java-options -XX:+PrintFlagsFinal -verbose:gc` and
    so on. For the executor, specify `--conf spark.executor.extraJavaOptions=-XX:+PrintFlagsFinal
    -verbose:gc` and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Now, when your Spark job is executed, you will be able to see the logs and messages
    printed in the worker's node at `/var/log/logs` each time a GC occurs. The downside
    of this approach is that these logs will not be on your driver program but on
    your cluster's worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that `verbose:gc` only prints appropriate message or logs
    after each GC collection. Correspondingly, it prints details about memory. However,
    if you are interested in looking for more critical issues, such as a memory leak,
    `verbose:gc` may not be enough. In that case, you can use some visualization tools,
    such as jhat and VisualVM. A better way of GC tuning in your Spark application
    can be read at [https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html).
  prefs: []
  type: TYPE_NORMAL
- en: Level of parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although you can control the number of map tasks to be executed through optional
    parameters to the `SparkContext.text` file, Spark sets the same on each file according
    to its size automatically. In addition to this, for a distributed `reduce` operation
    such as `groupByKey` and `reduceByKey`, Spark uses the largest parent RDD's number
    of partitions. However, sometimes, we make one mistake, that is, not utilizing
    the full computing resources for your nodes in a computing cluster. As a result,
    the full computing resources will not be fully exploited unless you set and specify
    the level of parallelism for your Spark job explicitly. Therefore, you should
    set the level of parallelism as the second argument.
  prefs: []
  type: TYPE_NORMAL
- en: For more on this option, please refer to [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions.](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can do it by setting the config property spark.default.parallelism
    to change the default. For operations such as parallelizing with no parent RDDs,
    the level of parallelism depends on the cluster manager, that is, standalone,
    Mesos, or YARN. For the local mode, set the level of parallelism equal to the
    number of cores on the local machine. For Mesos or YARN, set fine-grained mode
    to 8\. In other cases, the total number of cores on all executor nodes or 2, whichever
    is larger, and in general, 2-3 tasks per CPU core in your cluster is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A broadcast variable enables a Spark developer to keep a read-only copy of an
    instance or class variable cached on each driver program, rather than transferring
    a copy of its own with the dependent tasks. However, an explicit creation of a
    broadcast variable is useful only when tasks across multiple stages need the same
    data in deserialize form.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark application development, using the broadcasting option of SparkContext
    can reduce the size of each serialized task greatly. This also helps to reduce
    the cost of initiating a Spark job in a cluster. If you have a certain task in
    your Spark job that uses large objects from the driver program, you should turn
    it into a broadcast variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use a broadcast variable in a Spark application, you can instantiate it
    using `SparkContext.broadcast`. Later on, use the value method from the class
    to access the shared value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Output/log: `bv: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(0)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Output/log: `res0: Int = 1`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00359.jpeg)**Figure 28:** Broadcasting a value from driver to executors'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Broadcast feature of Spark uses the **SparkContext** to create broadcast
    values. After that, the **BroadcastManager** and **ContextCleaner** are used to
    control their life cycle, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00368.jpeg)**Figure 29:** SparkContext broadcasts the variable/value
    using BroadcastManager and ContextCleaner'
  prefs: []
  type: TYPE_NORMAL
- en: Spark application in the driver program automatically prints the serialized
    size of each task on the driver. Therefore, you can decide whether your tasks
    are too large to make it parallel. If your task is larger than 20 KB, it's probably
    worth optimizing.
  prefs: []
  type: TYPE_NORMAL
- en: Data locality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data locality means how close the data is to the code to be processed. Technically,
    data locality can have a nontrivial impact on the performance of a Spark job to
    be executed locally or in cluster mode. As a result, if the data and the code
    to be processed are tied together, computation is supposed to be much faster.
    Usually, shipping a serialized code from a driver to an executor is much faster
    since the code size is much smaller than that of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Spark application development and job execution, there are several levels
    of locality. In order from closest to farthest, the level depends on the current
    location of the data you have to process:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data Locality** | **Meaning** | **Special Notes** |'
  prefs: []
  type: TYPE_TB
- en: '| `PROCESS_LOCAL` | Data and code are in the same location | Best locality
    possible |'
  prefs: []
  type: TYPE_TB
- en: '| `NODE_LOCAL` | Data and the code are on the same node, for example, data
    stored on HDFS | A bit slower than `PROCESS_LOCAL` since the data has to propagate
    across the processes and network |'
  prefs: []
  type: TYPE_TB
- en: '| `NO_PREF` | The data is accessed equally from somewhere else | Has no locality
    preference |'
  prefs: []
  type: TYPE_TB
- en: '| `RACK_LOCAL` | The data is on the same rack of servers over the network |
    Suitable for large-scale data processing |'
  prefs: []
  type: TYPE_TB
- en: '| `ANY` | The data is elsewhere on the network and not in the same rack | Not
    recommended unless there are no other options available |'
  prefs: []
  type: TYPE_TB
- en: '**Table 2:** Data locality and Spark'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark is developed such that it prefers to schedule all tasks at the best locality
    level, but this is not guaranteed and not always possible either. As a result,
    based on the situation in the computing nodes, Spark switches to lower locality
    levels if available computing resources are too occupied. Moreover, if you would
    like to have the best data locality, there are two choices for you:'
  prefs: []
  type: TYPE_NORMAL
- en: Wait until a busy CPU gets free to start a task on your data on the same server
    or same node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immediately start a new one, which requires moving data there
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed some advanced topics of Spark toward making your
    Spark job's performance better. We discussed some basic techniques to tune your
    Spark jobs. We discussed how to monitor your jobs by accessing Spark web UI. We
    discussed how to set Spark configuration parameters. We also discussed some common
    mistakes made by Spark users and provided some recommendations. Finally, we discussed
    some optimization techniques that help tune Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will see how to test Spark applications and debug to
    solve most common issues.
  prefs: []
  type: TYPE_NORMAL
