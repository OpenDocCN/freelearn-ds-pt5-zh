- en: Chapter 11.  Building Data Science Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data science applications are garnering a lot of excitement, mainly because
    of the promise they hold in harnessing data and extracting consumable results.
    There are already several successful data products that have had a transformative
    effect on our daily lives. The ubiquitous recommender systems, e-mail spam filters,
    and targeted advertisements and news content have become part and parcel of life.
    Music and movies have become data products streaming from providers such as iTunes
    and Netflix. Businesses, especially in the domains such as retail, are actively
    pursuing ways to gain a competitive advantage by studying the market and customer
    behavior using a data-driven approach.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed the data analytics workflow up to the model building phase
    so far in the previous chapters. But the real value of a model is when it is actually
    deployed in a production system. The end product, the fruit of a data science
    workflow, is an operationalized data product. In this chapter, we discuss this
    culminating stage of the data analytics workflow. We will not get into actual
    code snippets but take a step back to get the complete picture, including the
    non-technical aspects.
  prefs: []
  type: TYPE_NORMAL
- en: The complete picture is not limited to the development process alone. It comprises
    the user application, developments in Spark itself, as well as rapid changes happening
    in the big data landscape. We'll start with the development process of the user
    application first and discuss various options at each stage. Then we'll delve
    into the features and enhancements in the latest Spark 2.0 release and future
    plans. Finally, we'll attempt to give a broad overview of the big data trends,
    especially the Hadoop ecosystem. References and useful links are included in individual
    sections in addition to the end of the chapter for further information about the
    specific context.
  prefs: []
  type: TYPE_NORMAL
- en: Scope of development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data analytics workflow can be roughly divided into two phases, the build phase
    and the operationalization phase. The first phase is usually a one-time exercise,
    with heavy human intervention. Once we''ve attained reasonable end results, we
    are ready to operationalize the product. The second phase starts with the models
    generated in the first phase and makes them available as a part of some production
    workflow. In this section, we''ll discuss the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Expectations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presentation options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expectations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The primary goal of data science applications is to build "actionable" insights,
    actionable being the keyword. Many use cases such as fraud detection need the
    insights to be generated and made available in a consumable fashion in near real
    time, if you expect any action-ability at all. The end users of the data product
    vary with the use case. They may be customers of an e-commerce site or a decision
    maker of a major conglomerate. The end user need not always be a human being.
    It could be a risk assessment software tool in a financial institution. A one-size-fits-all
    approach does not fit in with many software products, and data products are no
    exception. However, there are some common expectations for data products, as listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: The first and foremost expectation is that the insight generation time frame
    based on real-world data should be within "actionable" timeframes. The actual
    time frame varies based on the use case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data product should integrate into some (often already existing) production
    workflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The insights should be translated into something that people can use instead
    of obscure numbers or hard-to-interpret charts. The presentation should be unobtrusive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data product should have the ability to fine-tune itself (self-adapting)
    based on the incoming data inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, there has to be some way to receive human feedback, which can be used
    as a source for self-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There should be a mechanism that quantitatively assesses its effectiveness periodically
    and automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presentation options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The varied nature of data products calls for varied modes of presentation. Sometimes
    the end result of a data analytics exercise is to publish a research paper. Sometimes
    it could be a part of a dashboard, where this becomes one of several sources publishing
    results on a single web page. They may be overt and targeted for human consumption,
    or covert and feeding into some other software application. You may use a general-purpose
    engine such as Spark to build your solution, but the presentation must be highly
    aligned to the targeted user base.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes all you need to do is write an e-mail with your findings or just export
    a CSV file of insights. Or you may have to develop a dedicated web application
    around your data product. Some other common options are discussed here, and you
    have to choose the right one that fits the problem on hand.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive notebooks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interactive notebooks are web applications that allow you to create and share
    documents that contain code chunks, results, equations, images, videos, and explanation
    text. They may be viewed as executable documents or REPL shells with visualization
    and equation support. These documents can be exported as PDFs, Markdown, or HTML.
    Notebooks contain several "kernels" or "computational engines" that execute code
    chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive notebooks are the most suitable choice if the end goal of your data
    analytics workflow is to generate a written report. There are several notebooks
    and many of them have Spark support. These notebooks are useful tools during the
    exploration phase also. We have already introduced IPython and Zeppelin notebooks
    in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The IPython Notebook: A Comprehensive Tool for Data Science: [http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233](http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparkly Notebook: Interactive Analysis and Visualization with Spark: [http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark](http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An **Application Programming Interface** (**API**) is a software-to-software
    interface; a specification that describes the available functionality, how it
    must be used, and what the inputs and outputs are. The software (service) provider
    exposes some of its functionality as an API. A developer may develop a software
    component that consumes this API. For example, Twitter offers APIs to get or post
    data onto Twitter or to query data programmatically. A Spark enthusiast may write
    a software component that automatically collects all tweets on #Spark, categorizes
    according to their requirements, and publishes that data on their personal website.
    Web APIs are a type of APIs where the interface is defined as a set of **Hypertext
    Transfer Protocol** (**HTTP**) request messages along with a definition of the
    structure of response messages. Nowadays REST-ful (Representational State Transfer)
    have become the de facto standard.'
  prefs: []
  type: TYPE_NORMAL
- en: You can implement your data product as an API, and perhaps this is the most
    powerful option. It can then be plugged into one or more applications, say the
    management dashboard as well as the marketing analytics workflow. You may develop
    a domain specific "insights-as-a-service" as a public Web API with a subscription
    model. The simplicity and ubiquity of Web APIs make them the most compelling choice
    for building data products.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Application programming interface: [https://en.wikipedia.org/wiki/Application_programming_interface](https://en.wikipedia.org/wiki/Application_programming_interface)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ready for APIs? Three steps to unlock the data economy's most promising channel:[http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5](http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Insights-as-a-service is growing based on big data: [http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html](http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PMML and PFA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes you may have to expose your model in a way that other data mining
    tools can understand. The model and the complete pre- and post-processing steps
    should be converted into a standard format. PMML and PFA are two such standard
    formats in the data mining domain.
  prefs: []
  type: TYPE_NORMAL
- en: '**Predictive Model Markup Language** (**PMML**) is an XML-based predictive
    model interchange format and Apache Spark API convert models into PMML out of
    the box. A PMML message may contain a myriad of data transformations as well as
    one or more predictive models. Different data mining tools can export or import
    PMML messages without the need for custom code.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Portable Format for Analytics** (**PFA**) is the next generation of predictive
    model interchange format. It exchanges JSON documents and straightaway inherits
    all advantages of JSON documents as against XML documents. In addition, PFA is
    more flexible than PMML.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PMML FAQ: Predictive Model Markup Language: [http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html](http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Portable Format for Analytics: moving models to production: [http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html](http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is PFA for?: [http://dmg.org/pfa/docs/motivation/](http://dmg.org/pfa/docs/motivation/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Development and testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apache Spark is a general-purpose cluster computing system that can run both
    by itself or over several existing cluster managers such as Apache Mesos, Hadoop,
    Yarn, and Amazon EC2\. In addition, several big data and enterprise software companies
    have already integrated Spark into their offerings: Microsoft Azure HDInsight,
    Cloudera, IBM Analytics for Apache Spark, SAP HANA, and the list goes on. Databricks,
    a company founded by the creators of Apache Spark, have their own product for
    data science workflow, from ingestion to production. Your responsibility is to
    understand your organizational requirements and existing talent pool and decide
    which option is the best for you.'
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the option chosen, follow the usual best practices in any software
    development life cycle, such as version control and peer reviews. Try to use high-level
    APIs wherever applicable. The data transformation pipelines used in production
    should be the same as the ones used in building the model. Document any questions
    that arise during the data analytics workflow. Often these may result in business
    process improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, testing is extremely important for the success of your product.
    You have to maintain a set of automated scripts that give easy-to-understand results.
    The test cases should cover the following at the minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: Adherence to timeframe and resource consumption requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resilience to bad data (for example, data type violations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New value in a categorical feature that was not encountered during the model
    building phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very little data or too heavy data that is expected in the target production
    system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monitor logs, resource utilization, and so on to uncover any performance bottlenecks.
    The Spark UI provides a wealth of information to monitor Spark applications. The
    following are some common tips that will help you improve performance:'
  prefs: []
  type: TYPE_NORMAL
- en: Cache any input or intermediate data that might be used multiple times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look at the Spark UI and identify jobs that are causing a lot of shuffle. Check
    the code and see whether you can reduce the shuffles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions may transfer the data from workers to the driver. See that you are not
    transferring any data that is not absolutely necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "Stragglers; that run slower than others; \x94may increase the overall job completion\
    \ time. There may be several reasons for a straggler. If a job is running slow\
    \ due to a slow node, you may set `spark.speculation` to `true`. Then Spark automatically\
    \ relaunches such a task on a different node. Otherwise, you may have to revisit\
    \ the logic and see whether it can be improved."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Investigating Spark's performance: [http://radar.oreilly.com/2015/04/investigating-sparks-performance.html](http://radar.oreilly.com/2015/04/investigating-sparks-performance.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning and Debugging in Apache Spark by Patrick Wendell: [https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/](https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to tune your Apache Spark jobs: http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/ and
    part 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the outset, let's not forget that we are trying to build fault-tolerant software
    data products from unreliable, often unstructured, and uncontrolled data sources.
    So data quality management gains even more importance in a data science workflow.
    Sometimes the data may solely come from controlled data sources, such as automated
    internal process workflows in an organization. But in all other cases, you need
    to carefully craft your data cleansing processes to protect the subsequent processing.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata consists of the structure and meaning of data, and obviously the most
    critical repository to work with. It is the information about the structure of
    individual data sources and what each component in that structure means. You may
    not always be able to write some script and extract this data. A single data source
    may contain data with different structures or an individual component (column)
    may mean different things during different times. A label such as owner or high
    may mean different things in different data sources. Collecting and understanding
    all such nuances and documenting is a tedious, iterative task. Standardization
    of metadata is a prerequisite to data transformation development.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some broad guidelines that are applicable to most use cases are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: All data sources must be versioned and timestamped
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data quality management processes often require involvement of the highest authorities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mask or anonymize sensitive data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important step that is often missed out is to maintain traceability; a link
    between each data element (say a row) and its original source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Scala advantage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark allows you to write applications in Python, R, Java, or Scala.
    With this flexibility comes the responsibility of choosing the right language
    for your requirements. But regardless of your usual language of choice, you may
    want to consider Scala for your Spark-powered application. In this section, we
    will explain why.
  prefs: []
  type: TYPE_NORMAL
- en: Let's digress to gain a high-level understanding of imperative and functional
    programming paradigms first. Languages such as C, Python, and Java belong to the
    imperative programming paradigm. In the imperative programming paradigm, a program
    is a sequence of instructions and it has a program state. The program state is
    usually represented as a set of variables and their values at any given point
    in time. Assignments and reassignments are fairly common. Variable values are
    expected to change over the period of execution by one or more functions. Variable
    value modification in a function is not limited to local variables. Global variables
    and public class variables are some examples of such variables.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, programs written in functional programming languages such as Erlang
    can be viewed as stateless expression evaluators. Data is immutable. If a function
    is called with the same set of input arguments, then it is expected to produce
    the same result (that is, referential transparency). This is possible due to the
    absence of interference from a variable context in the form of global variables
    and the like. This implies that the sequence of function evaluation is of little
    importance. Functions can be passed as arguments to other functions. Recursive
    calls replace loops. The absence of state makes parallel programming much easier
    because it eliminates the need for locking and possible deadlocks. Coordination
    gets simplified when the execution order is less important. These factors make
    the functional programming paradigm a neat fit for parallel programming.
  prefs: []
  type: TYPE_NORMAL
- en: Pure functional programming languages are hard to work with because most of
    the programs require state changes. Most functional programming languages, including
    good old Lisp, do allow storing of data in variables (side-effects). Some languages
    such as Scala draw from multiple programming paradigms.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to Scala, it is a JVM-based, statically typed multi-paradigm programming
    language. Its built-in-type inference mechanism allows programmers to omit some
    redundant type information. This gives a feel of the flexibility offered by dynamic
    languages while retaining the robustness of better compile time checks and fast
    runtime. Scala is an object-oriented language in the sense that every value is
    an object, including numerical values. Functions are first-class objects, which
    can be used as any data type, and they can be passed as arguments to other functions.
    Scala interoperates well with Java and its tools because Scala runs on JVM. Java
    and Scala classes can be freely mixed. That implies that Scala can easily interact
    with the Hadoop ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: All of these factors should be taken into account when you choose the right
    programming language for your application.
  prefs: []
  type: TYPE_NORMAL
- en: Spark development status
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark has become the most currently active project in the Hadoop ecosystem
    in terms of the number of contributors by the end of 2015\. Having started as
    a research project at UC Berkeley AMPLAB in 2009, Spark is still relatively young
    when compared to projects such as Apache Hadoop and is still in active development.
    There were three releases in the year 2015, from 1.3 through 1.5, packed with
    features such as DataFrames API, SparkR, and Project Tungsten respectively. Version
    1.6 was released in early 2016 and included the new Dataset API and expansion
    of data science functionality. Spark 2.0 was released in July 2016, and this being
    a major release has a lot of new features and enhancements that deserve a section
    of their own.
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.0's features and enhancements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Spark 2.0 included three major new features and several other performance
    improvements and under-the-hood changes. This section attempts to give a high-level
    overview yet step into the details to give a conceptual understanding wherever
    required.
  prefs: []
  type: TYPE_NORMAL
- en: Unifying Datasets and DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DataFrames are high-level APIs that support a data abstraction conceptually
    equivalent to a table in a relational database or a DataFrame in R and Python
    (the pandas library). Datasets are an extension of the DataFrame API that provide
    a type-safe, object-oriented programming interface. Datasets add static types
    to DataFrames. Defining a structure on top of DataFrames provides information
    to the core that enables optimizations. It also helps in catching analysis errors
    early on, even before a distributed job starts.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs, Datasets, and DataFrames are interchangeable. RDDs continue to be the
    low-level API. DataFrames, Datasets, and SQL share the same optimization and execution
    pipeline. Machine learning libraries take either DataFrames or Datasets. Both
    DataFrames and Datasets run on Tungsten, an initiative to improve runtime performance.
    They leverage Tungsten's fast in-memory encoding, which is responsible for converting
    between JVM objects and Spark's internal representation. The same APIs work on
    streams also, introducing the concept of continuous DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Structure Streaming APIs are high-level APIs that are built on the Spark SQL
    engine and extend DataFrames and Datasets. Structured Streaming unifies streaming,
    interactive, and batch queries. In most use cases, streaming data needs to be
    combined with batch and interactive queries to form continuous applications. These
    APIs are designed to address that requirement. Spark takes care of running the
    query incrementally and continuously on streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: The first release of structured streaming will be focusing on ETL workloads.
    Users will be able to specify the input, query, trigger, and type of output. An
    input stream is logically equivalent to an append-only table. Users define queries
    just the way they would on a traditional SQL table. The trigger is a timeframe,
    say one second. The output modes offered are complete output, deltas, or updates
    in place (for example, a DB table).
  prefs: []
  type: TYPE_NORMAL
- en: 'Take this example: you can aggregate the data in a stream, serve it using the
    Spark SQL JDBC server, and pass it to a database such as MySQL for downstream
    applications. Or you could run ad hoc SQL queries that act on the latest data.
    You can also build and apply machine learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: Project Tungsten phase 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The central idea behind project Tungsten is to bring Spark''s performance closer
    to bare metal through native memory management and runtime code generation. It
    was first included in Spark 1.4 and enhancements were added in 1.5 and 1.6\. It
    focuses on substantially improving the efficiency of memory and CPU for Spark
    applications, primarily by the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Managing memory explicitly and eliminating the overhead of JVM object model
    and garbage collection. For example, a four-byte string would occupy around 48
    bytes in the JVM object model. Since Spark is not a general-purpose application
    and has more knowledge about the life cycle of memory blocks than the garbage
    collector, it can manage memory more efficiently than JVM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing cache-friendly algorithms and data structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark performs code generation to compile parts of queries to Java bytecode.
    This is being broadened to cover most built-in expressions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark 2.0 rolls out phase 2, which is an order of magnitude faster and includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Whole stage code generation by removing expensive iterator calls and fusing
    across multiple operators so that the generated code looks like hand-optimized
    code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimized input and output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What's in store?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apache Spark 2.1 is expected to have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous SQL** (**CSQL**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BI application integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for more streaming sources and sinks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inclusion of additional operators and libraries for structured streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancements to a machine learning package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columnar in-memory support in Tungsten
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The big data trends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data processing has been an integral part of the IT industry, more so in
    the past decade. Apache Hadoop and other similar endeavors are focused on building
    the infrastructure to store and process massive amounts of data. After being around
    for over 10 years, the Hadoop platform is considered mature and almost synonymous
    with big data processing. Apache Spark, a general computing engine that works
    well with is and not limited to the Hadoop ecosystem, was quite successful in
    the year 2015.
  prefs: []
  type: TYPE_NORMAL
- en: Building data science applications requires knowledge of the big data landscape
    and what software products are available out of that box. We need to carefully
    map the right blocks that fit our requirements. There are several options with
    overlapping functionality, and picking the right tools is easier said than done.
    The success of the application very much depends on assembling the right mix of
    technologies and processes. The good news is that there are several open source
    options that drive down the cost of doing big data analytics; and at the same
    time, you have enterprise-quality end-to-end platforms backed by companies such
    as Databricks. In addition to the use case on hand, keeping track of the industry
    trends in general is equally important.
  prefs: []
  type: TYPE_NORMAL
- en: The recent surge in NOSQL data stores with their own interfaces are adding SQL-based
    interfaces even though they are not relational data stores and may not adhere
    to ACID properties. This is a welcome trend because converging to a single, age-old
    interface across relational and non-relational data stores improves programmer
    productivity.
  prefs: []
  type: TYPE_NORMAL
- en: The operational (OLTP) and analytical (OLAP) systems were being maintained as
    separate systems over the past couple of decades, but that's one more place where
    convergence is happening. This convergence brings us to near-real-time use cases
    such as fraud prevention. Apache Kylin is one open source distributed analytics
    engine in the Hadoop ecosystem that offers an extremely fast OLAP engine at scale.
  prefs: []
  type: TYPE_NORMAL
- en: The advent of the Internet of Things is accelerating real-time and streaming
    analytics, bringing in a whole lot of new use cases. The cloud frees up organizations
    from the operations and IT management overheads so that they can concentrate on
    their core competence, especially in big data processing. Cloud-based analytic
    engines, self-service data preparation tools, self-service BI, just-in-time data
    warehousing, advanced analytics, rich media analytics, and agile analytics are
    some of the commonly used buzzwords. The term big data itself is slowly evaporating
    or becoming implicit.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are plenty of software products and libraries in the big data landscape
    with overlapping functionalities, as shown in this infographic (http://mattturck.com/wp-content/uploads/2016/02/matt_turck_big_data_landscape_v11.png).
    Choosing the right blocks for your application is a daunting but very important
    task. Here is a short list of projects to get you started. The list excludes popular
    names such as Cassandra and tries to include blocks with complementing functionality
    and mostly from Apache Software Foundation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Arrow** ([https://arrow.apache.org/](https://arrow.apache.org/)) is
    an in-memory columnar layer used to accelerate analytical processing and interchange.
    It is a high-performance, cross-system, and in-memory data representation that
    is expected to bring in 100 times the performance improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Parquet** ([https://parquet.apache.org/](https://parquet.apache.org/))
    is a columnar storage format. Spark SQL provides support for both reading and
    writing parquet files while automatically capturing the structure of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Kafka** ([http://kafka.apache.org/](http://kafka.apache.org/)) is
    a popular, high-throughput distributed messaging system. Spark streaming has a
    direct API to support streaming data ingestion from Kafka.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alluxio** ([http://alluxio.org/](http://alluxio.org/)), formerly called Tachyon,
    is a memory-centric, virtual distributed storage system that enables data sharing
    across clusters at memory speed. It aims to become the de facto storage unification
    layer for big data. Alluxio sits between computation frameworks such as Spark
    and storage systems such as Amazon S3, HDFS, and others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphFrames** (https://databricks.com/blog/2016/03/03/introducing-graphframes.html)
    is a graph processing library for Apache spark that is built on top of DataFrames
    API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Kylin** ([http://kylin.apache.org/](http://kylin.apache.org/)) is
    a distributed analytics engine designed to provide SQL interface and multidimensional
    analysis (OLAP) on Hadoop, supporting extremely large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Sentry** ([http://sentry.apache.org/](http://sentry.apache.org/))
    is a system for enforcing fine-grained role-based authorization to data and metadata
    stored on a Hadoop cluster. It is in the incubation stage at the time of writing
    this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Solr** ([http://lucene.apache.org/solr/](http://lucene.apache.org/solr/))
    is a blazing fast search platform. Check this [presentation](https://spark-summit.org/2015/events/integrating-spark-and-solr/)
    for integrating Solr and Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow** ([https://www.tensorflow.org/](https://www.tensorflow.org/))
    is a machine learning library with extensive built-in support for deep learning.
    Check out this [blog](https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html)
    to learn how it can be used with Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zeppelin** ([http://zeppelin.incubator.apache.org/](http://zeppelin.incubator.apache.org/))
    is a web-based notebook that enables interactive data analytics. It is covered
    in the data visualization chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter, we discussed how to build real-world applications using
    Spark. We discussed the big picture consisting of technical and non-technical
    aspects of data analytics workflows.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark Summit site has a wealth of information on Apache Spark and related
    projects from completed events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interview with *Matei Zaharia* by KDnuggets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Why Spark Reached the Tipping Point* in 2015 from KDnuggets by *Matthew Mayo*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Going Live: Preparing your first Spark production deployment is a very good
    starting point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is Scala?* from the Scala home page'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Martin Odersky*, creator of Scala, explains the reasons why Scala fuses together
    imperative and functional programming'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
