<html><head></head><body>
        

                            
                    <h1 class="header-title">From Engagement to Conversion</h1>
                
            
            
                
<p>In this chapter, we will expand your knowledge of explanatory analysis and show you how to use <strong>decision trees</strong> to understand the drivers behind consumer behavior. We will start by comparing and explaining the differences between logistic regression and decision tree models, and then we will discuss how decision trees are built and trained. Next, we will discuss how a trained decision tree model can be used to extract information about the relationships between the attributes (or features) of individual consumers and the target output variables.</p>
<p>For programming exercises, we will use the bank marketing dataset from the UCI Machine Learning Repository to understand the drivers behind conversions. We will start with some data analysis, so that you can better understand the dataset; then, we will build decision tree models by using the <kbd>scikit-learn</kbd> package in Python and the <kbd>rpart</kbd> package in R. Lastly, you will learn how to interpret these trained decision tree models by visualizing them using the <kbd>graphviz</kbd> package in Python and the <kbd>rattle</kbd> package in R. By the end of this chapter, you will be familiar with decision trees and will have a better understanding of when and how to use them with Python or R.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Decision trees</li>
<li>Decision trees and interpretations with Python</li>
<li>Decision trees and interpretations with R</li>
</ul>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Decision trees</h1>
                
            
            
                
<p>In the previous chapter, we discussed explanatory analysis and regression analysis. We are going to continue with that theme and introduce another machine learning algorithm that we can use to draw insights on customer behavior from data. In this chapter, we will be discussing a machine learning algorithm called <strong>decision trees</strong>:<strong> </strong>how they learn from the data and how we can interpret their results.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Logistic regression versus decision trees</h1>
                
            
            
                
<p>If you recall from the previous chapter, a <strong>logistic regression</strong> model learns from the data by finding the linear combination of the feature variables that best estimates the log odds of an event occurring. Decision trees, as the name suggests, learn from the data by growing a tree. We are going to discuss how decision tree models grow and to build trees in more detail in the following section, but the main difference between the logistic regression and decision tree models is the fact that logistic regression algorithms search for a single best linear boundary in the feature set, whereas the decision tree algorithm partitions the data to find the subgroups of data that have high likelihoods of an event occurring. It will be easier to explain this with an example. Let's take a look at the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1029 image-border" src="img/8d0bb394-1e86-48ec-9bfd-8c8bda7e1d97.png" style="width:38.50em;height:25.83em;"/></p>
<p>This is an example of a decision tree model. As you can see in this diagram, it partitions the data with certain criteria. In this example, the root node is split into child nodes by a criterion of <kbd>previous &lt; 0.5</kbd>. If this condition is met and true, then it traverses to the left child node. If not, then it traverses to the right child node. The left child node is then split into its child nodes by a criterion of <kbd>age &lt; 61</kbd>. The tree grows until it finds pure nodes (meaning that all of the data points in each node belong to one class) or until it meets certain criteria to stop, such as the maximum depth of the tree.</p>
<p>As you can see in this example, the data are split into seven partitions. The leftmost node or partition at the bottom is for those data points with values less than <kbd>0.5</kbd> for the <kbd>previous</kbd> variable and with values less than <kbd>61</kbd> for the <kbd>age</kbd> variable. On the other hand, the rightmost node at the bottom is for those data points with values greater than <kbd>0.5</kbd> for the <kbd>previous</kbd> variable and with values other than <kbd>yes</kbd> for the <kbd>housing</kbd> variable.</p>
<p>One thing that is noticeable here is that there are a lot of interactions between different variables. No single leaf node in this example tree is partitioned with one condition. Every partition in this tree is formed with more than one criterion and interactions between different <kbd>feature</kbd> variables. This is the main difference from logistic regression models. When there is no linear structure in the data, logistic regression models will not be able to perform well, as they try to find linear combinations among the feature variables. On the other hand, decision tree models will perform better for non-linear datasets, as they only try to partition the data at the purest levels they can.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Growing decision trees</h1>
                
            
            
                
<p>When we are growing decision trees, the trees need to come up with a logic to split a node into child nodes. There are two main methods that are commonly used for splitting the data: <strong>Gini impurity</strong> and <strong>entropy information gain</strong>. Simply put, <em>Gini</em> impurity measures how impure a partition is, and entropy information gain measures how much information it gains from splitting the data with the criteria being tested.</p>
<p>Let's take a quick look at the equation to compute the <em>Gini</em> impurity measure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/dcc06319-5223-49a9-bbff-b4475b53664f.png" style="width:10.33em;height:3.92em;"/></p>
<p>Here, <em>c</em> stands for the class labels, and <em>P<sub>i</sub></em> stands for the probability of a record with the class label <em>i</em> being chosen. By subtracting the sum of squared probabilities from one, the <em>Gini</em> impurity measure reaches zero, that is, when all records in each partition or node of a tree are pure with a single target class.</p>
<p>The equation to compute the <em>entropy</em> looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/85b079d0-d694-434a-a6eb-ae22b25f62f5.png" style="width:14.92em;height:3.83em;"/></p>
<p>Like before, <em>c</em> stands for the class labels, and <em>P<sub>i</sub></em> stands for the probability of a record with the class label <em>i</em> being chosen. When growing the tree, the entropy of each possible split needs to be calculated and compared against the entropy before the split. Then, the split that gives the biggest change in entropy measures or the highest information gain will be chosen to grow the tree. This process will be repeated until all of the nodes are pure, or until it meets the stopping criteria. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Decision trees and interpretations with Python</h1>
                
            
            
                
<p>In this section, you are going to learn how to use the <kbd>scikit-learn</kbd> package in Python to build decision tree models and interpret the results via visualizations using Python's <kbd>graphviz</kbd> package. For those readers that would like to use R instead of Python for this exercise, you can skip to the next section. We will start this section by analyzing the bank marketing dataset in depth, using the <kbd>pandas</kbd> and <kbd>matplotlib</kbd> packages, and then we will discuss how to build and interpret decision tree models.</p>
<p>For this exercise, we will be using one of the publicly available datasets from the UCI Machine Learning Repository, which can be found at <a href="https://archive.ics.uci.edu/ml/datasets/bank+marketing">https://archive.ics.uci.edu/ml/datasets/bank+marketing</a>. You can follow the link and download the data in ZIP format. We will use the <kbd>bank.zip</kbd> file for this exercise. When you unzip this file, you will see two CSV files: <kbd>bank.csv</kbd> and <kbd>bank-full.csv</kbd>. We are going to use the <kbd>bank-full.csv</kbd> file for this Python exercise.</p>
<p>In order to load this data into your Jupyter Notebook, you can run the following code:</p>
<pre>%matplotlib inline<br/><br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/><br/>df = pd.read_csv('../data/bank-full.csv', sep=";")</pre>
<p>As you can see from this code snippet, we use the <kbd>%matplotlib inline</kbd> command to show plots on the Jupyter Notebook. Then, we import the <kbd>matplotlib</kbd> and <kbd>pandas</kbd> packages that we are going to use for the data analysis step. Lastly, we can easily read the data file by using the <kbd>read_csv</kbd> function in the <kbd>pandas</kbd> package. One thing to note here is the <kbd>sep</kbd> argument in the <kbd>read_csv</kbd> function. If you look at the data closely, you will notice that the fields in the <kbd>bank-full.csv</kbd> file are separated by semicolons (<kbd>;</kbd>), not commas (<kbd>,</kbd>). In order to correctly load the data into a <kbd>pandas</kbd> DataFrame, we will need to tell the <kbd>read_csv</kbd> function to use semicolons as the separators, instead of commas.</p>
<p>Once you have loaded the data, it should look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1013 image-border" src="img/edd8cfd2-6ceb-4846-aab1-752933a982c3.png" style="width:165.50em;height:49.83em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data analysis and visualization</h1>
                
            
            
                
<p>Before we start to analyze the data, we will first encode the output variable, <kbd>y</kbd>, which has information about whether a customer has converted or subscribed to a term deposit, with numerical values. You can use the following code to encode the output variable, <kbd>y</kbd>, with zeros and ones:</p>
<pre>df['conversion'] = df['y'].apply(lambda x: 0 if x == 'no' else 1)</pre>
<p>As you can see from this code snippet, you can use the <kbd>apply</kbd> function to encode the output variable. We stored these encoded values in a new column, named <kbd>conversion</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Conversion rate</h1>
                
            
            
                
<p>Let's first take a look at the aggregate conversion rate. The <strong>conversion rate</strong> is simply the percentage of customers that subscribed to a term deposit. Take a look at the following code:</p>
<pre>conversion_rate_df = pd.DataFrame(<br/>    df.groupby('conversion').count()['y'] / df.shape[0] * 100.0<br/>)</pre>
<p>As you can see from this code snippet, we are grouping by a column, <kbd>conversion</kbd>, which is encoded with <kbd>1</kbd> for those that have subscribed to a term deposit, and with <kbd>0</kbd> for those that have not. Then, we are counting the number of customers in each group and dividing it by the total number of customers in the dataset. The result looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1028 image-border" src="img/99f9f19c-3871-4394-bd59-55c3fce7f530.png" style="width:23.58em;height:20.42em;"/></p>
<p>To make it easier to view, you can transpose the DataFrame by using the <kbd>T</kbd> attribute of the <kbd>pandas</kbd> DataFrame. As you can see, only about 11.7% were converted or subscribed to a term deposit. From these results, we can see that there is a large imbalance between the conversion group and the non-conversion group, which is common and is frequently observed among various marketing datasets.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Conversion rates by job</h1>
                
            
            
                
<p>It might be true that certain job categories tend to convert more frequently than others. Let's take a look at the conversion rates across different job categories. You can achieve this by using the following code:</p>
<pre>conversion_rate_by_job = df.groupby(<br/>    by='job'<br/>)['conversion'].sum() / df.groupby(<br/>    by='job'<br/>)['conversion'].count() * 100.0</pre>
<p>Let's take a deeper look at this code. We first group by the column, <kbd>job</kbd>, which contains information about the job category that each customer belongs to. Then, we sum over the <kbd>conversion</kbd> column for each job category, from which we get the total number of conversions for each job category. Lastly, we divide these conversion numbers by the total number of customers in each job category, in order to get the conversion rates for each job category.</p>
<p>The results look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1008 image-border" src="img/e5a1ed43-084d-475a-b90a-3346bd87687a.png" style="width:23.50em;height:17.67em;"/></p>
<p>As you can see from these results, the <kbd>student</kbd> group tends to convert much more frequently than the others, and the <kbd>retired</kbd> group comes next. However, it is a bit difficult to compare these from the raw output, and we could present this data better by using a chart. We can build a horizontal bar chart by using the following code:</p>
<pre>ax = conversion_rate_by_job.plot(<br/>    kind='barh',<br/>    color='skyblue',<br/>    grid=True,<br/>    figsize=(10, 7),<br/>    title='Conversion Rates by Job'<br/>)<br/><br/>ax.set_xlabel('conversion rate (%)')<br/>ax.set_ylabel('Job')<br/><br/>plt.show()</pre>
<p>If you look at this code, we are using the <kbd>plot</kbd> function of the <kbd>pandas</kbd> DataFrame, and we defined the type of this plot to be a horizontal bar chart by providing <kbd>barh</kbd> as the input to the <kbd>kind</kbd> argument. You can simply adjust the color, size, and title of the chart with the <kbd>color</kbd>, <kbd>figsize</kbd>, and <kbd>title</kbd> arguments, respectively. You can also easily change the <em>x</em>-axis and <em>y</em>-axis labels, using the <kbd>set_xlabel</kbd> and <kbd>set_ylabel</kbd> functions. </p>
<p>The resulting chart looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1026 image-border" src="img/3b42d4b7-1d54-4889-977a-3a30c5dd02a4.png" style="width:113.25em;height:73.67em;"/></p>
<p>As you can see, it is much easier to spot the differences in the conversion rates by each job category with a horizontal bar chart. We can easily see that the <kbd>student</kbd> and <kbd>retired</kbd> groups are the two groups with the highest conversion rates, whereas the <kbd>blue-collar</kbd> and <kbd>entrepreneur</kbd> groups are the two groups with the lowest conversion rates.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Default rates by conversions</h1>
                
            
            
                
<p>Another attribute of a customer that would be interesting to see is the default rate, and how it differs between those who subscribed to a term deposit and those who did not. We are going to use the <kbd>pivot_table</kbd> function in the <kbd>pandas</kbd> library to analyze the default rates by conversions. Let's take a look at the following code:</p>
<pre>default_by_conversion_df = pd.pivot_table(<br/>    df, <br/>    values='y', <br/>    index='default', <br/>    columns='conversion', <br/>    aggfunc=len<br/>)</pre>
<p>As you can see from this code, we are pivoting the DataFrame, <kbd>df</kbd>, by the <kbd>y</kbd> and <kbd>default</kbd> columns. By using <kbd>len</kbd> as the aggregation function, we can count how many customers fall under each cell of the pivot table. The results look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1020 image-border" src="img/8257487f-4abc-4403-b5d1-6cb79ec4fc94.png" style="width:23.00em;height:11.83em;"/></p>
<p>It is a bit difficult to compare how the default rates differ between the conversion and non-conversion groups by looking at these raw numbers. One way to visualize this data is through a pie chart. You can use the following code to build a pie chart:</p>
<pre>default_by_conversion_df.plot(<br/>    kind='pie',<br/>    figsize=(15, 7),<br/>    startangle=90,<br/>    subplots=True,<br/>    autopct=lambda x: '%0.1f%%' % x<br/>)<br/><br/>plt.show()</pre>
<p>As you can see from this code, we are simply passing <kbd>'pie'</kbd> as input to the <kbd>kind</kbd> argument of the <kbd>plot</kbd> function. The resulting pie chart appears as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1012 image-border" src="img/eba95421-1547-4767-aadd-5494b5440972.png" style="width:137.33em;height:63.50em;"/></p>
<p>As you can see from these pie charts, it is much easier to compare the default rates between the conversion and non-conversion groups. Although the overall percentage of the previous default is low in both groups, the default rate in the non-conversion group is about twice as high as the conversion group.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Bank balances by conversions</h1>
                
            
            
                
<p>Next, we will try to see if there are any differences in the distributions of bank balances between the conversion and non-conversion groups. A box plot is typically a good way to visualize the distribution of a variable. Let's take a look at the following code:</p>
<pre>ax = df[['conversion', 'balance']].boxplot(<br/>    by='conversion',<br/>    showfliers=True,<br/>    figsize=(10, 7)<br/>)<br/><br/>ax.set_xlabel('Conversion')<br/>ax.set_ylabel('Average Bank Balance')<br/>ax.set_title('Average Bank Balance Distributions by Conversion')<br/><br/>plt.suptitle("")<br/>plt.show()</pre>
<p>You should be familiar with this code by now, as we have discussed how to build box plots using the <kbd>pandas</kbd> package. Using the <kbd>boxplot</kbd> function, we can easily build box plots such as the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1019 image-border" src="img/f7970841-e6de-49b0-a7b1-1f4c8084aa40.png" style="width:112.25em;height:75.92em;"/></p>
<p>Because there are so many outliers, it is quite difficult to identify any differences between the two distributions. Let's build another box plot without outliers. The only thing that you need to change from the previous code is the <kbd>showfliers=True</kbd> argument in the <kbd>boxplot</kbd> function, as you can see in the following code:</p>
<pre>ax = df[['conversion', 'balance']].boxplot(<br/>    by='conversion',<br/>    showfliers=False,<br/>    figsize=(10, 7)<br/>)<br/><br/>ax.set_xlabel('Conversion')<br/>ax.set_ylabel('Average Bank Balance')<br/>ax.set_title('Average Bank Balance Distributions by Conversion')<br/><br/>plt.suptitle("")<br/>plt.show()</pre>
<p>Using this code, you will see the following box plots for the distributions of bank balances between the two groups:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/5c8dbee9-2db5-4d9d-bb6a-c721e18d129b.png" style="width:60.00em;height:40.00em;"/></p>
<p>From these box plots, we can see that the median of the bank balance is slightly higher for the conversion group, as compared to the non-conversion group. Also, the bank balances of converted customers seem to vary more than those of non-converted customers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Conversion rates by number of contacts</h1>
                
            
            
                
<p>Lastly, we will look at how the conversion rates vary by the number of contacts. Typically, in marketing, a higher number of marketing touches can result in marketing fatigue, where the conversion rates drop as you reach out to your customers more frequently. Let's see whether there is any marketing fatigue in our data. Take a look at the following code:</p>
<pre>conversions_by_num_contacts = df.groupby(<br/>    by='campaign'<br/>)['conversion'].sum() / df.groupby(<br/>    by='campaign'<br/>)['conversion'].count() * 100.0</pre>
<p>In this code snippet, you can see that we are grouping by the <kbd>campaign</kbd> column (which has information about the number of contacts performed during the marketing campaign for this customer) and computing the conversion rates for each number of contacts. The resulting data appears as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1006 image-border" src="img/db83f22a-027b-4c4c-aac7-b418f88aa2f0.png" style="width:26.75em;height:23.00em;"/></p>
<p>Like before, it would be easier to look at a chart, rather than raw numbers. We can plot this data by using bar charts, with the following code:</p>
<pre>ax = conversions_by_num_contacts.plot(<br/>    kind='bar',<br/>    figsize=(10, 7),<br/>    title='Conversion Rates by Number of Contacts',<br/>    grid=True,<br/>    color='skyblue'<br/>)<br/><br/>ax.set_xlabel('Number of Contacts')<br/>ax.set_ylabel('Conversion Rate (%)')<br/><br/>plt.show()</pre>
<p>The plot looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1010 image-border" src="img/6cbe0312-537f-4600-b2f6-e49f6a64e8ef.png" style="width:59.67em;height:42.92em;"/></p>
<p>There's some noise in a higher numbers of contacts, as the sample size is smaller for them, but you can easily see the overall downward trend in this bar chart. As the number of contacts increases, the conversion rates slowly decrease. This suggests that the expected conversion rate decreases as you contact a client more frequently for a given campaign.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Encoding categorical variables</h1>
                
            
            
                
<p>There are eight categorical variables in this dataset: <kbd>job</kbd>, <kbd>marital</kbd>, <kbd>education</kbd>, <kbd>default</kbd>, <kbd>housing</kbd>, <kbd>loan</kbd>, <kbd>contact</kbd>, and <kbd>month</kbd>. Before we start to build decision trees, we need to encode these categorical variables with numerical values. We'll take a look at how we can encode some of these categorical variables in this section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Encoding months</h1>
                
            
            
                
<p>We all know that there can only be 12 unique values for the <kbd>month</kbd> variable. Let's take a quick look at what we have in our dataset. Take a look at the following code:</p>
<pre>df['month'].unique()</pre>
<p>The <kbd>pandas</kbd> function, <kbd>unique</kbd>, helps you to quickly get the unique values in the given column. When you run this code, you will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1009 image-border" src="img/57913c36-6c06-41d1-bea1-943abd0d37bf.png" style="width:40.58em;height:5.33em;"/></p>
<p>As expected, we have 12 unique values for the <kbd>month</kbd> column, from January to December. Since there is a natural ordering in the values of <kbd>month</kbd>, we can encode each of the values with a corresponding number. One way to encode the string values of <kbd>month</kbd> with numbers is shown as follows:</p>
<pre>months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']<br/><br/>df['month'] = df['month'].apply(<br/>    lambda x: months.index(x)+1<br/>)</pre>
<p>Using this code, the unique values for the column <kbd>month</kbd> look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1017 image-border" src="img/3311d783-0ac2-4a7f-9157-7083a3a88926.png" style="width:33.50em;height:4.00em;"/></p>
<p>To see how many records we have for each month, we can use the following code:</p>
<pre>df.groupby('month').count()['conversion']</pre>
<p>The results are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1018 image-border" src="img/aae40fb2-16e3-4a60-ba13-4afb2472f525.png" style="width:27.75em;height:17.33em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Encoding jobs</h1>
                
            
            
                
<p>Next, let's look at how we can encode the different categories of the <kbd>job</kbd> column. We will first look at the unique values in this column, using the following code:</p>
<pre>df['job'].unique()</pre>
<p>The unique values in the  <kbd>job</kbd> column look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1007 image-border" src="img/b7e7b157-3f55-40e3-b11e-01ce1297c386.png" style="width:40.25em;height:6.83em;"/></p>
<p>As you can see in this output, there is no natural ordering for this variable. One <kbd>job</kbd> category does not precede the other, so we cannot encode this variable like we did for <kbd>month</kbd>. We are going to create dummy variables for each of the <kbd>job</kbd> categories. If you recall from the previous chapter, a <strong>dummy variable</strong> is a variable that is encoded with <kbd>1</kbd> if a given record belongs to the category, and <kbd>0</kbd> if not. We can do this easily by using the following code:</p>
<pre>jobs_encoded_df = pd.get_dummies(df['job'])<br/>jobs_encoded_df.columns = ['job_%s' % x for x in jobs_encoded_df.columns]</pre>
<p>As you can see from this code snippet, the <kbd>get_dummies</kbd> function in the <kbd>pandas</kbd> package creates one dummy variable for each category in the <kbd>job</kbd> variable, and encodes each record with <kbd>1</kbd> if the given record belongs to the corresponding category, and <kbd>0</kbd> if not. Then, we rename the columns by prefixing each column with <kbd>job_</kbd>. The result looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1003 image-border" src="img/aa02a263-7126-4b15-a075-8e0c219d860d.png" style="width:162.50em;height:23.33em;"/></p>
<p>As you can see from this screenshot, the first record (or customer) belongs to the <kbd>management</kbd> job category, while the second record belongs to the <kbd>technician</kbd> job category. Now that we have created dummy variables for each job category, we need to append this data to the existing DataFrame. Take a look at the following code:</p>
<pre>df = pd.concat([df, jobs_encoded_df], axis=1)<br/>df.head()</pre>
<p>Using the <kbd>concat</kbd> function in the <kbd>pandas</kbd> package, you can easily add the newly created DataFrame with dummy variables, <kbd>jobs_encoded_df</kbd>, to the original DataFrame, <kbd>df</kbd>. The <kbd>axis=1</kbd> argument tells the <kbd>concat</kbd> function to concatenate the second DataFrame to the first DataFrame as columns, not as rows. The resulting DataFrame looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0895e1e0-aa45-40b8-9e99-a680449c2c17.png"/></p>
<p>As you can see, the newly created dummy variables are added to the original DataFrame as new columns for each record.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Encoding marital</h1>
                
            
            
                
<p>Similar to how we encoded the categorical variable, <kbd>job</kbd>, we are going to create dummy variables for each category of the <kbd>marital</kbd> variable. Like before, we are using the following code to encode the <kbd>marital</kbd> column:</p>
<pre>marital_encoded_df = pd.get_dummies(df['marital'])<br/>marital_encoded_df.columns = ['marital_%s' % x for x in marital_encoded_df.columns]</pre>
<p>The encoding results are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1024 image-border" src="img/56513a91-bce6-454c-939a-8a8c3b51cb84.png" style="width:26.67em;height:14.42em;"/></p>
<p>As you can see, three new variables are created for the original variable, <kbd>marital</kbd>: <kbd>marital_divorced</kbd>, <kbd>marital_married</kbd>, and <kbd>marital_single</kbd>, representing whether a given customer is divorced, married, or single, respectively. In order to add these newly created dummy variables to the original DataFrame, we can use the following code:</p>
<pre>df = pd.concat([df, marital_encoded_df], axis=1)</pre>
<p>Once you have come this far, your original DataFrame, <kbd>df</kbd>, should contain all of the original columns, plus newly created dummy variables for the <kbd>job</kbd> and <kbd>marital</kbd> columns.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Encoding the housing and loan variables</h1>
                
            
            
                
<p>The last two categorical variables that we are going to encode in this section are <kbd>housing</kbd> and <kbd>loan</kbd>. The <kbd>housing</kbd> variable has two unique values, <kbd>'yes'</kbd> and <kbd>'no'</kbd>, and contains information on whether a customer has a housing loan. The other variable, <kbd>loan</kbd>, also has two unique values, <kbd>'yes'</kbd> and <kbd>'no'</kbd>, and tells us whether a customer has a personal loan. We can easily encode these two variables by using the following code:</p>
<pre>df['housing'] = df['housing'].apply(lambda x: 1 if x == 'yes' else 0)<br/><br/>df['loan'] = df['loan'].apply(lambda x: 1 if x == 'yes' else 0)</pre>
<p>As you can see, we are using the <kbd>apply</kbd> function to encode <kbd>yes</kbd> as <kbd>1</kbd> and <kbd>no</kbd> as <kbd>0</kbd> for both the housing and loan variables. For those categorical variables that we have not discussed in this section, you can use the same techniques that we have discussed to encode them if you wish to explore beyond this exercise.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Building decision trees</h1>
                
            
            
                
<p>Now that we have encoded all of the categorical variables, we can finally start to build decision tree models. We are going to use the following variables as features in our decision tree models:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1025 image-border" src="img/33e32db6-b239-4b18-b1f5-13d2e9086fac.png" style="width:18.83em;height:24.17em;"/></p>
<p>In order to build and train a decision tree model with Python, we are going to use the <kbd>tree</kbd> module in the <kbd>scikit-learn</kbd> (<kbd>sklearn</kbd>) package. You can import the required module by using the following line of code:</p>
<pre>from sklearn import tree</pre>
<p>Under the <kbd>tree</kbd> module in the <kbd>sklearn</kbd> package, there is a class named <kbd>DecisionTreeClassifier</kbd>, which we can use to train a decision tree model. Take a look at the following code:</p>
<pre>dt_model = tree.DecisionTreeClassifier(<br/>    max_depth=4<br/>)</pre>
<p>There are many arguments to the <kbd>DecisionTreeClassifier</kbd> class, aside from the one that we are using here, <kbd>max_depth</kbd>. The<kbd>max_depth</kbd> argument controls how much a tree can grow, and here, we limit it to <kbd>4</kbd>, meaning that the maximum length from the root to a leaf can be <kbd>4</kbd>. You can also use the <kbd>criterion</kbd> argument to choose between the Gini impurity and the entropy information gain measures for the quality of a split. There are many other ways to tune your decision tree model, and we recommend that, for more information, you take a closer look at the documentation at <a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a>.</p>
<p>In order to train this decision tree model, you can use the following code:</p>
<pre>dt_model.fit(df[features], df[response_var])</pre>
<p>As you can see from this code, the <kbd>fit</kbd> function takes two arguments: the <kbd>predictor</kbd> or <kbd>feature</kbd> variables and the <kbd>response</kbd> or <kbd>target</kbd> variables. In our case, <kbd>response_var</kbd> is the <kbd>conversion</kbd> column of the DataFrame, <kbd>df</kbd>. Once you have run this code, the decision tree model will learn how to make classifications. In the following section, we will discuss how we can interpret the results of this trained decision tree model.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Interpreting decision trees</h1>
                
            
            
                
<p>Now that we have trained a decision tree model, we need to extract the insights from the model. In this section, we are going to use a package called <kbd>graphviz</kbd>. You can install this package by using the following command in your Terminal:</p>
<pre><strong>conda install python-graphviz</strong></pre>
<p>Once you have installed this package correctly, you should be able to import the package as follows:</p>
<pre><strong>import graphviz</strong></pre>
<p>Now that we have set up our environment with the new package, <kbd>graphviz</kbd>, let's take a look at the following code to see how we can visualize the trained decision tree:</p>
<pre>dot_data = tree.export_graphviz(<br/>    dt_model, <br/>    feature_names=features, <br/>    class_names=['0', '1'], <br/>    filled=True, <br/>    rounded=True, <br/>    special_characters=True<br/>) <br/><br/>graph = graphviz.Source(dot_data)</pre>
<p>As you can see, we first export the trained decision tree model, <kbd>dt_model</kbd>, using the <kbd>export_graphviz</kbd> function in the <kbd>tree</kbd> module of the <kbd>sklearn</kbd> package. We can define the feature variables that we used to train this model by using the <kbd>feature_names</kbd> argument. Then, we can define the classes (conversion versus non-conversion) that this model is trained to classify. The <kbd>export_graphviz</kbd> function exports the trained decision tree model in a DOT format, which is a graphic description language. You can then pass <kbd>dot_data</kbd> on to the <kbd>graphviz</kbd> <kbd>Source</kbd> class. The <kbd>graph</kbd> variable now contains a renderable graph. The root node and its direct children look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d20c75f2-fa12-4a5b-8ee3-bd4e5fc9d310.png" style="width:47.58em;height:18.83em;"/></p>
<p>The tree on the left half (or the children of the root node's left child) looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/5e156f7d-b72b-4e9f-956c-d646b7f276cf.png"/></p>
<p>The tree on the right half (or the children of the root node's right child) looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/852140bc-7f6a-4553-b94c-b0373d043921.png"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's take a closer look at this diagram. Each node contains five lines that describe the information that the given node has. The top line tells us the criteria of the split. The root node, for example, is split into its child nodes based on the value of the <kbd>previous</kbd> variable. If the value of this <kbd>previous</kbd> variable is less than or equal to <kbd>0.5</kbd>, then it goes to the left child. On the other hand, if the value of this <kbd>previous</kbd> variable is larger than <kbd>0.5</kbd>, then it goes to the right child. </p>
<p>The second line tells us the value of the quality measure for the split. Here, we selected the <kbd>gini</kbd> impurity measure for the criteria, so we can see the changes in the impurity measures in each node from the second line. The third line tells us the total number of records that belong to the given node. For example, there are <kbd>45,211</kbd> samples in the root node, and there are <kbd>8,257</kbd> samples in the right child of the root node.</p>
<p>The fourth line in each node tells us the composition of the records in two different classes. The first element stands for the number of records in the non-conversion group, and the second element stands for the number of records in the conversion group. For example, in the root node, there are <kbd>39,922</kbd> records in the non-conversion group and <kbd>5,289</kbd> records in the conversion group. Lastly, the fifth line in each node tells us what the prediction or classification will be for the given node. For example, if a sample belongs to the leftmost leaf, the classification by this decision tree model will be <kbd>0</kbd>, meaning non-conversion. On the other hand, if a sample belongs to the eighth leaf from the left, the classification by this decision tree model will be <kbd>1</kbd>, meaning conversion.</p>
<p>Now that we know what each of the lines in each nodes means, let's discuss how we can draw insights from this tree graph. In order to understand the customers that belong to each leaf node, we need to follow through the tree. For example, those customers that belong to the eighth leaf node from the left are those with a <kbd>0</kbd> value for the <kbd>previous</kbd> variable, <kbd>age</kbd> greater than <kbd>60.5</kbd>, a <kbd>marital_divorced</kbd> variable with a value of <kbd>1</kbd>, and a <kbd>job_self-employed</kbd> variable with a value of <kbd>1</kbd>. In other words, those who were not contacted before this campaign and who are older than <kbd>60.5</kbd>, divorced, and self-employed belong to this node, and have a high chance of converting.</p>
<p>Let's take a look at another example. Those customers that belong to the second leaf node from the right are those with a value of <kbd>1</kbd> for the <kbd>previous</kbd> variable, a value of <kbd>1</kbd> for the <kbd>housing</kbd> variable, <kbd>age</kbd> greater than <kbd>60.5</kbd>, and <kbd>balance</kbd> less than or equal to <kbd>4,660.5</kbd>. In other words, those customers that were contacted before this campaign and that have a housing loan, are older than <kbd>60.5</kbd>, and have a bank balance less than <kbd>4,660.5</kbd> belong to this node and <kbd>20</kbd> out of <kbd>29</kbd> that belong to this node have converted and subscribed to a term deposit.</p>
<p>As you will have noticed from these two examples, you can draw useful insights about who is more or less likely to convert from trained decision tree models, by visualizing the trained tree. You simply need to follow through the nodes and understand what kinds of attributes are highly correlated with your target class. For this exercise, we restricted the tree to only growing up to a depth of <kbd>4</kbd>, but you can choose to grow a tree larger or smaller than the one we used in this exercise.</p>
<p>The full code for this chapter's Python exercise can be found in the repository at <a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.4/python/From%20Engagement%20to%20Conversions.ipynb">https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.4/python/From%20Engagement%20to%20Conversions.ipynb</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Decision trees and interpretations with R</h1>
                
            
            
                
<p>In this section, you are going to learn how to use the <kbd>rpart</kbd> package in R to build decision tree models and interpret the results via visualizations with the R <kbd>rattle</kbd> package. For those readers that would like to use Python instead of R for this exercise, you can work through the Python examples in the previous section. We will start this section by analyzing the bank marketing dataset in depth, using the <kbd>dplyr</kbd> and <kbd>ggplot2</kbd> libraries, and then we will discuss how to build and interpret decision tree models.</p>
<p>For this exercise, we will be using one of the publicly available datasets from the UCI Machine Learning Repository, which can be found at <a href="https://archive.ics.uci.edu/ml/datasets/bank+marketing">https://archive.ics.uci.edu/ml/datasets/bank+marketing</a>. You can follow the link and download the data in ZIP format. We will use the <kbd>bank.zip</kbd> file for this exercise. When you unzip this file, you will see two CSV files: <kbd>bank.csv</kbd> and <kbd>bank-full.csv</kbd>. We are going to use the <kbd>bank-full.csv</kbd> file for this exercise.</p>
<p>In order to load this data into your RStudio, you can run the following code:</p>
<pre>df &lt;- read.csv(<br/>  file="../data/bank-full.csv", <br/>  header=TRUE, <br/>  sep=";"<br/>)</pre>
<p>As you can see from this code snippet, we can easily read the data file by using the <kbd>read.csv</kbd> function in R. One thing to note here is the <kbd>sep</kbd> argument in the <kbd>read.csv</kbd> function. If you look at the data closely, you will notice that the fields in the <kbd>bank-full.csv</kbd> file are separated by semicolons (<kbd>;</kbd>), not commas (<kbd>,</kbd>). In order to correctly load the data into a DataFrame, we will need to tell the <kbd>read.csv</kbd> function to use semicolons as the separators, instead of commas.</p>
<p>Once you have loaded this data, it should look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1023 image-border" src="img/de1dbdf1-84f4-4b82-ad07-743d1e2b754f.png" style="width:156.33em;height:42.33em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data analysis and visualizations</h1>
                
            
            
                
<p>Before we start to analyze the data, we will first encode the output variable, <kbd>y</kbd>, which has information about whether a customer has converted or subscribed to a term deposit, with numerical values. You can use the following code to encode the output variable, <kbd>y</kbd>, with zeros and ones:</p>
<pre># Encode conversions as 0s and 1s<br/>df$conversion &lt;- as.integer(df$y) - 1</pre>
<p>As you can see from this code snippet, you can use the <kbd>as.integer</kbd> function to encode the output variable. Since this function will encode <kbd>no</kbd> values in the <kbd>y</kbd> variable  as <kbd>1</kbd> and <kbd>yes</kbd> values in the <kbd>y</kbd> variable as <kbd>2</kbd>, we subtract the values by <kbd>1</kbd> to encode them as <kbd>0</kbd> and <kbd>1</kbd>, respectively. We stored these encoded values into a new column, named <kbd>conversion</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Conversion rate</h1>
                
            
            
                
<p>The first thing that we are going to take a look at is the aggregate conversion rate. The conversion rate is simply the percentage of customers that subscribed to a term deposit, or those encoded with <kbd>1</kbd> in the column <kbd>conversion</kbd>. Take a look at the following code:</p>
<pre>sprintf("conversion rate: %0.2f%%", sum(df$conversion)/nrow(df)*100.0)</pre>
<p>As you can see from this code snippet, we simply sum all of the values in the <kbd>conversion</kbd> column and divide by the number of records or customers in the DataFrame, <kbd>df</kbd>. Using the <kbd>sprintf</kbd> function, we format this conversion rate number with two decimal point numbers. The result looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c21a0e9b-e719-424d-ba6c-f915041eecc7.png" style="width:36.92em;height:2.83em;"/></p>
<p>As you can see from this output, only about <kbd>11.7%</kbd> were converted or subscribed to a term deposit. From these results, we can see that there is a large imbalance between the conversion group and the non-conversion group, which is quite common and is frequently observed among various marketing datasets.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Conversion rates by job</h1>
                
            
            
                
<p>It might be true that certain job categories tend to convert more frequently than others. Let's take a look at the conversion rates across different job categories. You can achieve this by running the following code:</p>
<pre>conversionsByJob &lt;- df %&gt;% <br/>  group_by(Job=job) %&gt;% <br/>  summarise(Count=n(), NumConversions=sum(conversion)) %&gt;%<br/>  mutate(ConversionRate=NumConversions/Count*100.0)</pre>
<p>Let's take a more detailed look at this code. We first group by the column, <kbd>job</kbd>, which contains information about the job category that each customer belongs to. Then, we count the total number of customers in a given job category by using the <kbd>n()</kbd> function, and sum over the <kbd>conversion</kbd> column for each job category by using the <kbd>sum</kbd> function. Lastly, we divide the total number of conversions, <kbd>NumConversion</kbd>, by the total number of customers in each job category, <kbd>Count</kbd>, and multiply these numbers by <kbd>100.0</kbd> to get the conversion rates for each job category.</p>
<p>The results look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1016 image-border" src="img/d05fb8b6-299f-466b-b73b-4fb7093fff02.png" style="width:30.17em;height:23.00em;"/></p>
<p>As you can see from these results, the <kbd>student</kbd> group tends to convert much more frequently than the others, and the <kbd>retired</kbd> group comes next. However, it is a bit difficult to compare these with raw output, and we will be able to better present this data by using a chart. We can build a horizontal bar chart by using the following code:</p>
<pre>ggplot(conversionsByJob, aes(x=Job, y=ConversionRate)) +<br/>  geom_bar(width=0.5, stat="identity") +<br/>  coord_flip() +<br/>  ggtitle('Conversion Rates by Job') +<br/>  xlab("Job") +<br/>  ylab("Conversion Rate (%)") +<br/>  theme(plot.title = element_text(hjust = 0.5)) </pre>
<p>If you look at this code, we are using the <kbd>ggplot</kbd> and <kbd>geom_bar</kbd> functions to build a bar chart with the <kbd>conversionsByJob</kbd> data (which we built in the previous code), and with the <kbd>Job</kbd> variable in the <em>x</em>-axis and the <kbd>ConversionRate</kbd> variable in the <em>y</em>-axis. Then, we use the <kbd>coord_flip</kbd> function to flip the vertical bar chart to a horizontal bar chart. You can use the <kbd>ggtitle</kbd>, <kbd>xlab</kbd>, and <kbd>ylab</kbd> functions to change the title, <em>x</em>-axis label, and <em>y</em>-axis label as you wish.</p>
<p>The resulting chart looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1027 image-border" src="img/cbd51610-3022-4180-b258-0618f507ee19.png" style="width:53.92em;height:33.67em;"/></p>
<p>As you can see, it is much easier to see the differences in the conversion rates by each job category with a horizontal bar chart. We can easily see that the <kbd>student</kbd> and <kbd>retired</kbd> groups are the two groups with the highest conversion rates, whereas, the <kbd>blue-collar</kbd> and <kbd>entrepreneur</kbd> groups are the two groups with the lowest conversion rates.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Default rates by conversions</h1>
                
            
            
                
<p>Another attribute of a customer that would be interesting to see is the default rate, and how it differs between those who subscribed to a term deposit and those who did not. Let's take a look at the following R code:</p>
<pre>defaultByConversion &lt;- df %&gt;% <br/>  group_by(Default=default, Conversion=conversion) %&gt;% <br/>  summarise(Count=n())</pre>
<p>As you can see from this code, we are grouping the DataFrame, <kbd>df</kbd>, by the two columns, <kbd>default</kbd> and <kbd>conversion</kbd>, using the <kbd>group_by</kbd> function. By using <kbd>n()</kbd> as the aggregation function, we can count how many customers fall under each cell of the four cases. Let's look at the following results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1022 image-border" src="img/320df0e0-2b56-4fc7-8c26-e85a4ed037e9.png" style="width:16.08em;height:8.42em;"/></p>
<p>It is a bit difficult to compare how the default rates differ between the conversion and non-conversion groups from looking at these raw numbers. One way to visualize this data is through a pie chart. You can use the following code to build a pie chart:</p>
<pre>ggplot(defaultByConversion, aes(x="", y=Count, fill=Default)) + <br/>  geom_bar(width=1, stat = "identity", position=position_fill()) +<br/>  geom_text(aes(x=1.25, label=Count), position=position_fill(vjust = 0.5)) +<br/>  coord_polar("y") +<br/>  facet_wrap(~Conversion) +<br/>  ggtitle('Default (0: Non Conversions, 1: Conversions)') +<br/>  theme(<br/>    axis.title.x=element_blank(),<br/>    axis.title.y=element_blank(),<br/>    plot.title=element_text(hjust=0.5),<br/>    legend.position='bottom'<br/>  )</pre>
<p>As you can see, we are utilizing three functions here: <kbd>ggplot</kbd>, <kbd>geom_bar</kbd>, and <kbd>coord_polar("y")</kbd>. With the <kbd>coord_polar("y")</kbd> function, we can get the pie chart from a bar chart. Then, we can use the <kbd>facet_wrap</kbd> function to split it into two pie charts: one for the conversion group and another for the non-conversion group.</p>
<p>Take a look at the following pie chart:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1005 image-border" src="img/1fe05b29-dda3-45fa-bd31-a123981a2bc1.png" style="width:42.75em;height:28.08em;"/></p>
<p>As you can see from these pie charts, it is much easier to compare the default rates between the conversion and non-conversion groups. Although the overall percentage of previous default is low in both groups, the default rate in the non-conversion group is about twice as high as the conversion group.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Bank balance by conversions</h1>
                
            
            
                
<p>Next, we will try to see whether there are any differences in the distributions of the bank balances between the conversion and non-conversion groups. A box plot is typically a good way to visualize the distribution of a variable. Let's take a look at the following code:</p>
<pre>ggplot(df, aes(x="", y=balance)) + <br/>  geom_boxplot() +<br/>  facet_wrap(~conversion) +<br/>  ylab("balance") +<br/>  xlab("0: Non-Conversion, 1: Conversion") +<br/>  ggtitle("Conversion vs. Non-Conversions: Balance") +<br/>  theme(plot.title=element_text(hjust=0.5))</pre>
<p>You should be familiar with this code by now, as we discussed how to build box plots in the previous chapter, using the <kbd>ggplot</kbd> and <kbd>geom_boxplot</kbd> functions. When you run this code, you will see the following box plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1014 image-border" src="img/f3aa26a3-e3d0-4b3d-b3b2-04c62b23b2ab.png" style="width:53.83em;height:33.83em;"/></p>
<p>Because there are so many outliers, it is quite difficult to identify any differences between the two distributions. Let's build another box plot without outliers. The only thing that you need to change from the previous code is the <kbd>outlier.shape = NA</kbd> argument in the <kbd>geom_boxplot</kbd> function, as you can see in the following code:</p>
<pre>ggplot(df, aes(x="", y=balance)) + <br/>  geom_boxplot(outlier.shape = NA) +<br/>  scale_y_continuous(limits = c(-2000, 5000)) +<br/>  facet_wrap(~conversion) +<br/>  ylab("balance") +<br/>  xlab("0: Non-Conversion, 1: Conversion") +<br/>  ggtitle("Conversion vs. Non-Conversions: Balance") +<br/>  theme(plot.title=element_text(hjust=0.5))</pre>
<p>Using this code, you will see the following box plots for the distribution of bank balances between the two groups:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1004 image-border" src="img/237999b5-a990-408b-8fd1-a96d7a52acbe.png" style="width:54.00em;height:34.00em;"/></p>
<p>From these box plots, we can see that the median of the bank balance is slightly higher for the conversion group, as compared to the non-conversion group. Also, the bank balances of converted customers seem to vary more than those of non-converted customers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Conversion rates by number of contacts</h1>
                
            
            
                
<p>Lastly, we will look at how the conversion rates vary by the number of contacts. Typically, in marketing, a higher number of marketing contacts can result in marketing fatigue, wherein the conversion rates drop as you reach out to your customers more frequently. Let's see whether there is any marketing fatigue in our data. Take a look at the following code:</p>
<pre>conversionsByNumContacts &lt;- df %&gt;% <br/>  group_by(Campaign=campaign) %&gt;% <br/>  summarise(Count=n(), NumConversions=sum(conversion)) %&gt;%<br/>  mutate(ConversionRate=NumConversions/Count*100.0)</pre>
<p>From this code snippet, you can see that we are grouping by the  <kbd>campaign</kbd> column (which has information about the number of contacts performed during the marketing campaign for this customer) and computing the conversion rate for each number of contacts. The resulting data looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1015 image-border" src="img/f126d9cd-bfdd-4aa3-a15e-92f5bf715f6a.png" style="width:27.75em;height:25.42em;"/></p>
<p>Like before, it would be easier to look at a chart rather than raw numbers. We can plot this data with bar charts by using the following code:</p>
<pre>ggplot(conversionsByNumContacts, aes(x=Campaign, y=ConversionRate)) +<br/>  geom_bar(width=0.5, stat="identity") +<br/>  ggtitle('Conversion Rates by Number of Contacts') +<br/>  xlab("Number of Contacts") +<br/>  ylab("Conversion Rate (%)") +<br/>  theme(plot.title = element_text(hjust = 0.5)) </pre>
<p>The plot looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/59b26b80-2ed8-46cf-88e9-c12a1b471cac.png" style="width:40.00em;height:25.17em;"/></p>
<p>There is some noise in higher numbers of contacts, as the sample size is smaller for them, but you can easily see the overall downward trend in this bar chart. As the number of contacts increases, the conversion rates slowly decrease. This suggests that the expected conversion rate decreases as you contact a client more frequently for a given campaign.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Encoding categorical variables</h1>
                
            
            
                
<p>There are eight categorical variables in this dataset: <kbd>job</kbd>, <kbd>marital</kbd>, <kbd>education</kbd>, <kbd>default</kbd>, <kbd>housing</kbd>, <kbd>loan</kbd>, <kbd>contact</kbd>, and <kbd>month</kbd>. Before we start to build decision trees, we need to encode some of these categorical variables with numerical values. We'll take a look at how we can encode some of these categorical variables in this section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Encoding the month</h1>
                
            
            
                
<p>We all know that there can only be 12 unique values for the <kbd>month</kbd> variable. Let's take a quick look at what we have in our dataset. Take a look at the following code:</p>
<pre>unique(df$month)</pre>
<p class="mce-root"/>
<p>The <kbd>unique</kbd> function helps you to quickly get the unique values in the given column. When you run this code, you will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/bea193fa-38af-45e1-a779-c06d0491a339.png" style="width:27.42em;height:3.75em;"/></p>
<p>As we expected, we have 12 unique values for the <kbd>month</kbd> column, from January to December. Since there is a natural order in the values of <kbd>month</kbd>, we can encode each of the values with the corresponding number. One way to encode the string values of <kbd>month</kbd> with numbers is as follows:</p>
<pre>months = lapply(month.abb, function(x) tolower(x))<br/>df$month &lt;- match(df$month, months)</pre>
<p>Let's take a closer look at this code. <kbd>month.abb</kbd> is a built-in R constant that contains the three-letter abbreviated names for the month names, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/2f8db0b0-9ba1-4eea-8438-57e61afed48d.png" style="width:38.83em;height:2.83em;"/></p>
<p>As you can see, the first letters of each abbreviated <kbd>month</kbd> name are capitalized. However, the month names in the <kbd>month</kbd> column of our data are all in lowercase. That is why we use the <kbd>tolower</kbd> function to make all of the values in the <kbd>month.abb</kbd> constant lowercase. Using the <kbd>lapply</kbd> function, we can apply this <kbd>tolower</kbd> function across the <kbd>month.abb</kbd> list. Then, we use the <kbd>match</kbd> function, which returns the position of the matching string in an array, to convert the string values in the <kbd>month</kbd> column of the DataFrame to corresponding numerical values.</p>
<p>Using this code, the unique values for the <kbd>month</kbd> column look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1021 image-border" src="img/423f83ae-563e-48b4-8a30-3c8290e24a1b.png" style="width:21.42em;height:2.67em;"/></p>
<p>To see how many records we have for each month, we can use the following code:</p>
<pre>df %&gt;% <br/>  group_by(month) %&gt;% <br/>  summarise(Count=n())</pre>
<p>The results are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/4271c54d-3965-4a56-a046-162327636d1b.png" style="width:13.58em;height:22.17em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Encoding the job, housing, and marital variables</h1>
                
            
            
                
<p>Next, we are going to encode the three variables: <kbd>job</kbd>, <kbd>housing</kbd>, and <kbd>marital</kbd>. Since these variables do not have natural orders, we do not need to worry about which category gets encoded with which value. The simplest way to encode categorical variables with no orders in R is to use the <kbd>factor</kbd> function. Let's take a look at the following code:</p>
<pre>df$job &lt;- factor(df$job)<br/>df$housing &lt;- factor(df$housing)<br/>df$marital &lt;- factor(df$marital)</pre>
<p>As you can see from this code, we are simply applying the <kbd>factor</kbd> function for these three variables, <kbd>job</kbd>, <kbd>housing</kbd>, and <kbd>marital</kbd>, and storing the encoded values back to the DataFrame, <kbd>df</kbd>. For the categorical variables that we have not discussed in this section, you can use the same techniques that we discussed in this section to encode them if you wish to explore beyond this exercise.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Building decision trees</h1>
                
            
            
                
<p>Now that we have encoded all of the categorical variables, we can finally start to build decision tree models. We are going to use these variables as features for our decision tree models: <kbd>age</kbd>, <kbd>balance</kbd>, <kbd>campaign</kbd>, <kbd>previous</kbd>, <kbd>housing</kbd>, <kbd>job</kbd>, and <kbd>marital</kbd>. In order to build and train a decision tree model with R, we are going to use the <kbd>rpart</kbd> package. You can import the required library by using the following line of code:</p>
<pre>library(rpart)</pre>
<p>If you do not have the <kbd>rpart</kbd> package installed, you can install it by using the following command:</p>
<pre>install.packages("rpart")</pre>
<p>Once you have imported the required library, you can use the following code to build a decision tree model:</p>
<pre>fit &lt;- rpart(<br/>  conversion ~ age + balance + campaign + previous + housing + job + marital,<br/>  method="class", <br/>  data=df,<br/>  control=rpart.control(maxdepth=4, cp=0.0001)<br/>)</pre>
<p>As you can see, the first argument of the <kbd>rpart</kbd> model is <kbd>formula</kbd>, which defines the features and the target variable. Here, we are using the aforementioned variables as the features and <kbd>conversion</kbd> as the target variable. Then, we define this decision tree model to be a classification model with the <kbd>method="class"</kbd> input. Lastly, you can fine-tune the decision tree model with the <kbd>control</kbd> input. There are many parameters that you can tune with the <kbd>control</kbd> input. In this example, we are only restricting the maximum depth of the tree to be <kbd>4</kbd> with the <kbd>maxdepth</kbd> argument, and setting the value for <kbd>cp</kbd>, which is the complexity parameter, to be small enough for the tree to be able to be split. There are many other ways to tune your decision tree model, and we recommend that you take a closer look at the R documentation for more information, by running the <kbd>help(rpart)</kbd> or <kbd>help(rpart.control)</kbd> commands.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Interpreting decision trees</h1>
                
            
            
                
<p>Now that we have trained a decision tree model, we need to extract the insights from the model. In this section, we are going to use a library called <kbd>rattle</kbd>:</p>
<ol>
<li>You can install this package by using the following command in your RStudio:</li>
</ol>
<pre style="padding-left: 90px">install.packages("rattle")</pre>
<ol start="2">
<li>Once you have installed this library correctly, you should be able to import the library as follows:</li>
</ol>
<pre style="padding-left: 90px">library(rattle)</pre>
<ol start="3">
<li>Once you have set up your R environment with this new library, <kbd>rattle</kbd>, it requires just one line of code to visualize the trained decision tree. Take a look at the following code:</li>
</ol>
<pre style="padding-left: 90px">fancyRpartPlot(fit)</pre>
<ol start="4">
<li>As you can see, the <kbd>fancyRpartPlot</kbd> function takes in an <kbd>rpart</kbd> model object. Here, the model object, <kbd>fit</kbd>, is the decision tree model that we built in the previous step. Once you run this command, it will show the following diagram:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/7829b6ee-ce98-41ba-9a99-c0b02ba49e54.png" style="width:33.08em;height:20.75em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's take a closer look at this tree diagram. Each node contains three lines that describe the information that the given node has. The number on top of the node is the label and the order of the node that was built. We will use this label to refer to each of the nodes in this tree graph. Then, the top line in each node tells us what the prediction or classification will be for the given node. For example, if a sample belongs to the node that is labeled <kbd>4</kbd>, the classification by this decision tree model will be zero, meaning non-conversion. On the other hand, if a sample belongs to the node labeled <kbd>23</kbd>, the classification by this decision tree model will be one, meaning conversion.</p>
<p>The second line in each node tells us the percentage of records in each class for the given node. For example, <kbd>52%</kbd> of the records in node <kbd>22</kbd> are in the class <kbd>0</kbd>, or the non-conversion group, and the remaining <kbd>48%</kbd> are in the class <kbd>1</kbd>, or the conversion group. On the other hand, <kbd>39%</kbd> of the customers in node <kbd>13</kbd> are in the class <kbd>0</kbd>, and the remaining <kbd>61%</kbd> of the customers in node <kbd>13</kbd> are in the class <kbd>1</kbd>. Lastly, the bottom line in each node tells us the percentage of the total number of records that belong to each node. For example, about <kbd>80%</kbd> of the customers fall under the category of node <kbd>4</kbd>, while close to <kbd>0%</kbd> of the customers fall under the category of node <kbd>13</kbd>.</p>
<p>Now that we know what each of the lines in each nodes means, let's discuss how we can draw insights from this tree diagram. In order to understand the customers that belong to each leaf node, we need to follow through the tree. For example, those customers that belong to node <kbd>13</kbd> are those with values greater than <kbd>0.5</kbd> for the <kbd>previous</kbd> variable, with a housing loan and <kbd>age</kbd> greater than or equal to <kbd>61</kbd>. In other words, those who were contacted before this campaign and who are older than <kbd>61</kbd>, with housing loans, belong to node <kbd>13</kbd> and have a high chance of converting.</p>
<p>Let's take a look at another example. In order to get to node <kbd>22</kbd> from the root node, we need to have a <kbd>0</kbd> value for the <kbd>previous</kbd> variable, an <kbd>age</kbd> greater than or equal to <kbd>61</kbd>, a <kbd>marital</kbd> status other than <kbd>married</kbd> or <kbd>single</kbd>, and a <kbd>job</kbd> in one of these categories: <kbd>admin</kbd>, <kbd>blue-collar</kbd>, <kbd>entrepreneur</kbd>, <kbd>housemaid</kbd>, <kbd>retired</kbd>, or <kbd>unknown</kbd>. In other words, those customers that have not been contacted before this campaign and who are older than <kbd>61</kbd>, divorced, and have a job in one of the previously mentioned categories, belong to the node <kbd>22</kbd> and have a roughly <kbd>50%</kbd> chance of converting.</p>
<p>As you will have noticed from these two examples, you can draw useful insights on who is more or less likely to convert from trained decision tree models, by visualizing the trained tree. You simply need to follow through the nodes and understand what kinds of attributes are highly correlated with your target class. For this exercise, we restricted the tree to only growing up to a depth of <kbd>4</kbd>, but you can choose to grow a tree larger or smaller than the one that we used in this exercise.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The full code for this chapter's R exercise can be found in the repository at <a href="https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.4/R/FromEngagementToConversions.R">https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.4/R/FromEngagementToConversions.R</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we introduced a new machine learning algorithm, decision trees, which we can use for marketing analytics in order to better understand the data and draw insights on customer behaviors. We discussed how decision tree models are different from logistic regression models, which you learned about in the previous chapter. You saw that decision tree models learn the data by partitioning the data points based on certain criteria. We also discussed the two measures that are frequently used when growing decision trees: the Gini impurity and entropy information gain. Using either of these measures, decision trees can grow until all of the nodes are pure, or until the stopping criteria are met.</p>
<p>During our programming exercises in Python and R, we used the bank marketing dataset from the UCI Machine Learning Repository. We started our programming exercised by analyzing the data in depth, using the <kbd>pandas</kbd> and <kbd>matplotlib</kbd> packages in Python and the <kbd>dplyr</kbd> and <kbd>ggplot2</kbd> libraries in R. Then, you learned how we can train and grow decision trees, using the <kbd>sklearn</kbd> package in Python and the <kbd>rpart</kbd> library in R. With these trained decision tree models, you also learned how to visualize and interpret the results. For visualizations, we used the <kbd>graphviz</kbd> package in Python and the <kbd>rattle</kbd> library in R. Moreover, you saw how we can interpret the decision tree results and understand the customer groups that are more likely to convert or subscribe to a term deposit by traversing through the trained decision trees, which is useful when we want to conduct an explanatory analysis of customer behaviors.</p>
<p>In the following chapters, we are going to switch gears and focus on product analytics. In the next chapter, we will discuss the kinds of exploratory analysis that we can run to understand and identify patterns and trends in product data. With the product analytics results from the next chapter, we will show how we can build a product recommendation model.</p>


            

            
        
    </body></html>