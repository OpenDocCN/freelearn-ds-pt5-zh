<html><head></head><body><div><div><h1 id="_idParaDest-113"><em class="italics"><a id="_idTextAnchor121"/>Chapter 5</em></h1>
		</div>
		<div><h1 id="_idParaDest-114"><a id="_idTextAnchor122"/>Mastering Structured Data</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Work with structured data to create highly accurate models</li>
				<li class="bullets">Use the XGBoost library to train boosting models</li>
				<li class="bullets">Use the Keras library to train neural network models</li>
				<li class="bullets">Fine-tune model parameters to get the best accuracy</li>
				<li class="bullets">Use cross-validation</li>
				<li class="bullets">Save and load your trained models</li>
			</ul>
			<p><a id="_idTextAnchor123"/>This chapter will cover the basics on how to create highly accurate structured data models.</p>
		</div>
		<div><h2 id="_idParaDest-115"><a id="_idTextAnchor124"/>Introduction</h2>
			<p>There are two main types of data, structured and unstructured. Structured data refers to data that has a defined format and is usually shaped as a table, such as data stored in an Excel sheet or a relational database. Unstructured data does not have a predefined schema. Anything that cannot be stored in a table falls under this category. Examples include voice files, images, and PDFs.</p>
			<p><a id="_idTextAnchor125"/>In this chapter, we will focus on structured data and creating machine learning models using XGBoost and Keras. The XGBoost algorithm is widely used by industry experts and researchers due to the speed at which it delivers high-precision models, and also due to its distributed nature. The distributed nature refers to the ability to process data and train models in parallel; this enables faster training and much shorter turnaround time for data scientists. Keras on the other hand lets us create neural network models. Neural networks work much better than boosting algorithms in some cases, but finding the right network and the right configuration of the network is tough. The following topics will help you get familiar with both libraries, making sure that you can tackle any structured data in your data science journey.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor126"/>Boosting Algorithms</h2>
			<p>Boosting is a way to improve the accuracy of any learning algorithm. Boosting works by combining rough, high-level rules into a single prediction that is more accurate than any single rule. Iteratively, a subset of the training dataset is ingested into a "weak" algorithm to generate a weak model. These weak models are then combined to form the final prediction. Two of the most effective boosting algorithms are gradient boosting machine and XGBoost.</p>
			<h3 id="_idParaDest-117"><a id="_idTextAnchor127"/>Gradient Boosting Machine (GBM)</h3>
			<p>GBM makes use of classification trees as the weak algorithm. The results are generated by improving estimations from these weak models using a differentiable loss function. The model fits consecutive trees by considering the net loss of the previous trees; therefore, each tree is partially present in the final solution. Hence, boosting trees decreases the speed of the algorithm, and the transparency that they provide gives much better results. The GBM algorithm has a lot of parameters and it is sensitive to noise and extreme values. At the same time, GBM overfits the data, and thus a proper stopping point is required, but it is often the best possible model.</p>
			<h3 id="_idParaDest-118"><a id="_idTextAnchor128"/>XGBoost (Extreme Gradient Boosting)</h3>
			<p>XGBoost is the algorithm of choice for researchers across the world when modelling structured data. XGBoost also uses trees as the weak algorithm. So, why is it the first algorithm that comes to mind when data scientists see structured data? XGBoost is portable and distributed, which means that it can be easily used in different architectures and can use multiple cores (single machine) or multiple machines (clusters). As a bonus, the XGBoost library is written in C++, which makes it fast. It is also useful when working with a huge dataset, as it allows you to store data on an external disk and not load all the data on to the memory.</p>
			<h4>Note</h4>
			<p class="callout">You can read more about XGBoost here: <a href="">https://arxiv.org/abs/1603.02754</a></p>
			<h3 id="_idParaDest-119"><a id="_idTextAnchor129"/>Exercise 44: Using the XGBoost library to Perform Classification</h3>
			<p>In this exercise, we will perform classification on the wholesale customer dataset (<a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data</a>) using XGBoost library for Python. The dataset contains purchase data for clients of a wholesale distributor. It includes the annual spending on diverse range of product categories. We will predict the channel based on the annual spend on various products. The channel here describes whether the client is either a horeca (hotel/restaurant/caf√©) or a retail customer.</p>
			<ol>
				<li>Open the Jupyter Notebook from your virtual environment.</li>
				<li>Import XGBoost, Pandas, and sklearn for the function that we will use to calculate the accuracy. The accuracy is required to understand how our model is performing.<pre>import pandas as pd
import xgboost as xgb from sklearn.metrics import accuracy_score</pre></li>
				<li>Read the wholesale customer dataset using pandas and check to see if it was loaded successfully using the following command: <pre>data = pd.read_csv("data/wholesale-data.csv")</pre></li>
				<li>Check the first five entries of the dataset using the <code>head()</code> command. The output is shown in the following screenshot:<pre>data.head()</pre><div><img src="img/C13322_05_01.jpg" alt="Figure 5.1: Screenshot showing first five elements of dataset" width="603" height="178"/></div><h6>Figure 5.1: Screenshot showing first five elements of dataset</h6></li>
				<li>Now the "<code>data</code>" dataframe has all the data. It has the target variable, which is "<code>Channel</code>" in our case, and it has the predictor variables. So, we split the data into features (predictor) and labels (target).<pre>X = data.copy()X.drop("Channel", inplace = True, axis = 1)Y = data.Channel</pre></li>
				<li>Create training and test sets as discussed in previous chapters. Here, we use an 80:20 split as the number of data points in the dataset is less. You can experiment with different splits.<pre>X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values</pre></li>
				<li>Convert the pandas dataframe into a DMatrix, an internal data structure that is used by XGBoost to store training and testing datasets.<pre>train = xgb.DMatrix(X_train, label=Y_train)test = xgb.DMatrix(X_test, label=Y_test)</pre></li>
				<li>Specify the training parameters and train the model. <h4>Note</h4><pre>param = {'max_depth':6, 'eta':0.1, 'silent':1, 'objective':'multi:softmax', 'num_class': 3}num_round = 5 model = xgb.train(param, train, num_round)</pre><h4>Note</h4><p class="callout">By default, XGBoost uses all threads available to it for multiprocessing. To limit this, you can use the nthread parameter. Refer the next section for more information.</p></li>
				<li>Predict the "<code>Channel</code>" values of the test set using the model that we just created.<pre>preds = model.predict(test)</pre></li>
				<li>Get the accuracy of the model that we have trained for the test dataset.<pre>acc = accuracy_score(Y_test, preds)print("Accuracy: %.2f%%" % (acc * 100.0))</pre><p>The output screenshot is as follows:</p></li>
			</ol>
			<div><div><img src="img/C13322_05_02.jpg" alt="Figure 6.2: Final accuracy" width="493" height="20"/>
				</div>
			</div>
			<h6>Figure 5.2: Final accuracy</h6>
			<p>Congratulations! You just made your first XGBoost model with approximately 90% accuracy without much fine-tuning!</p>
			<h2 id="_idParaDest-120">XG<a id="_idTextAnchor130"/>Boost Library</h2>
			<p>The library we used to perform the above classification is named XGBoost. The library enables a lot of customization using the many parameters it has. In the following sections, we will dive in and understand the different parameters and functions of the XGBoost library.</p>
			<h4>Note</h4>
			<p class="callout">For more information about XGBoost, refer the website: <a href="">https://xgboost.readthedocs.io</a></p>
			<p><strong class="bold">Tr<a id="_idTextAnchor131"/>aining</strong></p>
			<p>Parameters that affect the training of any XGBoost model are listed below.</p>
			<ul>
				<li><code>booster</code>: Even though we mentioned in the introduction that the base learner of XGBoost is a regression tree, using this library, we can use linear regression as the weak learner as well. Another weak learner, DART booster, is a new method to tree boosting, which drops trees at random to prevent overfitting. To use tree boosting, pass "<code>gbtree</code>" (default); for linear regression, pass "<code>gblinear</code>"; and for tree boosting with dropout, pass "<code>dart</code>".<h4> Note</h4><p class="callout">You may learn more about DART from this paper: <a href="">http://www.jmlr.org/proceedings/papers/v38/korlakaivinayak15.pdf</a></p></li>
				<li><code>silent</code>: 0 prints the training logs, whereas 1 is the silent mode.</li>
				<li><code>nthread</code>: This signifies the number of parallel threads to be used. It defaults to the maximum number of threads available in the system.<h4>Note</h4><p class="callout">The parameter silent has been deprecated and has been replaced with verbosity, which takes any of the following values: 0 (silent), 1 (warning), 2 (info), 3 (debug). </p></li>
				<li><code>seed</code>: This is the seed value for the random number generator. Set a constant value here to get reproducible results. The default value is 0.</li>
				<li><code>objective</code>: This is a function that the model tries to minimize. The next few points cover the objective functions.<p><code>reg:linear</code>: Linear regression should be used with continuous target variables (regression problem). (Default) </p><p><code>binary:logistic</code>: logistic regression to be used in case of binary classification. It outputs probability and not classes.</p><p><code>binary:hinge</code>: This is binary classification that outputs predictions of 0 or 1, rather than the probabilities. Use this when you are not concerned about the probabilities.</p><p><code>multi:softmax</code>: If you want to do a multiclass classification, use this to perform the classification using the softmax objective. It is mandatory to set the <code>num_class</code> parameter to the number of classes for this.</p><p><code>multi:softprob</code>: This works the same as softmax, but the outputs predict the probability of each data point instead of predicting a class.</p></li>
				<li><code>eval_metric</code>: The performance of a model needs to be observed on the validation set (as discussed in <em class="italics">Chapter 1</em>, <em class="italics">Introduction to Data Science and Data Preprocessing</em>). This parameter takes the evaluation metric for the validation data. The default metric is chosen according to the objective function (<code>rmse</code> for regression and <code>logloss</code> for classification). You can use multiple evaluation metrics.<p><code>rmse</code>: Root mean square error (RMSE) penalizes large errors more. So, it is appropriate when being off by 1 is more than three times as bad as being off by 3.</p><p><code>mae</code>: Mean absolute error (MAE) can be used in cases where being off by 1 is similar to being off by 3.</p><p>The following graph shows the increase in the error with the increase in difference between the actual and predicted values. Here, it would be the following:</p></li>
			</ul>
			<div><div><img src="img/C13322_05_03.jpg" alt="Figure 5.3: Difference between actual and predicted value" width="532" height="16"/>
				</div>
			</div>
			<h6>Figure 5.3: Difference between actual and predicted value</h6>
			<div><div><img src="img/C13322_05_04.jpg" alt="Figure 5.4: Variation of penalty with variation in error; |X| is mae and X2  is rmse" width="777" height="402"/>
				</div>
			</div>
			<h6>Figure 5.4: Variation of penalty with variation in error; |X| is mae and X2  is rmse</h6>
			<p><code>logloss</code>: The negative log-likelihood, <code>logloss</code> of a model is equivalent to maximising the model's accuracy. It is defined mathematically as: </p>
			<div><div><img src="img/C13322_05_05.jpg" alt="Figure 5.5: Logloss equation diagram" width="580" height="65"/>
				</div>
			</div>
			<h6>Figure 5.5: Logloss equation diagram</h6>
			<p>Here, N is the number of data points, M is the number of classes and is either 1 or 0 depending on whether the prediction was correct or not,  is the probability of predicting label <em class="italics">j</em> for data point <em class="italics">i</em>. </p>
			<p><code>AUC</code>: <strong class="bold">Area under the curve</strong> is used widely for binary classification. You should always use this if your dataset has a <strong class="bold">class imbalance problem</strong>. A class imbalance problem occurs when your data is not split up into classes of similar sizes; for example, if class A makes up 90% of the data and class B makes up 10% of the data. We will talk more about the class imbalance problem in the Handling Imbalanced Datasets section.</p>
			<p><code>aucpr</code>: Area under the <strong class="keyword">precision-recall</strong> (<strong class="keyword">PR</strong>) curve is the same as the AUC curve, but should be preferred in case of a highly imbalanced dataset. We shall discuss this too in the Handling Imbalanced Datasets section.</p>
			<h4>Note</h4>
			<p class="callout">AUC or AUCPR should be used as a rule of thumb whenever you are working with a binary dataset.</p>
			<p><strong class="bold">Tree <a id="_idTextAnchor132"/>Booster</strong></p>
			<p>Parameters that are specific to tree-based models are listed below:</p>
			<ul>
				<li><code>eta</code>: This is the learning rate. Modify this value to prevent overfitting as discussed in <em class="italics">Chapter 1</em>, <em class="italics">Introduction to Data Science and Data Preprocessing</em>. The learning rate decides by how much the weights will get updated in each step. The gradient of weights gets multiplied by this and then added to the weight. This defaults to 0.3 and has a maximum value of 1 and a minimum value of 0.</li>
				<li><code>gamma</code>: This is the minimum loss reduction to make a partition. The larger gamma is, the more conservative the algorithm will be. Being more conservative prevents overfitting. The value depends on the dataset and other parameters used. It ranges from 0 to infinity and the default value is 0. A lower value leads to shallow trees and larger values give rise to deeper trees.<h4>Note</h4><p class="callout">Gamma values above 1 usually do not give good results.</p></li>
				<li><code>max_depth</code>: This is the maximum depth of any tree as discussed in <em class="italics">Chapter 3</em>, <em class="italics">Introduction to ML via Sklearn</em>. Increasing the max depth will make the model more likely to overfit. 0 means no limit. It defaults to 6.</li>
				<li><code>subsample</code>: Setting this to 0.5 will cause the algorithm to randomly sample half of the training data before growing the trees. This prevents overfitting. Subsampling occurs once every boosting iteration and defaults to 1, which makes the model take the complete dataset and not a sample.</li>
				<li><code>lambda</code>: This is the L2 regularization term. L2 regularization adds a squared magnitude of the coefficient as the penalty term to the loss function. Increasing this value prevents overfitting. Its default value is 1.</li>
				<li><code>alpha</code>: This is the L1 regularization term. L1 regularization adds an absolute magnitude of the coefficient as the penalty term to the loss function. Increasing this value prevents overfitting. Its default value is 0.</li>
				<li><code>scale_pos_weight</code>: This is useful when the classes are highly imbalanced. We will learn more about imbalanced data in the following sections. A typical value to consider introducing: the sum of negative instances / the sum of positive instances. Its default value is 1.</li>
				<li><code>predictor</code>: There are two predictors. <code>cpu_predictor</code> uses CPU for prediction. It is the default. <code>gpu_predictor</code> uses GPU for prediction.<h4>Note<a id="_idTextAnchor133"/></h4><p class="callout">Get a list of all the parameters here: https://xgboost.readthedocs.io/en/latest/parameter.html</p></li>
			</ul>
			<h3 id="_idParaDest-121">Contr<a id="_idTextAnchor134"/>olling Model Overfitting</h3>
			<p>If you observe high accuracy on the training dataset but a low accuracy on the test dataset, your model has overfit to the training data, as seen in <em class="italics">Chapter 1</em>, <em class="italics">Introduction to Data Science and Data Preprocessing</em>. There are two main ways to limit overfitting in XGBoost:</p>
			<ul>
				<li><code>max_depth</code>, <code>min_child_weight</code>, and <code>gamma</code> while monitoring the training and test metrics to get the best model without overfitting to the training dataset. You will learn more about this in the following sections.</li>
				<li><code>colsample_bytree</code> does the same thing as subsampling but with columns instead of rows.</li>
			</ul>
			<p>To understand better, see the training and accuracy graphs in the following figure:</p>
			<div><div><img src="img/C13322_05_06.jpg" alt="Figure 5.6: Training and accuracy graphs" width="1404" height="544"/>
				</div>
			</div>
			<h6>Figure 5.6: Training and accuracy graphs</h6>
			<p>To understand the conceptualization of a dataset with overfit and proper-fit models, refer to the following figure:</p>
			<div><div><img src="img/C13322_05_07.jpg" alt="Figure 5.7: Illustration of a dataset with overfit and proper-fit models" width="1800" height="612"/>
				</div>
			</div>
			<h6>Figure 5.7: Illustration of a dataset with overfit and proper-fit models</h6>
			<h4>Note</h4>
			<p class="callout">The black line represents the model that has a proper fit, whereas the model represented by the red line has overfit the dataset.</p>
			<h3 id="_idParaDest-122">Handlin<a id="_idTextAnchor135"/>g Imbalanced Datasets</h3>
			<p>Imbalanced datasets cause a lot of problems to data scientists. One example of an imbalanced dataset is credit card fraud data. Here, about 95% of transactions will be legitimate and only 5% will be fraudulent. In this case, a model that predicts every transaction to be a correct transaction will get 95% accuracy, but, it is a very bad model. To see the distribution of your classes, you can use the following function:</p>
			<pre>data['target_variable'].value_counts()</pre>
			<p>The output would be as follows:</p>
			<div><div><img src="img/C13322_05_08.jpg" alt="Figure 6.8: Class distribution" width="659" height="75"/>
				</div>
			</div>
			<h6>Figure 5.8: Class distribution</h6>
			<p>To handle imbalanced datasets, you can use the following methods:</p>
			<ul>
				<li><strong class="bold">Undersample the class that has a higher number of records</strong>: In the case of credit card fraud, you can randomly sample legitimate transactions to get records equal to the fraudulent records. This will result in equal distribution of the two classes, fraudulent and legitimate.</li>
				<li><strong class="bold">Oversample the class that has lesser records</strong>: In the case of credit card fraud, you can introduce more samples of the fraudulent transactions by adding either new data points or by copying the existing data points. This will result in equal distribution of the two classes, <strong class="bold">fraudulent</strong> and <strong class="bold">legitimate</strong>.</li>
				<li><strong class="bold">Balance the positive and negative weights with scale_pos_weight</strong>: You can use this parameter to allot a higher weight to the class with a smaller number of data points and thus artificially balance the classes. The value of the parameter can be:</li>
			</ul>
			<div><div><img src="img/C13322_05_09.jpg" alt="Figure 5.9: Value parameter equation" width="614" height="39"/>
				</div>
			</div>
			<h6>Figure 5.9: Value parameter equation</h6>
			<p>You can check the distribution of the classes using the following code: </p>
			<pre>positive = sum(Y == 1)
negative = sum(Y == 0)
scale_pos_weight = negative/positive</pre>
			<ul>
				<li><strong class="bold">Use AUC or AUCPR for evaluation</strong>: As mentioned earlier, the AUC and AUCPR metrics are sensitive to imbalanced dataset, unlike accuracy, which gives you a high value for a bad model that predicts the majority class most of the time. AUC can be plotted only for binary classification problems. It is a representation of the <strong class="bold">True Positive Rate vs. the False Positive Rate</strong> at different thresholds (0, 0.01, 0.02‚Ä¶ 1) of the predicted value. It is shown in the following figure:</li>
			</ul>
			<div><div><img src="img/C13322_05_10.jpg" alt="Figure 5.10: TPR and FPR equations" width="602" height="87"/>
				</div>
			</div>
			<h6>Figure 5.10: TPR and FPR equations</h6>
			<p>The metric is the area under the curve that we get after plotting <strong class="bold">TPR</strong> and <strong class="bold">FPR</strong>. When dealing with highly skewed datasets, AUCPR gives a better picture and is thus preferred. AUCPR is the representation of precision and recall at different thresholds</p>
			<div><div><img src="img/C13322_05_11.jpg" alt="Figure 6.11: Precision and recall equations" width="581" height="84"/>
				</div>
			</div>
			<h6>Figure 5.11: Precision and recall equations</h6>
			<p>As a rule of thumb, you should use AUC or AUCPR as the evaluation metric when dealing with imbalanced classes as it gives a clearer picture of the model.</p>
			<h4>Note </h4>
			<p class="callout">Machine learning algorithms cannot easily process strings or categorical variables represented as strings, so we have to convert them into numbers.</p>
			<h3 id="_idParaDest-123">Activity 14:<a id="_idTextAnchor136"/> Training and Predicting the Income of a Person</h3>
			<p>In this activity, we will attempt to predict whether or not the income of an individual exceeds $50,000. The adult income dataset (<a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data</a>) has its data sourced from the 1994 census dataset (<a href="">https://archive.ics.uci.edu/ml/datasets/adult</a>) and contains information such as income, education qualification of a person, and their occupation. Let's look at the following scenario: You work at a car company and you need to create a system by which the sales representatives of your firm can figure out what kind of car to sell to which person. </p>
			<p>To do this, you create a machine learning model that predicts the income of a prospective buyer and thus provides the salesperson with the right information to sell the right car.</p>
			<ol>
				<li value="1">Load the income dataset (<code>adult-data.csv</code>) using pandas. </li>
				<li>The data should look like this:<div><img src="img/C13322_05_12.jpg" alt="Figure 5.12: Screenshot showing five elements of census dataset" width="877" height="212"/></div><h6>Figure 5.12: Screenshot showing five elements of census dataset</h6><p>Use the following code to specify column names: </p><pre>data = pd.read_csv("../data/adult-data.csv", names=['age', 'workclass','education-num', 'occupation', 'capital-gain', 'capital-loss', 'hoursper-week', 'income'])</pre></li>
				<li>Convert all the categorical variables from strings to integers using sklearn.</li>
				<li>Perform prediction using the XGBoost library and perform parameter tuning to improve the accuracy to be more than 80%.</li>
			</ol>
			<p>We have successfully predicted the income using the dataset with around 83% accuracy.</p>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 360.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor137"/>External Memory Usage</h2>
			<p>When you have an exceptionally large dataset that you can't load on to your RAM, the external memory feature of the XGBoost library will come to your rescue. This feature will train XGBoost models for you without loading the entire dataset on the RAM.</p>
			<p>Using this feature requires minimal effort; you just need to add a cache prefix at the end of the filename.</p>
			<pre>train = xgb.DMatrix('data/wholesale-data.dat.train#train.cache')</pre>
			<p>This feature supports only <code>libsvm</code> file. So, we will now convert a dataset loaded in pandas into a <code>libsvm</code> file to be used with the external memory feature.</p>
			<h4>Note</h4>
			<p class="callout">You might have to do this in batches depending on how big your dataset is.</p>
			<pre>from sklearn.datasets import dump_svmlight_file
dump_svmlight_file(X_train, Y_train, 'data/wholesale-data.dat.train', zero_based=True, multilabel=False)</pre>
			<p>Here, <code>X_train</code> and <code>Y_train</code> are the predictor and target variables respectively. The <code>libsvm</code> file will get saved into the data folder.</p>
			<h2 id="_idParaDest-125">Cross-valida<a id="_idTextAnchor138"/>tion</h2>
			<p>Cross-validation is a technique that helps data scientists evaluate their models on unseen data. It is helpful when your dataset isn't large enough to create three splits (training, testing, and validation). Cross-validation helps the model avoid overfitting by presenting it with different partitions of the same data. It works by feeding different training and validation sets of the dataset for every pass of cross-validation. 10-fold cross-validation is the most used, where the dataset is divided into 10 completely different subsets and is trained on each one of them, and finally, the metrics are averaged out to obtain the accurate prediction performance of the model. In every round of cross-validation, we do the following: </p>
			<ol>
				<li value="1">Shuffle the dataset and split it into k different groups (k=10 for 10-fold cross-validation).</li>
				<li>Train the model on k-1 groups and test it on 1 group. </li>
				<li>Evaluate the model and store the results.</li>
				<li>Repeat steps 2 and 3 with different groups until all k combinations are trained.</li>
				<li>The final metric is the mean of the metrics generated in the different rounds.</li>
			</ol>
			<div><div><img src="img/C13322_05_13.jpg" alt="Figure 5.13: Illustration of a cross-validation dataset" width="1365" height="640"/>
				</div>
			</div>
			<h6>Figure 5.13: Illustration of a cross-validation dataset</h6>
			<p>The XGBoost <a id="_idTextAnchor139"/>library has an inbuilt function to perform cross-validation. This section will help you get you familiar using it.</p>
			<h3 id="_idParaDest-126"><a id="_idTextAnchor140"/>Exercise 45: Using Cross-validation to Find the Best Hyperparameters</h3>
			<p>In this exercise, we will find the best hyperparameters for the adult dataset from the previous activity using the XGBoost library for Python. To do this, we will make use of the cross-validation feature of the library.</p>
			<ol>
				<li value="1">Load the census dataset from <em class="italics">Activity 14</em> and perform all the preprocessing steps. <pre>import pandas as pd
import numpy as np
data = pd.read_csv("../data/adult-data.csv", names=['age', 'workclass', 'fnlwgt', 'education-num', 'occupation', 'capital-gain', 'capital-loss', 'hours-per-week', 'income'])</pre><p>Use Label Encoder from sklearn to encode strings. First, import Label Encoder, then encode all the string categorical columns one by one.</p><pre>from sklearn.preprocessing import LabelEncoder
data['workclass'] = LabelEncoder().fit_transform(data['workclass'])
data['occupation'] = LabelEncoder().fit_transform(data['occupation'])
data['income'] = LabelEncoder().fit_transform(data['income'])</pre></li>
				<li>Make train and test sets from the data and convert the data into Dmatrix.<pre>import xgboost as xgb
X = data.copy()
X.drop("income", inplace = True, axis = 1)
Y = data.income
X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values
Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values
train = xgb.DMatrix(X_train, label=Y_train)
test = xgb.DMatrix(X_test, label=Y_test)</pre></li>
				<li>Instead of using the train function, use the following code to perform 10-fold cross-validation and store the result in the <code>model_metrics</code> dataframe. The for loop iterates over different tree depth values to find the best one for our dataset.<pre>test_error = {}
for i in range(20):
¬†¬†¬†¬†param = {'max_depth':i, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'}
¬†¬†¬†¬†num_round = 50
¬†¬†¬†¬†model_metrics = xgb.cv(param, train, num_round, nfold = 10)
¬†¬†¬†¬†test_error[i] = model_metrics.iloc[-1]['test-error-mean']</pre></li>
				<li>Visualize the results using Matplotlib.<pre>import matplotlib.pyplot as plt
plt.scatter(test_error.keys(),test_error.values())
plt.xlabel('Max Depth')
plt.ylabel('Test Error')
plt.show()</pre><div><img src="img/C13322_05_14.jpg" alt="Figure 6.14: Graph of max depth with test error" width="531" height="266"/></div><h6>Figure 5.14: Graph of max depth with test error</h6><p>From the graph, we understand that the max depth of 9 works best for our dataset as it has the lowest test error.</p></li>
				<li>Find the best learning rate. Running this piece of code will take a while as it iterates over a lot of learning rates for 500 rounds each.<pre>for i in range(1,100,5):
¬†¬†¬†¬†param = {'max_depth':9, 'eta':0.001*i, 'silent':1, 'objective':'binary:hinge'}
¬†¬†¬†¬†num_round = 500
¬†¬†¬†¬†model_metrics = xgb.cv(param, train, num_round, nfold = 10)
¬†¬†¬†¬†test_error[i] = model_metrics.iloc[-1]['test-error-mean']</pre></li>
				<li>Visualize the results.<pre>lr = [0.001*(i) for i in test_error.keys()]
plt.scatter(temp,test_error.values())
plt.xlabel('Learning Rate')
plt.ylabel('Error')
plt.show()</pre><div><img src="img/C13322_05_15.jpg" alt="Figure 5.15: Graph of learning rate with test error" width="514" height="266"/></div><h6>Figure 5.15: Graph of learning rate with test error</h6><p>From the graph, we can see that a learning rate of about 0.01 works best for our model as it has the lowest error.</p></li>
				<li>Let us visualize the training and testing errors for each round for the learning rate 0.01.<pre>param = {'max_depth':9, 'eta':0.01, 'silent':1, 'objective':'binary:hinge'}
num_round = 500
model_metrics = xgb.cv(param, train, num_round, nfold = 10)
plt.scatter(range(500),model_metrics['test-error-mean'], s = 0.7, label = 'Test Error')
plt.scatter(range(500),model_metrics['train-error-mean'], s = 0.7, label = 'Train Error')
plt.legend()
plt.show()</pre><div><img src="img/C13322_05_16.jpg" alt="Figure 5.16: Graph of training and testing errors with respect to number of rounds&#13;&#10;" width="503" height="252"/></div><h6>Figure 5.16: Graph of training and testing errors with respect to number of rounds</h6><h4>Note</h4><pre>list(model_metrics['test-error-mean']).index(min(model_metrics['test-error-mean']))</pre></li>
				<li>To understand, check out the output. </li>
			</ol>
			<div><div><img src="img/C13322_05_17.jpg" alt="Figure 6.17: Least error " width="1624" height="58"/>
				</div>
			</div>
			<h6>Figure 5.17: Least error </h6>
			<h4>Note</h4>
			<p class="callout">The final model parameters that work best for this dataset:</p>
			<p class="callout">Max depth = 9</p>
			<p class="callout">Lea<a id="_idTextAnchor141"/>rning rate = 0.01</p>
			<p class="callout">Number of rounds = 496</p>
			<h2 id="_idParaDest-127">Saving and Loadin<a id="_idTextAnchor142"/>g a Model</h2>
			<p>The last piece in mastering structured data is the ability to save and load the models that you have trained and fine-tuned. Training a new model every time we need a prediction will waste a lot of time, so being able to save a trained model is imperative for data scientists. The saved model allows us to replicate the results and to create apps and services that make use of the machine learning model. The steps are as follows:</p>
			<ol>
				<li value="1">To save an XGBoost model, you need to call the <code>save_model</code> function.<pre>model.save_model('wholesale-model.model')</pre></li>
				<li>To load a previously saved model, you have to call load_model on an initialized XGBoost variable.<pre>loaded_model = xgb.Booster({'nthread': 2})
loaded_model.load_model('wholesale-model.model')</pre><h4>Note </h4><p class="callout">If you give XGBoost access to all the threads it can get, your computer might become slow while training or predicting.</p></li>
			</ol>
			<p>You are now ready to get started on modeling your structured dataset using the XGBoost library!</p>
			<h3 id="_idParaDest-128"><a id="_idTextAnchor143"/>Exercise 46: Creating a Python Pcript that Predicts Based on Real-time Input</h3>
			<p>In this exercise, we will first create a model and save it. We will then create a Python script that will make use of this saved model to perform predictions on the data input by the user.</p>
			<ol>
				<li value="1">Load the income dataset from Activity 14 as a pandas dataframe.<pre>import pandas as pd
import numpy as np
data = pd.read_csv("../data/adult-data.csv", names=['age', 'workclass', 'education-num', 'occupation', 'capital-gain', 'capital-loss', 'hours-per-week', 'income'])</pre></li>
				<li>Strip away all trailing spaces.<pre>data[['workclass', 'occupation', 'income']] = data[['workclass', 'occupation', 'income']].apply(lambda x: x.str.strip())</pre></li>
				<li>Convert all the categorical variables from strings to integers using scikit.<pre>from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
label_dict = defaultdict(LabelEncoder)
data[['workclass', 'occupation', 'income']] = data[['workclass', 'occupation', 'income']].apply(lambda x: label_dict[x.name].fit_transform(x))</pre></li>
				<li>Save the label encoder in a pickle file for future use. A pickle file stores Python objects so that we can access them later when we need them.<pre>import pickle
with open( 'income_labels.pkl', 'wb') as f:
¬†¬†¬†¬†¬†¬†¬†¬†pickle.dump(label_dict, f, pickle.HIGHEST_PROTOCOL)</pre></li>
				<li>Split the dataset into training and testing and create the model.</li>
				<li>Save the model to a file.<pre>model.save_model('income-model.model')</pre></li>
				<li>In a Python script, load the model and the label encoder.<pre>import xgboost as xgb
loaded_model = xgb.Booster({'nthread': 8})
loaded_model.load_model('income-model.model')
def load_obj(file):
¬†¬†¬†¬†¬†¬†with open(file + '.pkl', 'rb') as f:
¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†return pickle.load(f)
label_dict = load_obj('income_labels')</pre></li>
				<li>Read the input from the user.<pre>age = input("Please enter age: ")
workclass = input("Please enter workclass: ")
education_num = input("Please enter education_num: ")
occupation = input("Please enter occupation: ")
capital_gain = input("Please enter capital_gain: ")
capital_loss = input("Please enter capital_loss: ")
hours_per_week = input("Please enter hours_per_week: ")</pre></li>
				<li>Create a dataframe to store this data.<pre>data_list = [age, workclass, education_num, occupation, capital_gain, capital_loss, hours_per_week]
data = pd.DataFrame([data_list])
data.columns = ['age', 'workclass', 'education-num', 'occupation', 'capital-gain', 'capital-loss', 'hours-per-week']</pre></li>
				<li>Preprocess the data.<pre>data[['workclass', 'occupation']] = data[['workclass', 'occupation']].apply(lambda x: label_dict[x.name].transform(x))</pre></li>
				<li>Convert into Dmatrix and perform prediction using the model.<pre>data = data.astype(int)
data_xgb = xgb.DMatrix(data)
pred = loaded_model.predict(data_xgb)</pre></li>
				<li>Perform inverse transformation to get the results.<pre>income = label_dict['income'].inverse_transform([int(pred[0])])</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C13322_05_18.jpg" alt="Figure 5.18: Inverse transformation output" width="713" height="173"/>
				</div>
			</div>
			<h6>Figure 5.18: Inverse transformation output</h6>
			<h4>Note</h4>
			<p class="callout">Make sure that the values of <code>workclass</code> and <code>occupation</code> that you enter as input are present in the training data, otherwise the script will throw an error. This error occurs when the <code>LabelEncoder</code> encounters a new value it has not seen before. </p>
			<p>Congratulations! You built a script that predicts the outcome using user input data. You will now be able to deploy your models anywhere you want to.</p>
			<h3 id="_idParaDest-129"><a id="_idTextAnchor144"/>Activity 15: Predicting the Loss of Customers</h3>
			<p>In this activity, we will attempt to predict whether a customer will move to another telecom provider. The data is sourced from IBM sample datasets. Let's look at the following scenario: You work at a telecom company, and recently, a lot of your users have started moving to other providers. Now, to be able to give defecting customers a price cut, you need to predict which customer is the most likely to defect before they do so. To do this, you need to create a machine learning model that predicts which customer will defect.</p>
			<ol>
				<li value="1">Load the telecom churn (<code>telco-churn.csv</code>) dataset (<a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data</a>) using pandas. This dataset contains information about the customers of a telecom provider. The original source of the dataset is at: <a href="">https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/</a>. It contains multiple fields such as charges, tenure, and streaming information, along with a variable that tells us if the customer churned or not. The first few rows should look like this:<div><img src="img/C13322_05_19.jpg" alt="" width="1253" height="303"/></div><h6>Figure 5.19: Screenshot showing first five elements of telecom churn dataset</h6></li>
				<li>Remove unnecessary variables.</li>
				<li>Convert all the categorical variables from strings to integers using scikit. You can use the following code: <code>data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')</code></li>
				<li>Fix the data type mismatch when loading with pandas.</li>
				<li>Perform prediction using the XGBoost library and perform parameter tuning using cross-validation to improve accuracy to be more than 80%.</li>
				<li>Save your model for future use.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 361.</p></li>
			</ol>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor145"/>Neural Networks</h2>
			<p>A neural network is one of the most popular machine learning algorithms available to data scientists. It has consistently outperformed traditional machine learning algorithms in problems where images or digital media are required to find the solution. Given enough data, it outperforms traditional machine learning algorithms in structured data problems. Neural networks that have more than 2 layers are referred to as deep neural networks and the process of using these "deep" networks to solve problems is referred to as deep learning. Two handle unstructured data there are two main types of neural networks: a <strong class="keyword">convolutional neural network</strong> (<strong class="keyword">CNN</strong>) can be used to process images and a <strong class="keyword">recurrent neural network</strong> (<strong class="keyword">RNN</strong>) can be used to process time series and natural language data. We will talk more about CNNs and RNNs in <em class="italics">Chapter 6</em>, <em class="italics">Decoding Images</em> and <em class="italics">Chapter 7</em>, <em class="italics">Processing Human Language</em>. Let us now see how a vanilla neural network really works. In this section, we will go over the different parts of a neural network in brief. We will explain each topic in detail in the following chapters.</p>
			<h3 id="_idParaDest-131"><a id="_idTextAnchor146"/>What Is a Neural Network?</h3>
			<p>The basic unit of a neural network is a neuron. The inspiration for neural networks was taken from the biological brain, which is where the name neuron was inspired from. All connections in the neural network, like the synapses in the brain, can transmit information from one neuron to another. In a neural network, a weighted combination of the input signal is aggregated, and the output signal is then transmitted forward after passing it through a function. This function is a nonlinear activation function and is the neuron's activation threshold. Multiple layers of these interconnected neurons form a neural network. Only the non-output layers of a neural network include bias units. The weights associated with every neuron along with these biases determine the output of the entire network; hence, these are the parameters we modify to fit the data during training.</p>
			<div><div><img src="img/C13322_05_20.jpg" alt="Figure 5.20: Representation of single layer neural network" width="893" height="520"/>
				</div>
			</div>
			<h6>Figure 5.20: Representation of single layer neural network</h6>
			<p>The first layer of the neural network has nodes equal to the number of independent variables in the dataset. This layer is thus called the input layer, which is followed by multiple hidden layers, at the end of which is the output layer. Each neuron of the input layer takes in one independent variable of the dataset. The output layer outputs the final prediction. These outputs can be continuous (such as 0.2, 0.6, 0.8) if it is a regression problem or categorical (such as 2, 4, 5) if it is a classification problem. The training of a neural network modifies the weights and biases of the network to minimize the error, which is the difference between the expected and the output values. Weights are multiplied with the input to the neuron and then the bias value is added to the combination of these weights to get the output.</p>
			<div><div><img src="img/C13322_05_21.jpg" alt="Figure 5.21: Neuron output" width="520" height="33"/>
				</div>
			</div>
			<h6>Figure 5.21: Neuron output</h6>
			<p>Here, <em class="italics">y</em> is the output of the neuron and <em class="italics">x</em> the input, <em class="italics">w</em> and <em class="italics">b</em> are the weights and bias respectively, and <em class="italics">f</em> is the activation function, which we will learn more about later.</p>
			<h3 id="_idParaDest-132"><a id="_idTextAnchor147"/>Optimization Algorithms</h3>
			<p>To minimize the error of the model, we train the neural network to minimize a predefined loss function using an optimization algorithm. There are many choices for this optimization algorithm, and you can choose one depending on your data and model. For most of this book, we will work with <strong class="keyword">stochastic gradient descent</strong> (<strong class="keyword">SGD</strong>), which works well in most cases, but we will explain other optimizers as and when they are required. SGD works by iteratively finding out the gradient, which is the change in the weights with respect to the error. In mathematical terms, it is the partial derivative with respect to the inputs. It finds the gradient that would help it minimize a given function, which in our case is called the loss function. As we get closer to the solution, this gradient reduces in magnitude, thus preventing us from overshooting the optimal solution. </p>
			<p>The most intuitive way to understand SGD is the act of descending the bottom of a valley. Initially, we take steep descents, and then when we are close to the bottom, the slope reduces.</p>
			<div><div><img src="img/C13322_05_22.jpg" alt="Figure 5.22: Intuition of gradient descent (k represents magnitude of gradient)" width="1750" height="879"/>
				</div>
			</div>
			<h6>Figure 5.22: Intuition of gradient descent (k represents magnitude of gradient)</h6>
			<h3 id="_idParaDest-133"><a id="_idTextAnchor148"/>Hyperparameters</h3>
			<p>A big parameter that determines the time required to train a model is called the <strong class="keyword">learning rate</strong>, which essentially is the size of the step that we take to perform the descent. Too small a step, and it will take the model a long time to get to the optimal solution; too big, and it will overshoot the optimal solution. To circumvent this, we start with a large learning rate and reduce the learning rate after a few steps. This helps us reach the minimum point faster, and due to the reduction in step size, prevents the model from overshooting the solution.</p>
			<p>Next is the initialization of the weights. We need to perform initialization of the weights of a neural network to have a starting point from where we can then modify the weights to minimize the error. Initialization plays a major role in preventing the <strong class="bold">vanishing</strong> and <strong class="bold">exploding gradient</strong> problems.</p>
			<p><strong class="keyword">Vanishing gradient problem</strong> refers to the reducing gradients with every layer as the product of any number smaller than 1 is even smaller, so over multiple layers, this value becomes 0.</p>
			<p><strong class="keyword">Exploding gradient problem</strong> occurs when large error gradients add up and result in a very large update to the model. If the model loss goes to NaN, this could be a problem.</p>
			<p>Using the <strong class="bold">Xavier initialization</strong>, we can prevent these problems as it factors the size of the network while initializing the weights. The Xavier initialization initializes the weights, drawing them from a truncated normal distribution centered on 0 with standard deviation</p>
			<div><div><img src="img/C13322_05_23.jpg" alt="Figure 5.23: Xavier initialization" width="563" height="54"/>
				</div>
			</div>
			<h6>Figure 5.23: Standard deviation that the Xavier initialization uses.</h6>
			<p>Where xi is the number of input neurons and yi is the number of output neurons for that layer </p>
			<p>This ensures that the variance of both the inputs and the outputs remains the same even if the number of layers in the network is very large.</p>
			<p><strong class="bold">Loss Function</strong></p>
			<p>Another hyperparameter to consider is the loss function. Different loss functions are used depending on the type of the problem, classification or regression. For classification, we choose loss functions such as cross entropy and hinge. For regression, we use loss functions such as mean squared error, mean absolute error (MAE), and Huber. Different functions work well with different datasets. We will go over them as we use them.</p>
			<p><strong class="bold">Activation Function</strong></p>
			<p>While creating the neural network layers, you will have to define an activation function, which depends on whether the layer is a hidden layer or an output layer. In the case of a hidden layer, we will use the ReLU or the tanh activation functions. Activation functions help the neural network model non-linear functions. Almost no real-life situation can be solved using a linear model. Now, apart from this, different activation functions have different features. Sigmoid output has values between 0 and 1, whereas tanh centers the output around 0, which enables better learning. ReLU on the other hand prevents the vanishing gradient problem and is computationally efficient. This is the representation of a ReLU graph.</p>
			<div><div><img src="img/C13322_05_24.jpg" alt="" width="1800" height="960"/>
				</div>
			</div>
			<h6>Figure 5.24: Representation of ReLU activation function</h6>
			<p>Softmax outputs probabilities and is used when multiclass classification is being performed, whereas sigmoid outputs a value between 0 and 1 and is used only for binary classification. Linear activation is mostly used for models that solve the regression problem. A representation of the sigmoid activation function is shown in the following figure:</p>
			<div><div><img src="img/C13322_05_25.jpg" alt="Figure 5.25: Representation of sigmoid activation function" width="1800" height="927"/>
				</div>
			</div>
			<h6>Figure 5.25: Representation of sigmoid activation function</h6>
			<p>The previous section had a lot of new information; if you are confused, do not worry. We will apply all these concepts practically in the rest of the chapters, which will reinforce all these topics.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor149"/>Keras</h2>
			<p>Keras is an open-source, high-level neural network API written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit (CNTK), or Theano. Keras was developed to enable fast experimentation and thus help in rapid application development. Using Keras, one can get from idea to result with the least possible delay. Keras supports almost all the latest data science models relating to neural networks due to the huge community support. It contains multiple implementations of commonly used building blocks such as layers, batch normalization, dropout, objective functions, activation functions, and optimizers. Also, Keras allows users to create models for smartphones (Android and iOS), the web, or for the <strong class="keyword">Java Virtual Machine</strong> (<strong class="keyword">JVM</strong>). With Keras, you can train your models on your GPU without any change in code.</p>
			<p>Given all these features of Keras, it is imperative for data scientists to learn how to use all the different aspects of the library. Mastering the use of Keras will help you tremendously in your journey as a data scientist. To demonstrate the power of Keras, we will now install it and create a single layer neural network model.</p>
			<h4>Note</h4>
			<p class="callout">You can read more about Keras here: <a href="">https://keras.io/</a></p>
			<h3 id="_idParaDest-135"><a id="_idTextAnchor150"/>Exercise 47: Installing the Keras library for Python and Using it to Perform Classification</h3>
			<p>In this exercise, we will perform classification on the wholesale customer dataset (which we used in <em class="italics">Exercise 44</em>), using the Keras library for Python.</p>
			<ol>
				<li value="1">Run the following command in your virtual environment to install Keras.<pre>pip3 install keras</pre></li>
				<li>Open Jupyter Notebook from your virtual environment.</li>
				<li>Import Keras and other required libraries.<pre>import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
import numpy as np
from sklearn.metrics import accuracy_score</pre></li>
				<li>Read the wholesale customer dataset using pandas and check to see if it was loaded successfully using the following command:<pre>data = pd.read_csv("data/wholesale-data.csv")
data.head()</pre><p>The output should look like this:</p><div><img src="img/C13322_05_26.jpg" alt="Figure 5.26: Screenshot showing first five elements of dataset" width="600" height="177"/></div><h6>Figure 5.26: Screenshot showing first five elements of dataset</h6></li>
				<li>Split the data into features and labels.<pre>X = data.copy()X.drop("Channel", inplace = True, axis = 1)Y = data.Channel</pre></li>
				<li>Create training and test sets.<pre>X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values</pre></li>
				<li>Create the neural network model.<pre>model = Sequential()
model.add(Dense(units=8, activation='relu', input_dim=7))
model.add(Dense(units=16, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))</pre><p>Here, we create a four-layer network, with one input layer, two hidden layers, and one output layer. The hidden layers have ReLU activation and the output layer has softmax activation.</p></li>
				<li>Compile and train the model. We use the binary cross-entropy loss function, which is the same as the logloss we discussed before; we have chosen the optimizer to be stochastic gradient descent. We run the training for five epochs with a batch size of eight.<pre>model.compile(loss='binary_crossentropy',
¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†optimizer='sgd',
¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†metrics=['accuracy'])
model.fit(X_train, Y_train, epochs=5, batch_size=8)</pre><h4>Note</h4><p class="callout">You will see the model training log. Epoch refers to the training iteration, and 352 refers to the size of the dataset divided by the batch size. After the progress bar, you can see the time taken for one iteration. Next to that, you see the average time taken to train each batch. Next comes the loss of the model, which over here is the binary cross-entropy loss, followed by the accuracy after the iteration. A few of these terms are new, but we will understand each of them in the following sections.</p><div><img src="img/C13322_05_27.jpg" alt="Figure 5.27: Screenshot of model training logs" width="925" height="227"/></div><h6>Figure 5.27: Screenshot of model training logs</h6></li>
				<li>Predict the values of the test set.<pre>preds = model.predict(X_test, batch_size=128)</pre></li>
				<li>Obtain the accuracy of the model.<pre>accuracy = accuracy_score(Y_test, preds.astype(int))
print("Accuracy: %.2f%%" % (accuracy * 100.0))</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C13322_05_28.jpg" alt="Figure 5.28: Output accuracy" width="469" height="19"/>
				</div>
			</div>
			<h6>Figure 5.28: Output accuracy</h6>
			<p>Congratulations! You just made your first neural network model with around 81% accuracy, without any fine-tuning! You will notice that this accuracy is quite low when compared with XGBoost. In the following sections, you will figure out how to improve this accuracy. A major reason for the low accuracy is the size of the data. For a neural network model to really shine, it must have a large dataset to train on; otherwise, it overfits the data.</p>
			<h3 id="_idParaDest-136"><a id="_idTextAnchor151"/>Keras Library</h3>
			<p>Keras enables modularity. All initializers, cost functions, optimizers, layers, regularizers, and activation functions are standalone modules that can be used for any type of data and network architecture. You will find almost all the latest functions already implemented in Keras. This allows reusability of code and enables fast experimentation. You as a data scientist are not limited by the inbuilt modules; it is extremely easy to create your own custom modules and use them with other inbuilt modules. This enables research and helps with different use cases. For example, you might have to write a custom loss function to maximize the volume of cars sold, giving more weight to cars that have bigger margins, leading to higher profits.</p>
			<p>All the different kinds of layers that you would need to create a neural network are defined in Keras. We will investigate them as we use them. There are two main ways to create neural models in Keras, the sequential model and the functional API.</p>
			<p><strong class="bold">Sequential</strong>: The <strong class="keyword">sequential model</strong> is a linear stack of layers. This is the easiest way to create neural network models with Keras. A snippet of this model is given below:</p>
			<pre>model = Sequential()model.add(Dense(128, input_dim=784))model.add(Activation('relu'))
model.add(Dense(10))model.add(Activation('softmax'))</pre>
			<p><strong class="bold">Functional API</strong>: <strong class="keyword">Functional API</strong> is the way to go for complex models. Due to the linear nature of the sequential model, creating a complex model is not possible. The functional API lets you create multiple parts of the model and then merge them together. The same model in the functional API is given below:</p>
			<pre>inputs = Input(shape=(784,))
x = Dense(128, activation='relu')(inputs)prediction = Dense(10, activation='softmax')(x)model = Model(inputs=inputs, outputs=prediction)</pre>
			<p>A powerful feature of Keras is callback. Callbacks allow you to use a function at any stage of the training process. This proves to be useful to get the statistics and save the model at different stages. It can be used to apply a custom decay to the learning rate and also to perform early stopping.</p>
			<pre>filepath="model-weights-{epoch:02d}-{val_loss:.2f}.hdf5"
model_ckpt = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')
callbacks = [model_ckpt]</pre>
			<p>To save models you trained on Keras, you need to just use the following line of code:</p>
			<pre>model.save('Path to model')</pre>
			<p>To load the model from a file, use the following code:</p>
			<pre>keras.models.load_model('Path to model')</pre>
			<p>Early stopping is a useful feature that can be implemented using callbacks. Early stopping helps you save time when training models. It stops the training process if the change in the metric specified is less than the set threshold.</p>
			<pre>EarlyStopping(monitor='val_loss', min_delta=0.01, patience=5, verbose=1, mode='auto')</pre>
			<p>The callback mentioned above stops training if the change in the validation loss is less than 0.01 for five epochs.</p>
			<h4>Note </h4>
			<p class="callout">Always use <code>ModelCheckpoint</code> to store the model state. This is especially important for larger datasets and larger networks.</p>
			<h3 id="_idParaDest-137"><a id="_idTextAnchor152"/>Exercise 48: Predicting Avocado Price Using Neural Networks</h3>
			<p>Let us apply the knowledge that we received in this section to create an excellent neural network model that will predict the price of different kinds of avocados. The dataset (<a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data</a>) contains information such as average price of the produce, volume of the produce, region where the avocado was produced, and size of the bags that were used. It also has a few unknown variables that might help us with the model. </p>
			<h4>Note</h4>
			<p class="callout">Original source site: <a href="http://www.hassavocadoboard.com/retail/volume-and-price-data">www.hassavocadoboard.com/retail/volume-and-price-data</a></p>
			<ol>
				<li value="1">Import the avocado dataset and observe the columns. You will see something like this:<pre>import pandas as pd
import numpy as np
data = pd.read_csv('data/avocado.csv')
data.T</pre><div><img src="img/C13322_05_29.jpg" alt="Figure 5.29: Screenshot showing avocado dataset" width="887" height="492"/></div><h6>Figure 5.29: Screenshot showing avocado dataset</h6></li>
				<li>Look through the data and split the date column into days and months. This will help us catch the seasonality while ignoring the year. Now, drop the date and unnamed columns.<pre>data['Day'], data['Month'] = data.Date.str[:2], data.Date.str[3:5] 
data = data.drop(['Unnamed: 0', 'Date'], axis = 1)</pre></li>
				<li>Encode the categorical variables using the <code>LabelEncoder</code> so that Keras can use it to train the model.<pre>from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
label_dict = defaultdict(LabelEncoder)
data[['region', 'type', 'Day', 'Month', 'year']] = data[['region', 'type', 'Day', 'Month', 'year']].apply(lambda x: label_dict[x.name].fit_transform(x))</pre></li>
				<li>Split the data into training and testing sets.<pre>from sklearn.model_selection import train_test_split
X = data
y = X.pop('AveragePrice')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=9)</pre></li>
				<li>Use callbacks to save the model whenever the loss improves and for early stopping of the model if it starts performing poorly.<pre>from keras.callbacks import ModelCheckpoint, EarlyStopping
filepath="avocado-{epoch:02d}-{val_loss:.2f}.hdf5"
model_ckpt = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')
es = EarlyStopping(monitor='val_loss', min_delta=1, patience=5, verbose=1)
callbacks = [model_ckpt, es]</pre></li>
				<li>Create a neural network model. Here, we make use of the same model as before.<pre>from keras.models import Sequential
from keras.layers import Dense
model = Sequential()
model.add(Dense(units=16, activation='relu', input_dim=13))
model.add(Dense(units=8, activation='relu'))
model.add(Dense(units=1, activation='linear'))
model.compile(loss='mse', optimizer='adam')</pre></li>
				<li>Train and evaluate the model to get the MSE of the model.<pre>model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=40, batch_size=32)
model.evaluate(X_test, y_test) </pre></li>
				<li>Check the final output in the following screenshot:</li>
			</ol>
			<div><div><img src="img/C13322_05_30.jpg" alt="Figure 5.30: MSE of model" width="612" height="60"/>
				</div>
			</div>
			<h6>Figure 5.30: MSE of model</h6>
			<p>Congratulations! You have just trained your neural network to get a reasonable error on the avocado dataset. The value shown above is the mean square error of the model. Modify some hyperparameters and use the rest of the data to see if you can get a better error score. Make use of the information provided in the previous sections.</p>
			<h4>Note</h4>
			<p class="callout">A decrease in MSE is favorable. The most optimal value will depend on the situation. For example, while predicting the speed of a car, values less than 100 are ideal, whereas when predicting the GDP of a country, an MSE of 1000 is good enough.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor153"/>Categorical Variables</h2>
			<p>A categorical variable is one whose values can be represented in different categories. Examples are colours of a ball, breed of dogs, and zip codes. Mapping these categorical variables in a single dimension creates a sort of dependence on each other, which is incorrect. Even though these categorical variables do not have an order or dependence, inputting them to a neural network as a single feature makes the neural network create dependence between these variables depending on the order, whereas in reality, the order does not mean anything. In this section, we will learn about the ways in which can fix this issue and train effective models.</p>
			<h3 id="_idParaDest-139"><a id="_idTextAnchor154"/>One-hot Encoding</h3>
			<p>The easiest and the most widely used method of mapping categorical variables is to use one-hot encoding. Using this method, we convert a categorical feature into features equal to the number of categories in the feature.</p>
			<div><div><img src="img/C13322_05_31.jpg" alt="Figure 5.31: Categorical feature conversion" width="658" height="127"/>
				</div>
			</div>
			<h6>Figure 5.31: Categorical feature conversion</h6>
			<p>Use the following steps to convert categorical variables into one-hot encoded variables:</p>
			<ol>
				<li value="1">Convert the data into a number if represented as a data type other than int. There are two ways to do this</li>
				<li>You can directly use the <code>LabelEncoder</code> method of sklearn. </li>
				<li>Create bins to reduce the number of categories. The higher the number of categories, the more difficult it is for the model. You can choose an integer to represent each bin. Do keep in mind that doing this will lead to a loss in information and might result in a bad model. You can perform histogram binning using the following rule:<p>If the number of categorical columns is less than 25, use 5 bins.</p><p>If it is between 25 and 100, use n/5 bins, where n is the number of categorical columns, and if it is more than 100, use 10 * log (n) bins.</p><h4>Note</h4><p class="callout">You can combine the categories with frequencies smaller than 5% into one category.</p></li>
				<li>Convert the numerical array from Step 1 into a one-hot vector using the <code>get_dummies</code> function of pandas.</li>
			</ol>
			<div><div><img src="img/C13322_05_32.jpg" alt="Figure 5.32: Output of get_dummies function" width="1254" height="275"/>
				</div>
			</div>
			<h6>Figure 5.32: Output of get_dummies function</h6>
			<p>There are two main reasons why one-hot encoding isn't the best way to use categorical data:</p>
			<ul>
				<li>Different values of the categorical variables are assumed to be completely independent of each other. This leads to a loss of information about the relationship between them.</li>
				<li>Categorical variables with many categories result in a very computationally expensive model. Wider datasets require more data points to make meaningful models. This is known as the curse of dimensionality. </li>
			</ul>
			<p>To work through these issues, we will use entity embedding.</p>
			<h3 id="_idParaDest-140"><a id="_idTextAnchor155"/>Entity Embedding</h3>
			<p>Entity embedding represents categorical features in a multidimensional space. This ensures that the network learns the correct relationship between the different categories of a feature. The dimensions of this multidimensional space do not represent anything specific; it could be anything that the model deems fit to learn. For example, in the case of the days of a week, one dimension could be whether the day is a weekday or not and another could be the distance from a weekday. This method has been inspired from word embedding that is used in natural language processing to learn the semantic similarities between words and phrases. Creating an embedding can help teach the neural networks how Friday is different from Wednesday or how a puppy and a dog are different. For example, a four-dimensional embedding matrix for the days of the week will look something like this:</p>
			<div><div><img src="img/C13322_05_33.jpg" alt="Figure 5.33: Four-dimensional embedding matrix&#13;&#10;" width="594" height="80"/>
				</div>
			</div>
			<h6>Figure 5.33: Four-dimensional embedding matrix</h6>
			<p>From the above matrix, you can see that the embedding learns the dependence between the categories: it knows that Saturday and Sunday are more similar than Thursday and Friday, as the vectors for Saturday and Sunday are similar. Entity embedding gives a big edge when you have a lot of categorical variables in your dataset. To create entity embedding in Keras, you can use the embedding layer.</p>
			<h4>Note </h4>
			<p class="callout">Always try to use word embedding as it gives the best result.</p>
			<h3 id="_idParaDest-141"><a id="_idTextAnchor156"/>Exercise 49: Predicting Avocado Price Using Entity Embedding</h3>
			<p>Let us use the knowledge of entity embedding to predict the avocado price by creating a better neural network model. We will use the same avocado dataset from before.</p>
			<ol>
				<li value="1">Import the avocado price dataset and check for null values. Split the date column into month and day columns.<pre>import pandas as pd
import numpy as np
data = pd.read_csv('data/avocado.csv')
data['Day'], data['Month'] = data.Date.str[:2], data.Date.str[3:5]
data = data.drop(['Unnamed: 0', 'Date'], axis = 1)
data = data.dropna()</pre></li>
				<li>Encode the categorical variables.<pre>from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
label_dict = defaultdict(LabelEncoder)
data[['region', 'type', 'Day', 'Month', 'year']] = data[['region', 'type', 'Day', 'Month', 'year']].apply(lambda x: label_dict[x.name].fit_transform(x))</pre></li>
				<li>Split the data into training and testing sets.<pre>from sklearn.model_selection import train_test_split
X = data
y = X.pop('AveragePrice')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=9)</pre></li>
				<li>Create a dictionary that maps categorical column names to the unique values in them.<pre>cat_cols_dict = {col: list(data[col].unique()) for col in ['region', 'type', 'Day', 'Month', 'year'] }</pre></li>
				<li>Next, get the input data in the format that the embedding neural network will accept.<pre>train_input_list = []
test_input_list = []
for col in cat_cols_dict.keys():
¬†¬†¬†¬†raw_values = np.unique(data[col])
¬†¬†¬†¬†value_map = {}
¬†¬†¬†¬†for i in range(len(raw_values)):
¬†¬†¬†¬†¬†¬†¬†¬†value_map[raw_values[i]] = i       
¬†¬†¬†¬†train_input_list.append(X_train[col].map(value_map).values)
¬†¬†¬†¬†test_input_list.append(X_test[col].map(value_map).fillna(0).values)
other_cols = [col for col in data.columns if (not col in cat_cols_dict.keys())]
train_input_list.append(X_train[other_cols].values)
test_input_list.append(X_test[other_cols].values)</pre><p>Here, what we are doing is creating a list of arrays of all the variables.</p></li>
				<li>Next, create a dictionary that will store the output dimensions of the embedding layers. This is the number of values the variable will be denoted by. You must get to the right number with trial and error.<pre>cols_out_dict = {
¬†¬†¬†¬†'region': 12, 
¬†¬†¬†¬†'type': 1, 
¬†¬†¬†¬†'Day': 10, 
¬†¬†¬†¬†'Month': 3, 
¬†¬†¬†¬†'year': 1
}</pre></li>
				<li>Now, create the embedding layers for the categorical variables. In each iteration of the loop, we create one embedding layer for the categorical variable.<pre>from keras.models import Model
from keras.layers import Input, Dense, Concatenate, Reshape, Dropout
from keras.layers.embeddings import Embedding
inputs = []
embeddings = []
for col in cat_cols_dict.keys():
¬†¬†¬†¬†
¬†¬†¬†¬†inp = Input(shape=(1,), name = 'input_' + col)
¬†¬†¬†¬†embedding = Embedding(cat_cols_dict[col], cols_out_dict[col], input_length=1, name = 'embedding_' + col)(inp)
¬†¬†¬†¬†embedding = Reshape(target_shape=(cols_out_dict[col],))(embedding)
¬†¬†¬†¬†inputs.append(inp)
¬†¬†¬†¬†embeddings.append(embedding)</pre></li>
				<li>Now, add the continuous variable to the network and complete the model.<pre>input_numeric = Input(shape=(8,))
embedding_numeric = Dense(16)(input_numeric) 
inputs.append(input_numeric)
embeddings.append(embedding_numeric)
x = Concatenate()(embeddings)
x = Dense(16, activation='relu')(x)
x = Dense(4, activation='relu')(x)
output = Dense(1, activation='linear')(x)
model = Model(inputs, output)
model.compile(loss='mse', optimizer='adam')</pre></li>
				<li>Train the model with the train_input_list, which we created in Step 5 for 50 epochs.<pre>model.fit(train_input_list, y_train, validation_data = (test_input_list, y_test), epochs=50, batch_size=32)</pre></li>
				<li>Now, get the weights from the embedding layers to visualize the embedding.<pre>embedding_region = model.get_layer('embedding_region').get_weights()[0]</pre></li>
				<li>Perform PCA and plot the output using the region labels (which you can get by performing inverse transform on the dictionary that we created earlier). PCA displays similar data points closer, by reducing the dimensionality to two dimensions. Here, we plot only the first 25 regions. <p>You can plot all of them if you want.</p><pre>import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
Y = pca.fit_transform(embedding_region[:25])
plt.figure(figsize=(8,8))
plt.scatter(-Y[:, 0], -Y[:, 1])
for i, txt in enumerate((label_dict['region'].inverse_transform(cat_cols_dict['region']))[:25]):
¬†¬†¬†¬†plt.annotate(txt, (-Y[i, 0],-Y[i, 1]), xytext = (-20, 8), textcoords = 'offset points')
plt.show()</pre></li>
			</ol>
			<div><div><img src="img/C13322_05_34.jpg" alt="Figure 5.34: Graphical representation of avocado growing region using entity embedding" width="525" height="469"/>
				</div>
			</div>
			<h6>Figure 5.34: Graphical representation of avocado growing region using entity embedding</h6>
			<p>Congratulations! You improved the accuracy of your model by using entity embedding. As you can see from the embedding plot, the model was able to figure out the regions with high and low average prices. You can plot the embedding of other variables to see what relationships the network makes from the data. Also, try to improve the accuracy of this model through hyperparameter tuning.</p>
			<h3 id="_idParaDest-142"><a id="_idTextAnchor157"/>Activity 16: Predicting a Customer's Purchase Amount</h3>
			<p>In this activity, we will attempt to predict the amount a customer will spend on a product category. The dataset (<a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter05/data</a>) contains transactions made in a retail store. Let's look at the following scenario: You work at a big retail chain and want to predict which kind of customer will spend how much money on a particular product category. Doing this will help your frontline staff recommend the right kind of products to customers, thus increasing sales and customer satisfaction. To do this, you need to create a machine learning model that predicts the purchase value of a transaction.</p>
			<ol>
				<li value="1">Load the Black Friday dataset using pandas. This dataset is a collection of transactions made in a retail store. The information it contains is the age, city, marital status of the customer, product category of the item being bought, and bill amount. The first few rows should look like this:<div><img src="img/C13322_05_35.jpg" alt="Figure 5.35: Screenshot showing first five elements of Black Friday dataset " width="1169" height="261"/></div><h6>Figure 5.35: Screenshot showing first five elements of Black Friday dataset </h6><p>Remove unnecessary variables and null values. Remove <code>Product_Category_2</code> and <code>Product_Category_3</code> columns.</p></li>
				<li>Encode all the categorical variables.</li>
				<li>Perform prediction by creating a neural network with the Keras library. Make use of entity embedding and perform hyperparameter tuning.</li>
				<li>Save your model for future use.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 364.</p></li>
			</ol>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor158"/>Summary</h2>
			<p>In this chapter, we learnt how to create highly accurate structured data models and understood what XGBoost is and how to use the library to train models, Before we started, we were wondering what neural networks are and how to use the Keras library to train models. After learning about neural networks, we moved to handling categorical data. Finally, we learned what cross-validation is and how to use it.</p>
			<p>Now that you have completed this chapter, you can handle any kind of structured data and creating machine learning models with it. In the next chapter, you will learn how to create neural network models for image data.</p>
		</div>
	</div></body></html>