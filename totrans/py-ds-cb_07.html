<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch07" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 7. Machine Learning 2</h1></div></div></div><p class="calibre11">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Predicting real-valued numbers using regression</li><li class="listitem">Learning regression with L2 shrinkage – ridge</li><li class="listitem">Learning regression with L1 shrinkage – LASSO</li><li class="listitem">Using cross-validation iterators with L1 and L2 shrinkage</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch07lvl1sec72" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Introduction</h1></div></div></div><p class="calibre11">In this chapter, we will introduce regression techniques and how they can be coded in Python. We will follow it up with a discussion on some of the drawbacks that are inherent with regression methods, and discuss how to address the same using shrinkage methods. There are some parameters that need to be set in the shrinkage methods. We will discuss cross-validation techniques to find the optimal parameter values for the shrinkage methods.</p><p class="calibre11">We saw classification problems in the previous chapter. In this chapter, let's turn our attention towards regression problems. In classification, the response variable <code class="literal">Y</code> was either binary or a set of discrete values (in the case of multiclass and multilabel problems). In contrast, the response variable in regression is a real-valued number.</p><p class="calibre11">Regression can be thought of as a function approximation. The job of regression is to find a function such that when <code class="literal">X</code>, a set of random variables, is provided as an input to that function, it should return <code class="literal">Y</code>, the response variable. <code class="literal">X</code> is also referred to as an independent variable and <code class="literal">Y</code> is referred as a dependent variable.</p><p class="calibre11">We will leverage the techniques that we learnt in the previous chapter to divide our dataset into train, dev, and test sets, build our model iteratively on the train set, and validate it on dev. Finally, we will use our test set to get a good picture of the goodness of our model.</p><p class="calibre11">We will start the chapter with a recipe for simple linear regression using the least square estimation. At the beginning of the first recipe, we will provide a crisp introduction to the framework of regression, which is essential background information required to understand the other recipes in this chapter. Though very powerful, the simple regression framework suffers from a drawback. As there is no control over the upper and lower limits on the values that the coefficients of linear regression can take, they tend to overfit the given data. (The cost equation of linear regression is unconstrained. We will discuss more about it in the first recipe). The output regression model may not perform very well on unseen datasets. Shrinkage methods are used to address this problem. Shrinkage methods are also called regularization methods. In the next two recipes, we will cover two different shrinkage methods called LASSO and ridge. In our final recipe, we will introduce the concept of cross-validation and see how we can use it to our advantage in estimating the parameter, alpha, that is passed to ridge regression, a type of shrinkage method.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch07lvl1sec73" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Predicting real-valued numbers using regression</h1></div></div></div><p class="calibre11">Before we delve into this recipe, let's quickly understand how regression generally operates. This introduction is essential for understanding this and subsequent recipes.</p><p class="calibre11">Regression<a id="id528" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is a special<a id="id529" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> form of<a id="id530" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> function approximation. Here are the set of predictors:</p><div><img src="img/B04041_07_01.jpg" alt="Predicting real-valued numbers using regression" class="calibre135"/></div><p class="calibre11">With each instance, <code class="literal">xi</code> has <code class="literal">m</code> attributes:</p><div><img src="img/B04041_07_02.jpg" alt="Predicting real-valued numbers using regression" class="calibre136"/></div><p class="calibre11">The job of regression is to find a function such that when X is provided as an input to that function, it should return a Y response variable. Y is a vector of real-valued entries:</p><div><img src="img/B04041_07_03.jpg" alt="Predicting real-valued numbers using regression" class="calibre137"/></div><p class="calibre11">We will use the Boston housing dataset in order to explain the regression framework.</p><p class="calibre11">The following link provides a good introduction to the Boston housing dataset:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names">https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names</a>.</p><p class="calibre11">The response variable, <code class="literal">Y</code> in this case, is the median value of an owner-occupied home in the Boston area. There are 13 predictors. The preceding web link provides a good description of all the predictor <a id="id531" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>variables.The <a id="id532" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>regression problem is defined as finding a function, <code class="literal">F</code>, such that if we give a previously unseen predictor value to this function, it should be able to give us the median house price.</p><p class="calibre11">The function, <code class="literal">F(X)</code>, which is the output of our linear regression model, is a linear combination of the input, <code class="literal">X</code>, hence the name linear regression:</p><div><img src="img/B04041_07_04.jpg" alt="Predicting real-valued numbers using regression" class="calibre138"/></div><p class="calibre11">The <code class="literal">wi</code> variable is the unknown value in the preceding equation. The modeling exercise is about discovering the <code class="literal">wi</code> variable. Using our training data, we will find the value of <code class="literal">wi</code>; <code class="literal">wi</code> is called the coefficient of the regression model.</p><p class="calibre11">A linear regression modeling problem is framed as: using the training data to find the coefficients:</p><div><img src="img/B04041_07_05.jpg" alt="Predicting real-valued numbers using regression" class="calibre139"/></div><p class="calibre11">Such that:</p><div><img src="img/B04041_07_06.jpg" alt="Predicting real-valued numbers using regression" class="calibre140"/></div><p class="calibre11">The above formula is as low as possible.</p><p class="calibre11">The lower the value of this equation (called the <a id="id533" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cost function in optimization terminology), the better the linear regression model. So, the optimization problem is to minimize the preceding equation, that is, find the values for the <code class="literal">wi</code> coefficient so that it minimizes the equation. We will not delve into the details of the optimization routines that are used. However, it is good to know this objective function because the next two recipes expect you to understand it.</p><p class="calibre11">Now, the question is how do we know that the model that we built using the training data, that is, our newly found coefficients, <code class="literal">w1, w2,..wm</code> are good enough to accurately predict unseen records? Once again, we will leverage the cost function defined previously. When we apply the model in our dev set or test set, we find the average square of the difference between the actual and predicted values, as follows:</p><div><img src="img/B04041_07_07.jpg" alt="Predicting real-valued numbers using regression" class="calibre141"/></div><p class="calibre11">The preceding <a id="id534" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>equation is<a id="id535" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> called the mean squared error—the metric by which we can say that our regression model is worthy of use. We want an output model where the average square of the difference between the actual and predicted values is as low as possible. This method of finding the coefficients is called the least square estimation.</p><p class="calibre11">We will use scikit-learn's <code class="literal">LinearRegression</code> class. However, it internally uses the <code class="literal">scipy.linalg.lstsq</code> method. The method of least squares provides us with a closed form solution to the regression problem. Refer to the following links for more information on the method of least squares and the derivation<a id="id536" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> for the least squares:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://en.wikipedia.org/wiki/Least_squares">https://en.wikipedia.org/wiki/Least_squares</a>.</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)">https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)</a>.</p><p class="calibre11">We gave a very simple introduction to regression. Curious readers can refer to the following books <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.amazon.com/exec/obidos/ASIN/0387952845/trevorhastie-20">http://www.amazon.com/exec/obidos/ASIN/0387952845/trevorhastie-20</a>.
</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.amazon.com/Neural-Networks-Learning-Machines-Edition/dp/0131471392">http://www.amazon.com/Neural-Networks-Learning-Machines-Edition/dp/0131471392</a>
</p><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec257" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">The Boston data has 13 attributes and 506 instances. The target variable is a real number and the median value of the houses is in the thousands.</p><p class="calibre11">Refer to the following UCI link for more information about the Boston dataset: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names">https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names</a>.</p><p class="calibre11">We will provide the names of these predictor and response variables, as follows:</p><div><img src="img/B04041_07_08.jpg" alt="Getting ready" class="calibre142"/></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec258" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">We will start with loading <a id="id537" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>all the necessary<a id="id538" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> libraries. We will follow it up by defining our first function, <code class="literal">get_data()</code>. In this function, we will read the Boston dataset and return it as predictor <code class="literal">x</code> and response variable <code class="literal">y</code>:</p><div><pre class="programlisting"># Load libraries
from sklearn.datasets import load_boston
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
def get_data():
    """
    Return boston dataset
    as x - predictor and
    y - response variable
    """
    data = load_boston()
    x    = data['data']
    y    = data['target']
    return x,y    </pre></div><p class="calibre11">In our <code class="literal">build_model</code> function, we will construct our linear regression model with the given data. The<a id="id539" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> following <a id="id540" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>two functions, <code class="literal">view_model</code> and <code class="literal">model_worth</code>, are used to introspect the model that we have built:</p><div><pre class="programlisting">def build_model(x,y):
    """
    Build a linear regression model
    """
    model = LinearRegression(normalize=True,fit_intercept=True)
    model.fit(x,y)
    return model    

def view_model(model):
    """
    Look at model coeffiecients
    """
    print "\n Model coeffiecients"
    print "======================\n"
    for i,coef in enumerate(model.coef_):
        print "\tCoefficient %d  %0.3f"%(i+1,coef)
            
    print "\n\tIntercept %0.3f"%(model.intercept_)

def model_worth(true_y,predicted_y):
    """
    Evaluate the model
    """
    print "\tMean squared error = %0.2f"%(mean_squared_error(true_y,predicted_y))</pre></div><p class="calibre11">The <code class="literal">plot_residual</code> function is used to plot the errors in our regression model:</p><div><pre class="programlisting">def plot_residual(y,predicted_y):
    """
    Plot residuals
    """
    plt.cla()
    plt.xlabel("Predicted Y")
    plt.ylabel("Residual")
    plt.title("Residual Plot")
    plt.figure(1)
    diff = y - predicted_y
    plt.plot(predicted_y,diff,'go')
    plt.show()</pre></div><p class="calibre11">Finally, we will write <a id="id541" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>our <code class="literal">main</code> function, which<a id="id542" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is used to invoke all the preceding functions:</p><div><pre class="programlisting">if __name__ == "__main__":
    
    x,y = get_data()
    
    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
    
    # Build the model
    model = build_model(x_train,y_train)
    predicted_y = model.predict(x_train)
    
    # Plot the residual
    plot_residual(y_train,predicted_y)
    # View model coeffiecients    
    view_model(model)
    
    print "\n Model Performance in Training set\n"
    model_worth(y_train,predicted_y)  
    
    # Apply the model on dev set
    predicted_y = model.predict(x_dev)
    print "\n Model Performance in Dev set\n"
    model_worth(y_dev,predicted_y)  
    
    #Prepare some polynomial features
    poly_features = PolynomialFeatures(2)
    poly_features.fit(x_train)
    x_train_poly = poly_features.transform(x_train)
    x_dev_poly   = poly_features.transform(x_dev)
    
    # Build model with polynomial features
    model_poly = build_model(x_train_poly,y_train)
    predicted_y = model_poly.predict(x_train_poly)
    print "\n Model Performance in Training set (Polynomial features)\n"
    model_worth(y_train,predicted_y)  
    
    # Apply the model on dev set
    predicted_y = model_poly.predict(x_dev_poly)
    print "\n Model Performance in Dev set  (Polynomial features)\n"
    model_worth(y_dev,predicted_y)  
    
    # Apply the model on Test set
    x_test_poly = poly_features.transform(x_test)
    predicted_y = model_poly.predict(x_test_poly)

    print "\n Model Performance in Test set  (Polynomial features)\n"
    model_worth(y_test,predicted_y)  

    predicted_y = model.predict(x_test)
    print "\n Model Performance in Test set  (Regular features)\n"
    model_worth(y_test,predicted_y)  </pre></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec259" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let's start with the <a id="id543" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>main module <a id="id544" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and follow the code. We will load the predictor <code class="literal">x</code> and response variable <code class="literal">y</code> using the <code class="literal">get_data</code> function:</p><div><pre class="programlisting">def get_data():
    """
    Return boston dataset
    as x - predictor and
    y - response variable
    """
    data = load_boston()
    x    = data['data']
    y    = data['target']
    return x,y    </pre></div><p class="calibre11">The function invokes scikit-learn's convenient <code class="literal">load_boston()</code> function in order to retrieve the Boston house pricing dataset as NumPy arrays.</p><p class="calibre11">We will proceed to divide the data into the train and test sets using the <code class="literal">train_test_split</code> function from the Scikit library. We will reserve 30 percent of our dataset to test:</p><div><pre class="programlisting">x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)</pre></div><p class="calibre11">Out of which, we will extract the dev set in the next line:</p><div><pre class="programlisting">x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)</pre></div><p class="calibre11">In the next line, we will proceed to build our model using the training dataset by calling the <code class="literal">build_model</code> method. This model creates an object of a <code class="literal">LinearRegression</code> type. The <code class="literal">LinearRegression</code> class encloses SciPy's least squares method:</p><div><pre class="programlisting">    model = LinearRegression(normalize=True,fit_intercept=True)</pre></div><p class="calibre11">Let's look at the parameters passed when initializing this class.</p><p class="calibre11">The <code class="literal">fit_intercept</code> parameter is set to <code class="literal">True</code>. This tells the linear regression class to center the data. By centering the data, the mean value of each of our predictors is set to zero. The linear regression methods require the data to be centered by its mean value for a better interpretation of the intercepts. In addition to centering each attribute by its mean, we will also normalize each attribute by its standard deviation. We will achieve this using the <code class="literal">normalize</code> parameter and setting it to <code class="literal">True</code>. Refer to the <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <em class="calibre15">Scaling and data standardization</em> recipes on how to<a id="id545" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> perform <a id="id546" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>normalization by each column. With the <code class="literal">fit_intercept</code> parameter, we will instruct the algorithm to include an intercept in order to accommodate any constant shift in the response variable. Finally, we will fit the model by invoking the fit function with our response variable <code class="literal">y</code> and predictor <code class="literal">x</code>.</p><div><div><h3 class="title5"><a id="note24" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Refer to the book, The Elements of Statistical Learning by Trevor Hastie et al. for more information about linear regression methodologies.</p></div></div><p class="calibre11">It is good practice to inspect the model that we built so that we can have a better understanding of the model for further improvement or interpretability.</p><p class="calibre11">Let's now plot the residuals (the difference between the predicted <code class="literal">y</code> and actual <code class="literal">y</code>) and the predicted <code class="literal">y</code> values as a scatter plot. We will invoke the <code class="literal">plot_residual</code> method to do this:</p><div><pre class="programlisting">    # Plot the residual
    plot_residual(y_train,predicted_y)</pre></div><p class="calibre11">Let's look at the following graph:</p><div><img src="img/B04041_07_09.jpg" alt="How it works…" class="calibre143"/></div><p class="calibre11">We can validate the regression assumptions in our dataset using this scatter plot. We don't see any pattern and the points are scattered uniformly along zero residual values.</p><div><div><h3 class="title5"><a id="note25" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Refer to the book, <em class="calibre15">Data Mining Methods and Models</em> by <em class="calibre15">Daniel. T. Larose</em> for more information about using residual plots in order to validate linear regression assumptions.</p></div></div><p class="calibre11">We will then<a id="id547" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> inspect our <a id="id548" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>model using the <code class="literal">view_model</code> method. In this method, we will print our intercept and coefficient values. The linear regression object has two attributes, one called <code class="literal">coef_</code>, which provides us with an array of coefficients, and one called <code class="literal">intercept_</code>, which gives the intercept value:</p><div><img src="img/B04041_07_10.jpg" alt="How it works…" class="calibre144"/></div><p class="calibre11">Let's take <code class="literal">coefficient 6</code>, which is the number of livable rooms in the house. The coefficient value is interpreted as: for every additional room, the price moves up three times.</p><p class="calibre11">Finally, we will look at how good our model is by invoking the <code class="literal">model_worth</code> function with our predicted response values and actual response values, both from our training and dev sets.</p><p class="calibre11">This function prints out the mean squared error value, which is the average square of the difference between the actual and predicted values:</p><div><img src="img/B04041_07_11.jpg" alt="How it works…" class="calibre145"/></div><p class="calibre11">We have a lower value in our dev set, which is an indication of how good our model is. Let's check whether we can improve our mean squared error. What if we provide more features to our model? Let's create some features from our existing attributes. We will use the <code class="literal">PolynomialFeatures</code> class from scikit-learn to create second order polynomials:</p><div><pre class="programlisting">    #Prepare some polynomial features
    poly_features = PolynomialFeatures(2)
    poly_features.fit(x_train)
    x_train_poly = poly_features.transform(x_train)
    x_dev_poly   = poly_features.transform(x_dev)</pre></div><p class="calibre11">We will pass <code class="literal">2</code> as <a id="id549" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a parameter<a id="id550" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to <code class="literal">PolynomialFeatures</code> to indicate that we need second order polynomials. <code class="literal">2</code> is also the default value used if the class is initialized as empty:</p><div><img src="img/B04041_07_12.jpg" alt="How it works…" class="calibre146"/></div><p class="calibre11">A quick look at the shape of the new <code class="literal">x</code> reveals that we now have 105 attributes, compared with 13. Let's build the model using the new polynomial features and check out the model's accuracy:</p><div><pre class="programlisting">    # Build model with polynomial features
    model_poly = build_model(x_train_poly,y_train)
    predicted_y = model_poly.predict(x_train_poly)
    print "\n Model Performance in Training set (Polynomial           features)\n"
    model_worth(y_train,predicted_y)  

    # Apply the model on dev set
    predicted_y = model_poly.predict(x_dev_poly)
    print "\n Model Performance in Dev set  (Polynomial features)\n"
    model_worth(y_dev,predicted_y)  </pre></div><div><img src="img/B04041_07_13.jpg" alt="How it works…" class="calibre147"/></div><p class="calibre11">Our model has fitted well with the training dataset. Both in the dev and training sets, our polynomial features performed better than the original features.</p><p class="calibre11">Let's finally look at how the model with the polynomial features and the model with the regular features perform with our test set:</p><div><pre class="programlisting">    # Apply the model on Test set
    x_test_poly = poly_features.transform(x_test)
    predicted_y = model_poly.predict(x_test_poly)

    print "\n Model Performance in Test set  (Polynomial features)\n"
    model_worth(y_test,predicted_y)  
    
predicted_y = model.predict(x_test)
    print "\n Model Performance in Test set  (Regular features)\n"
    model_worth(y_test,predicted_y)  </pre></div><div><img src="img/B04041_07_14.jpg" alt="How it works…" class="calibre148"/></div><p class="calibre11">We can see that <a id="id551" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>our polynomial <a id="id552" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>features have fared better than our original set of features using the test dataset.</p><p class="calibre11">That is all you need to know about how to do linear regression in Python. We looked at how linear regression works and how we can build models to predict real-valued numbers.</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec260" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more...</h2></div></div></div><p class="calibre11">Before we move on, we will see one more parameter setting in the <code class="literal">PolynomialFeatures</code> class called <code class="literal">interaction_only</code>:</p><div><pre class="programlisting">poly_features = PolynomialFeatures(interaction_only=True)</pre></div><p class="calibre11">By setting <code class="literal">interaction_only</code> to <code class="literal">true—with x1</code> and <code class="literal">x2 attributes—only</code> the <code class="literal">x1*x2</code> attribute is created. The squares of <code class="literal">x1</code> and <code class="literal">x2</code> are not created, assuming that the degree is two.</p><p class="calibre11">Our test set results were not as good as our dev set results for both the normal and polynomial features. This is a known problem with linear regression. Linear regression is not well equipped to handle variance. The problem that we are facing is a high variance and low bias. As the model complexity increases, that is, the number of attributes presented to the model increases. The model tends to fit the training data very well—hence a low bias—but starts to give degrading outputs with the test data. There are several techniques available to handle this problem.</p><p class="calibre11">Let's look at a method called recursive feature selection. The number of required attributes is passed as a parameter to this method. It recursively filters the features. In the ith run, a linear model is fitted to the data <a id="id553" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and, based on the coefficients' values, the attributes are filtered; the attributes with lower <a id="id554" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>weights are left out. Thus, the iteration continues with the remaining set of attributes. Finally, when we <a id="id555" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have the required number of attributes, the iteration stops. Let's look at a code example:</p><div><pre class="programlisting"># Load libraries
from sklearn.datasets import load_boston
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from itertools import combinations
from sklearn.feature_selection import RFE

def get_data():
    """
    Return boston dataset
    as x - predictor and
    y - response variable
    """
    data = load_boston()
    x    = data['data']
    y    = data['target']
    return x,y    

def build_model(x,y,no_features):
    """
    Build a linear regression model
    """
    model = LinearRegression(normalize=True,fit_intercept=True)
    rfe_model = RFE(estimator=model,n_features_to_select=no_features)
    rfe_model.fit(x,y)
    return rfe_model    

def view_model(model):
    """
    Look at model coeffiecients
    """
    print "\n Model coeffiecients"
    print "======================\n"
    for i,coef in enumerate(model.coef_):
        print "\tCoefficient %d  %0.3f"%(i+1,coef)
            
    print "\n\tIntercept %0.3f"%(model.intercept_)

def model_worth(true_y,predicted_y):
    """
    Evaluate the model
    """
    print "\tMean squared error = %0.2f"%(mean_squared_error(true_y,predicted_y))
    return mean_squared_error(true_y,predicted_y)

def plot_residual(y,predicted_y):
    """
    Plot residuals
    """
    plt.cla()
    plt.xlabel("Predicted Y")
    plt.ylabel("Residual")
    plt.title("Residual Plot")
    plt.figure(1)
    diff = y - predicted_y
    plt.plot(predicted_y,diff,'go')
    plt.show()

    

def subset_selection(x,y):
    """
    subset selection method
    """
    # declare variables to track
    # the model and attributes which produces
    # lowest mean square error
    choosen_subset = None
    low_mse = 1e100
    choosen_model = None
    # k value ranges from 1 to the number of 
    # attributes in x
    for k in range(1,x.shape[1]+1):
        print "k= %d "%(k)
        # evaluate all attribute combinations
        # of size k+1
        subsets = combinations(range(0,x.shape[1]),k+1)
        for subset in subsets:
            x_subset = x[:,subset]
            model = build_model(x_subset,y)
            predicted_y = model.predict(x_subset)
            current_mse = mean_squared_error(y,predicted_y)
            if current_mse &lt; low_mse:
                low_mse = current_mse
                choosen_subset = subset
                choosen_model = model

    return choosen_model, choosen_subset,low_mse    

if __name__ == "__main__":
    
    x,y = get_data()
    
    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
    
    #Prepare some polynomial features
    poly_features = PolynomialFeatures(interaction_only=True)
    poly_features.fit(x_train)
    x_train_poly = poly_features.transform(x_train)
    x_dev_poly   = poly_features.transform(x_dev)
    
    #choosen_model,choosen_subset,low_mse = subset_selection(x_train_poly,y_train)    
    choosen_model = build_model(x_train_poly,y_train,20)
    #print choosen_subse
    predicted_y = choosen_model.predict(x_train_poly)
    print "\n Model Performance in Training set (Polynomial features)\n"
    mse  = model_worth(y_train,predicted_y)  
            
    # Apply the model on dev set
    predicted_y = choosen_model.predict(x_dev_poly)
    print "\n Model Performance in Dev set  (Polynomial features)\n"
    model_worth(y_dev,predicted_y)  
    
    # Apply the model on Test set
    x_test_poly = poly_features.transform(x_test)
    predicted_y = choosen_model.predict(x_test_poly)

    print "\n Model Performance in Test set  (Polynomial features)\n"
    model_worth(y_test,predicted_y)  </pre></div><p class="calibre11">This code is very similar to the preceding linear regression code, except for the <code class="literal">build_model</code> method:</p><div><pre class="programlisting">def build_model(x,y,no_features):
    """
    Build a linear regression model
    """
    model = LinearRegression(normalize=True,fit_intercept=True)
    rfe_model = RFE(estimator=model,n_features_to_select=no_features)
    rfe_model.fit(x,y)
    return rfe_model    </pre></div><p class="calibre11">In addition to the <a id="id556" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>predictor <code class="literal">x</code> and <a id="id557" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>response variable <code class="literal">y</code>, <code class="literal">build_model</code> also accepts the number of features to retain <code class="literal">no_features</code> as a parameter. In this case, we passed a value of 20, asking recursive feature elimination to retain only 20 significant features. As you can see, we first created a linear regression object. This object is passed to the <code class="literal">RFE</code> class. RFE stands for recursive feature elimination, a class provided by scikit-learn to implement recursive feature elimination. Let's now evaluate our model against the training, dev, and test datasets:</p><div><img src="img/B04041_07_15.jpg" alt="There's more..." class="calibre149"/></div><p class="calibre11">The mean squared error of the test dataset is 13.20, almost half of what we had earlier. Thus, we are able to use the recursive feature elimination method to perform feature selection effectively and hence improve our model.</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec261" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Scaling the data</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <em class="calibre15">Data Analysis – Explore and Wrangle</em></li><li class="listitem"><em class="calibre15">Standardizing the data</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <em class="calibre15">Data Analysis – Explore and Wrangle</em></li><li class="listitem"><em class="calibre15">Preparing data for model building</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6. Machine Learning 1">Chapter 6</a>, <em class="calibre15">Machine Learning I</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch07lvl1sec74" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Learning regression with L2 shrinkage – ridge</h1></div></div></div><p class="calibre11">Let's extend the regression technique discussed before to include regularization. While training a linear <a id="id558" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>regression model, some of the coefficients may take very high values, leading to instability in the model. Regularization or <a id="id559" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>shrinkage is a way of controlling the weights of the coefficients such that <a id="id560" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>they don't take very large values. Let's look at the linear regression cost function once again to understand what issues are inherently present with regression, and what we mean by controlling the weights of the coefficients:</p><div><img src="img/B04041_07_05.jpg" alt="Learning regression with L2 shrinkage – ridge" class="calibre139"/></div><div><img src="img/B04041_07_06.jpg" alt="Learning regression with L2 shrinkage – ridge" class="calibre140"/></div><p class="calibre11">Linear regression tries to find the coefficients, <code class="literal">w0…wm</code>, such that it minimizes the preceding equation. There are a few issues with linear regression.</p><p class="calibre11">If the dataset contains many correlated predictors, very small changes in the data can lead to an unstable model. Additionally, we will face a problem with interpreting the model results. For example, if we have two variables that are negatively correlated, these variables will have an opposite effect on the response variable. We can manually look at these correlations and remove one of the variables that is responsible and then proceed with the model building. However, it will be helpful if we can handle these scenarios automatically.</p><p class="calibre11">We introduced a method called recursive feature elimination in the previous recipe to keep the most informative attributes and discard the rest. However, in this approach, we either keep a variable or don't keep it; our decisions are binary. In this section, we will see a way by which we can control the weights associated with the variables in such a way that the unnecessary variables are penalized heavily and they receive extremely low weights.</p><p class="calibre11">We will change the cost function of linear regression to include the coefficients. As you know, the value of the cost function should be at a minimum for the best model. By including the coefficients in the cost function, we can heavily penalize the coefficients that take a very high value. In general, these techniques are known as shrinkage methods, as they try to shrink the value of the coefficients. In this recipe, we will see L2 shrinkage, most commonly called ridge regression. Let's look at the cost function for <a id="id561" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>ridge regression:</p><div><img src="img/B04041_07_16.jpg" alt="Learning regression with L2 shrinkage – ridge" class="calibre150"/></div><p class="calibre11">As you can see, the <a id="id562" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sum of the <a id="id563" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>square of the coefficients is added to the cost function. This way, when the optimization routine tries to minimize the preceding function, it has to heavily reduce the value of the coefficients to attain its objective. The alpha parameter decides the amount of shrinkage. Greater the alpha value, greater the shrinkage. The coefficient values are reduced to zero.</p><p class="calibre11">With this little math background, let's jump into our recipe to see ridge regression in action.</p><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec262" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Once again, we will use the Boston dataset to demonstrate ridge regression. The Boston data has 13 attributes and 506 instances. The target variable is a real number and the median value of the houses is in the thousands. Refer to the following UCI link for more information about the Boston dataset:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names">https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names</a>
</p><p class="calibre11">We intend to generate the polynomial features of degree two and consider only the interaction effects. At the end of this recipe, we will see how much the coefficients are penalized.</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec263" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">We will start by loading all the necessary libraries. We will follow it up by defining our first function, <code class="literal">get_data()</code>. In this function, we will read the Boston dataset and return it as predictor <code class="literal">x</code> and response variable <code class="literal">y</code>:</p><div><pre class="programlisting"># Load libraries
from sklearn.datasets import load_boston
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures


def get_data():
    """
    Return boston dataset
    as x - predictor and
    y - response variable
    """
    data = load_boston()
    x    = data['data']
    y    = data['target']
    x    = x - np.mean(x,axis=0)
    
    return x,y    </pre></div><p class="calibre11">In our next <code class="literal">build_model</code> function, we will construct our ridge regression model with the given data. The<a id="id564" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> following two<a id="id565" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> functions, <code class="literal">view_model</code> and <code class="literal">model_worth</code>, are used to introspect the model that we built:</p><div><pre class="programlisting">def build_model(x,y):
    """
    Build a Ridge regression model
    """
    model = Ridge(normalize=True,alpha=0.015)
    model.fit(x,y)
    # Track the scores- Mean squared residual for plot
    return model    

def view_model(model):
    """
    Look at model coeffiecients
    """
    print "\n Model coeffiecients"
    print "======================\n"
    for i,coef in enumerate(model.coef_):
        print "\tCoefficient %d  %0.3f"%(i+1,coef)
            
    print "\n\tIntercept %0.3f"%(model.intercept_)

def model_worth(true_y,predicted_y):
    """
    Evaluate the model
    """
    print "\tMean squared error = %0.2f"%(mean_squared_error(true_y,predicted_y))
    return mean_squared_error(true_y,predicted_y)</pre></div><p class="calibre11">Finally, we will write <a id="id566" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>our <code class="literal">main</code> function, which is<a id="id567" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> used to invoke all the preceding functions:</p><div><pre class="programlisting">if __name__ == "__main__":
    
    x,y = get_data()
    
    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
    
    #Prepare some polynomial features
    poly_features = PolynomialFeatures(interaction_only=True)
    poly_features.fit(x_train)
    x_train_poly = poly_features.transform(x_train)
    x_dev_poly   = poly_features.transform(x_dev)
    x_test_poly = poly_features.transform(x_test)
    
    #choosen_model,choosen_subset,low_mse = subset_selection(x_train_poly,y_train)    
    choosen_model = build_model(x_train_poly,y_train)

    predicted_y = choosen_model.predict(x_train_poly)
    print "\n Model Performance in Training set (Polynomial features)\n"
    mse  = model_worth(y_train,predicted_y)  
    view_model(choosen_model)
            
    # Apply the model on dev set
    predicted_y = choosen_model.predict(x_dev_poly)
    print "\n Model Performance in Dev set  (Polynomial features)\n"
    model_worth(y_dev,predicted_y)  
    
    # Apply the model on Test set
    predicted_y = choosen_model.predict(x_test_poly)

    print "\n Model Performance in Test set  (Polynomial features)\n"
    model_worth(y_test,predicted_y)  </pre></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec264" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let's start with the main module and follow the code. We loaded the predictor <code class="literal">x</code> and response variable <code class="literal">y</code> using the <code class="literal">get_data</code> function. This function invokes scikit-learn' s convenient <code class="literal">load_boston()</code> function to retrieve the Boston house pricing dataset as NumPy arrays.</p><p class="calibre11">We will proceed to divide the data into train and test sets using the <code class="literal">train_test_split</code> function from the scikit-learn library. We will reserve 30 percent of our dataset to test. Out of this, we will extract the dev set in the next line.</p><p class="calibre11">We will then <a id="id568" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>build the<a id="id569" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> polynomial features:</p><div><pre class="programlisting">poly_features = PolynomialFeatures(interaction_only=True)
poly_features.fit(x_train)</pre></div><p class="calibre11">As you can see, we set <code class="literal">interaction_only</code> to true. By setting <code class="literal">interaction_only</code> to true—with <code class="literal">x1</code> and <code class="literal">x2</code> attributes—only the <code class="literal">x1*x2</code> attribute is created. The squares of <code class="literal">x1</code> and <code class="literal">x2</code> are not created, assuming that the degree is 2. The default degree is two:</p><div><pre class="programlisting">x_train_poly = poly_features.transform(x_train)
x_dev_poly = poly_features.transform(x_dev)
x_test_poly = poly_features.transform(x_test)</pre></div><p class="calibre11">Using the <code class="literal">transform</code> function, we will transform our train, dev, and test datasets to include the polynomial features.</p><p class="calibre11">In the next line, we will build our ridge regression model using the training dataset by calling the <code class="literal">build_model</code> method:</p><div><pre class="programlisting">model = Ridge(normalize=True,alpha=0.015)
model.fit(x,y)</pre></div><p class="calibre11">The attributes in the dataset are centered by its mean and standardized by its standard deviation using the <code class="literal">normalize</code> parameter and setting it to <code class="literal">true</code>. <code class="literal">Alpha</code> controls the amount of shrinkage. Its value is set to <code class="literal">0.015</code>. We didn't arrive at this number magically, but by running the model several times. Later in this chapter, we will see how to empirically arrive at the right value for this parameter. We will also fit the intercept for this model using the <code class="literal">fit_intercept </code>parameter . However, by default, the <code class="literal">fit_intercept</code> parameter is set to <code class="literal">true</code> and hence we do not specify it explicitly.</p><p class="calibre11">Let's now see how the model has performed in the training set. We will call the <code class="literal">model_worth</code> method to get the mean square error. This method takes the predicted response variable and the actual response variable to return the mean square error:</p><div><pre class="programlisting">predicted_y = choosen_model.predict(x_train_poly)
print "\n Model Performance in Training set (Polynomial features)\n"
mse = model_worth(y_train,predicted_y) </pre></div><p class="calibre11">Our output looks as follows:</p><div><img src="img/B04041_07_17.jpg" alt="How it works…" class="calibre151"/></div><p class="calibre11">Before we apply our model to the test set, let's look at the coefficients' weights. We will call a function called <code class="literal">view_model</code> to view the coefficient's weight:</p><div><pre class="programlisting">view_model(choosen_model)</pre></div><div><img src="img/B04041_07_18.jpg" alt="How it works…" class="calibre152"/></div><p class="calibre11">We have not<a id="id570" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> shown all the <a id="id571" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>coefficients. There are a total of 92. However, looking at some of them, the shrinkage effect should be visible. For instance, Coefficient 1 is almost 0 (remember that it is a very small value and we have shown only the first three decimal places here).</p><p class="calibre11">Let's proceed to see how our model has performed in the dev set:</p><div><pre class="programlisting">predicted_y = choosen_model.predict(x_dev_poly)
print "\n Model Performance in Dev set (Polynomial features)\n"
model_worth(y_dev,predicted_y) </pre></div><div><img src="img/B04041_07_19.jpg" alt="How it works…" class="calibre153"/></div><p class="calibre11">Not bad, we have reached a mean square error lower than our training error. Finally, let's look at our model performance on the test set:</p><div><img src="img/B04041_07_20.jpg" alt="How it works…" class="calibre154"/></div><p class="calibre11">Compared with<a id="id572" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> our linear<a id="id573" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> regression model in the previous recipe, we performed better on our test set.</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec265" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">We mentioned earlier that linear regression models are very sensitive to even small changes in the dataset. Let's see a small example that will demonstrate this:</p><div><pre class="programlisting"># Load libraries
from sklearn.datasets import load_boston
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures


def get_data():
    """
    Return boston dataset
    as x - predictor and
    y - response variable
    """
    data = load_boston()
    x    = data['data']
    y    = data['target']
    x    = x - np.mean(x,axis=0)
    
    return x,y    </pre></div><p class="calibre11">In this code, we will fit both the linear regression and ridge regression models on the original data using the <code class="literal">build_model</code> function:</p><div><pre class="programlisting">lin_model,ridg_model = build_model(x,y)</pre></div><p class="calibre11">We will introduce a small noise in our original data, as follows:</p><div><pre class="programlisting"># Add some noise to the dataset
noise = np.random.normal(0,1,(x.shape))
x = x + noise</pre></div><p class="calibre11">Once again, we will fit the models on the noisy dataset. Finally, we will compare the coefficients' weights:</p><div><img src="img/B04041_07_21.jpg" alt="There's more…" class="calibre155"/></div><p class="calibre11">After adding a <a id="id574" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>small noise, when <a id="id575" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we try to fit a model using linear regression, the weights assigned are very different to the the weights assigned by the previous model. Now, let's see how ridge regression performs:</p><div><img src="img/B04041_07_22.jpg" alt="There's more…" class="calibre156"/></div><p class="calibre11">The weights have <a id="id576" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>not varied<a id="id577" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> starkly between the first and second model. Hopefully, this demonstrates the stability of ridge regression under noisy data conditions.</p><p class="calibre11">It is always tricky to choose the appropriate alpha value. A brute force approach is to run it through multiple values and trace the path of the coefficients. From the path, choose the alpha value where the weights don't vary dramatically. We will plot the coefficients' weights using the <code class="literal">coeff_path</code> function.</p><p class="calibre11">Let's look at the <code class="literal">coeff_path</code> function. It first generates a list of the alpha values:</p><div><pre class="programlisting">alpha_range = np.linspace(10,100.2,300)</pre></div><p class="calibre11">In this case, we generated 300 uniformly spaced numbers between 10 and 100. For each of these alpha values, we will build a model and save its coefficients:</p><div><pre class="programlisting">for alpha in alpha_range:
    model = Ridge(normalize=True,alpha=alpha)
    model.fit(x,y)
    coeffs.append(model.coef_)</pre></div><p class="calibre11">Finally, we will <a id="id578" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>plot these <a id="id579" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>coefficient weights against the alpha value:</p><div><img src="img/B04041_07_23.jpg" alt="There's more…" class="calibre157"/></div><p class="calibre11">As you can see, the values stabilize around the alpha value of 100. You can further zoom into a range close to 100 and look for an ideal value.</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec266" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Predicting real valued numbers using regression</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>, <em class="calibre15">Machine Learning II</em></li><li class="listitem"><em class="calibre15">Scaling the data</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <em class="calibre15">Data Analysis – Explore and Wrangle</em></li><li class="listitem"><em class="calibre15">Standardizing the data</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <em class="calibre15">Data Analysis – Explore and Wrangle</em></li><li class="listitem"><em class="calibre15">Preparing data for model building</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6. Machine Learning 1">Chapter 6</a>, <em class="calibre15">Machine Learning I</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch07lvl1sec75" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Learning regression with L1 shrinkage – LASSO</h1></div></div></div><p class="calibre11">
<strong class="calibre12">Least absolute shrinkage and selection operator </strong>(<strong class="calibre12">LASSO</strong>) is another shrinkage method popularly used <a id="id580" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with regression problems. LASSO leads to sparse solutions compared with ridge. A solution is called sparse if most of the coefficients are reduced to zero. In LASSO, a lot of the coefficients are <a id="id581" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>made zero. In the case of correlated variables, LASSO selects only one of them, whereas <a id="id582" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>ridge assigns equal weights to the coefficients of both the variables. This attribute of LASSO can hence be leveraged for variable selection. In this recipe, let's see how we can leverage LASSO for variable selection.</p><p class="calibre11">Let's look at the cost function of LASSO regression. If you followed through the previous two recipes, you can quickly identify the difference:</p><div><img src="img/B04041_07_24.jpg" alt="Learning regression with L1 shrinkage – LASSO" class="calibre158"/></div><p class="calibre11">The coefficients are penalized by the sum of the absolute value of the coefficients. Once again, the alpha controls the level of penalization. Let's try to understand the intuition behind why L1 shrinkage leads to a sparse solution.</p><p class="calibre11">We can rewrite the preceding equation as an unconstrained cost function and a constraint, as follows:</p><p class="calibre11">Minimize:</p><div><img src="img/B04041_07_25.jpg" alt="Learning regression with L1 shrinkage – LASSO" class="calibre159"/></div><p class="calibre11">Subject to the constraint:</p><div><img src="img/B04041_07_25b.jpg" alt="Learning regression with L1 shrinkage – LASSO" class="calibre160"/></div><p class="calibre11">With this equation in mind, let's plot the cost function values in the coefficient space for two coefficients, <code class="literal">w0</code> and <code class="literal">w1</code>:</p><div><img src="img/B04041_07_26.jpg" alt="Learning regression with L1 shrinkage – LASSO" class="calibre161"/></div><p class="calibre11">The blue lines<a id="id583" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> represent the <a id="id584" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>contours of the cost function (without constraint) values for the different values of <code class="literal">w0</code> and <code class="literal">w1</code>. The green region represents the constraint shape dictated by the eta value. The optimized value where both the regions meet is when <code class="literal">w0</code> is set to 0. We depicted a two-dimensional space where our solution is made sparse with <code class="literal">w0</code> set to 0. In a multidimensional space, we will have a rhomboid in the green region, and LASSO will give a sparse solution by reducing many of the coefficients to zero.</p><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec267" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Once again, we will use the Boston dataset to demonstrate LASSO regression. The Boston data has 13 attributes and 506 instances. The target variable is a real number and the median value of the houses is in the thousands.</p><p class="calibre11">Refer to the following UCI link for more information on the Boston dataset:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names">https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names</a>.</p><p class="calibre11">We will see how we can use LASSO for variable selection.</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec268" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">We will start by loading all the necessary libraries. We will follow it up by defining our first function, <code class="literal">get_data()</code>. In this function, we will read the Boston dataset and return it as a predictor <code class="literal">x</code> and response variable <code class="literal">y</code>:</p><div><pre class="programlisting"># Load libraries
from sklearn.datasets import load_boston
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import Lasso, LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

def get_data():
    """
    Return boston dataset
    as x - predictor and
    y - response variable
    """
    data = load_boston()
    x    = data['data']
    y    = data['target']
    return x,y    </pre></div><p class="calibre11">In our next <code class="literal">build_model</code> function, we will construct our LASSO regression model with the given data. The<a id="id585" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> following two functions, <code class="literal">view_model</code> and <code class="literal">model_worth</code>, are used to introspect the model that we<a id="id586" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> built:</p><div><pre class="programlisting">def build_models(x,y):
    """
    Build a Lasso regression model
    """
    # Alpha values uniformly
    # spaced between 0.01 and 0.02
    alpha_range = np.linspace(0,0.5,200)
    model = Lasso(normalize=True)
    coeffiecients = []
    # Fit a model for each alpha value
    for alpha in alpha_range:
        model.set_params(alpha=alpha)
        model.fit(x,y)
        # Track the coeffiecients for plot
        coeffiecients.append(model.coef_)
    # Plot coeffients weight decay vs alpha value
    # Plot model RMSE vs alpha value
    coeff_path(alpha_range,coeffiecients)
    # View coeffiecient value
    #view_model(model)

def view_model(model):
    """
    Look at model coeffiecients
    """
    print "\n Model coeffiecients"
    print "======================\n"
    for i,coef in enumerate(model.coef_):
        print "\tCoefficient %d  %0.3f"%(i+1,coef)
            
    print "\n\tIntercept %0.3f"%(model.intercept_)

def model_worth(true_y,predicted_y):
    """
    Evaluate the model
    """
    print "\t Mean squared error = %0.2f\n"%(mean_squared_error(true_y,predicted_y))</pre></div><p class="calibre11">We will define<a id="id587" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> two functions, <code class="literal">coeff_path</code> and <code class="literal">get_coeff</code>, to inspect our model coefficients. The <code class="literal">coeff_path</code> function is invoked from the <code class="literal">build_model</code> function to plot the weights of the coefficients for<a id="id588" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> different alpha values. The <code class="literal">get_coeff</code> function is invoked from the main function:</p><div><pre class="programlisting">def coeff_path(alpha_range,coeffiecients):
    """
    Plot residuals
    """
    plt.close('all')
    plt.cla()

    plt.figure(1)
    plt.xlabel("Alpha Values")
    plt.ylabel("Coeffiecient Weight")
    plt.title("Coeffiecient weights for different alpha values")
    plt.plot(alpha_range,coeffiecients)
    plt.axis('tight')
   
    plt.show()

def get_coeff(x,y,alpha):
    model = Lasso(normalize=True,alpha=alpha)
    model.fit(x,y)
    coefs = model.coef_
    indices = [i for i,coef in enumerate(coefs) if abs(coef) &gt; 0.0]
    return indices</pre></div><p class="calibre11">Finally, we will write our <code class="literal">main</code> function, which is used to invoke all the preceding functions:</p><div><pre class="programlisting">if __name__ == "__main__":
    
    x,y = get_data()
    # Build multiple models for different alpha values
    # and plot them    
    build_models(x,y)
    print "\nPredicting using all the variables"
    full_model = LinearRegression(normalize=True)
    full_model.fit(x,y)
    predicted_y = full_model.predict(x)
    model_worth(y,predicted_y)    
           
    print "\nModels at different alpha values\n"
    alpa_values = [0.22,0.08,0.01]
    for alpha in alpa_values:
        
        indices = get_coeff(x,y,alpha)   
        print "\t alpah =%0.2f Number of variables selected = %d "%(alpha,len(indices))
        print "\t attributes include ", indices
        x_new = x[:,indices]
        model = LinearRegression(normalize=True)
        model.fit(x_new,y)
        predicted_y = model.predict(x_new)
        model_worth(y,predicted_y)</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec269" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/> How it works…</h2></div></div></div><p class="calibre11">Let's start with the <a id="id589" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>main module <a id="id590" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and follow the code. We will load the predictor <code class="literal">x</code> and response variable <code class="literal">y</code> using the <code class="literal">get_data</code> function. The function invokes scikit-learn's convenient <code class="literal">load_boston()</code> function to retrieve the Boston house pricing dataset as NumPy arrays.</p><p class="calibre11">We will proceed by calling <code class="literal">build_models</code>. In <code class="literal">build_models</code>, we will construct multiple models for the different values of <code class="literal">alpha</code>:</p><div><pre class="programlisting">alpha_range = np.linspace(0,0.5,200)
model = Lasso(normalize=True)
coeffiecients = []
# Fit a model for each alpha value
for alpha in alpha_range:
model.set_params(alpha=alpha)
model.fit(x,y)
# Track the coeffiecients for plot
coeffiecients.append(model.coef_)</pre></div><p class="calibre11">As you can see, in the for loop, we also store the coefficient values for different values of alpha in a list.</p><p class="calibre11">Let's plot the coefficient values for different alpha values by calling the <code class="literal">coeff_path</code> function:</p><div><pre class="programlisting">plt.close('all')
plt.cla()

plt.figure(1)
plt.xlabel("Alpha Values")
plt.ylabel("Coeffiecient Weight")
plt.title("Coeffiecient weights for different alpha values")
plt.plot(alpha_range,coeffiecients)
plt.axis('tight')
plt.show()</pre></div><p class="calibre11">In the <code class="literal">x</code> axis, you can<a id="id591" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> see that we <a id="id592" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have the alpha values, and in the <code class="literal">y</code> axis, we will plot all the coefficients for a given alpha value. Let's see the output plot:</p><div><img src="img/B04041_07_27.jpg" alt="How it works…" class="calibre162"/></div><p class="calibre11">The different colored lines represent different coefficient values. As you can see, as the value of alpha increases, the coefficient weights merge towards zero. From this plot, we can select the value of alpha.</p><p class="calibre11">For our reference, let's fit a simple linear regression model:</p><div><pre class="programlisting">print "\nPredicting using all the variables"
full_model = LinearRegression(normalize=True)
full_model.fit(x,y)
predicted_y = full_model.predict(x)
model_worth(y,predicted_y) </pre></div><p class="calibre11">Let's look at the<a id="id593" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> mean square<a id="id594" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> error when we try to predict using our newly built model:</p><div><img src="img/B04041_07_28.jpg" alt="How it works…" class="calibre163"/></div><p class="calibre11">Let's proceed to select the coefficients based on LASSO:</p><div><pre class="programlisting">print "\nModels at different alpha values\n"
alpa_values = [0.22,0.08,0.01]
for alpha in alpa_values:
indices = get_coeff(x,y,alpha) </pre></div><p class="calibre11">Based on our preceding graph, we selected <code class="literal">0.22</code>, <code class="literal">0.08</code>, and <code class="literal">0.01</code> as the alpha values. In the loop, we will call the <code class="literal">get_coeff</code> method. This method fits a LASSO model with the given alpha values and returns only the non-zero coefficients' indices:</p><div><pre class="programlisting">model = Lasso(normalize=True,alpha=alpha)
model.fit(x,y)
coefs = model.coef_

indices = [i for i,coef in enumerate(coefs) if abs(coef) &gt; 0.0]</pre></div><p class="calibre11">Essentially, we are selecting only those attributes that have a non-zero coefficient value—feature selection. Let's get back to our <code class="literal">for</code> loop where we will fit a linear regression model with the reduced coefficients:</p><div><pre class="programlisting">print "\t alpah =%0.2f Number of variables selected = %d "%(alpha,len(indices))
print "\t attributes include ", indices
x_new = x[:,indices]
model = LinearRegression(normalize=True)
model.fit(x_new,y)
predicted_y = model.predict(x_new)
model_worth(y,predicted_y)</pre></div><p class="calibre11">What we want to know is how good our models would be if we predicted them with the reduced set of attributes, compared with the model that we built initially using the whole dataset:</p><div><img src="img/B04041_07_29.jpg" alt="How it works…" class="calibre164"/></div><p class="calibre11">Look at the first pass <a id="id595" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>where our <a id="id596" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>alpha value is <code class="literal">0.22</code>. There are only two coefficients with non-zero values, <code class="literal">5</code> and <code class="literal">12</code>. The mean squared error is <code class="literal">30.51</code>, which is only <code class="literal">9</code> more than the model fitted with all the variables.</p><p class="calibre11">Similarly, for the alpha value of <code class="literal">0.08</code>, there are three non-zero coefficients. We can see some improvement in the mean squared error. Finally, with <code class="literal">0.01</code> alpha value, 9 out of 13 attributes are selected and the mean square error is very close to the model built with all the attributes.</p><p class="calibre11">As you can see, we didn't fit the model with all the attributes. We are able to choose a subset of the attributes automatically using LASSO. Thus, we have seen how LASSO can be used for variable selection.</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec270" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">By keeping only the most important variables, LASSO avoids overfitting. However, as you can see, the mean squared error values are not that good. We can see that there is a loss in the predictive power because of LASSO.</p><p class="calibre11">As said before, in the case of the correlated variables, LASSO selects only one of them, whereas ridge assigns equal weights to the coefficients of both the variables. Hence, ridge has a higher predictive power compared with LASSO. However, LASSO can do variable selection, which Ridge is not capable of performing.</p><div><div><h3 class="title5"><a id="note26" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Refer to the book, <em class="calibre15">Statistical learning with sparsity: The Lasso and generalization</em> by <em class="calibre15">Trevor Hastie et al.</em> for more information about the LASSO and ridge methodologies.</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec271" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Scaling the data</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <em class="calibre15">Data Analysis – Explore and Wrangle</em></li><li class="listitem"><em class="calibre15">Standardizing the data</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <em class="calibre15">Data Analysis – Explore and Wrangle</em></li><li class="listitem"><em class="calibre15">Preparing data for model building</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6. Machine Learning 1">Chapter 6</a>, <em class="calibre15">Machine Learning I</em></li><li class="listitem"><em class="calibre15">Regression with L2 Shrinkage – Ridge</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>, <em class="calibre15">Machine Learning II</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch07lvl1sec76" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using cross-validation iterators with L1 and L2 shrinkage</h1></div></div></div><p class="calibre11">In the previous chapter, we saw methods to divide the data into train and test sets. In the subsequent recipes, we again performed a split on the test dataset to arrive at a dev dataset. The idea was to keep the test set away from the model building cycle. However, as we need to improve our<a id="id597" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> model continuously, we used the dev set to test the model accuracy in each iteration. Though it's a good<a id="id598" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> approach, this <a id="id599" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>method is difficult to implement if we don't have a large dataset. We want to provide as much data as possible to train our model but still need to hold some of the data for the evaluation and final testing. In many real-world scenarios, it is very rare to get a very large dataset.</p><p class="calibre11">In this recipe, we will see a method called cross-validation to help us address this issue. This approach is typically called <a id="id600" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>k-fold cross-validation. The training set is divided into k-folds. The model is trained on K-1 (K minus 1) folds and the left out fold is used to test. This way, we don't need a separate dev dataset.</p><p class="calibre11">Let's see some of the iterators provided by the scikit-learn library to perform the k-fold cross-validation effectively. Equipped with the knowledge of cross-validation, we will further see how we can leverage cross-validation for the selection of the alpha values in shrinkage methods.</p><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec272" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">We will use the Iris dataset to demonstrate the various cross-validation iterators concepts. We will return to our Boston housing dataset to demonstrate how cross-validation can be used successfully to find the ideal alpha values in shrinkage methods.</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec273" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's look at how to use the cross validation iterator:</p><div><pre class="programlisting">from sklearn.datasets import load_iris
from sklearn.cross_validation import KFold,StratifiedKFold

def get_data():
    data = load_iris()
    x = data['data']
    y = data['target']
    return x,y

def class_distribution(y):
        class_dist = {}
        total = 0
        for entry in y:
            try:
                class_dist[entry]+=1
            except KeyError:
                class_dist[entry]=1
            total+=1
        
        for k,v in class_dist.items():
            print "\tclass %d percentage =%0.2f"%(k,v/(1.0*total))
    
if __name__ == "__main__":
    x,y = get_data()
    # K Fold
    # 3 folds
    kfolds = KFold(n=y.shape[0],n_folds=3)
    fold_count  =1
    print
    for train,test in kfolds:
        print "Fold %d x train shape"%(fold_count),x[train].shape,\
        " x test shape",x[test].shape
        fold_count==1
    print
    #Stratified KFold
    skfolds = StratifiedKFold(y,n_folds=3)
    fold_count  =1
    for train,test in skfolds:
        print "\nFold %d x train shape"%(fold_count),x[train].shape,\
        " x test shape",x[test].shape
        y_train = y[train]
        y_test  = y[test]
        print "Train Class Distribution"
        class_distribution(y_train)
        print "Test Class Distribution"
        class_distribution(y_test)

        fold_count+=1

    print</pre></div><p class="calibre11">In our main function, we will call the <code class="literal">get_data</code> function to load the Iris dataset. We will then proceed to demonstrate a simple k-fold and stratified k-folds.</p><p class="calibre11">With the knowledge<a id="id601" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of k-fold cross-validation, let's write a recipe to leverage this newly found knowledge in <a id="id602" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>enhancing ridge <a id="id603" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>regression:</p><div><pre class="programlisting"># Load libraries
from sklearn.datasets import load_boston
from sklearn.cross_validation import KFold,train_test_split
from sklearn.linear_model import Ridge
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
import numpy as np


def get_data():
    """
    Return boston dataset
    as x - predictor and
    y - response variable
    """
    data = load_boston()
    x    = data['data']
    y    = data['target']
    return x,y    </pre></div><p class="calibre11">We will start by loading all the necessary libraries. We will follow it up by defining our first function, <code class="literal">get_data()</code>. In this function, we will read the Boston dataset and return it as a predictor <code class="literal">x</code> and response variable <code class="literal">y</code>.</p><p class="calibre11">In our next <code class="literal">build_model</code> function, we will construct our ridge regression model with the given data. We will leverage the k-fold cross-validation.</p><p class="calibre11">The following two functions, <code class="literal">view_model</code> and <code class="literal">model_worth</code>, are used to introspect the model that we built.</p><p class="calibre11">Finally, we will write the <code class="literal">display_param_results</code> function to view the model errors in each fold:</p><div><pre class="programlisting">def build_model(x,y):
    """
    Build a Ridge regression model
    """
    kfold = KFold(y.shape[0],5)
    model = Ridge(normalize=True)

    alpha_range = np.linspace(0.0015,0.0017,30)
    grid_param = {"alpha":alpha_range}
    grid = GridSearchCV(estimator=model,param_grid=grid_param,cv=kfold,scoring='mean_squared_error')
    grid.fit(x,y)
    display_param_results(grid.grid_scores_)
    print grid.best_params_
    # Track the scores- Mean squared residual for plot
    return grid.best_estimator_

def view_model(model):
    """
    Look at model coeffiecients
    """
    #print "\n Estimated Alpha = %0.3f"%model.alpha_
    print "\n Model coeffiecients"
    print "======================\n"
    for i,coef in enumerate(model.coef_):
        print "\tCoefficient %d  %0.3f"%(i+1,coef)
            
    print "\n\tIntercept %0.3f"%(model.intercept_)

def model_worth(true_y,predicted_y):
    """
    Evaluate the model
    """
    print "\tMean squared error = %0.2f"%(mean_squared_error(true_y,predicted_y))
    return mean_squared_error(true_y,predicted_y)

def display_param_results(param_results):
    fold = 1
    for param_result in param_results:
        print "Fold %d Mean squared error %0.2f"%(fold,abs(param_result[1])),param_result[0]
        fold+=1</pre></div><p class="calibre11">Finally, we will <a id="id604" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>write <a id="id605" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>our <code class="literal">main</code> function, which is <a id="id606" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>used to invoke all the preceding functions:</p><div><pre class="programlisting">if __name__ == "__main__":
    
    x,y = get_data()
    
    # Divide the data into Train and test    
    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state=9)

    #Prepare some polynomial features
    poly_features = PolynomialFeatures(interaction_only=True)
    poly_features.fit(x_train)
    x_train_poly = poly_features.transform(x_train)
    x_test_poly  = poly_features.transform(x_test)
    
    choosen_model = build_model(x_train_poly,y_train)
    predicted_y = choosen_model.predict(x_train_poly)
    model_worth(y_train,predicted_y)
    
    view_model(choosen_model)
    
    predicted_y = choosen_model.predict(x_test_poly)
    model_worth(y_test,predicted_y)</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec274" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let's start with <a id="id607" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>our main <a id="id608" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>method. We <a id="id609" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will start with the <code class="literal">KFold</code> class. This iterator class is instantiated with the number of instances in our dataset and the number of folds that we require:</p><div><pre class="programlisting">kfolds = KFold(n=y.shape[0],n_folds=3)</pre></div><p class="calibre11">Now, we can iterate through the folds, as follows:</p><div><pre class="programlisting">fold_count =1
print
for train,test in kfolds:
print "Fold %d x train shape"%(fold_count),x[train].shape,\
" x test shape",x[test].shape
fold_count==1</pre></div><p class="calibre11">Let's see the print statement output:</p><div><img src="img/B04041_07_30.jpg" alt="How it works…" class="calibre165"/></div><p class="calibre11">We can see that the data is split into three parts, each with 100 instances to train and 50 instances to test.</p><p class="calibre11">We will move on next to <code class="literal">StratifiedKFold</code>. Recall our discussion on having a uniform class distribution in the train and test split from the previous chapter. <code class="literal">StratifiedKFold</code> achieves a uniform class distribution across the three folds.</p><p class="calibre11">It is invoked as follows:</p><div><pre class="programlisting">skfolds = StratifiedKFold(y,n_folds=3)</pre></div><p class="calibre11">As it needs to know the distribution of the class label in the dataset, this iterator object takes the response variable <code class="literal">y</code> as one of its parameters. The other parameter is the number of folds requested.</p><p class="calibre11">Let's print the shape <a id="id610" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of our train<a id="id611" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> and test sets in these three folds, along with their class distribution. We <a id="id612" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will use the <code class="literal">class_distribution</code> function to print the distribution of the classes in each of the folds:</p><div><pre class="programlisting">fold_count =1
for train,test in skfolds:
print "\nFold %d x train shape"%(fold_count),x[train].shape,\
" x test shape",x[test].shape
y_train = y[train]
y_test = y[test]
print "Train Class Distribution"
class_distribution(y_train)
print "Test Class Distribution"
class_distribution(y_test)

fold_count+=1</pre></div><div><img src="img/B04041_07_31.jpg" alt="How it works…" class="calibre166"/></div><p class="calibre11">You can see that the classes are distributed uniformly.</p><p class="calibre11">Let's assume that you build a five-fold dataset, you fit five different models, and you have five different accuracy scores. You can now take the mean of these scores to evaluate how good your model has turned out to be. If you are not satisfied, you can go ahead and start rebuilding your model with a different set of parameters and again run it on the five-fold data and see the mean accuracy score. This way, you can continuously improve the model by finding the right parameter values only using the training dataset.</p><p class="calibre11">Armed with this knowledge, let's revisit our old ridge regression problem.</p><p class="calibre11">Let's start with the <code class="literal">main</code> module and follow the code. We will load the predictor <code class="literal">x</code> and response variable <code class="literal">y</code> using the <code class="literal">get_data</code> function. This function invokes scikit-learn's convenient <code class="literal">load_boston()</code> function to retrieve the Boston house pricing dataset as NumPy arrays.</p><p class="calibre11">We will proceed to <a id="id613" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>divide the <a id="id614" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data into train<a id="id615" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> and test sets using the <code class="literal">train_test_split</code> function from the scikit-learn library. We will reserve 30 percent of our dataset to test.</p><p class="calibre11">We will then to build the polynomial features:</p><div><pre class="programlisting">poly_features = PolynomialFeatures(interaction_only=True)
poly_features.fit(x_train)</pre></div><p class="calibre11">As you can see, we set <code class="literal">interaction_only</code> to <code class="literal">true</code>. By setting <code class="literal">interaction_only</code> to <code class="literal">true</code>—with <code class="literal">x1</code> and <code class="literal">x2</code> attributes—only the <code class="literal">x1*x2</code> attribute is created. The squares of <code class="literal">x1</code> and <code class="literal">x2</code> are not created, assuming that the degree is two. The default degree is two:</p><div><pre class="programlisting">x_train_poly = poly_features.transform(x_train)
x_test_poly = poly_features.transform(x_test)</pre></div><p class="calibre11">Using the transform function, we will transform our train and test dataset to include the polynomial features. Let's call the <code class="literal">build_model</code> function. The first thing that we notice in the <code class="literal">build_model</code> function is the k-fold declaration. We will apply our knowledge of cross-validation here and create a five-fold dataset:</p><div><pre class="programlisting">kfold = KFold(y.shape[0],5)</pre></div><p class="calibre11">We will then create our ridge object:</p><div><pre class="programlisting">model = Ridge(normalize=True)</pre></div><p class="calibre11">Let's now see how we can leverage our k-folds to figure out the ideal alpha value for our ridge regression. In the next line, we will create an object out of <code class="literal">GridSearchCV:</code>
</p><div><pre class="programlisting">grid = GridSearchCV(estimator=model,param_grid=grid_param,cv=kfold,scoring='mean_squared_error')</pre></div><p class="calibre11">
<code class="literal">GridSearchCV</code> is a convenient function from scikit-learn that helps us train our models with a range of parameters. In this case, we want to find the ideal alpha value, and hence, would like to train our models with different alpha values. Let's look at the parameters passed to <code class="literal">GridSearchCV</code>:</p><p class="calibre11">Estimator: This is the type of model that should be run with the given parameter and data. In our case, we want to run ridge regression. Hence, we will create a ridge object and pass it to <code class="literal">GridSearchCV</code>.</p><p class="calibre11">Param-grid: This is a<a id="id616" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> dictionary of<a id="id617" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> parameters that we want to evaluate our model on. Let's work this through in detail. We will first <a id="id618" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>declare the range of alpha values that we want to build our model on:</p><div><pre class="programlisting">alpha_range = np.linspace(0.0015,0.0017,30)</pre></div><p class="calibre11">This gives us a NumPy array of 30 uniformly spaced elements starting from 0.0015 and ending at 0.0017. We want to build a model for each of these values. We will create a dictionary object called <code class="literal">grid_param</code> and make an entry under the alpha key with the generated NumPy array of alpha values:</p><div><pre class="programlisting">grid_param = {"alpha":alpha_range}</pre></div><p class="calibre11">We will pass this dictionary as a parameter to <code class="literal">GridSearchCV</code>. Look at the entry, <code class="literal">param_grid=grid_param</code>.</p><p class="calibre11">Cv: This defines the kind of cross-validation that we are interested in. We will pass the k-fold (five-fold) iterator that we created before as the cv parameter.</p><p class="calibre11">Finally, we need to define a scoring function. In our case, we are interested in finding out the squared error. This is the metric with which we will evaluate our model.</p><p class="calibre11">So, internal <code class="literal">GridSearchCV</code> will build five models for each of our parameter values and return the mean score when tested in the left out folds. In our case, we have five folds of test data, so the average of the score values across these five folds of test data is returned.</p><p class="calibre11">With this explained, we will then fit our model, that is, start our grid search activity.</p><p class="calibre11">Finally, we want to see the output at the various parameter settings. We will use the <code class="literal">display_param_results</code> function to display the average mean squared error across the different folds:</p><div><img src="img/B04041_07_32.jpg" alt="How it works…" class="calibre167"/></div><p class="calibre11">Each line in the <a id="id619" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>output<a id="id620" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> displays the <a id="id621" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>parameter alpha value and average mean squared error from the test folds. We can see that as we move deep into the 0.0016 range, the mean square error is increasing. Hence, we decide to stop at 0.0015. We can query the grid object to get the best parameter and estimator:</p><div><pre class="programlisting">print grid.best_params_
return grid.best_estimator_</pre></div><p class="calibre11">This was not the first set of alpha values that we tested it with. Our initial alpha values were as follows:</p><div><pre class="programlisting">alpha_range = np.linspace(0.01,1.0,30)</pre></div><p class="calibre11">The following was our output:</p><div><img src="img/B04041_07_33.jpg" alt="How it works…" class="calibre168"/></div><p class="calibre11">When our alpha <a id="id622" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>values <a id="id623" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>were above 0.01, the <a id="id624" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>mean squared error was shooting up. Hence, we again gave a new range:</p><div><pre class="programlisting">alpha_range = np.linspace(0.001,0.1,30)</pre></div><p class="calibre11">Our output was as follows:</p><div><img src="img/B04041_07_34.jpg" alt="How it works…" class="calibre169"/></div><p class="calibre11">This way, iteratively<a id="id625" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> we arrived <a id="id626" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>at the range<a id="id627" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> starting at 0.0015 and ending at 0.0017.</p><p class="calibre11">We will then get the best estimator from our grid search and apply it to our train and test data:</p><div><pre class="programlisting">choosen_model = build_model(x_train_poly,y_train)
predicted_y = choosen_model.predict(x_train_poly)
model_worth(y_train,predicted_y)</pre></div><p class="calibre11">Our <code class="literal">model_worth</code> function prints the mean squared error value in our training dataset:</p><div><img src="img/B04041_07_35.jpg" alt="How it works…" class="calibre170"/></div><p class="calibre11">Let's view our coefficient weights:</p><div><img src="img/B04041_07_36.jpg" alt="How it works…" class="calibre171"/></div><p class="calibre11">We have not<a id="id628" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> displayed <a id="id629" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>all of them <a id="id630" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>but when you run the code, you can view all the values.</p><p class="calibre11">Finally, let's apply the model to our test dataset:</p><div><img src="img/B04041_07_37.jpg" alt="How it works…" class="calibre172"/></div><p class="calibre11">Thus, we used cross-validation and grid search to arrive at an alpha value for our ridge regression successfully. Our model has resulted in a lower mean squared error compared with the value in the ridge regression recipe.</p></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec275" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">There are other cross-validation iterators available with scikit-learn. Of particular interest in this case is the<a id="id631" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> leave-one-out iterator. You can read more about this at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/cross_validation.html#leave-one-out-loo">http://scikit-learn.org/stable/modules/cross_validation.html#leave-one-out-loo</a>.</p><p class="calibre11">In this method, given<a id="id632" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the number <a id="id633" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of folds, it leaves one record to test and returns the rest to train. For example, if your input data has 100 instances and if we require five folds, we will get 99 instances to train and <a id="id634" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>one to test in each fold.</p><p class="calibre11">In the grid search method that we used before, if we don't provide a custom iterator to the <strong class="calibre12">cross validation</strong> (<strong class="calibre12">cv</strong>) parameter, it will by default use the leave-one-out cross-validation method:</p><div><pre class="programlisting">grid = GridSearchCV(estimator=model,param_grid=grid_param,cv=None,scoring='mean_squared_error')</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch07lvl2sec276" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Scaling the data</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <em class="calibre15">Data Analysis – Explore and Wrangle</em></li><li class="listitem"><em class="calibre15">Standardizing the data</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <em class="calibre15">Data Analysis – Explore and Wrangle</em></li><li class="listitem"><em class="calibre15">Preparing data for model building</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6. Machine Learning 1">Chapter 6</a>, <em class="calibre15">Machine Learning I</em></li><li class="listitem"><em class="calibre15">Regression with L2 Shrinkage – Ridge</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>, <em class="calibre15">Machine Learning II</em></li><li class="listitem"><em class="calibre15">Regression with L2 Shrinkage – Lasso</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>, <em class="calibre15">Machine Learning II</em></li></ul></div></div></div></div>



  </body></html>