["```py\n    import pandas as pd\n    Link = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv'\n    #reading the data into the dataframe into the object data\n    df = pd.read_csv(Link, header=0)\n    ```", "```py\n    #Finding number of rows and columns\n    print(\"Number of rows and columns : \",df.shape)\n    ```", "```py\n    #Printing all the columns\n    print(list(df.columns))\n    ```", "```py\n    #Basic Statistics of each column\n    df.describe().transpose()\n    ```", "```py\n    #Basic Information of each column\n    print(df.info())\n    ```", "```py\n    #finding the data types of each column and checking for null\n    null_ = df.isna().any()\n    dtypes = df.dtypes\n    sum_na_ = df.isna().sum()\n    info = pd.concat([null_,sum_na_,dtypes],axis = 1,keys = ['isNullExist','NullSum','type'])\n    info\n    ```", "```py\n    #removing Null values\n    df = df.dropna()\n    #Total number of null in each column\n    print(df.isna().sum())# No NA\n    ```", "```py\n    df.education.value_counts()\n    ```", "```py\n    df.education.unique()  \n    ```", "```py\n    df.education.replace({\"basic.9y\":\"Basic\",\"basic.6y\":\"Basic\",\"basic.4y\":\"Basic\"},inplace=True)\n    ```", "```py\n    df.education.unique()  \n    ```", "```py\n    #Select all the non numeric data using select_dtypes function\n    data_column_category = df.select_dtypes(exclude=[np.number]).columns\n    ```", "```py\n    cat_vars=data_column_category\n    for var in cat_vars:\n        cat_list='var'+'_'+var\n        cat_list = pd.get_dummies(df[var], prefix=var)\n        data1=df.join(cat_list)\n        df=data1\n     df.columns\n    ```", "```py\n    #Categorical features\n    cat_vars=data_column_category\n    #All features\n    data_vars=df.columns.values.tolist()\n    #neglecting the categorical column for which we have done encoding\n    to_keep = []\n    for i in data_vars:\n        if i not in cat_vars:\n            to_keep.append(i)\n\n    #selecting only the numerical and encoded catergorical column\n    data_final=df[to_keep]\n    data_final.columns\n    ```", "```py\n    #Segregating Independent and Target variable\n    X=data_final.drop(columns='y')\n    y=data_final['y']\n    from sklearn. model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    print(\"FULL Dateset X Shape: \", X.shape )\n    print(\"Train Dateset X Shape: \", X_train.shape )\n    print(\"Test Dateset X Shape: \", X_test.shape )\n    ```", "```py\n    x = ['January','February','March','April','May','June']\n    ```", "```py\n    y = [1000, 1200, 1400, 1600, 1800, 2000]\n    ```", "```py\n    plt.plot(x, y, '*:b')\n    ```", "```py\n    plt.xlabel('Month')\n    ```", "```py\n    plt.ylabel('Items Sold')\n    ```", "```py\n    plt.title('Items Sold has been Increasing Linearly')\n    ```", "```py\n    x = ['Boston Celtics','Los Angeles Lakers', 'Chicago Bulls', 'Golden State Warriors', 'San Antonio Spurs']\n    ```", "```py\n    y = [17, 16, 6, 6, 5]\n    ```", "```py\n    import pandas as pd\n\n    df = pd.DataFrame({'Team': x,\n                       'Titles': y})\n    ```", "```py\n    df_sorted = df.sort_values(by=('Titles'), ascending=False)\n    ```", "```py\n    team_with_most_titles = df_sorted['Team'][0]\n    ```", "```py\n    most_titles = df_sorted['Titles'][0]\n    ```", "```py\n    title = 'The {} have the most titles with {}'.format(team_with_most_titles, most_titles)\n    ```", "```py\n    import matplotlib.pyplot as plt\n\n    plt.bar(df_sorted['Team'], df_sorted['Titles'], color='red')\n    ```", "```py\n    plt.xlabel('Team')\n    ```", "```py\n    plt.ylabel('Number of Championships')\n    ```", "```py\n    plt.xticks(rotation=45)\n    ```", "```py\n    plt.title(title)\n    ```", "```py\n    plt.savefig('Titles_by_Team)\n    ```", "```py\n    plt.savefig('Titles_by_Team', bbox_inches='tight')\n    ```", "```py\n    import pandas as pd\n\n    Items_by_Week = pd.read_csv('Items_Sold_by_Week.csv')\n    ```", "```py\n    Weight_by_Height = pd.read_csv('Weight_by_Height.csv')\n    ```", "```py\n    y = np.random.normal(loc=0, scale=0.1, size=100)\n    ```", "```py\n    import matplotlib.pyplot as plt\n\n    fig, axes = plt.subplots(nrows=3, ncols=2)\n    plt.tight_layout()\n    ```", "```py\n    axes[0,0].set_title('Line') \n    axes[0,1].set_title('Bar') \n    axes[1,0].set_title('Horizontal Bar') \n    axes[1,1].set_title('Histogram') \n    axes[2,0].set_title('Scatter') \n    axes[2,1].set_title('Box-and-Whisker') \n    ```", "```py\n    axes[0,0].plot(Items_by_Week['Week'], Items_by_Week['Items_Sold'])\n    axes[0,1].bar(Items_by_Week['Week'], Items_by_Week['Items_Sold'])\n    axes[1,0].barh(Items_by_Week['Week'], Items_by_Week['Items_Sold'])\n    ```", "```py\n    axes[1,1].hist(y, bins=20)axes[2,1].boxplot(y)\n    ```", "```py\n    axes[2,0].scatter(Weight_by_Height['Height'], Weight_by_Height['Weight'])\n    ```", "```py\n    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8,8))\n    ```", "```py\n    fig.savefig('Six_Subplots')\n    ```", "```py\n    predictions = model.predict(X_test)\n    2.    Plot the predicted versus actual values on a scatterplot using the following code:\n    import matplotlib.pyplot as plt\n    from scipy.stats import pearsonr\n\n    plt.scatter(y_test, predictions)\n    plt.xlabel('Y Test (True Values)')\n    plt.ylabel('Predicted Values')\n    plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, predictions)[0], 2))\n    plt.show()\n    ```", "```py\n    import seaborn as sns\n    from scipy.stats import shapiro\n\n    sns.distplot((y_test - predictions), bins = 50)\n    plt.xlabel('Residuals')\n    plt.ylabel('Density')\n    plt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - predictions)[1]))\n    plt.show()\n    ```", "```py\n    from sklearn import metrics\n    import numpy as np\n\n    metrics_df = pd.DataFrame({'Metric': ['MAE', \n                                          'MSE', \n                                          'RMSE', \n                                          'R-Squared'],\n                              'Value': [metrics.mean_absolute_error(y_test, predictions),\n                                          metrics.mean_squared_error(y_test, predictions),\n                                        np.sqrt(metrics.mean_squared_error(y_test, predictions)),\n                                        metrics.explained_variance_score(y_test, predictions)]}).round(3)\n    print(metrics_df)\n    ```", "```py\n    predicted_prob = model.predict_proba(X_test)[:,1]\n    ```", "```py\n    from sklearn.metrics import confusion_matrix\n    import numpy as np\n\n    cm = pd.DataFrame(confusion_matrix(y_test, predicted_class))\n    cm['Total'] = np.sum(cm, axis=1)\n    cm = cm.append(np.sum(cm, axis=0), ignore_index=True)\n    cm.columns = ['Predicted No', 'Predicted Yes', 'Total']\n    cm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])\n    print(cm)\n    ```", "```py\n    from sklearn.metrics import classification_report\n\n    print(classification_report(y_test, predicted_class))\n    ```", "```py\n    predicted_class = model.predict(X_test)\n    ```", "```py\n    from sklearn.metrics import confusion_matrix\n    import numpy as np\n\n    cm = pd.DataFrame(confusion_matrix(y_test, predicted_class))\n    cm['Total'] = np.sum(cm, axis=1)\n    cm = cm.append(np.sum(cm, axis=0), ignore_index=True)\n    cm.columns = ['Predicted No', 'Predicted Yes', 'Total']\n    cm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])\n    print(cm)\n    ```", "```py\n    from sklearn.metrics import classification_report\n\n    print(classification_report(y_test, predicted_class))\n    ```", "```py\n    import pandas as pd\n\n    df = pd.read_csv('weather.csv')\n    ```", "```py\n    import pandas as pd\n\n    df_dummies = pd.get_dummies(df, drop_first=True)\n    ```", "```py\n    from sklearn.utils import shuffle\n\n    df_shuffled = shuffle(df_dummies, random_state=42)\n    ```", "```py\n    DV = 'Rain'\n    X = df_shuffled.drop(DV, axis=1)\n    y = df_shuffled[DV]\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    ```", "```py\n    from sklearn.preprocessing import StandardScaler\n\n    model = StandardScaler()\n    X_train_scaled = model.fit_transform(X_train)\n    X_test_scaled = model.transform(X_test)\n    ```", "```py\n    predicted_prob = model.predict_proba(X_test_scaled)[:,1]\n    ```", "```py\n    predicted_class = model.predict(X_test)\n    ```", "```py\n    from sklearn.metrics import confusion_matrix\n    import numpy as np\n\n    cm = pd.DataFrame(confusion_matrix(y_test, predicted_class))\n    cm['Total'] = np.sum(cm, axis=1)\n    cm = cm.append(np.sum(cm, axis=0), ignore_index=True)\n    cm.columns = ['Predicted No', 'Predicted Yes', 'Total']\n    cm = cm.set_index([['Actual No', 'Actual Yes', 'Total']])\n    print(cm)\n    ```", "```py\n    from sklearn.metrics import classification_report\n    print(classification_report(y_test, predicted_class))\n    ```", "```py\n    import numpy as np\n\n    grid = {'criterion': ['mse','mae'],\n            'max_features': ['auto', 'sqrt', 'log2', None],\n            'min_impurity_decrease': np.linspace(0.0, 1.0, 10),\n            'bootstrap': [True, False],\n            'warm_start': [True, False]}\n    ```", "```py\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.ensemble import RandomForestRegressor\n\n    model = GridSearchCV(RandomForestRegressor(), grid, scoring='explained_variance', cv=5)\n    ```", "```py\n    model.fit(X_train_scaled, y_train)\n    ```", "```py\n    best_parameters = model.best_params_\n    print(best_parameters)\n    ```", "```py\n    predictions = model.predict(X_test_scaled)\n    ```", "```py\n    import matplotlib.pyplot as plt\n    from scipy.stats import pearsonr\n\n    plt.scatter(y_test, predictions)\n    plt.xlabel('Y Test (True Values)')\n    plt.ylabel('Predicted Values')\n    plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, predictions)[0], 2))\n    plt.show()\n    ```", "```py\n    import seaborn as sns\n    from scipy.stats import shapiro\n\n    sns.distplot((y_test - predictions), bins = 50)\n    plt.xlabel('Residuals')\n    plt.ylabel('Density')\n    plt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - predictions)[1]))\n    plt.show()\n    ```", "```py\n    from sklearn import metrics\n    import numpy as np\n\n    metrics_df = pd.DataFrame({'Metric': ['MAE', \n                                          'MSE', \n                                          'RMSE', \n                                          'R-Squared'],\n                              'Value': [metrics.mean_absolute_error(y_test, predictions),\n                                        metrics.mean_squared_error(y_test, predictions),\n                                        np.sqrt(metrics.mean_squared_error(y_test, predictions)),\n                                        metrics.explained_variance_score(y_test, predictions)]}).round(3)\n    print(metrics_df)\n    ```", "```py\n    import pandas as pd\n    labels_df = pd.DataFrame()\n    ```", "```py\n    from sklearn.cluster import KMeans\n    ```", "```py\n    for i in range(0, 100):\n    ```", "```py\n    model = KMeans(n_clusters=2)\n    ```", "```py\n    model.fit(scaled_features)\n    ```", "```py\n    labels = model.labels_\n    ```", "```py\n    labels_df['Model_{}_Labels'.format(i+1)] = labels\n    ```", "```py\n    row_mode = labels_df.mode(axis=1)\n    ```", "```py\n    labels_df['row_mode'] = row_mode\n    ```", "```py\n    print(labels_df.head(5))\n    ```", "```py\n    from sklearn.decomposition import PCA\n    model = PCA(n_components=best_n_components)\n    ```", "```py\n    df_pca = model.fit_transform(scaled_features)\n    ```", "```py\n    from sklearn.cluster import KMeans\n    import numpy as np\n    ```", "```py\n    inertia_list = []\n    ```", "```py\n    for i in range(100):\n    ```", "```py\n    model = KMeans(n_clusters=x)\n    ```", "```py\n    model.fit(df_pca)\n    ```", "```py\n    inertia = model.inertia_\n    ```", "```py\n    inertia_list.append(inertia)\n    ```", "```py\n    mean_inertia_list_PCA = []\n    ```", "```py\n    for x in range(1, 11):\n    ```", "```py\n    mean_inertia = np.mean(inertia_list)\n    ```", "```py\n    mean_inertia_list_PCA.append(mean_inertia)\n    ```", "```py\n    print(mean_inertia_list_PCA)\n    ```", "```py\n    import pandas as pd\n    import xgboost as xgb\n    import numpy as np\n    from sklearn.metrics import accuracy_score\n    data = pd.read_csv(\"../data/adult-data.csv\", names=['age', 'workclass', 'education-num', 'occupation', 'capital-gain', 'capital-loss', 'hours-per-week', 'income'])\n    ```", "```py\n    from sklearn.preprocessing import LabelEncoder\n    data['workclass'] = LabelEncoder().fit_transform(data['workclass'])\n    data['occupation'] = LabelEncoder().fit_transform(data['occupation'])\n    data['income'] = LabelEncoder().fit_transform(data['income'])\n    ```", "```py\n    X = data.copy()\n    X.drop(\"income\", inplace = True, axis = 1)\n    Y = data.income\n    ```", "```py\n    X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values\n    Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values\n    ```", "```py\n    train = xgb.DMatrix(X_train, label=Y_train)\n    test = xgb.DMatrix(X_test, label=Y_test)\n    ```", "```py\n    param = {'max_depth':7, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'} num_round = 50\n    model = xgb.train(param, train, num_round)\n    ```", "```py\n    preds = model.predict(test)\n    accuracy = accuracy_score(Y[int(Y.shape[0]*0.8):].values, preds)\n    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    data = data = pd.read_csv(\"data/telco-churn.csv\")\n    ```", "```py\n    data.drop('customerID', axis = 1, inplace = True)\n    ```", "```py\n    from sklearn.preprocessing import LabelEncoder\n    data['gender'] = LabelEncoder().fit_transform(data['gender'])\n    ```", "```py\n    data.dtypes\n    ```", "```py\n    data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')\n    ```", "```py\n    import xgboost as xgb\n    import matplotlib.pyplot as plt\n    X = data.copy()\n    X.drop(\"Churn\", inplace = True, axis = 1)\n    Y = data.Churn\n    X_train, X_test = X[:int(X.shape[0]*0.8)].values, X[int(X.shape[0]*0.8):].values\n    Y_train, Y_test = Y[:int(Y.shape[0]*0.8)].values, Y[int(Y.shape[0]*0.8):].values\n    train = xgb.DMatrix(X_train, label=Y_train)\n    test = xgb.DMatrix(X_test, label=Y_test)\n    test_error = {}\n    for i in range(20):\n        param = {'max_depth':i, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'}\n        num_round = 50\n        model_metrics = xgb.cv(param, train, num_round, nfold = 10)\n        test_error[i] = model_metrics.iloc[-1]['test-error-mean']\n\n    plt.scatter(test_error.keys(),test_error.values())\n    plt.xlabel('Max Depth')\n    plt.ylabel('Test Error')\n    plt.show()\n    ```", "```py\n    param = {'max_depth':4, 'eta':0.1, 'silent':1, 'objective':'binary:hinge'}\n    num_round = 100\n    model = xgb.train(param, train, num_round)\n    preds = model.predict(test)\n    from sklearn.metrics import accuracy_score\n    accuracy = accuracy_score(Y[int(Y.shape[0]*0.8):].values, preds)\n    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n    ```", "```py\n    model.save_model('churn-model.model')\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    data = data = pd.read_csv(\"data/BlackFriday.csv\")\n    ```", "```py\n    data.isnull().sum()\n    data.drop(['User_ID', 'Product_Category_2', 'Product_Category_3'], axis = 1, inplace = True)\n    ```", "```py\n    from collections import defaultdict\n    from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n    label_dict = defaultdict(LabelEncoder)\n    data[['Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1']] = data[['Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1']].apply(lambda x: label_dict[x.name].fit_transform(x)) \n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X = data\n    y = X.pop('Purchase')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=9)\n\n    cat_cols_dict = {col: list(data[col].unique()) for col in ['Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status', 'Product_Category_1']}\n    train_input_list = []\n    test_input_list = []\n\n    for col in cat_cols_dict.keys():\n        raw_values = np.unique(data[col])\n        value_map = {}\n        for i in range(len(raw_values)):\n            value_map[raw_values[i]] = i       \n        train_input_list.append(X_train[col].map(value_map).values)\n        test_input_list.append(X_test[col].map(value_map).fillna(0).values)\n    ```", "```py\n    from keras.models import Model\n    from keras.layers import Input, Dense, Concatenate, Reshape, Dropout\n    from keras.layers.embeddings import Embedding\n    cols_out_dict = {\n        'Product_ID': 20,\n        'Gender': 1,\n        'Age': 2,\n        'Occupation': 6,\n        'City_Category': 1,\n        'Stay_In_Current_City_Years': 2,\n        'Marital_Status': 1,\n        'Product_Category_1': 9\n    }\n\n    inputs = []\n    embeddings = []\n\n    for col in cat_cols_dict.keys():\n\n        inp = Input(shape=(1,), name = 'input_' + col)\n        embedding = Embedding(len(cat_cols_dict[col]), cols_out_dict[col], input_length=1, name = 'embedding_' + col)(inp)\n        embedding = Reshape(target_shape=(cols_out_dict[col],))(embedding)\n        inputs.append(inp)\n        embeddings.append(embedding)\n    ```", "```py\n    x = Concatenate()(embeddings)\n    x = Dense(4, activation='relu')(x)\n    x = Dense(2, activation='relu')(x)\n    output = Dense(1, activation='relu')(x)\n\n    model = Model(inputs, output)\n\n    model.compile(loss='mae', optimizer='adam')\n\n    model.fit(train_input_list, y_train, validation_data = (test_input_list, y_test), epochs=20, batch_size=128)\n    ```", "```py\n    from sklearn.metrics import mean_squared_error\n    y_pred = model.predict(test_input_list)\n    np.sqrt(mean_squared_error(y_test, y_pred))\n    ```", "```py\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    embedding_Product_ID = model.get_layer('embedding_Product_ID').get_weights()[0]\n    pca = PCA(n_components=2) \n    Y = pca.fit_transform(embedding_Product_ID[:40])\n    plt.figure(figsize=(8,8))\n    plt.scatter(-Y[:, 0], -Y[:, 1])\n    for i, txt in enumerate(label_dict['Product_ID'].inverse_transform(cat_cols_dict['Product_ID'])[:40]):\n        plt.annotate(txt, (-Y[i, 0],-Y[i, 1]), xytext = (-20, 8), textcoords = 'offset points')\n    plt.show()\n    ```", "```py\n    model.save ('black-friday.model')\n    ```", "```py\n    def get_label(file):\n        class_label = file.split('.')[0]\n        if class_label == 'dog': label_vector = [1,0]\n        elif class_label == 'cat': label_vector = [0,1]\n        return label_vector\n    ```", "```py\n    import os\n    import numpy as np\n    from PIL import Image\n    from tqdm import tqdm\n    from random import shuffle\n\n    SIZE = 50\n\n    def get_data():\n        data = []\n        files = os.listdir(PATH)\n\n        for image in tqdm(files): \n            label_vector = get_label(image) \n            img = Image.open(PATH + image).convert('L')\n            img = img.resize((SIZE,SIZE)) \n            data.append([np.asarray(img),np.array(label_vector)])\n\n        shuffle(data)\n        return data\n    ```", "```py\n    data = get_data()\n    train = data[:7000]\n    test = data[7000:]\n    x_train = [data[0] for data in train]\n    y_train = [data[1] for data in train]\n    x_test = [data[0] for data in test]\n    y_test = [data[1] for data in test]\n    ```", "```py\n    y_train = np.array(y_train)\n    y_test = np.array(y_test)\n    x_train = np.array(x_train).reshape(-1, SIZE, SIZE, 1)\n    x_test = np.array(x_test).reshape(-1, SIZE, SIZE, 1)\n    ```", "```py\n    from keras.models import Sequential\n    from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization\n    model = Sequential() \n    ```", "```py\n    model.add(Conv2D(48, (3, 3), activation='relu', padding='same', input_shape=(50,50,1)))    \n    model.add(Conv2D(48, (3, 3), activation='relu'))  \n    ```", "```py\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    ```", "```py\n    model.add(BatchNormalization())\n    model.add(Dropout(0.10))\n    ```", "```py\n    model.add(Flatten())\n    ```", "```py\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(2, activation='softmax'))\n    ```", "```py\n    model.compile(loss='categorical_crossentropy', \n                  optimizer='adam',\n                  metrics = ['accuracy'])\n    Define the number of epochs you want to train the model for:\n    EPOCHS = 10\n    model_details = model.fit(x_train, y_train,\n                        batch_size = 128, \n                        epochs = EPOCHS, \n                        validation_data= (x_test, y_test),\n                        verbose=1)\n    ```", "```py\n    score = model.evaluate(x_test, y_test)\n    print(\"Accuracy: {0:.2f}%\".format(score[1]*100)) \n    ```", "```py\n    score = model.evaluate(x_train, y_train)\n    print(\"Accuracy: {0:.2f}%\".format(score[1]*100)) \n    ```", "```py\nimport matplotlib.pyplot as plt\ny_pred = model.predict(x_test)\nincorrect_indices = np.nonzero(np.argmax(y_pred,axis=1) != np.argmax(y_test,axis=1))[0]\nlabels = ['dog', 'cat']\nimage = 5\nplt.imshow(x_test[incorrect_indices[image]].reshape(50,50),  cmap=plt.get_cmap('gray'))\nplt.show()\nprint(\"Prediction: {0}\".format(labels[np.argmax(y_pred[incorrect_indices[image]])]))\n```", "```py\n    from PIL import Image\n    def get_input(file):\n        return Image.open(PATH+file)\n\n    def get_output(file):\n        class_label = file.split('.')[0]\n        if class_label == 'dog': label_vector = [1,0]\n        elif class_label == 'cat': label_vector = [0,1]\n        return label_vector\n    ```", "```py\n    SIZE = 50\n    def preprocess_input(image):\n        # Data preprocessing\n        image = image.convert('L')\n        image = image.resize((SIZE,SIZE))\n\n        # Data augmentation\n        random_vertical_shift(image, shift=0.2)\n        random_horizontal_shift(image, shift=0.2)\n        random_rotate(image, rot_range=45)\n        random_horizontal_flip(image)\n\n        return np.array(image).reshape(SIZE,SIZE,1)\n    ```", "```py\n    import random\n    def random_horizontal_flip(image):\n        toss = random.randint(1, 2)\n        if toss == 1:\n            return image.transpose(Image.FLIP_LEFT_RIGHT)\n        else:\n            return image\n    ```", "```py\n    def random_rotate(image, rot_range):\n        value = random.randint(-rot_range,rot_range)\n        return image.rotate(value)\n    ```", "```py\n    import PIL\n    def random_horizontal_shift(image, shift):\n        width, height = image.size\n        rand_shift = random.randint(0,shift*width)\n        image = PIL.ImageChops.offset(image, rand_shift, 0)\n        image.paste((0), (0, 0, rand_shift, height))\n        return image\n     def random_vertical_shift(image, shift):\n        width, height = image.size\n        rand_shift = random.randint(0,shift*height)\n        image = PIL.ImageChops.offset(image, 0, rand_shift)\n        image.paste((0), (0, 0, width, rand_shift))\n        return image\n    ```", "```py\n    import numpy as np\n    def custom_image_generator(images, batch_size = 128):\n        while True:\n            # Randomly select images for the batch\n            batch_images = np.random.choice(images, size = batch_size)\n            batch_input = []\n            batch_output = [] \n\n            # Read image, perform preprocessing and get labels\n            for file in batch_images:\n                # Function that reads and returns the image\n                input_image = get_input(file)\n                # Function that gets the label of the image\n                label = get_output(file)\n                # Function that pre-processes and augments the image\n                image = preprocess_input(input_image)\n\n                batch_input.append(image)\n                batch_output.append(label)\n\n            batch_x = np.array(batch_input)\n            batch_y = np.array(batch_output)\n\n            # Return a tuple of (images,labels) to feed the network\n            yield(batch_x, batch_y)\n    ```", "```py\n    def get_label(file):\n        class_label = file.split('.')[0]\n        if class_label == 'dog': label_vector = [1,0]\n        elif class_label == 'cat': label_vector = [0,1]\n        return label_vector\n    ```", "```py\n    def get_data(files):\n        data_image = []\n        labels = []\n        for image in tqdm(files):\n\n            label_vector = get_label(image)\n\n            img = Image.open(PATH + image).convert('L')\n            img = img.resize((SIZE,SIZE))\n\n            labels.append(label_vector)\n            data_image.append(np.asarray(img).reshape(SIZE,SIZE,1))\n\n        data_x = np.array(data_image)\n        data_y = np.array(labels)\n\n        return (data_x, data_y)\n    ```", "```py\n    import os\n    files = os.listdir(PATH)\n    random.shuffle(files)\n    train = files[:7000]\n    test = files[7000:]\n    validation_data = get_data(test)\n    ```", "```py\n    from keras.models import Sequential\n    model = Sequential()\n    ```", "```py\n    from keras.layers import Input, Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(50,50,1)))    \n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    ```", "```py\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    ```", "```py\n    model.add(BatchNormalization())\n    model.add(Dropout(0.10))\n    ```", "```py\n    model.add(Flatten())\n    ```", "```py\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(2, activation='softmax'))\n    ```", "```py\n    EPOCHS = 10\n    BATCH_SIZE = 128\n    model.compile(loss='categorical_crossentropy', \n                  optimizer='adam',\n                  metrics = ['accuracy']) \n    model_details = model.fit_generator(custom_image_generator(train, batch_size = BATCH_SIZE),\n                        steps_per_epoch = len(train) // BATCH_SIZE, \n                        epochs = EPOCHS, \n                        validation_data= validation_data,\n                        verbose=1) \n    ```", "```py\nimport matplotlib.pyplot as plt\ny_pred = model.predict(validation_data[0])\nincorrect_indices = np.nonzero(np.argmax(y_pred,axis=1) != np.argmax(validation_data[1],axis=1))[0]\nlabels = ['dog', 'cat']\nimage = 7\nplt.imshow(validation_data[0][incorrect_indices[image]].reshape(50,50), cmap=plt.get_cmap('gray'))\nplt.show()\nprint(\"Prediction: {0}\".format(labels[np.argmax(y_pred[incorrect_indices[image]])]))\n```", "```py\n    import pandas as pd\n    data = pd.read_csv('../../chapter 7/data/movie_reviews.csv', encoding='latin-1')\n    ```", "```py\n    data.text = data.text.str.lower()\n    ```", "```py\n    import re\n    def clean_str(string):\n\n        string = re.sub(r\"https?\\://\\S+\", '', string)\n        string = re.sub(r'\\<a href', ' ', string)\n        string = re.sub(r'&amp;', '', string) \n        string = re.sub(r'<br />', ' ', string)\n        string = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', string)\n        string = re.sub('\\d','', string)\n        string = re.sub(r\"can\\'t\", \"cannot\", string)\n        string = re.sub(r\"it\\'s\", \"it is\", string)\n        return string\n    data.SentimentText = data.SentimentText.apply(lambda x: clean_str(str(x)))\n    ```", "```py\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize,sent_tokenize\n    stop_words = stopwords.words('english') + ['movie', 'film', 'time']\n    stop_words = set(stop_words)\n    remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]\n    data['SentimentText'] = data['SentimentText'].apply(remove_stop_words)\n    ```", "```py\n    from gensim.models import Word2Vec\n    model = Word2Vec(\n            data['SentimentText'].apply(lambda x: x[0]),\n            iter=10,\n            size=16,\n            window=5,\n            min_count=5,\n            workers=10)\n    model.wv.save_word2vec_format('movie_embedding.txt', binary=False)\n    ```", "```py\n    def combine_text(text):    \n        try:\n            return ' '.join(text[0])\n        except:\n            return np.nan\n\n    data.SentimentText = data.SentimentText.apply(lambda x: combine_text(x))\n    data = data.dropna(how='any')\n    ```", "```py\n    from keras.preprocessing.text import Tokenizer\n    tokenizer = Tokenizer(num_words=5000)\n    tokenizer.fit_on_texts(list(data['SentimentText']))\n    sequences = tokenizer.texts_to_sequences(data['SentimentText'])\n    word_index = tokenizer.word_index\n    ```", "```py\n    from keras.preprocessing.sequence import pad_sequences\n    reviews = pad_sequences(sequences, maxlen=100)\n    ```", "```py\n    import numpy as np\n\n    def load_embedding(filename, word_index , num_words, embedding_dim):\n        embeddings_index = {}\n        file = open(filename, encoding=\"utf-8\")\n        for line in file:\n            values = line.split()\n            word = values[0]\n            coef = np.asarray(values[1:])\n            embeddings_index[word] = coef\n        file.close()\n\n        embedding_matrix = np.zeros((num_words, embedding_dim))\n        for word, pos in word_index.items():\n            if pos >= num_words:\n                continue\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[pos] = embedding_vector\n        return embedding_matrix\n\n    embedding_matrix = load_embedding('movie_embedding.txt', word_index, len(word_index), 16)\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    labels = pd.get_dummies(data.Sentiment)\n    X_train, X_test, y_train, y_test = train_test_split(reviews,labels, test_size=0.2, random_state=9)\n    ```", "```py\n    from keras.layers import Input, Dense, Dropout, BatchNormalization, Embedding, Flatten\n    from keras.models import Model\n    inp = Input((100,))\n    embedding_layer = Embedding(len(word_index),\n                        16,\n                        weights=[embedding_matrix],\n                        input_length=100,\n                        trainable=False)(inp)\n    ```", "```py\n    model = Flatten()(embedding_layer)\n    model = BatchNormalization()(model)\n    model = Dropout(0.10)(model)\n    model = Dense(units=1024, activation='relu')(model)\n    model = Dense(units=256, activation='relu')(model)\n    model = Dropout(0.5)(model)\n    predictions = Dense(units=2, activation='softmax')(model)\n    model = Model(inputs = inp, outputs = predictions)\n    ```", "```py\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = ['acc'])\n    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=256)\n    ```", "```py\n    from sklearn.metrics import accuracy_score\n    preds = model.predict(X_test)\n    accuracy_score(np.argmax(preds, 1), np.argmax(y_test.values, 1))\n    ```", "```py\n    y_actual = pd.Series(np.argmax(y_test.values, axis=1), name='Actual')\n    y_pred = pd.Series(np.argmax(preds, axis=1), name='Predicted')\n    pd.crosstab(y_actual, y_pred, margins=True)\n    ```", "```py\n    review_num = 111\n    print(\"Review: \\n\"+tokenizer.sequences_to_texts([X_test[review_num]])[0])\n    sentiment = \"Positive\" if np.argmax(preds[review_num]) else \"Negative\"\n    print(\"\\nPredicted sentiment = \"+ sentiment)\n    sentiment = \"Positive\" if np.argmax(y_test.values[review_num]) else \"Negative\"\n    print(\"\\nActual sentiment = \"+ sentiment)\n    ```", "```py\n    import pandas as pd\n    data = pd.read_csv('tweet-data.csv', encoding='latin-1', header=None)\n    data.columns = ['sentiment', 'id', 'date', 'q', 'user', 'text']\n    ```", "```py\n    data = data.drop(['id', 'date', 'q', 'user'], axis=1)\n    ```", "```py\n    data = data.sample(400000).reset_index(drop=True)\n    ```", "```py\n    data.text = data.text.str.lower()\n    ```", "```py\n    import re\n    def clean_str(string):\n        string = re.sub(r\"https?\\://\\S+\", '', string)\n        string = re.sub(r\"@\\w*\\s\", '', string)\n        string = re.sub(r'\\<a href', ' ', string)\n        string = re.sub(r'&amp;', '', string) \n        string = re.sub(r'<br />', ' ', string)\n        string = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', string)\n        string = re.sub('\\d','', string)\n        return string\n\n    data.text = data.text.apply(lambda x: clean_str(str(x)))\n    ```", "```py\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize,sent_tokenize\n    stop_words = stopwords.words('english')\n    stop_words = set(stop_words)\n    remove_stop_words = lambda r: [[word for word in word_tokenize(sente) if word not in stop_words] for sente in sent_tokenize(r)]\n    data['text'] = data['text'].apply(remove_stop_words)\n\n    def combine_text(text):    \n        try:\n            return ' '.join(text[0])\n        except:\n            return np.nan\n\n    data.text = data.text.apply(lambda x: combine_text(x))\n\n    data = data.dropna(how='any')\n    ```", "```py\n    from keras.preprocessing.text import Tokenizer\n    tokenizer = Tokenizer(num_words=5000)\n    tokenizer.fit_on_texts(list(data['text']))\n    sequences = tokenizer.texts_to_sequences(data['text'])\n    word_index = tokenizer.word_index\n    ```", "```py\n    from keras.preprocessing.sequence import pad_sequences\n    tweets = pad_sequences(sequences, maxlen=50)\n    ```", "```py\n    import numpy as np\n    def load_embedding(filename, word_index , num_words, embedding_dim):\n        embeddings_index = {}\n        file = open(filename, encoding=\"utf-8\")\n        for line in file:\n            values = line.split()\n            word = values[0]\n            coef = np.asarray(values[1:])\n            embeddings_index[word] = coef\n        file.close()\n\n        embedding_matrix = np.zeros((num_words, embedding_dim))\n        for word, pos in word_index.items():\n            if pos >= num_words:\n                continue\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[pos] = embedding_vector\n        return embedding_matrix\n\n    embedding_matrix = load_embedding('../../embedding/glove.twitter.27B.50d.txt', word_index, len(word_index), 50)\n    ```", "```py\n    from sklearn.model_selection import train_test_split  \n    X_train, X_test, y_train, y_test = train_test_split(tweets, pd.get_dummies(data.sentiment), test_size=0.2, random_state=9)\n    ```", "```py\n    from keras.models import Sequential\n    from keras.layers import Dense, Dropout, BatchNormalization, Embedding, Flatten, LSTM\n    embedding_layer = Embedding(len(word_index),\n                               50,\n                               weights=[embedding_matrix],\n                               input_length=50,\n                                trainable=False)\n    model = Sequential()\n    model.add(embedding_layer)\n    model.add(Dropout(0.5))\n    model.add(LSTM(100, dropout=0.2))\n    model.add(Dense(2, activation='softmax'))\n\n    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics = ['acc'])\n    ```", "```py\n    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=256)\n    ```", "```py\n    preds = model.predict(X_test)\n    review_num = 1\n    print(\"Tweet: \\n\"+tokenizer.sequences_to_texts([X_test[review_num]])[0])\n    sentiment = \"Positive\" if np.argmax(preds[review_num]) else \"Negative\"\n    print(\"\\nPredicted sentiment = \"+ sentiment)\n    sentiment = \"Positive\" if np.argmax(y_test.values[review_num]) else \"Negative\"\n    print(\"\\nActual sentiment = \"+ sentiment)\n    ```", "```py\n    from PIL import Image\n    def get_input(file):\n        return Image.open(PATH+file)\n    def get_output(file):\n        class_label = file.split('.')[0]\n        if class_label == 'dog': label_vector = [1,0]\n        elif class_label == 'cat': label_vector = [0,1]\n        return label_vector\n    ```", "```py\n    SIZE = 200\n    CHANNELS = 3\n    ```", "```py\n    def preprocess_input(image):\n\n        # Data preprocessing\n        image = image.resize((SIZE,SIZE))\n        image = np.array(image).reshape(SIZE,SIZE,CHANNELS)\n\n        # Normalize image \n        image = image/255.0\n\n        return image\n    ```", "```py\n    import numpy as np\n    def custom_image_generator(images, batch_size = 128):\n\n        while True:\n            # Randomly select images for the batch\n            batch_images = np.random.choice(images, size = batch_size)\n            batch_input = []\n            batch_output = [] \n\n            # Read image, perform preprocessing and get labels\n            for file in batch_images:\n                # Function that reads and returns the image\n                input_image = get_input(file)\n                # Function that gets the label of the image\n                label = get_output(file)\n                # Function that pre-processes and augments the image\n                image = preprocess_input(input_image)\n\n                batch_input.append(image)\n                batch_output.append(label)\n\n            batch_x = np.array(batch_input)\n            batch_y = np.array(batch_output)\n\n            # Return a tuple of (images,labels) to feed the network\n            yield(batch_x, batch_y)\n    ```", "```py\n    from tqdm import tqdm\n    def get_data(files):\n        data_image = []\n        labels = []\n        for image in tqdm(files):\n            label_vector = get_output(image)\n\n            img = Image.open(PATH + image)\n            img = img.resize((SIZE,SIZE))\n\n            labels.append(label_vector)\n            img = np.asarray(img).reshape(SIZE,SIZE,CHANNELS)\n            img = img/255.0\n            data_image.append(img)\n\n        data_x = np.array(data_image)\n        data_y = np.array(labels)\n\n        return (data_x, data_y)\n    ```", "```py\n    import os\n    files = os.listdir(PATH)\n    random.shuffle(files)\n    train = files[:7000]\n    test = files[7000:]\n    validation_data = get_data(test)\n    7\\.    Plot a few images from the dataset to see whether you loaded the files correctly:\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(20,10))\n    columns = 5\n    for i in range(columns):\n        plt.subplot(5 / columns + 1, columns, i + 1)\n        plt.imshow(validation_data[0][i])\n    ```", "```py\n    from keras.applications.inception_v3 import InceptionV3\n    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(SIZE,SIZE,CHANNELS))\n    ```", "```py\n    from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n    from keras.models import Model\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    predictions = Dense(2, activation='softmax')(x)\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n    ```", "```py\n    model.compile(loss='categorical_crossentropy', \n                  optimizer='adam',\n                  metrics = ['accuracy'])\n    And then perform the training of the model:\n    EPOCHS = 50\n    BATCH_SIZE = 128\n\n    model_details = model.fit_generator(custom_image_generator(train, batch_size = BATCH_SIZE),\n                        steps_per_epoch = len(train) // BATCH_SIZE, \n                        epochs = EPOCHS, \n                        validation_data= validation_data,\n                        verbose=1)\n    ```", "```py\n    score = model.evaluate(validation_data[0], validation_data[1])\n    print(\"Accuracy: {0:.2f}%\".format(score[1]*100))\n    ```", "```py\n    from numpy.random import seed\n    seed(1)\n    from tensorflow import set_random_seed\n    set_random_seed(1)\n    ```", "```py\n    SIZE = 200\n    CHANNELS = 3\n    ```", "```py\n    from PIL import Image\n    def get_input(file):\n        return Image.open(PATH+file)\n    def get_output(file):\n        class_label = file.split('.')[0]\n        if class_label == 'dog': label_vector = [1,0]\n        elif class_label == 'cat': label_vector = [0,1]\n        return label_vector\n    ```", "```py\n    def preprocess_input(image):\n\n        # Data preprocessing\n        image = image.resize((SIZE,SIZE))\n        image = np.array(image).reshape(SIZE,SIZE,CHANNELS)\n\n        # Normalize image \n        image = image/255.0\n\n        return image\n    ```", "```py\n    import numpy as np\n    def custom_image_generator(images, batch_size = 128):\n\n        while True:\n            # Randomly select images for the batch\n            batch_images = np.random.choice(images, size = batch_size)\n            batch_input = []\n            batch_output = [] \n\n            # Read image, perform preprocessing and get labels\n            for file in batch_images:\n                # Function that reads and returns the image\n                input_image = get_input(file)\n                # Function that gets the label of the image\n                label = get_output(file)\n                # Function that pre-processes and augments the image\n                image = preprocess_input(input_image)\n\n                batch_input.append(image)\n                batch_output.append(label)\n\n            batch_x = np.array(batch_input)\n            batch_y = np.array(batch_output)\n\n            # Return a tuple of (images,labels) to feed the network\n            yield(batch_x, batch_y)\n    ```", "```py\n    from tqdm import tqdm\n    def get_data(files):\n        data_image = []\n        labels = []\n        for image in tqdm(files):\n\n            label_vector = get_output(image)\n\n            img = Image.open(PATH + image)\n            img = img.resize((SIZE,SIZE))\n\n            labels.append(label_vector)\n            img = np.asarray(img).reshape(SIZE,SIZE,CHANNELS)\n            img = img/255.0\n            data_image.append(img)\n\n        data_x = np.array(data_image)\n        data_y = np.array(labels)\n\n        return (data_x, data_y)\n    ```", "```py\n    import random\n    random.shuffle(files)\n    train = files[:7000]\n    development = files[7000:8500]\n    test = files[8500:]\n    development_data = get_data(development)\n    test_data = get_data(test)\n    ```", "```py\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(20,10))\n    columns = 5\n    for i in range(columns):\n        plt.subplot(5 / columns + 1, columns, i + 1)\n        plt.imshow(validation_data[0][i])\n    ```", "```py\n    from keras.applications.inception_v3 import InceptionV3\n    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(200,200,3))\n    10\\.  Add the output dense layer according to our problem:\n    from keras.models import Model\n    from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(256, activation='relu')(x)\n    keep_prob = 0.5\n    x = Dropout(rate = 1 - keep_prob)(x)\n    predictions = Dense(2, activation='softmax')(x)\n\n    model = Model(inputs=base_model.input, outputs=predictions)\n    ```", "```py\n    for layer in base_model.layers[:5]:\n        layer.trainable = False\n    ```", "```py\n    model.compile(loss='categorical_crossentropy', \n                  optimizer='adam',\n                  metrics = ['accuracy'])\n    ```", "```py\n    from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n    callbacks = [\n        TensorBoard(log_dir='./logs',\n                    update_freq='epoch'),\n        EarlyStopping(monitor = \"val_loss\",\n                     patience = 18,\n                     verbose = 1,\n                     min_delta = 0.001,\n                     mode = \"min\"),\n        ReduceLROnPlateau(monitor = \"val_loss\",\n                         factor = 0.2,\n                         patience = 8,\n                         verbose = 1,\n                         mode = \"min\"),\n        ModelCheckpoint(monitor = \"val_loss\",\n                       filepath = \"Dogs-vs-Cats-InceptionV3-{epoch:02d}-{val_loss:.2f}.hdf5\", \n                       save_best_only=True,\n                       period = 1)]\n    ```", "```py\n    EPOCHS = 50\n    BATCH_SIZE = 128\n    model_details = model.fit_generator(custom_image_generator(train, batch_size = BATCH_SIZE),\n                       steps_per_epoch = len(train) // BATCH_SIZE, \n                       epochs = EPOCHS, \n                       callbacks = callbacks,\n                       validation_data= development_data,\n                       verbose=1)\n    ```", "```py\n    score = model.evaluate(test_data[0], test_data[1])\n    print(\"Accuracy: {0:.2f}%\".format(score[1]*100))\n    ```"]