- en: Chapter 8. Clustering and Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter demonstrates algorithms that intelligently cluster and categorize
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the k-means clustering algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a hierarchical clustering library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the number of clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering words by their lexemes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the parts of speech of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying key words in a corpus of text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a parts-of-speech tagger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a decision tree classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a k-Nearest Neighbors classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing points using Graphics.EasyPlot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Introduction](img/ch08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Computer algorithms are becoming better and better at analyzing large datasets.
    As their performance enhances, their ability to detect interesting patterns in
    data also improves.
  prefs: []
  type: TYPE_NORMAL
- en: The first few algorithms in this chapter demonstrate how to look at thousands
    of points and identify clusters. A **cluster** is simply a congregation of points
    defined by how closely they lie together. This measure of "closeness" is entirely
    up to us. One of the most popular closeness metrics is the Euclidian distance.
  prefs: []
  type: TYPE_NORMAL
- en: We can understand clusters by looking up at the night sky and pointing at stars
    that appear together. Our ancestors found it convenient to name "clusters" of
    stars, of which we refer to as constellations. We will be finding our own constellations
    in the "sky" of data points.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also focuses on classifying words. We will label words by their
    parts of speech as well as topic.
  prefs: []
  type: TYPE_NORMAL
- en: We will implement our own decision tree to classify practical data. Lastly,
    we will visualize clusters and points using plotting libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the k-means clustering algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The k-means clustering algorithm partitions data into k different groups. These
    k groupings are called clusters, and the location of these clusters are adjusted
    iteratively. We compute the arithmetic mean of all the points in a group to obtain
    a centroid point that we use, replacing the previous cluster location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, after this succinct explanation, the name *k-means clustering* no
    longer sounds completely foreign. One of the best places to learn more about this
    algorithm is on Coursera: [https://class.coursera.org/ml-003/lecture/78](https://class.coursera.org/ml-003/lecture/78).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Create a new file, which we call `Main.hs`, and perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following built-in libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a type synonym for points shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the Euclidian distance function between two points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the assignment step in the k-means algorithm. Each point will be assigned
    to its closest centroid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the relocation step in the k-means algorithm. Each centroid is relocated
    to the arithmetic mean of its corresponding points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the k-means algorithm repeatedly until the centroids no longer move around:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test out the clustering with a couple of hardcoded points. The usual way to
    implement k-means chooses the starting centroids randomly. However, in this recipe,
    we will simply take the first k points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the algorithm converges, the resulting centroids will be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The algorithm repeatedly follows two procedures until the clusters are found.
    The first procedure is to partition the points by assigning each point to its
    closest centroid. The following diagram shows the data assignment step. Initially,
    there are three centroids represented by a star, square, and circle around three
    different points. The first part of the algorithm assigns each point a corresponding
    centroid.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/6331OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next step is to relocate the centroids to the center, or arithmetic mean,
    of their corresponding points. In the following diagram, the arithmetic mean of
    each cluster is computed, and the centroid is shifted to the new center:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/6331OS_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This algorithm continues until the centroids no longer move around. The final
    categorization of each point is the cluster to which each point belongs.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although easy to implement and understand, this algorithm has a couple of limitations.
    The output of the k-means clustering algorithm is sensitive to the initial centroids
    chosen. Also, using the Euclidian distance metric forces the clusters to be described
    only by circular regions. Another limitation of k-means clustering is that the
    initial number of clusters k must be specified by the user. The user should visualize
    the data and use their judgment to determine the number of clusters before beginning
    the algorithm. Moreover, the convergence condition for the algorithm is an issue
    for special edge-cases.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For another type of clustering algorithm, see the next recipe on *Implementing
    hierarchical clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way to cluster data is by first assuming each data item as its own cluster.
    We can then take a step back and merge together two of the nearest clusters. This
    process forms a hierarchy of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Take, for example, an analogy relating to islands and water level. An island
    is nothing more than a mountain tip surrounded by water. Imagine we have islands
    scattered across a sea. If we were to slowly drop the water level of the sea,
    two nearby small islands would merge into a larger island because they are connected
    to the same mountain formation. We can stop the water level from dropping any
    time we have the desired number of larger islands.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a new file, which we name `Main.hs`, insert this code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the built-in functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a type synonym for points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a convenience function to compute the arithmetic mean of list of points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Combine the two clusters that are nearest to each other:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the hierarchical algorithm until there are k clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize so that every point is its own cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the clustering algorithm on some input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The algorithm will output the following centroids:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two main ways to implement hierarchical clustering. The algorithm
    described in this recipe implements the *agglomerative* bottom-up approach. Each
    point is pre-emptively considered to be a cluster, and at each step the two closest
    clusters merge together. However, another approach to implement is top-down in
    a *divisive* approach where every point starts in one massive cluster that iteratively
    splits the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we begin by first assuming that every point is its own cluster.
    Then we take a step back and merge two of the nearest clusters. This step repeats
    until a desired convergence state is reached. In our example, we stop once we
    have exactly two clusters. The following diagram shows the three iterations of
    a hierarchical clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/6331OS_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like most clustering algorithms, the choice of distance metric greatly affects
    the results. In this recipe, we assumed the Euclidean metric, but depending on
    the data, perhaps the distance metric should be the Manhattan distance or cosine
    similarity.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a non-hierarchical clustering algorithm, see the previous recipe on *Implementing
    the k-means clustering algorithm*.
  prefs: []
  type: TYPE_NORMAL
- en: Using a hierarchical clustering library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will group together a list of points using a hierarchical clustering approach.
    We will start by assuming that each point is its own cluster. The two closest
    clusters merge together and the algorithm repeats until the stopping criteria
    is met. In this algorithm, we will use a library to run hierarchical clustering
    until there are a specific number of clusters remaining.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Install the hierarchical clustering package using cabal as follows (documentation
    is available at [http://hackage.haskell.org/package/hierarchical-clustering](http://hackage.haskell.org/package/hierarchical-clustering)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Insert the following code in a new file, which we call `Main.hs`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a Point data type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the Euclidian distance metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print out the clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the clustering algorithm on some points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each of the three clusters are printed out as lists of points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `dendogram` function has the type `Linkage -> [a] -> (a -> a -> Distance)
    -> Dendogram a`. The linkage describes how distance is calculated. In this recipe,
    we use `SingleLinkage` as the first argument, which means that the distance between
    two clusters is the minimum distance between all their elements.
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is the list of points, followed by a distance metric. The
    result of this function is a **dendogram**, otherwise referred to as a hierarchical
    tree diagram. We use the defined `printCluster` function to display the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The other types of linkage in this library include the following mentioned
    along with their description present on Hackage:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SingleLinkage`: This is the minimum distance between two clusters.![There''s
    more…](img/6331OS_08_05.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*"O(n^2) time and O(n) space, using the SLINK algorithm. This algorithm is
    optimal in both space and time and gives the same answer as the naive algorithm
    using a distance matrix."*'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`CompleteLinkage`: This is the maximum distance between two clusters.![There''s
    more…](img/6331OS_08_06.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*"O(n^3) time and O(n^2) space, using the naive algorithm with a distance matrix.
    Use CLINK if you need more performance."*'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: Complete linkage with **CLINK** is the same as the previous linkage type, except
    that it uses a faster but not always optimal algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*"O(n^2) time and O(n) space, using the CLINK algorithm. Note that this algorithm
    doesn''t always give the same answer as the naive algorithm using a distance matrix,
    but it''s much faster."*'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: UPGMA is the average distance between the two clusters.![There's more…](img/6331OS_08_07.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*"O(n^3) time and O(n^2) space, using the naive algorithm with a distance matrix."*'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: And lastly, FakeAverageLinkage is similar to the previous UPGMA linkage but
    weighs both clusters equally in its calculations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*"O(n^3) time and O(n^2) space, using the naive algorithm with a distance matrix."*'
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use our own hierarchical clustering algorithm, see the previous recipe on
    *Implementing hierarchical clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the number of clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, we do not know the number of clusters in a dataset, yet most clustering
    algorithms require this information a priori. One way to find the number of clusters
    is to run the clustering algorithm on all possible number of clusters and compute
    the average variance of the clusters. We can then graph the average variance for
    the number of clusters, and identify the number of clusters by finding the first
    fluctuation of the curve.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Review the k-means recipe titled *Implementing the k-means clustering algorithm*.
    We will be using the `kmeans` and `assign` functions defined in that recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the Statistics package from cabal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create a new file and insert the following code. We name this file `Main.hs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `variance` function and the helper `fromList` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the average of the variance of each cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In `main`, define a list of points. Notice how there appears to be three clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the average of the variance of each set of clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output will be a list of numbers. Once plotted, we can see that the number
    of clusters is three, which occurs at the knee, or just before local maxima, of
    the curve as shown in the following image:![How to do it…](img/6331OS_08_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clustering words by their lexemes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Words that look alike can easily be clustered together. The clustering algorithm
    in the lexeme-clustering package is based on Janicki''s research paper titled,
    "*A Lexeme-Clustering Algorithm for Unsupervised Learning of Morphology*". A direct
    link to this paper can be found through the following URL: [http://skil.informatik.uni-leipzig.de/blog/wp-content/uploads/proceedings/2012/Janicki2012.37.pdf](http://skil.informatik.uni-leipzig.de/blog/wp-content/uploads/proceedings/2012/Janicki2012.37.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An Internet connection is necessary for this recipe to download the package
    from GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to install and use the library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain the lexeme-clustering library from GitHub. If Git is installed, enter
    the following command, otherwise download it from [https://github.com/BinRoot/lexeme-clustering/archive/master.zip](https://github.com/BinRoot/lexeme-clustering/archive/master.zip):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change into the library''s directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an input file with a different word on each line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the lexeme-clustering algorithm on the input file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting output clusters are then displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The related words are clustered together by carefully looking at each word's
    **morpheme**, or smallest meaningful component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a short excerpt from the abstract of the research paper of which this
    algorithm is based:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Initially, a trie of words is built and each node in the trie is considered
    a candidate for stem. The suffixes, with which it occurs, are clustered according
    to mutual information in order to identify inflectional paradigms."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For clustering points of data, see the previous algorithms on *Implementing
    the k-means clustering algorithm*, *Implementing hierarchical clustering*, and
    *Using a hierarchical clustering library*.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying the parts of speech of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will demonstrate how to identify the parts of speech of each word
    in a sentence. We will be using a handy library called **chatter**, which contains
    very useful **Natural Language Processing** (**NLP**) tools. It can be obtained
    from Hackage at [http://hackage.haskell.org/package/chatter](http://hackage.haskell.org/package/chatter).
  prefs: []
  type: TYPE_NORMAL
- en: NLP is the study of human language embedded in a machine. Our naturally spoken
    or written language may seem obvious to us in our day-to-day lives, but producing
    meaning out of words is still a difficult task for computers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Install the NLP library using cabal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a new file, which we name `Main.hs`, enter the following source code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the parts of speech library and the pack function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain the default tagger provided by the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Feed the `tag` function a tagger and a text to see the corresponding parts
    of speech per each word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be an association list of the word to its part of speech:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A parts of speech tagger is trained from a corpus of text. In this example,
    we use the default tagger provided by the library, which trains on the corpus
    in the following directory of the package, `data/models/brown-train.model.gz`.
    This corpus is called the Brown University Standard Corpus of Present-Day American
    English, created in the 1960s.
  prefs: []
  type: TYPE_NORMAL
- en: Definitions of each of the abbreviations such as at, `jjt`, or `nns` can be
    found on [http://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used](http://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used).
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also train our own parts of speech taggers by loading a tagger from
    a file path, `loadTagger :: FilePath -> IO POSTagger`.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To categorize words as something other than parts of speech, see the next recipe
    on *Identifying key words in a corpus of text*.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying key words in a corpus of text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to predict the topic of a paragraph or sentence is by identifying what
    the words mean. While the parts of speech give some insight about each word, they
    still don't reveal the connotation of that word. In this recipe, we will use a
    Haskell library to tag words by topics such as `PERSON`, `CITY`, `DATE`, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An Internet connection is necessary for this recipe to download the `sequor`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install it from cabal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, follow these directions to install it manually:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain the latest version of the sequor library by opening up a browser and
    visiting the following URL: [http://hackage.haskell.org/package/sequor](http://hackage.haskell.org/package/sequor).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the **Downloads** section, download the cabal source package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On Windows, it is easiest to using 7-Zip, an easy-to-use file archiver. Install
    it on your machine by going to [http://www.7-zip.org](http://www.7-zip.org). Then
    using 7-Zip, extract the contents of the tarball.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On other operating systems, run the following command to extract the tarball.
    Replace the numbers in the following command to the correct version numbers of
    your download because a new version (that is, 0.7.3) may be out:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Go into the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure to read the `README` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the library using the following Cabal command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will set up an input file to feed into the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an `input.txt` file using the CoNLL format, which requires one token
    per line, and sentences separated by a blank line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now run the word tagging on the input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is saved in the `output.txt` file. Open up the file and review the
    corresponding tags found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The library uses Collins'' sequence perceptron, based off a paper published
    in 2002 titled "*Discriminative Training Methods for Hidden Markov Models: Theory
    and Experiments with Perceptron Algorithms*". His website ([http://www.cs.columbia.edu/~mcollins/](http://www.cs.columbia.edu/~mcollins/))
    contains comprehensive notes on designing the algorithm used in this recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use an existing parts of speech tagger, see the previous recipe on *Classifying
    the parts of speech of words*. To train our own parts-of-speech tagger, see the
    next recipe on *Training a parts-of-speech tagger*.
  prefs: []
  type: TYPE_NORMAL
- en: Training a parts-of-speech tagger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use a Haskell library, sequor, to train our own parts of speech tagger.
    Then we can use this newly trained model on our own input.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please refer to the *Getting ready* section of the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a new file, which we name `Main.hs`, enter the following source code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `sequor` executable to train the parts of speech tagger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first argument to `sequor` will be `train`, to indicate that we are about
    to train a tagger
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The next argument is the template-file, `data/all.features`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we provide the train-file, `data/train.conll`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The last file path we need to provide is the location of where to save the trained
    model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can specify a learning rate using the `-rate` flag
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The beam size can be modified using the `-beam` flag
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the number of iterations using the `-iter` flag
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use hashing instead of a feature dictionary using the `-hash` flag
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide a path to the held out data using the `-heldout` flag
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of the sequor command in use is as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Test out the trained model on a sample input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first few lines of the output `test.labels` file will be:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The library uses Collins'' sequence perceptron, based off a paper published
    in 2002 titled "*Discriminative Training Methods for Hidden Markov Models: Theory
    and Experiments with Perceptron Algorithms*". The Hackage documentation can be
    found on [http://hackage.haskell.org/package/sequor](http://hackage.haskell.org/package/sequor).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use an existing parts of speech tagger, see the previous recipe on *Classifying
    the parts of speech of words*.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a decision tree classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is a model for classifying data effectively. Each child of a
    node in the tree represents a feature about the item we are classifying. Traversing
    down the tree to leaf nodes represent an item's classification. It's often desirable
    to create the smallest possible tree to represent a large sample of data.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we implement the ID3 decision tree algorithm in Haskell. It
    is one of the easiest to implement and produces useful results. However, ID3 does
    not guarantee an optimal solution, may be computationally inefficient compared
    to other algorithms, and only supports discrete data. While these issues can be
    addressed by a more complicated algorithm such as C4.5, the code in this recipe
    is enough to get up and running with a working decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create a CSV file representing samples of data. The last column should be the
    classification. Name this file `input.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/6331OS_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The weather data is represented with four attributes, namely outlook, temperature,
    humidity, and wind. The last column represents whether it is a good idea to play
    outside.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the CSV helper library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Insert this code into a new file, which we call `Main.hs`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the built-in libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define some type synonyms to better understand what data is being passed around:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the main function to read in the CSV file and handle any errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the file was read successfully, remove any invalid CSV records and construct
    a decision tree out of it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define helper functions to break up the `DataSet` tuple into a list of samples
    or a list of classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the entropy of a list of values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split an attribute by its features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain each of the entropies from splitting up an attribute by its features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the information gain from splitting up an attribute by its features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Determine which attribute contributes the highest information gain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the data structure for a decision tree that we will soon construct:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split up the dataset by the attribute that contributes the highest information
    gain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a helper function to determine if all elements of a list are equal.
    We use this to check if further splitting of the dataset is necessary by checking
    if its classes are identical:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Construct the decision tree from a labeling and a dataset of samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code to see the tree printed out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It can be visualized using the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/6331OS_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ID3 algorithm uses the concept of Shannon's entropy to divide up a set of
    samples by the attribute that maximize the information gain. This process is recursively
    repeated until we're dealing with samples of the same classification or when we
    run out of attributes.
  prefs: []
  type: TYPE_NORMAL
- en: In the field of Information Theory, **Entropy** is the measure of unpredictability.
    A fair coin has higher entropy than a biased coin. Entropy can be calculated by
    taking the expected value of the information content, where information content
    of a random variable X has the form — *ln(P(X))*. When the logarithm in the equation
    is to the base of 2, the units of entropy are called *bits*.
  prefs: []
  type: TYPE_NORMAL
- en: Information Gain is the change in entropy from the prior state to the new state.
    It has the equation *IG = H[1] – H[2]*, where *H[1]* is the original entropy of
    the sample. And *H[2]* is the new entropy given an attribute to split.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a k-Nearest Neighbors classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One simple way to classify an item is to look at only its neighboring data.
    The k-Nearest Neighbors algorithm looks at k items located closest to the item
    in question. The item is then classified as the most common classification of
    its k neighbors. This heuristic has been very promising for a wide variety of
    classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will implement the k-Nearest Neighbors algorithm using a
    **k-d tree** data structure, which is a binary tree with special properties that
    allow efficient representation of points in a k-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we have a web server for our hip new website. Every time someone requests
    a web page, our web server will fetch the file and present the page. However,
    bots can easily hammer a web server with thousands of requests, potentially causing
    a denial of service attack. In this recipe, we will classify whether a web request
    is being made by a human or a bot.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Install the `KdTree`, `CSV`, and `iproute` packages using cabal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Create a CSV file containing the IP addresses and number of seconds since last
    access. The last field of each CSV record should be the classification *Human*
    or *Bot*. We call our file `input.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/6331OS_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After creating a new file called `Main.hs`, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert an IPv4 address string into its 32-bit representation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Parse data from a CSV file to obtain a list of points and their associated
    classifications:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the item in a list that occurs most often:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Classify a test point given the KdTree, the number of nearest neighbors to
    use, and the training set of points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define `main` to read a CSV file and process the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Handle an error if the CSV cannot be read properly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Otherwise create a KdTree from the CSV data and test out a couple of examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the code to see the resulting classifications of the example points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The k-Nearest Neighbor algorithm looks at the k closest points from the training
    set and returns the most frequent classification between these k points. Since
    we are dealing with points, each of the coordinates should be orderable. Fortunately,
    an IP address has a faint sense of hierarchy that we can leverage. We convert
    an IP to its 32-bit number to obtain a useful ordering that we can treat as a
    coordinate of a point in space.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing points using Graphics.EasyPlot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, it's convenient to simply visualize data points before clustering
    or classifying to inspect the data. This recipe will feed a list of points to
    a plotting library to easily see a diagram of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Install easyplot from cabal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a CSV file containing two-dimensional points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a new file, `Main.hs`, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required library to read in CSV data as well the library to plot
    points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a helper function to convert a list of string records into a list of
    doubles. For example, we want to convert `[ "1.0,2.0", "3.5,4.5" ]` into `[ (1.0,
    2.0), (3.5, 4.5) ]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In `main`, parse the CSV file to be used later on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the CSV file is valid, plot the points using the `plot :: TerminalType ->
    a -> IO Bool` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first argument to `plot` tells gnuplot where its output should be displayed.
    For example, we use X11 to output to the X Window System on Linux. Depending on
    the computer, we can choose between different terminal types. The constructors
    for `TerminalType` are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Aqua`: Output on Mac OS X (Aqua Terminal)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Windows`: Output for MS Windows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X11`: Output to the X Window System'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PS FilePath`: Output into a PostScript file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EPS FilePath`: Output into an EPS file path'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PNG FilePath`: Output as Portable Network Graphic into a file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PDF FilePath`: Output as Portable Document Format into a file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SVG FilePath`: Output as Scalable Vector Graphic into a file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GIF FilePath`: Output as Graphics Interchange Format into a file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JPEG FilePath`: Output into a JPEG file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Latex FilePath`: Output as LaTeX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second argument to plot is the graph, which may be a `Graph2D`, or `Graph3D`,
    or a list of these.
  prefs: []
  type: TYPE_NORMAL
