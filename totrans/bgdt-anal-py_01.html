<html><head></head><body>
		<div class="Content" id="_idContainer009">
			<h1 id="_idParaDest-13"><em class="italics"><a id="_idTextAnchor014"/>Chapter 1</em></h1>
		</div>
		<div class="Content" id="_idContainer010">
			<h1 id="_idParaDest-14"><a id="_idTextAnchor015"/>The Python Data Science Stack</h1>
		</div>
		<div class="Content" id="_idContainer011">
			<h2>Learning Objectives</h2>
			<p>We will start our journey by understanding the power of Python to manipulate and visualize data, creating useful analysis.</p>
			<p><a id="_idTextAnchor016"/>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Use all components of the Python data science stack</li>
				<li class="bullets">Manipulate data using pandas DataFrames</li>
				<li class="bullets">Create simple plots using pandas and Matplotlib</li>
			</ul>
			<p>In this chapter, we will learn how to use NumPy, Pandas, Matplotlib, IPython, Jupyter notebook. Later in the chapter, we will explore how the deployment of <strong class="inline">virtualenv</strong>, <strong class="inline">pyenv</strong>, works, soon after that we will plot basic visualization using Matplotlib and Seaborn libraries.</p>
		</div>
		<div class="Content" id="_idContainer030">
			<h2 id="_idParaDest-15"><a id="_idTextAnchor017"/>Introduction</h2>
			<p>The Python data science stack is an informal name for a set of libraries used together to tackle data science problems. There is no consensus on which libraries are part of this list; it usually depends on the data scientist and the problem to be solved. We will present the libraries most commonly used together and explain how they can be used.</p>
			<p>In this chapter, we will learn how to manipulate tabular data with the Python data science stack. The Python data science stack is the first stepping stone to manipulate large datasets, although these libraries are not commonly used for big data themselves. The ideas and the methods that are used here will be very helpful when we get to large datasets.</p>
			<h2 id="_idParaDest-16"><a id="_idTextAnchor018"/>Python Libraries and Packages</h2>
			<p>One of the main reasons Python is a powerful programming language is the libraries and packages that come with it. There are more than 130,000 packages on the <strong class="bold">Python Package Index</strong> (<strong class="bold">PyPI</strong>) and counting! Let's explore some of the libraries and packages that are part of the data science stack.</p>
			<p>The components of the data science stack are as follows:</p>
			<ul>
				<li><strong class="bold">NumPy</strong>: A numerical manipulation package</li>
				<li><strong class="bold">pandas</strong>: A data manipulation and analysis library</li>
				<li><strong class="bold">SciPy library</strong>: A collection of mathematical algorithms built on top of NumPy</li>
				<li><strong class="bold">Matplotlib</strong>: A plotting and graph library</li>
				<li><strong class="bold">IPython</strong>: An interactive Python shell</li>
				<li><strong class="bold">Jupyter notebook</strong>: A web document application for interactive computing</li>
			</ul>
			<p>The combination of these libraries forms a powerful tool set for handling data manipulation and analysis. We will go through each of the libraries, explore their functionalities, and show how they work together. Let's start with the interpreters.</p>
			<h3 id="_idParaDest-17"><a id="_idTextAnchor019"/>IPython: A Powerful Interactive Shell</h3>
			<p>The IPython shell (<a href="https://ipython.org/">https://ipython.org/</a>) is an interactive Python command interpreter that can handle several languages. It allows us to test ideas quickly rather than going through creating files and running them. Most Python installations have a bundled command interpreter, usually called the <strong class="bold">shell</strong>, where you can execute commands iteratively. Although it's handy, this standard Python shell is a bit cumbersome to use. IPython has more features:</p>
			<ul>
				<li>Input history that is available between sessions, so when you restart your shell, the previous commands that you typed can be reused.</li>
				<li>Using <em class="italics">Tab</em> completion for commands and variables, you can type the first letters of a Python command, function, or variable and IPython will autocomplete it.</li>
				<li>Magic commands that extend the functionality of the shell. Magic functions can enhance IPython functionality, such as adding a module that can reload imported modules after they are changed in the disk, without having to restart IPython.</li>
				<li>Syntax highlighting.</li>
			</ul>
			<h3 id="_idParaDest-18"><a id="_idTextAnchor020"/>Exercise 1: Interacting with the Python Shell Using the IPython Commands</h3>
			<p>Getting started with the Python shell is simple. Let's follow these steps to interact with the IPython shell:</p>
			<ol>
				<li>To start the Python shell, type the <strong class="inline">ipython</strong> command in the console:<p class="snippet">&gt; ipython</p><p class="snippet">In [1]:</p><p>The IPython shell is now ready and waiting for further commands. First, let's do a simple exercise to solve a sorting problem with one of the basic sorting methods, called <strong class="bold">straight insertion</strong>.</p></li>
				<li>In the IPython shell, copy-paste the following code:<p class="snippet">import numpy as np</p><p class="snippet">vec = np.random.randint(0, 100, size=5)</p><p class="snippet">print(vec)</p><p>Now, the output for the randomly generated numbers will be similar to the following:</p><p class="snippet">[23, 66, 12, 54, 98, 3]</p></li>
				<li>Use the following logic to print the elements of the <strong class="inline">vec</strong> array in ascending order:<p class="snippet">for j in np.arange(1, vec.size):</p><p class="snippet">    v = vec[j]</p><p class="snippet">    i = j</p><p class="snippet">    while i &gt; 0 and vec[i-1] &gt; v:</p><p class="snippet">        vec[i] = vec[i-1]</p><p class="snippet">        i = i - 1</p><p class="snippet">    vec[i] = v</p><p>Use the <strong class="inline">print(vec)</strong> command to print the output on the console:</p><p class="snippet">[3, 12, 23, 54, 66, 98]</p></li>
				<li>Now modify the code. Instead of creating an array of 5 elements, change its parameters so it creates an array with 20 elements, using the <em class="italics">up</em> arrow to edit the pasted code. After changing the relevant section, use the <em class="italics">down</em> arrow to move to the end of the code and press <em class="italics">Enter</em> to execute it.</li>
			</ol>
			<p>Notice the number on the left, indicating the instruction number. This number always increases. We attributed the value to a variable and executed an operation on that variable, getting the result interactively. We will use IPython in the following sections.</p>
			<h3 id="_idParaDest-19"><a id="_idTextAnchor021"/>The Jupyter Notebook</h3>
			<p>The Jupyter notebook (<a href="https://jupyter.org/">https://jupyter.org/</a>) started as part of IPython but was separated in version 4 and extended, and lives now as a separate project. The notebook concept is based on the extension of the interactive shell model, creating documents that can run code, show documentation, and present results such as graphs and images.</p>
			<p>Jupyter is a web application, so it runs in your web browser directly, without having to install separate software, and enabling it to be used across the internet. Jupyter can use IPython as a kernel for running Python, but it has support for more than 40 kernels that are contributed by the developer community.</p>
			<h4>Note</h4>
			<p class="callout">A kernel, in Jupyter parlance, is a computation engine that runs the code that is typed into a code cell in a notebook. For example, the IPython kernel executes Python code in a notebook. There are kernels for other languages, such as R and Julia.</p>
			<p>It has become a de facto platform for performing operations related to data science from beginners to power users, and from small to large enterprises, and even academia. Its popularity has increased tremendously in the last few years. A Jupyter notebook contains both the input and the output of the code you run on it. It allows text, images, mathematical formulas, and more, and is an excellent platform for developing code and communicating results. Because of its web format, notebooks can be shared over the internet. It also supports the Markdown markup language and renders Markdown text as rich text, with formatting and other features supported.</p>
			<p>As we've seen before, each notebook has a kernel. This kernel is the interpreter that will execute the code in the cells. The basic unit of a notebook is called a <strong class="bold">cell</strong>. A cell is a container for either code or text. We have two main types of cells:</p>
			<ul>
				<li><strong class="bold">Code cell</strong></li>
				<li><strong class="bold">Markdown cell</strong></li>
			</ul>
			<p>A code cell accepts code to be executed in the kernel, displaying the output just below it. A Markdown cell accepts Markdown and will parse the text in Markdown to formatted text when the cell is executed.</p>
			<p>Let's run the following exercise to get hands-on experience in the Jupyter notebook.</p>
			<p>The fundamental component of a notebook is a cell, which can accept code or text depending on the mode that is selected.</p>
			<p>Let's start a notebook to demonstrate how to work with cells, which have two states:</p>
			<ul>
				<li>Edit mode</li>
				<li>Run mode</li>
			</ul>
			<p>When in edit mode, the contents of the cell can be edited, while in run mode, the cell is ready to be executed, either by the kernel or by being parsed to formatted text. </p>
			<p>You can add a new cell by using the <strong class="bold">Insert</strong> menu option or using a keyboard shortcut, <em class="italics">Ctrl</em> + <em class="italics">B</em>. Cells can be converted between Markdown mode and code mode again using the menu or the <em class="italics">Y</em> shortcut key for a code cell and <em class="italics">M</em> for a Markdown cell.</p>
			<p>To execute a cell, click on the <strong class="bold">Run</strong> option or use the <em class="italics">Ctrl</em> + <em class="italics">Enter</em> shortcut.</p>
			<h3 id="_idParaDest-20"><a id="_idTextAnchor022"/>Exercise 2: Getting Started with the Jupyter Notebook</h3>
			<p>Let's execute the following steps to demonstrate how to start to execute simple programs in a Jupyter notebook.</p>
			<p>Working with a Jupyter notebook for the first time can be a little confusing, but let's try to explore its interface and functionality. The reference notebook for this exercise is provided on GitHub.</p>
			<p>Now, start a Jupyter notebook server and work on it by following these steps:</p>
			<ol>
				<li value="1">To start the Jupyter notebook server, run the following command on the console:<p class="snippet">&gt; jupyter notebook</p></li>
				<li>After successfully running or installing Jupyter, open a browser window and navigate to <a href="http://localhost:8888">http://localhost:8888</a> to access the notebook.</li>
				<li>You should see a notebook similar to the one shown in the following screenshot:<div class="IMG---Figure" id="_idContainer012"><img alt="Figure 1.1: Jupyter notebook" src="image/C12913_01_01.jpg"/></div><h6>Figure 1.1: Jupyter notebook</h6></li>
				<li>After that, from the top-right corner, click on <strong class="bold">New</strong> and select <strong class="bold">Python 3</strong> from the list.</li>
				<li>A new notebook should appear. The first input cell that appears is a <strong class="bold">Code</strong> cell. The default cell type is <strong class="bold">Code</strong>. You can change it via the <strong class="bold">Cell Type</strong> option located under the <strong class="bold">Cell</strong> menu:<div class="IMG---Figure" id="_idContainer013"><img alt="Figure 1.2: Options in the cell menu of Jupyter" src="image/C12913_01_02.jpg"/></div><h6>Figure 1.2: Options in the cell menu of Jupyter</h6></li>
				<li>Now, in the newly generated <strong class="bold">Code</strong> cell, add the following arithmetic function in the first cell:<p class="snippet">In []: x = 2</p><p class="snippet">       print(x*2)</p><p class="snippet">Out []: 4</p></li>
				<li>Now, add a function that returns the arithmetic mean of two numbers, and then execute the cell:<p class="snippet">In []: def mean(a,b):</p><p class="snippet">       return (a+b)/2</p></li>
				<li>Let's now use the <strong class="inline">mean</strong> function and call the function with two values, 10 and 20. Execute this cell. What happens? The function is called, and the answer is printed:<p class="snippet">In []: mean(10,20)</p><p class="snippet">Out[]: 15.0</p></li>
				<li>We need to document this function. Now, create a new Markdown cell and edit the text in the Markdown cell, documenting what the function does:<div class="IMG---Figure" id="_idContainer014"><img alt="Figure 1.3: Markdown in Jupyter" src="image/C12913_01_03.jpg"/></div><h6>Figure 1.3: Markdown in Jupyter</h6></li>
				<li>Then, include an image from the web. The idea is that the notebook is a document that should register all parts of analysis, so sometimes we need to include a diagram or graph from other sources to explain a point.</li>
				<li>Now, finally, include the mathematical expression in <strong class="bold">LaTex</strong> in the same Markdown cell:</li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer015">
					<img alt="Figure 1.4: LaTex expression in Jupyter Markdown" src="image/C12913_01_04.jpg"/>
				</div>
			</div>
			<h6>Figure 1.4: LaTex expression in Jupyter Markdown</h6>
			<p>As we will see in the rest of the book, the notebook is the cornerstone of our analysis process. The steps that we just followed illustrate the use of different kinds of cells and the different ways we can document our analysis.</p>
			<h3 id="_idParaDest-21">IPyt<a id="_idTextAnchor023"/>hon or Jupyter?</h3>
			<p>Both IPython and Jupyter have a place in the analysis workflow. Usually, the IPython shell is used for quick interaction and more data-heavy work, such as debugging scripts or running asynchronous tasks. Jupyter notebooks, on the other hand, are great for presenting results and generating visual narratives with code, text, and figures. Most of the examples that we will show can be executed in both, except the graphical parts. </p>
			<p>IPython is capable of showing graphs, but usually, the inclusion of graphs is more natural in a notebook. We will usually use Jupyter notebooks in this book, but the instructions should also be applicable to IPython notebooks.</p>
			<h3 id="_idParaDest-22">Acti<a id="_idTextAnchor024"/>vity 1: IPython and Jupyter</h3>
			<p>Let's demonstrate common Python development in IPython and Jupyter. We will import NumPy, define a function, and iterate the results:</p>
			<ol>
				<li value="1">Open the <strong class="inline">python_script_student.py</strong> file in a text editor, copy the contents to a notebook in IPython, and execute the operations.</li>
				<li>Copy and paste the code from the Python script into a Jupyter notebook.</li>
				<li>Now, update the values of the <strong class="inline">x</strong> and <strong class="inline">c</strong> constants. Then, change the definition of the function.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 200.</p></li>
			</ol>
			<p>We now know how to handle functions and change function definitions on the fly in the notebook. This is very helpful when we are exploring and discovering the right approach for some code or an analysis. The iterative approach allowed by the notebook can be very productive in prototyping and faster than writing code to a script and executing that script, checking the results, and changing the script again.</p>
			<h3 id="_idParaDest-23">NumP<a id="_idTextAnchor025"/>y</h3>
			<p>NumPy (<a href="http://www.numpy.org">http://www.numpy.org</a>) is a package that came from the Python scientific computing community. NumPy is great for manipulating multidimensional arrays and applying linear algebra functions to those arrays. It also has tools to integrate C, C++, and Fortran code, increasing its performance capabilities even more. There are a large number of Python packages that use NumPy as their numerical engine, including pandas and scikit-learn. These packages are part of SciPy, an ecosystem for packages used in mathematics, science, and engineering.</p>
			<p>To import the package, open the Jupyter notebook used in the previous activity and type the following command:</p>
			<p class="snippet">import numpy as np</p>
			<p>The basic NumPy object is <strong class="inline">ndarray</strong>, a homogeneous multidimensional array, usually composed of numbers, but it can hold generic data. NumPy also includes several functions for array manipulation, linear algebra, matrix operations, statistics, and other areas. One of the ways that NumPy shines is in scientific computing, where matrix and linear algebra operations are common. Another strength of NumPy is its tools that integrate with C++ and FORTRAN code. NumPy is also heavily used by other Python libraries, such as pandas.</p>
			<h3 id="_idParaDest-24">SciP<a id="_idTextAnchor026"/>y</h3>
			<p><strong class="bold">SciPy</strong> (<a href="https://www.scipy.org">https://www.scipy.org</a>) is an ecosystem of libraries for mathematics, science, and engineering. NumPy, SciPy, scikit-learn, and others are part of this ecosystem. It is also the name of a library that includes the core functionality for lots of scientific areas.</p>
			<h3 id="_idParaDest-25">Matp<a id="_idTextAnchor027"/>lotlib</h3>
			<p><strong class="bold">Matplotlib</strong> (<a href="https://matplotlib.org">https://matplotlib.org</a>) is a plotting library for Python for 2D graphs. It's capable of generating figures in a variety of hard-copy formats for interactive use. It can use native Python data types, NumPy arrays, and pandas DataFrames as data sources. Matplotlib supports several backend—the part that supports the output generation in interactive or file format. This allows Matplotlib to be multiplatform. This flexibility also allows Matplotlib to be extended with toolkits that generate other kinds of plots, such as geographical plots and 3D plots.</p>
			<p>The interactive interface for Matplotlib was inspired by the MATLAB plotting interface. It can be accessed via the <strong class="inline">matplotlib.pyplot</strong> module. The file output can write files directly to disk. Matplotlib can be used in scripts, in IPython or Jupyter environments, in web servers, and in other platforms. Matplotlib is sometimes considered low level because several lines of code are needed to generate a plot with more details. One of the tools that we will look at in this book that plots graphs, which are common in analysis, is the <strong class="bold">Seaborn</strong> library, one of the extensions that we mentioned before.</p>
			<p>To import the interactive interface, use the following command in the Jupyter notebook:</p>
			<p class="snippet">import matplotlib.pyplot as plt</p>
			<p>To have access to the plotting capabilities. We will show how to use Matplotlib in more detail in the next chapter.</p>
			<h3 id="_idParaDest-26"><a id="_idTextAnchor028"/>Pandas</h3>
			<p><strong class="bold">Pandas</strong> (<a href="https://pandas.pydata.org">https://pandas.pydata.org</a>) is a data manipulation and analysis library that's widely used in the data science community. Pandas is designed to work with tabular or labeled data, similar to SQL tables and Excel files.</p>
			<p>We will explore the operations that are possible with pandas in more detail. For now, it's important to learn about the two basic pandas data structures: the <strong class="bold">series</strong>, a unidimensional data structure; and the data science workhorse, the bi-dimensional <strong class="bold">DataFrame</strong>, a two-dimensional data structure that supports indexes. </p>
			<p>Data in DataFrames and series can be ordered or unordered, homogeneous, or heterogeneous. Other great pandas features are the ability to easily add or remove rows and columns, and operations that SQL users are more familiar with, such as GroupBy, joins, subsetting, and indexing columns. Pandas is also great at handling time series data, with easy and flexible datetime indexing and selection.</p>
			<p>Let's import pandas into the Jupyter notebook from the previous activity with the following command:</p>
			<p class="snippet">import pandas as pd</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor029"/>Using Pandas</h2>
			<p>We will demonstrate the main operations for data manipulation using pandas. This approach is used as a standard for other data manipulation tools, such as Spark, so it's helpful to learn how to manipulate data using pandas. It's common in a big data pipeline to convert part of the data or a data sample to a pandas DataFrame to apply a more complex transformation, to visualize the data, or to use more refined machine learning models with the <strong class="inline">scikit-learn</strong> library. Pandas is also fast for in-memory, single-machine operations. Although there is a memory overhead between the data size and the pandas DataFrame, it can be used to manipulate large volumes of data quickly.</p>
			<p>We will learn how to apply the basic operations:</p>
			<ul>
				<li>Read data into a DataFrame</li>
				<li>Selection and filtering</li>
				<li>Apply a function to data</li>
				<li>GroupBy and aggregation</li>
				<li>Visualize data from DataFrames</li>
			</ul>
			<p>Let's start by reading data into a pandas DataFrame.</p>
			<h3 id="_idParaDest-28">Read<a id="_idTextAnchor030"/>ing Data</h3>
			<p>Pandas accepts several data formats and ways to ingest data. Let's start with the more common way, reading a CSV file. Pandas has a function called <strong class="inline">read_csv</strong>, which can be used to read a CSV file, either locally or from a URL. Let's read some data from the Socrata Open Data initiative, a RadNet Laboratory Analysis from the U.S. Environmental Protection Agency (EPA), which lists the radioactive content collected by the EPA.</p>
			<h3 id="_idParaDest-29"><a id="_idTextAnchor031"/>Exercise 3: Reading Data with Pandas</h3>
			<p>How can an analyst start data analysis without data? We need to learn how to get data from an internet source into our notebook so that we can start our analysis. Let's demonstrate how pandas can read CSV data from an internet source so we can analyze it:</p>
			<ol>
				<li value="1">Import pandas library.<p class="snippet">import pandas as pd</p></li>
				<li>Read the <strong class="bold">Automobile mileage dataset</strong>, available at this URL: <a href="https://github.com/TrainingByPackt/Big-Data-Analysis-with-Python/blob/master/Lesson01/imports-85.data">https://github.com/TrainingByPackt/Big-Data-Analysis-with-Python/blob/master/Lesson01/imports-85.data</a>. Convert it to csv.</li>
				<li>Use the column names to name the data, with the parameter <strong class="inline">names</strong> on the <strong class="inline">read_csv</strong> function.<p class="snippet">Sample code : df = pd.read_csv("/path/to/imports-85.csv", names = columns)</p></li>
				<li>Use the function <strong class="inline">read_csv</strong> from pandas and show the first rows calling the method <strong class="inline">head</strong> on the DataFrame:<p class="snippet">import pandas as pd</p><p class="snippet">df = pd.read_csv("imports-85.csv")</p><p class="snippet">df.head()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer016">
					<img alt="Figure 1.5: Entries of the Automobile mileage dataset" src="image/C12913_01_05.jpg"/>
				</div>
			</div>
			<h6>Figure 1.5: Entries of the Automobile mileage dataset</h6>
			<p>Pandas can read more formats:</p>
			<ul>
				<li>JSON</li>
				<li>Excel</li>
				<li>HTML</li>
				<li>HDF5</li>
				<li>Parquet (with PyArrow)</li>
				<li>SQL databases</li>
				<li>Google Big Query</li>
			</ul>
			<p>Try to read other formats from pandas, such as Excel sheets.</p>
			<h3 id="_idParaDest-30"><a id="_idTextAnchor032"/>Data Manipulation</h3>
			<p>By data manipulation we mean any selection, transformation, or aggregation that is applied over the data. Data manipulation can be done for several reasons:</p>
			<ul>
				<li>To select a subset of data for analysis</li>
				<li>To clean a dataset, removing invalid, erroneous, or missing values</li>
				<li>To group data into meaningful sets and apply aggregation functions</li>
			</ul>
			<p>Pandas was designed to let the analyst do these transformations in an efficient way.</p>
			<h3 id="_idParaDest-31"><strong class="keyword"><a id="_idTextAnchor033"/>Selection and Filtering</strong></h3>
			<p>Pandas DataFrames can be sliced similarly to Python lists. For example, to select a subset of the first 10 rows of the DataFrame, we can use the <strong class="inline">[0:10]</strong> notation. We can see in the following screenshot that the selection of the interval <strong class="inline">[1:3]</strong> that in the NumPy representation selects the rows <strong class="inline">1</strong> and <strong class="inline">2</strong>.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer017">
					<img alt="" src="image/C12913_01_06.jpg"/>
				</div>
			</div>
			<h6>Figure 1.6: Selection in a pandas DataFrame</h6>
			<p>In the following section, we'll explore the selection and filtering operation in depth.</p>
			<h3 id="_idParaDest-32"><strong class="keyword"><a id="_idTextAnchor034"/>Selecting Rows Using Slicing</strong></h3>
			<p>When performing data analysis, we usually want to see how data behaves differently under certain conditions, such as comparing a few columns, selecting only a few columns to help read the data, or even plotting. We may want to check specific values, such as the behavior of the rest of the data when one column has a specific value.</p>
			<p>After selecting with slicing, we can use other methods, such as the <strong class="inline">head</strong> method, to select only a few rows from the beginning of the DataFrame. But how can we select some columns in a DataFrame?</p>
			<p>To select a column, just use the name of the column. We will use the notebook. Let's select the <strong class="inline">cylinders</strong> column in our DataFrame using the following command:</p>
			<p class="snippet">df['State']</p>
			<p>The output is as follows:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer018">
					<img alt="Figure 1.7: DataFrame showing the state" src="image/C12913_01_07.jpg"/>
				</div>
			</div>
			<h6>Figure 1.7: DataFrame showing the state</h6>
			<p>Another form of selection that can be done is filtering by a specific value in a column. For example, let's say that we want to select all rows that have the <strong class="inline">State</strong> column with the <strong class="inline">MN</strong> value. How can we do that? Try to use the Python equality operator and the DataFrame selection operation:</p>
			<p class="snippet">df[df.State == "MN"]</p>
			<div>
				<div class="IMG---Figure" id="_idContainer019">
					<img alt="Figure 1.8: DataFrame showing the MN states " src="image/C12913_01_08.jpg"/>
				</div>
			</div>
			<h6>Figure 1.8: DataFrame showing the MN states </h6>
			<p>More than one filter can be applied at the same time. The <strong class="inline">OR</strong>, <strong class="inline">NOT</strong>, and <strong class="inline">AND</strong> logic operations can be used when combining more than one filter. For example, to select all rows that have <strong class="inline">State</strong> equal to <strong class="inline">AK</strong> and a <strong class="inline">Location</strong> of <strong class="inline">Nome</strong>, use the <strong class="inline">&amp;</strong> operator:</p>
			<p class="snippet">df[(df.State == "AK") &amp; (df.Location == "Nome")]</p>
			<div>
				<div class="IMG---Figure" id="_idContainer020">
					<img alt="Figure 1.9: DataFrame showing State AK and Location Nome" src="image/C12913_01_09.jpg"/>
				</div>
			</div>
			<h6>Figure 1.9: DataFrame showing State AK and Location Nome</h6>
			<p>Another powerful method is <strong class="inline">.loc</strong>. This method has two arguments, the row selection and the column selection, enabling fine-grained selection. An important caveat at this point is that, depending on the applied operation, the return type can be either a DataFrame or a series. The <strong class="inline">.loc</strong> method returns a series, as selecting only a column. This is expected, because each DataFrame column is a series. This is also important when more than one column should be selected. To do that, use two brackets instead of one, and use as many columns as you want to select.</p>
			<h3 id="_idParaDest-33">Exercise <a id="_idTextAnchor035"/>4: Data Selection and the .loc Method</h3>
			<p>As we saw before, selecting data, separating variables, and viewing columns and rows of interest is fundamental to the analysis process. Let's say we want to analyze the radiation from <strong class="inline">I-131</strong> in the state of <strong class="inline">Minnesota</strong>:</p>
			<ol>
				<li value="1">Import the NumPy and pandas libraries using the following command in the Jupyter notebook:<p class="snippet">import numpy as np</p><p class="snippet">import pandas as pd</p></li>
				<li>Read the RadNet dataset from the EPA, available from the Socrata project at <a href="https://github.com/TrainingByPackt/Big-Data-Analysis-with-Python/blob/master/Lesson01/RadNet_Laboratory_Analysis.csv">https://github.com/TrainingByPackt/Big-Data-Analysis-with-Python/blob/master/Lesson01/RadNet_Laboratory_Analysis.csv</a>:<p class="snippet">url = "https://opendata.socrata.com/api/views/cf4r-dfwe/rows.csv?accessType=DOWNLOAD"</p><p class="snippet">df = pd.read_csv(url)</p></li>
				<li>Start by selecting a column using the <strong class="inline">['&lt;name of the column&gt;']</strong> notation. Use the <strong class="inline">State</strong> column:<p class="snippet">df['State'].head()</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer021"><img alt="Figure 1.10: Data in the State column" src="image/C12913_01_10.jpg"/></div><h6>Figure 1.10: Data in the State column</h6></li>
				<li>Now filter the selected values in a column using the <strong class="inline">MN</strong> column name:<p class="snippet">df[df.State == "MN"]</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer022"><img alt="Figure 1.11: DataFrame showing States with MN" src="image/C12913_01_11.jpg"/></div><h6>Figure 1.11: DataFrame showing States with MN</h6></li>
				<li>Select more than one column per condition. Add the <strong class="inline">Sample Type</strong> column for filtering:<p class="snippet">df[(df.State == 'CA') &amp; (df['Sample Type'] == 'Drinking Water')]</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer023"><img alt="" src="image/C12913_01_12.jpg"/></div><h6>Figure 1.12: DataFrame with State CA and Sample type as Drinking water</h6></li>
				<li>Next, select the <strong class="inline">MN</strong> state and the isotope <strong class="inline">I-131</strong>:<p class="snippet">df[(df.State == "MN") ]["I-131"]</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer024"><img alt="Figure 1.13: Data showing the DataFrame with State Minnesota and Isotope I-131" src="image/C12913_01_13.jpg"/></div><h6>Figure 1.13: Data showing the DataFrame with State Minnesota and Isotope I-131</h6><p>The radiation in the state of Minnesota with ID <strong class="inline">555</strong> is the highest.</p></li>
				<li>We can do the same more easily with the <strong class="inline">.loc</strong> method, filtering by state and selecting a column on the same <strong class="inline">.loc</strong> call:<p class="snippet">df_rad.loc[df_rad.State == "MN", "I-131"]</p><p class="snippet">df[['I-132']].head()</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer025">
					<img alt="Figure 1.14: DataFrame with I-132" src="image/C12913_01_14.jpg"/>
				</div>
			</div>
			<h6>Figure 1.14: DataFrame with I-132</h6>
			<p>In this exercise, we learned how to filter and select values, either on columns or rows, using the NumPy slice notation or the <strong class="inline">.loc</strong> method. This can help when analyzing data, as we can check and manipulate only a subset of the data instead having to handle the entire dataset at the same time.</p>
			<h4>Note</h4>
			<p class="callout">The result of the <strong class="inline">.loc</strong> filter is a <strong class="bold">series</strong> and not a DataFrame. This depends on the operation and selection done on the DataFrame and not is caused only by <strong class="inline">.loc</strong>. Because the DataFrame can be understood as a 2D combination of series, the selection of one column will return a series. To make a selection and still return a DataFrame, use double brackets:</p>
			<p class="callout"><strong class="inline">df[['I-132']].head()</strong></p>
			<h3 id="_idParaDest-34"><strong class="keyword"><a id="_idTextAnchor036"/>Applying a Function to a Column</strong></h3>
			<p>Data is never clean. There are always cleaning tasks that have to be done before a dataset can be analyzed. One of the most common tasks in data cleaning is applying a function to a column, changing a value to a more adequate one. In our example dataset, when no concentration was measured, the <strong class="inline">non-detect</strong> value was inserted. As this column is a numerical one, analyzing it could become complicated. We can apply a transformation over a column, changing from <strong class="inline">non-detect</strong> to <strong class="inline">numpy.NaN</strong>, which makes manipulating numerical values more easy, filling with other values such as the mean, and so on.</p>
			<p>To apply a function to more than one column, use the <strong class="inline">applymap</strong> method, with the same logic as the <strong class="inline">apply</strong> method. For example, another common operation is removing spaces from strings. Again, we can use the <strong class="inline">apply</strong> and <strong class="inline">applymap</strong> functions to fix the data. We can also apply a function to rows instead of to columns, using the axis parameter (<strong class="inline">0</strong> for rows, <strong class="inline">1</strong> for columns).</p>
			<h3 id="_idParaDest-35"><a id="_idTextAnchor037"/>Activity 2: Working with Data Problems</h3>
			<p>Before starting an analysis, we need to check for data problems, and when we find them (which is very common!), we have to correct the issues by transforming the DataFrame. One way to do that, for instance, is by applying a function to a column, or to the entire DataFrame. It's common for some numbers in a DataFrame, when it's read, to not be converted correctly to floating-point numbers. Let's fix this issue by applying functions:</p>
			<ol>
				<li value="1">Import <strong class="inline">pandas</strong> and <strong class="inline">numpy</strong> library.</li>
				<li>Read the RadNet dataset from the U.S. Environmental Protection Agency.</li>
				<li>Create a list with numeric columns for radionuclides in the RadNet dataset.</li>
				<li>Use the <strong class="inline">apply</strong> method on one column, with a lambda function that compares the <strong class="inline">Non-detect</strong> string.</li>
				<li>Replace the text values by <strong class="inline">NaN</strong> in one column with <strong class="inline">np.nan</strong>.</li>
				<li>Use the same lambda comparison and use the <strong class="inline">applymap</strong> method on several columns at the same time, using the list created in the first step.</li>
				<li>Create a list of the remaining columns that are not numeric.</li>
				<li>Remove any spaces from these columns.</li>
				<li>Using the selection and filtering methods, verify that the names of the string columns don't have any more spaces.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 200.</p></li>
			</ol>
			<p>The spaces in column names can be extraneous and can make selection and filtering more complicated. Fixing the numeric types helps when statistics must be computed using the data. If there is a value that is not valid for a numeric column, such as a string in a numeric column, the statistical operations will not work. This scenario happens, for example, when there is an error in the data input process, where the operator types the information by hand and makes mistakes, or the storage file was converted from one format to another, leaving incorrect values in the columns.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor038"/>Data Type Conversion</h2>
			<p>Another common operation in data cleanup is getting the data types right. This helps with detecting invalid values and applying the right operations. The main types stored in pandas are as follows:</p>
			<ul>
				<li><strong class="inline">float</strong> (<strong class="inline">float64</strong>, <strong class="inline">float32</strong>)</li>
				<li><strong class="inline">integer</strong> (<strong class="inline">int64</strong>, <strong class="inline">int32</strong>)</li>
				<li><strong class="inline">datetime</strong> (<strong class="inline">datetime64[ns, tz]</strong>)</li>
				<li><strong class="inline">timedelta</strong> (<strong class="inline">timedelta[ns]</strong>)</li>
				<li><strong class="inline">bool</strong></li>
				<li><strong class="inline">object</strong></li>
				<li><strong class="inline">category</strong></li>
			</ul>
			<p>Types can be set on, read, or inferred by pandas. Usually, if pandas cannot detect what data type the column is, it assumes that is object that stores the data as strings.</p>
			<p>To transform the data into the right data types, we can use conversion functions such as <strong class="inline">to_datetime</strong>, <strong class="inline">to_numeric</strong>, or <strong class="inline">astype</strong>. Category types, columns that can only assume a limited number of options, are encoded as the <strong class="inline">category</strong> type.</p>
			<h3 id="_idParaDest-37"><a id="_idTextAnchor039"/>Exercise 5: Exploring Data Types</h3>
			<p>Transform the data types in our example DataFrame to the correct types with the pandas <strong class="inline">astype</strong> function. Let's use the sample dataset from <a href="https://opendata.socrata.com/">https://opendata.socrata.com/</a>:</p>
			<ol>
				<li value="1">Import the required libraries, as illustrated here:<p class="snippet">import numpy as np</p><p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import seaborn as sns</p></li>
				<li>Read the data from the dataset as follows:<p class="snippet">url = "https://opendata.socrata.com/api/views/cf4r-dfwe/rows.csv?accessType=DOWNLOAD"</p><p class="snippet">df = pd.read_csv(url)</p></li>
				<li>Check the current data types using the <strong class="inline">dtypes</strong> function on the DataFrame:<p class="snippet">df.dtypes</p></li>
				<li>Use the <strong class="inline">to_datetime</strong> method to convert the dates from string format to <strong class="inline">datetime</strong> format:<p class="snippet">df['Date Posted'] = pd.to_datetime(df['Date Posted'])</p><p class="snippet">df['Date Collected'] = pd.to_datetime(df['Date Collected'])</p><p class="snippet">columns = df.columns</p><p class="snippet">id_cols = ['State', 'Location', "Date Posted", 'Date Collected', 'Sample Type', 'Unit']</p><p class="snippet">columns = list(set(columns) - set(id_cols))</p><p class="snippet">columns</p><p>The output is as follows:</p><p class="snippet">['Co-60',</p><p class="snippet"> 'Cs-136',</p><p class="snippet"> 'I-131',</p><p class="snippet"> 'Te-129',</p><p class="snippet"> 'Ba-140',</p><p class="snippet"> 'Cs-137',</p><p class="snippet"> 'Cs-134',</p><p class="snippet"> 'I-133',</p><p class="snippet"> 'I-132',</p><p class="snippet"> 'Te-132',</p><p class="snippet"> 'Te-129m']</p></li>
				<li>Use Lambda function:<p class="snippet">df['Cs-134'] = df['Cs-134'].apply(lambda x: np.nan if x == "Non-detect" else x)</p><p class="snippet">df.loc[:, columns] = df.loc[:, columns].applymap(lambda x: np.nan if x == 'Non-detect' else x)</p><p class="snippet">df.loc[:, columns] = df.loc[:, columns].applymap(lambda x: np.nan if x == 'ND' else x)</p></li>
				<li>Apply the <strong class="inline">to_numeric</strong> method to the list of numeric columns created in the previous activity to convert the columns to the correct numeric types:<p class="snippet">for col in columns:</p><p class="snippet">    df[col] = pd.to_numeric(df[col])</p></li>
				<li>Check the types of the columns again. They should be <strong class="inline">float64</strong> for the numeric columns and <strong class="inline">datetime64[ns]</strong> for the date columns:<p class="snippet">df.dypes</p></li>
				<li>Use the <strong class="inline">astype</strong> method to transform the columns that are not numeric to the <strong class="inline">category</strong> type:<p class="snippet">df['State'] = df['State'].astype('category')</p><p class="snippet">df['Location'] = df['Location'].astype('category')</p><p class="snippet">df['Unit'] = df['Unit'].astype('category')</p><p class="snippet">df['Sample Type'] = df['Sample Type'].astype('category')</p></li>
				<li>Check the types with the <strong class="inline">dtype</strong> function for the last time:<p class="snippet">df.dtypes</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer026">
					<img alt="Figure 1.15: DataFrame and its types" src="image/C12913_01_15.jpg"/>
				</div>
			</div>
			<h6>Figure 1.15: DataFrame and its types</h6>
			<p>Now our dataset looks fine, with all values correctly cast to the right types. But correcting the data is only part of the story. We want, as analysts, to understand the data from different perspectives. For example, we may want to know which state has the most contamination, or the radionuclide that is the least prevalent across cities. We may ask about the number of valid measurements present in the dataset. All these questions have in common transformations that involve grouping data together and aggregating several values. With pandas, this is accomplished with <strong class="inline">GroupBy</strong>. Let's see how we can use it by key and aggregate the data.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor040"/>Aggregation and Grouping</h2>
			<p>After getting the dataset, our analyst may have to answer a few questions. For example, we know the value of the radionuclide concentration per city, but an analyst may be asked to answer: which state, on average, has the highest radionuclide concentration?</p>
			<p>To answer the questions posed, we need to group the data somehow and calculate an aggregation on it. But before we go into grouping data, we have to prepare the dataset so that we can manipulate it in an efficient manner. Getting the right types in a pandas DataFrame can be a huge boost for performance and can be leveraged to enforce data consistency— it makes sure that numeric data really is numeric and allows us to execute operations that we want to use to get the answers.</p>
			<p><strong class="inline">GroupBy</strong> allows us to get a more general view of a feature, arranging data given a <strong class="inline">GroupBy</strong> key and an aggregation operation. In pandas, this operation is done with the <strong class="inline">GroupBy</strong> method, over a selected column, such as State. Note the aggregation operation after the <strong class="inline">GroupBy</strong> method. Some examples of the operations that can be applied are as follows:</p>
			<ul>
				<li><strong class="inline">mean</strong></li>
				<li><strong class="inline">median</strong></li>
				<li><strong class="inline">std</strong> (<strong class="bold">standard deviation</strong>)</li>
				<li><strong class="inline">mad</strong> (<strong class="bold">mean absolute deviation</strong>)</li>
				<li><strong class="inline">sum</strong></li>
				<li><strong class="inline">count</strong></li>
				<li><strong class="inline">abs</strong><h4>Note</h4><p class="callout">Several statistics, such as <strong class="bold">mean</strong> and <strong class="bold">standard deviation</strong>, only make sense with numeric data.</p></li>
			</ul>
			<p>After applying <strong class="inline">GroupBy</strong>, a specific column can be selected and the aggregation operation can be applied to it, or all the remaining columns can be aggregated by the same function. Like SQL, <strong class="inline">GroupBy</strong> can be applied to more than one column at a time, and more than one aggregation operation can be applied to selected columns, one operation per column.</p>
			<p>The <strong class="inline">GroupBy</strong> command in Pandas has some options, such as <strong class="inline">as_index</strong>, which can override the standard of transforming grouping key's columns to indexes and leaving them as normal columns. This is helpful when a new index will be created after the <strong class="inline">GroupBy</strong> operation, for example.</p>
			<p>Aggregation operations can be done over several columns and different statistical methods at the same time with the <strong class="inline">agg</strong> method, passing a dictionary with the name of the column as the <strong class="bold">key</strong> and a list of statistical operations as <strong class="bold">values</strong>.</p>
			<h3 id="_idParaDest-39"><a id="_idTextAnchor041"/>Exercise 6: Aggregation and Grouping Data</h3>
			<p>Remember that we have to answer the question of which state has, on average, the highest radionuclide concentration. As there are several cities per state, we have to combine the values of all cities in one state and calculate the average. This is one of the applications of <strong class="inline">GroupBy</strong>: calculating the average values of one variable as per a grouping. We can answer the question using <strong class="inline">GroupBy</strong>:</p>
			<ol>
				<li value="1">Import the required libraries:<p class="snippet">import numpy as np</p><p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">import seaborn as sns</p></li>
				<li>Load the datasets from the <a href="https://opendata.socrata.com/">https://opendata.socrata.com/</a>:<p class="snippet">df = pd.read_csv('RadNet_Laboratory_Analysis.csv')</p></li>
				<li>Group the DataFrame using the <strong class="inline">State</strong> column.<p class="snippet">df.groupby('State')</p></li>
				<li>Select the radionuclide <strong class="inline">Cs-134</strong> and calculate the average value per group:<p class="snippet">df.groupby('State')['Cs-134'].head()</p></li>
				<li>Do the same for all columns, grouping per state and applying directly the <strong class="inline">mean</strong> function:<p class="snippet">df.groupby('State').mean().head()</p></li>
				<li>Now, group by more than one column, using a list of grouping columns.</li>
				<li>Aggregate using several aggregation operations per column with the <strong class="inline">agg</strong> method. Use the <strong class="inline">State</strong> and <strong class="inline">Location</strong> columns:<p class="snippet">df.groupby(['State', 'Location']).agg({'Cs-134':['mean', 'std'], 'Te-129':['min', 'max']})</p></li>
			</ol>
			<h3 id="_idParaDest-40">NumPy on Pandas<a id="_idTextAnchor042"/></h3>
			<p>NumPy functions can be applied to DataFrames directly or through the <strong class="inline">apply</strong> and <strong class="inline">applymap</strong> methods. Other NumPy functions, such as <strong class="inline">np.where</strong>, also work with DataFrames.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor043"/>Exporting Data from Pandas</h2>
			<p>After creating an intermediate or final dataset in pandas, we can export the values from the DataFrame to several other formats. The most common one is CSV, and the command to do so is <strong class="inline">df.to_csv('filename.csv')</strong>. Other formats, such as Parquet and JSON, are also supported. </p>
			<h4>Note</h4>
			<p class="callout">Parquet is particularly interesting, and it is one of the big data formats that we will discuss later in the book.</p>
			<h3 id="_idParaDest-42"><a id="_idTextAnchor044"/>Exercise 7: Exporting Data in Different Formats</h3>
			<p>After finishing our analysis, we may want to save our transformed dataset with all the corrections, so if we want to share this dataset or redo our analysis, we don't have to transform the dataset again. We can also include our analysis as part of a larger data pipeline or even use the prepared data in the analysis as input to a machine learning algorithm. We can accomplish data exporting our DataFrame to a file with the right format:</p>
			<ol>
				<li value="1">Import all the required libraries and read the data from the dataset using the following command:<p class="snippet">import numpy as np</p><p class="snippet">import pandas as pd</p><p class="snippet">url = "https://opendata.socrata.com/api/views/cf4r-dfwe/rows.csv?accessType=DOWNLOAD"</p><p class="snippet">df = pd.read_csv(url)</p><p>Redo all adjustments for the data types (date, numeric, and categorical) in the RadNet data. The type should be the same as in <em class="italics">Exercise 6: Aggregation and Grouping Data</em>.</p></li>
				<li>Select the numeric columns and the categorical columns, creating a list for each of them:<p class="snippet">columns = df.columns</p><p class="snippet">id_cols = ['State', 'Location', "Date Posted", 'Date Collected', 'Sample Type', 'Unit']</p><p class="snippet">columns = list(set(columns) - set(id_cols))</p><p class="snippet">columns</p><p>The output is as follows:</p><div class="IMG---Figure" id="_idContainer027"><img alt="Figure 1.16: List of columns" src="image/C12913_01_16.jpg"/></div><h6>Figure 1.16: List of columns</h6></li>
				<li>Apply the lambda function that replaces <strong class="inline">Non-detect</strong> with <strong class="inline">np.nan</strong>:<p class="snippet">df['Cs-134'] = df['Cs-134'].apply(lambda x: np.nan if x == "Non-detect" else x)</p><p class="snippet">df.loc[:, columns] = df.loc[:, columns].applymap(lambda x: np.nan if x == 'Non-detect' else x)</p><p class="snippet">df.loc[:, columns] = df.loc[:, columns].applymap(lambda x: np.nan if x == 'ND' else x)</p></li>
				<li>Remove the spaces from the categorical columns:<p class="snippet">df.loc[:, ['State', 'Location', 'Sample Type', 'Unit']] = df.loc[:, ['State', 'Location', 'Sample Type', 'Unit']].applymap(lambda x: x.strip())</p></li>
				<li>Transform the date columns to the <strong class="inline">datetime</strong> format:<p class="snippet">df['Date Posted'] = pd.to_datetime(df['Date Posted'])</p><p class="snippet">df['Date Collected'] = pd.to_datetime(df['Date Collected'])</p></li>
				<li>Transform all numeric columns to the correct numeric format with the <strong class="inline">to_numeric</strong> method:<p class="snippet">for col in columns:</p><p class="snippet">    df[col] = pd.to_numeric(df[col])</p></li>
				<li>Transform all categorical variables to the <strong class="inline">category</strong> type:<p class="snippet">df['State'] = df['State'].astype('category')</p><p class="snippet">df['Location'] = df['Location'].astype('category')</p><p class="snippet">df['Unit'] = df['Unit'].astype('category')</p><p class="snippet">df['Sample Type'] = df['Sample Type'].astype('category')</p></li>
				<li>Export our transformed DataFrame, with the right values and columns, to the CSV format with the <strong class="inline">to_csv</strong> function. Exclude the index using <strong class="inline">index=False</strong>, use a semicolon as the separator <strong class="inline">sep=";"</strong>, and encode the data as UTF-8 <strong class="inline">encoding="utf-8"</strong>:<p class="snippet">df.to_csv('radiation_clean.csv', index=False, sep=';', encoding='utf-8')</p></li>
				<li>Export the same DataFrame to the Parquet columnar and binary format with the <strong class="inline">to_parquet</strong> method:<p class="snippet">df.to_parquet('radiation_clean.prq', index=False)</p><h4>Note</h4><p class="callout">Be careful when converting a datetime to a string!</p></li>
			</ol>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor045"/>Visualization with Pandas</h2>
			<p>Pandas can be thought as a data Swiss Army knife, and one thing that a data scientist always needs when analyzing data is to visualize that data. We will go into detail on the kinds of plot that we can apply in an analysis. For now, the idea is to show how to do <strong class="bold">quick and dirty</strong> plots directly from pandas.</p>
			<p>The <strong class="inline">plot</strong> function can be called directly from the DataFrame selection, allowing fast visualizations. A scatter plot can be created by using Matplotlib and passing data from the DataFrame to the plotting function. Now that we know the tools, let's focus on the pandas interface for data manipulation. This interface is so powerful that it is replicated by other projects that we will see in this course, such as Spark. We will explain the plot components and methods in more detail in the next chapter.</p>
			<p>You will see how to create graphs that are useful for statistical analysis in the next chapter. Focus here on the mechanics of creating plots from pandas for quick visualizations.</p>
			<h3 id="_idParaDest-44"><a id="_idTextAnchor046"/>Activity 3: Plotting Data with Pandas</h3>
			<p>To finish up our activity, let's redo all the previous steps and plot graphs with the results, as we would do in a preliminary analysis:</p>
			<ol>
				<li value="1">Use the RadNet DataFrame that we have been working with.</li>
				<li>Fix all the data type problems, as we saw before.</li>
				<li>Create a plot with a filter per <strong class="inline">Location</strong>, selecting the city of <strong class="inline">San Bernardino</strong>, and one radionuclide, with the <em class="italics">x</em>-axis as date and the <em class="italics">y</em>-axis as radionuclide <strong class="inline">I-131</strong>:<div class="IMG---Figure" id="_idContainer028"><img alt="Figure 1.17: Plot of Location with I-131" src="image/C12913_01_17.jpg"/></div><h6>Figure 1.17: Plot of Location with I-131</h6></li>
				<li>Create a scatter plot with the concentration of two related radionuclides, <strong class="inline">I-131</strong> and <strong class="inline">I-132</strong>:</li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer029">
					<img alt="Figure 1.18: Plot of I-131 and I-132" src="image/C12913_01_18.jpg"/>
				</div>
			</div>
			<h6>Figure 1.18: Plot of I-131 and I-132</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 203.</p>
			<p>We are getting a bit ahead of ourselves here with the plotting, so we don't need to worry about the details of the plot or how we attribute titles, labels, and so on. The important takeaway here is understanding that we can plot directly from the DataFrame for quick analysis and visualization.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor047"/>Summary</h2>
			<p>We have learned about the most common Python libraries used in data analysis and data science, which make up the Python data science stack. We learned how to ingest data, select it, filter it, and aggregate it. We saw how to export the results of our analysis and generate some quick graphs.</p>
			<p>These are steps done in almost any data analysis. The ideas and operations demonstrated here can be applied to data manipulation with big data. Spark DataFrames were created with the pandas interface in mind, and several operations are performed in a very similar fashion in pandas and Spark, greatly simplifying the analysis process. Another great advantage of knowing your way around pandas is that Spark can convert its DataFrames to pandas DataFrames and back again, enabling analysts to work with the best tool for the job.</p>
			<p>Before going into big data, we need to understand how to better visualize the results of our analysis. Our understanding of the data and its behavior can be greatly enhanced if we visualize it using the correct plots. We can draw inferences and see anomalies and patterns when we plot the data.</p>
			<p>In the next chapter, we will learn how to choose the right graph for each kind of data and analysis, and how to plot it using Matplotlib and Seaborn.</p>
		</div>
	</body></html>