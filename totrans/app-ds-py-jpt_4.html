<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer133" class="Content">
			<p class="hidden">&gt;</p>
		</div>
		<div id="_idContainer134" class="Content">
			<h1 id="_idParaDest-69"><a id="_idTextAnchor071"/>Appendix A</h1>
		</div>
		<div id="_idContainer135" class="Content">
			<h2><a id="_idTextAnchor072"/>About</h2>
			<p>This section is included to assist the students to perform the activities present in the book. It includes detailed steps that are to be performed by the students to complete and achieve the objectives of the activity.</p>
		</div>
		<div id="_idContainer172" class="Content">
			<h2 id="_idParaDest-70"><a id="_idTextAnchor073"/>Chapter 1: Jupyter Fundamentals</h2>
			<h3 id="_idParaDest-71"><a id="_idTextAnchor074"/>Activity 1: Building a Third-Order Polynomial Model</h3>
			<ol>
				<li>Scroll to the empty cells at the bottom of <strong class="inline">Subtopic C</strong> in your Jupyter Notebook. </li>
				<li>These will be found beneath the linear-model <strong class="bold">MSE</strong> calculation cell under the <strong class="inline">Activity</strong> heading.<h4>Note</h4><p class="callout">You should fill these empty cells in with code as we complete the activity. You may need to insert new cells as these become filled up; please do so as needed.</p></li>
				<li>We will first pull out our dependent feature from and target variable from <strong class="inline">df.</strong> using the following:<p class="snippet">y = df['MEDV'].values</p><p class="snippet">x = df['LSTAT'].values.reshape(-1,1)</p><p>This is identical to what we did earlier for the linear model.</p></li>
				<li>Verify what <strong class="inline">x</strong> looks like by printing the first few samples with <strong class="inline">print(x[:3])</strong>:<div id="_idContainer136" class="IMG---Figure"><img src="Images/C13018_01_491.jpg" alt="Figure 1.49: Printing first three values of x using print()" width="921" height="238"/></div><h6>Figure 1.49: Printing first three values of x using print()</h6><p>Notice how each element in the array is itself an array with length 1. This is what <strong class="inline">reshape(-1,1)</strong> does, and it is the form expected by scikit-learn.</p></li>
				<li>Transform <strong class="inline">x</strong> into "polynomial features" by importing the appropriate transformation tool from scikit-learn and instantiating the third-degree polynomial feature transformer:<p class="snippet">from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures(degree=3)</p><p>The rationale for this step may not be immediately obvious but will be explained shortly.</p></li>
				<li>Transform the <strong class="bold">LSTAT</strong> feature (as stored in the variable <strong class="inline">x</strong>) by running the <strong class="inline">fit_transform</strong> method. Build the polynomial feature set by running the following code:<p class="snippet">x_poly = poly.fit_transform(x)</p><p>Here, we have used the instance of the transformer feature to transform the LSTAT variable.</p></li>
				<li>Verify what <strong class="inline">x_poly</strong> looks like by printing the first few samples with <strong class="inline">print(x_poly[:3])</strong>.<div id="_idContainer137" class="IMG---Figure"><img src="Images/C13018_01_501.jpg" alt="Figure 1.50: Printing first three values of x_poly using print()" width="1251" height="250"/></div><h6>Figure 1.50: Printing first three values of x_poly using print()</h6><p>Unlike <em class="italics">x</em>, the arrays in each row now have length 4, where the values have been calculated as <em class="italics">xº</em>, <em class="italics">x¹</em>, <em class="italics">x²</em> and <em class="italics">x³</em>.</p><p>We are now going to use this data to fit a linear model. Labeling the features as <em class="italics">a</em>, <em class="italics">b</em>, <em class="italics">c</em>, and <em class="italics">d</em>, we will calculate the coefficients αₒ, α1, α2, and α3 and of the linear model:</p><div id="_idContainer138" class="IMG---Figure"><img src="Images/C13018_01_50_a.jpg" alt="" width="1638" height="92"/></div><p>We can plug in the definitions of a, b, c, and d, to get the following polynomial model, where the coefficients are the same as the previous ones:</p><div id="_idContainer139" class="IMG---Figure"><img src="Images/C13018_01_50_b.jpg" alt="" width="1464" height="94"/></div></li>
				<li>Import the <strong class="inline">LinearRegression</strong> class and build our linear classification model the same way as done while calculating the MSE. Run the following:<p class="snippet">from sklearn.linear_model import LinearRegression clf = </p><p class="snippet">LinearRegression()</p><p class="snippet">clf.fit(x_poly, y)</p></li>
				<li>Extract the coefficients and print the polynomial model using the following code:<p class="snippet">a_0 = clf.intercept_ + clf.coef_[0] # intercept a_1, a_2, a_3 = clf.coef_[1:]       # other coefficients</p><p class="snippet">msg = 'model: y = {:.3f} + {:.3f}x + {:.3f}x^2 + {:.3f}x^3'\        .format(a_0, a_1, a_2, a_3)print(msg)</p><div id="_idContainer140" class="IMG---Figure"><img src="Images/C13018_01_511.jpg" alt="Figure 1.51: Extracting coefficients and printing the polynomial model" width="1635" height="188"/></div><h6>Figure 1.51: Extracting coefficients and printing the polynomial model</h6><p>To get the actual model intercept, we have to add the <strong class="inline">intercept_</strong> and <strong class="inline">coef_</strong></p><p><strong class="inline">[0]</strong> attributes. The higher-order coefficients are then given by the remaining values of <strong class="inline">coef_</strong>.</p></li>
				<li>Determine the predicted values for each sample and calculate the residuals by running the following code:<p class="snippet">y_pred = clf.predict(x_poly) resid_MEDV = y - y_pred</p></li>
				<li>Print some of the residual values by running <strong class="inline">print(resid_MEDV[:10])</strong>:<div id="_idContainer141" class="IMG---Figure"><img src="Images/C13018_01_521.jpg" alt="Figure 1.52: Printing residual values" width="1073" height="134"/></div><h6>Figure 1.52: Printing residual values</h6><p>We'll plot these soon to compare with the linear model residuals, but first we will calculate the MSE.</p></li>
				<li>Run the following code to print the MSE for the third-order polynomial model:<p class="snippet">from sklearn.metrics import mean_squared_error error = mean_squared_error(y, y_pred) print('mse = {:.2f}'.format(error))</p><div id="_idContainer142" class="IMG---Figure"><img src="Images/C13018_01_531.jpg" alt="Figure 1.53: Calculating the mean squared error" width="1340" height="138"/></div><h6>Figure 1.53: Calculating the mean squared error</h6><p>As can be seen, the <strong class="bold">MSE</strong> is significantly less for the polynomial model compared to the linear model (which was 38.5). This error metric can be converted to an average error in dollars by taking the square root. Doing this for the polynomial model, we find the average error for the median house value is only $5,300.</p><p>Now, we'll visualize the model by plotting the polynomial line of best fit along with the data.</p></li>
				<li>Plot the polynomial model along with the samples by running the following:<p class="snippet">fig, ax = plt.subplots() # Plot the samples</p><p class="snippet">ax.scatter(x.flatten(), y, alpha=0.6)</p><p class="snippet"># Plot the polynomial model</p><p class="snippet">x_ = np.linspace(2, 38, 50).reshape(-1, 1) x_poly = poly.fit_transform(x_)</p><p class="snippet">y_ = clf.predict(x_poly)</p><p class="snippet">ax.plot(x_, y_, color='red', alpha=0.8) ax.set_xlabel('LSTAT'); ax.set_ylabel('MEDV');</p><div id="_idContainer143" class="IMG---Figure"><img src="Images/C13018_01_541.jpg" alt="Figure 1.54: Plotting the polynomial model for MEDV and LSTAT" width="1538" height="858"/></div><h6>Figure 1.54: Plotting the polynomial model for MEDV and LSTAT</h6><p>Here, we are plotting the red curve by calculating the polynomial model predictions on an array of <strong class="inline">x</strong> values. The array of <strong class="inline">x</strong> values was created using <strong class="inline">np.linspace</strong>, resulting in 50 values arranged evenly between 2 and 38.</p><p>Now, we'll plot the corresponding residuals. Whereas we used Seaborn for this earlier, we'll have to do it manually to show results for a scikit-learn model. Since we already calculated the residuals earlier, as reference by the <strong class="inline">resid_MEDV</strong> variable, we simply need to plot this list of values on a scatter chart.</p></li>
				<li>Plot the residuals by running the following:<p class="snippet">fig, ax = plt.subplots(figsize=(5, 7)) ax.scatter(x, resid_MEDV, alpha=0.6) ax.set_xlabel('LSTAT')</p><p class="snippet">ax.set_ylabel('MEDV Residual $(y-\hat{y})$') plt.axhline(0, color='black', ls='dotted');</p></li>
			</ol>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="Images/C13018_01_551.jpg" alt="Figure 1.55: Plotting the residuals for LSTAT and MEDV" width="1493" height="1100"/>
				</div>
			</div>
			<h6>Figure 1.55: Plotting the residuals for LSTAT and MEDV</h6>
			<p>Compared to the linear model <strong class="bold">LSTAT</strong> residual plot, the polynomial model residuals appear to be more closely clustered around <em class="italics">y - </em>ŷ<em class="italics"> = 0.</em> Note that y is the sample <strong class="bold">MEDV</strong> and ŷ is the predicted value. There are still clear patterns, such as the cluster near <em class="italics">x = 7</em> and <em class="italics">y = -7</em> that indicates suboptimal modeling.</p>
			<p>Having successfully modeled the data using a polynomial model, let's finish up this chapter by looking at categorical features. In particular, we are going to build a set of categorical features and use them to explore the dataset in more detail.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor075"/>Chapter 2: Data Cleaning and Advanced Machine</h2>
			<h3 id="_idParaDest-73"><a id="_idTextAnchor076"/>Activity 2: Preparing to Train a Predictive Model for the Employee-Retention Problem</h3>
			<ol>
				<li value="1">Scroll to the <strong class="inline">Activity A</strong> section of the<strong class="inline"> lesson-2-workbook.ipynb</strong> notebook file.</li>
				<li>Check the head of the table by running the following code:<p class="snippet">%%bash</p><p class="snippet">head ../data/hr-analytics/hr_data.csv</p><p>Judging by the output, convince yourself that it looks to be in standard CSV format. For CSV files, we should be able to simply load the data with pd.read_csv.</p></li>
				<li>Load the data with Pandas by running <strong class="inline">df = pd.read_csv('../data/hr- analytics/hr_data.csv')</strong>. Write it out yourself and use tab completion to help type the file path.</li>
				<li>Inspect the columns by printing <strong class="inline">df.columns</strong> and make sure the data has loaded as expected by printing the DataFrame <strong class="inline">head</strong> and <strong class="inline">tail</strong> with <strong class="inline">df.head()</strong> and <strong class="inline">df.tail()</strong>:<div id="_idContainer145" class="IMG---Figure"><img src="Images/C13018_02_46.jpg" alt="Figure 2.46: Output for inspecting head and tail of columns" width="1800" height="1013"/></div><h6>Figure 2.46: Output for inspecting head and tail of columns</h6><p>We can see that it appears to have loaded correctly. Based on the tail index values, there are nearly 15,000 rows; let's make sure we didn't miss any.</p></li>
				<li>Check the number of rows (including the header) in the CSV file with the following code:<p class="snippet">with open('../data/hr-analytics/hr_data.csv') as f: print(len(f.read().splitlines()))</p><div id="_idContainer146" class="IMG---Figure"><img src="Images/C13018_02_47.jpg" alt="Figure 2.46: Output after checking for number of rows" width="1423" height="226"/></div><h6>Figure 2.47: Output after checking for number of rows</h6></li>
				<li>Compare this result to <strong class="inline">len(df)</strong> to make sure you've loaded all the data:<div id="_idContainer147" class="IMG---Figure"><img src="Images/C13018_02_48.jpg" alt="Figure 2.48: Output after checking for number of sample uploaded" width="1786" height="198"/></div><h6>Figure 2.48: Output after checking for number of sample uploaded</h6><p>Now that our client's data has been properly loaded, let's think about how we can use predictive analytics to find insights into why their employees are leaving.</p><p>Let's run through the first steps for creating a predictive analytics plan:</p><p>Look at the available data: You've already done this by looking at the columns, datatypes, and the number of samples.</p><p>Determine the business needs: The client has clearly expressed their needs: reduce the number of employees who leave.</p><p>Assess the data for suitability: Let's try to determine a plan that can help satisfy the client's needs, given the provided data</p><p>Recall, as mentioned earlier, that effective analytics techniques lead to impactful business decisions. With that in mind, if we were able to predict how likely an employee is to quit, the business could selectively target those employees for special treatment. For example, their salary could be raised or their number of projects reduced. Furthermore, the impact of these changes could be estimated using the model!</p><p>To assess the validity of this plan, let's think about our data. Each row represents an employee who either works for the company or has left, as labeled by the column named left. We can therefore train a model to predict this target, given a set of features.</p></li>
				<li>Assess the target variable. Check the distribution and number of missing entries by running the following code:<p class="snippet">df.left.value_counts().plot('barh') print(df.left.isnull().sum()</p><div id="_idContainer148" class="IMG---Figure"><img src="Images/C13018_02_49.jpg" alt="Figure 2.49: Distribution of the target variables" width="1414" height="582"/></div><h6>Figure 2.49: Distribution of the target variables</h6><p>Here's the output of the second code line:</p><div id="_idContainer149" class="IMG---Figure"><img src="Images/C13018_02_50.jpg" alt="Figure 2.50: Output to check missing data points" width="577" height="79"/></div><h6>Figure 2.50: Output to check missing data points</h6><p>About three-quarters of the samples are employees who have not left. The group that has left make up the other quarter of the samples. This tells us we are dealing with an imbalanced classification problem, which means we'll have to take special measures to account for each class when calculating accuracies. We also see that none of the target variables are missing (no <strong class="inline">NaN</strong> values).</p><p>Now, we'll assess the features:</p></li>
				<li>Print the datatype of each by executing <strong class="inline">df.dtypes</strong>. Observe how we have a mix of continuous and discrete features:<div id="_idContainer150" class="IMG---Figure"><img src="Images/C13018_02_51.jpg" alt="Figure 2.51: Printing data types for verification" width="1244" height="586"/></div><h6>Figure 2.51: Printing data types for verification</h6></li>
				<li>Display the feature distributions by running the following code:<p class="snippet">for f in df.columns: try:</p><p class="snippet">fig = plt.figure()</p><p class="snippet">…</p><p class="snippet">…</p><p class="snippet">print('-'*30)</p><h4>Note</h4><p class="callout">For the complete code, refer to the following: <a href="https://bit.ly/2D3iKL2">https://bit.ly/2D3iKL2</a>.</p><p>This code snippet is a little complicated, but it's very useful for showing an overview of both the continuous and discrete features in our dataset. Essentially, it assumes each feature is continuous and attempts to plot its distribution, and reverts to simply plotting the value counts if the feature turns out to be discrete.</p><p>The result is as follows:</p><div id="_idContainer151" class="IMG---Figure"><img src="Images/C13018_02_52.jpg" alt="Figure 2.52: Distribution of all features: satisfaction_level and last_evaluation" width="1800" height="1408"/></div><h6>Figure 2.52: Distribution of all features: satisfaction_level and last_evaluation</h6><div id="_idContainer152" class="IMG---Figure"><img src="Images/C13018_02_53.jpg" alt="Figure 2.52: Distribution of all remaining features" width="1587" height="1244"/></div><h6>Figure 2.53: Distribution of all remaining features</h6><div id="_idContainer153" class="IMG---Figure"><img src="Images/C13018_02_54.jpg" alt="Figure 2.54: Distribution for the variable promotion_last_5years" width="1330" height="630"/></div><h6>Figu<a id="_idTextAnchor077"/>re 2.54: Distribution for the variable promotion_last_5years</h6><p>For many features, we see a wide distribution over the possible values, indicating a good variety in the feature spaces. This is encouraging; features that are strongly grouped around a small range of values may not be very informative for the model. This is the case for <strong class="inline">promotion_last_5years</strong>, where we see that the vast majority of samples are <strong class="inline">0</strong>.</p><p>The next thing we need to do is remove any <strong class="inline">NaN</strong> values from the dataset.</p></li>
				<li>Check how many <strong class="inline">NaN</strong> values are in each column by running the following code:<p class="snippet">df.isnull().sum() / len(df) * 100</p><div id="_idContainer154" class="IMG---Figure"><img src="Images/C13018_02_55.jpg" alt="Figure 2.55: Verification for the number of NaN values" width="1483" height="624"/></div><h6>Figure 2.55: Verification for the number of NaN values</h6><p>We can see there are about 2.5% missing for <strong class="inline">average_montly_hours</strong>, 1% missing for <strong class="inline">time_spend_company</strong>, and 98% missing for <strong class="inline">is_smoker!</strong> Let's use a couple of different strategies that you've learned to handle these.</p></li>
				<li>Drop the <strong class="inline">is_smoker</strong> column as there is barely any information in this metric. Do this by running: <strong class="inline">del df['is_smoker']</strong>.</li>
				<li>Fill the <strong class="inline">NaN</strong> values in the <strong class="inline">time_spend_company</strong> column. This can be done with the following code:<p class="snippet">fill_value = df.time_spend_company.median()</p><p class="snippet">df.time_spend_company = df.time_spend_company.fillna(fill_ value)</p><p>The final column to deal with is <strong class="inline">average_montly_hours</strong>. We could do something similar and use the median or rounded mean as the integer fill value. Instead though, let's try to take advantage of its relationship with another variable. This may allow us to fill the missing data more accurately.</p></li>
				<li>Make a boxplot of <strong class="inline">average_montly_hours</strong> segmented by <strong class="inline">number_project</strong>. This can be done by running the following code:<p class="snippet">sns.boxplot(x='number_project', y='average_montly_hours', data=df)</p><div id="_idContainer155" class="IMG---Figure"><img src="Images/C13018_02_56.jpg" alt="Figure 2.56: Boxplot for average_monthly_hours and number_project" width="1800" height="844"/></div><h6>Figure 2.56: Boxplot for average_monthly_hours and number_project</h6><p>We can see how the number of projects is correlated with <strong class="inline">average_ monthly_hours</strong>, a result that is hardly surprising. We'll exploit this relationship by filling in the <strong class="inline">NaN</strong> values of <strong class="inline">average_montly_hours</strong> differently, depending on the number of projects for that sample.</p><p>Specifically, we'll use the mean of each group.</p></li>
				<li> Calculate the mean of each group by running the following code:<p class="snippet">mean_per_project = df.groupby('number_project')\</p><p class="snippet">.average_montly_hours.mean() mean_per_project = dict(mean_per_project) print(mean_per_project)</p><div id="_idContainer156" class="IMG---Figure"><img src="Images/C13018_02_57.jpg" alt="Figure 2.57: Calculation of mean values for average_monthly_hours" width="1112" height="514"/></div><h6>Figure 2.57: Calculation of mean values for average_monthly_hours</h6><p>We can then map this onto the <strong class="inline">number_project</strong> column and pass the resulting series object as the argument to <strong class="inline">fillna</strong>.</p></li>
				<li>Fill the <strong class="inline">NaN</strong> values in <strong class="inline">average_montly_hours</strong> by executing the following code:<p class="snippet">fill_values = df.number_project.map(mean_per_project)</p><p class="snippet">df.average_montly_hours = df.average_montly_hours. fillna(fill_values)</p></li>
				<li>Confirm that <strong class="inline">df</strong> has no more <strong class="inline">NaN</strong> values by running the following assertion test. If it does not raise an error, then you have successfully removed the <strong class="inline">NaNs</strong> from the table:<p class="snippet">assert df.isnull().sum().sum() == 0</p><h4>Note</h4><p class="callout">We pass <strong class="inline">index=False</strong> so that the index is not written to file. In this case, the index is a set of integers spanning from 0 to the DataFrame length, and it therefore tells us nothing important.</p></li>
				<li>Transform the string and Boolean fields into integer representations. In particular, we'll manually convert the target variable <strong class="inline">left</strong> from <strong class="inline">yes</strong> and <strong class="inline">no</strong> to <strong class="inline">1</strong> and <strong class="inline">0</strong> and build the one-hot encoded features. Do this by running the following code:<p class="snippet">df.left = df.left.map({'no': 0, 'yes': 1}) df = pd.get_dummies(df)</p></li>
				<li>Print <strong class="inline">df.columns</strong> to show the fields:<div id="_idContainer157" class="IMG---Figure"><img src="Images/C13018_02_58.jpg" alt="Figure 2.58: A screenshot of the different fields in the dataframe" width="1646" height="406"/></div><h6>Figure 2.58: A screenshot of the different fields in the dataframe</h6><p>We can see that department and salary have been split into various binary features.</p><p>The final step to prepare our data for machine learning is scaling the features, but for various reasons (for example, some models do not require scaling), we'll do it as part of the model-training workflow in the next activity.</p></li>
				<li>We have completed the data preprocessing and are ready to move on to training models! Let's save our preprocessed data by running the following code:<p class="snippet">df.to_csv('../data/hr-analytics/hr_data_processed.csv', index=False)</p></li>
			</ol>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor078"/>Chapter 3: Web Scraping and Interactive Visualizations</h2>
			<h3 id="_idParaDest-75"><a id="_idTextAnchor079"/>Activity 3: Web Scraping with Jupyter Notebooks</h3>
			<ol>
				<li value="1">For this page, the data can be scraped using the following code snippet:<p class="snippet">data = []</p><p class="snippet">for i, row in enumerate(soup.find_all('tr')): row_data = row.find_all('td')</p><p class="snippet">try:</p><p class="snippet">d1, d2, d3 = row_data[1], row_data[5], row_data[6] d1 = d1.find('a').text</p><p class="snippet">d2 = float(d2.text)</p><p class="snippet">d3 = d3.find_all('span')[1].text.replace('+', '') data.append([d1, d2, d3])</p><p class="snippet">except:</p><p class="snippet">print('Ignoring row {}'.format(i)</p></li>
				<li>In the <strong class="inline">lesson-3-workbook.ipynb</strong> Jupyter Notebook, scroll to <strong class="inline">Activity A: Web scraping with Python</strong>.</li>
				<li>Set the <strong class="inline">url</strong> variable and load an IFrame of our page in the notebook by running the following code:<p class="snippet"><a href="http://www.worldometers.info/world-population/">url = 'http://www.worldometers.info/world-population/</a> population-by-country/'</p><p class="snippet">IFrame(url, height=300, width=800)</p><p>The page should load in the notebook. Scrolling down, we can see the <strong class="bold">Countries in the world by population</strong> heading and the table of values beneath it. We'll scrape the first three columns from this table to get the countries, populations, and yearly population changes.</p></li>
				<li>Close the IFrame by selecting the cell and clicking <strong class="bold">Current Outputs</strong> | <strong class="bold">Clear</strong> from the <strong class="bold">Cell</strong> menu in the Jupyter Notebook.</li>
				<li>Request the page and load it as a <strong class="inline">BeautifulSoup</strong> object by running the following code:<p class="snippet">page = requests.get(url)</p><p class="snippet">soup = BeautifulSoup(page.content, 'html.parser')</p><p>We feed the page content to the <strong class="inline">BeautifulSoup</strong> constructor. Recall that previously, we used <strong class="inline">page.text</strong> here instead. The difference is that <strong class="inline">page.content</strong> returns the raw binary response content, whereas <strong class="inline">page.text</strong> returns the UTF-8 decoded content. It's usually best practice to pass the bytes object and let <strong class="inline">BeautifulSoup</strong> decode it, rather than doing it with Requests using <strong class="inline">page.text</strong>.</p></li>
				<li>Print the H1 for the page by running the following code:<p class="snippet">soup.find_all('h1')</p><p class="snippet">&gt;&gt; [&lt;h1&gt;Countries in the world by population (2017)&lt;/h1&gt;]</p><p>We'll scrape the table by searching for <strong class="inline">&lt;th&gt;</strong>, <strong class="inline">&lt;tr&gt;</strong>, and <strong class="inline">&lt;td&gt;</strong> elements, as in the previous exercise.</p></li>
				<li>Get and print the table headings by running the following code:<p class="snippet">table_headers = soup.find_all('th') table_headers</p><p class="snippet">&gt;&gt; [&lt;th&gt;#&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;Country (or dependency)&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;Population&lt;br/&gt; (2017)&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;Yearly&lt;br/&gt; Change&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;Net&lt;br/&gt; Change&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;Density&lt;br/&gt; (P/Km²)&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;Land Area&lt;br/&gt; (Km²)&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;Migrants&lt;br/&gt; (net)&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;Fert.&lt;br/&gt; Rate&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;Med.&lt;br/&gt; Age&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;Urban&lt;br/&gt; Pop %&lt;/th&gt;,</p><p class="snippet">&lt;th&gt;World&lt;br/&gt; Share&lt;/th&gt;]</p></li>
				<li>We are only interested in the first three columns. Select these and parse the text with the following code: <p class="snippet">table_headers = table_headers[1:4] table_headers = [t.text.replace('\n', '') for t in table_ headers]</p><p>After selecting the subset of table headers we want, we parse the text content from each and remove any newline characters.</p><p>Now, we'll get the data. Following the same prescription as the previous exercise, we'll test how to parse the data for a sample row.</p></li>
				<li>Get the data for a sample row by running the following code:<p class="snippet">row_number = 2</p><p class="snippet">row_data = soup.find_all('tr')[row_number]\</p><p class="snippet">.find_all('td') </p></li>
				<li>How many columns of data do we have? Print the length of <strong class="inline">row_data</strong> by running <strong class="inline">print(len(row_data))</strong>.</li>
				<li>Print the first elements by running <strong class="inline">print(row_data[:4])</strong>:<p class="snippet">&gt;&gt; [&lt;td&gt;2&lt;/td&gt;,</p><p class="snippet">&lt;td style="font-weight: bold; font-size:15px; text-align:left"&gt;&lt;a href="/world-population/india- population/"&gt;India&lt;/a&gt;&lt;/td&gt;,</p><p class="snippet">&lt;td style="font-weight: bold;"&gt;1,339,180,127&lt;/td&gt;,</p><p class="snippet">&lt;td&gt;1.13 %&lt;/td&gt;]</p><p>It's pretty obvious that we want to select list indices 1, 2, and 3. The first data value can be ignored, as it's simply the index.</p></li>
				<li>Select the data elements we're interested in parsing by running the following code:<p class="snippet">d1, d2, d3 = row_data[1:4]</p></li>
				<li>Looking at the <strong class="inline">row_data</strong> output, we can find out how to correctly parse the data. We'll want to select the content of the <strong class="inline">&lt;a&gt;</strong> element in the first data element, and then simply get the text from the others. Test these assumptions by running the following code:<p class="snippet">print(d1.find('a').text) print(d2.text) print(d3.text)</p><p class="snippet">&gt;&gt; India</p><p class="snippet">&gt;&gt; 1,339,180,127</p><p class="snippet">&gt;&gt; 1.13 %</p><p>Excellent! This looks to be working well. Now, we're ready to scrape the entire table.</p></li>
				<li>Scrape and parse the table data by running the following code:<p class="snippet">data = []</p><p class="snippet">for i, row in enumerate(soup.find_all('tr')): try:</p><p class="snippet">d1, d2, d3 = row.find_all('td')[1:4] d1 = d1.find('a').text</p><p class="snippet">d2 = d2.text d3 = d3.text</p><p class="snippet">data.append([d1, d2, d3]) except:</p><p class="snippet">print('Error parsing row {}'.format(i))</p><p class="snippet">&gt;&gt; Error parsing row 0</p><p>This is quite similar to before, where we try to parse the text and skip the row if there's some error.</p></li>
				<li>Print the head of the scraped data by running <strong class="inline">print(data[:10])</strong>:<p class="snippet">&gt;&gt; [['China', '1,409,517,397', '0.43 %'],</p><p class="snippet">['India', '1,339,180,127', '1.13 %'],</p><p class="snippet">['U.S.', '324,459,463', '0.71 %'],</p><p class="snippet">['Indonesia', '263,991,379', '1.10 %'],</p><p class="snippet">['Brazil', '209,288,278', '0.79 %'],</p><p class="snippet">['Pakistan', '197,015,955', '1.97 %'],</p><p class="snippet">['Nigeria', '190,886,311', '2.63 %'],</p><p class="snippet">['Bangladesh', '164,669,751', '1.05 %'],</p><p class="snippet">['Russia', '143,989,754', '0.02 %'],</p><p class="snippet">['Mexico', '129,163,276', '1.27 %']]</p><p>It looks like we have managed to scrape the data! Notice how similar the process was for this table compared to the Wikipedia one, even though this web page is completely different. Of course, it will not always be the case that data is contained within a table, but regardless, we can usually use <strong class="inline">find_all</strong> as the primary method for parsing.</p></li>
				<li>Finally, save the data to a CSV file for later use. Do this by running the following code:<p class="snippet">f_path = '../data/countries/populations.csv' with open(f_path, 'w') as f:</p><p class="snippet">f.write('{};{};{}\n'.format(*table_headers)) for d in data:</p><p class="snippet">f.write('{};{};{}\n'.format(*d))</p></li>
			</ol>
			<h3 id="_idParaDest-76"><a id="_idTextAnchor080"/>Activity 4: Exploring Data with Interactive Visualizations</h3>
			<ol>
				<li value="1">In the <strong class="inline">lesson-3-workbook.ipynb</strong> file, scroll to the <strong class="inline">Activity B: Interactive visualizations with Bokeh</strong> section.</li>
				<li>Load the previously scraped, merged, and cleaned web page data by running the following code:<p class="snippet">df = pd.read_csv('../data/countries/merged.csv')</p><p class="snippet">df['Date of last change'] = pd.to_datetime(df['Date of last change'])</p></li>
				<li>Recall what the data looks like by displaying the DataFrame:<div id="_idContainer158" class="IMG---Figure"><img src="Images/C13018_03_18.jpg" alt="Figure 3.18: Output of the data within DataFrame" width="849" height="435"/></div><h6>Figure 3.18: Output of the data within DataFrame</h6><p>Whereas in the previous exercise we were interested in learning how Bokeh worked, now we are interested in what this data looks like. In order to explore this dataset, we are going to use interactive visualizations.</p></li>
				<li>Draw a scatter plot of the population as a function of the interest rate by running the following code:<p class="snippet">source = ColumnDataSource(data=dict( x=df['Interest rate'], y=df['Population'], desc=df['Country'],</p><p class="snippet">))</p><p class="snippet">hover = HoverTool(tooltips=[ ('Country', '@desc'),</p><p class="snippet">('Interest Rate (%)', '@x'), ('Population', '@y')</p><p class="snippet">])</p><p class="snippet">tools = [hover, PanTool(), BoxZoomTool(), WheelZoomTool(), ResetTool()]</p><p class="snippet">p = figure(tools=tools,</p><p class="snippet">x_axis_label='Interest Rate (%)', y_axis_label='Population')</p><p class="snippet">p.circle('x', 'y', size=10, alpha=0.5, source=source) show(p)</p><div id="_idContainer159" class="IMG---Figure"><img src="Images/C13018_03_19.jpg" alt="Figure 3.19: Scatter plot of population and interest rate&#13;&#10;" width="1800" height="892"/></div><h6>Figure 3.19: Scatter plot of population and interest rate</h6><p>This is quite similar to the final examples we looked at when introducing Bokeh in the previous exercise. We set up a customized data source with the <em class="italics">x</em> and <em class="italics">y </em>coordinates for each point, along with the country name. This country name is passed to the Hover Tool, so that it's visible when hovering the mouse over the dot. We pass this tool to the figure, along with a set of other useful tools.</p></li>
				<li>In the data, we see some clear outliers with high populations. Hover over these to see what they are:<div id="_idContainer160" class="IMG---Figure"><img src="Images/C13018_03_20.jpg" alt="Figure 3.20: Labels obtained by hovering over data points&#13;&#10;" width="502" height="176"/></div><h6>Figure 3.20: Labels obtained by hovering over data points</h6><p>We see they belong to India and China. These countries have fairly average interest rates. Let's focus on the rest of the points by using the Box Zoom tool to modify the view window size.</p></li>
				<li>Select the Box Zoom tool and alter the viewing window to better see the majority of the data:<div id="_idContainer161" class="IMG---Figure"><img src="Images/C13018_03_21.jpg" alt="Figure 3.21: The Box Zoom tool&#13;&#10;" width="1800" height="386"/></div><h6>Figure 3.21: The Box Zoom tool</h6><div id="_idContainer162" class="IMG---Figure"><img src="Images/C13018_03_22.jpg" alt="Figure 3.22: Scatter plot with majority of the data points within the box&#13;&#10;" width="1800" height="833"/></div><h6>Figure 3.22: Scatter plot with majority of the data points within the box</h6><p>Explore the points and see how the interest rates compare for various countries. What are the countries with the highest interest rates?:</p><div id="_idContainer163" class="IMG---Figure"><img src="Images/C13018_03_23.jpg" alt="Figure 3.23: Hovering over data points to view detailed data&#13;&#10;" width="657" height="288"/></div><h6>Figure 3.23: Hovering over data points to view detailed data</h6></li>
				<li>Some of the lower population countries appear to have negative interest rates. Select the <strong class="bold">Wheel Zoom </strong>tool and use it to zoom in on this region. Use the <strong class="bold">Pan </strong>tool to re-center the plot, if needed, so that the negative interest rate samples are in view. Hover over some of these and see what countries they correspond to:<div id="_idContainer164" class="IMG---Figure"><img src="Images/C13018_03_24.jpg" alt="Figure 3.24: Screen shot of the Wheel Zoom tool&#13;&#10;" width="1176" height="240"/></div><h6>Figure 3.24: Screen shot of the Wheel Zoom tool</h6><div id="_idContainer165" class="IMG---Figure"><img src="Images/C13018_03_25.jpg" alt="Figure 3.25: Data points of negative interest rates countries&#13;&#10;" width="1418" height="614"/></div><h6>Figure 3.25: Data points of negative interest rates countries</h6><p>Let's re-plot this, adding a color based on the date of last interest rate change. This will be useful to search for relations between the date of last change and the interest rate or population size.</p></li>
				<li>Add a <strong class="bold">Year of last change</strong> column to the DataFrame by running the following code:<p class="snippet">def get_year(x):</p><p class="snippet">year = x.strftime('%Y')</p><p class="snippet">if year in ['2018', '2017', '2016']:</p><p class="snippet">return year else: return 'Other'</p><p class="snippet">df['Year of last change'] = df['Date of last change']. apply(get_year)</p></li>
				<li>Create a map to group the last change date into color categories by running the following code:<p class="snippet">year_to_color = { '2018': 'black',</p><p class="snippet">'2017': 'blue',</p><p class="snippet">'2016': 'orange',</p><p class="snippet">'Other':'red'</p><p class="snippet">}</p><p>Once mapped to the <strong class="bold">Year of last change</strong> column, this will assign values to colors based on the available categories: 2018, 2017, 2016, and Other. The colors here are standard strings, but they could alternatively by represented by hexadecimal codes.</p></li>
				<li>Create the colored visualization by running the following code:<p class="snippet">source = ColumnDataSource(data=dict( x=df['Interest rate'],</p><p class="snippet">...</p><p class="snippet">...</p><p class="snippet">fill_color='colors', line_color='black', legend='label')</p><p class="snippet">show(p)</p><h4>Note</h4><p class="callout">For the complete code, refer to the following: <a href="https://bit.ly/2Si3K04">https://bit.ly/2Si3K04</a></p><div id="_idContainer166" class="IMG---Figure"><img src="Images/C13018_03_26.jpg" alt="Figure 3.26: Visualization obtained after assigning values to colors&#13;&#10;" width="1800" height="1013"/></div><h6>Figure 3.26: Visualization obtained after assigning values to colors</h6><p>There are some technical details that are important here. First of all, we add the colors and labels for each point to the <strong class="inline">ColumnDataSource</strong>. These are then referenced when plotting the circles by setting the <strong class="inline">fill_color</strong> and legend arguments.</p></li>
				<li>Looking for patterns, zoom in on the lower population countries:<div id="_idContainer167" class="IMG---Figure"><img src="Images/C13018_03_27.jpg" alt="Figure 3.27: A zoomed in view of the lower population countries&#13;&#10;" width="1296" height="591"/></div><h6>Figure 3.27: A zoomed in view of the lower population countries</h6><p>We can see how the dark dots are more prevalent to the right-hand side of the plot. This indicates that countries that have higher interest rates are more likely to have been recently updated.</p><p>The one data column we have not yet looked at is the year-over-year change in population. Let's visualize this compared to the interest rate and see if there is any trend. We'll also enhance the plot by setting the circle size based on the country population.</p></li>
				<li>Plot the interest rate as a function of the year-over-year population change by running the following code:<p class="snippet">source = ColumnDataSource(data=dict( x=df['Yearly Change'],</p><p class="snippet">...</p><p class="snippet">...</p><p class="snippet">p.circle('x', 'y', size=10, alpha=0.5, source=source, radius='radii')</p><p class="snippet">show(p)</p><div id="_idContainer168" class="IMG---Figure"><img src="Images/C13018_03_28.jpg" alt="Figure 3.28: Plotting interest rate as a function of YoY population change&#13;&#10;" width="1800" height="947"/></div><h6>Figure 3.28: Plotting interest rate as a function of YoY population change</h6><p>Here, we use the square root of the population for the radii, making sure to also scale down the result to a good size for the visualization.</p><p>We see a strong correlation between the year-over-year population change and the interest rate. This correlation is especially strong when we take the population sizes into account, by looking primarily at the bigger circles. Let's add a line of best fit to the plot to illustrate this correlation.</p><p>We'll use scikit-learn to create the line of best fit, using the country populations (as visualized in the preceding plot) as weights.</p></li>
				<li>Determine the line of best fit for the previously plotted relationship by running the following code:<p class="snippet">from sklearn.linear_model import LinearRegression X = df['Yearly Change'].values.reshape(-1, 1)</p><p class="snippet">y = df['Interest rate'].values</p><p class="snippet">weights = np.sqrt(df['Population'])/1e5</p><p class="snippet">lm = LinearRegression()</p><p class="snippet">lm.fit(X, y, sample_weight=weights)</p><p class="snippet">lm_x = np.linspace(X.flatten().min(), X.flatten().max(), 50)</p><p class="snippet">lm_y = lm.predict(lm_x.reshape(-1, 1))</p><p>The scikit-learn code should be familiar from earlier in this book. As promised, we are using the transformed populations, as seen in the previous plot, as the weights. The line of best fit is then calculated by predicting the linear model values for a range of x values.</p><p>To plot the line, we can reuse the preceding code, adding an extra call to the <strong class="inline">line</strong> module in Bokeh. We'll also have to set a new data source for this line.</p></li>
				<li>Re-plot the preceding figure, adding a line of best fit, by running the following code:<p class="snippet">source = ColumnDataSource(data=dict( x=df['Yearly Change'], y=df['Interest rate'],</p><p class="snippet">...</p><p class="snippet">...</p><p class="snippet">p.line('x', 'y', line_width=2, line_color='red', source=lm_source)</p><p class="snippet">show(p)</p><div id="_idContainer169" class="IMG---Figure"><img src="Images/C13018_03_29.jpg" alt="Figure 3.29: Adding a best fit line to the plot of YoY population change and interest rates&#13;&#10;" width="1800" height="1034"/></div><h6>Figure 3.29: Adding a best fit line to the plot of YoY population change and interest rates</h6><p>For the line source, <strong class="inline">lm_source</strong>, we include <strong class="inline">N/A</strong> as the country name and population, as these are not applicable values for the line of best fit. As can be seen by hovering over the line, they indeed appear in the tooltip.</p><p>The interactive nature of this visualization gives us a unique opportunity to explore outliers in this dataset, for example, the tiny dot in the lower-right corner.</p></li>
				<li>Explore the plot by using the zoom tools and hovering over interesting samples. Note the following:<p>Ukraine has an unusually high interest rate, given the low year-over-year population change:</p></li>
			</ol>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="Images/C13018_03_30.jpg" alt="Figure 3.30: Using the Zoom tool to explore the data for Ukraine&#13;&#10;" width="1800" height="862"/>
				</div>
			</div>
			<h6>Figure 3.30: Using the Zoom tool to explore the data for Ukraine</h6>
			<p>The small country of Bahrain has an unusually low interest rate, given the high year-over-year population change:</p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="Images/C13018_03_31.jpg" alt="Figure 3.31: Using the Zoom tool to explore the data for Bahrain" width="1800" height="841"/>
				</div>
			</div>
			<h6>Figure 3.31: Using the Zoom tool to explore the data for Bahrain</h6>
		</div>
		<div>
			<div id="_idContainer173" class="Content">
			</div>
		</div>
	</div>
<div id="sbo-rt-content"><nav id="toc" epub:type="toc">
		<h2>Contents</h2>
			<ol>
				<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-1">Preface</a>
					<ol>
						<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-2">About the Book</a>
							<ol>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-3">About the Author</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-4">Objectives</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-5">Audience</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-6">Approach</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-7">Minimum Hardware Requirements</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-8">Software Requirements</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-9">Installation and Setup</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-10">Installing Anaconda</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-11">Updating Jupyter and Installing Dependencies</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-12">Additional Resources</a></li>
								<li><a href="C13018_Preface_comm_Final.xhtml#_idParaDest-13">Conventions</a></li>
							</ol>
						</li>
					</ol>
				</li>
				<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-14">Jupyter Fundamentals</a>
					<ol>
						<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-15">Introduction</a></li>
						<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-16">Basic Functionality and Features</a>
							<ol>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-17">What is a Jupyter Notebook and Why is it Useful?</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-18">Navigating the Platform</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-19">Exercise 1: Introducing Jupyter Notebooks</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-20">Jupyter Features</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-21">Exercise 2: Implementing Jupyter's Most Useful Features</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-22">Converting a Jupyter Notebook to a Python Script</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-23">Python Libraries</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-24">Exercise 3: Importing the External Libraries and Setting Up the Plotting Environment</a></li>
							</ol>
						</li>
						<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-25">Our First Analysis - The Boston Housing Dataset</a>
							<ol>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-26">Loading the Data into Jupyter Using a Pandas DataFrame</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-27">Exercise 4: Loading the Boston Housing Dataset</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-28">Data Exploration</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-29">Exercise 5: Analyzing the Boston Housing Dataset</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-30">Introduction to Predictive Analytics with Jupyter Notebooks</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-31">Exercise 6: Applying Linear Models With Seaborn and Scikit-learn</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-32">Activity 1: Building a Third-Order Polynomial Model</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-33">Using Categorical Features for Segmentation Analysis</a></li>
								<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-34">Exercise 7: Creating Categorical Fields From Continuous Variables and Make Segmented Visualizations</a></li>
							</ol>
						</li>
						<li><a href="C13018_1_comm_Final.xhtml#_idParaDest-35">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-36">Data Cleaning and Advanced Machine Learning</a>
					<ol>
						<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-37">Introduction</a></li>
						<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-38">Preparing to Train a Predictive Model</a>
							<ol>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-39">Determining a Plan for Predictive Analytics</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-40">Exercise 8: Explore Data Preprocessing Tools and Methods</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-41">Activity 2: Preparing to Train a Predictive Model for the Employee-Retention Problem</a></li>
							</ol>
						</li>
						<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-42">Training Classification Models</a>
							<ol>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-43">Introduction to Classification Algorithms</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-44">Exercise 9: Training Two-Feature Classification Models With Scikit-learn</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-45">The plot_decision_regions Function</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-46">Exercise 10: Training K-nearest Neighbors for Our Model</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-47">Exercise 11: Training a Random Forest</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-48">Assessing Models With K-fold Cross-Validation and Validation Curves</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-49">Exercise 12: Using K-fold Cross Validation and Validation Curves in Python With Scikit-learn</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-50">Dimensionality Reduction Techniques</a></li>
								<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-51">Exercise 13: Training a Predictive Model for the Employee Retention Problem</a></li>
							</ol>
						</li>
						<li><a href="C13018_2_comm_Final.xhtml#_idParaDest-52">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-53">Web Scraping and Interactive Visualizations</a>
					<ol>
						<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-54">Introduction</a></li>
						<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-55">Scraping Web Page Data</a>
							<ol>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-56">Introduction to HTTP Requests</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-57">Making HTTP Requests in the Jupyter Notebook</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-58">Exercise 14: Handling HTTP Requests With Python in a Jupyter Notebook</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-59">Parsing HTML in the Jupyter Notebook</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-60">Exercise 15: Parsing HTML With Python in a Jupyter Notebook</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-61">Activity 3: Web Scraping With Jupyter Notebooks</a></li>
							</ol>
						</li>
						<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-62">Interactive Visualizations</a>
							<ol>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-63">Building a DataFrame to Store and Organize Data</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-64">Exercise 16: Building and Merging Pandas DataFrames</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-65">Introduction to Bokeh</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-66">Exercise 17: Introduction to Interactive Visualization With Bokeh</a></li>
								<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-67">Activity 4: Exploring Data with Interactive Visualizations</a></li>
							</ol>
						</li>
						<li><a href="C13018_3_comm_Final.xhtml#_idParaDest-68">Summary</a></li>
					</ol>
				</li>
				<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-69">Appendix A</a>
					<ol>
						<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-70">Chapter 1: Jupyter Fundamentals</a>
							<ol>
								<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-71">Activity 1: Building a Third-Order Polynomial Model</a></li>
							</ol>
						</li>
						<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-72">Chapter 2: Data Cleaning and Advanced Machine</a>
							<ol>
								<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-73">Activity 2: Preparing to Train a Predictive Model for the Employee-Retention Problem</a></li>
							</ol>
						</li>
						<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-74">Chapter 3: Web Scraping and Interactive Visualizations</a>
							<ol>
								<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-75">Activity 3: Web Scraping with Jupyter Notebooks</a></li>
								<li><a href="C13018_Solutions_Final.xhtml#_idParaDest-76">Activity 4: Exploring Data with Interactive Visualizations</a></li>
							</ol>
						</li>
					</ol>
				</li>
			</ol>
		</nav>
		<nav epub:type="landmarks">
		<h2>Landmarks</h2>
			<ol>
				<li><a epub:type="cover" href="Images/cover.xhtml">Cover</a></li>
				<li><a epub:type="toc" href="C13018_Credits_comm_Final.xhtml#_idContainer004">Table of Contents</a></li>
			</ol>
		</nav>
	</div></body></html>