<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Jupyter and Big Data</h1>
                </header>
            
            <article>
                
<p>Big data is the topic on everyone's mind. I thought it would be good to see what can be done with big data in Jupyter. One up-and-coming language for dealing with large datasets is Spark. Spark is an open source toolset. We can use Spark coding in Jupyter much like the other languages we have seen.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li> Installing Spark for use in Jupyter</li>
<li> Using Spark's features</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Spark</h1>
                </header>
            
            <article>
                
<p><span>One of the tools we will be using is Apache Spark. Spark is an open source toolset for cluster computing. While we will not be using a cluster, typical usage for Spark is a larger set of machines or clusters that operate in parallel to analyze a big dataset. Installation instructions are available at <a href="https://www.dataquest.io/blog/pyspark-installation-guide">https://www.dataquest.io/blog/pyspark-installation-guide</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Spark on macOS</h1>
                </header>
            
            <article>
                
<p><span>Up-to-date instructions for installing Spark are available at <a href="https://medium.freecodecamp.org/installing-scala-and-apache-spark-on-mac-os-837ae57d283f">https://medium.freecodecamp.org/installing-scala-and-apache-spark-on-mac-os-837ae57d283f</a>. The main steps are:</span></p>
<ol>
<li> <span>Get Homebrew from <a href="http://brew.sh">http://brew.sh</a>. </span>If you are doing software development on macOS, you will likely already have Homebrew.</li>
<li>Install <kbd>xcode-select</kbd>: <kbd>xcode-select</kbd> <span>is used for different languages. For Spark we use Java, Scala and, of course, Spark as follows:</span></li>
</ol>
<pre style="padding-left: 90px"><strong>xcode-select -install</strong> </pre>
<p style="padding-left: 90px">Again, it is likely that you will already have this for other software development tasks.</p>
<ol start="3">
<li> Use Homebrew to install Java:</li>
</ol>
<pre style="padding-left: 60px"><strong>brew cask install java</strong> </pre>
<ol start="4">
<li>Use Homebrew to install Scala:</li>
</ol>
<pre style="padding-left: 60px"><strong>brew install scala</strong> </pre>
<ol start="5">
<li>Use Homebrew to install Apache Spark:</li>
</ol>
<pre style="padding-left: 60px"><strong>brew install apache-spark</strong> </pre>
<ol start="6">
<li>You should test whether this is working using the Spark shell, as in this command:</li>
</ol>
<pre style="padding-left: 60px"><strong>spark-shell</strong> </pre>
<p class="mce-root"/>
<ol start="7">
<li>This will result in the familiar logo display:</li>
</ol>
<pre style="padding-left: 60px"><strong>Welcome to</strong> 
<strong>      ____              __</strong> 
<strong>     / __/__  ___ _____/ /__</strong> 
<strong>    _\ \/ _ \/ _ `/ __/  '_/</strong> 
<strong>   /__ / .__/\_,_/_/ /_/\_\   version 2.0.0</strong> 
<strong>      /_/</strong> 
 
<strong>Using Python version 2.7.12 (default, Jul  2 2016 17:43:17)</strong> 
<strong>SparkSession available as 'spark'.</strong> 
<strong>&gt;&gt;&gt;</strong> </pre>
<p>The site continues on to talk about setting up exports and the like, but I did not need to do this.</p>
<p>At this point, we can bring up a Python 3 Notebook and start using Spark in Jupyter.</p>
<p>You can type <kbd><span class="CodeHighlightedPACKT">quit()</span></kbd> to exit.</p>
<p>Now, when we run our Notebook while using a Python kernel, we can access Spark.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Windows install</h1>
                </header>
            
            <article>
                
<p>I had run Spark in Python 2 in previous of Jupyter. I could not get the installs to work correctly for Windows.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">First Spark script</h1>
                </header>
            
            <article>
                
<p>Our first script reads in a text file and sees how much the line lengths add up to, as shown next. Note that we are reading in the Notebook file we are running; the Notebook is named <kbd>Spark File Lengths</kbd>, and is stored in the <kbd>Spark File Lengths.ipynb</kbd> file:</p>
<pre>import pyspark<br/>if not 'sc' in globals():<br/>    sc = pyspark.SparkContext()<br/>lines = sc.textFile("Spark File Line Lengths.ipynb")<br/>lineLengths = lines.map(lambda s: len(s))<br/>totalLengths = lineLengths.reduce(lambda a, b: a + b)<br/>print(totalLengths)</pre>
<p class="mce-root"/>
<p>In the <kbd><span>print(totalLengths)</span></kbd> <span>script, </span>we first initialize Spark, but only if we have not done so already. Spark will complain if you try to initialize it more than once, so all Spark scripts should have this <kbd><span class="CodeHighlightedPACKT">if</span></kbd> statement prefix.</p>
<p>The script reads in a text file (the source of this script), takes every line and computes its length, and then adds all the lengths together.</p>
<p>A <kbd><span class="CodeHighlightedPACKT">lambda</span></kbd> function is an anonymous (not named) function that takes arguments and returns a value. In the first case, given a <span class="CodeHighlightedPACKT"><kbd>s</kbd> string</span> return its length.</p>
<p>A <kbd><span class="CodeHighlightedPACKT">reduce</span></kbd> function takes each value as an argument, applies the second argument to it, replaces the first value with the result, and then proceeds with the rest of the list. In our case, it walks through the line lengths and adds them all up.</p>
<p>Then, running this in a Notebook, we see the following screenshot. Note that the size of your file may be slightly different.</p>
<p>Also, the first time you begin the Spark engine (using the line <kbd><span class="CodeHighlightedPACKT">sc = pyspark.SparkContext()</span></kbd>), it may take a while and your script may not complete successfully. If that happens, just try it again:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/25b44c28-0313-4131-8024-e3d8b4e3a4f5.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark word count</h1>
                </header>
            
            <article>
                
<p>Now that we have seen some of the functionality, let's explore further. We can use a script <span>similar to the following </span>to count the word occurrences in a file:</p>
<pre>import pyspark<br/>if not 'sc' in globals():<br/>    sc = pyspark.SparkContext()<br/>    <br/>#load in the file<br/>text_file = sc.textFile("Spark File Words.ipynb")<br/><br/>#split file into distinct words<br/>counts = text_file.flatMap(lambda line: line.split(" ")) \<br/>    .map(lambda word: (word, 1)) \<br/>    .reduceByKey(lambda a, b: a + b)<br/>    <br/># print out words found<br/>for x in counts.collect():<br/>    print(x)</pre>
<p>We have the same preamble to the coding. Then, we load the text file into memory.</p>
<p>Once the file is loaded, we split each line into words and use a <kbd>lambda</kbd> function to tick off each occurrence of a word. The code is truly creating a new record for each word occurrence, such as at appears one. The idea is that this process could be split over multiple processors, where each processor generates these low-level information bits. We are not concerned with optimizing this process at all.</p>
<p>Once we have all of these records, we reduce/summarize the record set according to the word occurrences mentioned.</p>
<p>The <kbd><span class="CodeHighlightedPACKT">counts</span></kbd> <span>object is called a <strong>Resilient Distributed Dataset</strong> (<strong>RDD</strong>) in Spark. It is resilient since care is taken to persist the dataset. The RDD is distributed as it can be manipulated by all nodes in the operating cluster. And, of course, it is a dataset consisting of a variety of data items.</span></p>
<p>The last <kbd><span class="CodeHighlightedPACKT">for</span></kbd> loop runs <kbd><span class="CodeHighlightedPACKT">collect()</span></kbd> against the RDD. As mentioned, this RDD could be distributed among many nodes. The <kbd><span class="CodeHighlightedPACKT">collect()</span></kbd> function pulls all copies of the RDD into one location. Then, we loop through each record.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When we run this in Jupyter, we see something akin to this displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6a7cdef0-9bef-45ad-a289-bcfdba125d25.png"/></p>
<p>The listing is abbreviated as the list of words continues for some time. What is curious is that the word splitting logic in Spark does not appear to work very well; some of the results are not words, such as the first entry an empty string.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sorted word count</h1>
                </header>
            
            <article>
                
<p>Using the same script with a minor modification, we can make one more call and sort the results. The script now looks as follows:</p>
<pre>import pyspark<br/>if not 'sc' in globals():<br/>    sc = pyspark.SparkContext()<br/>    <br/>#load in the file<br/>text_file = sc.textFile("Spark Sort Words from File.ipynb")<br/><br/>#split file into sorted, distinct words<br/>sorted_counts = text_file.flatMap(lambda line: line.split(" ")) \<br/>    .map(lambda word: (word, 1)) \<br/>    .reduceByKey(lambda a, b: a + b) \<br/>    .sortByKey()<br/>    <br/># print out words found (in sorted order)<br/>for x in sorted_counts.collect():<br/>    print(x)</pre>
<p>Here, we have added another function call to RDD creation, <kbd><span class="CodeHighlightedPACKT">sortByKey()</span></kbd>. So, after we have mapped/reduced, and arrived at a list of words and occurrences, we can then easily sort the results.</p>
<p>The resultning output looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/93cdf7dd-0eff-4bd5-bfd0-2eb6b597200e.png" style="width:43.67em;height:33.25em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Estimate pi</h1>
                </header>
            
            <article>
                
<p>We can use <kbd>map</kbd> or <kbd>reduce</kbd> to estimate pi if we have code like this:</p>
<pre>import pyspark 
import random 
if not 'sc' in globals(): 
    sc = pyspark.SparkContext() 
<strong>    </strong> 
NUM_SAMPLES = 10000 
random.seed(113) 
 
def sample(p): 
    x, y = random.random(), random.random() 
    return 1 if x*x + y*y &lt; 1 else 0 
 
count = sc.parallelize(range(0, NUM_SAMPLES)) \ 
    .map(sample) \ 
    .reduce(lambda a, b: a + b) 
     
print("Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)) </pre>
<p>This code has the same preamble. We are using the Python <kbd><span class="CodeHighlightedPACKT">random</span></kbd> package. There is a constant for the number of samples to attempt.</p>
<p>We are building an RDD called <span class="CodeHighlightedPACKT"><kbd>count</kbd></span>. We call the <kbd><span class="CodeHighlightedPACKT">parallelize</span></kbd> function to split this process between the nodes available. The code just maps the result of the <kbd><span class="CodeHighlightedPACKT">sample</span></kbd> function call. Finally, we reduce the generated map set by adding all the samples.</p>
<p>The <kbd><span class="CodeHighlightedPACKT">sample</span></kbd> function gets two random numbers and returns a one or a zero depending on where the two numbers end up in size. We are looking for random numbers in a small range and then checking whether they occur within a circle of the same diameter. With a large enough sample, we would end up with <kbd>PI(3.141...)</kbd>.</p>
<p>If we run this in Jupyter, we see the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/23444f8f-7695-43d8-b936-47962275bb0a.png" style="width:42.92em;height:27.75em;"/></p>
<p class="mce-root">When I ran this with <kbd><span class="CodeHighlightedPACKT">NUM_SAMPLES = 100000</span></kbd>, I ended up with <kbd>PI = 3.126400</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Log file examination</h1>
                </header>
            
            <article>
                
<p>I downloaded one of the <kbd>access_log</kbd> files from <kbd>monitorware.com</kbd>. Like any other web access log, we have one line per entry, like this:</p>
<pre style="padding-left: 60px">64.242.88.10 - - [07/Mar/2004:16:05:49 -0800] "GET /twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables HTTP/1.1" 401 12846 </pre>
<p>The first part is the IP address of the caller, followed by a timestamp, the type of HTTP access, the URL referenced, the HTTP type, the resulting HTTP response code, and finally the number of bytes in the response.</p>
<p>We can use Spark to load in and parse out some statistics of the log entries, as in this script:</p>
<pre style="padding-left: 90px">import pyspark<br/>if not 'sc' in globals():<br/>    sc = pyspark.SparkContext()<br/>    <br/>textFile = sc.textFile("access_log")<br/>print(textFile.count(), "access records")<br/><br/>gets = textFile.filter(lambda line: "GET" in line)<br/>print(gets.count(), "GETs")<br/><br/>posts = textFile.filter(lambda line: "POST" in line)<br/>print(posts.count(), "POSTs")<br/><br/>other = textFile.subtract(gets).subtract(posts)<br/>print(other.count(), "Other")<br/><br/>#curious what Other requests may have been<br/>for x in other.collect():<br/>    print(x)</pre>
<p>This script has the same preamble as the others. We read in the <kbd>access_log</kbd> file. Then, we print the <kbd>count</kbd> record.</p>
<p>Similarly, we find out how many log entries were <kbd>GET</kbd> and <kbd>POST</kbd> operations. <kbd>GET</kbd> is assumed to be the most prevalent.</p>
<p>When I first did this, I really didn't expect anything else, so I removed <kbd>gets</kbd> and <kbd>posts</kbd> from the set and printed out the outliers to see what they were.</p>
<p class="mce-root"/>
<p>When we run this in Jupyter, we see the expected output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6fb013de-6368-4a87-8871-32ed6e0fc998.png" style="width:46.33em;height:48.25em;"/></p>
<p>The text processing was not very fast (especially for so few records).</p>
<p>I liked being able to work with the data frames in such a way. There is something pleasing about being able to do basic algebra with sets in a programmatic way, without having to be concerned about edge cases.</p>
<p>By the way, a <kbd>HEAD</kbd> request works just like  <kbd>GET</kbd>, but does not return the <kbd>HTTP</kbd> body. This allows a caller to determine what kind of response would have come back and respond appropriately.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark primes</h1>
                </header>
            
            <article>
                
<p>We can run a series of numbers through a <span class="CodeHighlightedPACKT">filter</span> to determine whether each number is prime or not. We can use this script:</p>
<pre>import pyspark<br/>if not 'sc' in globals():<br/>    sc = pyspark.SparkContext()<br/>    <br/>def is_it_prime(number):<br/>    <br/>    #make sure n is a positive integer<br/>    number = abs(number)<br/>    <br/>    #simple tests<br/>    if number &lt; 2:<br/>        return False<br/>    <br/>    #2 is special case<br/>    if number == 2:<br/>        return True<br/>    <br/>    #all other even numbers are not prime<br/>    if not number &amp; 1:<br/>        return False<br/>    <br/>    #divisible into it's square root<br/>    for x in range(3, int(number**0.5)+1, 2):<br/>        if number % x == 0:<br/>            return False<br/>        <br/>    #must be a prime<br/>    return True<br/><br/># pick a good range<br/>numbers = sc.parallelize(range(100000))<br/><br/># see how many primes are in that range<br/>print(numbers.filter(is_it_prime).count())</pre>
<p>The script generates numbers up to <kbd>100000</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We then loop over each of the numbers and pass it to our filter. If the filter returns <kbd>True</kbd>, we get a record. Then, we just count how many results we found.</p>
<p>Running this in Jupyter, we see the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/17a3cee1-e1b9-4f85-90d9-12257698f786.png"/></p>
<p>This was very fast. I was waiting and didn't notice that it went so quickly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark text file analysis</h1>
                </header>
            
            <article>
                
<p>In this example, we will look through a news article to determine some basic information from the article.</p>
<p>We will be using the following script against the 2600 raid news article from <a href="https://www.newsitem.com" target="_blank">https://www.newsitem.com</a>:</p>
<pre>import pyspark<br/>if not 'sc' in globals():<br/>    sc = pyspark.SparkContext()<br/><br/>#pull out sentences from article<br/>sentences = sc.textFile('2600raid.txt') \<br/>    .glom() \<br/>    .map(lambda x: " ".join(x)) \<br/>    .flatMap(lambda x: x.split("."))<br/>print(sentences.count(),"sentences")<br/><br/>#find the bigrams in the sentences<br/>bigrams = sentences.map(lambda x:x.split()) \<br/>    .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0, len(x)-1)])<br/>print(bigrams.count(),"bigrams")<br/><br/>#find the (10) most common bigrams<br/>frequent_bigrams = bigrams.reduceByKey(lambda x,y:x+y) \<br/>    .map(lambda x:(x[1], x[0])) \<br/>    .sortByKey(False)<br/>frequent_bigrams.take(10)</pre>
<p>The code reads in the article and splits it into <kbd>sentences</kbd>, as determined by the appearance of a period. From there, the code maps out the <kbd>bigrams</kbd> present. A bigram is a pair of words that appear next to each other. We then sort the list and print out the top ten most prevalent.</p>
<p class="mce-root"/>
<p>When we run this in a Notebook, we see these results:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/043463c5-8d7b-4b08-a66f-d04458eb481e.png"/></p>
<p>I really had no idea what to expect from the output. It's curious that you can glean some insight into the article as <kbd>the</kbd> and <kbd>mall</kbd> appear <kbd>15</kbd> times and <kbd>the</kbd> and <kbd>guards</kbd> appear <kbd>11</kbd> times—a raid must have occurred in a mall and included the security guards in some manner.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark evaluating history data</h1>
                </header>
            
            <article>
                
<p>In this example, we combine the previous sections to look at some historical data and determine a number of useful attributes.</p>
<p>The historical data we are using is the guest list for the Jon Stewart television show. A typical record from the data looks as follows:</p>
<pre>1999,actor,1/11/99,Acting,Michael J. Fox </pre>
<p>This contains the year, the occupation of the guest, the date of appearance, a logical grouping of the occupations, and the name of the guest.</p>
<p>For our analysis, we will be looking at the number of appearances per year, the occupation that appears most frequently, and the personality who appears most frequently.</p>
<p>We will be using this script:</p>
<pre>#Spark Daily Show Guests<br/>import pyspark<br/>import csv<br/>import operator<br/>import itertools<br/>import collections<br/><br/>if not 'sc' in globals():<br/> sc = pyspark.SparkContext()<br/> <br/>years = {}<br/>occupations = {}<br/>guests = {}<br/><br/>#file header contains column descriptors:<br/>#YEAR, GoogleKnowledge_Occupation, Show, Group, Raw_Guest_List<br/><br/>with open('daily_show_guests.csv', 'rt', errors = 'ignore') as csvfile: <br/> reader = csv.DictReader(csvfile)<br/> for row in reader:<br/> year = row['YEAR']<br/> if year in years:<br/> years[year] = years[year] + 1<br/> else:<br/> years[year] = 1<br/> <br/> occupation = row['GoogleKnowlege_Occupation']<br/> if occupation in occupations:<br/> occupations[occupation] = occupations[occupation] + 1<br/> else:<br/> occupations[occupation] = 1<br/> <br/> guest = row['Raw_Guest_List']<br/> if guest in guests:<br/> guests[guest] = guests[guest] + 1<br/> else:<br/> guests[guest] = 1<br/> <br/>#sort for higher occurrence<br/>syears = sorted(years.items(), key = operator.itemgetter(1), reverse = True)<br/>soccupations = sorted(occupations.items(), key = operator.itemgetter(1), reverse = True)<br/>sguests = sorted(guests.items(), key = operator.itemgetter(1), reverse = True)<br/><br/>#print out top 5's<br/>print(syears[:5])<br/>print(soccupations[:5])<br/>print(sguests[:5]) </pre>
<p>The script has a number of features:</p>
<ul>
<li> We are using several packages.</li>
<li> It has the familiar context preamble.</li>
<li> We start dictionaries for <kbd>years</kbd>, <kbd>occupations</kbd>, and <kbd>guests</kbd>. A dictionary contains <kbd>key</kbd> and <kbd>value</kbd>. For this use, the key will be the raw value from the CSV. The value will be the number of occurrences in the dataset.</li>
<li> We open the file and start reading line by line, using a <kbd>reader</kbd> object. We are using ignore errors as there are a couple of nulls in the file.</li>
<li>On each line, we take the value of interest (<kbd>year</kbd>, <kbd>occupation</kbd>, <kbd>name</kbd>).</li>
<li> We see if the value is present in the appropriate dictionary.</li>
<li> If it is there, increment the value (counter).</li>
<li>Otherwise, initialize an entry in the dictionary.</li>
<li> We then sort each of the dictionaries in reverse order of the number of appearances of the item.</li>
<li> Finally, we display the top five values for each dictionary.</li>
</ul>
<p class="mce-root"/>
<p>If we run this in a Notebook, we have the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/db77a6e1-123b-46a5-bf85-43732af2ed73.png"/></p>
<p>We show the tail of the script and the preceding output.</p>
<p>There may be a smarter way to do all of this, but I am not aware of it.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The build-up of the accumulators is pretty standard, regardless of what language you are using. I think there is an opportunity to use a <kbd>map()</kbd> function here.</p>
<p>I really liked just trimming off the lists/arrays so easily instead of having to call a function.</p>
<p>The number of guests per year is very consistent. Actors are prevalent—probably the group of people of most interest to the audience. The guest list was a little surprising. The guests are mostly actors, but I think all have strong political directions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we used Spark functionality via Python coding for Jupyter. First, we installed the Spark additions to Jupyter. We wrote an initial script that just read lines from a text file. We went further and determined the word counts in that file. We added sorting to the results. We wrote was a script to estimate pi. We evaluated web log files for anomalies. We determined a set of prime numbers, and we evaluated a text stream for certain characteristics.</p>


            </article>

            
        </section>
    </body></html>