<html><head></head><body>
        <section id="75QNI1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Introduce a Little Structure - Spark SQL</h1>
                
            
            <article>
                
<div class="book-info-bottom-author-body">"One machine can do the work of fifty ordinary men. No machine can do the work of one extraordinary man."</div>
<p class="cdpalignright">- Elbert Hubbard</p>
<p class="mce-root">In this chapter, you will learn how to use Spark for the analysis of structured data (unstructured data, such as a document containing arbitrary text or some other format has to be transformed into a structured form); we will see how DataFrames/datasets are the corner stone here, and how Spark SQL's APIs make querying structured data simple yet robust. Moreover, we introduce datasets and see the difference between datasets, DataFrames, and RDDs. In a nutshell, the following topics will be covered in this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Spark SQL and DataFrames</li>
<li class="mce-root1">DataFrame and SQL API</li>
<li class="mce-root1">DataFrame schema</li>
<li class="mce-root1">datasets and encoders</li>
<li class="mce-root1">Loading and saving data</li>
<li class="mce-root1">Aggregations</li>
<li class="mce-root1">Joins</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header id="76P842-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">Spark SQL and DataFrames</h1>
                
            
            <article>
                
<p class="mce-root">Before Apache Spark, Apache Hive was the go-to technology whenever anyone wanted to run an SQL-like query on a large amount of data. Apache Hive essentially translated SQL queries into MapReduce-like, like logic, automatically making it very easy to perform many kinds of analytics on big data without actually learning to write complex code in Java and Scala.</p>
<p class="mce-root">With the advent of Apache Spark, there was a paradigm shift in how we can perform analysis on big data scale. Spark SQL provides an easy-to-use SQL-like layer on top of Apache Spark's distributed computation abilities. In fact, Spark SQL can be used as an online analytical processing database.</p>
<div class="cdpaligncenter"><img class="image-border91" src="../images/00297.jpeg"/></div>
<p class="mce-root">Spark SQL works by parsing the SQL-like statement into an <strong class="calibre1">Abstract Syntax Tree</strong> (<strong class="calibre1">AST</strong>), subsequently converting that plan to a logical plan and then optimizing the logical plan into a physical plan that can be executed. The final execution uses the underlying DataFrame API, making it very easy for anyone to use DataFrame APIs by simply using an SQL-like interface rather than learning all the internals. Since this book dives into technical details of various APIs, we will primarily cover the DataFrame APIs, showing Spark SQL API in some places to contrast the different ways of using the APIs.</p>
<p class="mce-root">Thus, DataFrame API is the underlying layer beneath Spark SQL. In this chapter, we will show you how to create DataFrames using various techniques, including SQL queries and performing operations on the DataFrames.</p>
<p class="mce-root">A DataFrame is an abstraction of the <strong class="calibre1">Resilient Distributed dataset</strong> (<strong class="calibre1">RDD</strong>), dealing with higher level functions optimized using catalyst optimizer and also highly performant via the Tungsten Initiative. You can think of a dataset as an efficient table of an RDD with heavily optimized binary representation of the data. The binary representation is achieved using encoders, which serializes the various objects into a binary structure for much better performance than RDD representation. Since DataFrames uses the RDD internally anyway, a DataFrame/dataset is also distributed exactly like an RDD, and thus is also a distributed dataset. Obviously, this also means datasets are immutable.</p>
<p class="mce-root">The following is an illustration of the binary representation of data:</p>
<div class="cdpaligncenter"><img class="image-border92" src="../images/00037.jpeg"/></div>
<p class="mce-root">datasets were added in Spark 1.6 and provide the benefits of strong typing on top of DataFrames. In fact, since Spark 2.0, the DataFrame is simply an alias of a dataset.</p>
<div class="packt_infobox"><kbd class="calibre22">org.apache.spark.sql</kbd> defines type <kbd class="calibre22">DataFrame</kbd> as a <kbd class="calibre22">dataset[Row]</kbd>, which means that most of the APIs will work well with both datasets and <kbd class="calibre22">DataFrames</kbd><br class="calibre23"/>
<strong class="calibre27">type DataFrame = dataset[Row]</strong></div>
<p class="mce-root">A DataFrame is conceptually similar to a table in a Relational Database. Hence, a DataFrame contains rows of data, with each row comprised of several columns.</p>
<p class="mce-root">One of the first things we need to keep in mind is that, just like RDDs, DataFrames are immutable. This property of DataFrames being immutable means every transformation or action creates a new DataFrame.</p>
<div class="cdpaligncenter"><img class="image-border93" src="../images/00034.jpeg"/></div>
<p class="mce-root">Let's start looking more into DataFrames and how they are different from RDDs. RDD's, as seen before, represent a low-level API of data manipulation in Apache Spark. The DataFrames were created on top of RDDs to abstract the low-level inner workings of RDDs and expose high-level APIs, which are easier to use and provide a lot of functionality out-of-the-box. DataFrame was created by following similar concepts found in the Python pandas package, R language, Julia language, and so on.</p>
<p class="mce-root">As we mentioned before, DataFrames translate the SQL code and domain specific language expressions into optimized execution plans to be run on top of Spark Core APIs in order for the SQL statements to perform a wide variety of operations. DataFrames support many different types of input data sources and many types of operations. These includes all types of SQL operations, such as joins, group by, aggregations, and window functions, as most of the databases. Spark SQL is also quite similar to the Hive query language, and since Spark provides a natural adapter to Apache Hive, users who have been working in Apache Hive can easily transfer their knowledge, applying it to Spark SQL, thus minimizing the transition time.</p>
<p class="mce-root">DataFrames essentially depend on the concept of a table, as seen previously. The table can be operated on very similar to how Apache Hive works. In fact, many of the operations on the tables in Apache Spark are similar to how Apache Hive handles tables and operates on those tables. Once you have a table that is the DataFrame, the DataFrame can be registered as a table and you can operate on the data using Spark SQL statements in lieu of DataFrame APIs.</p>
<p class="mce-root">DataFrames depend on the catalyst optimizer and the Tungsten performance improvements, so let's briefly examine how catalyst optimizer works. A catalyst optimizer creates a parsed logical plan from the input SQL and then analyzes the logical plan by looking at all the various attributes and columns used in the SQL statement. Once the analyzed logical plan is created, catalyst optimizer further tries to optimize the plan by combining several operations and also rearranging the logic to get better performance.</p>
<div class="packt_infobox">In order to understand the catalyst optimizer, think about it as a common sense logic Optimizer which can reorder operations such as filters and transformations, sometimes grouping several operations into one so as to minimize the amount of data that is shuffled across the worker nodes. For example, catalyst optimizer may decide to broadcast the smaller datasets when performing joint operations between different datasets. Use explain to look at the execution plan of any data frame. The catalyst optimizer also computes statistics of the DataFrame's columns and partitions, improving the speed of execution.</div>
<p class="mce-root">For example, if there are transformations and filters on the data partitions, then the order in which we filter data and apply transformations matters a lot to the overall performance of the operations. As a result of all the optimizations, the optimized logical plan is generated, which is then converted into a physical plan. Obviously, several physical plans are possibilities to execute the same SQL statement and generate the same result. The cost optimization logic determines and picks a good physical plan, based on cost optimizations and estimations.</p>
<p class="mce-root">Tungsten performance improvements are another key ingredient in the secret sauce behind the phenomenal performance improvements offered by Spark 2.x compared to the previous releases, such as Spark 1.6 and older. Tungsten implements a complete overhaul of memory management and other performance improvements. Most important memory management improvements use binary encoding of the objects and referencing them in both off-heap and on-heap memory. Thus, Tungsten allows the usage of office heap memory using the binary encoding mechanism to encode all the objects. Binary encoded objects take up much less memory. Project Tungsten also improves shuffle performance.</p>
<p class="mce-root">The data is typically loaded into DataFrames through the <kbd class="calibre11">DataFrameReader</kbd>, and data is saved from DataFrames through <kbd class="calibre11">DataFrameWriter</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header id="77NOM2-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">DataFrame API and SQL API</h1>
                
            
            <article>
                
<p class="mce-root">The creation of a DataFrame can be done in several ways:</p>
<ul class="calibre9">
<li class="mce-root1">By executing SQL queries</li>
<li class="mce-root1">Loading external data such as Parquet, JSON, CSV, text, Hive, JDBC, and so on</li>
<li class="mce-root1">Converting RDDs to data frames</li>
</ul>
<p class="mce-root">A DataFrame can be created by loading a CSV file. We will look at a CSV <kbd class="calibre11">statesPopulation.csv</kbd>, which is being loaded as a DataFrame.</p>
<p class="mce-root">The CSV has the following format of US states populations from years 2010 to 2016.</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">State</strong></td>
<td class="calibre7"><strong class="calibre1">Year</strong></td>
<td class="calibre7"><strong class="calibre1">Population</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">Alabama</td>
<td class="calibre7">2010</td>
<td class="calibre7">4785492</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Alaska</td>
<td class="calibre7">2010</td>
<td class="calibre7">714031</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Arizona</td>
<td class="calibre7">2010</td>
<td class="calibre7">6408312</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Arkansas</td>
<td class="calibre7">2010</td>
<td class="calibre7">2921995</td>
</tr>
<tr class="calibre6">
<td class="calibre7">California</td>
<td class="calibre7">2010</td>
<td class="calibre7">37332685</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">Since this CSV has a header, we can use it to quickly load into a DataFrame with an implicit schema detection.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val statesDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesPopulation.csv")</strong><br class="title-page-name"/>statesDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]
</pre>
<p class="mce-root">Once the DataFrame is loaded, it can be examined for the schema:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.printSchema</strong><br class="title-page-name"/>root<br class="title-page-name"/> |-- State: string (nullable = true)<br class="title-page-name"/> |-- Year: integer (nullable = true)<br class="title-page-name"/> |-- Population: integer (nullable = true)
</pre>
<div class="packt_tip"><kbd class="calibre22">option("header", "true").option("inferschema", "true").option("sep", ",")</kbd> tells Spark that the CSV has a <kbd class="calibre22">header</kbd>; a comma separator is used to separate the fields/columns and also that schema can be inferred implicitly.</div>
<p class="mce-root">DataFrame works by parsing the logical plan, analyzing the logical plan, optimizing the plan, and then finally executing the physical plan of execution.</p>
<p class="mce-root">Using explain on DataFrame shows the plan of execution:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.explain(true)</strong><br class="title-page-name"/>== Parsed Logical Plan ==<br class="title-page-name"/>Relation[State#0,Year#1,Population#2] csv<br class="title-page-name"/>== Analyzed Logical Plan ==<br class="title-page-name"/>State: string, Year: int, Population: int<br class="title-page-name"/>Relation[State#0,Year#1,Population#2] csv<br class="title-page-name"/>== Optimized Logical Plan ==<br class="title-page-name"/>Relation[State#0,Year#1,Population#2] csv<br class="title-page-name"/>== Physical Plan ==<br class="title-page-name"/>*FileScan csv [State#0,Year#1,Population#2] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/states.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;State:string,Year:int,Population:int&gt;
</pre>
<p class="mce-root">A DataFrame can also be registered as a table name (shown as follows), which will then allow you to type SQL statements like a relational Database.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.createOrReplaceTempView("states")</strong>
</pre>
<p class="mce-root">Once we have the DataFrame as a structured DataFrame or a table, we can run commands to operate on the data:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.show(5)</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; spark.sql("select * from states limit 5").show</strong><br class="title-page-name"/>+----------+----+----------+<br class="title-page-name"/>| State|Year|Population|<br class="title-page-name"/>+----------+----+----------+<br class="title-page-name"/>| Alabama|2010| 4785492|<br class="title-page-name"/>| Alaska|2010| 714031|<br class="title-page-name"/>| Arizona|2010| 6408312|<br class="title-page-name"/>| Arkansas|2010| 2921995|<br class="title-page-name"/>|California|2010| 37332685|<br class="title-page-name"/>+----------+----+----------+
</pre>
<p class="mce-root">If you see in the preceding piece of code, we have written an SQL-like statement and executed it using <kbd class="calibre11">spark.sql</kbd> API.</p>
<div class="packt_tip"><br class="calibre23"/>
Note that the Spark SQL is simply converted to the DataFrame API for execution and the SQL is only a DSL for ease of use.</div>
<p class="mce-root">Using the <kbd class="calibre11">sort</kbd> operation on the DataFrame, you can order the rows in the DataFrame by any column. We see the effect of descending <kbd class="calibre11">sort</kbd> using the <kbd class="calibre11">Population</kbd> column as follows. The rows are ordered by the Population in a descending order.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.sort(col("Population").desc).show(5)</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; spark.sql("select * from states order by Population desc limit 5").show</strong><br class="title-page-name"/>+----------+----+----------+<br class="title-page-name"/>| State|Year|Population|<br class="title-page-name"/>+----------+----+----------+<br class="title-page-name"/>|California|2016| 39250017|<br class="title-page-name"/>|California|2015| 38993940|<br class="title-page-name"/>|California|2014| 38680810|<br class="title-page-name"/>|California|2013| 38335203|<br class="title-page-name"/>|California|2012| 38011074|<br class="title-page-name"/>+----------+----+----------+
</pre>
<p class="mce-root">Using <kbd class="calibre11">groupBy</kbd> we can group the DataFrame by any column. The following is the code to group the rows by <kbd class="calibre11">State</kbd> and then add up the <kbd class="calibre11">Population</kbd> counts for each <kbd class="calibre11">State</kbd>.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.groupBy("State").sum("Population").show(5)</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; spark.sql("select State, sum(Population) from states group by State limit 5").show</strong><br class="title-page-name"/>+---------+---------------+<br class="title-page-name"/>| State|sum(Population)|<br class="title-page-name"/>+---------+---------------+<br class="title-page-name"/>| Utah| 20333580|<br class="title-page-name"/>| Hawaii| 9810173|<br class="title-page-name"/>|Minnesota| 37914011|<br class="title-page-name"/>| Ohio| 81020539|<br class="title-page-name"/>| Arkansas| 20703849|<br class="title-page-name"/>+---------+---------------+
</pre>
<p class="mce-root">Using <span><span>the <kbd class="calibre11">agg</kbd> operation,</span></span> you can perform many different operations on columns of the DataFrame, such as finding the <kbd class="calibre11">min</kbd>, <kbd class="calibre11">max</kbd>, and <kbd class="calibre11">avg</kbd> of a column. You can also perform the operation and rename the column at the same time to suit your use case.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.groupBy("State").agg(sum("Population").alias("Total")).show(5)</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; spark.sql("select State, sum(Population) as Total from states group by State limit 5").show</strong><br class="title-page-name"/>+---------+--------+<br class="title-page-name"/>| State| Total|<br class="title-page-name"/>+---------+--------+<br class="title-page-name"/>| Utah|20333580|<br class="title-page-name"/>| Hawaii| 9810173|<br class="title-page-name"/>|Minnesota|37914011|<br class="title-page-name"/>| Ohio|81020539|<br class="title-page-name"/>| Arkansas|20703849|<br class="title-page-name"/>+---------+--------+
</pre>
<p class="mce-root">Naturally, the more complicated the logic gets, the execution plan also gets more complicated. Let's look at the plan for the preceding operation of <kbd class="calibre11">groupBy</kbd> and <kbd class="calibre11">agg</kbd> API invocations to better understand what is really going on under the hood. The following is the code showing the execution plan of the group by and summation of population per <kbd class="calibre11">State</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.groupBy("State").agg(sum("Population").alias("Total")).explain(true)</strong><br class="title-page-name"/><strong class="calibre1"><br class="title-page-name"/></strong>== Parsed Logical Plan ==<br class="title-page-name"/>'Aggregate [State#0], [State#0, sum('Population) AS Total#31886]<br class="title-page-name"/>+- Relation[State#0,Year#1,Population#2] csv<br class="title-page-name"/><br class="title-page-name"/>== Analyzed Logical Plan ==<br class="title-page-name"/>State: string, Total: bigint<br class="title-page-name"/>Aggregate [State#0], [State#0, sum(cast(Population#2 as bigint)) AS Total#31886L]<br class="title-page-name"/>+- Relation[State#0,Year#1,Population#2] csv<br class="title-page-name"/><br class="title-page-name"/>== Optimized Logical Plan ==<br class="title-page-name"/>Aggregate [State#0], [State#0, sum(cast(Population#2 as bigint)) AS Total#31886L]<br class="title-page-name"/>+- Project [State#0, Population#2]<br class="title-page-name"/> +- Relation[State#0,Year#1,Population#2] csv<br class="title-page-name"/><br class="title-page-name"/>== Physical Plan ==<br class="title-page-name"/>*HashAggregate(keys=[State#0], functions=[sum(cast(Population#2 as bigint))], output=[State#0, Total#31886L])<br class="title-page-name"/>+- Exchange hashpartitioning(State#0, 200)<br class="title-page-name"/> +- *HashAggregate(keys=[State#0], functions=[partial_sum(cast(Population#2 as bigint))], output=[State#0, sum#31892L])<br class="title-page-name"/> +- *FileScan csv [State#0,Population#2] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/states.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;State:string,Population:int&gt;
</pre>
<p class="mce-root">DataFrame operations can be chained together very well so that the execution takes advantage of the cost optimization (Tungsten performance improvements and catalyst optimizer working together).</p>
<p class="mce-root">We can also chain the operations together in a single statement, as shown as follows, where we not only group the data by <kbd class="calibre11">State</kbd> column and then sum the <kbd class="calibre11">Population</kbd> value, but also sort the DataFrame by the summation column:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.groupBy("State").agg(sum("Population").alias("Total")).sort(col("Total").desc).show(5)</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; spark.sql("select State, sum(Population) as Total from states group by State order by Total desc limit 5").show</strong><br class="title-page-name"/>+----------+---------+<br class="title-page-name"/>| State| Total|<br class="title-page-name"/>+----------+---------+<br class="title-page-name"/>|California|268280590|<br class="title-page-name"/>| Texas|185672865|<br class="title-page-name"/>| Florida|137618322|<br class="title-page-name"/>| New York|137409471|<br class="title-page-name"/>| Illinois| 89960023|<br class="title-page-name"/>+----------+---------+
</pre>
<p class="mce-root">The preceding chained operation consists of multiple transformations and actions, which can be visualized using the following diagram:</p>
<div class="cdpaligncenter"><img class="image-border94" src="../images/00042.jpeg"/></div>
<p class="mce-root">It's also possible to create multiple aggregations at the same time, as follows:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.groupBy("State").agg(<br class="title-page-name"/>             min("Population").alias("minTotal"), <br class="title-page-name"/>             max("Population").alias("maxTotal"),        <br class="title-page-name"/>             avg("Population").alias("avgTotal"))<br class="title-page-name"/>           .sort(col("minTotal").desc).show(5)<br class="title-page-name"/></strong><br class="title-page-name"/><strong class="calibre1">scala&gt; spark.sql("select State, min(Population) as minTotal, max(Population) as maxTotal, avg(Population) as avgTotal from states group by State order by minTotal desc limit 5").show</strong><br class="title-page-name"/>+----------+--------+--------+--------------------+<br class="title-page-name"/>| State|minTotal|maxTotal| avgTotal|<br class="title-page-name"/>+----------+--------+--------+--------------------+<br class="title-page-name"/>|California|37332685|39250017|3.8325798571428575E7|<br class="title-page-name"/>| Texas|25244310|27862596| 2.6524695E7|<br class="title-page-name"/>| New York|19402640|19747183| 1.962992442857143E7|<br class="title-page-name"/>| Florida|18849098|20612439|1.9659760285714287E7|<br class="title-page-name"/>| Illinois|12801539|12879505|1.2851431857142856E7|<br class="title-page-name"/>+----------+--------+--------+--------------------+
</pre>


            </article>

            
        </section>
    

        <section id="78M981-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Pivots</h1>
                
            
            <article>
                
<p class="mce-root">Pivoting is a great way of transforming the table to create a different view, more suitable to doing many summarizations and aggregations. This is accomplished by taking the values of a column and making each of the values an actual column.</p>
<p class="mce-root">To understand this better, let's pivot the rows of the DataFrame by <kbd class="calibre11">Year</kbd> and examine the result, which shows that, now, the column <kbd class="calibre11">Year</kbd> created several new columns by converting each unique value into an actual column. The end result of this is that, now, instead of just looking at year columns, we can use the per year columns created to summarize and aggregate by <kbd class="calibre11">Year</kbd>.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.groupBy("State").pivot("Year").sum("Population").show(5)</strong><br class="title-page-name"/>+---------+--------+--------+--------+--------+--------+--------+--------+<br class="title-page-name"/>| State| 2010| 2011| 2012| 2013| 2014| 2015| 2016|<br class="title-page-name"/>+---------+--------+--------+--------+--------+--------+--------+--------+<br class="title-page-name"/>| Utah| 2775326| 2816124| 2855782| 2902663| 2941836| 2990632| 3051217|<br class="title-page-name"/>| Hawaii| 1363945| 1377864| 1391820| 1406481| 1416349| 1425157| 1428557|<br class="title-page-name"/>|Minnesota| 5311147| 5348562| 5380285| 5418521| 5453109| 5482435| 5519952|<br class="title-page-name"/>| Ohio|11540983|11544824|11550839|11570022|11594408|11605090|11614373|<br class="title-page-name"/>| Arkansas| 2921995| 2939493| 2950685| 2958663| 2966912| 2977853| 2988248|<br class="title-page-name"/>+---------+--------+--------+--------+--------+--------+--------+--------+
</pre>


            </article>

            
        </section>
    

        <section id="79KPQ1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Filters</h1>
                
            
            <article>
                
<p class="mce-root">DataFrame also supports Filters, which can be used to quickly filter the DataFrame rows to generate new DataFrames. The Filters enable very important transformations of the data to narrow down the DataFrame to our use case. For example, if all you want is to analyze the state of California, then using <kbd class="calibre11">filter</kbd> API performs the elimination of non-matching rows on every partition of data, thus improving the performance of the operations.</p>
<p class="mce-root">Let's look at the execution plan for the filtering of the DataFrame to only consider the state of California.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.filter("State == 'California'").explain(true)</strong><br class="title-page-name"/><br class="title-page-name"/>== Parsed Logical Plan ==<br class="title-page-name"/>'Filter ('State = California)<br class="title-page-name"/>+- Relation[State#0,Year#1,Population#2] csv<br class="title-page-name"/><br class="title-page-name"/>== Analyzed Logical Plan ==<br class="title-page-name"/>State: string, Year: int, Population: int<br class="title-page-name"/>Filter (State#0 = California)<br class="title-page-name"/>+- Relation[State#0,Year#1,Population#2] csv<br class="title-page-name"/><br class="title-page-name"/>== Optimized Logical Plan ==<br class="title-page-name"/>Filter (isnotnull(State#0) &amp;&amp; (State#0 = California))<br class="title-page-name"/>+- Relation[State#0,Year#1,Population#2] csv<br class="title-page-name"/><br class="title-page-name"/>== Physical Plan ==<br class="title-page-name"/>*Project [State#0, Year#1, Population#2]<br class="title-page-name"/>+- *Filter (isnotnull(State#0) &amp;&amp; (State#0 = California))<br class="title-page-name"/> +- *FileScan csv [State#0,Year#1,Population#2] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/states.csv], PartitionFilters: [], PushedFilters: [IsNotNull(State), EqualTo(State,California)], ReadSchema: struct&lt;State:string,Year:int,Population:int&gt;<strong class="calibre1">
</strong>
</pre>
<p class="mce-root">Now that we can seen the execution plan, let's now execute the <kbd class="calibre11">filter</kbd> command, as follows:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.filter("State == 'California'").show</strong><br class="title-page-name"/>+----------+----+----------+<br class="title-page-name"/>| State|Year|Population|<br class="title-page-name"/>+----------+----+----------+<br class="title-page-name"/>|California|2010| 37332685|<br class="title-page-name"/>|California|2011| 37676861|<br class="title-page-name"/>|California|2012| 38011074|<br class="title-page-name"/>|California|2013| 38335203|<br class="title-page-name"/>|California|2014| 38680810|<br class="title-page-name"/>|California|2015| 38993940|<br class="title-page-name"/>|California|2016| 39250017|<br class="title-page-name"/>+----------+----+----------+
</pre>


            </article>

            
        </section>
    

        <section id="7AJAC1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">User-Defined Functions (UDFs)</h1>
                
            
            <article>
                
<p class="mce-root">UDFs define new column-based functions that extend the functionality of Spark SQL. Often, the inbuilt functions provided in Spark do not handle the exact need we have. In such cases, Apache Spark supports the creation of UDFs, which can be used.</p>
<div class="packt_infobox"><kbd class="calibre22">udf()</kbd> internally calls a case class User-Defined Function, which itself calls ScalaUDF internally.</div>
<p class="mce-root">Let's go through an example of an UDF which simply converts State column values to uppercase.</p>
<p class="mce-root">First, we create the function we need in Scala.</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/><br class="title-page-name"/>scala&gt; val toUpper: String =&gt; String = _.toUpperCase</strong><br class="title-page-name"/>toUpper: String =&gt; String = &lt;function1&gt;
</pre>
<p class="mce-root">Then, we have to encapsulate the created function inside the <kbd class="calibre11">udf</kbd> to create the UDF.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val toUpperUDF = udf(toUpper)</strong><br class="title-page-name"/>toUpperUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(StringType)))
</pre>
<p class="mce-root">Now that we have created the <kbd class="calibre11">udf</kbd>, we can use it to convert the State column to uppercase.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesDF.withColumn("StateUpperCase", toUpperUDF(col("State"))).show(5)</strong><br class="title-page-name"/>+----------+----+----------+--------------+<br class="title-page-name"/>| State|Year|Population|StateUpperCase|<br class="title-page-name"/>+----------+----+----------+--------------+<br class="title-page-name"/>| Alabama|2010| 4785492| ALABAMA|<br class="title-page-name"/>| Alaska|2010| 714031| ALASKA|<br class="title-page-name"/>| Arizona|2010| 6408312| ARIZONA|<br class="title-page-name"/>| Arkansas|2010| 2921995| ARKANSAS|<br class="title-page-name"/>|California|2010| 37332685| CALIFORNIA|<br class="title-page-name"/>+----------+----+----------+--------------+
</pre>


            </article>

            
        </section>
    

        <section id="7BHQU1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Schema   structure of data</h1>
                
            
            <article>
                
<p class="mce-root">A schema is the description of the structure of your data and can be either Implicit or Explicit.</p>
<p class="mce-root">Since the DataFrames are internally based on the RDD, there are two main methods of converting existing RDDs into datasets. An RDD can be converted into a dataset by using reflection to infer the schema of the RDD. A second method for creating datasets is through a programmatic interface, using which you can take an existing RDD and provide a schema to convert the RDD into a dataset with schema.</p>
<p class="mce-root">In order to create a DataFrame from an RDD by inferring the schema using reflection, the Scala API for Spark provides case classes which can be used to define the schema of the table. The DataFrame is created programmatically from the RDD, because the case classes are not easy to use in all cases. For instance, creating a case classes on a 1000 column table is time consuming.</p>


            </article>

            
        </section>
    

        <section id="7CGBG1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implicit schema</h1>
                
            
            <article>
                
<p class="mce-root">Let us look at an example of loading a <strong class="calibre1">CSV</strong> (c<strong class="calibre1">omma-separated Values</strong>) file into a DataFrame. Whenever a text file contains a header, read API can infer the schema by reading the header line. We also have the option to specify the separator to be used to split the text file lines.</p>
<p class="mce-root">We read the <kbd class="calibre11">csv</kbd> inferring the schema from the header line and uses comma (<kbd class="calibre11">,</kbd>) as the separator. We also show use of <kbd class="calibre11">schema</kbd> command and <kbd class="calibre11">printSchema</kbd> command to verify the schema of the input file.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val statesDF = spark.read.option("header", "true")<br class="title-page-name"/>                                .option("inferschema", "true")<br class="title-page-name"/>                                .option("sep", ",")<br class="title-page-name"/>                                .csv("statesPopulation.csv")</strong><br class="title-page-name"/>statesDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesDF.schema</strong><br class="title-page-name"/>res92: org.apache.spark.sql.types.StructType = StructType(<br class="title-page-name"/>                                                  StructField(State,StringType,true),<br class="title-page-name"/>                                                  StructField(Year,IntegerType,true),<br class="title-page-name"/>                                                  StructField(Population,IntegerType,true))<strong class="calibre1"><br class="title-page-name"/></strong><br class="title-page-name"/><strong class="calibre1">scala&gt; statesDF.printSchema</strong><br class="title-page-name"/>root<br class="title-page-name"/> |-- State: string (nullable = true)<br class="title-page-name"/> |-- Year: integer (nullable = true)<br class="title-page-name"/> |-- Population: integer (nullable = true)
</pre>


            </article>

            
        </section>
    

        <section id="7DES21-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Explicit schema</h1>
                
            
            <article>
                
<p class="mce-root">A schema is described using <kbd class="calibre11">StructType</kbd>, which is a collection of <kbd class="calibre11">StructField</kbd> objects.</p>
<div class="packt_infobox"><kbd class="calibre22">StructType</kbd> and <kbd class="calibre22">StructField</kbd> belong to the <kbd class="calibre22">org.apache.spark.sql.types</kbd> package.<br class="calibre23"/>
<span class="field">DataTypes such as</span> <kbd class="calibre22">IntegerType</kbd><span class="field">,</span> <kbd class="calibre22">StringType</kbd> <span class="field">also belong to the</span> <kbd class="calibre22">org.apache.spark.sql.types</kbd> <span class="field">package.</span></div>
<p class="mce-root">Using these imports, we can define a custom explicit schema.</p>
<p class="mce-root">First, import the necessary classes:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; import org.apache.spark.sql.types.{StructType, IntegerType, StringType}</strong><br class="title-page-name"/>import org.apache.spark.sql.types.{StructType, IntegerType, StringType}
</pre>
<p class="mce-root">Define a schema with two columns/fields-an <kbd class="calibre11">Integer</kbd> followed by a <kbd class="calibre11">String</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val schema = new StructType().add("i", IntegerType).add("s", StringType)</strong><br class="title-page-name"/>schema: org.apache.spark.sql.types.StructType = StructType(StructField(i,IntegerType,true), StructField(s,StringType,true))
</pre>
<p class="mce-root">It's easy to print the newly created <kbd class="calibre11">schema</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; schema.printTreeString</strong><br class="title-page-name"/>root<br class="title-page-name"/> |-- i: integer (nullable = true)<br class="title-page-name"/> |-- s: string (nullable = true)
</pre>
<p class="mce-root">There is also an option to print JSON, which is as follows, using <kbd class="calibre11">prettyJson</kbd> function:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; schema.prettyJson</strong><br class="title-page-name"/>res85: String =<br class="title-page-name"/>{<br class="title-page-name"/> "type" : "struct",<br class="title-page-name"/> "fields" : [ {<br class="title-page-name"/> "name" : "i",<br class="title-page-name"/> "type" : "integer",<br class="title-page-name"/> "nullable" : true,<br class="title-page-name"/> "metadata" : { }<br class="title-page-name"/> }, {<br class="title-page-name"/> "name" : "s",<br class="title-page-name"/> "type" : "string",<br class="title-page-name"/> "nullable" : true,<br class="title-page-name"/> "metadata" : { }<br class="title-page-name"/> } ]<br class="title-page-name"/>}
</pre>
<p class="mce-root">All the data types of Spark SQL are located in the package <kbd class="calibre11">org.apache.spark.sql.types</kbd>. You can access them by doing:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.types._</strong>
</pre>


            </article>

            
        </section>
    

        <section id="7EDCK1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Encoders</h1>
                
            
            <article>
                
<p class="mce-root">Spark 2.x supports a different way of defining schema for complex data types. First, let's look at a simple example.</p>
<p class="mce-root">Encoders must be imported using the import statement in order for you to use Encoders:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.Encoders<br class="title-page-name"/></strong>
</pre>
<p class="mce-root">Let's look at a simple example of defining a tuple as a data type to be used in the dataset APIs:</p>
<pre class="calibre19">
<br class="title-page-name"/><strong class="calibre1">scala&gt; Encoders.product[(Integer, String)].schema.printTreeString</strong><br class="title-page-name"/>root<br class="title-page-name"/> |-- _1: integer (nullable = true)<br class="title-page-name"/> |-- _2: string (nullable = true)
</pre>
<p class="mce-root">The preceding code looks complicated to use all the time, so we can also define a case class for our need and then use it. We can define a case class <kbd class="calibre11">Record</kbd> with two fields-an <kbd class="calibre11">Integer</kbd> and a <kbd class="calibre11">String</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; case class Record(i: Integer, s: String)</strong><br class="title-page-name"/>defined class Record
</pre>
<p class="mce-root">Using <kbd class="calibre11">Encoders</kbd> , we can easily create a <kbd class="calibre11">schema</kbd> on top of the case class, thus allowing us to use the various APIs with ease:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; Encoders.product[Record].schema.printTreeString</strong><br class="title-page-name"/>root<br class="title-page-name"/> |-- i: integer (nullable = true)<br class="title-page-name"/> |-- s: string (nullable = true)
</pre>
<p class="mce-root">All the data types of Spark SQL are located in the package <strong class="calibre1"><kbd class="calibre11">org.apache.spark.sql.types</kbd></strong>. You can access them by doing:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.types._</strong>
</pre>
<p class="mce-root">You should use the <kbd class="calibre11">DataTypes</kbd> object in your code to create complex Spark SQL types such as arrays or maps, as follows:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; import org.apache.spark.sql.types.DataTypes</strong><br class="title-page-name"/>import org.apache.spark.sql.types.DataTypes<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val arrayType = DataTypes.createArrayType(IntegerType)</strong><br class="title-page-name"/>arrayType: org.apache.spark.sql.types.ArrayType = ArrayType(IntegerType,true)
</pre>
<p class="mce-root">The following are the data types supported in Spark SQL APIs:</p>
<table class="calibre36">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Data type</strong></td>
<td class="calibre7"><strong class="calibre1">Value type in Scala</strong></td>
<td class="calibre7"><strong class="calibre1">API to access or create a data type</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">ByteType</kbd></td>
<td class="calibre7"><kbd class="calibre34">Byte</kbd></td>
<td class="calibre7"><kbd class="calibre34">ByteType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">ShortType</kbd></td>
<td class="calibre7"><kbd class="calibre34">Short</kbd></td>
<td class="calibre7"><kbd class="calibre34">ShortType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">IntegerType</kbd></td>
<td class="calibre7"><kbd class="calibre34">Int</kbd></td>
<td class="calibre7"><kbd class="calibre34">IntegerType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">LongType</kbd></td>
<td class="calibre7"><kbd class="calibre34">Long</kbd></td>
<td class="calibre7"><kbd class="calibre34">LongType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">FloatType</kbd></td>
<td class="calibre7"><kbd class="calibre34">Float</kbd></td>
<td class="calibre7"><kbd class="calibre34">FloatType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">DoubleType</kbd></td>
<td class="calibre7"><kbd class="calibre34">Double</kbd></td>
<td class="calibre7"><kbd class="calibre34">DoubleType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">DecimalType</kbd></td>
<td class="calibre7"><kbd class="calibre34">java.math.BigDecimal</kbd></td>
<td class="calibre7"><kbd class="calibre34">DecimalType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">StringType</kbd></td>
<td class="calibre7"><kbd class="calibre34">String</kbd></td>
<td class="calibre7"><kbd class="calibre34">StringType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">BinaryType</kbd></td>
<td class="calibre7"><kbd class="calibre34">Array[Byte]</kbd></td>
<td class="calibre7"><kbd class="calibre34">BinaryType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">BooleanType</kbd></td>
<td class="calibre7"><kbd class="calibre34">Boolean</kbd></td>
<td class="calibre7"><kbd class="calibre34">BooleanType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">TimestampType</kbd></td>
<td class="calibre7"><kbd class="calibre34">java.sql.Timestamp</kbd></td>
<td class="calibre7"><kbd class="calibre34">TimestampType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">DateType</kbd></td>
<td class="calibre7"><kbd class="calibre34">java.sql.Date</kbd></td>
<td class="calibre7"><kbd class="calibre34">DateType</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">ArrayType</kbd></td>
<td class="calibre7"><kbd class="calibre34">scala.collection.Seq</kbd></td>
<td class="calibre7"><kbd class="calibre34">ArrayType(elementType, [containsNull])</kbd></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">MapType</kbd></td>
<td class="calibre7"><kbd class="calibre34">scala.collection.Map</kbd></td>
<td class="calibre7"><kbd class="calibre34">MapType(keyType, valueType, [valueContainsNull])</kbd> Note: The default value of <kbd class="calibre34">valueContainsNull</kbd> is <kbd class="calibre34">true</kbd>.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><kbd class="calibre34">StructType</kbd></td>
<td class="calibre7"><kbd class="calibre34">org.apache.spark.sql.Row</kbd></td>
<td class="calibre7">
<p class="mce-root"><kbd class="calibre11">StructType(fields)</kbd> Note: fields is a <kbd class="calibre11">Seq</kbd> of <kbd class="calibre11">StructFields</kbd>. Also, two fields with the same name are not allowed.</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section id="7FBT61-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Loading and saving datasets</h1>
                
            
            <article>
                
<p class="mce-root">We need to have data read into the cluster as input and output or results written back to the storage to do anything practical with our code. Input data can be read from a variety of datasets and sources such as Files, Amazon S3 storage, Databases, NoSQLs, and Hive, and the output can similarly also be saved to Files, S3, Databases, Hive, and so on.</p>
<p class="mce-root">Several systems have support for Spark via a connector, and this number is growing day by day as more systems are latching onto the Spark processing framework.</p>


            </article>

            
        </section>
    

        <section id="7GADO1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Loading datasets</h1>
                
            
            <article>
                
<div class="title-page-name">
<p class="mce-root">Spark SQL can read data from external storage systems such as files, Hive tables, and JDBC databases through the <kbd class="calibre11">DataFrameReader</kbd> interface.</p>
<p class="mce-root">The format of the API call is <kbd class="calibre11">spark.read.inputtype</kbd></p>
<ul class="calibre9">
<li class="mce-root1">Parquet</li>
<li class="mce-root1">CSV</li>
<li class="mce-root1">Hive Table</li>
<li class="mce-root1">JDBC</li>
<li class="mce-root1">ORC</li>
<li class="mce-root1">Text</li>
<li class="mce-root1">JSON</li>
</ul>
<p class="mce-root">Let's look at a couple of simple examples of reading CSV files into DataFrames:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val statesPopulationDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesPopulation.csv")</strong><br class="title-page-name"/>statesPopulationDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val statesTaxRatesDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesTaxRates.csv")</strong><br class="title-page-name"/>statesTaxRatesDF: org.apache.spark.sql.DataFrame = [State: string, TaxRate: double]
</pre></div>


            </article>

            
        </section>
    

        <section id="7H8UA1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Saving datasets</h1>
                
            
            <article>
                
<p class="mce-root">Spark SQL can save data to external storage systems such as files, Hive tables and JDBC databases through <kbd class="calibre11">DataFrameWriter</kbd> interface.</p>
<div class="title-page-name">
<p class="mce-root"><span>The format of the API call is <kbd class="calibre11">dataframe</kbd></span><kbd class="calibre11"><span>.write.</span>outputtype</kbd></p>
</div>
<ul class="calibre9">
<li class="mce-root1">Parquet</li>
<li class="mce-root1">ORC</li>
<li class="mce-root1">Text</li>
<li class="mce-root1">Hive table</li>
<li class="mce-root1">JSON</li>
<li class="mce-root1">CSV</li>
<li class="mce-root1">JDBC</li>
</ul>
<p class="mce-root">Let's look at a couple of examples of writing or saving a DataFrame to a CSV file:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesPopulationDF.write.option("header", "true").csv("statesPopulation_dup.csv")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesTaxRatesDF.write.option("header", "true").csv("statesTaxRates_dup.csv")</strong>
</pre>


            </article>

            
        </section>
    

        <section id="7I7ES1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Aggregations</h1>
                
            
            <article>
                
<p class="mce-root">Aggregation is the method of collecting data based on a condition and performing analytics on the data. Aggregation is very important to make sense of data of all sizes, as just having raw records of data is not that useful for most use cases.</p>
<p class="mce-root">For example, if you look at the following table and then the aggregated view, it is obvious that just raw records do not help you understand the data.</p>
<div class="packt_tip">Imagine a table containing one temperature measurement per day for every city in the world for five years.</div>
<p class="mce-root">Shown in the following is a table containing records of average temperature per day per city:</p>
<table class="calibre37">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">City</strong></p>
</td>
<td class="calibre7"><strong class="calibre1">Date</strong></td>
<td class="calibre7"><strong class="calibre1">Temperature</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">Boston</td>
<td class="calibre7">12/23/2016</td>
<td class="calibre7">32</td>
</tr>
<tr class="calibre6">
<td class="calibre7">New York</td>
<td class="calibre7"><span>12/24/2016</span></td>
<td class="calibre7">36</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Boston</td>
<td class="calibre7"><span>12/24/2016</span></td>
<td class="calibre7">30</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Philadelphia</td>
<td class="calibre7"><span>12/25/2016</span></td>
<td class="calibre7">34</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Boston</td>
<td class="calibre7"><span>12/25/2016</span></td>
<td class="calibre7">28</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">If we want to compute the average temperature per city for all the days we have measurements for in the above table, we can see results which look similar to the following table:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">City</strong></td>
<td class="calibre7"><strong class="calibre1">Average Temperature</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">Boston</td>
<td class="calibre7">30 - <em class="calibre8">(32 + 30 + 28)/3</em></td>
</tr>
<tr class="calibre6">
<td class="calibre7">New York</td>
<td class="calibre7">36</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Philadelphia</td>
<td class="calibre7">34</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section id="7J5VE1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Aggregate functions</h1>
                
            
            <article>
                
<p class="mce-root">Most aggregations can be done using functions that can be found in the <kbd class="calibre11">org.apache.spark.sql.functions</kbd> package. In addition, custom aggregation functions can also be created, also known as <strong class="calibre1">User Defined Aggregation Functions</strong> (<strong class="calibre1">UDAF</strong>).</p>
<div class="packt_tip">Each grouping operation returns a <kbd class="calibre22">RelationalGroupeddataset</kbd>, on which you can specify aggregations.</div>
<p class="mce-root">We will load the sample data to illustrate all the different types of aggregate functions in this section:</p>
<pre class="calibre19">
<strong class="calibre1">val statesPopulationDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesPopulation.csv")</strong>
</pre>


            </article>

            
        </section>
    

        <section id="7K4G01-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Count</h1>
                
            
            <article>
                
<p class="mce-root">Count is the most basic aggregate function, which simply counts the number of rows for the column specified. An extension is the <kbd class="calibre11">countDistinct</kbd>, which also eliminates duplicates.</p>
<p class="mce-root">The <kbd class="calibre11">count</kbd> API has several implementations, as follows. The exact API used depends on the specific use case:</p>
<pre class="calibre19">
<strong class="calibre1">def count(columnName: String): TypedColumn[Any, Long]</strong><br class="title-page-name"/>Aggregate function: returns the number of items in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def count(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the number of items in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def countDistinct(columnName: String, columnNames: String*): Column</strong><br class="title-page-name"/>Aggregate function: returns the number of distinct items in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def countDistinct(expr: Column, exprs: Column*): Column</strong><br class="title-page-name"/>Aggregate function: returns the number of distinct items in a group.
</pre>
<p class="mce-root">Let's look at examples of invoking <kbd class="calibre11">count</kbd> and <kbd class="calibre11">countDistinct</kbd> on the DataFrame to print the row counts:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(col("*")).agg(count("State")).show</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; statesPopulationDF.select(count("State")).show</strong><br class="title-page-name"/>+------------+<br class="title-page-name"/>|count(State)|<br class="title-page-name"/>+------------+<br class="title-page-name"/>| 350|<br class="title-page-name"/>+------------+<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesPopulationDF.select(col("*")).agg(countDistinct("State")).show</strong><br class="title-page-name"/><strong class="calibre1">scala&gt; statesPopulationDF.select(countDistinct("State")).show</strong><br class="title-page-name"/>+---------------------+<br class="title-page-name"/>|count(DISTINCT State)|<br class="title-page-name"/>+---------------------+<br class="title-page-name"/>| 50|
</pre>


            </article>

            
        </section>
    

        <section id="7L30I1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">First</h1>
                
            
            <article>
                
<p class="mce-root">Gets the first record in the <kbd class="calibre11">RelationalGroupeddataset.</kbd></p>
<p class="mce-root">The <kbd class="calibre11">first</kbd> <span>API has several implementations, as follows. The exact API used depends on the specific use case:</span></p>
<pre class="calibre19">
<strong class="calibre1">def first(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the first value of a column in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def first(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the first value in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def first(columnName: String, ignoreNulls: Boolean): Column</strong><br class="title-page-name"/>Aggregate function: returns the first value of a column in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def first(e: Column, ignoreNulls: Boolean): Column</strong><br class="title-page-name"/>Aggregate function: returns the first value in a group.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking</span> <span><kbd class="calibre11">first</kbd></span> <span>on the DataFrame to output the first row:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(first("State")).show</strong><br class="title-page-name"/>+-------------------+<br class="title-page-name"/>|first(State, false)|<br class="title-page-name"/>+-------------------+<br class="title-page-name"/>| Alabama|<br class="title-page-name"/>+-------------------+
</pre>


            </article>

            
        </section>
    

        <section id="7M1H41-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Last</h1>
                
            
            <article>
                
<p class="mce-root">Gets the last record in the <kbd class="calibre11">RelationalGroupeddataset</kbd>.</p>
<p class="mce-root">The <span><kbd class="calibre11">last</kbd> API has several implementations, as follows. The exact API used depends on the specific use case:</span></p>
<pre class="calibre19">
<strong class="calibre1">def last(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the last value of the column in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def last(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the last value in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def last(columnName: String, ignoreNulls: Boolean): Column</strong><br class="title-page-name"/>Aggregate function: returns the last value of the column in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def last(e: Column, ignoreNulls: Boolean): Column</strong><br class="title-page-name"/>Aggregate function: returns the last value in a group.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking</span> <span><kbd class="calibre11">last</kbd></span> <span>on the DataFrame to output the last row.</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(last("State")).show</strong><br class="title-page-name"/>+------------------+<br class="title-page-name"/>|last(State, false)|<br class="title-page-name"/>+------------------+<br class="title-page-name"/>| Wyoming|<br class="title-page-name"/>+------------------+
</pre>


            </article>

            
        </section>
    

        <section id="7N01M1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">approx_count_distinct</h1>
                
            
            <article>
                
<p class="mce-root">Approximate distinct count is much faster at approximately counting the distinct records rather than doing an exact count, which usually needs a lot of shuffles and other operations. While the approximate count is not 100% accurate, many use cases can perform equally well even without an exact count.</p>
<p class="mce-root">The <kbd class="calibre11">approx_count_distinct</kbd> <span>API has several implementations, as follows. The exact API used depends on the specific use case.</span></p>
<pre class="calibre19">
<strong class="calibre1">def approx_count_distinct(columnName: String, rsd: Double): Column</strong><br class="title-page-name"/>Aggregate function: returns the approximate number of distinct items in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def approx_count_distinct(e: Column, rsd: Double): Column</strong><br class="title-page-name"/>Aggregate function: returns the approximate number of distinct items in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def approx_count_distinct(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the approximate number of distinct items in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def approx_count_distinct(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the approximate number of distinct items in a group.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking</span> <span><kbd class="calibre11">approx_count_distinct</kbd></span> <span>on the DataFrame to print the approximate count of the DataFrame:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(col("*")).agg(approx_count_distinct("State")).show</strong><br class="title-page-name"/>+----------------------------+<br class="title-page-name"/>|approx_count_distinct(State)|<br class="title-page-name"/>+----------------------------+<br class="title-page-name"/>| 48|<br class="title-page-name"/>+----------------------------+<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesPopulationDF.select(approx_count_distinct("State", 0.2)).show</strong><br class="title-page-name"/>+----------------------------+<br class="title-page-name"/>|approx_count_distinct(State)|<br class="title-page-name"/>+----------------------------+<br class="title-page-name"/>| 49|<br class="title-page-name"/>+----------------------------+
</pre>


            </article>

            
        </section>
    

        <section id="7NUI81-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Min</h1>
                
            
            <article>
                
<p class="mce-root">The minimum of the column value of one of the columns in the DataFrame. An example is if you want to find the minimum temperature of a city.</p>
<p class="mce-root">The <kbd class="calibre11">min</kbd> <span>API has several implementations, as follows. The exact API used depends on the specific use case:</span></p>
<pre class="calibre19">
<strong class="calibre1">def min(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the minimum value of the column in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def min(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the minimum value of the expression in a group.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking</span> <span><kbd class="calibre11">min</kbd></span> <span>on the DataFrame to print the minimum Population:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(min("Population")).show</strong><br class="title-page-name"/>+---------------+<br class="title-page-name"/>|min(Population)|<br class="title-page-name"/>+---------------+<br class="title-page-name"/>| 564513|<br class="title-page-name"/>+---------------+
</pre>


            </article>

            
        </section>
    

        <section id="7OT2Q1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Max</h1>
                
            
            <article>
                
<p class="mce-root">The maximum of the column value of one of the columns in the DataFrame. An example is if you want to find the maximum temperature of a city.</p>
<p class="mce-root">The <kbd class="calibre11">max</kbd> <span>API has several implementations, as follows. The exact API used depends on the specific use case.</span></p>
<pre class="calibre19">
<strong class="calibre1">def max(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the maximum value of the column in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def max(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the maximum value of the expression in a group.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking</span> <span><kbd class="calibre11">max</kbd></span> <span>on the DataFrame to print the maximum Population:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(max("Population")).show</strong><br class="title-page-name"/>+---------------+<br class="title-page-name"/>|max(Population)|<br class="title-page-name"/>+---------------+<br class="title-page-name"/>| 39250017|<br class="title-page-name"/>+---------------+
</pre>


            </article>

            
        </section>
    

        <section id="7PRJC1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Average</h1>
                
            
            <article>
                
<p class="mce-root">The average of the values is calculated by adding the values and dividing by the number of values.</p>
<div class="packt_tip"><br class="calibre23"/>
Average of 1,2,3 is (1 + 2 + 3) / 3 = 6/3 = 2</div>
<p class="mce-root">The <kbd class="calibre11">avg</kbd> <span>API has several implementations, as follows. The exact API used depends on the specific use case:</span></p>
<pre class="calibre19">
<strong class="calibre1">def avg(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the average of the values in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def avg(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the average of the values in a group.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking <kbd class="calibre11">avg</kbd></span> <span>on the DataFrame to print the average population:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(avg("Population")).show</strong><br class="title-page-name"/>+-----------------+<br class="title-page-name"/>| avg(Population)|<br class="title-page-name"/>+-----------------+<br class="title-page-name"/>|6253399.371428572|<br class="title-page-name"/>+-----------------+
</pre>


            </article>

            
        </section>
    

        <section id="7QQ3U1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Sum</h1>
                
            
            <article>
                
<p class="mce-root">Computes the sum of the values of the column. Optionally, <kbd class="calibre11">sumDistinct</kbd> can be used to only add up distinct values.</p>
<p class="mce-root">The <kbd class="calibre11">sum</kbd> <span>API has several implementations, as follows. The exact API used depends on the specific use case:</span></p>
<pre class="calibre19">
<strong class="calibre1">def sum(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the sum of all values in the given column.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def sum(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the sum of all values in the expression.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def sumDistinct(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the sum of distinct values in the expression<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def sumDistinct(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the sum of distinct values in the expression.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking</span> <span><kbd class="calibre11">sum</kbd></span> <span>on the DataFrame to print the summation (total) <kbd class="calibre11">Population</kbd>.</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(sum("Population")).show</strong><br class="title-page-name"/>+---------------+<br class="title-page-name"/>|sum(Population)|<br class="title-page-name"/>+---------------+<br class="title-page-name"/>| 2188689780|<br class="title-page-name"/>+---------------+
</pre>


            </article>

            
        </section>
    

        <section id="7ROKG1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Kurtosis</h1>
                
            
            <article>
                
<p class="mce-root">Kurtosis is a way of quantifying differences in the shape of distributions, which may look very similar in terms of means and variances, yet are actually different. In such cases, kurtosis becomes a good measure of the weight of the distribution at the tail of the distribution, as compared to the middle of the distribution.</p>
<p class="mce-root">The <kbd class="calibre11">kurtosis</kbd> <span>API has several implementations, as follows. The exact API used depends on the specific use case.</span></p>
<pre class="calibre19">
<strong class="calibre1">def kurtosis(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the kurtosis of the values in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def kurtosis(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the kurtosis of the values in a group.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking</span> <span><kbd class="calibre11">kurtosis</kbd></span> <span>on the DataFrame on the <kbd class="calibre11">Population</kbd> column:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(kurtosis("Population")).show</strong><br class="title-page-name"/>+--------------------+<br class="title-page-name"/>|kurtosis(Population)|<br class="title-page-name"/>+--------------------+<br class="title-page-name"/>| 7.727421920829375|<br class="title-page-name"/>+--------------------+
</pre>


            </article>

            
        </section>
    

        <section id="7SN521-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Skewness</h1>
                
            
            <article>
                
<p class="mce-root">Skewness measures the asymmetry of the values in your data around the average or mean.</p>
<p class="mce-root">The <kbd class="calibre11">skewness</kbd> <span>API has several implementations, as follows. The exact API used depends on the specific use case.</span></p>
<pre class="calibre19">
<strong class="calibre1">def skewness(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the skewness of the values in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def skewness(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the skewness of the values in a group.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking</span> <span><kbd class="calibre11">skewness</kbd></span> <span>on the DataFrame on the Population column:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(skewness("Population")).show</strong><br class="title-page-name"/>+--------------------+<br class="title-page-name"/>|skewness(Population)|<br class="title-page-name"/>+--------------------+<br class="title-page-name"/>| 2.5675329049100024|<br class="title-page-name"/>+--------------------+
</pre>


            </article>

            
        </section>
    

        <section id="7TLLK1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Variance</h1>
                
            
            <article>
                
<p class="mce-root">Variance is the average of the squared differences of each of the values from the mean.</p>
<p class="mce-root">The <kbd class="calibre11">var</kbd> <span>API has several implementations, as follows. The exact API used depends on the specific use case:</span></p>
<pre class="calibre19">
<strong class="calibre1">def var_pop(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the population variance of the values in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def var_pop(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the population variance of the values in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def var_samp(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the unbiased variance of the values in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def var_samp(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the unbiased variance of the values in a group.
</pre>
<p class="mce-root"><span>Now, let's look at an example of invoking <kbd class="calibre11">var_pop</kbd></span> <span>on the DataFrame measuring variance of <kbd class="calibre11">Population</kbd>:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(var_pop("Population")).show</strong><br class="title-page-name"/>+--------------------+<br class="title-page-name"/>| var_pop(Population)|<br class="title-page-name"/>+--------------------+<br class="title-page-name"/>|4.948359064356177E13|<br class="title-page-name"/>+--------------------+
</pre>


            </article>

            
        </section>
    

        <section id="7UK661-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Standard deviation</h1>
                
            
            <article>
                
<p class="mce-root">Standard deviation is the square root of the variance (see previously).</p>
<p class="mce-root">The <span><kbd class="calibre11">stddev</kbd> API has several implementations, as follows. The exact API used depends on the specific use case:</span></p>
<pre class="calibre19">
<strong class="calibre1">def stddev(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: alias for stddev_samp.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def stddev(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: alias for stddev_samp.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def stddev_pop(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the population standard deviation of the expression in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def stddev_pop(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the population standard deviation of the expression in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def stddev_samp(columnName: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the sample standard deviation of the expression in a group.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def stddev_samp(e: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the sample standard deviation of the expression in a group.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking</span> <span><kbd class="calibre11">stddev</kbd></span> <span>on the DataFrame printing the standard deviation of <kbd class="calibre11">Population</kbd>:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(stddev("Population")).show</strong><br class="title-page-name"/>+-----------------------+<br class="title-page-name"/>|stddev_samp(Population)|<br class="title-page-name"/>+-----------------------+<br class="title-page-name"/>| 7044528.191173398|<br class="title-page-name"/>+-----------------------+
</pre>


            </article>

            
        </section>
    

        <section id="7VIMO1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Covariance</h1>
                
            
            <article>
                
<p class="mce-root">Covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly corresponds with the greater values of the other variable, and the same holds for the lesser values, then the variables tend to show similar behavior and the covariance is positive. If the opposite is true, and the greater values of one variable correspond with the lesser values of the other variable, then the covariance is negative.</p>
<p class="mce-root">The <kbd class="calibre11">covar</kbd> <span>API has several implementations, as follows. The exact API used depends on the specific use case.</span></p>
<pre class="calibre19">
<strong class="calibre1">def covar_pop(columnName1: String, columnName2: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the population covariance for two columns.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def covar_pop(column1: Column, column2: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the population covariance for two columns.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def covar_samp(columnName1: String, columnName2: String): Column</strong><br class="title-page-name"/>Aggregate function: returns the sample covariance for two columns.<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">def covar_samp(column1: Column, column2: Column): Column</strong><br class="title-page-name"/>Aggregate function: returns the sample covariance for two columns.
</pre>
<p class="mce-root"><span>Let's look at an example of invoking</span> <span><kbd class="calibre11">covar_pop</kbd></span> <span>on the DataFrame to calculate the covariance between the year and population columns:</span></p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(covar_pop("Year", "Population")).show</strong><br class="title-page-name"/>+---------------------------+<br class="title-page-name"/>|covar_pop(Year, Population)|<br class="title-page-name"/>+---------------------------+<br class="title-page-name"/>| 183977.56000006935|<br class="title-page-name"/>+---------------------------+
</pre>


            </article>

            
        </section>
    

        <section id="80H7A1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">groupBy</h1>
                
            
            <article>
                
<p class="mce-root">A common task seen in data analysis is to group the data into grouped categories and then perform calculations on the resultant groups of data.</p>
<div class="packt_tip">A quick way to understand grouping is to imagine being asked to assess what supplies you need for your office very quickly. You could start looking around you and just group different types of items, such as pens, paper, staplers, and analyze what you have and what you need.</div>
<p class="mce-root">Let's run <kbd class="calibre11">groupBy</kbd> function on the <kbd class="calibre11">DataFrame</kbd> to print aggregate counts of each State:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesPopulationDF.groupBy("State").count.show(5)</strong><br class="title-page-name"/>+---------+-----+<br class="title-page-name"/>| State|count|<br class="title-page-name"/>+---------+-----+<br class="title-page-name"/>| Utah| 7|<br class="title-page-name"/>| Hawaii| 7|<br class="title-page-name"/>|Minnesota| 7|<br class="title-page-name"/>| Ohio| 7|<br class="title-page-name"/>| Arkansas| 7|<br class="title-page-name"/>+---------+-----+<br class="title-page-name"/><br class="title-page-name"/>
</pre>
<p class="mce-root">You can also <kbd class="calibre11">groupBy</kbd> and then apply any of the aggregate functions seen previously, such as <kbd class="calibre11">min</kbd>, <kbd class="calibre11">max</kbd>, <kbd class="calibre11">avg</kbd>, <kbd class="calibre11">stddev</kbd>, and so on:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.groupBy("State").agg(min("Population"), avg("Population")).show(5)</strong><br class="title-page-name"/>+---------+---------------+--------------------+<br class="title-page-name"/>| State|min(Population)| avg(Population)|<br class="title-page-name"/>+---------+---------------+--------------------+<br class="title-page-name"/>| Utah| 2775326| 2904797.1428571427|<br class="title-page-name"/>| Hawaii| 1363945| 1401453.2857142857|<br class="title-page-name"/>|Minnesota| 5311147| 5416287.285714285|<br class="title-page-name"/>| Ohio| 11540983|1.1574362714285715E7|<br class="title-page-name"/>| Arkansas| 2921995| 2957692.714285714|<br class="title-page-name"/>+---------+---------------+--------------------+<br class="title-page-name"/><br class="title-page-name"/>
</pre>


            </article>

            
        </section>
    

        <section id="81FNS1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Rollup</h1>
                
            
            <article>
                
<p class="mce-root">Rollup is a multi-dimensional aggregation used to perform hierarchical or nested calculations. For example, if we want to show the number of records for each State+Year group, as well as for each State (aggregating over all years to give a grand total for each <kbd class="calibre11">State</kbd> irrespective of the <kbd class="calibre11">Year</kbd>), we can use <kbd class="calibre11">rollup</kbd> as follows:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesPopulationDF.rollup("State", "Year").count.show(5)</strong><br class="title-page-name"/>+------------+----+-----+<br class="title-page-name"/>| State|Year|count|<br class="title-page-name"/>+------------+----+-----+<br class="title-page-name"/>|South Dakota|2010| 1|<br class="title-page-name"/>| New York|2012| 1|<br class="title-page-name"/>| California|2014| 1|<br class="title-page-name"/>| Wyoming|2014| 1|<br class="title-page-name"/>| Hawaii|null| 7|<br class="title-page-name"/>+------------+----+-----+
</pre>
<p class="mce-root">The <kbd class="calibre11">rollup</kbd> calculates the count for state and year, such as California+2014, as well as California state (adding up all years).</p>


            </article>

            
        </section>
    

        <section id="82E8E1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Cube</h1>
                
            
            <article>
                
<p class="mce-root"><span>Cube is a multi-dimensional aggregation used to perform hierarchical or nested calculations just like rollup, but with the difference that cube does the same operation for all dimensions. For example, if we want to show the number of records for each <kbd class="calibre11">State</kbd> and <kbd class="calibre11">Year</kbd> group, as well as for each <kbd class="calibre11">State</kbd> (aggregating over all Years to give a grand total for each State irrespective of the <kbd class="calibre11">Year</kbd>), we</span> <span>can</span> <span>use rollup as follows. In addition, <kbd class="calibre11">cube</kbd> also shows a grand total for each Year (irrespective of the <kbd class="calibre11">State</kbd>):</span></p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; statesPopulationDF.cube("State", "Year").count.show(5)</strong><br class="title-page-name"/>+------------+----+-----+<br class="title-page-name"/>| State|Year|count|<br class="title-page-name"/>+------------+----+-----+<br class="title-page-name"/>|South Dakota|2010| 1|<br class="title-page-name"/>| New York|2012| 1|<br class="title-page-name"/>| null|2014| 50|<br class="title-page-name"/>| Wyoming|2014| 1|<br class="title-page-name"/>| Hawaii|null| 7|<br class="title-page-name"/>+------------+----+-----+
</pre>


            </article>

            
        </section>
    

        <section id="83CP01-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Window functions</h1>
                
            
            <article>
                
<p class="mce-root">Window functions allow you to perform aggregations over a window of data rather than entire data or some filtered data. The use cases of such window functions are:</p>
<ul class="calibre9">
<li class="mce-root1">Cumulative sum</li>
<li class="mce-root1">Delta from previous value for same key</li>
<li class="mce-root1">Weighted moving average</li>
</ul>
<p class="mce-root">The best way to understand window functions is to imagine a sliding window over the larger dataset universe. You can specify a window looking at three rows T-1, T, and T+1, and by performing a simple calculation. You can also specify a window of latest/most recent ten values:</p>
<div class="cdpaligncenter"><img class="image-border95" src="../images/00047.jpeg"/></div>
<p class="cdpalignleft1">The API for the window specification requires three properties, the <kbd class="calibre11">partitionBy()</kbd>, <kbd class="calibre11">orderBy()</kbd>, and the <kbd class="calibre11">rowsBetween()</kbd>. The <kbd class="calibre11">partitionBy</kbd> chunks the data into the partitions/groups as specified by <kbd class="calibre11">partitionBy()</kbd>. <kbd class="calibre11">orderBy()</kbd> is used to order the data within each partition of data.</p>
<p class="cdpalignleft1">The <kbd class="calibre11">rowsBetween()</kbd> specifies the window frame or the span of the sliding window to perform the calculations.</p>
<p class="cdpalignleft1">To try out the windows function, there are certain packages that are needed. You can import the necessary packages using import directives, as follows:</p>
<pre class="cdpalignleft3">
<strong class="calibre1">import org.apache.spark.sql.expressions.Window</strong><br class="title-page-name"/><strong class="calibre1">import org.apache.spark.sql.functions.col<br class="title-page-name"/></strong><strong class="calibre1">import org.apache.spark.sql.functions.max</strong>
</pre>
<p class="cdpalignleft1">Now, you are ready to write some code to learn about the window functions. Let's create a window specification for the partitions sorted by <kbd class="calibre11">Population</kbd> and partitioned by <kbd class="calibre11">State</kbd>. Also, specify that we want to consider all rows until the current row as part of the <kbd class="calibre11">Window</kbd>.</p>
<pre class="cdpalignleft3">
<strong class="calibre1"> val windowSpec = Window</strong><br class="title-page-name"/><strong class="calibre1"> .partitionBy("State")</strong><br class="title-page-name"/><strong class="calibre1"> .orderBy(col("Population").desc)</strong><br class="title-page-name"/><strong class="calibre1"> .rowsBetween(Window.unboundedPreceding, Window.currentRow)</strong>
</pre>
<p class="cdpalignleft1">Compute the <kbd class="calibre11">rank</kbd> over the window specification. The result will be a rank (row number) added to each row, as long as it falls within the <kbd class="calibre11">Window</kbd> specified. In this example, we chose to partition by <kbd class="calibre11">State</kbd> and then order the rows of each <kbd class="calibre11">State</kbd> further by descending order. Hence, all State rows have their own rank numbers assigned.</p>
<pre class="cdpalignleft3">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(col("State"), col("Year"), max("Population").over(windowSpec), rank().over(windowSpec)).sort("State", "Year").show(10)</strong><br class="title-page-name"/>+-------+----+------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+<br class="title-page-name"/>| State|Year|max(Population) OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|RANK() OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|<br class="title-page-name"/>+-------+----+------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+<br class="title-page-name"/>|Alabama|2010| 4863300| 6|<br class="title-page-name"/>|Alabama|2011| 4863300| 7|<br class="title-page-name"/>|Alabama|2012| 4863300| 5|<br class="title-page-name"/>|Alabama|2013| 4863300| 4|<br class="title-page-name"/>|Alabama|2014| 4863300| 3|
</pre>


            </article>

            
        </section>
    

        <section id="84B9I1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">ntiles</h1>
                
            
            <article>
                
<p class="cdpalignleft1">The ntiles is a popular aggregation over a window and is commonly used to divide input dataset into n parts. For example, in predictive analytics, deciles (10 parts) are often used to first group the data and then divide it into 10 parts to get a fair distribution of data. This is a natural function of the window function approach, hence ntiles is a good example of how window functions can help.</p>
<p class="cdpalignleft1">For example, if we want to partition the <kbd class="calibre11">statesPopulationDF</kbd> by <kbd class="calibre11">State</kbd> (window specification was shown previously), order by population, and then divide into two portions, we can use <kbd class="calibre11">ntile</kbd> over the <kbd class="calibre11">windowspec</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.sql.functions._<br class="title-page-name"/>scala&gt; statesPopulationDF.select(col("State"), col("Year"), ntile(2).over(windowSpec), rank().over(windowSpec)).sort("State", "Year").show(10)</strong><br class="title-page-name"/>+-------+----+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+<br class="title-page-name"/>| State|Year|ntile(2) OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|RANK() OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|<br class="title-page-name"/>+-------+----+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------+<br class="title-page-name"/>|Alabama|2010| 2| 6|<br class="title-page-name"/>|Alabama|2011| 2| 7|<br class="title-page-name"/>|Alabama|2012| 2| 5|<br class="title-page-name"/>|Alabama|2013| 1| 4|<br class="title-page-name"/>|Alabama|2014| 1| 3|<br class="title-page-name"/>|Alabama|2015| 1| 2|<br class="title-page-name"/>|Alabama|2016| 1| 1|<br class="title-page-name"/>| Alaska|2010| 2| 7|<br class="title-page-name"/>| Alaska|2011| 2| 6|<br class="title-page-name"/>| Alaska|2012| 2| 5|<br class="title-page-name"/>+-------+----+-----------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------
</pre>
<p class="mce-root">As shown previously, we have used <kbd class="calibre11">Window</kbd> function and <kbd class="calibre11">ntile()</kbd> together to divide the rows of each <kbd class="calibre11">State</kbd> into two equal portions.</p>
<div class="packt_tip"><br class="calibre23"/>
A popular use of this function is to compute deciles used in data science Models.</div>


            </article>

            
        </section>
    

        <section id="859Q41-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Joins</h1>
                
            
            <article>
                
<p class="mce-root">In traditional databases, joins are used to join one transaction table with another lookup table to generate a more complete view. For example, if you have a table of online transactions by customer ID and another table containing the customer city and customer ID, you can use join to generate reports on the transactions by city.</p>
<p class="mce-root"><strong class="calibre1">Transactions table</strong>: The following table has three columns, the <strong class="calibre1">CustomerID</strong>, the <strong class="calibre1">Purchased item,</strong> and how much the customer paid for the item:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">CustomerID</strong></td>
<td class="calibre7"><strong class="calibre1">Purchased item</strong></td>
<td class="calibre7"><strong class="calibre1">Price paid</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">1</td>
<td class="calibre7">Headphone</td>
<td class="calibre7">25.00</td>
</tr>
<tr class="calibre6">
<td class="calibre7">2</td>
<td class="calibre7">Watch</td>
<td class="calibre7">100.00</td>
</tr>
<tr class="calibre6">
<td class="calibre7">3</td>
<td class="calibre7">Keyboard</td>
<td class="calibre7">20.00</td>
</tr>
<tr class="calibre6">
<td class="calibre7">1</td>
<td class="calibre7">Mouse</td>
<td class="calibre7">10.00</td>
</tr>
<tr class="calibre6">
<td class="calibre7">4</td>
<td class="calibre7">Cable</td>
<td class="calibre7">10.00</td>
</tr>
<tr class="calibre6">
<td class="calibre7">3</td>
<td class="calibre7">Headphone</td>
<td class="calibre7">30.00</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root"><strong class="calibre1">Customer Info table:</strong> The following table has two columns, the <strong class="calibre1">CustomerID</strong> and the <strong class="calibre1">City</strong> the customer lives in:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">CustomerID</strong></td>
<td class="calibre7"><strong class="calibre1">City</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">1</td>
<td class="calibre7">Boston</td>
</tr>
<tr class="calibre6">
<td class="calibre7">2</td>
<td class="calibre7">New York</td>
</tr>
<tr class="calibre6">
<td class="calibre7">3</td>
<td class="calibre7">Philadelphia</td>
</tr>
<tr class="calibre6">
<td class="calibre7">4</td>
<td class="calibre7">Boston</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">Joining the transaction table with the customer info table will generate a view as follows:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">CustomerID</strong></td>
<td class="calibre7"><strong class="calibre1">Purchased item</strong></td>
<td class="calibre7"><strong class="calibre1">Price paid</strong></td>
<td class="calibre7"><strong class="calibre1">City</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">1</td>
<td class="calibre7">Headphone</td>
<td class="calibre7">25.00</td>
<td class="calibre7">Boston</td>
</tr>
<tr class="calibre6">
<td class="calibre7">2</td>
<td class="calibre7">Watch</td>
<td class="calibre7">100.00</td>
<td class="calibre7">New York</td>
</tr>
<tr class="calibre6">
<td class="calibre7">3</td>
<td class="calibre7">Keyboard</td>
<td class="calibre7">20.00</td>
<td class="calibre7">Philadelphia</td>
</tr>
<tr class="calibre6">
<td class="calibre7">1</td>
<td class="calibre7">Mouse</td>
<td class="calibre7">10.00</td>
<td class="calibre7">Boston</td>
</tr>
<tr class="calibre6">
<td class="calibre7">4</td>
<td class="calibre7">Cable</td>
<td class="calibre7">10.00</td>
<td class="calibre7">Boston</td>
</tr>
<tr class="calibre6">
<td class="calibre7">3</td>
<td class="calibre7">Headphone</td>
<td class="calibre7">30.00</td>
<td class="calibre7">Philadelphia</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">Now, we can use this joined view to generate a report of <strong class="calibre1">Total sale price</strong> by <strong class="calibre1">City</strong>:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">City</strong></td>
<td class="calibre7"><strong class="calibre1">#Items</strong></td>
<td class="calibre7"><strong class="calibre1">Total sale price</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">Boston</td>
<td class="calibre7">3</td>
<td class="calibre7">45.00</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Philadelphia</td>
<td class="calibre7">2</td>
<td class="calibre7">50.00</td>
</tr>
<tr class="calibre6">
<td class="calibre7">New York</td>
<td class="calibre7">1</td>
<td class="calibre7">100.00</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">Joins are an important function of Spark SQL, as they enable you to bring two datasets together, as seen previously. Spark, of course, is not only meant to generate reports, but is used to process data on a petabyte scale to handle real-time streaming use cases, machine learning algorithms, or plain analytics. In order to accomplish these goals, Spark provides the API functions needed.</p>
<p class="mce-root">A typical join between two datasets takes place using one or more keys of the left and right datasets and then evaluates a conditional expression on the sets of keys as a Boolean expression. If the result of the Boolean expression returns true, then the join is successful, else the joined DataFrame will not contain the corresponding join.</p>
<p class="mce-root">The join API has 6 different implementations:</p>
<pre class="calibre19">
<strong class="calibre1">join(right: dataset[_]): DataFrame</strong><br class="title-page-name"/>Condition-less inner join<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">join(right: dataset[_], usingColumn: String): DataFrame</strong><br class="title-page-name"/>Inner join with a single column<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">join(right: dataset[_], usingColumns: Seq[String]): DataFrame </strong><br class="title-page-name"/>Inner join with multiple columns<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">join(right: dataset[_], usingColumns: Seq[String], joinType: String): DataFrame</strong><br class="title-page-name"/>Join with multiple columns and a join type (inner, outer,....)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">join(right: dataset[_], joinExprs: Column): DataFrame</strong><br class="title-page-name"/>Inner Join using a join expression<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">join(right: dataset[_], joinExprs: Column, joinType: String): DataFrame </strong><br class="title-page-name"/>Join using a Join expression and a join type (inner, outer, ...)
</pre>
<p class="mce-root">We will use one of the APIs to understand how to use join APIs ; however, you can choose to use other APIs depending on the use case:</p>
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre19">
<strong class="calibre1"><span><span>def</span> </span> <span> <span>join</span><span>(right: dataset[_], joinExprs: Column, joinType: <span>String</span>)</span><span>: DataFrame</span> <br class="title-page-name"/></span></strong>Join with another DataFrame using the given join expression<br class="title-page-name"/><br class="title-page-name"/>right: Right side of the join.<br class="title-page-name"/>joinExprs: Join expression.<br class="title-page-name"/>joinType : Type of join to perform. Default is <em class="calibre8">inner</em> join<br class="title-page-name"/><br class="title-page-name"/><span>// Scala:</span><br class="title-page-name"/><strong class="calibre1"><span>import</span> org.apache.spark.sql.functions._<br class="title-page-name"/>import spark.implicits._</strong><br class="title-page-name"/><strong class="calibre1">df1.join(df2, $<span>"df1Key"</span> === $<span>"df2Key"</span>, <span>"outer"</span>)<br class="title-page-name"/></strong>
</pre></div>
</div>
<p class="mce-root">Note that joins will be covered in detail in the next few sections.</p>


            </article>

            
        </section>
    

        <section id="868AM1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Inner workings of join</h1>
                
            
            <article>
                
<p class="mce-root">Join works by operating on the partitions of a DataFrame using the multiple executors. However, the actual operations and the subsequent performance depends on the type of <kbd class="calibre11">join</kbd> and the nature of the datasets being joined. In the next section, we will look at the types of joins.</p>


            </article>

            
        </section>
    

        <section id="876R81-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Shuffle join</h1>
                
            
            <article>
                
<p class="mce-root">Join between two big datasets involves shuffle join where partitions of both left and right datasets are spread across the executors. Shuffles are expensive and it's important to analyze the logic to make sure the distribution of partitions and shuffles is done optimally. The following is an illustration of how shuffle join works internally:</p>
<div class="cdpaligncenter"><img class="image-border96" src="../images/00166.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="885BQ1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Broadcast join</h1>
                
            
            <article>
                
<p class="mce-root">A join between one large dataset and a smaller dataset can be done by broadcasting the smaller dataset to all executors where a partition from the left dataset exists. The following is an illustration of how a broadcast join works internally:</p>
<div class="cdpaligncenter"><img class="image-border97" src="../images/00194.jpeg"/></div>


            </article>

            
        </section>
    

        <section id="893SC1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Join types</h1>
                
            
            <article>
                
<p class="mce-root">The following is a table of the different types of joins. This is important, as the choice made when joining two datasets makes all the difference in the output, and also the performance.</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Join type</strong></td>
<td class="calibre7"><strong class="calibre1">Description</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">inner</strong></td>
<td class="calibre7">The inner join compares each row from <em class="calibre8">left</em> to rows from <em class="calibre8">right</em> and combines matched pair of rows from <em class="calibre8">left</em> and <em class="calibre8">right</em> datasets only when both have non-NULL values.</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">cross</strong></p>
</td>
<td class="calibre7">The cross join matches every row from <em class="calibre8">left</em> with every row from <em class="calibre8">right</em> generating a Cartesian cross product.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">outer, full, fullouter</strong></td>
<td class="calibre7">The full outer Join gives all rows in <em class="calibre8">left</em> and <em class="calibre8">right</em> filling in NULL if only in <em class="calibre8">right</em> or <em class="calibre8">left</em>.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">leftanti</strong></td>
<td class="calibre7"><span>The leftanti Join gives</span> <span>only</span> <span>rows in <em class="calibre8">left</em> based on non-existence on <em class="calibre8">right</em> side.</span></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">left, leftouter</strong></td>
<td class="calibre7">The leftouter Join gives all rows in <em class="calibre8">left</em> plus common rows of <em class="calibre8">left</em> and <em class="calibre8">right</em> (inner join). Fills in NULL if not in <em class="calibre8">right</em>.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">leftsemi</strong></td>
<td class="calibre7">The leftsemi Join gives only rows in <em class="calibre8">left</em> based on existence on <em class="calibre8">right</em> side. The does not include <em class="calibre8">right-</em>side values.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">right, rightouter</strong></td>
<td class="calibre7"><span>The rightouter Join gives all rows in <em class="calibre8">right</em> plus common rows of <em class="calibre8">left</em> and <em class="calibre8">right</em> (inner join). Fills in NULL if not in <em class="calibre8">left</em>.</span></td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">We will examine how the different join types work by using the sample datasets.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val statesPopulationDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesPopulation.csv")</strong><br class="title-page-name"/>statesPopulationDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1 more field]<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val statesTaxRatesDF = spark.read.option("header", "true").option("inferschema", "true").option("sep", ",").csv("statesTaxRates.csv")</strong><br class="title-page-name"/>statesTaxRatesDF: org.apache.spark.sql.DataFrame = [State: string, TaxRate: double]<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesPopulationDF.count</strong><br class="title-page-name"/>res21: Long = 357<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; statesTaxRatesDF.count</strong><br class="title-page-name"/>res32: Long = 47<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">%sql</strong><br class="title-page-name"/><strong class="calibre1">statesPopulationDF.createOrReplaceTempView("statesPopulationDF")</strong><br class="title-page-name"/><strong class="calibre1">statesTaxRatesDF.createOrReplaceTempView("statesTaxRatesDF")</strong><br class="title-page-name"/><br class="title-page-name"/>
</pre>


            </article>

            
        </section>
    

        <section id="8A2CU1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Inner join</h1>
                
            
            <article>
                
<p class="mce-root">Inner join results in rows from both <kbd class="calibre11">statesPopulationDF</kbd> and <kbd class="calibre11">statesTaxRatesDF</kbd> when state is non-NULL in both datasets.</p>
<div class="cdpaligncenter"><img class="image-border98" src="../images/00095.jpeg"/></div>
<p class="mce-root">Join the two datasets by the state column as follows:</p>
<pre class="calibre19">
<strong class="calibre1">val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "inner")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">%sql</strong><br class="title-page-name"/><strong class="calibre1">val joinDF = spark.sql("SELECT * FROM statesPopulationDF INNER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.count</strong><br class="title-page-name"/>res22: Long = 329<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.show</strong><br class="title-page-name"/>+--------------------+----+----------+--------------------+-------+<br class="title-page-name"/>| State|Year|Population| State|TaxRate|<br class="title-page-name"/>+--------------------+----+----------+--------------------+-------+<br class="title-page-name"/>| Alabama|2010| 4785492| Alabama| 4.0|<br class="title-page-name"/>| Arizona|2010| 6408312| Arizona| 5.6|<br class="title-page-name"/>| Arkansas|2010| 2921995| Arkansas| 6.5|<br class="title-page-name"/>| California|2010| 37332685| California| 7.5|<br class="title-page-name"/>| Colorado|2010| 5048644| Colorado| 2.9|<br class="title-page-name"/>| Connecticut|2010| 3579899| Connecticut| 6.35|
</pre>
<p class="mce-root">You can run the <kbd class="calibre11">explain()</kbd> on the <kbd class="calibre11">joinDF</kbd> to look at the execution plan:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; joinDF.explain</strong><br class="title-page-name"/>== Physical Plan ==<br class="title-page-name"/>*BroadcastHashJoin [State#570], [State#577], Inner, BuildRight<br class="title-page-name"/>:- *Project [State#570, Year#571, Population#572]<br class="title-page-name"/>: +- *Filter isnotnull(State#570)<br class="title-page-name"/>: +- *FileScan csv [State#570,Year#571,Population#572] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/spark-2.1.0-bin-hadoop2.7/statesPopulation.csv], PartitionFilters: [], PushedFilters: [IsNotNull(State)], ReadSchema: struct&lt;State:string,Year:int,Population:int&gt;<br class="title-page-name"/>+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))<br class="title-page-name"/> +- *Project [State#577, TaxRate#578]<br class="title-page-name"/> +- *Filter isnotnull(State#577)<br class="title-page-name"/> +- *FileScan csv [State#577,TaxRate#578] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/spark-2.1.0-bin-hadoop2.7/statesTaxRates.csv], PartitionFilters: [], PushedFilters: [IsNotNull(State)], ReadSchema: struct&lt;State:string,TaxRate:double&gt;
</pre>


            </article>

            
        </section>
    

        <section id="8B0TG1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Left outer join</h1>
                
            
            <article>
                
<p class="mce-root">Left outer join results in all rows from <span><kbd class="calibre11">statesPopulationDF</kbd>, including any common in</span> <kbd class="calibre11">statesPopulationDF</kbd> and <kbd class="calibre11">statesTaxRatesDF</kbd>.</p>
<div class="cdpaligncenter"><img class="image-border99" src="../images/00273.jpeg"/></div>
<p class="cdpalignleft1"><span>Join the two datasets by the state column, shown as follows:</span></p>
<pre class="calibre19">
<strong class="calibre1">val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "leftouter")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">%sql</strong><br class="title-page-name"/><strong class="calibre1">val joinDF = spark.sql("SELECT * FROM statesPopulationDF LEFT OUTER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.count</strong><br class="title-page-name"/>res22: Long = 357<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.show(5)</strong><br class="title-page-name"/>+----------+----+----------+----------+-------+<br class="title-page-name"/>| State|Year|Population| State|TaxRate|<br class="title-page-name"/>+----------+----+----------+----------+-------+<br class="title-page-name"/>| Alabama|2010| 4785492| Alabama| 4.0|<br class="title-page-name"/>| Alaska|2010| 714031| null| null|<br class="title-page-name"/>| Arizona|2010| 6408312| Arizona| 5.6|<br class="title-page-name"/>| Arkansas|2010| 2921995| Arkansas| 6.5|<br class="title-page-name"/>|California|2010| 37332685|California| 7.5|<br class="title-page-name"/>+----------+----+----------+----------+-------+
</pre>


            </article>

            
        </section>
    

        <section id="8BVE21-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Right outer join</h1>
                
            
            <article>
                
<p class="mce-root"><span>Right outer join results in all rows from <kbd class="calibre11">statesTaxRatesDF</kbd></span><span>, including any common in</span> <span><kbd class="calibre11">statesPopulationDF</kbd> and <kbd class="calibre11">statesTaxRatesDF</kbd></span><span>.</span></p>
<div class="cdpaligncenter"><img class="image-border100" src="../images/00319.jpeg"/></div>
<p class="mce-root"><span>Join the two datasets by the <kbd class="calibre11">State</kbd> column as follows:</span></p>
<pre class="calibre19">
<strong class="calibre1">val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "rightouter")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">%sql</strong><br class="title-page-name"/><strong class="calibre1">val joinDF = spark.sql("SELECT * FROM statesPopulationDF RIGHT OUTER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.count</strong><br class="title-page-name"/>res22: Long = 323<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.show</strong><br class="title-page-name"/>+--------------------+----+----------+--------------------+-------+<br class="title-page-name"/>| State|Year|Population| State|TaxRate|<br class="title-page-name"/>+--------------------+----+----------+--------------------+-------+<br class="title-page-name"/>| Colorado|2011| 5118360| Colorado| 2.9|<br class="title-page-name"/>| Colorado|2010| 5048644| Colorado| 2.9|<br class="title-page-name"/>| null|null| null|Connecticut| 6.35|<br class="title-page-name"/>| Florida|2016| 20612439| Florida| 6.0|<br class="title-page-name"/>| Florida|2015| 20244914| Florida| 6.0|<br class="title-page-name"/>| Florida|2014| 19888741| Florida| 6.0|
</pre>


            </article>

            
        </section>
    

        <section id="8CTUK1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Outer join</h1>
                
            
            <article>
                
<p class="mce-root"><span>Outer join results in all rows from</span> <span><kbd class="calibre11">statesPopulationDF</kbd> and <kbd class="calibre11">statesTaxRatesDF</kbd></span><span>.</span></p>
<div class="cdpaligncenter"><img class="image-border101" src="../images/00245.jpeg"/></div>
<p class="cdpalignleft1"><span>Join the two datasets by the <kbd class="calibre11">State</kbd> column as follows:</span></p>
<pre class="calibre19">
<strong class="calibre1">val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "fullouter")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">%sql</strong><br class="title-page-name"/><strong class="calibre1">val joinDF = spark.sql("SELECT * FROM statesPopulationDF FULL OUTER JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.count</strong><br class="title-page-name"/>res22: Long = 351<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.show</strong><br class="title-page-name"/>+--------------------+----+----------+--------------------+-------+<br class="title-page-name"/>| State|Year|Population| State|TaxRate|<br class="title-page-name"/>+--------------------+----+----------+--------------------+-------+<br class="title-page-name"/>| Delaware|2010| 899816| null| null|<br class="title-page-name"/>| Delaware|2011| 907924| null| null|<br class="title-page-name"/>| West Virginia|2010| 1854230| West Virginia| 6.0|<br class="title-page-name"/>| West Virginia|2011| 1854972| West Virginia| 6.0|<br class="title-page-name"/>| Missouri|2010| 5996118| Missouri| 4.225|<br class="title-page-name"/>| null|null| null|  Connecticut|   6.35|
</pre>


            </article>

            
        </section>
    

        <section id="8DSF61-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Left anti join</h1>
                
            
            <article>
                
<p class="mce-root"><span>Left anti join results in rows from only</span> <span><kbd class="calibre11">statesPopulationDF</kbd> if, and only if, there is NO corresponding row in</span> <kbd class="calibre11"><span>statesTaxRatesDF</span></kbd><span>.</span></p>
<div class="cdpaligncenter"><img class="image-border102" src="../images/00072.jpeg"/></div>
<p class="cdpalignleft1"><span>Join the two datasets by the <kbd class="calibre11">State</kbd> column as follows:</span></p>
<pre class="calibre19">
<strong class="calibre1">val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "leftanti")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">%sql</strong><br class="title-page-name"/><strong class="calibre1">val joinDF = spark.sql("SELECT * FROM statesPopulationDF LEFT ANTI JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.count</strong><br class="title-page-name"/>res22: Long = 28<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.show(5)</strong><br class="title-page-name"/>+--------+----+----------+<br class="title-page-name"/>| State|Year|Population|<br class="title-page-name"/>+--------+----+----------+<br class="title-page-name"/>| Alaska|2010| 714031|<br class="title-page-name"/>|Delaware|2010| 899816|<br class="title-page-name"/>| Montana|2010| 990641|<br class="title-page-name"/>| Oregon|2010| 3838048|<br class="title-page-name"/>| Alaska|2011| 722713|<br class="title-page-name"/>+--------+----+----------+
</pre>


            </article>

            
        </section>
    

        <section id="8EQVO1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Left semi join</h1>
                
            
            <article>
                
<p class="mce-root"><span>Left semi join results in rows from only</span> <span><kbd class="calibre11">statesPopulationDF</kbd> if, and only if, there is a corresponding row in <kbd class="calibre11">statesTaxRatesDF</kbd>.</span></p>
<div class="cdpaligncenter"><img class="image-border103" src="../images/00097.jpeg"/></div>
<p class="cdpalignleft1"><span>Join the two datasets by the state column as follows:</span></p>
<pre class="calibre19">
<strong class="calibre1">val joinDF = statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State") === statesTaxRatesDF("State"), "leftsemi")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">%sql</strong><br class="title-page-name"/><strong class="calibre1">val joinDF = spark.sql("SELECT * FROM statesPopulationDF LEFT SEMI JOIN statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.count</strong><br class="title-page-name"/>res22: Long = 322<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.show(5)</strong><br class="title-page-name"/>+----------+----+----------+<br class="title-page-name"/>| State|Year|Population|<br class="title-page-name"/>+----------+----+----------+<br class="title-page-name"/>| Alabama|2010| 4785492|<br class="title-page-name"/>| Arizona|2010| 6408312|<br class="title-page-name"/>| Arkansas|2010| 2921995|<br class="title-page-name"/>|California|2010| 37332685|<br class="title-page-name"/>| Colorado|2010| 5048644|<br class="title-page-name"/>+----------+----+----------+
</pre>


            </article>

            
        </section>
    

        <section id="8FPGA1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Cross join</h1>
                
            
            <article>
                
<p class="mce-root"><span>Cross join matches every row from</span> <em class="calibre8">left</em> <span>with every row from</span> <em class="calibre8">right,</em> <span>generating a Cartesian cross product</span>.</p>
<div class="cdpaligncenter"><img class="image-border104" src="../images/00312.jpeg"/></div>
<p class="cdpalignleft1"><span>Join the two datasets by the <kbd class="calibre11">State</kbd> column as follows:</span></p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val joinDF=statesPopulationDF.crossJoin(statesTaxRatesDF)</strong><br class="title-page-name"/><strong class="calibre1">joinDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 3 more fields]</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">%sql</strong><br class="title-page-name"/><strong class="calibre1">val joinDF = spark.sql("SELECT * FROM statesPopulationDF CROSS JOIN statesTaxRatesDF")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.count</strong><br class="title-page-name"/>res46: Long = 16450<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; joinDF.show(10)</strong><br class="title-page-name"/>+-------+----+----------+-----------+-------+<br class="title-page-name"/>| State|Year|Population| State|TaxRate|<br class="title-page-name"/>+-------+----+----------+-----------+-------+<br class="title-page-name"/>|Alabama|2010| 4785492| Alabama| 4.0|<br class="title-page-name"/>|Alabama|2010| 4785492| Arizona| 5.6|<br class="title-page-name"/>|Alabama|2010| 4785492| Arkansas| 6.5|<br class="title-page-name"/>|Alabama|2010| 4785492| California| 7.5|<br class="title-page-name"/>|Alabama|2010| 4785492| Colorado| 2.9|<br class="title-page-name"/>|Alabama|2010| 4785492|Connecticut| 6.35|<br class="title-page-name"/>|Alabama|2010| 4785492| Florida| 6.0|<br class="title-page-name"/>|Alabama|2010| 4785492| Georgia| 4.0|<br class="title-page-name"/>|Alabama|2010| 4785492| Hawaii| 4.0|<br class="title-page-name"/>|Alabama|2010| 4785492| Idaho| 6.0|<br class="title-page-name"/>+-------+----+----------+-----------+-------+
</pre>
<div class="packt_tip">You can also use join with cross jointype instead of calling the cross join API. <kbd class="calibre22">statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State").isNotNull, "cross").count</kbd>.</div>


            </article>

            
        </section>
    

        <section id="8GO0S1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Performance implications of join</h1>
                
            
            <article>
                
<p class="mce-root">The join type chosen directly impacts the performance of the join. This is because joins require the shuffling of data between executors to execute the tasks, hence different joins, and even the order of the joins, need to be considered when using join.</p>
<p class="mce-root">The following is a table you could use to refer to when writing <kbd class="calibre11">Join</kbd> code:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Join type</strong></td>
<td class="calibre7"><strong class="calibre1">Performance considerations and tips</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">inner</strong></td>
<td class="calibre7">Inner join requires the left and right tables to have the same column. If you have duplicate or multiple copies of the keys on either the left or right side, the join will quickly blow up into a sort of a Cartesian join, taking a lot longer to complete than if designed correctly to minimize the multiple keys.</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">cross</strong></p>
</td>
<td class="calibre7">Cross Join matches every row from <em class="calibre8">left</em> with every row from <em class="calibre8">right,</em> generating a Cartesian cross product. This is to be used with caution, as this is the worst performant join, to be used in specific use cases only.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">outer, full, fullouter</strong></td>
<td class="calibre7">Fullouter Join gives all rows in <em class="calibre8">left</em> and <em class="calibre8">right</em> filling in NULL if only in <em class="calibre8">right</em> or <em class="calibre8">left</em>. If used on tables with little in common, can result in very large results and thus slow performance.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">leftanti</strong></td>
<td class="calibre7"><span>Leftanti Join gives</span> <span>only</span> <span>rows in <em class="calibre8">left</em> based on non-existence on <em class="calibre8">right</em> side. Very good performance, as only one table is fully considered and the other is only checked for the join condition.</span></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">left, leftouter</strong></td>
<td class="calibre7">Leftouter Join gives all rows in <em class="calibre8">left</em> plus common rows of <em class="calibre8">left</em> and <em class="calibre8">right</em> (inner join). Fills in NULL if not in <em class="calibre8">right</em>. If used on tables with little in common, can result in very large results and thus slow performance.</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">leftsemi</strong></td>
<td class="calibre7">Leftsemi Join gives only rows in <em class="calibre8">left</em> based on existence on <em class="calibre8">right</em> side. Does not include <em class="calibre8">right</em> side values. <span>Very good performance, as only one table is fully considered and other is only checked for the join condition.</span></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">right, rightouter</strong></td>
<td class="calibre7"><span>Rightouter Join gives all rows in <em class="calibre8">right</em> plus common rows of <em class="calibre8">left</em> and <em class="calibre8">right</em> (inner join). Fills in NULL if not in <em class="calibre8">left</em>. Performance is similar to the leftouter join mentioned previously in this table.</span></td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section id="8HMHE1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="mce-root">In this chapter, we discussed the origin of DataFrames and how Spark SQL provides the SQL interface on top of DataFrames. The power of DataFrames is such that execution times have decreased manyfold over original RDD-based computations. Having such a powerful layer with a simple SQL-like interface makes them all the more powerful. We also looked at various APIs to create, and manipulate DataFrames, as well as digging deeper into the sophisticated features of aggregations, including <kbd class="calibre11">groupBy</kbd>, <kbd class="calibre11">Window</kbd>, <kbd class="calibre11">rollup</kbd>, and <kbd class="calibre11">cubes</kbd>. Finally, we also looked at the concept of joining datasets and the various types of joins possible, such as inner, outer, cross, and so on.</p>
<p class="mce-root">In the next chapter, we will explore the exciting world of real-time data processing and analytics in the <a href="part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 9</a>, <span><em class="calibre8">Stream Me Up, Scotty - Spark Streaming</em>.</span></p>
<p class="mce-root"> </p>


            </article>

            
        </section>
    </body></html>