["```py\n5, 5.2, 6, 4.7,…\n```", "```py\n1946, 41\n1947, 23\n1948, 16\n1949, 28\n1950, 20\n1951, 11\n1952, 19\n1953, 14\n1954, 39\n1955, 32\n1956, \n1957, 14\n1958, \n1959, 16\n1960, 6\n```", "```py\n# Load libraries\nimport numpy as np\nfrom matplotlib.pylab import frange\nimport matplotlib.pyplot as plt\n\nfill_data = lambda x : int(x.strip() or 0)\ndata = np.genfromtxt('president.txt',dtype=(int,int),converters={1:fill_data},\\\n            delimiter=\",\")\nx = data[:,0]\ny = data[:,1]\n\n# 2.Plot the data to look for any trends or values\nplt.close('all')\nplt.figure(1)\nplt.title(\"All data\")\nplt.plot(x,y,'ro')\nplt.xlabel('year')plt.ylabel('No Presedential Request')\n```", "```py\n#3.Calculate percentile values (25th, 50th,75th) for the data to understand data distribution\nperc_25 = np.percentile(y,25)\nperc_50 = np.percentile(y,50)\nperc_75 = np.percentile(y,75)\nprint\nprint \"25th Percentile    = %0.2f\"%(perc_25)\nprint \"50th Percentile    = %0.2f\"%(perc_50)\nprint \"75th Percentile    = %0.2f\"%(perc_75)\nprint\n#4.Plot these percentile values as reference in the plot we generated in the previous step.\n# Draw horizontal lines at 25,50 and 75th percentile\nplt.axhline(perc_25,label='25th perc',c='r')\nplt.axhline(perc_50,label='50th perc',c='g')\nplt.axhline(perc_75,label='75th perc',c='m')plt.legend(loc='best')\n```", "```py\n#5.Look for outliers if any in the data by visual inspection.\n# Remove outliers using mask function \n# Remove outliers 0 and 54\ny_masked = np.ma.masked_where(y==0,y)\n#  Remove point 54\ny_masked = np.ma.masked_where(y_masked==54,y_masked)\n\n#6 Plot the data again.\nplt.figure(2)\nplt.title(\"Masked data\")\nplt.plot(x,y_masked,'ro')\nplt.xlabel('year')\nplt.ylabel('No Presedential Request')\nplt.ylim(0,60)\n\n# Draw horizontal lines at 25,50 and 75th percentile\nplt.axhline(perc_25,label='25th perc',c='r')\nplt.axhline(perc_50,label='50th perc',c='g')\nplt.axhline(perc_75,label='75th perc',c='m')\nplt.legend(loc='best')plt.show()\n```", "```py\nfill_data = lambda x : int(x.strip() or 0)\n```", "```py\ndata = np.genfromtxt('president.txt',dtype=(int,int),converters={1:fill_data},delimiter=\",\")\n```", "```py\n>>> data[7:15]\narray([[1953,   14],\n       [1954,   39],\n       [1955,   32],\n       [1956,    0],\n       [1957,   14],\n       [1958,    0],\n       [1959,   16],\n       [1960,    6]])\n>>>\n```", "```py\nx = data[:,0]\ny = data[:,1]\n```", "```py\nplt.close('all')\n```", "```py\nplt.figure(1)\n```", "```py\nplt.title(\"All data\")\n```", "```py\nplt.plot(x,y,'ro')\n```", "```py\nplt.xlabel('year')\nplt.ylabel('No Presedential Request')\n```", "```py\n# Draw horizontal lines at 25,50 and 75th percentile\nplt.axhline(perc_25,label='25th perc',c='r')\nplt.axhline(perc_50,label='50th perc',c='g')\nplt.axhline(perc_75,label='75th perc',c='m')\nplt.legend(loc='best')\n```", "```py\n# Remove zero values\ny_masked = np.ma.masked_where(y==0,y)\n#  Remove 54\ny_masked = np.ma.masked_where(y_masked==54,y_masked)\n```", "```py\n# Load libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom collections import OrderedDict\nfrom matplotlib.pylab import frange\n\n# 1.Load the data and handle missing values.\nfill_data = lambda x : int(x.strip() or 0)\ndata = np.genfromtxt('president.txt',dtype=(int,int),converters={1:fill_data},delimiter=\",\")\nx = data[:,0]\ny = data[:,1]\n\n# 2.Group data using frequency (count of individual data points).\n# Given a set of points, Counter() returns a dictionary, where key is a data point,\n# and value is the frequency of data point in the dataset.\nx_freq = Counter(y)\nx_ = np.array(x_freq.keys())y_ = np.array(x_freq.values())\n```", "```py\n# 3.Group data by range of years\nx_group = OrderedDict()\ngroup= 5\ngroup_count=1\nkeys = []\nvalues = []\nfor i,xx in enumerate(x):\n    # Individual data point is appended to list keys\n    keys.append(xx)\n    values.append(y[i])\n    # If we have processed five data points (i.e. five years)\n    if group_count == group:\n        # Convert the list of keys to a tuple\n        # use the new tuple as the ke to x_group dictionary\n        x_group[tuple(keys)] = values\n        keys= []\n        values =[]\n        group_count = 1\n\n    group_count+=1\n# Accomodate the last batch of keys and values\nx_group[tuple(keys)] = values \n\nprint x_group\n# 4.Plot the grouped data as dot plot.\nplt.subplot(311)\nplt.title(\"Dot Plot by Frequency\")\n# Plot the frequency\nplt.plot(y_,x_,'ro')\nplt.xlabel('Count')\nplt.ylabel('# Presedential Request')\n# Set the min and max limits for x axis\nplt.xlim(min(y_)-1,max(y_)+1)\n\nplt.subplot(312)\nplt.title(\"Simple dot plot\")\nplt.xlabel('# Presendtial Request')plt.ylabel('Frequency')\n```", "```py\n# Prepare the data for simple dot plot\n# For every (item, frequency) pair create a \n# new x and y\n# where x is a list, created using using np.repeat\n# function, where the item is repeated frequency times.\n# y is a list between 0.1 and frequency/10, incremented\n# by 0.1\nfor key,value in x_freq.items():\n    x__ = np.repeat(key,value)\n    y__ = frange(0.1,(value/10.0),0.1)\n    try:\n        plt.plot(x__,y__,'go')\n    except ValueError:\n        print x__.shape, y__.shape\n    # Set the min and max limits of x and y axis\n    plt.ylim(0.0,0.4)\n    plt.xlim(xmin=-1) \n\nplt.xticks(x_freq.keys())\n\nplt.subplot(313)\nx_vals =[]\nx_labels =[]\ny_vals =[]\nx_tick = 1\nfor k,v in x_group.items():\n    for i in range(len(k)):\n        x_vals.append(x_tick)\n        x_label = '-'.join([str(kk) if not i else str(kk)[-2:] for i,kk in enumerate(k)])\n        x_labels.append(x_label)\n    y_vals.extend(list(v))\n    x_tick+=1\n\nplt.title(\"Dot Plot by Year Grouping\")\nplt.xlabel('Year Group')\nplt.ylabel('No Presedential Request')\ntry:\n    plt.plot(x_vals,y_vals,'ro')\nexcept ValueError:\n    print len(x_vals),len(y_vals)\n\nplt.xticks(x_vals,x_labels,rotation=-35)plt.show()\n```", "```py\ngroup= 5\ngroup_count=1\nkeys = []\nvalues = []\n```", "```py\nfor i,xx in enumerate(x):\nkeys.append(xx)\nvalues.append(y[i])\nif group_count == group:\nx_group[tuple(keys)] = values\nkeys= []\nvalues =[]\ngroup_count = 0\n    group_count+=1\nx_group[tuple(keys)] = values \n```", "```py\n# Load Librarires\nfrom sklearn.datasets import load_iris\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\n# 1\\. Load Iris dataset\ndata = load_iris()\nx = data['data']\ny = data['target']col_names = data['feature_names']\n```", "```py\n# 2.Perform a simple scatter plot. \n# Plot 6 graphs, combinations of our columns, sepal length, sepal width,\n# petal length and petal width.\nplt.close('all')\nplt.figure(1)\n# We want a plot with\n# 3 rows and 2 columns, 3 and 2 in\n# below variable signifies that.\nsubplot_start = 321\ncol_numbers = range(0,4)\n# Need it for labeling the graph\ncol_pairs = itertools.combinations(col_numbers,2)\nplt.subplots_adjust(wspace = 0.5)\n\nfor col_pair in col_pairs:\n    plt.subplot(subplot_start)\n    plt.scatter(x[:,col_pair[0]],x[:,col_pair[1]],c=y)\n    plt.xlabel(col_names[col_pair[0]])\n    plt.ylabel(col_names[col_pair[1]])\n    subplot_start+=1plt.show()\n```", "```py\n>>> x.shape\n(150, 4)\n>>> y.shape\n(150,)\n>>>\n```", "```py\n>>> data['feature_names']\n\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n>>>\n```", "```py\ncol_pairs = itertools.combinations(col_numbers,2)\n```", "```py\nplt.scatter(x[:,col_pair[0]],x[:,col_pair[1]],c=y)\n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import scale\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1\\. Load iris dataset\ndata = load_iris()\nx = data['data']\ny = data['target']\ncol_names = data['feature_names']\n\n# 2\\. Scale the variables, with mean value\nx = scale(x,with_std=False)\nx_ = x[1:26,]y_labels = range(1,26)\n```", "```py\n# 3\\. Plot the Heat map\nplt.close('all')\n\nplt.figure(1)\nfig,ax = plt.subplots()\nax.pcolor(x_,cmap=plt.cm.Greens,edgecolors='k')\nax.set_xticks(np.arange(0,x_.shape[1])+0.5)\nax.set_yticks(np.arange(0,x_.shape[0])+0.5)\nax.xaxis.tick_top()\nax.yaxis.tick_left()\nax.set_xticklabels(col_names,minor=False,fontsize=10)\nax.set_yticklabels(y_labels,minor=False,fontsize=10)plt.show()\n```", "```py\nx = scale(x,with_std=False)\n```", "```py\nx = x[1:26,]\ncol_names = data['feature_names']\ny_labels = range(1,26)\n```", "```py\nax.set_xticks(np.arange(0,x.shape[1])+0.5)\nax.set_yticks(np.arange(0,x.shape[0])+0.5)\n```", "```py\nax.xaxis.tick_top()\n```", "```py\nax.yaxis.tick_left()\n```", "```py\nax.set_xticklabels(col_names,minor=False,fontsize=10)\nax.set_yticklabels(y_labels,minor=False,fontsize=10)\n```", "```py\nx1 = x[0:50]\nx2 = x[50:99]\nx3 = x[100:149]\n\nx1 = scale(x1,with_std=False)\nx2 = scale(x2,with_std=False)\nx3 = scale(x3,with_std=False)\n\nplt.close('all')\nplt.figure(2)\nfig,(ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)\ny_labels = range(1,51)\n\nax1.set_xticks(np.arange(0,x.shape[1])+0.5)\nax1.set_yticks(np.arange(0,50,10))\n\nax1.xaxis.tick_bottom()\nax1.set_xticklabels(col_names,minor=False,fontsize=2)\n\nax1.pcolor(x1,cmap=plt.cm.Greens,edgecolors='k')\nax1.set_title(data['target_names'][0])\n\nax2.pcolor(x2,cmap=plt.cm.Greens,edgecolors='k')\nax2.set_title(data['target_names'][1])\n\nax3.pcolor(x3,cmap=plt.cm.Greens,edgecolors='k')\nax3.set_title(data['target_names'][2])plt.show()   \n```", "```py\n# Load Libraries\nfrom sklearn.datasets import load_iris\nimport numpy as np\nfrom scipy.stats import trim_mean\n\n# Load iris data\ndata = load_iris()\nx = data['data']\ny = data['target']col_names = data['feature_names']\n```", "```py\n# 1.\tCalculate and print the mean value of each column in the Iris dataset\nprint \"col name,mean value\"\nfor i,col_name in enumerate(col_names):\n    print \"%s,%0.2f\"%(col_name,np.mean(x[:,i]))\nprint    \n\n# 2.\tTrimmed mean calculation.\np = 0.1 # 10% trimmed mean\nprint\nprint \"col name,trimmed mean value\"\nfor i,col_name in enumerate(col_names):\n    print \"%s,%0.2f\"%(col_name,trim_mean(x[:,i],p))\nprint\n\n# 3.\tData dispersion, calculating and display the range values.\nprint \"col_names,max,min,range\"\nfor i,col_name in enumerate(col_names):\n    print \"%s,%0.2f,%0.2f,%0.2f\"%(col_name,max(x[:,i]),min(x[:,i]),max(x[:,i])-min(x[:,i]))\nprint\n```", "```py\n# 4.\tData dispersion, variance and standard deviation\nprint \"col_names,variance,std-dev\"\nfor i,col_name in enumerate(col_names):\n    print \"%s,%0.2f,%0.2f\"%(col_name,np.var(x[:,i]),np.std(x[:,i]))\nprint\n\n# 5.\tMean absolute deviation calculation  \ndef mad(x,axis=None):\n    mean = np.mean(x,axis=axis)\n    return np.sum(np.abs(x-mean))/(1.0 * len(x))\n\nprint \"col_names,mad\"\nfor i,col_name in enumerate(col_names):\n    print \"%s,%0.2f\"%(col_name,mad(x[:,i]))\nprint\n\n# 6.\tMedian absolute deviation calculation\ndef mdad(x,axis=None):\n    median = np.median(x,axis=axis)\n    return np.median(np.abs(x-median))\n\nprint \"col_names,median,median abs dev,inter quartile range\"\nfor i,col_name in enumerate(col_names):\n    iqr = np.percentile(x[:,i],75) - np.percentile(x[i,:],25)\n    print \"%s,%0.2f,%0.2f,%0.2f\"%(col_name,np.median(x[:,i]), mdad(x[:,i]),iqr)\nprint\n```", "```py\nnp.mean(x[:,i])\n```", "```py\ndef mad(x,axis=None):\nmean = np.mean(x,axis=axis)\nreturn np.sum(np.abs(x-mean))/(1.0 * len(x))\n```", "```py\n>>> \n>>> a = [8,9,10,11]\n>>> np.median(a)\n9.5\n>>> np.percentile(a,50)\n9.5\n```", "```py\ndef mdad(x,axis=None):\nmedian = np.median(x,axis=axis)\nreturn np.median(np.abs(x-median))\n```", "```py\n# Load Libraries\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n# Load Iris dataset\ndata = load_iris()\nx = data['data']\nplt.close('all')\n```", "```py\n# Plot the box and whisker\nfig = plt.figure(1)\nax = fig.add_subplot(111)\nax.boxplot(x)\nax.set_xticklabels(data['feature_names'])\nplt.show()    \n```", "```py\ny=data['target']\nclass_labels = data['target_names']\n\nfig = plt.figure(2,figsize=(18,10))\nsub_plt_count = 321\nfor t in range(0,3):\n    ax = fig.add_subplot(sub_plt_count)\n    y_index = np.where(y==t)[0]\n    x_ = x[y_index,:]\n    ax.boxplot(x_)\n    ax.set_title(class_labels[t])   \n    ax.set_xticklabels(data['feature_names'])\n    sub_plt_count+=1\nplt.show()\n```", "```py\n# Load Libraries\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import Imputer\nimport numpy as np\nimport numpy.ma as ma\n\n# 1\\. Load Iris Data Set\ndata = load_iris()\nx = data['data']\ny = data['target']\n\n# Make a copy of hte original x value\nx_t = x.copy()\n\n# 2.\tIntroduce missing values into second row\nx_t[2,:] = np.repeat(0,x.shape[1])\n```", "```py\n# 3.\tNow create an imputer object with strategy as mean, \n# i.e. fill the missing values with the mean value of the missing column.\nimputer = Imputer(missing_values=0,strategy=\"mean\")\nx_imputed = imputer.fit_transform(x_t)\n\nmask = np.zeros_like(x_t)\nmask[2,:] = 1\nx_t_m = ma.masked_array(x_t,mask)\n\nprint np.mean(x_t_m,axis=0)print x_imputed[2,:]\n```", "```py\nimputer = Imputer(missing_values=0,strategy=\"mean\")\n```", "```py\n>>> x[2,:]\narray([ 4.7,  3.2,  1.3,  0.2])\n\n```", "```py\nimport numpy.ma as ma\nmask = np.zeros_like(x_t)\nmask[2,:] = 1\nx_t_m = ma.masked_array(x_t,mask)\n\nprint np.mean(x_t_m,axis=0)\n```", "```py\n[5.851006711409397 3.053020134228189 3.7751677852349017 1.2053691275167793]\n```", "```py\nprint x_imputed[2,:]\n```", "```py\n[ 5.85100671  3.05302013  3.77516779  1.20536913]\n```", "```py\n# Impute based on class label\nmissing_y = y[2]\nx_missing = np.where(y==missing_y)[0]\ny = data['target']\n# Mean stragegy \nprint np.mean(x[x_missing,:],axis=0)\n# Median stragegy\nprint np.median(x[x_missing,:],axis=0)\n```", "```py\nmissing_y = y[2]\n```", "```py\nx_missing = np.where(y==missing_y)[0]\n```", "```py\n# Mean stragegy \nprint np.mean(x[x_missing,:],axis=0)\n# Median stragegy\nprint np.median(x[x_missing,:],axis=0)\n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_iris\nimport numpy as np\n\n# 1.\tLoad the Iris data set\ndata = load_iris()\nx = data['data']\n```", "```py\n# 2.\tRandomly sample 10 records from the loaded dataset\nno_records = 10\nx_sample_indx = np.random.choice(range(x.shape[0]),no_records)\nprint x[x_sample_indx,:]\n```", "```py\n# Load Libraries\nimport numpy as np\n\n# 1.\tGenerate some random data for scaling\nnp.random.seed(10)\nx = [np.random.randint(10,25)*1.0 for i in range(10)]\n```", "```py\n# 2.Define a function, which can perform min max scaling given a list of numbers\ndef min_max(x):\n    return [round((xx-min(x))/(1.0*(max(x)-min(x))),2) for xx in x]\n\n# 3.Perform scaling on the given input list.    \nprint x \nprint min_max(x)    \n```", "```py\nx_scaled = x – min(x) / max(x) –min (x)\n```", "```py\n[19, 23, 14, 10, 11, 21, 22, 19, 23, 10]\n```", "```py\n[0.69, 1.0, 0.31, 0.0, 0.08, 0.85, 0.92, 0.69, 1.0, 0.0]\n```", "```py\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\nnp.random.seed(10)\nx = np.matrix([np.random.randint(10,25)*1.0 for i in range(10)])\nx = x.T\nminmax = MinMaxScaler(feature_range=(0.0,1.0))\nprint x\nx_t = minmax.fit_transform(x)\nprint x_t\n```", "```py\n[19.0, 23.0, 14.0, 10.0, 11.0, 21.0, 22.0, 19.0, 23.0, 10.0]\n[0.69, 1.0, 0.31, 0.0, 0.08, 0.85, 0.92, 0.69, 1.0, 0.0]\n```", "```py\nx_scaled =  ( x – min(x) / max(x) –min (x) ) * (nr_max- nr_min) + nr_min\n```", "```py\nimport numpy as np\n\nnp.random.seed(10)\nx = [np.random.randint(10,25)*1.0 for i in range(10)]\n\ndef min_max_range(x,range_values):\n    return [round( ((xx-min(x))/(1.0*(max(x)-min(x))))*(range_values[1]-range_values[0]) \\\n    + range_values[0],2) for xx in x]\n\nprint min_max_range(x,(100,200))\n```", "```py\nprint min_max_range(x,(100,200))\n\n[169.23, 200.0, 130.77, 100.0, 107.69, 184.62, 192.31, 169.23, 200.0, 100.0]\n```", "```py\n# Load Libraries\nimport numpy as np\nfrom sklearn.preprocessing import scale\n\n# Input data generation\nnp.random.seed(10)\nx = [np.random.randint(10,25)*1.0 for i in range(10)]\n```", "```py\nx_centered = scale(x,with_mean=True,with_std=False)\nx_standard = scale(x,with_mean=True,with_std=True)\n\nprint x\nprint x_centered\nprint x_standard\nprint \"Orginal x mean = %0.2f, Centered x mean = %0.2f, Std dev of \\\n        standard x =%0.2f\"%(np.mean(x),np.mean(x_centered),np.std(x_standard))\n```", "```py\nx = [np.random.randint(10,25)*1.0 for i in range(10)]\n```", "```py\nx_centered = scale(x,with_mean=True,with_std=False)\nx_standard = scale(x,with_mean=True,with_std=True)\n```", "```py\n[19.0, 23.0, 14.0, 10.0, 11.0, 21.0, 22.0, 19.0, 23.0, 10.0]\n\nNext, we will print x_centered, where we centered it with the mean value:\n\n[ 1.8  5.8 -3.2 -7.2 -6.2  3.8  4.8  1.8  5.8 -7.2]\n\nFinally we will print x_standardized, where we used both the mean and standard deviation:\n\n[ 0.35059022  1.12967961 -0.62327151 -1.4023609  -1.20758855  0.74013492\n  0.93490726  0.35059022  1.12967961 -1.4023609 ]\n\nOrginal x mean = 17.20, Centered x mean = 0.00, Std dev of standard x =1.00\n```", "```py\n# Load Libraries\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\n\n# 1.Let us use a very simple text to demonstrate tokenization\n# at sentence level and word level. You have seen this example in the\n# dictionary recipe, except for some punctuation which are added.\n\nsentence = \"Peter Piper picked a peck of pickled peppers. A peck of pickled \\\npeppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled \\\npeppers, Wheres the peck of pickled peppers Peter Piper picked ?\"\n\n# 2.Using nltk sentence tokenizer, we tokenize the given text into sentences\n# and verify the output using some print statements.\n\nsent_list = sent_tokenize(sentence)\n\nprint \"No sentences = %d\"%(len(sent_list))\nprint \"Sentences\"\nfor sent in sent_list: print sent\n\n# 3.With the sentences extracted let us proceed to extract\n# words from these sentences.\nword_dict = defaultdict(list)\nfor i,sent in enumerate(sent_list):\n    word_dict[i].extend(word_tokenize(sent))\n\nprint word_dict\n```", "```py\ndef sent_tokenize(text, language='english'):\n    \"\"\"\n    Return a sentence-tokenized copy of *text*,\n    using NLTK's recommended sentence tokenizer\n    (currently :class:`.PunktSentenceTokenizer`\n    for the specified language).\n\n    :param text: text to split into sentences\n    :param language: the model name in the Punkt corpus\n    \"\"\"\n    tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))\n    return tokenizer.tokenize(text)\n```", "```py\nNo sentences = 3\nSentences\nPeter Piper picked a peck of pickled peppers.\nA peck of pickled             peppers, Peter Piper picked !!!\nIf Peter Piper picked a peck of pickled             peppers, Wheres the peck of pickled peppers Peter Piper picked ?\n```", "```py\ndefaultdict(<type 'list'>, {0: ['Peter', 'Piper', 'picked', 'a', 'peck', 'of', 'pickled', 'peppers', '.'], 1: ['A', 'peck', 'of', 'pickled', 'peppers', ',', 'Peter', 'Piper', 'picked', '!', '!', '!'], 2: ['If', 'Peter', 'Piper', 'picked', 'a', 'peck', 'of', 'pickled', 'peppers', ',', 'Wheres', 'the', 'peck', 'of', 'pickled', 'peppers', 'Peter', 'Piper', 'picked', '?']})\n```", "```py\n# Load Libraries\nfrom nltk.tokenize import line_tokenize\n\nsentence = \"Peter Piper picked a peck of pickled peppers. A peck of pickled \\\npeppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled \\\npeppers, Wheres the peck of pickled peppers Peter Piper picked ?\"\n\nsent_list = line_tokenize(sentence)\nprint \"No sentences = %d\"%(len(sent_list))\nprint \"Sentences\"\nfor sent in sent_list: print sent\n\n# Include new line characters\nsentence = \"Peter Piper picked a peck of pickled peppers. A peck of pickled\\n \\\npeppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled\\n \\\npeppers, Wheres the peck of pickled peppers Peter Piper picked ?\"\n\nsent_list = line_tokenize(sentence)\nprint \"No sentences = %d\"%(len(sent_list))\nprint \"Sentences\"\nfor sent in sent_list: print sent\n```", "```py\nNo sentences = 1\nSentences\nPeter Piper picked a peck of pickled peppers. A peck of pickled             peppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled             peppers, Wheres the peck of pickled peppers Peter Piper picked ?\n```", "```py\nsentence = \"Peter Piper picked a peck of pickled peppers. A peck of pickled\\n \\\npeppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled\\n \\\npeppers, Wheres the peck of pickled peppers Peter Piper picked ?\"\n```", "```py\nNo sentences = 3\nSentences\nPeter Piper picked a peck of pickled peppers. A peck of pickled\n             peppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled\n             peppers, Wheres the peck of pickled peppers Peter Piper picked ?\n```", "```py\n>>> from nltk.corpus import stopwords\n>>> stopwords.words('english')\n[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now']\n>>>\n```", "```py\n# Load libraries\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport string\n\ntext = \"Text mining, also referred to as text data mining, roughly equivalent to text analytics,\\\nrefers to the process of deriving high-quality information from text. High-quality information is \\\ntypically derived through the devising of patterns and trends through means such as statistical \\\npattern learning. Text mining usually involves the process of structuring the input text \\\n(usually parsing, along with the addition of some derived linguistic features and the removal \\\nof others, and subsequent insertion into a database), deriving patterns within the structured data, \\\nand finally evaluation and interpretation of the output. 'High quality' in text mining usually \\\nrefers to some combination of relevance, novelty, and interestingness. Typical text mining tasks \\\ninclude text categorization, text clustering, concept/entity extraction, production of granular \\\ntaxonomies, sentiment analysis, document summarization, and entity relation modeling \\\n(i.e., learning relations between named entities).Text analysis involves information retrieval, \\\nlexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, \\\ninformation extraction, data mining techniques including link and association analysis, \\\nvisualization, and predictive analytics. The overarching goal is, essentially, to turn text \\\ninto data for analysis, via application of natural language processing (NLP) and analytical \\\nmethods.A typical application is to scan a set of documents written in a natural language and \\\neither model the document set for predictive classification purposes or populate a database \\\nor search index with the information extracted.\"\n```", "```py\nwords = word_tokenize(text)\n# 2.Let us get the list of stopwords from nltk stopwords english corpus.\nstop_words = stopwords.words('english')\n\nprint \"Number of words = %d\"%(len(words)) \n# 3.\tFilter out the stop words.\nwords = [w for w in words if w not in stop_words]\nprint \"Number of words,without stop words = %d\"%(len(words)) \n\nwords = [w for w in words if w not in string.punctuation]\nprint \"Number of words,without stop words and punctuations = %d\"%(len(words))\n```", "```py\nNumber of words = 259\n```", "```py\nNumber of words,without stop words = 195\n```", "```py\nimport string\nwords = [w for w in words if w not in string.punctuation]\nprint \"Number of words,without stop words and punctuations = %d\"%(len(words)) \n```", "```py\nNumber of words, without stop words and punctuations = 156\n```", "```py\n>>> import nltk.stem\n>>> dir(nltk.stem)\n['ISRIStemmer', 'LancasterStemmer', 'PorterStemmer', 'RSLPStemmer', 'RegexpStemmer', 'SnowballStemmer', 'StemmerI', 'WordNetLemmatizer', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', 'api', 'isri', 'lancaster', 'porter', 'regexp', 'rslp', 'snowball', 'wordnet']\n>>>  \n```", "```py\n# Load Libraries\nfrom nltk import stem\n\n#1\\. small input to figure out how the three stemmers perform.\ninput_words = ['movies','dogs','planes','flowers','flies','fries','fry','weeks','planted','running','throttle']\n```", "```py\n#2.Porter Stemming\nporter = stem.porter.PorterStemmer()\np_words = [porter.stem(w) for w in input_words]\nprint p_words\n\n#3.Lancaster Stemming\nlancaster = stem.lancaster.LancasterStemmer()\nl_words = [lancaster.stem(w) for w in input_words]\nprint l_words\n\n#4.Snowball stemming\nsnowball = stem.snowball.EnglishStemmer()\ns_words = [snowball.stem(w) for w in input_words]\nprint s_words\n\nwordnet_lemm = stem.WordNetLemmatizer()\nwn_words = [wordnet_lemm.lemmatize(w) for w in input_words]\nprint wn_words\n```", "```py\n[u'movi', u'dog', u'plane', u'flower', u'fli', u'fri', u'fri', u'week', u'plant', u'run', u'throttl']\n```", "```py\nMovies – movi\nDogs   - dog\nPlanes – plane\nRunning – run and so on.\n```", "```py\n[u'movy', 'dog', 'plan', 'flow', 'fli', 'fri', 'fry', 'week', 'plant', 'run', 'throttle']\n```", "```py\n[u'movi', u'dog', u'plane', u'flower', u'fli', u'fri', u'fri', u'week', u'plant', u'run', u'throttl']\n```", "```py\n# Load Libraries\nfrom nltk import stem\n\n#1\\. small input to figure out how the three stemmers perform.\ninput_words = ['movies','dogs','planes','flowers','flies','fries','fry','weeks', 'planted','running','throttle']\n\n#2.Perform lemmatization.\nwordnet_lemm = stem.WordNetLemmatizer()\nwn_words = [wordnet_lemm.lemmatize(w) for w in input_words]\nprint wn_words\n```", "```py\n[u'movie', u'dog', u'plane', u'flower', u'fly', u'fry', 'fry', u'week', 'planted', 'running', 'throttle']\n```", "```py\n>>> wordnet_lemm.lemmatize('running')\n'running'\n>>> porter.stem('running')\nu'run'\n>>> lancaster.stem('running')\n'run'\n>>> snowball.stem('running')\nu'run'\n```", "```py\n>>> wordnet_lemm.lemmatize('running','v') u'run'\n```", "```py\n# Load Libraries\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\n# 1.\tOur input text, we use the same input which we had used in stop word removal recipe.\ntext = \"Text mining, also referred to as text data mining, roughly equivalent to text analytics,\\\nrefers to the process of deriving high-quality information from text. High-quality information is \\\ntypically derived through the devising of patterns and trends through means such as statistical \\\npattern learning. Text mining usually involves the process of structuring the input text \\\n(usually parsing, along with the addition of some derived linguistic features and the removal \\\nof others, and subsequent insertion into a database), deriving patterns within the structured data, \\\nand finally evaluation and interpretation of the output. 'High quality' in text mining usually \\\nrefers to some combination of relevance, novelty, and interestingness. Typical text mining tasks \\\ninclude text categorization, text clustering, concept/entity extraction, production of granular \\\ntaxonomies, sentiment analysis, document summarization, and entity relation modeling \\\n(i.e., learning relations between named entities).Text analysis involves information retrieval, \\\nlexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, \\\ninformation extraction, data mining techniques including link and association analysis, \\\nvisualization, and predictive analytics. The overarching goal is, essentially, to turn text \\\ninto data for analysis, via application of natural language processing (NLP) and analytical \\\nmethods.A typical application is to scan a set of documents written in a natural language and \\\neither model the document set for predictive classification purposes or populate a database \\\nor search index with the information extracted.\"\n```", "```py\n#2.Let us divide the given text into sentences\nsentences = sent_tokenize(text)\n\n#3.Let us write the code to generate feature vectors.\ncount_v = CountVectorizer()\ntdm = count_v.fit_transform(sentences)\n\n# While creating a mapping from words to feature indices, we can ignore\n# some words by providing a stop word list.\nstop_words = stopwords.words('english')\ncount_v_sw = CountVectorizer(stop_words=stop_words)\nsw_tdm = count_v.fit_transform(sentences)\n\n# Use ngrams\ncount_v_ngram = CountVectorizer(stop_words=stop_words,ngram_range=(1,2))\nngram_tdm = count_v.fit_transform(sentences)\n```", "```py\n>>> len(sentences)\n6\n>>>\n```", "```py\n>>> count_v.get_feature_names()\n```", "```py\n>>> type(tdm)\n<class 'scipy.sparse.csr.csr_matrix'>\n>>>\n```", "```py\n>>> tdm.data[0:17]\narray([4, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)\n>>> tdm.indices[0:17]\narray([107,  60,   2,  83, 110,   9,  17,  90,  28,   5,  84, 108,  77,\n        67,  20,  40,  81])\n>>>\n```", "```py\n>>> count_v.get_feature_names()[107]\nu'text'\n>>> count_v.get_feature_names()[60]\nu'mining'\n```", "```py\n>>> count_v.get_params()\n{'binary': False, 'lowercase': True, 'stop_words': None, 'vocabulary': None, 'tokenizer': None, 'decode_error': u'strict', 'dtype': <type 'numpy.int64'>, 'charset_error': None, 'charset': None, 'analyzer': u'word', 'encoding': u'utf-8', 'ngram_range': (1, 1), 'max_df': 1.0, 'min_df': 1, 'max_features': None, 'input': u'content', 'strip_accents': None, 'token_pattern': u'(?u)\\\\b\\\\w\\\\w+\\\\b', 'preprocessor': None}\n>>>\t\n```", "```py\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\ncount_v = CountVectorizer(stop_words=stop_words)\nsw_tdm = count_v.fit_transform(sentences)\n```", "```py\n>>> len(count_v_sw.vocabulary_)\n106\n>>>\n```", "```py\ncount_v_ngram = CountVectorizer(stop_words=stop_words,ngram_range=(1,2))\nngram_tdm = count_v.fit_transform(sentences)\n```", "```py\n# Load Libraries\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# 1.\tWe create an input document as in the previous recipe.\n\ntext = \"Text mining, also referred to as text data mining, roughly equivalent to text analytics,\\\nrefers to the process of deriving high-quality information from text. High-quality information is \\\ntypically derived through the devising of patterns and trends through means such as statistical \\\npattern learning. Text mining usually involves the process of structuring the input text \\\n(usually parsing, along with the addition of some derived linguistic features and the removal \\\nof others, and subsequent insertion into a database), deriving patterns within the structured data, \\\nand finally evaluation and interpretation of the output. 'High quality' in text mining usually \\\nrefers to some combination of relevance, novelty, and interestingness. Typical text mining tasks \\\ninclude text categorization, text clustering, concept/entity extraction, production of granular \\\ntaxonomies, sentiment analysis, document summarization, and entity relation modeling \\\n(i.e., learning relations between named entities).Text analysis involves information retrieval, \\\nlexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, \\\ninformation extraction, data mining techniques including link and association analysis, \\\nvisualization, and predictive analytics. The overarching goal is, essentially, to turn text \\\ninto data for analysis, via application of natural language processing (NLP) and analytical \\\nmethods.A typical application is to scan a set of documents written in a natural language and \\\neither model the document set for predictive classification purposes or populate a database \\\nor search index with the information extracted.\"\n```", "```py\n# 2.\tLet us extract the sentences.\nsentences = sent_tokenize(text)\n\n# 3.\tCreate a matrix of term document frequency.\nstop_words = stopwords.words('english')\n\ncount_v = CountVectorizer(stop_words=stop_words)\ntdm = count_v.fit_transform(sentences)\n\n#4.\tCalcuate the TFIDF score.\ntfidf = TfidfTransformer()\ntdm_tfidf = tfidf.fit_transform(tdm)\n```", "```py\n>>> type(tdm)\n<class 'scipy.sparse.csr.csr_matrix'>\n>>>\n```", "```py\n>>> tfidf.get_params()\n{'use_idf': True, 'smooth_idf': True, 'sublinear_tf': False, 'norm': u'l2'}\n>>>\n```"]