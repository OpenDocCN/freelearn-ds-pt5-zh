<html><head></head><body>
        

                            
                    <h1 class="header-title">Big Data With Hadoop</h1>
                
            
            
                
<p>Hadoop has become the de facto standard in the world of big data, especially over the past three to four years. Hadoop started as a subproject of Apache Nutch in 2006 and introduced two key features related to distributed filesystems and distributed computing, also known as MapReduce, that caught on very rapidly among the open source community. Today, there are thousands of new products that have been developed leveraging the core features of Hadoop, and it has evolved into a vast ecosystem consisting of more than 150 related major products. Arguably, Hadoop was one of the primary catalysts that started the big data and analytics industry.</p>
<p>In this chapter, we will discuss the background and core concepts of Hadoop, the components of the Hadoop platform, and delve deeper into the major products in the Hadoop ecosystem. We will learn about the core concepts of distributed filesystems and distributed processing and optimizations to improve the performance of Hadoop deployments. We'll conclude with real-world hands-on exercises using the <strong>Cloudera Distribution of Hadoop</strong> (<strong>CDH</strong>). The topics we will cover are:</p>
<ul>
<li>The basics of Hadoop</li>
<li>The core components of Hadoop</li>
<li>Hadoop 1 and Hadoop 2</li>
<li>The Hadoop Distributed File System</li>
<li>Distributed computing principles with MapReduce</li>
<li>The Hadoop ecosystem</li>
<li>Overview of the Hadoop ecosystem</li>
<li>Hive, HBase, and more</li>
<li>Hadoop Enterprise deployments</li>
<li>In-house deployments</li>
<li>Cloud deployments</li>
<li>Hands-on with Cloudera Hadoop</li>
<li>Using HDFS</li>
<li>Using Hive</li>
<li>MapReduce with WordCount</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">The fundamentals of Hadoop</h1>
                
            
            
                
<p>In 2006, Doug Cutting, the creator of Hadoop, was working at Yahoo!. He was actively engaged in an open source project called Nutch that involved the development of a large-scale web crawler. A web crawler at a high level is essentially software that can browse and index web pages, generally in an automatic manner, on the internet. Intuitively, this involves efficient management and computation across large volumes of data. In late January of 2006, Doug formally announced the start of Hadoop. The first line of the request, still available on the internet at <a href="https://issues.apache.org/jira/browse/INFRA-700">https://issues.apache.org/jira/browse/INFRA-700,</a> was <em>The Lucene PMC has voted to split part of Nutch into a new subproject named Hadoop</em>. And thus, Hadoop was born.</p>
<p>At the onset, Hadoop had two core components : <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>) and MapReduce. This was the first iteration of Hadoop, also now known as Hadoop 1. Later, in 2012, a third component was added known as <strong>YARN</strong> (<strong>Yet Another Resource Negotiator</strong>) which decoupled the process of resource management and job scheduling. Before we delve into the core components in more detail, it would help to get an understanding of the fundamental premises of Hadoop:</p>
<div><img height="443" width="639" src="img/5dbca749-d341-4994-ad32-89f3820278d2.png"/></div>
<p>Doug Cutting's post at <a href="https://issues.apache.org/jira/browse/NUTCH-193">https://issues.apache.org/jira/browse/NUTCH-193</a> announced his intent to separate <strong>Nutch Distributed FS</strong> (<strong>NDFS</strong>) and MapReduce to a new subproject called Hadoop.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The fundamental premise of Hadoop</h1>
                
            
            
                
<p>The fundamental premise of Hadoop is that instead of attempting to perform a task on a single large machine, the task can be subdivided into smaller segments that can then be delegated to multiple smaller machines. These so-called smaller machines would then perform the task on their own portion of the data. Once the smaller machines have completed their tasks to produce the results on the tasks they were allocated, the individual units of results would then be aggregated to produce the final result.</p>
<p>Although, in theory, this may appear relatively simple, there are various technical considerations to bear in mind. For example:</p>
<ul>
<li>Is the network fast enough to collect the results from each individual server?</li>
<li>Can each individual server read data fast enough from the disk?</li>
<li>If one or more of the servers fail, do we have to start all over?</li>
<li>If there are multiple large tasks, how should they be prioritized?</li>
</ul>
<p>There are many more such considerations that must be considered when working with a distributed architecture of this nature.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The core modules of Hadoop</h1>
                
            
            
                
<p>The core modules of Hadoop consist of:</p>
<ul>
<li><strong>Hadoop Common</strong>: Libraries and other common helper utilities required by Hadoop</li>
<li><strong>HDFS</strong>: A distributed, highly-available, fault-tolerant filesystem that stores data</li>
<li><strong>Hadoop MapReduce</strong>: A programming paradigm involving distributed computing across commodity servers (or nodes)</li>
<li><strong>Hadoop YARN</strong>: A framework for job scheduling and resource management</li>
</ul>
<p>Of these core components, YARN was introduced in 2012 to address some of the shortcomings of the first release of Hadoop. The first version of Hadoop (or equivalently, the first model of Hadoop) used HDFS and MapReduce as its main components. As Hadoop gained in popularity, the need to use facilities beyond those provided by MapReduce became more and more important. This, along with some other technical considerations, led to the development of YARN.</p>
<p>Let's now look at the salient characteristics of Hadoop as itemized previously.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hadoop Distributed File System - HDFS</h1>
                
            
            
                
<p>The HDFS forms the underlying basis of all Hadoop installations. Files, or more generally data, is stored in HDFS and accessed by the nodes of Hadoop.</p>
<p>HDFS performs two main functions:</p>
<ul>
<li><strong>Namespaces</strong>: Provides namespaces that hold cluster metadata, that is, the location of data in the Hadoop cluster</li>
<li><strong>Data storage</strong>: Acts as storage for data used in the Hadoop cluster</li>
</ul>
<p>The filesystem is termed as distributed since the data is stored in chunks across multiple servers. An intuitive understanding of HDFS can be gained from a simple example, as follows. Consider a large book that consists of Chapters A - Z. In ordinary filesystems, the entire book would be stored as a single file on the disk. In HDFS, the book would be split into smaller chunks, say a chunk for Chapters A - H, another for I - P, and a third one for Q - Z. These chunks are then stored in separate racks (or bookshelves as with this analogy). Further, the chapters are replicated three times, such that there are three copies of each of the chapters.</p>
<p>Suppose, further, the size of the entire book is 1 GB, and each chapter is approximately 350 MB:</p>
<div><img height="209" width="261" src="img/46a63885-0e9f-4d87-a5d3-c9edb3be3e6a.png"/></div>
<p>A bookshelf analogy for HDFS</p>
<p>Storing the book in this manner achieves a few important objectives:</p>
<ul>
<li>Since the book has been split into three parts by groups of chapters and each part has been replicated three times, it means that our process can read the book in parallel by querying the parts from different servers. This reduces I/O contention and is a very fitting example of the proper use of parallelism.</li>
<li>If any of the racks are not available, we can retrieve the chapters from any of the other racks as there are multiple copies of each chapter available on different racks.</li>
<li>If a task I have been given only requires me to access a single chapter, for example, Chapter B, I need to access only the file corresponding to Chapters A-H. Since the size of the file corresponding to Chapters A-H is a third the size of the entire book, the time to access and read the file would be much smaller.</li>
<li>Other benefits, such as selective access rights to different chapter groups and so on, would also be possible with such a model.</li>
</ul>
<p>This may be an over-simplified analogy of the actual HDFS functionality, but it conveys the basic principle of the technology - that large files are subdivided into blocks (chunks) and spread across multiple servers in a high-availability redundant configuration. We'll now look at the actual HDFS architecture in a bit more detail:</p>
<div><img height="210" width="312" src="img/a9eace37-f3d1-4033-9f6c-9b85130a311b.png"/></div>
<p>The HDFS backend of Hadoop consists of:</p>
<ul>
<li><strong>NameNode</strong>: This can be considered the master node. The NameNode contains cluster metadata and is aware of what data is stored in which location - in short, it holds the namespace. It stores the entire namespace in RAM and when a request arrives, provides information on which servers hold the data required for the task. In Hadoop 2, there can be more than one NameNode. A secondary NameNode can be created that acts as a helper node to the primary. As such, it is not a backup NameNode, but one that helps in keeping cluster metadata up to date.</li>
<li><strong>DataNode</strong>: The DataNodes are the individual servers that are responsible for storing chunks of the data and performing compute operations when they receive a new request. These are primarily commodity servers that are less powerful in terms of resource and capacity than the NameNode that stores the cluster metadata.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Data storage process in HDFS</h1>
                
            
            
                
<p>The following points should give a good idea of the data storage process:</p>
<p>All data in HDFS is written in blocks, usually of size 128 MB. Thus, a single file of say size 512 MB would be split into four blocks (4 * 128 MB). These blocks are then written to DataNodes. To maintain redundancy and high availability, each block is replicated to create duplicate copies. In general, Hadoop installations have a replication factor of three, indicating that each block of data is replicated three times.</p>
<p>This guarantees redundancy such that in the event one of the servers fails or stops responding, there would always be a second and even a third copy available. To ensure that this process works seamlessly, the DataNode places the replicas in independent servers and can also ensure that the blocks are placed on servers in different racks in a data center. This is due to the fact that even if all the replicas were on independent servers, but all the servers were on the same rack, a rack power failure would mean that no replica would be available.</p>
<p>The general process of writing data into HDFS is as follows:</p>
<ol>
<li>The NameNode receives a request to write a new file to HDFS.</li>
<li>Since the data has to be written in blocks or chunks, the HDFS client (the entity that made the request) begins caching data into a local buffer and once the buffer reaches the allocated chunk size (for example, 128 MB), it informs the NameNode that it is ready to write the first block (chunk) of data.</li>
<li>The NameNode, based on information available to it about the state of the HDFS cluster, responds with information on the destination DataNode where the block needs to be stored.</li>
<li>The HDFS client writes data to the target DataNode and informs the NameNode once the write process for the block has completed.</li>
<li>The target DataNode, subsequently, begins copying its copy of the block of data to a second DataNode, which will serve as a replica for the current block.</li>
</ol>
<ol start="6">
<li>Once the second DataNode completes the write process, it sends the block of data to the third DataNode.</li>
<li>This process repeats until all the blocks corresponding to the data (or equivalently, the file) are copied across different nodes.</li>
</ol>
<p>Note that the number of chunks will depend on the file size. The following image illustrated the distribution of the data across 5 datanodes.</p>
<div><img height="272" width="396" src="img/0750456b-a13d-47df-ba1d-d734014189af.png"/></div>
<p>Master Node and Data Nodes</p>
<p>The HDFS architecture in the first release of Hadoop, also known as Hadoop 1, had the following characteristics:</p>
<ul>
<li>Single NameNode: Only one NameNode was available, and as a result it also acted as a single point of failure since it stored all the cluster metadata.</li>
<li>Multiple DataNodes that stored blocks of data, processed client requests, and performed I/O operations (create, read, delete, and so on) on the blocks.</li>
<li>The HDFS architecture in the second release of Hadoop, also known as Hadoop 2, provided all the benefits of the original HDFS design and also added some new features, most notably, the ability to have multiple NameNodes that can act as primary and secondary NameNodes. Other features included the facility to have multiple namespaces as well as HDFS Federation.</li>
<li>HDFS Federation deserves special mention. The following excerpt from <a href="http://hadoop.apache.org">http://hadoop.apache.org</a> explains the subject in a very precise manner:</li>
</ul>
<p>The NameNodes are federated; the NameNodes are independent and do not require coordination with each other. The DataNodes are used as common storage for blocks by all the NameNodes. Each DataNode registers with all the NameNodes in the cluster. DataNodes send periodic heartbeats and block reports.</p>
<p class="mce-root">The secondary NameNode is not a backup node in the sense that it cannot perform the same tasks as the NameNode in the event that the NameNode is not available. However, it makes the NameNode restart process much more efficient by performing housekeeping operations.</p>
<p>These operations (such as merging HDFS snapshot data with information on data changes) are generally performed by the NameNode when it is restarted and can take a long time depending on the amount of changes since the last restart. The secondary NameNode can, however, perform these housekeeping operations whilst the primary NameNode is still in operation, such that in the event of a restart the primary NameNode can recover much faster. Since the secondary NameNode essentially performs a checkpoint on the HDFS data at periodic intervals, it is also known as the checkpoint node.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hadoop MapReduce</h1>
                
            
            
                
<p>MapReduce was one of the seminal features of Hadoop that was arguably the most instrumental in bringing it to prominence. MapReduce works on the principle of dividing larger tasks into smaller subtasks. Instead of delegating a single machine to compute a large task, a network of smaller machines can instead be used to complete the smaller subtasks. By distributing the work in this manner, the task can be completed much more efficiently relative to using a single-machine architecture.</p>
<p>This is not dissimilar to how we go about completing work in our day-to-day lives. An example will help to make this clearer.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">An intuitive introduction to MapReduce</h1>
                
            
            
                
<p>Let's take the example of a hypothetical organization consisting of a CEO, directors, and managers. The CEO wants to know how many new hires have joined the company. The CEO sends a request to his or her directors to report back the number of hires in their departments. The directors in turn send a request to managers in their individual departments to provide the number of new hires. The managers provide the number to the directors, who in turn send the final value back to the CEO.</p>
<p>This can be considered to be a real-world example of MapReduce. In this analogy, the task was finding the number of new hires. Instead of collecting all the data on his or her own, the CEO delegated it to the directors and managers who provided their own individual departmental numbers as illustrated in the following image:</p>
<div><img height="238" width="359" src="img/1d094fb0-9403-42f9-b7cc-04ad8f8e0747.png"/></div>
<p>The Concept of MapReduce</p>
<p>In this rather simplistic scenario, the process of splitting a large task (find new hires in the entire company), into smaller tasks (new hires in each team), and then a final re-aggregation of the individual numbers, is analogous to how MapReduce works.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">A technical understanding of MapReduce</h1>
                
            
            
                
<p>MapReduce, as the name implies, has a map phase and a reduce phase. A map phase is generally a function that is applied on each element of its input, thus modifying its original value.</p>
<p>MapReduce generates key-value pairs as output.</p>
<div><strong>Key-value:</strong> A key-value pair establishes a relationship. For example, if John is 20 years old, a simple key-value pair could be (John, 20). In MapReduce, the map operation produces such key-value pairs that have an entity and the value assigned to the entity.</div>
<p>In practice, map functions can be complex and involve advanced functionalities.</p>
<p>The reduce phase takes the key-value input from the map function and performs a summarization operation. For example, consider the output of a map operation that contains the ages of students in different grades in a school:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Student name</strong></p>
</td>
<td>
<p><strong>Class</strong></p>
</td>
<td>
<p><strong>Age</strong></p>
</td>
</tr>
<tr>
<td>
<p>John</p>
</td>
<td>
<p>Grade 1</p>
</td>
<td>
<p>7</p>
</td>
</tr>
<tr>
<td>
<p>Mary</p>
</td>
<td>
<p>Grade 2</p>
</td>
<td>
<p>8</p>
</td>
</tr>
<tr>
<td>
<p>Jill</p>
</td>
<td>
<p>Grade 1</p>
</td>
<td>
<p>6</p>
</td>
</tr>
<tr>
<td>
<p>Tom</p>
</td>
<td>
<p>Grade 3</p>
</td>
<td>
<p>10</p>
</td>
</tr>
<tr>
<td>
<p>Mark</p>
</td>
<td>
<p>Grade 3</p>
</td>
<td>
<p>9</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We can create a simple key-value pair, taking for example the value of Class and Age (it can be anything, but I'm just taking these to provide the example). In this case, our key-value pairs would be (Grade 1, 7), (Grade 2, 8), (Grade 1, 6), (Grade 3, 10), and (Grade 3, 9).</p>
<p>An operation that calculates the average of the ages of students in each grade could then be defined as a reduce operation.</p>
<p>More concretely, we can sort the output and then send the tuples corresponding to each grade to a different server.</p>
<p>For example, Server A would receive the tuples (Grade 1, 7) and (Grade 1, 6), Server B would receive the tuple (Grade 2, 8), Server C would receive the tuples (Grade 3, 10) and (Grade 3, 9). Each of the servers, A, B, and C, would then find the average of the tuples and report back (Grade 1, 6.5), (Grade 2, 8), and (Grade 3, 9.5).</p>
<p>Observe that there was an intermediary step in this process that involved sending the output to a particular server and sorting the output to determine which server it should be sent to. And indeed, MapReduce requires a shuffle and sort phase, whereby the key-value pairs are sorted so that each reducer receives a fixed set of unique keys.</p>
<p>In this example, if say, instead of three servers there were only two, Server A could be assigned to computing averages for keys corresponding to Grades 1 and 2, and Server B could be assigned to computing an average for Grade 3.</p>
<p>In Hadoop, the following process takes place during MapReduce:</p>
<ol>
<li>The client sends a request for a task.</li>
<li>NameNode allocates DataNodes (individual servers) that will perform the map operation and ones that will perform the reduce operation. Note that the selection of the DataNode server is dependent upon whether the data that is required for the operation is <em>local to the server</em>. The servers where the data resides can only perform the map operation.</li>
<li>DataNodes perform the map phase and produce key-value (k,v) pairs.</li>
</ol>
<p>As the mapper produces the (k,v) pairs, they are sent to these reduce nodes based on the <em>keys</em> the node is assigned to compute. The allocation of keys to servers is dependent upon a partitioner function, which could be as simple as a hash value of the key (this is default in Hadoop).</p>
<p>Once the reduce node receives its set of data corresponding to the keys it is responsible to compute on, it applies the reduce function and generates the final output.</p>
<p>Hadoop maximizes the benefits of data locality. Map operations are performed by servers that hold the data locally, that is, on disk. More precisely, the map phase will be executed only by those servers that hold the blocks corresponding to the file. By delegating multiple individual nodes to perform computations independently, the Hadoop architecture can perform very large-scale data processing effectively.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Block size and number of mappers and reducers</h1>
                
            
            
                
<p>An important consideration in the MapReduce process is an understanding of HDFS block size, that is, the size of the chunks into which the files have been split. A MapReduce task that needs to access a certain file will need to perform the map operation on each block representing the file. For example, given a 512 MB file and a 128 MB block size, four blocks would be needed to store the entire file. Hence, a MapReduce operation will at a minimum require four map tasks whereby each map operation would be applied to each subset of the data (that is, each of the four blocks).</p>
<p>If the file was very large, however, and required say, 10,000 blocks to store, this means we would have required 10,000 map operations. But, if we had only 10 servers, then we'd have to send 1,000 map operations to each server. This might be sub-optimal as it can lead to a high penalty due to disk I/O operations and resource allocation settings on a per-map basis.</p>
<p>The number of reducers required is summarized very elegantly on Hadoop Wiki (<a href="https://wiki.apache.org/hadoop/HowManyMapsAndReduces" target="_blank">https://wiki.apache.org/hadoop/HowManyMapsAndReduces</a>).</p>
<p>The ideal reducers should be the optimal value that gets them closest to:<br/>
* A multiple of the block size * A task time between 5 and 15 minutes * Creates the fewest files possible<br/>
Anything other than that means there is a good chance your reducers are less than great. There is a tremendous tendency for users to use a REALLY high value ("More parallelism means faster!") or a REALLY low value ("I don't want to blow my namespace quota!"). Both are equally dangerous, resulting in one or more of:<br/>
* Terrible performance on the next phase of the workflow * Terrible performance due to the shuffle * Terrible overall performance because you've overloaded the namenode with objects that are ultimately useless * Destroying disk IO for no really sane reason * Lots of network transfers due to dealing with crazy amounts of CFIF/MFIF work</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hadoop YARN</h1>
                
            
            
                
<p>YARN was a module introduced in Hadoop 2. In Hadoop 1, the process of managing jobs and monitoring them was performed by processes known as JobTracker and TaskTracker(s). NameNodes that ran the JobTracker daemon (process) would submit jobs to the DataNodes which ran TaskTracker daemons (processes).</p>
<p>The JobTracker was responsible for the co-ordination of all MapReduce jobs and served as a central administrator for managing processes, handling server failure, re-allocating to new DataNodes, and so on. The TaskTracker monitored the execution of jobs local to its own instance in the DataNode and provided feedback on the status to the JobTracker as shown in the following:</p>
<div><img height="286" width="399" src="img/77c0cfdf-fc44-4b4d-91fe-2d0709ee0109.png"/></div>
<p>JobTracker and TaskTrackers</p>
<p>This design worked well for a long time, but as Hadoop evolved, the demands for more sophisticated and dynamic functionalities rose proportionally. In Hadoop 1, the NameNode, and consequently the JobTracker process, managed both job scheduling and resource monitoring. In the event the NameNode failed, all activities in the cluster would cease immediately. Lastly, all jobs had to be represented in MapReduce terms - that is, all code would have to be written in the MapReduce framework in order to be executed.</p>
<p>Hadoop 2 alleviated all these concerns:</p>
<ul>
<li>The process of job management, scheduling, and resource monitoring was decoupled and delegated to a new framework/module called YARN</li>
<li>A secondary NameNode could be defined which would act as a helper for the primary NameNode</li>
<li>Further, Hadoop 2.0 would accommodate frameworks beyond MapReduce</li>
<li>Instead of fixed map and reduce slots, Hadoop 2 would leverage containers</li>
</ul>
<p>In MapReduce, all data had to be read from disk, and this was fine for operations on large datasets but it was not optimal for operations on smaller datasets. In fact, any tasks that required very fast processing (low latency), were interactive in nature, or had multiple iterations (thus requiring multiple reads from the disk for the same data), would be extremely slow.</p>
<p>By removing these dependencies, Hadoop 2 allowed developers to implement new programming frameworks that would support jobs with diverse performance requirements, such as low latency and interactive real-time querying, iterative processing required for machine learning, different topologies such as the processing of streaming data, optimizations such as in-memory data caching/processing, and so on.</p>
<p>A few new terms became prominent:</p>
<ul>
<li><strong>ApplicationMaster</strong>: Responsible for managing the resources needed by applications. For example, if a certain job required more memory, the ApplicationMaster would be responsible for securing the required resource. An application in this context refers to application execution frameworks such as MapReduce, Spark, and so on.</li>
<li><strong>Containers</strong>: The unit of resource allocation (for example, 1 GB of memory and four CPUs). An application may require several such containers to execute. The ResourceManager allocates containers for executing tasks. Once the allocation is complete, the ApplicationMaster requests DataNodes to start the allocated containers and takes over the management of the containers.</li>
<li><strong>ResourceManager</strong>: A component of YARN that had the primary role of allocating resources to applications and functioned as a replacement for the JobTracker. The ResourceManager process ran on the NameNode just as the JobTracker did.</li>
<li><strong>NodeManagers</strong>: A replacement for TaskTracker, NodeManagers were responsible for reporting the status of jobs to the ResourceManager (RM) and monitoring the resource utilization of containers.</li>
</ul>
<p>The following image shows a high level view of the ResourceManager and NodeManagers in Hadoop 2.0:</p>
<div><img height="303" width="364" src="img/9debf45a-4304-4551-9c7b-ac04b702b13f.png"/></div>
<p>Hadoop 2.0</p>
<p>The prominent concepts inherent in Hadoop 2 have been illustrated in the next image:</p>
<div><img height="226" width="400" src="img/86aed323-c6e4-4dbe-9272-3fc5e50d9441.png"/></div>
<p>Hadoop 2.0 Concepts</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Job scheduling in YARN</h1>
                
            
            
                
<p>It is not uncommon for large Hadoop clusters to have multiple jobs running concurrently. The allocation of resources when there are multiple jobs submitted from multiple departments becomes an important and indeed interesting topic. Which request should receive priority if say, two departments, A and B, submit a job at the same time but each request is for the maximum available resources? In general, Hadoop uses a <strong>First-In-First-Out</strong> (<strong>FIFO</strong>) policy. That is, whoever submits the job first gets to use the resources first. But what if A submitted the job first but completing A's job will take five hours whereas B's job will complete in five minutes?</p>
<p>To deal with these nuances and variables in job scheduling, numerous scheduling methods have been implemented. Three of the more commonly used ones are:</p>
<ul>
<li><strong>FIFO</strong>: As described above, FIFO scheduling uses a queue to priorities jobs. Jobs are executed in the order in which they are submitted.</li>
<li><strong>CapacityScheduler</strong>: CapacityScheduler assigns a value on the number of jobs that can be submitted on a per-department basis, where a department can indicate a logical group of users. This is to ensure that each department or group can have access to the Hadoop cluster and be able to utilize a minimum number of resources. The scheduler also allows departments to scale up beyond their assigned capacity up to a maximum value set on a per-department basis if there are unused resources on the server. The model of CapacityScheduler thus provides a guarantee that each department can access the cluster on a deterministic basis.</li>
<li><strong>Fair Schedulers</strong>: These schedulers attempt to evenly balance the utilization of resources across different apps. While an even balance might not be feasible at a certain given point in time, balancing allocation over time such that the averages are more or less similar can be achieved using Fair Schedulers.</li>
</ul>
<p>These, and other schedulers, provide finely grained access controls (such as on a per-user or per-group basis) and primarily utilize queues in order to prioritize and allocate resources.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Other topics in Hadoop</h1>
                
            
            
                
<p>A few other aspects of Hadoop deserve special mention. As we have discussed the most important topics at length, this section provides an overview of some of the other subjects of interest.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Encryption</h1>
                
            
            
                
<p>Data encryption is mandated by official regulations for various types of data. In the US, data that identifies patient information is required to be compliant with the rules set forth by HIPAA that dictate how such records should be stored. Data in HDFS can be encrypted whilst at rest (on disk) and/or while in transit. The keys that are used to decrypt the data are generally managed by <strong>Key Management Systems</strong> (<strong>KMSs</strong>).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">User authentication</h1>
                
            
            
                
<p>Hadoop can use the native user-authentication methods of the server. For example, in Linux-based machines, users can be authenticated based on the IDs defined in the system's <kbd>/etc/passwd</kbd> files. In other words, Hadoop inherits the user authentication set up on the server side.</p>
<p>User authentication via Kerberos, a cross-platform authentication protocol, is also commonly used in Hadoop clusters. Kerberos works based on a concept of tickets that grant privileges to users on a temporary as-needed basis. Tickets can be invalidated using Kerberos commands, thus restricting the users' rights to access resources on the cluster as needed.</p>
<p>Note that even if the user is permitted to access data (user authentication), he or she can still be limited in what data can be accessed due to another feature known as authorization. The term implies that even if the user can authenticate and log in to the system, the user may be restricted to only the data the user is authorized to access. This level of authorization is generally performed using native HDFS commands to change directory and file ownerships to the named users.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hadoop data storage formats</h1>
                
            
            
                
<p>Since Hadoop involves storing very large-scale data, it is essential to select a storage type that is appropriate for your use cases. There are a few formats in which data can be stored in Hadoop, and the selection of the optimal storage format depends on your requirements in terms of read/write I/O speeds, how well the files can be compressed and decompressed on demand, and how easily the file can be split since the data will be eventually stored as blocks.</p>
<p>Some of the popular and commonly used storage formats are as follows:</p>
<ul>
<li><strong>Text/CSV</strong>: These are plain text CSV files, similar to Excel files, but saved in plain text format. Since CSV files contain records on a per-line basis, it is naturally trivial to split the files up into blocks of data.</li>
<li><strong>Avro</strong>: Avro was developed to improve the efficient sharing of data across heterogeneous systems. It stores both the schema as well as the actual data in a single compact binary using data serialization. Avro uses JSON to store the schema and binary format for the data and serializes them into a single Avro Object Container File. Multiple languages such as Python, Scala, C/C++, and others have native APIs that can read Avro files and consequently, it is very portable and well suited for cross-platform data exchange.</li>
<li><strong>Parquet</strong>: Parquet is a columnar data storage format. This helps to improve performance, sometimes significantly by permitting data storage and access on a per-column basis. Intuitively, if you were working on a 1 GB file with 100 columns and 1 million rows, and wanted to query data from only one of the 100 columns, being able to access just the individual column would be more efficient than having to access the entire file.</li>
<li><strong>ORCFiles</strong>: ORC stands for Optimized Row-Columnar. In a sense, it is a further layer of optimization over pure columnar formats such as Parquet. ORCFiles store data not only by columns, but also by rows, also known as stripes. A file with data in tabular format can thus be split into multiple smaller stripes where each stripe comprises of a subset of rows from the original file. By splitting data in this manner, if a user task requires access to only a small subsection of the data, the process can interrogate the specific stripe that holds the data.</li>
<li><strong>SequenceFiles</strong>: In SequenceFiles, data is represented as key-value pairs and stored in a binary serialized format. Due to serialization, data can be represented in a compact binary format that not only reduces the data size but consequently also improves I/O. Hadoop, and more concretely HDFS, is not efficient when there are multiple files of a small size, such as audio files. SequenceFiles solve this problem by allowing multiple small files to be stored as a single unit or SequenceFile. They are also very well suited for parallel operations that are splittable and are overall efficient for MapReduce jobs.</li>
<li><strong>HDFS Snapshots:</strong> HDFS Snapshots allow users to preserve data at a given point in time in a read-only mode. Users can create snapshots—in essence a replica of the data as it is at that point time—in in HDFS, such that they can be retrieved at a later stage as and when needed. This ensures that data can be recovered in the event that there is a file corruption or any other failure that affects the availability of data. In that regard, it can be also considered to be a backup. The snapshots are available in a .snapshot directory where they have been created.</li>
<li><strong>Handling of node failures:</strong> Large Hadoop clusters can contain tens of thousands of nodes. Hence it is likely that there would be server failures on any given day. So that the NameNode is aware of the status of all nodes in the cluster, the DataNodes send a periodic heartbeat to the NameNode. If the NameNode detects that a server has failed, that is, it has stopped receiving heartbeats, it marks the server as failed and replicates all the data that was local to the server onto a new instance.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">New features expected in Hadoop 3</h1>
                
            
            
                
<p>At the time of writing this book, Hadoop 3 is in Alpha stage. Details on the new changes that will be available in Hadoop 3 can be found on the internet. For example, <a href="http://hadoop.apache.org/docs/current/">http://hadoop.apache.org/docs/current/</a> provides the most up-to-date information on new changes to the architecture.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Hadoop ecosystem</h1>
                
            
            
                
<p>This chapter should be titled as the Apache ecosystem. Hadoop, like all the other projects that will be discussed in this section, is an Apache project. Apache is used loosely as a short form for the open source projects that are supported by the Apache Software Foundation. It originally has its roots in the development of the Apache HTTP server in the early 90s, and today is a collaborative global initiative that comprises entirely of volunteers who participate in releasing open source software to the global technical community.</p>
<p>Hadoop started out as, and still is, one of the projects in the Apache ecosystem. Due to its popularity, many other projects that are also part of Apache have been linked directly or indirectly to Hadoop as they support key functionalities in the Hadoop environment. That said, it is important to bear in mind that these projects can in most cases exist as independent products that can function without a Hadoop environment. Whether it would provide optimal functionality would be a separate topic.</p>
<p>In this section, we'll go over some of the Apache projects that have had a great deal of influence as well as an impact on the growth and usability of Hadoop as a standard IT enterprise solution, as detailed in the following figure:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Product</strong></p>
</td>
<td>
<p><strong>Functionality</strong></p>
</td>
</tr>
<tr>
<td>
<p>Apache Pig</p>
</td>
<td>
<p>Apache Pig, also known as Pig Latin, is a language specifically designed to represent MapReduce programs through concise statements that define workflows. Coding MapReduce programs in the traditional methods, such as with Java, can be quite complex, and Pig provides an easy abstraction to express a MapReduce workflow and complex <strong>Extract-Transform-Load</strong> (<strong>ETL</strong>) through the use of simple semantics. Pig programs are executed via the Grunt shell.</p>
</td>
</tr>
<tr>
<td>
<p>Apache HBase</p>
</td>
<td>
<p>Apache HBase is a distributed column-oriented database that sits on top of HDFS. It was modelled on Google's BigTable whereby data is represented in a columnar format. HBase supports low-latency read-write across tables with billions of records and is well suited to tasks that require direct random access to data. More concretely, HBase indexes data in three dimensions - row, column, and timestamp. It also provides a means to represent data with an arbitrary number of columns as column values can be expressed as key-value pairs within the cells of an HBase table.</p>
</td>
</tr>
<tr>
<td>
<p>Apache Hive</p>
</td>
<td>
<p>Apache Hive provides a SQL-like dialect to query data stored in HDFS. Hive stores data as serialized binary files in a folder-like structure in HDFS. Similar to tables in traditional database management systems, Hive stores data in tabular format in HDFS partitioned based on user-selected attributes. Partitions are thus subfolders of the higher-level directories or tables. There is a third level of abstraction provided by the concept of buckets, which reference files in the partitions of the Hive tables.</p>
</td>
</tr>
<tr>
<td>
<p>Apache Sqoop</p>
</td>
<td>
<p>Sqoop is used to extract data from traditional databases to HDFS. Large enterprises that have data stored in relational database management systems can thus use Sqoop to transfer data from their data warehouse to a Hadoop implementation.</p>
</td>
</tr>
<tr>
<td>
<p>Apache Flume</p>
</td>
<td>
<p>Flume is used for the management, aggregation, and analysis of large-scale log data.</p>
</td>
</tr>
<tr>
<td>
<p>Apache Kafka</p>
</td>
<td>
<p>Kafka is a publish/subscribe-based middleware system that can be used to analyze and subsequently persist (in HDFS) streaming data in real time.</p>
</td>
</tr>
<tr>
<td>
<p>Apache Oozie</p>
</td>
<td>
<p>Oozie is a workflow management system designed to schedule Hadoop jobs. It implements a key concept known as a <strong>directed acyclic graph</strong> (<strong>DAG</strong>), which will be discussed in our section on Spark.</p>
</td>
</tr>
<tr>
<td>
<p>Apache Spark</p>
</td>
<td>
<p>Spark is one of the most significant projects in Apache and was designed to address some of the shortcomings of the HDFS-MapReduce model. It started as a relatively small project at UC Berkeley and evolved rapidly to become one of the most prominent alternatives to using Hadoop for analytical tasks. Spark has seen a widespread adoption across the industry and comprises of various other subprojects that provide additional capabilities such as machine learning, streaming analytics, and others.</p>
</td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Hands-on with CDH</h1>
                
            
            
                
<p>In this section, we will utilize the CDH QuickStart VM to work through some of the topics that have been discussed in the current chapter. The exercises do not have to be necessarily performed in a chronological order and are not dependent upon the completion of any of the other exercises.</p>
<p>We will complete the following exercises in this section:</p>
<ul>
<li>WordCount using Hadoop MapReduce</li>
<li>Working with the HDFS</li>
<li>Downloading and querying data with Apache Hive</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">WordCount using Hadoop MapReduce</h1>
                
            
            
                
<p>In this exercise, we will be attempting to count the number of occurrences of each word in one of the longest novels ever written. For the exercise, we have selected the book <em>Artamène ou le Grand Cyrus</em> written by Georges and/or Madeleine de Scudéry between 1649-1653. The book is considered to be the second longest novel ever written, per the related list on Wikipedia (<a href="https://en.wikipedia.org/wiki/List_of_longest_novels" target="_blank">https://en.wikipedia.org/wiki/List_of_longest_novels</a>). The novel consists of 13,905 pages across 10 volumes and has close to two million words.</p>
<p>To begin, we need to launch the Cloudera Distribution of Hadoop Quickstart VM in VirtualBox and double-click on the Cloudera Quickstart VM instance:</p>
<div><img src="img/bcae8e19-32d6-4d52-9798-b80487582a38.png"/></div>
<p>It will take some time to start up as it initializes all the CDH-related processes such as the DataNode, NameNode, and so on:</p>
<div><img height="356" width="559" src="img/6f3d9fb5-6d5a-48ed-9227-371380b5fac8.png"/></div>
<p>Once the process starts up, it will launch a default landing page that contains references to numerous tutorials related to Hadoop. We'll be writing our MapReduce code in the Unix terminal for this section. Launch the terminal from the top-left menu, as shown in the following screenshot:</p>
<div><img height="147" width="304" src="img/26b31ed5-02d4-41c1-a81a-e2184b90b038.png"/></div>
<p>Now, we must follow these steps:</p>
<ol>
<li>Create a directory named <kbd>cyrus</kbd>. This is where we will store all the files which contain the text of the book.</li>
<li>Run <kbd>getCyrusFiles.sh</kbd> as shown in step 4. This will download the book into the <kbd>cyrus</kbd> directory.</li>
<li>Run <kbd>processCyrusFiles.sh</kbd> as shown. The book contains various Unicode and non-printable characters. Additionally, we would like to change all the words to lowercase in order to ignore double-counting words that are the same but have capitalizations.</li>
<li>This will produce a file called <kbd>cyrusprint.txt</kbd>. This document contains the entire text of the book. We will be running our MapReduce code on this text file.</li>
<li>Prepare <kbd>mapper.py</kbd> and <kbd>reducer.py</kbd>. As the name implies, <kbd>mapper.py</kbd> runs the map part of the MapReduce process. Similarly, <kbd>reducer.py</kbd> runs the reduce part of the MapReduce process. The file <kbd>mapper.py</kbd> will split the document into words and assign a value of one to each word in the document. The file, <kbd>reducer.py</kbd>, will read in the sorted output of <kbd>mapper.py</kbd> and sum the occurrences of the same word (by first initializing the count of the word to one and incrementing it for each new occurrence of the word). The final output is a file containing the count of each word in the document.</li>
</ol>
<p>The steps are as follows:</p>
<ol>
<li>Create <kbd>getCyrusFiles.sh</kbd> - this script will be used to retrieve the data from the web:</li>
</ol>
<pre style="padding-left: 60px">[cloudera@quickstart ~]$ mkdir cyrus 
[cloudera@quickstart ~]$ vi getCyrusFiles.sh 
[cloudera@quickstart ~]$ cat getCyrusFiles.sh  
for i in `seq 10` 
do 
curl www.artamene.org/documents/cyrus$i.txt -o cyrus$i.txt 
done </pre>
<ol start="2">
<li>Create <kbd>processCyrusFiles.sh</kbd> - this script will be used to concatenate and cleanse the files that were downloaded in the previous step:</li>
</ol>
<pre style="padding-left: 60px">[cloudera@quickstart ~]$ vi processCyrusFiles.sh 
[cloudera@quickstart ~]$ cat processCyrusFiles.sh  
cd ~/cyrus; 
for i in `ls cyrus*.txt` do cat $i &gt;&gt; cyrusorig.txt; done 
cat cyrusorig.txt | tr -dc '[:print:]' | tr A-Z a-z &gt; cyrusprint.txt  </pre>
<ol start="3">
<li>Change the permissions to 755 to make the <kbd>.sh</kbd> files executable at the command prompt:</li>
</ol>
<pre style="padding-left: 60px">[cloudera@quickstart ~]$ chmod 755 getCyrusFiles.sh  
[cloudera@quickstart ~]$ chmod 755 processCyrusFiles.sh  </pre>
<ol start="4">
<li>Execute <kbd>getCyrusFiles.sh</kbd>:</li>
</ol>
<pre style="padding-left: 60px">[cloudera@quickstart cyrus]$ ./getCyrusFiles.sh  
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                 Dload  Upload   Total   Spent    Left  Speed 
100  908k  100  908k    0     0   372k      0  0:00:02  0:00:02 --:--:--  421k 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                 Dload  Upload   Total   Spent    Left  Speed 
100 1125k  100 1125k    0     0   414k      0  0:00:02  0:00:02 --:--:--  471k 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                 Dload  Upload   Total   Spent    Left  Speed 
100 1084k  100 1084k    0     0   186k      0  0:00:05  0:00:05 --:--:--  236k 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                 Dload  Upload   Total   Spent    Left  Speed 
100 1048k  100 1048k    0     0   267k      0  0:00:03  0:00:03 --:--:--  291k 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                 Dload  Upload   Total   Spent    Left  Speed 
100 1116k  100 1116k    0     0   351k      0  0:00:03  0:00:03 --:--:--  489k 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                 Dload  Upload   Total   Spent    Left  Speed 
100 1213k  100 1213k    0     0   440k      0  0:00:02  0:00:02 --:--:--  488k 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                 Dload  Upload   Total   Spent    Left  Speed 
100 1119k  100 1119k    0     0   370k      0  0:00:03  0:00:03 --:--:--  407k 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                 Dload  Upload   Total   Spent    Left  Speed 
100 1132k  100 1132k    0     0   190k      0  0:00:05  0:00:05 --:--:--  249k 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                 Dload  Upload   Total   Spent    Left  Speed 
100 1084k  100 1084k    0     0   325k      0  0:00:03  0:00:03 --:--:--  365k 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current 
                                 Dload  Upload   Total   Spent    Left  Speed 
100 1259k  100 1259k    0     0   445k      0  0:00:02  0:00:02 --:--:--  486k 
 
[cloudera@quickstart cyrus]$ ls 
cyrus10.txt  cyrus3.txt  cyrus6.txt  cyrus9.txt 
cyrus1.txt   cyrus4.txt  cyrus7.txt  getCyrusFiles.sh 
cyrus2.txt   cyrus5.txt  cyrus8.txt  processCyrusFiles.sh 
 </pre>
<ol start="5">
<li>Execute <kbd>processCyrusFiles.sh</kbd>:</li>
</ol>
<pre style="padding-left: 60px"> 
[cloudera@quickstart cyrus]$ ./processCyrusFiles.sh  
 
[cloudera@quickstart cyrus]$ ls 
cyrus10.txt  cyrus3.txt  cyrus6.txt  cyrus9.txt      getCyrusFiles.sh 
cyrus1.txt   cyrus4.txt  cyrus7.txt  cyrusorig.txt   processCyrusFiles.sh 
cyrus2.txt   cyrus5.txt  cyrus8.txt  cyrusprint.txt 
 
[cloudera@quickstart cyrus]$ ls -altrh cyrusprint.txt  
-rw-rw-r-- 1 cloudera cloudera 11M Jun 28 20:02 cyrusprint.txt 
 
[cloudera@quickstart cyrus]$ wc -w cyrusprint.txt  
1953931 cyrusprint.txt </pre>
<ol start="6">
<li>Execute the following steps to copy the final file, named <kbd>cyrusprint.txt</kbd>, to HDFS, create the <kbd>mapper.py</kbd> and <kbd>reducer.py</kbd> scripts.</li>
</ol>
<p style="padding-left: 90px">The files, <kbd>mapper.py</kbd> and <kbd>reducer.py</kbd>, are referenced on Glenn Klockwood's website (<a href="http://www.glennklockwood.com/data-intensive/hadoop/streaming.html">http://www.glennklockwood.com/data-intensive/hadoop/streaming.html</a>), which provides a wealth of information on MapReduce and related topics.</p>
<p style="padding-left: 60px">The following code shows the contents of <kbd>mapper.py</kbd>:</p>
<pre style="padding-left: 60px">[cloudera@quickstart cyrus]$ hdfs dfs -ls /user/cloudera 
 
[cloudera@quickstart cyrus]$ hdfs dfs -mkdir /user/cloudera/input 
 
[cloudera@quickstart cyrus]$ hdfs dfs -put cyrusprint.txt /user/cloudera/input/ 
  
[cloudera@quickstart cyrus]$ vi mapper.py 
 
[cloudera@quickstart cyrus]$ cat mapper.py  
#!/usr/bin/env python 
#the above just indicates to use python to intepret this file 
#This mapper code will input a line of text and output &lt;word, 1&gt; # 
 
import sys 
sys.path.append('.') 
 
for line in sys.stdin: 
   line = line.strip() 
   keys = line.split() 
   for key in keys: 
          value = 1 
          print ("%s\t%d" % (key,value)) 
 
[cloudera@quickstart cyrus]$ vi reducer.py # Copy-Paste the content of reducer.py as shown below using the vi or nano Unix editor.
 
[cloudera@quickstart cyrus]$ cat reducer.py  
#!/usr/bin/env python 
 
import sys 
sys.path.append('.') 
 
last_key = None 
running_total = 0 
 
for input_line in sys.stdin: 
   input_line = input_line.strip() 
   this_key, value = input_line.split("\t", 1) 
   value = int(value) 
 
   if last_key == this_key: 
       running_total += value 
   else: 
       if last_key: 
           print("%s\t%d" % (last_key, running_total)) 
       running_total = value 
       last_key = this_key 
 
if last_key == this_key: 
   print( "%s\t%d" % (last_key, running_total) ) 
 
 
[cloudera@quickstart cyrus]$ chmod 755 *.py</pre>
<ol start="7">
<li>Execute the mapper and reducer scripts that will perform the MapReduce operations in order to produce the word count. You may see error messages as shown here, but for the purpose of this exercise (and for generating the results), you may disregard them:</li>
</ol>
<pre style="padding-left: 60px">[cloudera@quickstart cyrus]$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -input /user/cloudera/input -output /user/cloudera/output -mapper /home/cloudera/cyrus/mapper.py -reducer /home/cloudera/cyrus/reducer.py 
 
packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.10.0.jar] /tmp/streamjob1786353270976133464.jar tmpDir=null 
17/06/28 20:11:21 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 
17/06/28 20:11:21 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 
17/06/28 20:11:22 INFO mapred.FileInputFormat: Total input paths to process : 1 
17/06/28 20:11:22 INFO mapreduce.JobSubmitter: number of splits:2 
17/06/28 20:11:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1498704103152_0002 
17/06/28 20:11:23 INFO impl.YarnClientImpl: Submitted application application_1498704103152_0002 
17/06/28 20:11:23 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1498704103152_0002/ 
17/06/28 20:11:23 INFO mapreduce.Job: Running job: job_1498704103152_0002 
17/06/28 20:11:30 INFO mapreduce.Job: Job job_1498704103152_0002 running in uber mode : false 
17/06/28 20:11:30 INFO mapreduce.Job:  map 0% reduce 0% 
17/06/28 20:11:41 INFO mapreduce.Job:  map 50% reduce 0% 
17/06/28 20:11:54 INFO mapreduce.Job:  map 83% reduce 0% 
17/06/28 20:11:57 INFO mapreduce.Job:  map 100% reduce 0% 
17/06/28 20:12:04 INFO mapreduce.Job:  map 100% reduce 100% 
17/06/28 20:12:04 INFO mapreduce.Job: Job job_1498704103152_0002 completed successfully 
17/06/28 20:12:04 INFO mapreduce.Job: Counters: 50 
   File System Counters 
          FILE: Number of bytes read=18869506 
          FILE: Number of bytes written=38108830 
          FILE: Number of read operations=0 
          FILE: Number of large read operations=0 
          FILE: Number of write operations=0 
          HDFS: Number of bytes read=16633042 
          HDFS: Number of bytes written=547815 
          HDFS: Number of read operations=9 
          HDFS: Number of large read operations=0 
          HDFS: Number of write operations=2 
   Job Counters  
          Killed map tasks=1 
          Launched map tasks=3 
          Launched reduce tasks=1 
          Data-local map tasks=3 
          Total time spent by all maps in occupied slots (ms)=39591 
          Total time spent by all reduces in occupied slots (ms)=18844 
          Total time spent by all map tasks (ms)=39591 
          Total time spent by all reduce tasks (ms)=18844 
          Total vcore-seconds taken by all map tasks=39591 
          Total vcore-seconds taken by all reduce tasks=18844 
          Total megabyte-seconds taken by all map tasks=40541184 
          Total megabyte-seconds taken by all reduce tasks=19296256 
   Map-Reduce Framework 
          Map input records=1 
          Map output records=1953931 
          Map output bytes=14961638 
          Map output materialized bytes=18869512 
          Input split bytes=236 
          Combine input records=0 
          Combine output records=0 
          Reduce input groups=45962 
          Reduce shuffle bytes=18869512 
          Reduce input records=1953931 
          Reduce output records=45962 
          Spilled Records=3907862 
          Shuffled Maps =2 
          Failed Shuffles=0 
          Merged Map outputs=2 
          GC time elapsed (ms)=352 
          CPU time spent (ms)=8400 
          Physical memory (bytes) snapshot=602038272 
          Virtual memory (bytes) snapshot=4512694272 
          Total committed heap usage (bytes)=391979008 
   Shuffle Errors 
          BAD_ID=0 
          CONNECTION=0 
          IO_ERROR=0 
          WRONG_LENGTH=0 
          WRONG_MAP=0 
          WRONG_REDUCE=0 
   File Input Format Counters  
          Bytes Read=16632806 
   File Output Format Counters  
          Bytes Written=547815 
17/06/28 20:12:04 INFO streaming.StreamJob: Output directory: /user/cloudera/output</pre>
<ol start="8">
<li>The results are stored in HDFS under the <kbd>/user/cloudera/output</kbd> directory in files prefixed with <kbd>part-</kbd> :</li>
</ol>
<pre style="padding-left: 60px">[cloudera@quickstart cyrus]$ hdfs dfs -ls /user/cloudera/output 
Found 2 items 
-rw-r--r--   1 cloudera cloudera          0 2017-06-28 20:12 /user/cloudera/output/_SUCCESS 
-rw-r--r--   1 cloudera cloudera     547815 2017-06-28 20:12 /user/cloudera/output/part-00000  </pre>
<ol start="9">
<li>To view the contents of the file use <kbd>hdfs dfs -cat</kbd> and provide the name of the file. In this case we are viewing the first 10 lines of the output:</li>
</ol>
<pre style="padding-left: 60px">[cloudera@quickstart cyrus]$ hdfs dfs -cat /user/cloudera/output/part-00000 | head -10 
!  1206 
!) 1 
!quoy,    1 
'  3 
'' 1 
'. 1 
'a 32 
'appelloit 1 
'auoit    1 
'auroit   10  </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Analyzing oil import prices with Hive</h1>
                
            
            
                
<p>In this section, we will use Hive to analyze the import prices of oil in countries across the world from 1980-2016. The data is available from the site of the <strong>OECD</strong> (<strong>Organisation for Economic Co-operation and Development</strong>) at the URL shown in the following screenshot:</p>
<div><img src="img/23401bea-8fb7-4edb-af54-e882c4086d69.png"/></div>
<p>The actual CSV file is available at <a href="https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&amp;detail=code&amp;separator=comma&amp;csv-lang=en" target="_blank">https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&amp;amp;detail=code&amp;amp;separator=comma&amp;amp;csv-lang=en</a>.</p>
<p>Since we'll be loading the data in Hive, it makes sense to download the file into our home directory via the terminal in our Cloudera Quickstart CDH environment. The steps we'd execute are as follows:</p>
<ol>
<li>Download the CSV file into the CDH environment:</li>
</ol>
<div><img height="193" width="621" src="img/91b3f48c-3292-4ae8-bef9-e9b434b94fd9.png"/></div>
<pre style="padding-left: 60px"># Download the csv file 
cd /home/cloudera; 
wget -O oil.csv "https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&amp;amp;detail=code&amp;amp;separator=comma&amp;amp;csv-lang=en" </pre>
<ol start="2">
<li>Clean the CSV file. Data cleansing is an area of core importance in data science. In practice, it is very common to receive files that will require some level of cleansing. This is due to the fact that there could be invalid characters or values in columns, missing data, missing or additional delimiters, and so on. We noted that various values were enclosed in double-quotes ("). In Hive, we can ignore the quotes by specifying the <kbd>quoteChar</kbd> property whilst creating the table. Since Linux also offers simple and easy ways to remove such characters, we used <kbd>sed</kbd> to remove the quotation marks:</li>
</ol>
<pre style="padding-left: 60px">[cloudera@quickstart ~]$ sed -i 's/\"//g' oil.csv </pre>
<p style="padding-left: 90px">Moreover, in our downloaded file, <kbd>oil.csv</kbd>, we observed that there were non-printable characters that could cause issues. We removed them by issuing the following command:</p>
<pre style="padding-left: 60px">[cloudera@quickstart ~]$ tr -cd '\11\12\15\40-\176' oil_.csv &gt; oil_clean.csv</pre>
<p style="padding-left: 90px">(Source: <a href="http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix">http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix</a>)</p>
<p style="padding-left: 90px">Finally, we copied the new file (<kbd>oil_clean.csv</kbd>) to <kbd>oil.csv</kbd>. Since the <kbd>oil.csv</kbd> file already existed in the same folder, we were prompted with an overwrite message and we entered <kbd>yes</kbd>:</p>
<pre style="padding-left: 60px">[cloudera@quickstart ~]$ mv oil_clean.csv oil.csv 
mv: overwrite `oil.csv'? yes </pre>
<ol start="3">
<li>Log in to Cloudera Hue:</li>
</ol>
<p style="padding-left: 90px">Click on Hue on the Bookmarks bar in the browser. This will bring up the Cloudera login screen. Log in using ID <kbd>cloudera</kbd> and password <kbd>cloudera</kbd>:</p>
<div><img height="335" width="606" src="img/e279c451-220f-434d-947b-c166b798f4cf.png"/></div>
<ol start="4">
<li>Click on Hue from the drop-down menu on Quick Start at the top of the Hue login window:</li>
</ol>
<div><img height="287" width="507" src="img/ada22afe-0776-4378-9f95-a056ff1077f5.png"/></div>
<ol start="5">
<li>Create the table schema, load the CSV file, <kbd>oil.csv</kbd>, and view the records:</li>
</ol>
<pre style="padding-left: 60px">CREATE TABLE IF NOT EXISTS OIL 
   (location String, indicator String, subject String, measure String,  
   frequency String, time String, value Float, flagCode String) 
   ROW FORMAT DELIMITED 
   FIELDS TERMINATED BY ',' 
   LINES TERMINATED BY '\n' 
   STORED AS TEXTFILE 
   tblproperties("skip.header.line.count"="1"); 
 
LOAD DATA LOCAL INPATH '/home/cloudera/oil.csv' INTO TABLE OIL; 
SELECT * FROM OIL; </pre>
<div><img height="379" width="674" src="img/7663083b-9243-4b24-ad98-cc522f0faae4.png"/></div>
<ol start="6">
<li>Load the oil file.</li>
</ol>
<ol start="7">
<li>Now that the table has been loaded into Hive, you can run miscellaneous Hive commands using HiveQL. A full set of these commands is available at <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual">https://cwiki.apache.org/confluence/display/Hive/LanguageManual</a>.</li>
</ol>
<p>For instance, to find the maximum, minimum, and average value of oil prices in each country from 1980-2015 (the date range of the dataset), we can use familiar SQL operators. The query would be as follows:</p>
<pre>SELECT LOCATION, MIN(value) as MINPRICE, AVG(value) as AVGPRICE,  
MAX(value) as MAXPRICE 
FROM OIL 
WHERE FREQUENCY LIKE "A" 
GROUP BY LOCATION; </pre>
<p>Here is the screenshot of the same:</p>
<div><img height="312" width="667" src="img/e414b22d-47af-4f7d-a052-474c1f7cf74d.png"/></div>
<p>In similar ways, we can use an array of other SQL commands. The Hive Manual provides an in-depth look into these commands and the various ways data can be saved, queried, and retrieved.</p>
<p>Hue includes a set of useful features such as data visualization, data download, and others that allow users to perform ad hoc analysis on the data.</p>
<p>To access the visualization feature, click on the visualization icon underneath the grid icon in the results section, as shown in the following screenshot:</p>
<div><img height="329" width="545" src="img/499fcfb2-f240-4123-97d1-2f1bb8dcdd62.png"/></div>
<p>Select Scatter. In Hue, this type of chart, also known more generally as a scatterplot, allows users to create multivariate charts very easily. Different values for the x and y axes, as well as scatter size and grouping, can be selected, as shown in the following screenshot:</p>
<div><img height="235" width="485" src="img/259f4b41-4364-4376-b13e-befe47bd30a6.png"/></div>
<p>The following is a simple pie chart that can be constructed by selecting Pie in the drop-down menu:</p>
<div><img height="305" width="482" src="img/6106cd02-6123-4778-9133-00a18ac7e5d1.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Joining tables in Hive</h1>
                
            
            
                
<p>Hive supports advanced join functionalities. The following illustrates the process of using Left Join. As seen, the original table has data for each country represented by their three-letter country code. Since Hue supports map charts, we can add the values for latitude and longitude to overlay the oil pricing data on a world map.</p>
<p>To do so, we'll need to download a dataset containing the values for latitude and longitude:</p>
<pre># ENTER THE FOLLOWING IN THE UNIX TERMINAL 
# DOWNLOAD LATLONG CSV FILE 
 
cd /home/cloudera; 
wget -O latlong.csv "https://gist.githubusercontent.com/tadast/8827699/raw/7255fdfbf292c592b75cf5f7a19c16ea59735f74/countries_codes_and_coordinates.csv" 
 
# REMOVE QUOTATION MARKS 
sed -i 's/\"//g' latlong.csv </pre>
<p>Once the file has been downloaded and cleansed, define the schema and load the data in Hive:</p>
<pre>CREATE TABLE IF NOT EXISTS LATLONG 
   (country String, alpha2 String, alpha3 String, numCode Int, latitude Float, longitude Float) 
   ROW FORMAT DELIMITED 
   FIELDS TERMINATED BY ',' 
   LINES TERMINATED BY '\n' 
   STORED AS TEXTFILE 
   TBLPROPERTIES("skip.header.line.count"="1"); 
 
LOAD DATA LOCAL INPATH '/home/cloudera/latlong.csv' INTO TABLE LATLONG; <br/></pre>
<div><img src="img/c66e5c65-998c-425f-b7d3-0bbcb229eebb.png"/></div>
<p>Join the oil data with the lat/long data:</p>
<pre>SELECT DISTINCT * FROM 
(SELECT location, avg(value) as AVGPRICE from oil GROUP BY location) x 
LEFT JOIN 
(SELECT TRIM(ALPHA3) AS alpha3, latitude, longitude from LATLONG) y 
ON (x.location = y.alpha3); </pre>
<div><img height="356" width="595" src="img/6ff9ec7c-634e-43a5-bc35-4187a92c525b.png"/></div>
<p>We can now proceed with creating geospatial visualizations. It would be useful to bear in mind that these are preliminary visualizations in Hue that provide a very convenient means to view data. More in-depth visualizations can be developed on geographical data using shapefiles, polygons, and other advanced charting methods.</p>
<p>Select Gradient Map from the drop-down menu and enter the appropriate values to create the chart, as shown in the following figure:</p>
<div><img height="390" width="616" src="img/2b47321a-b355-411b-b262-9a5ea24c77cd.png"/></div>
<p>The next chart was developed using the Marker Map option in the drop-down menu. It uses the three-character country code in order to place markers and associated values on the respective regions, as shown in the following figure:</p>
<div><img height="405" width="543" src="img/08fb5f5c-9fd6-4352-9d0b-39d28dd86133.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p class="mce-root">This chapter provided a technical overview of Hadoop. We discussed the core components and core concepts that are fundamental to Hadoop, such as MapReduce and HDFS. We also looked at the technical challenges and considerations of using Hadoop. While it may appear simple in concept, the inner workings and a formal administration of a Hadoop architecture can be fairly complex. In this chapter we highlighted a few of them.</p>
<p>We concluded with a hands-on exercise on Hadoop using the Cloudera Distribution. For this tutorial, we used the CDH Virtual Machine downloaded earlier from Cloudera's website.</p>
<p>In the next chapter, we will look at NoSQL, an alternative or a complementary solution to Hadoop depending upon your individual and/or organization al needs. While Hadoop offers a far richer set of capabilities, if your intended use case(s) can be done with simply NoSQL solutions, the latter may be an easier choice in terms of the effort required.</p>


            

            
        
    </body></html>