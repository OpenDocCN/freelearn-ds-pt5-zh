- en: '*Chapter 4*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality Reduction and Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Compare hierarchical cluster analysis (HCA) and k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct an HCA and interpret the output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune a number of clusters for k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select an optimal number of principal components for dimension reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform supervised dimension compression using linear discriminant function
    analysis (LDA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter will cover various concepts that fall under dimensionality reduction
    and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In unsupervised learning, **descriptive models** are used for exploratory analysis
    to uncover patterns in unlabeled data. Examples of unsupervised learning tasks
    include algorithms for **clustering** and those for **dimension reduction**. In
    clustering, observations are assigned to groups in which there is high within-group
    homogeneity and between-group heterogeneity. Simply put, observations are placed
    into clusters of samples with other observations that are very similar. Use cases
    for clustering algorithms are vast. For example, analysts seeking to elevate sales
    by targeting selected customers for marketing advertisements and promotions separate
    customers by their shopping behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Additionally, hierarchical clustering has been implemented in academic neuroscience
    and motor behavior research (https://www.researchgate.net/profile/Ming-Yang_Cheng/project/The-Effect-of-SMR-Neurofeedback-Training-on-Mental-Representation-and-Golf-Putting-Performance/attachment/57c8419808aeef0362ac36a5/AS:401522300080128@1472741784217/download/Schack+-+2012+-+Measuring+mental+representations.pdf?context=ProjectUpdatesLog)
    and k-means clustering has been used in fraud detection (https://www.semanticscholar.org/paper/Fraud-Detection-in-Credit-Card-by-Clustering-Tech/3e98a9ac78b5b89944720c2b428ebf3e46d9950f).
  prefs: []
  type: TYPE_NORMAL
- en: However, when building descriptive or predictive models, it can be a challenge
    to determine which features to include in a model to improve it, and which features
    to exclude because they diminish a model. Too many features can be troublesome
    because the greater the number of variables in a model, the higher the probability
    of multicollinearity and subsequent overfitting of a model. Additionally, numerous
    features expand the complexity of a model and increase the time for model tuning
    and fitting.
  prefs: []
  type: TYPE_NORMAL
- en: This becomes troublesome with larger datasets. Fortunately, another use case
    for unsupervised learning is to reduce the number of features in a dataset by
    creating combinations of the original features. Reducing the number of features
    in data helps eliminate multicollinearity and converges on a combination of features
    to best produce a model that performs well on unseen test data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multicollinearity is a situation in which at least two variables are correlated.
    It is a problem in linear regression models because it does not allow the isolation
    of the relationship between each independent variable and the outcome measure.
    Thus, coefficients and p-values become unstable and less precise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering two widely used unsupervised clustering
    algorithms: *Hierarchical Cluster Analysis (HCA)* and *k-means clustering*. Additionally,
    we will explore dimension reduction using *principal component analysis (PCA)*
    and observe how reducing dimensionality can improve model performance. Lastly,
    we will implement linear discriminant function analysis *(LDA)* for supervised
    dimensionality reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Cluster Analysis (HCA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hierarchical cluster analysis (HCA) is best implemented when the user does not
    have a priori number of clusters to build. Thus, it is a common approach to use
    HCA as a precursor to other clustering techniques where a predetermined number
    of clusters is recommended. HCA works by merging observations that are similar
    into clusters and continues merging clusters that are closest in proximity until
    all observations are merged into a single cluster.
  prefs: []
  type: TYPE_NORMAL
- en: HCA determines similarity as the Euclidean distance between and among observations
    and creates links at the distance in which the two points lie.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the number of features indicated by *n*, the Euclidean distance is calculated
    using the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: The Euclidean distance](img/C13322_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: The Euclidean distance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After the distance between observations and cluster have been calculated, the
    relationships between and among all observations are displayed using a dendrogram.
    Dendrograms are tree-like structures displaying horizontal lines as the distance
    between links.
  prefs: []
  type: TYPE_NORMAL
- en: Dr. Thomas Schack (https://www.researchgate.net/profile/Ming-Yang_Cheng/project/The-Effect-of-SMR-Neurofeedback-Training-on-Mental-Representation-and-Golf-Putting-Performance/attachment/57c8419808aeef0362ac36a5/AS:401522300080128@1472741784217/download/Schack+-+2012+-+Measuring+mental+representations.pdf?context=ProjectUpdatesLog)
    relates this structure to the human brain in which each observation is a node
    and the links between observations are neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'This creates a hierarchical structure in which items that are closely related
    are "chunked" together into clusters. An example dendrogram is displayed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: An example dendrogram'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.2: An example dendrogram'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The y-axis indicates the Euclidean distance, while the x-axis indicates the
    row index for each observation. Horizontal lines denote links between observations;
    links closer to the x-axis indicate shorter distance and a subsequent closer relationship.
    In this example, there appear to be three clusters. The first cluster includes
    observations colored in green, the second cluster includes observations colored
    in red, and the third cluster includes observations colored in turquoise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 34: Building an HCA Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To demonstrate HCA, we will be use an adapted version of the glass dataset
    from the University of California – Irvine (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter04).
    This data contains 218 observations and 9 features corresponding to the percent
    weight of various oxides found in glass:'
  prefs: []
  type: TYPE_NORMAL
- en: 'RI: refractive index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Na: weight percent in sodium'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mg: weight percent in magnesium'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Al: weight percent in aluminum'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Si: weight percent in silicon'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K: weight percent in potassium'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ca: weight percent in calcium'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ba: weight percent in barium'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fe: weight percent in iron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this exercise, we will use the refractive index (RI) and weight percent in
    each oxide to segment the glass type.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, we will import pandas and read the `glass.csv` file using the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Look for some basic data frame information by printing `df.info()` to the console
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.3: DataFrame information'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_04_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.3: DataFrame information'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To remove any possible order effects in the data, we will shuffle the rows
    prior to building any models and save it as a new data frame object, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform each observation into a z-score by fitting and transforming shuffled
    data using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform hierarchical clustering using the linkage function on `scaled_features`.
    The following code will show you how:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You've successfully built an HCA model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 35: Plotting an HCA Model and Assigning Predictions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that the HCA model has been built, we will continue with the analysis by
    visualizing clusters using a dendrogram and using the visualization to generate
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Display the dendrogram by plotting the linkage model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.4: Dendogram for glass data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_04_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.4: Dendogram for glass data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The index for each observation or row in a dataset is on the x-axis. The Euclidean
    distance is on the y-axis. Horizontal lines are links between and among observations.
    By default, scipy will color code the different clusters that it finds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have the predicted clusters of observations, we can use the `fcluster`
    function to generate an array of labels that correspond to rows in `df_shuffled`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate predicted labels of the cluster which an observation belongs to using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the labels array as a column in the shuffled data and preview the first
    five rows using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the output in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.5: The first five rows of df_shuffled after predictions have been
    matched to observations.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.5: The first five rows of df_shuffled after predictions have been
    matched to observations.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We have successfully learned the difference between supervised and unsupervised
    learning, how to build an HCA model, how to visualize and interpret the HCA dendrogram,
    and how to assign the predicted cluster label to the appropriate observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have utilized HCA to cluster our data into three groups and matched
    the observations with their predicted cluster. Some pros of HCA models include:'
  prefs: []
  type: TYPE_NORMAL
- en: They are easy to build
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no need to specify the number of clusters in advance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizations are easy to interpret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, some drawbacks of HCA include:'
  prefs: []
  type: TYPE_NORMAL
- en: Vagueness in terms of the termination criteria (that is, when to finalize the
    number of clusters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm cannot adjust once the clustering decisions have been made
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be very computationally expensive to build HCA models on large datasets
    with many features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will introduce you to another clustering algorithm, k-means clustering.
    This algorithm addresses some of the HCA shortcomings by having the ability to
    adjust when the clusters have been initially generated. It is more computationally
    frugal than HCA.
  prefs: []
  type: TYPE_NORMAL
- en: K-means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like HCA, K-means also uses distance to assign observations into clusters not
    labeled in data. However, rather than linking observations to each other as in
    HCA, k-means assigns observations to *k* (user-defined number) clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine the cluster to which each observation belongs, kcluster centers
    are randomly generated, and observations are assigned to the cluster in which
    its Euclidean distance is closest to the cluster center. Like the starting weights
    in artificial neural networks, cluster centers are initialized at random. After
    cluster centers have been randomly generated there are two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Assignment phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The randomly generated cluster centers are important to remember, and we will
    be visiting it later in this chapter. Some refer to this random generation of
    cluster centers as a weakness of the algorithm, because results vary between fitting
    the same model on the same data, and it is not guaranteed to assign observations
    to the appropriate cluster. We can turn it into an advantage by leveraging the
    power of loops.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the assignment phase, observations are assigned to the cluster from which
    it has the smallest Euclidean distance, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: A scatterplot of observations and the cluster centers as denoted
    by the star, triangle, and diamond.](img/C13322_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: A scatterplot of observations and the cluster centers as denoted
    by the star, triangle, and diamond.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, in the updating phase, cluster centers are shifted to the mean position
    of the points in that cluster. These cluster means are known as the centroids,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: Shifting of the cluster centers to the cluster centroid.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.7: Shifting of the cluster centers to the cluster centroid.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'However, once the centroids have been calculated, some of the observations
    are reassigned to a different cluster due to being closer to the new centroid
    than the previous cluster center. Thus, the model must update its centroids once
    again. This is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Updating of the centroids after observation reassignment.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.8: Updating of the centroids after observation reassignment.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This process of updating centroids continues until there are no further observation
    reassignments. The final centroid is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: The final centroid position and cluster assignments.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.9: The final centroid position and cluster assignments.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the same glass dataset from *Exercise 34*, *Building an HCA Model*, we
    will fit a k-means model with user-defined number of clusters. Next, because of
    the randomness in which group centroids are chosen, we will increase the confidence
    in our predictions by building an ensemble of k-means models with a given number
    of clusters and assigning each observation to the mode of the predicted clusters.
    After that, we will tune the optimal number of clusters by monitoring the mean
    *inertia*, or within-cluster sum of squares, by number of clusters, and finding
    the point at which there are diminishing returns in inertia by adding more clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 36: Fitting k-means Model and Assigning Predictions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since our data has already been prepared (see *Exercise 34, Building an HCA
    Model*), and we understand concepts behind the k-Means algorithm, we will learn
    how easy it is to fit a k-means model, generate predictions, and assign these
    predictions to the appropriate observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the glass dataset has been imported, shuffled, and standardized:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a KMeans model with an arbitrary number of, in this case, two clusters,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to `scaled_features` using the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the cluster labels from our model into the array, labels, using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a frequency table of the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get a better idea, refer to the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.10: Frequency table of two clusters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_04_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.10: Frequency table of two clusters'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Using two clusters, 61 observations were placed into the first cluster and 157
    observations were grouped into the second cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add the labels array as the ''`Predicted Cluster`'' column into the `df_shuffled`
    data frame and preview the first five rows using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the output in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.11: First five rows of df_shuffled'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.11: First five rows of df_shuffled'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Activity 12: Ensemble k-means Clustering and Calculating Predictions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When algorithms use randomness as part of their method for finding the optimal
    solution (that is, in artificial neural networks and k-means clustering), running
    identical models on the same data may result in different conclusions, limiting
    the confidence we have in our predictions. It is advised to run these models many
    times and generate predictions using a summary measure across all models (that
    is, mean, median, and mode). In this activity, we will build an ensemble of 100
    k-means clustering models.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the glass dataset has been imported, shuffled, and standardized (see
    *Exercise 34*, *Building an HCA Model*):'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate an empty data frame to append the labels for each model and save
    it as the new data frame object `labels_df`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using a for loop, iterate through 100 models, appending the predicted labels
    to `labels_df` as a new column at each iteration. Calculate the mode for each
    row in `labels_df` and save it as a new column in `labels_df`. The output should
    be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.12: First five rows of labels_df'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.12: First five rows of labels_df'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 356.
  prefs: []
  type: TYPE_NORMAL
- en: We have drastically increased the confidence in our predictions by iterating
    through numerous models, saving the predictions at each iteration, and assigning
    the final predictions as the mode of these predictions. However, these predictions
    were generated by models using a predetermined number of clusters. Unless we know
    the number of clusters a priori, we will want to discover the optimal number of
    clusters to segment our observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 37: Calculating Mean Inertia by n_clusters'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The k-means algorithm groups observations into clusters by minimizing the within-cluster
    sum of squares, or inertia. Thus, to improve our confidence in the tuned number
    of clusters for our k-means model, we will place the loop we created in A*ctivity
    12, Ensemble k-means Clustering and Calculating Predictions* (with a few minor
    adjustments) inside of another loop which will iterate through a range of `n_clusters`.
    This creates a nested loop which iterates through 10 possible values for `n_clusters`
    and builds 100 models at each iteration. At each of the 100 inner iterations,
    model inertia will be calculated. For each of the 10 outer iterations, mean inertia
    over the 100 models will be computed, resulting in the mean inertia value for
    each `n_clusters` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the glass dataset has been imported, shuffled, and standardized (see *Exercise 34,
    Building an HCA Model*):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the packages we need outside of the loop as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is easier to build and comprehend nested loops by working from the inside-out.
    First, instantiate an empty list, `inertia_list`, for which we will append inertia
    values after each iteration of the inside loop as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the for loop, we will iterate through 100 models using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inside the loop, build a `KMeans` model with `n_clusters=x`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The value for x is determined by the outer for loop, which we have not covered
    yet, but we will cover in detail very shortly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model to `scaled_features` as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the inertia value and save it to the object inertia as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Append inertia to `inertia_list` using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move to the outside loop, instantiate another empty list to store the average
    inertia values, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Iterate through the values 1 through 10 for `n_clusters` using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the inside for loop has run through 100 iterations, and the inertia value
    for each of the 100 models have been appended to `inertia_list`, compute the mean
    of this list and save as the object, `mean_inertia` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Append `mean_inertia` to `mean_inertia_list` as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After 100 iterations have been completed 10 times for a total of 1000 iterations,
    `mean_inertia_list` contains 10 values that are the average inertia values for
    each value of `n_clusters`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Print `mean_inertia_list` as shown in the following code. The values are shown
    in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.13: mean_inertia_list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.13: mean_inertia_list'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 38: Plotting Mean Inertia by n_clusters'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 38:'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have generated mean inertia over 100 models for each value of `n_clusters`,
    we will plot mean inertia by `n_clusters`. Then, we will discuss how to visually
    assess the best value to use for `n_clusters`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import matplotlib as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a list of numbers and save it as the object x, so we can plot it on
    the x-axis as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save `mean_inertia_list`, as the object y as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the mean inertia by number of clusters, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the plot title to read ''`Mean Inertia by n_clusters`'' using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Label the x-axis ''`n_clusters`'' using `plt.xlabel(''n_clusters'')`, and label
    the y-axis ''`Mean Inertia`'' using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the tick labels on the x-axis as the values in x using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the plot used in `plt.show()`. To better understand, refer to the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the resultant output, refer to the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14: Mean inertia by n_clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.14: Mean inertia by n_clusters'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To determine the best number of `n_clusters`, we will use the "elbow method."
    That is, the point in the plot where there are diminishing returns for the added
    complexity of more clusters. From Figure 4.14, we can see that there are rapid
    decreases in mean inertia from `n_clusters` 1 to 3\. After `n_clusters` equals
    3, the decreases in mean inertia seem to become less rapid and the decrease in
    inertia may not be worth the added complexity of adding additional clusters. Thus,
    the appropriate number of `n_clusters` in this situation is 3.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the data has too many dimensions, the k-means algorithm can fall
    subject to the curse of dimensionality by inflated Euclidean distances and subsequent
    erroneous results. Thus, before fitting a k-Means model, using a dimension reduction
    strategy is encouraged.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the number of dimensions helps to eliminate multicollinearity and decreases
    the time to fit the model. **Principal component analysis** (**PCA**) is a common
    method to reduce the number of dimensions by discovering a set of underlying linear
    variables in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a high level, PCA is a technique for creating uncorrelated linear combinations
    from the original features termed **components**. Of the principal components,
    the first component explains the greatest proportion of variance in data, while
    the following components account for progressively less variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate PCA, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Fit PCA model with all principal components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune the number of principal components by setting a threshold of explained
    variance to remain in data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit those components to a k-means cluster analysis and compare k-means performance
    before and after the PCA transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 39: Fitting a PCA Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, you will learn to fit a generic PCA model using data we prepared
    in *Exercise 34, Building an HCA Model* and the brief explanation of PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a PCA model as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the PCA model to `scaled_features`, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the proportion of explained variance in the data for each component, save
    the array as the object `explained_var_ratio`, and print the values to the console
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the resultant output, refer to the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.15: Explained variance in the data for each principal component'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.15: Explained variance in the data for each principal component'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Each principal component explains a proportion of the variance in data. In this
    exercise, the first principal component explained .35 of the variance in data,
    the second explained. 25, the third .13%, and so on. Altogether, these nine components
    explain 100% of the variance in data. The goal of dimensionality reduction is
    to decrease the number of dimensions in data with the objectives of limiting overfitting
    and time to fit the subsequent model. Thus, we will not keep all nine components.
    However, if we retain too few components, the percent of explained variance in
    the data will be low and the subsequent model will under fit. Therefore, a challenge
    for data scientists exists in determining the number of `n_components` that minimize
    over fitting and under fitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 40: Choosing n_components using Threshold of Explained Variance'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Exercise 39*, *Fitting PCA Model*, you learned to fit a PCA model with all
    available principal components. However, keeping all of the principal components
    does not reduce the number of dimensions in data. In this exercise, we will reduce
    the number of dimensions in data by retaining the components that explain a threshold
    of variance in it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Determine the number of principal components in which a minimum of 95% of the
    variance in the data is explained by calculating the cumulative sum of explained
    variance by the principal component. Let''s look at the following code, to see
    how it''s done:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the resultant output, refer to the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.16: The cumulative sum of the explained variance for each principal
    component'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_04_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.16: The cumulative sum of the explained variance for each principal
    component'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Set the threshold for the percent of variance to keep in data as 95%, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using this threshold, we will loop through the list of cumulative explained
    variance and see where they explain no less than 95% of the variance in data.
    Since we will be looping through the indices of `cum_sum_explained_var`, we will
    instantiate our loop using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check to see if the item in `cum_sum_explained_var` is greater than or equal
    to 0.95, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If that logic is met, then we will add 1 to that index (because we cannot have
    0 principal components), save the value as an object, and break the loop. To do
    this, we will use `best_n_components = i+1` inside of the if statement and break
    in the next line. Look at the following code to get an idea:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last two lines in the if statement instruct the loop not to do anything
    if the logic is not met:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print a message detailing the best number of components using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'View the output from the previous line of code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.17: The output message displaying number of components'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.17: The output message displaying number of components'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The value for `best_n_components` is 6\. We can refit another PCA model with
    `n_components = 6`, transform the data into principal components, and use these
    components in a new k-means model to lower the inertia values. Additionally, we
    can compare the inertia values across `n_clusters` values for the models built
    using PCA transformed data to those using data that was not PCA transformed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 13: Evaluating Mean Inertia by Cluster after PCA Transformation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we know the number of components to retain at least 95% of the variance
    in the data, how to transform our features into principal components, and a way
    to tune the optimal number of clusters for k-means clustering with a nested loop,
    we will put them all together in this activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 40*:'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a PCA model with the value for the `n_components` argument equal
    to `best_n_components` (that is, remember, `best_n_components = 6`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to `scaled_features` and transform it into the first six principal
    components
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a nested loop, calculate the mean inertia over 100 models at values 1
    through 10 for `n_clusters` (see *Exercise 40*, *Choosing n_components using Threshold
    of Explained Variance*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.18: mean_inertia_list_PCA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.18: mean_inertia_list_PCA'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, much like in *Exercise 38*, *Plotting Mean Inertia by n_clusters*, we have
    a mean inertia value for each value of `n_clusters` (1 through 10). However, `mean_inertia_list_PCA`
    contains the mean inertia value for each value of `n_clusters` after PCA transformation.
    But, how do we know if the k-means model performs better after PCA transformation?
    In the next exercise, we will visually compare the mean inertia values before
    and after PCA transformation at each value of `n_clusters`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 357.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 41: Visual Comparison of Inertia by n_clusters'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To visually compare mean inertia by `n_clusters` before and after PCA transformation,
    we will slightly modify the plot created in *Exercise 38, Plotting Mean Inertia
    by n_clusters,* by:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding a second line to the plot showing mean inertia by `n_clusters` after
    PCA transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a legend distinguishing the lines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the title
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: For this visualization to work properly, `mean_inertia_list` from *Exercise
    38*, *Plotting Mean Inertia by n_clusters*, must still be in the environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Continuing from *Activity 13*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import Matplotlib using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a list of numbers and save it as the object x, so we can plot it on
    the x-axis as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save `mean_inertia_list_PCA` as the object y using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save `mean_inertia_list` as the object y2 using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot mean inertia after a PCA transformation by number of clusters using the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add our second line of mean inertia before a PCA transformation by number of
    clusters using the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the plot title to read ''`Mean Inertia by n_clusters for Original Features
    and PCA Transformed Features`'' as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Label the x-axis ''`n_clusters`'' using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Label the y-axis ''`Mean Inertia`'' using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set the tick labels on the x-axis as the values in x using `plt.xticks(x)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Show a legend using and display the plot as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.19: Mean inertia by n_clusters for original features (orange) and
    PCA transformed features (blue)](img/C13322_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: Mean inertia by n_clusters for original features (orange) and
    PCA transformed features (blue)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the plot, we can see that inertia is lower at every number of clusters
    in the model using the PCA transformed features. This indicates that there was
    less distance between the group centroids and observations in each cluster after
    the PCA transformation relative to before the transformation. Thus, using a PCA
    transformation on the original features, we were able to decrease the number of
    features and simultaneously improve our model by decreasing the within-cluster
    sum of squares (that is, inertia).
  prefs: []
  type: TYPE_NORMAL
- en: HCA and k-means clustering are two widely-used unsupervised learning techniques
    used for segmentation. PCA can be used to help reduce the number of dimensions
    in our data and improve models in an unsupervised fashion. Linear discriminant
    function analysis (LDA), on the other hand, is a supervised method for reducing
    the number of dimensions via data compression.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Data Compression using Linear Discriminant Analysis (LDA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed previously, PCA transforms features into a set of variables to
    maximize the variance among the features. In PCA, the output labels are not considered
    when fitting the model. Meanwhile, LDA uses the dependent variable to help compress
    data into features that best discriminate the classes of the outcome variable.
    In this section, we will walk through how to use LDA as a supervised data compression
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate using LDA as supervised dimensionality compression technique,
    we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Fit an LDA model with all possible `n_components`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform our features to `n_components`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune the number of `n_components`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 42: Fitting LDA Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To fit the model as a supervised learner using the default parameters of the
    LDA algorithm we will be using a slightly different glass data set, `glass_w_outcome.csv`.
    (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter04)
    This dataset contains the same nine features as glass, but also an outcome variable,
    Type, corresponding to the type of glass. Type is labeled 1, 2, and 3 for building
    windows float processed, building windows non float processed, and headlamps,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `glass_w_outcome.csv` file and save it as the object df using the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Shuffle the data to remove any ordering effects and save it as the data frame
    `df_shuffled` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save ‘`Type`’ as `DV` (I.e., dependent variable) as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Split the shuffled data into features (i.e., X) and outcome (i.e., y) using
    `X = df_shuffled.drop(DV, axis=1)` and `y = df_shuffled[DV]`, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Split X and y into testing and training as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Scale `X_train` and `X_test` separately using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Instantiate the LDA model and save it as model. The following will show you
    how.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: By instantiating an LDA model with no argument `for n_components` we will return
    all possible components.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model to the training data using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'See the resultant output below:![Figure 4.20: Output from fitting linear discriminant
    function analysis'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13322_04_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.20: Output from fitting linear discriminant function analysis'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Much like in PCA, we can return the percentage of variance explained by each
    component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is shown in the following figure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.21: Explained variance by component.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_04_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.21: Explained variance by component.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first component explains 95.86% of the variance in the data and the second
    component explains 4.14% of the variance in the data for a total of 100%.
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully fit an LDA model to compress our data from nine features
    to two features. Decreasing the features to two cuts the time to tune and fit
    machine learning models. However, prior to using these features in a classifier
    model we must transform the training and testing features into their two components.
    In the next exercise, we will show how this is done.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 43: Using LDA Transformed Components in Classification Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using supervised data compression, we will transform our training and testing
    features (i.e., `X_train_scaled` and `X_test_scaled`, respectively) into their
    components and fit a `RandomForestClassifier` model on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 42*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compress `X_train_scaled` into its components as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compress `X_test` into its components using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `RandomForestClassifier` model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: We will be using the default hyperparameters of the `RandomForestClassifier`
    model because tuning hyperparameters is beyond the scope of this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model to the compressed training data using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'See the resultant output below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.22: Output after fitting random forest classifier model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_04_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.22: Output after fitting random forest classifier model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Generate predictions on `X_test_LDA` and save them as the array, predictions
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate model performance by comparing predictions to `y_test` using a confusion
    matrix. To generate and print a confusion matrix see the code below:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.23: 3x3 confusion matrix for evaluating RandomForestClassifier model
    performance using the LDA compressed data](img/C13322_04_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.23: 3x3 confusion matrix for evaluating RandomForestClassifier model
    performance using the LDA compressed data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter introduced you to two widely used unsupervised, clustering algorithms,
    HCA and k-means clustering. While learning about k-means clustering, we leveraged
    the power of loops to create ensembles of models for tuning the number of clusters
    and to gain more confidence in our predictions. During the PCA section, we determined
    the number of principal components for dimensionality reduction and fit the components
    to a k-means model. Additionally, we compared the differences in k-means model
    performance before and after PCA transformation. We were introduced to an algorithm,
    LDA, which reduces dimensionality in a supervised manner. Lastly, we tuned the
    number of components in LDA by iterating through all possible values for components
    and programmatically returning the value resulting in the best accuracy score
    from a Random Forest classifier model. You should now feel comfortable with dimensionality
    reduction and unsupervised learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We were briefly introduced to creating plots in this chapter; however, in the
    next chapter, we will learn about structured data and how to work with XGboost
    and Keras libraries
  prefs: []
  type: TYPE_NORMAL
