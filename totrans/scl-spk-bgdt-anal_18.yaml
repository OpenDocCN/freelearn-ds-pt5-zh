- en: Testing and Debugging Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Everyone knows that debugging is twice as hard as writing a program in the
    first place. So if you''re as clever as you can be when you write it, how will
    you ever debug it?"'
  prefs: []
  type: TYPE_NORMAL
- en: '- Brian W. Kernighan'
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, we write perfect Spark codes and everything runs perfectly
    all the time, right? Just kidding; in practice, we know that working with large-scale
    datasets is hardly ever that easy, and there are inevitably some data points that
    will expose any corner cases with your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the aforementioned challenges, therefore, in this chapter, we will
    see how difficult it can be to test an application if it is distributed; then,
    we will see some ways to tackle this. In a nutshell, the following topics will
    be cover throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing in a distributed environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing Spark application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging Spark application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing in a distributed environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Leslie Lamport defined the term distributed system as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"A distributed system is one in which I cannot get any work done because some
    machine I have never heard of has crashed."'
  prefs: []
  type: TYPE_NORMAL
- en: Resource sharing through **World Wide Web** (aka **WWW**), a network of connected
    computers (aka a cluster), is a good example of distributed systems. These distributed
    environments are often complex and lots of heterogeneity occurs frequently. Testing
    in these kinds of the heterogeneous environments is also challenging. In this
    section, at first, we will observe some commons issues that are often raised while
    working with such system.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are numerous definitions of distributed systems. Let''s see some definition
    and then we will try to correlate the aforementioned categories afterward. Coulouris
    defines a distributed system as *a system in which hardware or software components
    located at networked computers communicate and coordinate their actions only by
    message passing*. On the other hand, Tanenbaum defines the term in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A collection of independent computers that appear to the users of the system
    as a single computer.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A system that consists of a collection of two or more independent Computers
    which coordinate their processing through the exchange of synchronous or asynchronous
    message passing.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A distributed system is a collection of autonomous computers linked by a network
    with software designed to produce an integrated computing facility.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, based on the preceding definition, distributed systems can be categorized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Only hardware and software are distributed:The local distributed system is connected
    through LAN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users are distributed, but there are computing and hardware resources that are
    running backend, for example, WWW.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both users and hardware/software are distributed: Distributed computing cluster
    that is connected through WAN. For example, you can get these types of computing
    facilities while using Amazon AWS, Microsoft Azure, Google Cloud, or Digital Ocean''s
    droplets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues in a distributed system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we will discuss some major issues that need to be taken care of during
    the software and hardware testing so that Spark jobs run smoothly in cluster computing,
    which is essentially a distributed computing environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that all the issues are unavoidable, but we can at least tune them for
    betterment. You should follow the instructions and recommendations given in the
    previous chapter. According to *Kamal Sheel Mishra* and *Anil Kumar Tripathi*,
    *Some Issues, Challenges and Problems of Distributed Software System*, in *International
    Journal of Computer Science and Information Technologies*, Vol. 5 (4), 2014, 4922-4925\.
    URL: [https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf](https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf),
    there are several issues that need to be addressed while working with software
    or hardware in a distributed environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heterogeneous languages, platform, and architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security and privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transparency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Openness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interoperability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quality of service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Failure management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Component selection for testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing for system scalability and performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Availability of source code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproducibility of events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deadlocks and race conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing for fault tolerance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling issue for distributed system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed task allocation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing distributed software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and control mechanism from the hardware abstraction level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It''s true that we cannot fully solve all of these issues, but However, using
    Spark, we can at least control a few of them that are related to distributed system.
    For example, scalability, resource management, quality of service, failure management,
    synchronization, communications, scheduling issue for distributed system, distributed
    task allocation, and monitoring and control mechanism in testing distributed software.
    Most of them were discussed in the previous two chapters. On the other hand, we
    can address some issues in the testing and software side: such as software architectures,
    performance analysis, generating test data, component selection for testing, test
    sequence, testing for system scalability and performance, and availability of
    source code. These will be covered explicitly or implicitly in this chapter at
    least.'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of software testing in a distributed environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some common challenges associated with the tasks in an agile software
    development, and those challenges become more complex while testing the software
    in a distributed environment before deploying them eventually. Often team members
    need to merge the software components in parallel after the bugs proliferating.
    However, based on urgency, often the merging occurs before testing phase. Sometimes,
    many stakeholders are distributed across teams. Therefore, there's a huge potential
    for misunderstanding and teams often lose in between.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Cloud Foundry ([https://www.cloudfoundry.org/](https://www.cloudfoundry.org/))
    is an open source heavily distributed PaaS software system for managing deployment
    and scalability of applications in the Cloud. It promises different features such
    as scalability, reliability, and elasticity that come inherently to deployments
    on Cloud Foundry require the underlying distributed system to implement measures
    to ensure robustness, resiliency, and failover.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of software testing is long known to comprise *unit testing*, *integration
    testing*, *smoke testing*, *acceptance testing*, *scalability testing*, *performance
    testing*, and *quality of service testing*. In Cloud Foundry, the process of testing
    a distributed system is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00106.jpeg)**Figure 1:** An example of software testing in a distributed
    environment like Cloud'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding figure (first column), the process of testing in a
    distributed environment like Cloud starts with running unit tests against the
    smallest points of contract in the system. Following successful execution of all
    the unit tests, integration tests are run to validate the behavior of interacting
    components as part of a single coherent software system (second column) running
    on a single box (for example, a **Virtual Machine** (**VM**) or bare metal). However,
    while these tests validate the overall behavior of the system as a monolith, they
    do not guarantee system validity in a distributed deployment. Once integration
    tests pass, the next step (third column) is to validate distributed deployment
    of the system and run the smoke tests.
  prefs: []
  type: TYPE_NORMAL
- en: As you know, that the successful configuration of the software and execution
    of unit tests prepares us to validate acceptability of system behavior. This verification
    is done by running acceptance tests (fourth column). Now, to overcome the aforementioned
    issues and challenges in distributed environments, there are also other hidden
    challenges that need to be solved by researchers and big data engineers, but those
    are actually out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what real challenges are for the software testing in a distributed
    environment, now let's start testing our Spark code a bit. The next section is
    dedicated to testing Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Spark applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many ways to try to test your Spark code, depending on whether it''s
    Java (you can do basic JUnit tests to test non-Spark pieces) or ScalaTest for
    your Scala code. You can also do full integration tests by running Spark locally
    or on a small test cluster. Another awesome choice from Holden Karau is using
    Spark-testing base. You probably know that there is no native library for unit
    testing in Spark as of yet. Nevertheless, we can have the following two alternatives
    to use two libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: ScalaTest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark-testing base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, before starting to test your Spark applications written in Scala, some
    background knowledge about unit testing and testing Scala methods is a mandate.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Scala methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will see some simple techniques for testing Scala methods. For Scala
    users, this is the most familiar unit testing framework (you can also use it for
    testing Java code and soon for JavaScript). ScalaTest supports a number of different
    testing styles, each designed to support a specific type of testing need. For
    details, see ScalaTest User Guide at [http://www.scalatest.org/user_guide/selecting_a_style](http://www.scalatest.org/user_guide/selecting_a_style).
    Although ScalaTest supports many styles, one of the quickest ways to get started
    is to use the following ScalaTest traits and write the tests in the **TDD** (**test-driven
    development**) style:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FunSuite`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Assertions`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`BeforeAndAfter`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to browse the preceding URLs to learn more about these traits; that
    will make rest of this tutorial go smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that the TDD is a programming technique to develop software,
    and it states that you should start development from tests. Hence, it doesn't
    affect how tests are written, but when tests are written. There is no trait or
    testing style to enforce or encourage TDD in `ScalaTest.FunSuite`, `Assertions`,
    and `BeforeAndAfter` are only more similar to the xUnit testing frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three assertions available in the ScalaTest in any style trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '`assert`: This is used for general assertions in your Scala program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assertResult`: This helps differentiate expected value from the actual values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assertThrows`: This is used to ensure a bit of code throws an expected exception.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ScalaTest''s assertions are defined in the trait `Assertions`, which is
    further extended by `Suite`. In brief, the `Suite` trait is the super trait for
    all the style traits. According to the ScalaTest documentation at [http://www.scalatest.org/user_guide/using_assertions](http://www.scalatest.org/user_guide/using_assertions),
    the `Assertions` trait also provides the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`assume` to conditionally cancel a test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fail` to fail a test unconditionally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cancel` to cancel a test unconditionally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`succeed` to make a test succeed unconditionally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intercept` to ensure a bit of code throws an expected exception and then make
    assertions about the exception'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assertDoesNotCompile` to ensure a bit of code does not compile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assertCompiles` to ensure a bit of code does compile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assertTypeError` to ensure a bit of code does not compile because of a type
    (not parse) error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`withClue` to add more information about a failure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the preceding list, we will show a few of them. In your Scala program,
    you can write assertions by calling `assert` and passing a `Boolean` expression
    in. You can simply start writing your simple unit test case using `Assertions`.
    The `Predef` is an object, where this behavior of assert is defined. Note that
    all the members of the `Predef` get imported into your every Scala source file.
    The following source code will print `Assertion success` for the following case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if you make `a = 2` and `b = 1`, for example, the assertion will fail
    and you will experience the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00271.jpeg)**Figure 2:** An example of assertion fail'
  prefs: []
  type: TYPE_NORMAL
- en: If you pass a true expression, assert will return normally. However, assert
    will terminate abruptly with an Assertion Error if the supplied expression is
    false. Unlike the `AssertionError` and `TestFailedException` forms, the ScalaTest's
    assert provides more information that will tell you exactly in which line the
    test case failed or for which expression. Therefore, ScalaTest's assert provides
    better error messages than Scala's assert.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for the following source code, you should experience `TestFailedException`
    that will tell that 5 did not equal 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding Scala test:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00180.jpeg)**Figure 3:** An example of TestFailedException'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following source code explains the use of the `assertResult` unit test
    to test the result of your method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding assertion will be failed and Scala will throw an exception `TestFailedException`
    and prints `Expected 3 but got 4` (*Figure 4*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00060.jpeg)**Figure 4:** Another example of TestFailedException'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see a unit testing to show expected exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you try to access an array element outside the index, the preceding code
    will tell you if you''re allowed to access the first character of the preceding
    string `Hello world!`. If your Scala program can access the value in an index,
    the assertion will fail. This also means that the test case has failed. Thus,
    the preceding test case will fail naturally since the first index contains the
    character `H`, and you should experience the following error message (*Figure
    5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00337.jpeg)**Figure 5:** Third example of TestFailedException'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, now let''s try to access the index at position `-1` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the assertion should be true, and consequently, the test case will be passed.
    Finally, the code will terminate normally. Now, let''s check our code snippets
    if it will compile or not. Very often, you may wish to ensure that a certain ordering
    of the code that represents emerging "user error" does not compile at all. The
    objective is to check the strength of the library against the error to disallow
    unwanted result and behavior. ScalaTest''s `Assertions` trait includes the following
    syntax for that purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to ensure that a snippet of code does not compile because of a
    type error (as opposed to a syntax error), use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A syntax error will still result on a thrown `TestFailedException`. Finally,
    if you want to state that a snippet of code does compile, you can make that more
    obvious with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A complete example is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00369.jpeg)**Figure 6:** Multiple tests together'
  prefs: []
  type: TYPE_NORMAL
- en: Now we would like to finish the Scala-based unit testing due to page limitation.
    However, for other unit test cases, you can refer the Scala test guideline at
    [http://www.scalatest.org/user_guide](http://www.scalatest.org/user_guide).
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In software engineering, often, individual units of source code are tested to
    determine whether they are fit for use or not. This way of software testing method
    is also called the unit testing. This testing ensures that the source code developed
    by a software engineer or developer meets the design specifications and works
    as intended.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the goal of unit testing is to separate each part of the
    program (that is, in a modular way). Then try to observe if all the individual
    parts are working normally. There are several benefits of unit testing in any
    software system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Find problems early:** It finds bugs or missing parts of the specification
    early in the development cycle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facilitates change:** It helps in refactoring and up gradation without worrying
    about breaking functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplifies integration:** It makes integration tests easier to write.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Documentation:** It provides a living documentation of the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Design:** It can act as the formal design of the project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing Spark applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already seen how to test your Scala code using built-in `ScalaTest`
    package of Scala. However, in this subsection, we will see how we could test our
    Spark application written in Scala. The following three methods will be discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Method 1:** Testing Spark applications using JUnit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method 2:** Testing Spark applications using `ScalaTest` package'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method 3:** Testing Spark applications using Spark testing base'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods 1 and 2 will be discussed here with some practical codes. However, a
    detailed discussion on method 3 will be provided in the next subsection. To keep
    the understanding easy and simple, we will use the famous word counting applications
    to demonstrate methods 1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 1: Using Scala JUnit test'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you have written an application in Scala that can tell you how many
    words are there in a document or text file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code simply parses a text file and performs a `flatMap` operation
    by simply splitting the words. Then, it performs another operation to take only
    the distinct words into consideration. Finally, the `myWordCounter` method counts
    how many words are there and returns the value of the counter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, before proceeding into formal testing, let''s check if the preceding method
    works well. Just add the main method and create an object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you execute the preceding code, you should observe the following output:
    `Number of words: 214`. Fantastic! It really works as a local application. Now,
    test the preceding test case using Scala JUnit test case.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If you see the earlier code carefully, I have used the `Test` annotation before
    the `test()` method. Inside the `test()` method, I invoked the `assert()` method,
    where the actual testing occurs. Here we tried to check if the return value of
    the `myWordCounter()` method is equal to 214\. Now run the earlier code as a Scala
    Unit test as follows (*Figure 7*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00151.jpeg)**Figure 7:** Running Scala code as Scala JUnit Test'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if the test case passes, you should observe the following output on your
    Eclipse IDE (*Figure 8*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00173.jpeg)**Figure 8:** Word count test case passed'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for example, try to assert in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If the preceding test case fails, you should observe the following output (*Figure
    9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00299.jpeg)**Figure 9:** Test case failed'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's have a look at method 2 and how it helps us for the betterment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 2: Testing Scala code using FunSuite'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s redesign the preceding test case by returning only the RDD of the
    texts in the document, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the `prepareWordCountRDD()` method in the preceding class returns an RDD
    of string and integer values. Now, if we want to test the `prepareWordCountRDD()`
    method''s functionality, we can do it more explicit by extending the test class
    with `FunSuite` and `BeforeAndAfterAll` from the `ScalaTest` package of Scala.
    The testing works in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Extend the test class with `FunSuite` and `BeforeAndAfterAll` from the `ScalaTest`
    package of Scala
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Override the `beforeAll()` that creates Spark context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform the test using the `test()` method and use the `assert()` method inside
    the `test()` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Override the `afterAll()` method that stops the Spark context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on the preceding steps, let''s see a class for testing the preceding
    `prepareWordCountRDD()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The first test says that if two RDDs materialize in two different ways, the
    contents should be the same. Thus, the first test should get passed. We will see
    this in following example. Now, for the second test, as we have seen previously,
    the word count of RDD is 214, but let's assume it unknown for a while. If it's
    214 coincidentally, the test case should pass, which is its expected behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we are expecting both tests to be passed. Now, on Eclipse, run the test
    suite as `ScalaTest-File`, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00342.jpeg) **Figure 10:** running the test suite as ScalaTest-File'
  prefs: []
  type: TYPE_NORMAL
- en: Now you should observe the following output (*Figure 11*). The output shows
    how many test cases we performed and how many of them passed, failed, canceled,
    ignored, or were (was) in pending. It also shows the time to execute the overall
    test.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00268.jpeg)**Figure 11:** Test result when running the two test suites
    as ScalaTest-file'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fantastic! The test case passed. Now, let''s try changing the compare value
    in the assertion in the two separate tests using the `test()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you should expect that the test case will be failed. Now run the earlier
    class as `ScalaTest-File` (*Figure 12*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)**Figure 12:** Test result when running the preceding two
    test suites as ScalaTest-File'
  prefs: []
  type: TYPE_NORMAL
- en: Well done! We have learned how to perform the unit testing using Scala's FunSuite.
    However, if you evaluate the preceding method carefully, you should agree that
    there are several disadvantages. For example, you need to ensure an explicit management
    of `SparkContext` creation and destruction. As a developer or programmer, you
    have to write more lines of code for testing a sample method. Sometimes, code
    duplication occurs as the *Before* and the *After* step has to be repeated in
    all test suites. However, this is debatable since the common code could be put
    in a common trait.
  prefs: []
  type: TYPE_NORMAL
- en: Now the question is how could we improve our experience? My recommendation is
    using the Spark testing base to make life easier and more straightforward. We
    will discuss how we could perform the unit testing the Spark testing base.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 3: Making life easier with Spark testing base'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark testing base helps you to test your most of the Spark codes with ease.
    So, what are the pros of this method then? There are many in fact. For example,
    using this the code is not verbose but we can get very succinct code. The API
    is itself richer than that of ScalaTest or JUnit. Multiple languages support,
    for example, Scala, Java, and Python. It has the support of built-in RDD comparators.
    You can also use it for testing streaming applications. And finally and most importantly,
    it supports both local and cluster mode testings. This is most important for the
    testing in a distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: The GitHub repo is located at [https://github.com/holdenk/spark-testing-base](https://github.com/holdenk/spark-testing-base).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting the unit testing with Spark testing base, you should include
    the following dependency in the Maven friendly `pom.xml` file in your project
    tree for Spark 2.x as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'For SBT, you can add the following dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note that it is recommended to add the preceding dependency in the `test` scope
    by specifying `<scope>test</scope>` for both the Maven and SBT cases. In addition
    to these, there are other considerations such as memory requirements and OOMs
    and disabling the parallel execution. The default Java options in the SBT testing
    are too small to support for running multiple tests. Sometimes it's harder to
    test Spark codes if the job is submitted in local mode! Now you can naturally
    understand how difficult it would be in a real cluster mode -i.e. YARN or Mesos.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get rid of this problem, you can increase the amount of memory in your `build.sbt`
    file in your project tree. Just add the following parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if you are using Surefire, you can add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In your Maven-based build, you can make it by setting the value in the environmental
    variable. For more on this issue, refer to [https://maven.apache.org/configure.html](https://maven.apache.org/configure.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is just an example to run spark testing base''s own tests. Therefore,
    you might need to set bigger value. Finally, make sure that you have disabled
    the parallel execution in your SBT by adding the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, if you''re using surefire, make sure that `forkCount` and
    `reuseForks` are set as 1 and true, respectively. Let''s see an example of using
    Spark testing base. The following source code has three test cases. The first
    test case is the dummy that compares if 1 is equal to 1 or not, which obviously
    will be passed. The second test case counts the number of words from the sentence,
    say `Hello world! My name is Reza`, and compares if this has six words or not.
    The final and the last test case tries to compare two RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding source code, we can see that we can perform multiple test
    cases using Spark testing base. Upon successful execution, you should observe
    the following output (*Figure 13*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00280.jpeg)![](img/00093.jpeg)**Figure 13:** A successful execution
    and passed test using Spark testing base'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Hadoop runtime on Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen how to test your Spark applications written in Scala on
    Eclipse or IntelliJ, but there is another potential issue that should not be overlooked.
    Although Spark works on Windows, Spark is designed to be run on the UNIX-like
    operating system. Therefore, if you are working on Windows environment, then extra
    care needs to be taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'While using Eclipse or IntelliJ to develop your Spark applications for solving
    data analytics, machine learning, data science, or deep learning applications
    on Windows, you might face an I/O exception error and your application might not
    compile successfully or may be interrupted. Actually, the thing is that Spark
    expects that there is a runtime environment for Hadoop on Windows too. For example,
    if you run a Spark application, say `KMeansDemo.scala`, on Eclipse for the first
    time, you will experience an I/O exception saying the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason is that by default, Hadoop is developed for the Linux environment,
    and if you are developing your Spark applications on Windows platform, a bridge
    is required that will provide an environment for the Hadoop runtime for Spark
    to be properly executed. The details of the I/O exception can be seen in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00088.gif)**Figure 14:** I/O exception occurred due to the failure
    of not to locate the winutils binary in the Hadoop binary path'
  prefs: []
  type: TYPE_NORMAL
- en: Now, how to get rid of this problem then? The solution is straightforward. As
    the error message says, we need to have an executable, namely `winutils.exe`.
    Now download the `winutils.exe` file from [https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin),
    paste it in the Spark distribution directory, and configure Eclipse. More specifically,
    suppose your Spark distribution containing Hadoop is located at `C:/Users/spark-2.1.0-bin-hadoop2.7`.
    Inside the Spark distribution, there is a directory named bin. Now, paste the
    executable there (that is, `path = C:/Users/spark-2.1.0-binhadoop2.7/bin/`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second phase of the solution is going to Eclipse and then selecting the
    main class (that is, `KMeansDemo.scala` in this case), and then going to the Run
    menu. From the Run menu, go to the Run Configurations option and from there select
    the Environment tab, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00049.jpeg)**Figure 15:** Solving the I/O exception occurred due to
    the absence of winutils binary in the Hadoop binary path'
  prefs: []
  type: TYPE_NORMAL
- en: If you select the tab, you a will have the option to create a new environmental
    variable for Eclipse suing the JVM. Now create a new environmental variable named
    `HADOOP_HOME` and put the value as `C:/Users/spark-2.1.0-bin-hadoop2.7/`. Now
    press on Apply button and rerun your application, and your problem should be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that while working with Spark on Windows in a PySpark, the
    `winutils.exe` file is required too. For PySpark reference, refer to the [Chapter
    19](part0571.html#H0HH61-21aec46d8593429cacea59dbdcd64e1c), *PySpark and SparkR*.
  prefs: []
  type: TYPE_NORMAL
- en: Please make a note that the preceding solution is also applicable in debugging
    your applications. Sometimes, even if the preceding error occurs, your Spark application
    will run properly. However, if the size of the dataset is large, it is most likely
    that the preceding error will occur.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how to debug Spark applications that are running
    locally (on Eclipse or IntelliJ), standalone or cluster mode in YARN or Mesos.
    However, before diving deeper, it is necessary to know about logging in the Spark
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with log4j with Spark recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already discussed this topic in [Chapter 14](part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c),
    *Time to Put Some Order - Cluster Your Data with Spark MLlib*. However, let''s
    replay the same contents to make your brain align with the current discussion
    *Debugging Spark applications*. As stated earlier, Spark uses log4j for its own
    logging. If you configured Spark properly, Spark gets logged all the operation
    to the shell console. A sample snapshot of the file can be seen from the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00259.jpeg)**Figure 16:** A snap of the log4j.properties file'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the default spark-shell log level to WARN. When running the spark-shell,
    the log level for this class is used to overwrite the root logger''s log level
    so that the user can have different defaults for the shell and regular Spark apps.
    We also need to append JVM arguments when launching a job executed by an executor
    and managed by the driver. For this, you should edit the `conf/spark-defaults.conf`.
    In short, the following options can be added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the discussion clearer, we need to hide all the logs generated by Spark.
    We then can redirect them to be logged in the file system. On the other hand,
    we want our own logs to be logged in the shell and a separate file so that they
    don''t get mixed up with the ones from Spark. From here, we will point Spark to
    the files where our own logs are, which in this particular case is `/var/log/sparkU.log`.
    This `log4j.properties` file is then picked up by Spark when the application starts,
    so we don''t have to do anything aside of placing it in the mentioned location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, everything is printed as INFO once the log level is
    set to `INFO` until you set the level to new level for example `WARN`. However,
    after that no info or trace and so on, that will note be printed. In addition
    to that, there are several valid logging levels supported by log4j with Spark.
    The successful execution of the preceding code should generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also set up the default logging for Spark shell in `conf/log4j.properties`.
    Spark provides a template of the log4j as a property file, and we can extend and
    modify that file for logging in Spark. Move to the `SPARK_HOME/conf` directory
    and you should see the `log4j.properties.template` file. You should use the following
    `conf/log4j.properties.template` after renaming it to `log4j.properties`. While
    developing your Spark application, you can put the `log4j.properties` file under
    your project directory while working on an IDE-based environment such as Eclipse.
    However, to disable logging completely, just set the `log4j.logger.org` flags
    as `OFF` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, everything is very easy. However, there is a problem we haven''t noticed
    yet in the preceding code segment. One drawback of the `org.apache.log4j.Logger`
    class is that it is not serializable, which implies that we cannot use it inside
    a closure while doing operations on some parts of the Spark API. For example,
    suppose we do the following in our Spark code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You should experience an exception that says `Task` not serializable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'At first, we can try to solve this problem in a naive way. What you can do
    is just make the Scala class (that does the actual operation) `Serializable` using
    `extends Serializable` . For example, the code looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This section is intended for carrying out a discussion on logging. However,
    we take the opportunity to make it more versatile for general purpose Spark programming
    and issues. In order to overcome the `task not serializable` error in a more efficient
    way, compiler will try to send the whole object (not only the lambda) by making
    it serializable and forces SPark to accept that. However, it increases shuffling
    significantly, especially for big objects! The other ways are making the whole
    class `Serializable` or by declaring the instance only within the lambda function
    passed in the map operation. Sometimes, keeping the not `Serializable` objects
    across the nodes can work. Lastly, use the `forEachPartition()` or `mapPartitions()`
    instead of just `map()` and create the not `Serializable` objects. In summary,
    these are the ways to solve the problem around:'
  prefs: []
  type: TYPE_NORMAL
- en: Serializable the class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declare the instance only within the lambda function passed in the map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make the NotSerializable object as a static and create it once per machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call the `forEachPartition ()` or `mapPartitions()` instead of `map()` and create
    the NotSerializable object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding code, we have used the annotation `@transient lazy`, which
    marks the `Logger` class to be nonpersistent. On the other hand, object containing
    the method apply (i.e. `MyMapperObject`) that instantiate the object of the `MyMapper`
    class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the object containing the `main()` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see another example that provides better insight to keep fighting
    the issue we are talking about. Suppose we have the following class that computes
    the multiplication of two integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, essentially, if you try to use this class for computing the multiplication
    in the lambda closure using `map()`, you will get the `Task Not Serializable`
    error that we described earlier. Now we simply can use `foreachPartition()` and
    the lambda inside as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you compile it, it should return the desired result. For your ease,
    the complete code with the `main()` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Debugging the Spark application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: "In this section, we will discuss how to debug Spark applications running on\
    \ locally on Eclipse or IntelliJ, as standalone or cluster mode in YARN or Mesos.\
    \ Before getting started, you can also read the debugging documentation at [\uFEFF\
    https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/](https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/)."
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark application on Eclipse as Scala debug
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make this happen, just configure your Eclipse to debug your Spark applications
    as a regular Scala code debug. To configure select Run | Debug Configuration |
    Scala Application as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00022.jpeg)**Figure 17:** Configuring Eclipse to debug Spark applications
    as a regular Scala code debug'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to debug our `KMeansDemo.scala` and ask Eclipse (you can have
    similar options on InteliJ IDE) to start the execution at line 56 and set the
    breakpoint in line 95\. To do so, run your Scala code as debugging and you should
    observe the following scenario on Eclipse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00327.jpeg)**Figure 18:** Debugging Spark applications on Eclipse'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, Eclipse will pause on the line you ask it to stop the execution in line
    95, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00221.jpeg)**Figure 19:** Debugging Spark applications on Eclipse (breakpoint)'
  prefs: []
  type: TYPE_IMG
- en: In summary, to simplify the preceding example, if there is any error between
    line 56 and line 95, Eclipse will show where the error actually occurs. Otherwise,
    it will follow the normal workflow if not interrupted.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark jobs running as local and standalone mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While debugging your Spark application locally or as standalone mode, you should
    know that debugging the driver program and debugging one of the executors is different
    since using these two types of nodes requires different submission parameters
    passed to `spark-submit`. Throughout this section, I''ll use port 4000 as the
    address. For example, if you want to debug the driver program, you can add the
    following to your `spark-submit` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: After that, you should set your remote debugger to connect to the node where
    you have submitted the driver program. For the preceding case, port number 4000
    was specified. However, if something (that is, other Spark jobs, other applications
    or services, and so on) is already running on that port, you might also need to
    customize that port, that is, change the port number.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, connecting to an executor is similar to the preceding option,
    except for the address option. More specifically, you will have to replace the
    address with your local machine''s address (IP address or host name with the port
    number). However, it is always a good practice and recommended to test that you
    can access your local machine from the Spark cluster where the actual computing
    occurs. For example, you can use the following options to make the debugging environment
    enable to your `spark-submit` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In summary, use the following command to submit your Spark jobs (the `KMeansDemo`
    application in this case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, start your local debugger in a listening mode and start your Spark program.
    Finally, wait for the executor to attach to your debugger. You will observe the
    following message on your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to know that you need to set the number of executors to 1 only.
    Setting multiple executors will all try to connect to your debugger and will eventually
    create some weird problems. It is to be noted that sometimes setting the `SPARK_JAVA_OPTS`
    helps in debugging your Spark applications that are running locally or as standalone
    mode. The command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: However, since Spark release 1.0.0, `SPARK_JAVA_OPTS` has been deprecated and
    replaced by `spark-defaults.conf` and command line arguments to Spark-submit or
    Spark-shell. It is also to be noted that setting `spark.driver.extraJavaOptions`
    and `spark.executor.extraJavaOptions`, which we saw in the previous section, in
    `spark-defaults.conf` is not a replacement for `SPARK_JAVA_OPTS`. But to be frank,
    `SPARK_JAVA_OPTS`, it still works pretty well and you can try as well.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark applications on YARN or Mesos cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you run a Spark application on YARN, there is an option that you can enable
    by modifying `yarn-env.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the remote debugging will be available through port 4000 on your Eclipse
    or IntelliJ IDE. The second option is by setting the `SPARK_SUBMIT_OPTS`. You
    can use either Eclipse or IntelliJ to develop your Spark applications that can
    be submitted to be executed on remote multinode YARN clusters. What I do is that
    I create a Maven project on Eclipse or IntelliJ and package my Java or Scala application
    as a jar file and then submit it as a Spark job. However, in order to attach your
    IDE such as Eclipse or IntelliJ debugger to your Spark application, you can define
    all the submission parameters using the `SPARK_SUBMIT_OPTS` environment variable
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then submit your Spark job as follows (please change the values accordingly
    based on your requirements and setup):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding command, it will wait until you connect your debugger,
    as shown in the following: `Listening for transport dt_socket at address: 4000`.
    Now you can configure your Java remote application (Scala application will work
    too) on the IntelliJ debugger, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00331.jpeg)**Figure 20:** Configuring remote debugger on IntelliJ'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the preceding case, 10.200.1.101 is the IP address of the remote computing
    node where your Spark job is basically running. Finally, you will have to start
    the debugger by clicking on Debug under IntelliJ''s Run menu. Then, if the debugger
    connects to your remote Spark app, you will see the logging info in the application
    console on IntelliJ. Now if you can set the breakpoints and the rests of them
    are normal debugging. The following figure shows an example how will you see on
    the IntelliJ when pausing a Spark job with a breakpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00007.jpeg)**Figure 21:** An example how will you see on the IntelliJ
    when pausing a Spark job with a breakpoint'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it works well, but sometimes I experienced that using `SPARK_JAVA_OPTS`
    won''t help you much in the debug process on Eclipse or even IntelliJ. Instead,
    use and export `SPARK_WORKER_OPTS` and `SPARK_MASTER_OPTS` while running your
    Spark jobs on a real cluster (YARN, Mesos, or AWS) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then start your Master node as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now open an SSH connection to your remote machine where the Spark job is actually
    running and map your localhost at 4000 (aka `localhost:4000`) to `host_name_to_your_computer.org:5000`,
    assuming the cluster is at `host_name_to_your_computer.org:5000` and listening
    on port 5000\. Now that your Eclipse will consider that you''re just debugging
    your Spark application as a local Spark application or process. However, to make
    this happen, you will have to configure the remote debugger on Eclipse, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00141.jpeg)**Figure 22:** Connecting remote host on Eclipse for debugging
    Spark application'
  prefs: []
  type: TYPE_NORMAL
- en: That's it! Now you can debug on your live cluster as if it were your desktop.
    The preceding examples are for running with the Spark Master set as YARN-client.
    However, it should also work when running on a Mesos cluster. If you're running
    using YARN-cluster mode, you may have to set the driver to attach to your debugger
    rather than attaching your debugger to the driver since you won't necessarily
    know in advance what mode the driver will be executing on.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark application using SBT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding setting works mostly on Eclipse or IntelliJ using the Maven project.
    Suppose that you already have your application done and are working on your preferred
    IDEs such as IntelliJ or Eclipse as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you want to get this job to the local cluster (standalone), the very
    first step is packaging the application with all its dependencies into a fat JAR.
    For doing this, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the fat JAR. Now the task is to submit the Spark job to
    a local cluster. You need to have spark-submit script somewhere on your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command exports a Java argument that will be used to start Spark
    with the debugger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding command, `--class` needs to point to a fully qualified class
    path to your job. Upon successful execution of this command, your Spark job will
    be executed without breaking at the breakpoints. Now to get the debugging facility
    on your IDE, say IntelliJ, you need to configure to connect to the cluster. For
    more details on the official IDEA documentation, refer to [http://stackoverflow.com/questions/21114066/attach-intellij-idea-debugger-to-a-running-java-process](http://stackoverflow.com/questions/21114066/attach-intellij-idea-debugger-to-a-running-java-process).
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that if you just create a default remote run/debug configuration
    and leave the default port of 5005, it should work fine. Now, when you submit
    the job for the next time and see the message to attach the debugger, you have
    eight seconds to switch to IntelliJ IDEA and trigger this run configuration. The
    program will then continue to execute and pause at any breakpoint you defined.
    You can then step through it like any normal Scala/Java program. You can even
    step into Spark functions to see what it's doing under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw how difficult the testing and debugging your Spark
    applications are. These can even be more critical in a distributed environment.
    We also discussed some advanced ways to tackle them altogether. In summary, you
    learned the way of testing in a distributed environment. Then you learned a better
    way of testing your Spark application. Finally, we discussed some advanced ways
    of debugging Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: We believe that this book will help you to gain some good understanding of Spark.
    Nevertheless, due to page limitation, we could not cover many APIs and their underlying
    functionalities. If you face any issues, please don't forget to report this to
    Spark user mailing list at `user@spark.apache.org`. Before doing so, make sure
    that you have subscribed to it.
  prefs: []
  type: TYPE_NORMAL
- en: This is more or less the end of our little journey with advanced topics on Spark.
    Now, a general suggestion from our side to you as readers or if you are relatively
    newer to the data science, data analytics, machine learning, Scala, or Spark is
    that you should at first try to understand what types of analytics you want to
    perform. To be more specific, for example, if your problem is a machine learning
    problem, try to guess what type of learning algorithms should be the best fit,
    that is, classification, clustering, regression, recommendation, or frequent pattern
    mining. Then define and formulate the problem, and after that, you should generate
    or download the appropriate data based on the feature engineering concept of Spark
    that we have discussed earlier. On the other hand, if you think that you can solve
    your problem using deep learning algorithms or APIs, you should use other third-party
    algorithms and integrate with Spark and work straight away.
  prefs: []
  type: TYPE_NORMAL
- en: Our final recommendation to the readers is to browse the Spark website (at [http://spark.apache.org/](http://spark.apache.org/))
    regularly to get the updates and also try to incorporate the regular Spark-provided
    APIs with other third-party applications or tools to get the best result of the
    collaboration.
  prefs: []
  type: TYPE_NORMAL
