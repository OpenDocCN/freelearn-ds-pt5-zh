- en: Big Data With Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop has become the de facto standard in the world of big data, especially
    over the past three to four years. Hadoop started as a subproject of Apache Nutch
    in 2006 and introduced two key features related to distributed filesystems and
    distributed computing, also known as MapReduce, that caught on very rapidly among
    the open source community. Today, there are thousands of new products that have
    been developed leveraging the core features of Hadoop, and it has evolved into
    a vast ecosystem consisting of more than 150 related major products. Arguably,
    Hadoop was one of the primary catalysts that started the big data and analytics
    industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the background and core concepts of Hadoop,
    the components of the Hadoop platform, and delve deeper into the major products
    in the Hadoop ecosystem. We will learn about the core concepts of distributed
    filesystems and distributed processing and optimizations to improve the performance
    of Hadoop deployments. We''ll conclude with real-world hands-on exercises using
    the **Cloudera Distribution of Hadoop** (**CDH**). The topics we will cover are:'
  prefs: []
  type: TYPE_NORMAL
- en: The basics of Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The core components of Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop 1 and Hadoop 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hadoop Distributed File System
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed computing principles with MapReduce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hadoop ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the Hadoop ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive, HBase, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop Enterprise deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-house deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on with Cloudera Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MapReduce with WordCount
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fundamentals of Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2006, Doug Cutting, the creator of Hadoop, was working at Yahoo!. He was
    actively engaged in an open source project called Nutch that involved the development
    of a large-scale web crawler. A web crawler at a high level is essentially software
    that can browse and index web pages, generally in an automatic manner, on the
    internet. Intuitively, this involves efficient management and computation across
    large volumes of data. In late January of 2006, Doug formally announced the start
    of Hadoop. The first line of the request, still available on the internet at [https://issues.apache.org/jira/browse/INFRA-700,](https://issues.apache.org/jira/browse/INFRA-700)
    was *The Lucene PMC has voted to split part of Nutch into a new subproject named
    Hadoop*. And thus, Hadoop was born.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the onset, Hadoop had two core components : **Hadoop Distributed File System**
    (**HDFS**) and MapReduce. This was the first iteration of Hadoop, also now known
    as Hadoop 1\. Later, in 2012, a third component was added known as **YARN** (**Yet
    Another Resource Negotiator**) which decoupled the process of resource management
    and job scheduling. Before we delve into the core components in more detail, it
    would help to get an understanding of the fundamental premises of Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5dbca749-d341-4994-ad32-89f3820278d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Doug Cutting's post at [https://issues.apache.org/jira/browse/NUTCH-193](https://issues.apache.org/jira/browse/NUTCH-193)
    announced his intent to separate **Nutch Distributed FS** (**NDFS**) and MapReduce
    to a new subproject called Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental premise of Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fundamental premise of Hadoop is that instead of attempting to perform a
    task on a single large machine, the task can be subdivided into smaller segments
    that can then be delegated to multiple smaller machines. These so-called smaller
    machines would then perform the task on their own portion of the data. Once the
    smaller machines have completed their tasks to produce the results on the tasks
    they were allocated, the individual units of results would then be aggregated
    to produce the final result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although, in theory, this may appear relatively simple, there are various technical
    considerations to bear in mind. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the network fast enough to collect the results from each individual server?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can each individual server read data fast enough from the disk?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If one or more of the servers fail, do we have to start all over?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are multiple large tasks, how should they be prioritized?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many more such considerations that must be considered when working
    with a distributed architecture of this nature.
  prefs: []
  type: TYPE_NORMAL
- en: The core modules of Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The core modules of Hadoop consist of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hadoop Common**: Libraries and other common helper utilities required by
    Hadoop'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HDFS**: A distributed, highly-available, fault-tolerant filesystem that stores
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop MapReduce**: A programming paradigm involving distributed computing
    across commodity servers (or nodes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop YARN**: A framework for job scheduling and resource management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of these core components, YARN was introduced in 2012 to address some of the
    shortcomings of the first release of Hadoop. The first version of Hadoop (or equivalently,
    the first model of Hadoop) used HDFS and MapReduce as its main components. As
    Hadoop gained in popularity, the need to use facilities beyond those provided
    by MapReduce became more and more important. This, along with some other technical
    considerations, led to the development of YARN.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at the salient characteristics of Hadoop as itemized previously.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop Distributed File System - HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The HDFS forms the underlying basis of all Hadoop installations. Files, or more
    generally data, is stored in HDFS and accessed by the nodes of Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: 'HDFS performs two main functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Namespaces**: Provides namespaces that hold cluster metadata, that is, the
    location of data in the Hadoop cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data storage**: Acts as storage for data used in the Hadoop cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The filesystem is termed as distributed since the data is stored in chunks across
    multiple servers. An intuitive understanding of HDFS can be gained from a simple
    example, as follows. Consider a large book that consists of Chapters A - Z. In
    ordinary filesystems, the entire book would be stored as a single file on the
    disk. In HDFS, the book would be split into smaller chunks, say a chunk for Chapters
    A - H, another for I - P, and a third one for Q - Z. These chunks are then stored
    in separate racks (or bookshelves as with this analogy). Further, the chapters
    are replicated three times, such that there are three copies of each of the chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose, further, the size of the entire book is 1 GB, and each chapter is
    approximately 350 MB:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46a63885-0e9f-4d87-a5d3-c9edb3be3e6a.png)'
  prefs: []
  type: TYPE_IMG
- en: A bookshelf analogy for HDFS
  prefs: []
  type: TYPE_NORMAL
- en: 'Storing the book in this manner achieves a few important objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: Since the book has been split into three parts by groups of chapters and each
    part has been replicated three times, it means that our process can read the book
    in parallel by querying the parts from different servers. This reduces I/O contention
    and is a very fitting example of the proper use of parallelism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any of the racks are not available, we can retrieve the chapters from any
    of the other racks as there are multiple copies of each chapter available on different
    racks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a task I have been given only requires me to access a single chapter, for
    example, Chapter B, I need to access only the file corresponding to Chapters A-H.
    Since the size of the file corresponding to Chapters A-H is a third the size of
    the entire book, the time to access and read the file would be much smaller.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other benefits, such as selective access rights to different chapter groups
    and so on, would also be possible with such a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This may be an over-simplified analogy of the actual HDFS functionality, but
    it conveys the basic principle of the technology - that large files are subdivided
    into blocks (chunks) and spread across multiple servers in a high-availability
    redundant configuration. We''ll now look at the actual HDFS architecture in a
    bit more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9eace37-f3d1-4033-9f6c-9b85130a311b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The HDFS backend of Hadoop consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NameNode**: This can be considered the master node. The NameNode contains
    cluster metadata and is aware of what data is stored in which location - in short,
    it holds the namespace. It stores the entire namespace in RAM and when a request
    arrives, provides information on which servers hold the data required for the
    task. In Hadoop 2, there can be more than one NameNode. A secondary NameNode can
    be created that acts as a helper node to the primary. As such, it is not a backup
    NameNode, but one that helps in keeping cluster metadata up to date.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataNode**: The DataNodes are the individual servers that are responsible
    for storing chunks of the data and performing compute operations when they receive
    a new request. These are primarily commodity servers that are less powerful in
    terms of resource and capacity than the NameNode that stores the cluster metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data storage process in HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following points should give a good idea of the data storage process:'
  prefs: []
  type: TYPE_NORMAL
- en: All data in HDFS is written in blocks, usually of size 128 MB. Thus, a single
    file of say size 512 MB would be split into four blocks (4 * 128 MB). These blocks
    are then written to DataNodes. To maintain redundancy and high availability, each
    block is replicated to create duplicate copies. In general, Hadoop installations
    have a replication factor of three, indicating that each block of data is replicated
    three times.
  prefs: []
  type: TYPE_NORMAL
- en: This guarantees redundancy such that in the event one of the servers fails or
    stops responding, there would always be a second and even a third copy available.
    To ensure that this process works seamlessly, the DataNode places the replicas
    in independent servers and can also ensure that the blocks are placed on servers
    in different racks in a data center. This is due to the fact that even if all
    the replicas were on independent servers, but all the servers were on the same
    rack, a rack power failure would mean that no replica would be available.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general process of writing data into HDFS is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The NameNode receives a request to write a new file to HDFS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the data has to be written in blocks or chunks, the HDFS client (the entity
    that made the request) begins caching data into a local buffer and once the buffer
    reaches the allocated chunk size (for example, 128 MB), it informs the NameNode
    that it is ready to write the first block (chunk) of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The NameNode, based on information available to it about the state of the HDFS
    cluster, responds with information on the destination DataNode where the block
    needs to be stored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The HDFS client writes data to the target DataNode and informs the NameNode
    once the write process for the block has completed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The target DataNode, subsequently, begins copying its copy of the block of data
    to a second DataNode, which will serve as a replica for the current block.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the second DataNode completes the write process, it sends the block of
    data to the third DataNode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process repeats until all the blocks corresponding to the data (or equivalently,
    the file) are copied across different nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the number of chunks will depend on the file size. The following image
    illustrated the distribution of the data across 5 datanodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0750456b-a13d-47df-ba1d-d734014189af.png)'
  prefs: []
  type: TYPE_IMG
- en: Master Node and Data Nodes
  prefs: []
  type: TYPE_NORMAL
- en: 'The HDFS architecture in the first release of Hadoop, also known as Hadoop
    1, had the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Single NameNode: Only one NameNode was available, and as a result it also acted
    as a single point of failure since it stored all the cluster metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple DataNodes that stored blocks of data, processed client requests, and
    performed I/O operations (create, read, delete, and so on) on the blocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HDFS architecture in the second release of Hadoop, also known as Hadoop
    2, provided all the benefits of the original HDFS design and also added some new
    features, most notably, the ability to have multiple NameNodes that can act as
    primary and secondary NameNodes. Other features included the facility to have
    multiple namespaces as well as HDFS Federation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HDFS Federation deserves special mention. The following excerpt from [http://hadoop.apache.org](http://hadoop.apache.org)
    explains the subject in a very precise manner:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NameNodes are federated; the NameNodes are independent and do not require
    coordination with each other. The DataNodes are used as common storage for blocks
    by all the NameNodes. Each DataNode registers with all the NameNodes in the cluster.
    DataNodes send periodic heartbeats and block reports.
  prefs: []
  type: TYPE_NORMAL
- en: The secondary NameNode is not a backup node in the sense that it cannot perform
    the same tasks as the NameNode in the event that the NameNode is not available.
    However, it makes the NameNode restart process much more efficient by performing
    housekeeping operations.
  prefs: []
  type: TYPE_NORMAL
- en: These operations (such as merging HDFS snapshot data with information on data
    changes) are generally performed by the NameNode when it is restarted and can
    take a long time depending on the amount of changes since the last restart. The
    secondary NameNode can, however, perform these housekeeping operations whilst
    the primary NameNode is still in operation, such that in the event of a restart
    the primary NameNode can recover much faster. Since the secondary NameNode essentially
    performs a checkpoint on the HDFS data at periodic intervals, it is also known
    as the checkpoint node.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MapReduce was one of the seminal features of Hadoop that was arguably the most
    instrumental in bringing it to prominence. MapReduce works on the principle of
    dividing larger tasks into smaller subtasks. Instead of delegating a single machine
    to compute a large task, a network of smaller machines can instead be used to
    complete the smaller subtasks. By distributing the work in this manner, the task
    can be completed much more efficiently relative to using a single-machine architecture.
  prefs: []
  type: TYPE_NORMAL
- en: This is not dissimilar to how we go about completing work in our day-to-day
    lives. An example will help to make this clearer.
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive introduction to MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take the example of a hypothetical organization consisting of a CEO, directors,
    and managers. The CEO wants to know how many new hires have joined the company.
    The CEO sends a request to his or her directors to report back the number of hires
    in their departments. The directors in turn send a request to managers in their
    individual departments to provide the number of new hires. The managers provide
    the number to the directors, who in turn send the final value back to the CEO.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be considered to be a real-world example of MapReduce. In this analogy,
    the task was finding the number of new hires. Instead of collecting all the data
    on his or her own, the CEO delegated it to the directors and managers who provided
    their own individual departmental numbers as illustrated in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d094fb0-9403-42f9-b7cc-04ad8f8e0747.png)'
  prefs: []
  type: TYPE_IMG
- en: The Concept of MapReduce
  prefs: []
  type: TYPE_NORMAL
- en: In this rather simplistic scenario, the process of splitting a large task (find
    new hires in the entire company), into smaller tasks (new hires in each team),
    and then a final re-aggregation of the individual numbers, is analogous to how
    MapReduce works.
  prefs: []
  type: TYPE_NORMAL
- en: A technical understanding of MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MapReduce, as the name implies, has a map phase and a reduce phase. A map phase
    is generally a function that is applied on each element of its input, thus modifying
    its original value.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce generates key-value pairs as output.
  prefs: []
  type: TYPE_NORMAL
- en: '**Key-value:** A key-value pair establishes a relationship. For example, if
    John is 20 years old, a simple key-value pair could be (John, 20). In MapReduce,
    the map operation produces such key-value pairs that have an entity and the value
    assigned to the entity.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, map functions can be complex and involve advanced functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reduce phase takes the key-value input from the map function and performs
    a summarization operation. For example, consider the output of a map operation
    that contains the ages of students in different grades in a school:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Student name** | **Class** | **Age** |'
  prefs: []
  type: TYPE_TB
- en: '| John | Grade 1 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| Mary | Grade 2 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Jill | Grade 1 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| Tom | Grade 3 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Mark | Grade 3 | 9 |'
  prefs: []
  type: TYPE_TB
- en: We can create a simple key-value pair, taking for example the value of Class
    and Age (it can be anything, but I'm just taking these to provide the example).
    In this case, our key-value pairs would be (Grade 1, 7), (Grade 2, 8), (Grade
    1, 6), (Grade 3, 10), and (Grade 3, 9).
  prefs: []
  type: TYPE_NORMAL
- en: An operation that calculates the average of the ages of students in each grade
    could then be defined as a reduce operation.
  prefs: []
  type: TYPE_NORMAL
- en: More concretely, we can sort the output and then send the tuples corresponding
    to each grade to a different server.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Server A would receive the tuples (Grade 1, 7) and (Grade 1, 6),
    Server B would receive the tuple (Grade 2, 8), Server C would receive the tuples
    (Grade 3, 10) and (Grade 3, 9). Each of the servers, A, B, and C, would then find
    the average of the tuples and report back (Grade 1, 6.5), (Grade 2, 8), and (Grade
    3, 9.5).
  prefs: []
  type: TYPE_NORMAL
- en: Observe that there was an intermediary step in this process that involved sending
    the output to a particular server and sorting the output to determine which server
    it should be sent to. And indeed, MapReduce requires a shuffle and sort phase,
    whereby the key-value pairs are sorted so that each reducer receives a fixed set
    of unique keys.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, if say, instead of three servers there were only two, Server
    A could be assigned to computing averages for keys corresponding to Grades 1 and
    2, and Server B could be assigned to computing an average for Grade 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Hadoop, the following process takes place during MapReduce:'
  prefs: []
  type: TYPE_NORMAL
- en: The client sends a request for a task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NameNode allocates DataNodes (individual servers) that will perform the map
    operation and ones that will perform the reduce operation. Note that the selection
    of the DataNode server is dependent upon whether the data that is required for
    the operation is *local to the server*. The servers where the data resides can
    only perform the map operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DataNodes perform the map phase and produce key-value (k,v) pairs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the mapper produces the (k,v) pairs, they are sent to these reduce nodes
    based on the *keys* the node is assigned to compute. The allocation of keys to
    servers is dependent upon a partitioner function, which could be as simple as
    a hash value of the key (this is default in Hadoop).
  prefs: []
  type: TYPE_NORMAL
- en: Once the reduce node receives its set of data corresponding to the keys it is
    responsible to compute on, it applies the reduce function and generates the final
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop maximizes the benefits of data locality. Map operations are performed
    by servers that hold the data locally, that is, on disk. More precisely, the map
    phase will be executed only by those servers that hold the blocks corresponding
    to the file. By delegating multiple individual nodes to perform computations independently,
    the Hadoop architecture can perform very large-scale data processing effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Block size and number of mappers and reducers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An important consideration in the MapReduce process is an understanding of HDFS
    block size, that is, the size of the chunks into which the files have been split.
    A MapReduce task that needs to access a certain file will need to perform the
    map operation on each block representing the file. For example, given a 512 MB
    file and a 128 MB block size, four blocks would be needed to store the entire
    file. Hence, a MapReduce operation will at a minimum require four map tasks whereby
    each map operation would be applied to each subset of the data (that is, each
    of the four blocks).
  prefs: []
  type: TYPE_NORMAL
- en: If the file was very large, however, and required say, 10,000 blocks to store,
    this means we would have required 10,000 map operations. But, if we had only 10
    servers, then we'd have to send 1,000 map operations to each server. This might
    be sub-optimal as it can lead to a high penalty due to disk I/O operations and
    resource allocation settings on a per-map basis.
  prefs: []
  type: TYPE_NORMAL
- en: The number of reducers required is summarized very elegantly on Hadoop Wiki
    ([https://wiki.apache.org/hadoop/HowManyMapsAndReduces](https://wiki.apache.org/hadoop/HowManyMapsAndReduces)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The ideal reducers should be the optimal value that gets them closest to:'
  prefs: []
  type: TYPE_NORMAL
- en: '* A multiple of the block size * A task time between 5 and 15 minutes * Creates
    the fewest files possible'
  prefs: []
  type: TYPE_NORMAL
- en: 'Anything other than that means there is a good chance your reducers are less
    than great. There is a tremendous tendency for users to use a REALLY high value
    ("More parallelism means faster!") or a REALLY low value ("I don''t want to blow
    my namespace quota!"). Both are equally dangerous, resulting in one or more of:'
  prefs: []
  type: TYPE_NORMAL
- en: '* Terrible performance on the next phase of the workflow * Terrible performance
    due to the shuffle * Terrible overall performance because you''ve overloaded the
    namenode with objects that are ultimately useless * Destroying disk IO for no
    really sane reason * Lots of network transfers due to dealing with crazy amounts
    of CFIF/MFIF work'
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: YARN was a module introduced in Hadoop 2\. In Hadoop 1, the process of managing
    jobs and monitoring them was performed by processes known as JobTracker and TaskTracker(s).
    NameNodes that ran the JobTracker daemon (process) would submit jobs to the DataNodes
    which ran TaskTracker daemons (processes).
  prefs: []
  type: TYPE_NORMAL
- en: 'The JobTracker was responsible for the co-ordination of all MapReduce jobs
    and served as a central administrator for managing processes, handling server
    failure, re-allocating to new DataNodes, and so on. The TaskTracker monitored
    the execution of jobs local to its own instance in the DataNode and provided feedback
    on the status to the JobTracker as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77c0cfdf-fc44-4b4d-91fe-2d0709ee0109.png)'
  prefs: []
  type: TYPE_IMG
- en: JobTracker and TaskTrackers
  prefs: []
  type: TYPE_NORMAL
- en: This design worked well for a long time, but as Hadoop evolved, the demands
    for more sophisticated and dynamic functionalities rose proportionally. In Hadoop
    1, the NameNode, and consequently the JobTracker process, managed both job scheduling
    and resource monitoring. In the event the NameNode failed, all activities in the
    cluster would cease immediately. Lastly, all jobs had to be represented in MapReduce
    terms - that is, all code would have to be written in the MapReduce framework
    in order to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hadoop 2 alleviated all these concerns:'
  prefs: []
  type: TYPE_NORMAL
- en: The process of job management, scheduling, and resource monitoring was decoupled
    and delegated to a new framework/module called YARN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A secondary NameNode could be defined which would act as a helper for the primary
    NameNode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further, Hadoop 2.0 would accommodate frameworks beyond MapReduce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of fixed map and reduce slots, Hadoop 2 would leverage containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In MapReduce, all data had to be read from disk, and this was fine for operations
    on large datasets but it was not optimal for operations on smaller datasets. In
    fact, any tasks that required very fast processing (low latency), were interactive
    in nature, or had multiple iterations (thus requiring multiple reads from the
    disk for the same data), would be extremely slow.
  prefs: []
  type: TYPE_NORMAL
- en: By removing these dependencies, Hadoop 2 allowed developers to implement new
    programming frameworks that would support jobs with diverse performance requirements,
    such as low latency and interactive real-time querying, iterative processing required
    for machine learning, different topologies such as the processing of streaming
    data, optimizations such as in-memory data caching/processing, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few new terms became prominent:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ApplicationMaster**: Responsible for managing the resources needed by applications.
    For example, if a certain job required more memory, the ApplicationMaster would
    be responsible for securing the required resource. An application in this context
    refers to application execution frameworks such as MapReduce, Spark, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Containers**: The unit of resource allocation (for example, 1 GB of memory
    and four CPUs). An application may require several such containers to execute.
    The ResourceManager allocates containers for executing tasks. Once the allocation
    is complete, the ApplicationMaster requests DataNodes to start the allocated containers
    and takes over the management of the containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ResourceManager**: A component of YARN that had the primary role of allocating
    resources to applications and functioned as a replacement for the JobTracker.
    The ResourceManager process ran on the NameNode just as the JobTracker did.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NodeManagers**: A replacement for TaskTracker, NodeManagers were responsible
    for reporting the status of jobs to the ResourceManager (RM) and monitoring the
    resource utilization of containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image shows a high level view of the ResourceManager and NodeManagers
    in Hadoop 2.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9debf45a-4304-4551-9c7b-ac04b702b13f.png)'
  prefs: []
  type: TYPE_IMG
- en: Hadoop 2.0
  prefs: []
  type: TYPE_NORMAL
- en: 'The prominent concepts inherent in Hadoop 2 have been illustrated in the next
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86aed323-c6e4-4dbe-9272-3fc5e50d9441.png)'
  prefs: []
  type: TYPE_IMG
- en: Hadoop 2.0 Concepts
  prefs: []
  type: TYPE_NORMAL
- en: Job scheduling in YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not uncommon for large Hadoop clusters to have multiple jobs running concurrently.
    The allocation of resources when there are multiple jobs submitted from multiple
    departments becomes an important and indeed interesting topic. Which request should
    receive priority if say, two departments, A and B, submit a job at the same time
    but each request is for the maximum available resources? In general, Hadoop uses
    a **First-In-First-Out** (**FIFO**) policy. That is, whoever submits the job first
    gets to use the resources first. But what if A submitted the job first but completing
    A's job will take five hours whereas B's job will complete in five minutes?
  prefs: []
  type: TYPE_NORMAL
- en: 'To deal with these nuances and variables in job scheduling, numerous scheduling
    methods have been implemented. Three of the more commonly used ones are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FIFO**: As described above, FIFO scheduling uses a queue to priorities jobs.
    Jobs are executed in the order in which they are submitted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CapacityScheduler**: CapacityScheduler assigns a value on the number of jobs
    that can be submitted on a per-department basis, where a department can indicate
    a logical group of users. This is to ensure that each department or group can
    have access to the Hadoop cluster and be able to utilize a minimum number of resources.
    The scheduler also allows departments to scale up beyond their assigned capacity
    up to a maximum value set on a per-department basis if there are unused resources
    on the server. The model of CapacityScheduler thus provides a guarantee that each
    department can access the cluster on a deterministic basis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fair Schedulers**: These schedulers attempt to evenly balance the utilization
    of resources across different apps. While an even balance might not be feasible
    at a certain given point in time, balancing allocation over time such that the
    averages are more or less similar can be achieved using Fair Schedulers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These, and other schedulers, provide finely grained access controls (such as
    on a per-user or per-group basis) and primarily utilize queues in order to prioritize
    and allocate resources.
  prefs: []
  type: TYPE_NORMAL
- en: Other topics in Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A few other aspects of Hadoop deserve special mention. As we have discussed
    the most important topics at length, this section provides an overview of some
    of the other subjects of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Encryption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data encryption is mandated by official regulations for various types of data.
    In the US, data that identifies patient information is required to be compliant
    with the rules set forth by HIPAA that dictate how such records should be stored.
    Data in HDFS can be encrypted whilst at rest (on disk) and/or while in transit.
    The keys that are used to decrypt the data are generally managed by **Key Management
    Systems** (**KMSs**).
  prefs: []
  type: TYPE_NORMAL
- en: User authentication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop can use the native user-authentication methods of the server. For example,
    in Linux-based machines, users can be authenticated based on the IDs defined in
    the system's `/etc/passwd` files. In other words, Hadoop inherits the user authentication
    set up on the server side.
  prefs: []
  type: TYPE_NORMAL
- en: User authentication via Kerberos, a cross-platform authentication protocol,
    is also commonly used in Hadoop clusters. Kerberos works based on a concept of
    tickets that grant privileges to users on a temporary as-needed basis. Tickets
    can be invalidated using Kerberos commands, thus restricting the users' rights
    to access resources on the cluster as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Note that even if the user is permitted to access data (user authentication),
    he or she can still be limited in what data can be accessed due to another feature
    known as authorization. The term implies that even if the user can authenticate
    and log in to the system, the user may be restricted to only the data the user
    is authorized to access. This level of authorization is generally performed using
    native HDFS commands to change directory and file ownerships to the named users.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop data storage formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since Hadoop involves storing very large-scale data, it is essential to select
    a storage type that is appropriate for your use cases. There are a few formats
    in which data can be stored in Hadoop, and the selection of the optimal storage
    format depends on your requirements in terms of read/write I/O speeds, how well
    the files can be compressed and decompressed on demand, and how easily the file
    can be split since the data will be eventually stored as blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the popular and commonly used storage formats are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text/CSV**: These are plain text CSV files, similar to Excel files, but saved
    in plain text format. Since CSV files contain records on a per-line basis, it
    is naturally trivial to split the files up into blocks of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avro**: Avro was developed to improve the efficient sharing of data across
    heterogeneous systems. It stores both the schema as well as the actual data in
    a single compact binary using data serialization. Avro uses JSON to store the
    schema and binary format for the data and serializes them into a single Avro Object
    Container File. Multiple languages such as Python, Scala, C/C++, and others have
    native APIs that can read Avro files and consequently, it is very portable and
    well suited for cross-platform data exchange.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parquet**: Parquet is a columnar data storage format. This helps to improve
    performance, sometimes significantly by permitting data storage and access on
    a per-column basis. Intuitively, if you were working on a 1 GB file with 100 columns
    and 1 million rows, and wanted to query data from only one of the 100 columns,
    being able to access just the individual column would be more efficient than having
    to access the entire file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ORCFiles**: ORC stands for Optimized Row-Columnar. In a sense, it is a further
    layer of optimization over pure columnar formats such as Parquet. ORCFiles store
    data not only by columns, but also by rows, also known as stripes. A file with
    data in tabular format can thus be split into multiple smaller stripes where each
    stripe comprises of a subset of rows from the original file. By splitting data
    in this manner, if a user task requires access to only a small subsection of the
    data, the process can interrogate the specific stripe that holds the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SequenceFiles**: In SequenceFiles, data is represented as key-value pairs
    and stored in a binary serialized format. Due to serialization, data can be represented
    in a compact binary format that not only reduces the data size but consequently
    also improves I/O. Hadoop, and more concretely HDFS, is not efficient when there
    are multiple files of a small size, such as audio files. SequenceFiles solve this
    problem by allowing multiple small files to be stored as a single unit or SequenceFile.
    They are also very well suited for parallel operations that are splittable and
    are overall efficient for MapReduce jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HDFS Snapshots:** HDFS Snapshots allow users to preserve data at a given
    point in time in a read-only mode. Users can create snapshots—in essence a replica
    of the data as it is at that point time—in in HDFS, such that they can be retrieved
    at a later stage as and when needed. This ensures that data can be recovered in
    the event that there is a file corruption or any other failure that affects the
    availability of data. In that regard, it can be also considered to be a backup.
    The snapshots are available in a .snapshot directory where they have been created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling of node failures:** Large Hadoop clusters can contain tens of thousands
    of nodes. Hence it is likely that there would be server failures on any given
    day. So that the NameNode is aware of the status of all nodes in the cluster,
    the DataNodes send a periodic heartbeat to the NameNode. If the NameNode detects
    that a server has failed, that is, it has stopped receiving heartbeats, it marks
    the server as failed and replicates all the data that was local to the server
    onto a new instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New features expected in Hadoop 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing this book, Hadoop 3 is in Alpha stage. Details on the
    new changes that will be available in Hadoop 3 can be found on the internet. For
    example, [http://hadoop.apache.org/docs/current/](http://hadoop.apache.org/docs/current/)
    provides the most up-to-date information on new changes to the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The Hadoop ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter should be titled as the Apache ecosystem. Hadoop, like all the
    other projects that will be discussed in this section, is an Apache project. Apache
    is used loosely as a short form for the open source projects that are supported
    by the Apache Software Foundation. It originally has its roots in the development
    of the Apache HTTP server in the early 90s, and today is a collaborative global
    initiative that comprises entirely of volunteers who participate in releasing
    open source software to the global technical community.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop started out as, and still is, one of the projects in the Apache ecosystem.
    Due to its popularity, many other projects that are also part of Apache have been
    linked directly or indirectly to Hadoop as they support key functionalities in
    the Hadoop environment. That said, it is important to bear in mind that these
    projects can in most cases exist as independent products that can function without
    a Hadoop environment. Whether it would provide optimal functionality would be
    a separate topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we''ll go over some of the Apache projects that have had a
    great deal of influence as well as an impact on the growth and usability of Hadoop
    as a standard IT enterprise solution, as detailed in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Product** | **Functionality** |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Pig | Apache Pig, also known as Pig Latin, is a language specifically
    designed to represent MapReduce programs through concise statements that define
    workflows. Coding MapReduce programs in the traditional methods, such as with
    Java, can be quite complex, and Pig provides an easy abstraction to express a
    MapReduce workflow and complex **Extract-Transform-Load** (**ETL**) through the
    use of simple semantics. Pig programs are executed via the Grunt shell. |'
  prefs: []
  type: TYPE_TB
- en: '| Apache HBase | Apache HBase is a distributed column-oriented database that
    sits on top of HDFS. It was modelled on Google''s BigTable whereby data is represented
    in a columnar format. HBase supports low-latency read-write across tables with
    billions of records and is well suited to tasks that require direct random access
    to data. More concretely, HBase indexes data in three dimensions - row, column,
    and timestamp. It also provides a means to represent data with an arbitrary number
    of columns as column values can be expressed as key-value pairs within the cells
    of an HBase table. |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Hive | Apache Hive provides a SQL-like dialect to query data stored
    in HDFS. Hive stores data as serialized binary files in a folder-like structure
    in HDFS. Similar to tables in traditional database management systems, Hive stores
    data in tabular format in HDFS partitioned based on user-selected attributes.
    Partitions are thus subfolders of the higher-level directories or tables. There
    is a third level of abstraction provided by the concept of buckets, which reference
    files in the partitions of the Hive tables. |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Sqoop | Sqoop is used to extract data from traditional databases to
    HDFS. Large enterprises that have data stored in relational database management
    systems can thus use Sqoop to transfer data from their data warehouse to a Hadoop
    implementation. |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Flume | Flume is used for the management, aggregation, and analysis
    of large-scale log data. |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Kafka | Kafka is a publish/subscribe-based middleware system that
    can be used to analyze and subsequently persist (in HDFS) streaming data in real
    time. |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Oozie | Oozie is a workflow management system designed to schedule
    Hadoop jobs. It implements a key concept known as a **directed acyclic graph**
    (**DAG**), which will be discussed in our section on Spark. |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Spark | Spark is one of the most significant projects in Apache and
    was designed to address some of the shortcomings of the HDFS-MapReduce model.
    It started as a relatively small project at UC Berkeley and evolved rapidly to
    become one of the most prominent alternatives to using Hadoop for analytical tasks.
    Spark has seen a widespread adoption across the industry and comprises of various
    other subprojects that provide additional capabilities such as machine learning,
    streaming analytics, and others. |'
  prefs: []
  type: TYPE_TB
- en: Hands-on with CDH
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will utilize the CDH QuickStart VM to work through some
    of the topics that have been discussed in the current chapter. The exercises do
    not have to be necessarily performed in a chronological order and are not dependent
    upon the completion of any of the other exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will complete the following exercises in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: WordCount using Hadoop MapReduce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with the HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and querying data with Apache Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WordCount using Hadoop MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this exercise, we will be attempting to count the number of occurrences of
    each word in one of the longest novels ever written. For the exercise, we have
    selected the book *Artamène ou le Grand Cyrus* written by Georges and/or Madeleine
    de Scudéry between 1649-1653\. The book is considered to be the second longest
    novel ever written, per the related list on Wikipedia ([https://en.wikipedia.org/wiki/List_of_longest_novels](https://en.wikipedia.org/wiki/List_of_longest_novels)).
    The novel consists of 13,905 pages across 10 volumes and has close to two million
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we need to launch the Cloudera Distribution of Hadoop Quickstart
    VM in VirtualBox and double-click on the Cloudera Quickstart VM instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcae8e19-32d6-4d52-9798-b80487582a38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It will take some time to start up as it initializes all the CDH-related processes
    such as the DataNode, NameNode, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f3d9fb5-6d5a-48ed-9227-371380b5fac8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the process starts up, it will launch a default landing page that contains
    references to numerous tutorials related to Hadoop. We''ll be writing our MapReduce
    code in the Unix terminal for this section. Launch the terminal from the top-left
    menu, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26b31ed5-02d4-41c1-a81a-e2184b90b038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we must follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a directory named `cyrus`. This is where we will store all the files
    which contain the text of the book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `getCyrusFiles.sh` as shown in step 4\. This will download the book into
    the `cyrus` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `processCyrusFiles.sh` as shown. The book contains various Unicode and non-printable
    characters. Additionally, we would like to change all the words to lowercase in
    order to ignore double-counting words that are the same but have capitalizations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will produce a file called `cyrusprint.txt`. This document contains the
    entire text of the book. We will be running our MapReduce code on this text file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare `mapper.py` and `reducer.py`. As the name implies, `mapper.py` runs
    the map part of the MapReduce process. Similarly, `reducer.py` runs the reduce
    part of the MapReduce process. The file `mapper.py` will split the document into
    words and assign a value of one to each word in the document. The file, `reducer.py`,
    will read in the sorted output of `mapper.py` and sum the occurrences of the same
    word (by first initializing the count of the word to one and incrementing it for
    each new occurrence of the word). The final output is a file containing the count
    of each word in the document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create `getCyrusFiles.sh` - this script will be used to retrieve the data from
    the web:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `processCyrusFiles.sh` - this script will be used to concatenate and
    cleanse the files that were downloaded in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the permissions to 755 to make the `.sh` files executable at the command
    prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute `getCyrusFiles.sh`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute `processCyrusFiles.sh`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Execute the following steps to copy the final file, named `cyrusprint.txt`,
    to HDFS, create the `mapper.py` and `reducer.py` scripts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The files, `mapper.py` and `reducer.py`, are referenced on Glenn Klockwood's
    website ([http://www.glennklockwood.com/data-intensive/hadoop/streaming.html](http://www.glennklockwood.com/data-intensive/hadoop/streaming.html)),
    which provides a wealth of information on MapReduce and related topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the contents of `mapper.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the mapper and reducer scripts that will perform the MapReduce operations
    in order to produce the word count. You may see error messages as shown here,
    but for the purpose of this exercise (and for generating the results), you may
    disregard them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are stored in HDFS under the `/user/cloudera/output` directory
    in files prefixed with `part-` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the contents of the file use `hdfs dfs -cat` and provide the name of
    the file. In this case we are viewing the first 10 lines of the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Analyzing oil import prices with Hive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will use Hive to analyze the import prices of oil in countries
    across the world from 1980-2016\. The data is available from the site of the **OECD**
    (**Organisation for Economic Co-operation and Development**) at the URL shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23401bea-8fb7-4edb-af54-e882c4086d69.png)'
  prefs: []
  type: TYPE_IMG
- en: The actual CSV file is available at [https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&amp;detail=code&amp;separator=comma&amp;csv-lang=en](https://stats.oecd.org/sdmx-json/data/DP_LIVE/.OILIMPPRICE.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we''ll be loading the data in Hive, it makes sense to download the file
    into our home directory via the terminal in our Cloudera Quickstart CDH environment.
    The steps we''d execute are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the CSV file into the CDH environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/91b3f48c-3292-4ae8-bef9-e9b434b94fd9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Clean the CSV file. Data cleansing is an area of core importance in data science.
    In practice, it is very common to receive files that will require some level of
    cleansing. This is due to the fact that there could be invalid characters or values
    in columns, missing data, missing or additional delimiters, and so on. We noted
    that various values were enclosed in double-quotes ("). In Hive, we can ignore
    the quotes by specifying the `quoteChar` property whilst creating the table. Since
    Linux also offers simple and easy ways to remove such characters, we used `sed`
    to remove the quotation marks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, in our downloaded file, `oil.csv`, we observed that there were non-printable
    characters that could cause issues. We removed them by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '(Source: [http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix](http://alvinalexander.com/blog/post/linux-unix/how-remove-non-printable-ascii-characters-file-unix))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we copied the new file (`oil_clean.csv`) to `oil.csv`. Since the `oil.csv`
    file already existed in the same folder, we were prompted with an overwrite message
    and we entered `yes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Log in to Cloudera Hue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on Hue on the Bookmarks bar in the browser. This will bring up the Cloudera
    login screen. Log in using ID `cloudera` and password `cloudera`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e279c451-220f-434d-947b-c166b798f4cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Hue from the drop-down menu on Quick Start at the top of the Hue login
    window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ada22afe-0776-4378-9f95-a056ff1077f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Create the table schema, load the CSV file, `oil.csv`, and view the records:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/7663083b-9243-4b24-ad98-cc522f0faae4.png)'
  prefs: []
  type: TYPE_IMG
- en: Load the oil file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the table has been loaded into Hive, you can run miscellaneous Hive
    commands using HiveQL. A full set of these commands is available at [https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For instance, to find the maximum, minimum, and average value of oil prices
    in each country from 1980-2015 (the date range of the dataset), we can use familiar
    SQL operators. The query would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the screenshot of the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e414b22d-47af-4f7d-a052-474c1f7cf74d.png)'
  prefs: []
  type: TYPE_IMG
- en: In similar ways, we can use an array of other SQL commands. The Hive Manual
    provides an in-depth look into these commands and the various ways data can be
    saved, queried, and retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: Hue includes a set of useful features such as data visualization, data download,
    and others that allow users to perform ad hoc analysis on the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the visualization feature, click on the visualization icon underneath
    the grid icon in the results section, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/499fcfb2-f240-4123-97d1-2f1bb8dcdd62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Select Scatter. In Hue, this type of chart, also known more generally as a
    scatterplot, allows users to create multivariate charts very easily. Different
    values for the x and y axes, as well as scatter size and grouping, can be selected,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/259f4b41-4364-4376-b13e-befe47bd30a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is a simple pie chart that can be constructed by selecting Pie
    in the drop-down menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6106cd02-6123-4778-9133-00a18ac7e5d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Joining tables in Hive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hive supports advanced join functionalities. The following illustrates the process
    of using Left Join. As seen, the original table has data for each country represented
    by their three-letter country code. Since Hue supports map charts, we can add
    the values for latitude and longitude to overlay the oil pricing data on a world
    map.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we''ll need to download a dataset containing the values for latitude
    and longitude:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the file has been downloaded and cleansed, define the schema and load
    the data in Hive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c66e5c65-998c-425f-b7d3-0bbcb229eebb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Join the oil data with the lat/long data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6ff9ec7c-634e-43a5-bc35-4187a92c525b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now proceed with creating geospatial visualizations. It would be useful
    to bear in mind that these are preliminary visualizations in Hue that provide
    a very convenient means to view data. More in-depth visualizations can be developed
    on geographical data using shapefiles, polygons, and other advanced charting methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select Gradient Map from the drop-down menu and enter the appropriate values
    to create the chart, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b47321a-b355-411b-b262-9a5ea24c77cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next chart was developed using the Marker Map option in the drop-down menu.
    It uses the three-character country code in order to place markers and associated
    values on the respective regions, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08fb5f5c-9fd6-4352-9d0b-39d28dd86133.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided a technical overview of Hadoop. We discussed the core
    components and core concepts that are fundamental to Hadoop, such as MapReduce
    and HDFS. We also looked at the technical challenges and considerations of using
    Hadoop. While it may appear simple in concept, the inner workings and a formal
    administration of a Hadoop architecture can be fairly complex. In this chapter
    we highlighted a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: We concluded with a hands-on exercise on Hadoop using the Cloudera Distribution.
    For this tutorial, we used the CDH Virtual Machine downloaded earlier from Cloudera's
    website.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at NoSQL, an alternative or a complementary
    solution to Hadoop depending upon your individual and/or organization al needs.
    While Hadoop offers a far richer set of capabilities, if your intended use case(s)
    can be done with simply NoSQL solutions, the latter may be an easier choice in
    terms of the effort required.
  prefs: []
  type: TYPE_NORMAL
