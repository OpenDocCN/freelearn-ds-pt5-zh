<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 4.  Unified Data Access"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch04" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 4.    Unified Data Access   </h1></div></div></div><p class="calibre11">Data integration from disparate data sources had always been a daunting feat. The three V's of big data and ever-shrinking processing time frames have made the task even more challenging. Delivering a clear view of well-curated data in near real time is extremely important for business. However, real-time curated data along with the ability to perform different operations such as ETL, ad hoc querying, and machine learning in a unified fashion is what is emerging as a key business differentiator.</p><p class="calibre11">Apache Spark was created to offer a single general-purpose engine that can process data from a variety of data sources and support large-scale data processing for various different operations. Spark enables developers to combine SQL, Streaming, graphs, and machine learning algorithms in a single workflow!</p><p class="calibre11">In the previous chapters, we discussed <span class="strong"><strong class="calibre19">Resilient Distributed Datasets</strong></span> (<span class="strong"><strong class="calibre19">RDDs</strong></span>) as well as DataFrames. In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3.  Introduction to DataFrames">Chapter 3</a>, <span class="strong"><em class="calibre22">Introduction to DataFrames</em></span>, we introduced Spark SQL and the Catalyst optimizer. This chapter builds on this foundation and delves deeper into these topics to help you realize the real essence of unified data access. We'll introduce new constructs such as Datasets and Structured Streaming. Specifically, we'll discuss the following:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Data abstractions in Apache Spark</li><li class="listitem">Datasets<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Working with Datasets</li><li class="listitem">Dataset API limitations</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Spark SQL<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">SQL operations</li><li class="listitem">Under the hood</li></ul></div><p class="calibre31">
</p></li></ul></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Structured Streaming<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Spark streaming programming model</li><li class="listitem">Under the hood</li><li class="listitem">Comparison with other streaming engines</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Continuous applications</li><li class="listitem">Summary</li></ul></div><div class="calibre2" title="Data abstractions in Apache Spark"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec27" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data abstractions in Apache Spark</h1></div></div></div><p class="calibre11">The MapReduce framework and its popular open source implementation Hadoop enjoyed widespread adoption in the past decade. However, iterative algorithms and interactive ad-hoc querying are not well supported. Any data sharing between jobs or stages within an algorithm is always through disk writes and reads as against in-memory data sharing. So, the logical next step would be to have a mechanism that facilitates reuse of intermediate results across multiple jobs. RDD is a general-purpose data abstraction that was developed to address this requirement.</p><p class="calibre11">RDD is the core abstraction in Apache Spark. It is an immutable, fault-tolerant distributed collection of statically typed objects that are usually stored in-memory. RDD API offer simple operations such as map, reduce, and filter that can be composed in arbitrary ways.</p><p class="calibre11">DataFrame abstraction is built on top of RDD and it adds "named" columns. So, a Spark DataFrame has rows of named columns similar to relational database tables and DataFrames in R and Python (pandas). This familiar higher level abstraction makes the development effort much easier because it lets you perceive data like an SQL table or an Excel file. Moreover, the Catalyst optimizer, under the hood, compiles the operations and generates JVM bytecode for efficient execution. However, the named columns approach gives rise to a new problem. Static type information is no longer available to the compiler, and hence we lose the advantage of compile-time type safety.</p><p class="calibre11">Dataset API was introduced to combine the best traits from both RDDs and DataFrames plus some more features of its own. Datasets provide row and column data abstraction similar to the DataFrames, but with a structure defined on top of them. This structure may be defined by a case class in Scala or a class in Java. They provide type safety and lambda functions like RDDs. So, they support both typed methods such as <code class="literal">map</code> and <code class="literal">groupByKey</code> as well as untyped methods such as <code class="literal">select</code> and <code class="literal">groupBy</code>. In addition to the Catalyst optimizer, Datasets leverage in-memory encoding provided by the Tungsten execution engine, which improves performance even further.</p><p class="calibre11">The data abstractions introduced so far form the core abstractions. There are some more specialized data abstractions that work on top of these abstractions. Streaming APIs are introduced to process real-time streaming data from various sources such as Flume and Kafka. These APIs work together to provide data engineers a unified, continuous DataFrame abstraction that can be used for interactive and batch queries. Another example of specialized data abstraction is a GraphFrame. This enables developers to analyze social networks and any other graphs alongside Excel-like two-dimensional data.</p><p class="calibre11">Now with the basics of the available data abstractions in mind, let's understand what we exactly mean by a unified data access platform:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_04_001.jpg" alt="Data abstractions in Apache Spark" class="calibre36"/></div><p class="calibre11">
</p><p class="calibre11">The intention behind this unified platform is that it not only lets you combine the static and streaming data together, but also allows various different kinds of operations on the data in a unified way! From the developer's perspective, a Dataset is the core abstraction to work with, and Spark SQL is the main interface to the Spark functionality. A two-dimensional data structure coupled with a SQL declarative programming interface had been a familiar way of dealing with data, thereby shortening the learning curve for the data engineers. So, understanding the unified platform translates to understanding Datasets and Spark SQL.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Datasets"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec28" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Datasets</h1></div></div></div><p class="calibre11">Apache Spark <span class="strong"><strong class="calibre19">Datasets</strong></span> are an extension of the DataFrame API that provide a type-safe object-oriented programming interface. This API was first introduced in the 1.6 release. Spark 2.0 version brought out unification of DataFrame and Dataset APIs. DataFrame becomes a generic, untyped Dataset; or a Dataset is a DataFrame with an added structure. The term "structure" in this context refers to a pattern or an organization of underlying data, more like a table schema in RDBMS parlance. The structure imposes a limit on what can be expressed or contained in the underlying data. This in turn enables better optimizations in memory organization as well as physical execution. Compile-time type checking leads to catching errors earlier than during runtime. For example, a type mismatch in a SQL comparison does not get caught until runtime, whereas it would be caught during compile time itself if it were expressed as a sequence of operations on Datasets. However, the inherent dynamic nature of Python and R implies that there is no compile-time type safety, and hence the concept Datasets does not apply to those languages. The unification of Datasets and DataFrames applies to Scala and Java API only.</p><p class="calibre11">At the core of Dataset abstraction are the <span class="strong"><strong class="calibre19">encoders</strong></span>. These encoders translate between JVM objects and Spark's internal Tungsten binary format. This internal representation bypasses JVM's memory management and garbage collection. Spark has its own C-style memory access that is specifically written to address the kind of workflows it supports. The resultant internal representations take less memory and have efficient memory management. Compact memory representation leads to reduced network load during shuffle operations. The encoders generate compact byte code that directly operates on serialized objects without de-serializing, thereby enhancing performance. Knowing the schema early on results in a more optimal layout in memory when caching Datasets.</p><div class="calibre2" title="Working with Datasets"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch04lvl2sec38" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Working with Datasets</h2></div></div></div><p class="calibre11">In this section, we will create Datasets and perform transformations and actions, much like DataFrames and RDDs.</p><p class="calibre11">Example 1-creating a Dataset from a simple collection:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//Create a Dataset from a simple collection 
scala&gt; val ds1 = List.range(1,5).toDS() 
ds1: org.apache.spark.sql.Dataset[Int] = [value: int] 
//Perform an action 
scala&gt; ds1.collect() 
res3: Array[Int] = Array(1, 2, 3, 4) 
 
//Create from an RDD 
scala&gt; val colors = List("red","orange","blue","green","yellow") 
scala&gt; val color_ds = sc.parallelize(colors).map(x =&gt; 
     (x,x.length)).toDS() 
//Add a case class 
case class Color(var color: String, var len: Int) 
val color_ds = sc.parallelize(colors).map(x =&gt; 
     Color(x,x.length)).toDS() 
</pre><p class="calibre11">As shown in the last example in the preceding code, <code class="literal">case class</code> adds structure information. Spark uses this structure to create the best data layout and encoding. The following code shows us the structure and the plan for execution:</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//Examine the structure 
scala&gt; color_ds.dtypes 
res26: Array[(String, String)] = Array((color,StringType), (len,IntegerType)) 
scala&gt; color_ds.schema 
res25: org.apache.spark.sql.types.StructType = StructType(StructField(color,StringType,true), 
StructField(len,IntegerType,false)) 
//Examine the execution plan 
scala&gt; color_ds.explain() 
== Physical Plan == 
Scan ExistingRDD[color#57,len#58] 
</pre><p class="calibre11">The preceding example shows the structure and the implementation physical plan as anticipated. If you want to get a more detailed execution plan, you have to pass explain (true), which prints extended information, including the logical plan as well.</p><p class="calibre11">We have examined Dataset creation from simple collections and RDDs. We have already discussed that DataFrames are just untyped Datasets. The following examples show conversion between Datasets and DataFrames.</p><p class="calibre11">Example 2-converting the Dataset to a DataFrame</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//Convert the dataset to a DataFrame 
scala&gt; val color_df = color_ds.toDF() 
color_df: org.apache.spark.sql.DataFrame = [color: string, len: int] 
 
scala&gt; color_df.show() 
+------+---+ 
| color|len| 
+------+---+ 
|   red|  3| 
|orange|  6| 
|  blue|  4| 
| green|  5| 
|yellow|  6| 
+------+---+ 
</pre><p class="calibre11">This example looks very much like the examples we have seen in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3.  Introduction to DataFrames">Chapter 3</a>, <span class="strong"><em class="calibre22">Introduction to DataFrames</em></span>. These conversions become very handy in the real world. Consider adding a structure (aka case class) to imperfect data. You may first read that data into a DataFrame, perform cleansing, and then convert it to a Dataset. Another use case could be that you want to expose only a subset (rows and columns) of the data based on some runtime information, such as <code class="literal">user_id</code>. You could read the data into a DataFrame, register it as a temporary table, apply conditions, and expose the subset as a Dataset. The following example creates a <code class="literal">DataFrame</code> first and then converts it into <code class="literal">Dataset</code>. Note that the DataFrame column names must match the case class.</p><p class="calibre11">Example 3-convert a DataFrame to a Dataset</p><pre class="programlisting">//Construct a DataFrame first 
scala&gt; val color_df = sc.parallelize(colors).map(x =&gt; 
           (x,x.length)).toDF("color","len") 
color_df: org.apache.spark.sql.DataFrame = [color: string, len: int] 
//Convert the DataFrame to a Dataset with a given structure 
scala&gt; val ds_from_df = color_df.as[Color] 
ds_from_df: org.apache.spark.sql.Dataset[Color] = [color: string, len: int] 
//Check the execution plan 
scala&gt; ds_from_df.explain 
== Physical Plan == 
WholeStageCodegen 
:  +- Project [_1#102 AS color#105,_2#103 AS len#106] 
:     +- INPUT 
+- Scan ExistingRDD[_1#102,_2#103] 
</pre><p class="calibre11">The explain command response shows <code class="literal">WholeStageCodegen</code>, which fuses multiple operations into a single Java function call. This enhances performance due to reduction in multiple virtual function calls. Code generation had been around in Spark engine since 1.1, but at that time it was limited to expression evaluation and a small number of operations such as filter. In contrast, whole stage code generation from Tungsten generates code for the entire query plan.</p><div class="calibre2" title="Creating Datasets from JSON"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch04lvl3sec24" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Creating Datasets from JSON</h3></div></div></div><p class="calibre11">Datasets can be created from JSON files, similar to DataFrames. Note that a JSON file may contain several records, but each record has to be on one line. If your source JSON has newlines, you have to programmatically remove them. The JSON records may have arrays and may be nested. They need not have uniform schema. The following example file has JSON records with one record having an additional tag and an array of data.</p><p class="calibre11">Example 4-creating a Dataset from JSON</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">//Set filepath 
scala&gt; val file_path = &lt;Your path&gt; 
file_path: String = ./authors.json 
//Create case class to match schema 
scala&gt; case class Auth(first_name: String, last_name: String,books: Array[String]) 
defined class Auth 
 
//Create dataset from json using case class 
//Note that the json document should have one record per line 
scala&gt; val auth = spark.read.json(file_path).as[Auth] 
auth: org.apache.spark.sql.Dataset[Auth] = [books: array&lt;string&gt;, firstName: string ... 1 more field] 
 
//Look at the data 
scala&gt; auth.show() 
+--------------------+----------+---------+ 
|               books|first_name|last_name| 
+--------------------+----------+---------+ 
|                null|      Mark|    Twain| 
|                null|   Charles|  Dickens| 
|[Jude the Obscure...|    Thomas|    Hardy| 
+--------------------+----------+---------+ 
 
//Try explode to see array contents on separate lines 
 
scala&gt; auth.select(explode($"books") as "book", 
            $"first_name",$"last_name").show(2,false) 
+------------------------+----------+---------+ 
|book                    |first_name|last_name| 
+------------------------+----------+---------+ 
|Jude the Obscure        |Thomas    |Hardy    | 
|The Return of the Native|Thomas    |Hardy    | 
+------------------------+----------+---------+ 
</pre></div></div><div class="calibre2" title="Datasets API's limitations"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch04lvl2sec39" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Datasets API's limitations</h2></div></div></div><p class="calibre11">Even though the Datasets API is created using the best of both RDDs and DataFrames, it still has some limitations as of its current stage of development:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">While querying the dataset, the selected fields should be given specific data types as in the case class, or else the output will become a DataFrame. An example is <code class="literal">auth.select(col("first_name").as[String])</code>.</li><li class="listitem">Python and R are inherently dynamic in nature, and hence typed Datasets do not fit in.</li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Spark SQL"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec29" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Spark SQL</h1></div></div></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Spark SQL</strong></span> is a Spark module for structured data processing that was introduced in Spark 1.0. This module is a tightly integrated relational engine that inert-operates with the core Spark API. It enables data engineers to write applications that load structured data from disparate sources and join them as a unified, and possibly continuous, Excel-like data frames; and then they can implement complex ETL workflows and advanced analytics.</p><p class="calibre11">The Spark 2.0 release brought in significant unification of APIs and expanded the SQL capabilities, including support for subqueries. The Dataset API and DataFrames API are now unified, with DataFrames being a "kind" of Datasets. The unified APIs build the foundation for Spark's future, spanning across all libraries. Developers can impose "structure" onto their data and can work with high-level declarative APIs, thereby improving performance as well as their productivity. The performance gains come as a result of the underlying optimization layer. DataFrames, Datasets, and SQL share the same optimization and execution pipeline.</p><div class="calibre2" title="SQL operations"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch04lvl2sec40" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>SQL operations</h2></div></div></div><p class="calibre11">SQL operations are most widely used constructs for data manipulation. Some of most used operations are, selecting all or some of the columns, filtering based on one or more conditions, sorting and grouping operations, and computing summary functions such as <code class="literal">average</code> on GroupedData. The  <code class="literal">JOIN</code> operations on multiple data sources and <code class="literal">set</code> operations such as <code class="literal">union</code>, <code class="literal">intersect</code> and <code class="literal">minus</code> are some other operations that are widely performed. Furthermore, data frames are registered as temporary tables and passed traditional SQL statements to perform the aforementioned operations. <span class="strong"><strong class="calibre19">User-Defined Functions</strong></span> (<span class="strong"><strong class="calibre19">UDF</strong></span>) are defined and used with and without registration. We'll be focusing on window operations, which have been just introduced in Spark 2.0. They address sliding window operations. For example, if you want to report the average peak temperature every day in the past seven days, then you are operating on a sliding window of seven days until today. Here is an example that computes average sales per month for the past three months. The data file contains 24 observations showing monthly sales for two products, P1 and P2.</p><p class="calibre11">Example 5-window example with moving average computation</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><pre class="programlisting">scala&gt; import org.apache.spark.sql.expressions.Window 
import org.apache.spark.sql.expressions.Window 
//Create a DataFrame containing monthly sales data for two products 
scala&gt; val monthlySales = spark.read.options(Map({"header"-&gt;"true"},{"inferSchema" -&gt; "true"})). 
                            csv("&lt;Your Path&gt;/MonthlySales.csv") 
monthlySales: org.apache.spark.sql.DataFrame = [Product: string, Month: int ... 1 more field] 
 
//Prepare WindowSpec to create a 3 month sliding window for a product 
//Negative subscript denotes rows above current row 
scala&gt; val w = Window.partitionBy(monthlySales("Product")).orderBy(monthlySales("Month")).rangeBetween(-2,0) 
w: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3cc2f15 
 
//Define compute on the sliding window, a moving average in this case 
scala&gt; val f = avg(monthlySales("Sales")).over(w) 
f: org.apache.spark.sql.Column = avg(Sales) OVER (PARTITION BY Product ORDER BY Month ASC RANGE BETWEEN 2 PRECEDING AND CURRENT ROW) 
//Apply the sliding window and compute. Examine the results 
scala&gt; monthlySales.select($"Product",$"Sales",$"Month", bround(f,2).alias("MovingAvg")). 
                    orderBy($"Product",$"Month").show(6) 
+-------+-----+-----+---------+                                                  
|Product|Sales|Month|MovingAvg| 
+-------+-----+-----+---------+ 
|     P1|   66|    1|     66.0| 
|     P1|   24|    2|     45.0| 
|     P1|   54|    3|     48.0| 
|     P1|    0|    4|     26.0| 
|     P1|   56|    5|    36.67| 
|     P1|   34|    6|     30.0| 
+-------+-----+-----+---------+ 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python:</strong></span>
</p><pre class="programlisting">    &gt;&gt;&gt; from pyspark.sql import Window
    &gt;&gt;&gt; import pyspark.sql.functions as func
    //Create a DataFrame containing monthly sales data for two products
    &gt;&gt; file_path = &lt;Your path&gt;/MonthlySales.csv"
    &gt;&gt;&gt; monthlySales = spark.read.csv(file_path,header=True, inferSchema=True)
    
    //Prepare WindowSpec to create a 3 month sliding window for a product
    //Negative subscript denotes rows above current row
    &gt;&gt;&gt; w = Window.partitionBy(monthlySales["Product"]).orderBy(monthlySales["Month"]).rangeBetween(-2,0)
    &gt;&gt;&gt; w
    &lt;pyspark.sql.window.WindowSpec object at 0x7fdc33774a50&gt;
    &gt;&gt;&gt;
    //Define compute on the sliding window, a moving average in this case
    &gt;&gt;&gt; f = func.avg(monthlySales["Sales"]).over(w)
    &gt;&gt;&gt; f
    Column&lt;avg(Sales) OVER (PARTITION BY Product ORDER BY Month ASC RANGE BETWEEN 2 PRECEDING AND CURRENT ROW)&gt;
    &gt;&gt;&gt;
    //Apply the sliding window and compute. Examine the results
    &gt;&gt;&gt; monthlySales.select(monthlySales.Product,monthlySales.Sales,monthlySales.Month,
                          func.bround(f,2).alias("MovingAvg")).orderBy(
                          monthlySales.Product,monthlySales.Month).show(6)
    +-------+-----+-----+---------+                                                 
    |Product|Sales|Month|MovingAvg|
    +-------+-----+-----+---------+
    |     P1|   66|    1|     66.0|
    |     P1|   24|    2|     45.0|
    |     P1|   54|    3|     48.0|
    |     P1|    0|    4|     26.0|
    |     P1|   56|    5|    36.67|
    |     P1|   34|    6|     30.0|
    +-------+-----+-----+---------+
</pre></div><div class="calibre2" title="Under the hood"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch04lvl2sec41" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Under the hood</h2></div></div></div><p class="calibre11">When a developer is writing programs using RDD API, efficient execution for the workload on hand is his/her responsibility. The data types and computations are not available for Spark. In contrast, when a developer is using DataFrames and Spark SQL, the underlying engine has information about the schema and operations. In this case, the developer can write less code while the optimizer does all the hard work.</p><p class="calibre11">The Catalyst optimizer contains libraries for representing trees and applying rules to transform the trees. These tree transformations are applied to create the most optimized logical and physical execution plans. In the final phase, it generates Java bytecode using a special feature of the Scala language called <span class="strong"><strong class="calibre19">quasiquotes</strong></span>. The optimizer also enables external developers to extend the optimizer by adding data-source-specific rules that result in pushing operations to external systems, or support for new data types.</p><p class="calibre11">The Catalyst optimizer arrives at the most optimized plan to execute the operations on hand. The actual execution and related improvements are provided by the Tungsten engine. The goal of Tungsten is to improve the memory and CPU efficiency of Spark backend execution. The following are some salient features of this engine:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Reducing the memory footprint and eliminating garbage collection overheads by bypassing (off-heap) Java memory management.</li><li class="listitem">Code generation fuses across multiple operators and too many virtual function calls are avoided. The generated code looks like hand-optimized code.</li><li class="listitem">Memory layout is in columnar, in-memory parquet format because that enables vectorized processing and is also closer to usual data access operations.</li><li class="listitem">In-memory encoding using encoders. Encoders use runtime code generation to build custom byte code for faster and compact serialization and deserialization. Many operations can be performed in-place without deserialization because they are already in Tungsten binary format.</li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Structured Streaming"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec30" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Structured Streaming</h1></div></div></div><p class="calibre11">Streaming is a seemingly broad topic! If you take a closer look at the real-world problems, businesses do not just want a streaming engine to make decisions in real time. There has always been a need to integrate both batch stack and streaming stack, and integrate with external storage systems and applications. Also, the solution should be such that it should adapt to dynamic changes in business logic to address new and changing business requirements.</p><p class="calibre11">Apache Spark 2.0 has the first version of the higher level stream processing API called the <span class="strong"><strong class="calibre19">Structured Streaming</strong></span> engine. This scalable and fault-tolerant engine leans on the Spark SQL API to simplify the development of real-time, continuous big data applications. It is probably the first successful attempt in unifying the batch and streaming computation.</p><p class="calibre11">At a technical level, Structured Streaming leans on the Spark SQL API, which extends DataFrames/Datasets, which we already discussed in the previous sections. Spark 2.0 lets you perform radically different activities in a unified way, such as:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Building ML models and applying them on streaming data</li><li class="listitem">Combining streaming data with other static data</li><li class="listitem">Performing ad hoc, interactive, and batch queries</li><li class="listitem">Changing queries at runtime</li><li class="listitem">Aggregating data streams and serving using Spark SQL JDBC</li></ul></div><p class="calibre11">Unlike other streaming engines, Spark lets you combine real-time <span class="strong"><strong class="calibre19">Streaming Data</strong></span> with <span class="strong"><strong class="calibre19">Static data</strong></span> and lets you perform the preceding operations.</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_04_002.jpg" alt="Structured Streaming" class="calibre37"/></div><p class="calibre11">
</p><p class="calibre11">Fundamentally, Structured Streaming is empowered by Spark SQL's Catalyst optimizer. So, it frees up the developers from worrying about the underlying plumbing of making queries more efficient while dealing with static or real-time streams of data.</p><p class="calibre11">As of this writing, Structured Streaming of Spark 2.0 is focused on ETL, and later versions will have more operators and libraries.</p><p class="calibre11">Let us look at a simple example. The following example listens to <span class="strong"><strong class="calibre19">System Activity Report</strong></span> (<span class="strong"><strong class="calibre19">sar</strong></span>) on Linux on a local machine and computes the average free memory. System Activity Report gives system activity statistics and the current example collects memory usage, reported 20 times at a 2-second interval. The Spark stream reads this streaming output and computes average memory. We use a handy networking utility <span class="strong"><strong class="calibre19">netcat</strong></span> (<span class="strong"><strong class="calibre19">nc</strong></span>) to redirect the <code class="literal">sar</code> output onto a given port. The options <code class="literal">l</code> and <code class="literal">k</code> specify that <code class="literal">nc</code> should listen for an incoming connection and it has to keep listening for another connection even after its current connection is completed.</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Scala:</strong></span>
</p><p class="calibre11">Example 6-Streaming example</p><pre class="programlisting">//Run the following command from one terminal window 
sar -r 2 20 | nc -lk 9999 
 
//In spark-shell window, do the following 
//Read stream 
scala&gt; val myStream = spark.readStream.format("socket"). 
                       option("host","localhost"). 
                       option("port",9999).load() 
myStream: org.apache.spark.sql.DataFrame = [value: string] 
 
//Filter out unwanted lines and then extract free memory part as a float 
//Drop missing values, if any 
scala&gt; val myDF = myStream.filter($"value".contains("IST")). 
               select(substring($"value",15,9).cast("float").as("memFree")). 
               na.drop().select($"memFree") 
myDF: org.apache.spark.sql.DataFrame = [memFree: float] 
 
//Define an aggregate function 
scala&gt; val avgMemFree = myDF.select(avg("memFree")) 
avgMemFree: org.apache.spark.sql.DataFrame = [avg(memFree): double] 
 
//Create StreamingQuery handle that writes on to the console 
scala&gt; val query = avgMemFree.writeStream. 
          outputMode("complete"). 
          format("console"). 
          start() 
query: org.apache.spark.sql.streaming.StreamingQuery = Streaming Query - query-0 [state = ACTIVE] 
 
Batch: 0 
------------------------------------------- 
+-----------------+ 
|     avg(memFree)| 
+-----------------+ 
|4116531.380952381| 
+-----------------+ 
.... 
</pre><p class="calibre11">
<span class="strong"><strong class="calibre19">Python:</strong></span>
</p><pre class="programlisting">    //Run the following command from one terminal window
     sar -r 2 20 | nc -lk 9999
    
    //In another window, open pyspark shell and do the following
    &gt;&gt;&gt; import pyspark.sql.functions as func
    //Read stream
    &gt;&gt;&gt; myStream = spark.readStream.format("socket"). \
                           option("host","localhost"). \
                           option("port",9999).load()
    myStream: org.apache.spark.sql.DataFrame = [value: string]
    
    //Filter out unwanted lines and then extract free memory part as a float
    //Drop missing values, if any
    &gt;&gt;&gt; myDF = myStream.filter("value rlike 'IST'"). \
               select(func.substring("value",15,9).cast("float"). \
               alias("memFree")).na.drop().select("memFree")
    
    //Define an aggregate function
    &gt;&gt;&gt; avgMemFree = myDF.select(func.avg("memFree"))
    
    //Create StreamingQuery handle that writes on to the console
    &gt;&gt;&gt; query = avgMemFree.writeStream. \
              outputMode("complete"). \
              format("console"). \
              start()
    Batch: 0
    -------------------------------------------
    +------------+
    |avg(memFree)|
    +------------+
    |   4042749.2|
    +------------+
    .....
</pre><p class="calibre11">The preceding example defined a continuous data frame (also known as stream) to listen to a particular port, perform some transformations, and aggregations and show continuous output.</p><div class="calibre2" title="The Spark streaming programming model"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch04lvl2sec42" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The Spark streaming programming model</h2></div></div></div><p class="calibre11">As demonstrated earlier in this chapter, there is just a single API to take care of both static and streaming data. The idea is to treat the real-time data stream as a table that is continuously being appended, as shown in the following figure:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_04_003.jpg" alt="The Spark streaming programming model" class="calibre38"/></div><p class="calibre11">
</p><p class="calibre11">So whether for static or streaming data, you just fire up the batch-like queries as you would do on static data tables, and Spark runs it as an incremental query on the unbounded input table, as shown in the following figure:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_04_004.jpg" alt="The Spark streaming programming model" class="calibre39"/></div><p class="calibre11">
</p><p class="calibre11">So, the developers define a query on the input table, in the same way for both static-bounded as well as dynamic-unbounded table. Let us understand the various technical jargons for this whole process to understand how it works:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Input:</strong></span> Data from sources as an append-only table</li><li class="listitem"><span class="strong"><strong class="calibre19">Trigger:</strong></span> When to check the input for new data</li><li class="listitem"><span class="strong"><strong class="calibre19">Query:</strong></span> What operation to perform on the data, such as filter, group, and so on</li><li class="listitem"><span class="strong"><strong class="calibre19">Result:</strong></span> The resultant table at every trigger interval</li><li class="listitem"><span class="strong"><strong class="calibre19">Output:</strong></span> Choose what part of the result to write to the data sink after every trigger</li></ul></div><p class="calibre11">Let's now look at how the Spark SQL planner treats the whole process:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_04_005.jpg" alt="The Spark streaming programming model" class="calibre40"/><div class="caption">Courtesy: Databricks</div></div><p class="calibre11">
</p><p class="calibre11">The preceding screenshot is very simply explained in the structured programming guide at the official Apache Spark site, as indicated in the <span class="strong"><em class="calibre22">References</em></span> section.</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_04_006.jpg" alt="The Spark streaming programming model" class="calibre41"/></div><p class="calibre11">
</p><p class="calibre11">At this point, we need to know about the supported output models. Every time the result table is updated, the changes need to be written to an external system, such as HDFS, S3, or any other database. We usually prefer to write output incrementally. For this purpose, Structured Streaming provides three output modes:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Append:</strong></span> In the external storage, only the new rows appended to the result table since the last trigger will be written. This is applicable only on queries where existing rows in the result table cannot change (for example, a map on an input stream).</li><li class="listitem"><span class="strong"><strong class="calibre19">Complete:</strong></span> In the external storage, the entire updated result table will be written as is.</li><li class="listitem"><span class="strong"><strong class="calibre19">Update:</strong></span> In the external storage, only the rows that were updated in the result table since the last trigger will be changed. This mode works for output sinks that can be updated in place, such as a MySQL table.</li></ul></div><p class="calibre11">In our example, we used complete mode, which was straightaway writing to the console. You may want to write into some external file such as Parquet to get a better understanding.</p></div><div class="calibre2" title="Under the hood"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch04lvl2sec43" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Under the hood</h2></div></div></div><p class="calibre11">If you look at the "behind the screen" execution mechanism of the operations performed on <span class="strong"><strong class="calibre19">DataFrames/Datasets</strong></span>, it would appear as the following figure suggests:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_04_007.jpg" alt="Under the hood" class="calibre42"/></div><p class="calibre11">
</p><p class="calibre11">Please note here that the <span class="strong"><strong class="calibre19">Planner</strong></span> knows apriori how to convert a streaming <span class="strong"><strong class="calibre19">Logical Plan</strong></span> to a continuous series of <span class="strong"><strong class="calibre19">Incremental Execution Plans</strong></span>. This can be represented by the following figure:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_04_008.jpg" alt="Under the hood" class="calibre43"/></div><p class="calibre11">
</p><p class="calibre11">The <span class="strong"><strong class="calibre19">Planner</strong></span> can poll the data sources for new data to be able to plan the execution in an optimized way.</p></div><div class="calibre2" title="Comparison with other streaming engines"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch04lvl2sec44" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Comparison with other streaming engines</h2></div></div></div><p class="calibre11">We have discussed many unique features of Structured Streaming. Let us now have a comparative view with other available streaming engines:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_04_009.jpg" alt="Comparison with other streaming engines" class="calibre44"/><div class="caption">Courtesy: Databricks</div></div><p class="calibre11">
</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Continuous applications"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec31" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Continuous applications</h1></div></div></div><p class="calibre11">We discussed how unified data access is empowered by Spark. It lets you process data in a myriad of ways to build end-to-end continuous applications by enabling various analytic workloads, such as ETL processing, ad hoc queries, online machine learning modeling, or to generate necessary reports... all of this in a unified way by letting you work on both static as well as streaming data using a high-level, SQL-like API. In this way, Structured Streaming has substantially simplified the development and maintenance of real-time, continuous applications.</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_04_010.jpg" alt="Continuous applications" class="calibre45"/><div class="caption">Courtesy: Databricks</div></div><p class="calibre11">
</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec32" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we discussed what is really meant by unified data access and how Spark serves this purpose. We took a closer look at the Datasets API and how real-time streaming is empowered through it. We learned the advantages of Datasets and also their limitations. We also looked at the fundamentals behind continuous applications.</p><p class="calibre11">In the following chapter, we will look at the various ways in which we can leverage the Spark platform for data analysis operations at scale.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec33" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf">http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf</a> : Spark SQL: Relational Data Processing in Spark</li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html">https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html</a> : A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets - When to use them and why</li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html">https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html</a> : Introducing Apache Spark Datasets</li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html">https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html</a> : Deep Dive into Spark SQL's Catalyst Optimizer</li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html">https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html</a> : Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop</li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html">https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html</a> : Bringing Spark closer to baremetal</li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html">https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html</a> : Structured Streaming API details</li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</a> : Spark Structured Streaming Programming Guide</li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/">https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/</a>: Structuring Apache Spark SQL, DataFrames, Datasets, and Streaming by Michael Armbrust</li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html">https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html</a>: Apache Spark Key terms explained</li></ul></div></div></div>



  </body></html>