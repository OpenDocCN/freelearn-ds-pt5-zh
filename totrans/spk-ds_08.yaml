- en: Chapter 8.  Analyzing Unstructured Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this Big Data era, the proliferation of unstructured data is overwhelming.
    Numerous methods such as data mining, **Natural Language Processing** (**NLP**),
    information retrieval, and so on, exist for analyzing unstructured data. Due to
    the rapid growth of unstructured data in all kinds of businesses, scalable solutions
    have become the need of the hour. Apache Spark is equipped with out of the box
    algorithms for text analytics, and it also supports custom development of algorithms
    that are not available by default.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter we have shown how SparkR, an R API to Spark for R programmers
    can harness the power of Spark, without learning a new language .  In this chapter,
    we are going to step into a whole new dimension and explore algorithms and techniques
    to extract information out of unstructured data by leveraging Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a prerequisite for this chapter, a basic understanding of programming in
    Python or Scala and an overall understanding of text analytics and machine learning
    are nice to have. However, we have covered some theoretical basics with the right
    set of practical examples to make those more comprehendible and easy to implement.
    The topics covered in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: Sources of unstructured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing unstructured data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count vectorizer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop-word removal
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization/scaling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: n-gram modeling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes classifier
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sources of unstructured data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data analytics has come very far since the spreadsheets and the BI tools in
    the eighties and nineties. Tremendous improvements in computing power, sophisticated
    algorithms, and an open source culture fueled unprecedented growth in data analytics,
    as well as in other fields. These advances in technologies paved the way for new
    opportunities and new challenges. Businesses started looking at generating insights
    from hitherto impossible to handle data sources such as internal memos, emails,
    customer satisfaction surveys, and the like. Data analytics now encompass this
    unstructured, usually text based data along with traditional rows and columns
    of data. Between the highly structured data stored in RDBMS table and completely
    unstructured plain text, we have semi-structured data sources in NoSQL data stores,
    XML or JSON documents, and graph or network data sources. As per current estimates,
    unstructured data forms about 80 percent of enterprise data and is growing rapidly.
    Satellite images, atmospheric data, social networks, blogs and other web pages,
    patient records and physicians' notes, companies' internal communications, and
    so on - all these combined are just a subset of unstructured data sources.
  prefs: []
  type: TYPE_NORMAL
- en: We have already been seeing successful data products that leverage unstructured
    data along with structured data. Some of the companies leverage the power of social
    networks to provide actionable insights to their customers. New fields such as
    **Sentiment Analysis** and **Multimedia Analytics** are emerging to draw insights
    from unstructured data. However, analyzing unstructured data is still a daunting
    feat. For example, contemporary text analytics tools and techniques cannot identify
    sarcasm. However, the potential benefits undoubtedly outweigh the limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Processing unstructured data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unstructured data does not lend itself to most of the programming tasks. It
    has to be processed in various different ways as applicable, to be able to serve
    as an input to any machine learning algorithm or for visual analysis. Broadly,
    the unstructured data analysis can be viewed as a series of steps as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Processing unstructured data](img/image_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data pre-processing is the most vital step in any unstructured data analysis.
    Fortunately, there have been several proven techniques accumulated over time that
    come in handy. Spark offers most of these techniques out of the box through the
    `ml.features` package. Most of the techniques aim to convert text data to concise
    numerical vectors that can be easily consumed by machine learning algorithms.
    Developers should understand the specific requirements of their organizations
    to arrive at the best pre-processing workflow. Remember that better, relevant
    data is the key to generate better insights.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore a couple of examples that process raw text and convert them into
    data frames. First example takes some text as input and extracts all date-like
    strings whereas the second example extracts tags from twitter text. First example
    is just a warm-up, using a simple, regex (regular expression) tokenizer feature
    transformer without using any spark-specific libraries. It also draws your attention
    to the possibility of misinterpretation. For example, a product code of the form
    1-11-1111 may be interpreted as a date. The second example illustrates a non-trivial,
    multi-step extraction process that resulted in just the required tags. **User
    defined functions** (**udf**) and ML pipelines come in handy in developing such
    multi-step extraction processes. Remaining part of this section describes some
    more handy tools supplied out of box in apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example-1:** **Extract date like strings from text**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example defined a regular expression pattern to recognize date
    strings. The regex pattern and the sample text DataFrame are passed to the `RegexTokenizer`
    to extract matching, date like strings. The `gaps=False` option picks matching
    strings and a value of `False` would use the given pattern as a separator. Note
    that `1-21-1111`, which is obviously not a date, is also selected.
  prefs: []
  type: TYPE_NORMAL
- en: Next example extracts tags from twitter text and identifies most popular tags.
    You can use the same approach to collect hash (`#`) tags too.
  prefs: []
  type: TYPE_NORMAL
- en: This example uses a built in function `explode`, which converts a single row
    with an array of values into multiple rows, one value per array element.
  prefs: []
  type: TYPE_NORMAL
- en: "**Example-2: Extract tags from twitter \"\x80\x9Ctext\"\x80\x9D**"
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Count vectorizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Count vectorizer extracts vocabulary (tokens) from documents and generates a
    `CountVectorizerModel` model when a dictionary is not available priori. As the
    name indicates, a text document is converted into a vector of tokens and counts.
    The model produces a sparse representation of the documents over the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: You can fine tune the behavior to limit the vocabulary size, minimum token count,
    and much more as applicable in your business case.
  prefs: []
  type: TYPE_NORMAL
- en: '//Example 3: Count Vectorizer example'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Input**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Output**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example demonstrates how `CountVectorizer` works as an estimator
    to extract the vocabulary and generate a `CountVectorizerModel`. Note that the
    features vector order corresponds to vocabulary and not the input sequence. Let's
    also look at how the same can be achieved by building a dictionary a-priori. However,
    keep in mind that they have their own use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 4: define CountVectorizerModel with a-priori vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: Not available as of Spark 2.0.0
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Term Frequency-Inverse Document Frequency** (**TF-IDF**) is perhaps one
    of the most popular measures in text analytics. This metric indicates the importance
    of a given term in a given document within a set of documents. This consists two
    measurements, **Term Frequency** (**TF**) and **Inverse Document Frequency** (**IDF**).
    Let us discuss them one by one and then see their combined effect.
  prefs: []
  type: TYPE_NORMAL
- en: TF is a measure of the relative importance of a term in a document, which is
    usually the frequency of that term divided by the number of terms in that document.
    Consider a text document containing 100 words wherein the word *apple* appears
    eight times. The TF for *apple* would be *TF = (8 / 100) = 0.08*. So, the more
    frequently a term occurs in a document, the larger is its TF coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: IDF is a measure of the importance of a particular term in the entire collection
    of documents, that is, how infrequently the word occurs across all the documents.
    The importance of a term is inversely proportional to its frequency. Spark provides
    two separate methods to perform these tasks. Assume we have 6 million documents
    and the word *apple* appears in 6000 of these. Then, IDF is calculated as *IDF
    = Log(6,000,000 / 6,000) = 3*. If you observe this carefully, the lower the denominator,
    the higher is the IDF value. This means that the fewer the number of documents
    containing a particular word, the higher would be its importance.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the TF-IDF score would be *TF * IDF = 0.08 * 3 = 0.24*. Note that it would
    penalize the words that are more frequent across documents and less important,
    such as *the*, *this*, *a*, and so on, and give more weight to the ones that are
    important.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark, TF is implemented as HashingTF. It takes a sequence of terms (often
    the output of a tokenizer) and produces a fixed length features vector. It performs
    feature hashing to convert the terms into fixed length indices. IDF then takes
    that features vector (the output of HashingTF) as input and scales it based on
    the term frequency in the set of documents. The previous chapter has an example
    of this transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Stop-word removal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Common words such as *is*, *was*, and *the* are called stop-words. They do not
    usually add value to analysis and should be dropped during the data preparation
    step. Spark provides `StopWordsRemover` transformer, which does just that. It
    takes a sequence of tokens as a series of string inputs, such as the output of
    a tokenizer, and removes all the stop words. Spark has a stop-words list by default
    that you may override by providing your own stop-words list as a parameter. You
    may optionally turn on `caseSensitive` match which is off by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 5: Stopword Remover'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Assume that we have the following DataFrame with columns `id` and `raw_text`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying `StopWordsRemover` with `raw_text` as the input column and `processed_text`
    as the output column for the preceding example, we should get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Normalization/scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normalization is a common and preliminary step in data preparation. Most of
    the machine learning algorithms work better when all features are on the same
    scale. For example, if there are two features where the value of one is about
    100 times greater than the other, bringing them to the same scale reflects meaningful
    relative activity between the two variables. Any non-numeric values, such as high,
    medium, and low, should ideally be converted to appropriate numerical quantification
    as a best practice. However, you need to be careful in doing so as it may require
    domain expertise. For example, if you assign 3, 2, and 1 for high, medium, and
    low respectively, then it should be checked that these three units are equidistant
    from each other.
  prefs: []
  type: TYPE_NORMAL
- en: The common methods of feature normalization are *scaling*, *mean subtraction*,
    and *feature standardization*, just to name a few. In scaling, each numerical
    feature vector is rescaled such that its value range is between *-1* to *+1* or
    *0* to *1* or something similar. In mean subtraction, you compute mean of a numerical
    feature vector and subtract that mean from each of the values. We are interested
    in the relative deflection from the mean, while the absolute value could be immaterial.
    Feature standardization refers to setting the data to zero mean and unit (1) variance.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides a `Normalizer` feature transformer to normalize each vector to
    have unit norm; `StandardScaler` to have unit norm and zero mean; and `MinMaxScaler`
    to rescale each feature to a specific range of values. By default, min and max
    are 0 and 1 but you may set the value parameters yourself as per the data requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Word2Vec is a type of PCA (you will find out more about this shortly) that
    takes a sequence of words and produces a map (of string, vector). The string is
    the word and the vector is a unique fixed size vector. The resulting word vector
    representation is useful in many machine learning and NLP applications, such as
    named entity recognition and tagging. Let us look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 6: Word2Vec**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: n-gram modelling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An n-gram is a contiguous sequence of *n* items from a given sequence of text
    or speech. An n-gram of size *1* is referred to as a *unigram*, size *2* is a
    *bigram*, and size *3* is a *trigram*. Alternatively, they can be referred to
    by the value of *n*, for example, four-gram, five-gram, and so on. Let us take
    a look at an example to understand the possible outcomes of this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an example of words to n-gram letters. The same is the case for sentence
    (or tokenized words) to n-gram words. For example, the 2-gram equivalent of the
    sentence *Kids love to eat chocolates* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '''Kids love'', ''love to'', ''to eat'', ''eat chocolates''.'
  prefs: []
  type: TYPE_NORMAL
- en: There are various applications of n-gram modelling in text mining and NLP. One
    of the examples is predicting the probability of each word occurring given a prior
    context (conditional probability).
  prefs: []
  type: TYPE_NORMAL
- en: In Spark, `NGram` is a feature transformer that converts the input array (for
    example, the output of a Tokenizer) of strings into an array of n-grams. Null
    values in the input array are ignored by default. It returns an array of n-grams
    where each n-gram is represented by a space-separated string of words.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 7: NGram**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text classification is about assigning a topic, subject category, genre, or
    something similar to the text blob. For example, spam filters assign spam or not
    spam to an email.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark supports various classifiers through MLlib and ML packages. The
    SVM classifier and Naive Bayes classifier are popular classifiers, and the former
    was already covered in the previous chapter. Let's take a look at the latter now.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Naive Bayes** (**NB**) classifier is a multiclass probabilistic classifier
    and is one of the best classification algorithms. It assumes strong independence
    between every pair of features. It computes the conditional probability distribution
    of each feature and a given label, and then applies Bayes' theorem to compute
    the conditional probability of a label given an observation. In terms of document
    classification, an observation is a document to be classified into some class.
    Despite its strong assumptions on data, it is quite popular. It works with small
    amount of training data - whether real or discrete. It works very efficiently
    because it takes a single pass through the training data; one constraint is that
    the feature vectors must be non-negative. By default, ML package supports multinomial
    NB. However, you may set the parameter `modelType` to `Bernoulli` if bernoulli
    NB is required.
  prefs: []
  type: TYPE_NORMAL
- en: The **laplace smoothing** technique may be applied by specifying the smoothing
    parameters and is extremely useful in situations where you want to assign a small
    non-zero probability to a rare word or new word so that the posterior probabilities
    do not suddenly drop to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark also provides some other hyper parameters such as `thresholds` also to
    gain fine grain control. Here is an example that categorizes twitter text. This
    example contains some hand-coded rules that assign a category to the train data.
    A particular category is assigned if any of the corresponding words are found
    in the text. For example, the category is "survey" if text contains "survey" or
    "poll". The model is trained based on this train data and evaluated on a different
    text sample collected at a different time:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 8: Naive Bayes**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Once this is done, a model can be trained with the output of this step, which
    can classify a text blob or file.
  prefs: []
  type: TYPE_NORMAL
- en: Text clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is an unsupervised learning technique. Intuitively, clustering groups
    objects into disjoint sets. We do not know how many groups exist in the data,
    or what might be the commonality within these groups (clusters).
  prefs: []
  type: TYPE_NORMAL
- en: Text clustering has several applications. For example, an organizational entity
    may want to organize its internal documents into similar clusters based on some
    similarity measure. The notion of similarity or distance is central to the clustering
    process. Common measures used are TF-IDF and cosine similarity. Cosine similarity,
    or the cosine distance, is the cos product of the word frequency vectors of two
    documents. Spark provides a variety of clustering algorithms that can be effectively used
    in text analytics.
  prefs: []
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Perhaps K-means is the most intuitive of all the clustering algorithms. The
    idea is to segregate data points as *K* different clusters based on some similarity
    measure, say cosine distance or Euclidean distance. This algorithm that starts
    with *K* random single point clusters, and each of the remaining data points are
    assigned to nearest cluster. Then cluster centers are recomputed and the algorithm
    loops through the data points once again. This process continues iteratively until
    there are no re-assignments or when pre-defined iteration count is reached.
  prefs: []
  type: TYPE_NORMAL
- en: How to fix the number of clusters (*K*) is not obvious. Identifying the initial
    cluster centers is also not obvious. Sometimes the business requirement may dictate
    the number of clusters; for example, partition all existing documents into 10
    different sections. But in most of the real world scenarios, we need to find *K*
    through trial and error. One way is to progressively increase the *K* value and
    compute the cluster quality, such as cluster variance. The quality ceases to improve
    significantly beyond a certain value of *K,* which could be your ideal *K*. There
    are various other techniques, such as the elbow method, **Akaike information criterion**
    (**AIC**), and **Bayesian information criterion** (**BIC**).
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, start with different starting points until the cluster quality is
    satisfactory. Then you may wish to validate your result using techniques such
    as Silhouette Score. However, these activities are computationally intensive.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides K-means from MLlib as well as ml packages. You may specify maximum
    iterations or convergence tolerance to fine tune algorithm performance.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a large matrix with many rows and columns. In many matrix applications,
    this large matrix can be represented by some narrow matrices with small number
    of rows and columns that still represents the original matrix. Then processing
    this smaller matrix may yield similar results as that of the original matrix.
    This can be computationally efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction is about finding that small matrix. MLLib supports
    two algorithms, SVD and PCA for dimensionality reduction on RowMatrix class. Both
    of these  algorithms allow us to specify the number of dimensions we are interested
    in retaining. Let us look at example first and then delve into the underlying
    theory .
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 9: Dimensionality reduction**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: Not available in Python as of Spark 2.0.0
  prefs: []
  type: TYPE_NORMAL
- en: Singular Value Decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Singular Value Decomposition** (**SVD**) is one of the centerpieces of
    linear algebra and is widely used for many real-world modeling requirements. It
    provides a convenient way of breaking a matrix into simpler, smaller matrices.
    This leads to a low-dimensional representation of a high-dimensional matrix. It
    helps us eliminate less important parts of the matrix to produce an approximate
    representation. This technique is useful in dimensionality reduction and data
    compression.
  prefs: []
  type: TYPE_NORMAL
- en: Let *M* be a matrix of size m-rows and n-columns. The rank of a matrix is the
    number of rows that are linearly independent. A row is considered independent
    if it has at least one non-zero element and it is not a linear combination of
    one or more rows. The same rank will be obtained if we considered columns instead
    of rows - as in linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the elements of one row are the sum of two rows, then that row is not independent.
    Then as a result of SVD, we find three matrices, *U*, *∑*, and *V* that satisfy
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*M = U∑VT*'
  prefs: []
  type: TYPE_NORMAL
- en: 'These three matrices have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**U**: This is a column-orthonormal matrix with m rows and r columns. An orthonormal
    matrix implies that each of the columns is a unit vector and the pairwise dot
    product between any two columns is 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V**: This is a column-orthonormal matrix with *n* rows and *r* columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**∑**: This is an *r* x *r* diagonal matrix with non-negative real numbers
    as principal diagonal values in descending order. In a diagonal matrix, all elements
    except the ones on the principal diagonal are zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The principal diagonal values in the *∑* matrix are called singular values.
    They are considered as the underlying *concepts* or *components* that connect
    the rows and columns of the matrix. Their magnitude represents the strength of
    the corresponding components. For example, imagine that the matrix in the previous
    example contains ratings of five books by six readers. SVD allows us to split
    them into three matrices: *∑* containing the singular values representing the
    *strength* of underlying topics; *U* connecting people to concepts; and *V* connecting
    concepts to books.'
  prefs: []
  type: TYPE_NORMAL
- en: In a large matrix, we can replace the lower magnitude singular values to zero
    and thereby reduce the corresponding rows in the remaining two matrices. Note
    that if we re-compute the matrix product on the right hand side and compare the
    value with the original matrix on the left hand side, they will be almost similar.
    We can use this technique to retain the desired number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: "**Principal Component Analysis** (**PCA**) is a technique that takes n-dimensional\
    \ data points and project onto a smaller (fewer dimensions) subspace with minimum\
    \ loss of information. A set of data points in a high dimensional space find the\
    \ directions along which these tuples line up best. In other words, we need to\
    \ find a rotation such that the first coordinate has the largest variance possible,\
    \ and each succeeding coordinate in turn has the largest variance possible. The\
    \ idea is to treat the set of tuples as a matrix *M* and fi\x81nd the eigenvectors\
    \ for MMT."
  prefs: []
  type: TYPE_NORMAL
- en: If *A* is a square matrix, *e* is a column matrix with the same number of rows
    as *A*, and *λ* is a constant such that *Me = λe*, then *e* is called the eigenvector
    of *M* and *λ* is called the eigenvalue of *M*. In terms of n-dimensional plane,
    the eigenvector is the direction and the eigenvalue is a measure of variance along
    that direction. We can drop the dimensions with a low eigenvalue, thereby finding
    a smaller subspace without loss of information.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we examined the sources of unstructured data and the motivation
    behind analyzing the unstructured data. We explained various techniques that are
    required in pre-processing unstructured data and how Spark provides most of these
    tools out of the box. We also covered some of the algorithms supported by Spark
    that can be used in text analytics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go through different types of visualization techniques
    that are insightful in different stages of data analytics lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf](http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf](https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://web.stanford.edu/class/cs124/lec/naivebayes.pdf](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html](http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.mmds.org/](http://www.mmds.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://sebastianraschka.com/Articles/2014_pca_step_by_step.html](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://arxiv.org/pdf/1404.1100.pdf](http://arxiv.org/pdf/1404.1100.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Count Vectorizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'n-gram modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
