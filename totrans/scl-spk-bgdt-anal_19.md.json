["```py\necho \"export SPARK_HOME=/home/asif/Spark\" >> ~/.bashrc\n\n```", "```py\necho \"export PYTHONPATH=$SPARK_HOME/python/\" >> ~/.bashrc\necho \"export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.1-src.zip\" >> ~/.bashrc\n\n```", "```py\necho \"export PATH=$PATH:$SPARK_HOME\" >> ~/.bashrc\necho \"export PATH=$PATH:$PYTHONPATH\" >> ~/.bashrc\n\n```", "```py\nsource ~/.bashrc\n\n```", "```py\n$ sudo pip install py4j\n\n```", "```py\n$ cd $SPARK_HOME\n$ ./bin/pyspark\n\n```", "```py\n$ python <python_file.py>\n\n```", "```py\n$ cd $SPARK_HOME\n$ ./bin/spark-submit  --master local[*] <python_file.py>\n\n```", "```py\n$ sudo pip install pyspark # for python 2.7 \n$ sudo pip3 install pyspark # for python 3.3+\n\n```", "```py\nimport os\nimport sys\nimport pyspark\n\n```", "```py\ntry: \n    from pyspark.ml.featureimport PCA\n    from pyspark.ml.linalgimport Vectors\n    from pyspark.sqlimport SparkSession\n    print (\"Successfully imported Spark Modules\")\n\n```", "```py\nExceptImportErroras e: \n    print(\"Can not import Spark Modules\", e)\n    sys.exit(1)\n\n```", "```py\nspark = SparkSession\\\n         .builder\\\n         .appName(\"PCAExample\")\\\n         .getOrCreate()\n\n```", "```py\ndata = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n         (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n         (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n df = spark.createDataFrame(data, [\"features\"])\n\n pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n model = pca.fit(df)\n\n result = model.transform(df).select(\"pcaFeatures\")\n result.show(truncate=False)\n\n```", "```py\nimport os\nimport sys\n\ntry:\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import PCA\nfrom pyspark.ml.linalg import Vectors\nprint (\"Successfully imported Spark Modules\")\n\nexcept ImportErrorase:\nprint (\"Can not import Spark Modules\", e)\n sys.exit(1)\n\nspark = SparkSession\\\n   .builder\\\n   .appName(\"PCAExample\")\\\n   .getOrCreate()\n\ndata = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n    (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n    (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\ndf = spark.createDataFrame(data, [\"features\"])\n\npca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\nmodel = pca.fit(df)\n\nresult = model.transform(df).select(\"pcaFeatures\")\nresult.show(truncate=False)\n\nspark.stop()\n\n```", "```py\n# Creating DataFrame from libsvm dataset\n myDF = spark.read.format(\"libsvm\").load(\"C:/Exp//mnist.bz2\")\n\n```", "```py\nmyDF.show() \n\n```", "```py\nmyDF= spark.read.format(\"libsvm\")\n           .option(\"numFeatures\", \"780\")\n           .load(\"data/Letterdata_libsvm.data\")\n\n```", "```py\n*Creating RDD from the libsvm data file* myRDD = MLUtils.loadLibSVMFile(spark.sparkContext, \"data/Letterdata_libsvm.data\")\n\n```", "```py\nmyRDD.saveAsTextFile(\"data/myRDD\")\n\n```", "```py\n# Creating DataFrame from data file in CSV formatdf = spark.read.format(\"com.databricks.spark.csv\")\n          .option(\"header\", \"true\")\n          .load(\"data/nycflights13.csv\")\n\n```", "```py\ndf.printSchema() \n\n```", "```py\ndf.show() \n\n```", "```py\nnumber\\tproduct_name\\ttransaction_id\\twebsite\\tprice\\tdate0\\tjeans\\t30160906182001\\tebay.com\\t100\\t12-02-20161\\tcamera\\t70151231120504\\tamazon.com\\t450\\t09-08-20172\\tlaptop\\t90151231120504\\tebay.ie\\t1500\\t07--5-20163\\tbook\\t80151231120506\\tpackt.com\\t45\\t03-12-20164\\tdrone\\t8876531120508\\talibaba.com\\t120\\t01-05-2017\n\n```", "```py\nmyRDD = spark.sparkContext.textFile(\"sample_raw_file.txt\")\n$cd myRDD\n$ cat part-00000  \nnumber\\tproduct_name\\ttransaction_id\\twebsite\\tprice\\tdate  0\\tjeans\\t30160906182001\\tebay.com\\t100\\t12-02-20161\\tcamera\\t70151231120504\\tamazon.com\\t450\\t09-08-2017\n\n```", "```py\nheader = myRDD.first() \n\n```", "```py\ntextRDD = myRDD.filter(lambda line: line != header)\nnewRDD = textRDD.map(lambda k: k.split(\"\\\\t\"))\n\n```", "```py\n textDF = newRDD.toDF(header.split(\"\\\\t\"))\n textDF.show()\n\n```", "```py\ntextDF.createOrReplaceTempView(\"transactions\")\nspark.sql(\"SELECT *** FROM transactions\").show()\nspark.sql(\"SELECT product_name, price FROM transactions WHERE price >=500 \").show()\nspark.sql(\"SELECT product_name, price FROM transactions ORDER BY price DESC\").show()\n\n```", "```py\n# Let's generate somerandom lists\n students = ['Jason', 'John', 'Geroge', 'David']\n courses = ['Math', 'Science', 'Geography', 'History', 'IT', 'Statistics']\n\n```", "```py\nrawData = []\nfor (student, course) in itertools.product(students, courses):\n    rawData.append((student, course, random.randint(0, 200)))\n\n```", "```py\nimport itertools\nimport random\n\n```", "```py\nfrom pyspark.sql.types\nimport StructType, StructField, IntegerType, StringType\n\n```", "```py\nschema = StructType([StructField(\"Student\", StringType(), nullable=False),\n                     StructField(\"Course\", StringType(), nullable=False),\n                     StructField(\"Score\", IntegerType(), nullable=False)])\n\n```", "```py\ncourseRDD = spark.sparkContext.parallelize(rawData)\n\n```", "```py\ncourseDF = spark.createDataFrame(courseRDD, schema) \ncoursedDF.show() \n\n```", "```py\n# Define udfdef scoreToCategory(grade):\n if grade >= 90:\n return 'A'\n elif grade >= 80:\n return 'B'\n elif grade >= 60:\n return 'C'\n else:\n return 'D'\n\n```", "```py\nfrom pyspark.sql.functions\nimport udf\nudfScoreToCategory = udf(scoreToCategory, StringType())\n\n```", "```py\ncourseDF.withColumn(\"Grade\", udfScoreToCategory(\"Score\")).show(100)\n\n```", "```py\nspark.udf.register(\"udfScoreToCategory\", scoreToCategory, StringType()) \n\n```", "```py\ncourseDF.createOrReplaceTempView(\"score\")\n\n```", "```py\nspark.sql(\"SELECT Student, Score, udfScoreToCategory(Score) as Grade FROM score\").show() \n\n```", "```py\nimport os\nimport sys\nimport itertools\nimport random\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\nfrom pyspark.sql.functions import udf\n\nspark = SparkSession \\\n        .builder \\\n        .appName(\"PCAExample\") \\\n        .getOrCreate()\n\n# Generate Random RDD\nstudents = ['Jason', 'John', 'Geroge', 'David']\ncourses = ['Math', 'Science', 'Geography', 'History', 'IT', 'Statistics']\nrawData = []\nfor (student, course) in itertools.product(students, courses):\n    rawData.append((student, course, random.randint(0, 200)))\n\n# Create Schema Object\nschema = StructType([\n    StructField(\"Student\", StringType(), nullable=False),\n    StructField(\"Course\", StringType(), nullable=False),\n    StructField(\"Score\", IntegerType(), nullable=False)\n])\n\ncourseRDD = spark.sparkContext.parallelize(rawData)\ncourseDF = spark.createDataFrame(courseRDD, schema)\ncourseDF.show()\n\n# Define udf\ndef scoreToCategory(grade):\n    if grade >= 90:\n        return 'A'\n    elif grade >= 80:\n        return 'B'\n    elif grade >= 60:\n        return 'C'\n    else:\n        return 'D'\n\nudfScoreToCategory = udf(scoreToCategory, StringType())\ncourseDF.withColumn(\"Grade\", udfScoreToCategory(\"Score\")).show(100)\n\nspark.udf.register(\"udfScoreToCategory\", scoreToCategory, StringType())\ncourseDF.createOrReplaceTempView(\"score\")\nspark.sql(\"SELECT Student, Score, udfScoreToCategory(Score) as Grade FROM score\").show()\n\nspark.stop()\n\n```", "```py\nINPUT = \"C:/Users/rezkar/Downloads/kddcup.data\" spark = SparkSession\\\n         .builder\\\n         .appName(\"PCAExample\")\\\n         .getOrCreate()\n\n kddcup_data = spark.sparkContext.textFile(INPUT)\n\n```", "```py\ncount = kddcup_data.count()\nprint(count)>>4898431\n\n```", "```py\nkdd = kddcup_data.map(lambda l: l.split(\",\"))\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(spark)\ndf = sqlContext.createDataFrame(kdd)\n\n```", "```py\ndf.select(\"_1\", \"_2\", \"_3\", \"_4\", \"_42\").show(5)\n\n```", "```py\n#Identifying the labels for unsupervised tasklabels = kddcup_data.map(lambda line: line.strip().split(\",\")[-1])\nfrom time import time\nstart_label_count = time()\nlabel_counts = labels.countByValue()\nlabel_count_time = time()-start_label_count\n\nfrom collections import OrderedDict\nsorted_labels = OrderedDict(sorted(label_counts.items(), key=lambda t: t[1], reverse=True))\nfor label, count in sorted_labels.items():\n print label, count\n\n```", "```py\nfrom numpy import array\ndef parse_interaction(line):\n     line_split = line.split(\",\")\n     clean_line_split = [line_split[0]]+line_split[4:-1]\n     return (line_split[-1], array([float(x) for x in clean_line_split]))\n\n parsed_data = kddcup_data.map(parse_interaction)\n pd_values = parsed_data.values().cache()\n\n```", "```py\n kdd_train = pd_values.sample(False, .75, 12345)\n kdd_test = pd_values.sample(False, .25, 12345)\n print(\"Training set feature count: \" + str(kdd_train.count()))\n print(\"Test set feature count: \" + str(kdd_test.count()))\n\n```", "```py\nTraining set feature count: 3674823 Test set feature count: 1225499\n\n```", "```py\nstandardizer = StandardScaler(True, True) \n\n```", "```py\nstandardizer_model = standardizer.fit(kdd_train) \n\n```", "```py\ndata_for_cluster = standardizer_model.transform(kdd_train) \n\n```", "```py\nimport numpy\nour_k = numpy.arange(10, 31, 10)\nmetrics = []\ndef computeError(point):\n center = clusters.centers[clusters.predict(point)]\n denseCenter = DenseVector(numpy.ndarray.tolist(center))\nreturn sqrt(sum([x**2 for x in (DenseVector(point.toArray()) - denseCenter)]))\nfor k in our_k:\n      clusters = KMeans.train(data_for_cluster, k, maxIterations=4, initializationMode=\"random\")\n      WSSSE = data_for_cluster.map(lambda point: computeError(point)).reduce(lambda x, y: x + y)\n      results = (k, WSSSE)\n metrics.append(results)\nprint(metrics)\n\n```", "```py\n[(10, 3364364.5203123973), (20, 3047748.5040717563), (30, 2503185.5418753517)]\n\n```", "```py\nmodelk30 = KMeans.train(data_for_cluster, 30, maxIterations=4, initializationMode=\"random\")\n cluster_membership = data_for_cluster.map(lambda x: modelk30.predict(x))\n cluster_idx = cluster_membership.zipWithIndex()\n cluster_idx.take(20)\n print(\"Final centers: \" + str(modelk30.clusterCenters))\n\n```", "```py\nprint(\"Total Cost: \" + str(modelk30.computeCost(data_for_cluster)))\n\n```", "```py\nTotal Cost: 68313502.459\n\n```", "```py\nWSSSE = data_for_cluster.map(lambda point: computeError\n(point)).reduce(lambda x, y: x + y)\n print(\"WSSSE: \" + str(WSSSE))\n\n```", "```py\nWSSSE: 2503185.54188\n\n```", "```py\nimport os\nimport sys\nimport numpy as np\nfrom collections import OrderedDict\n\ntry:\n    from collections import OrderedDict\n    from numpy import array\n    from math import sqrt\n    import numpy\n    import urllib\n    import pyspark\n    from pyspark.sql import SparkSession\n    from pyspark.mllib.feature import StandardScaler\n    from pyspark.mllib.clustering import KMeans, KMeansModel\n    from pyspark.mllib.linalg import DenseVector\n    from pyspark.mllib.linalg import SparseVector\n    from collections import OrderedDict\n    from time import time\n    from pyspark.sql.types import *\n    from pyspark.sql import DataFrame\n    from pyspark.sql import SQLContext\n    from pyspark.sql import Row\n    print(\"Successfully imported Spark Modules\")\n\nexcept ImportError as e:\n    print (\"Can not import Spark Modules\", e)\n    sys.exit(1)\n\nspark = SparkSession\\\n        .builder\\\n        .appName(\"PCAExample\")\\\n        .getOrCreate()\n\nINPUT = \"C:/Exp/kddcup.data.corrected\"\nkddcup_data = spark.sparkContext.textFile(INPUT)\ncount = kddcup_data.count()\nprint(count)\nkddcup_data.take(5)\nkdd = kddcup_data.map(lambda l: l.split(\",\"))\nsqlContext = SQLContext(spark)\ndf = sqlContext.createDataFrame(kdd)\ndf.select(\"_1\", \"_2\", \"_3\", \"_4\", \"_42\").show(5)\n\n#Identifying the leabels for unsupervised task\nlabels = kddcup_data.map(lambda line: line.strip().split(\",\")[-1])\nstart_label_count = time()\nlabel_counts = labels.countByValue()\nlabel_count_time = time()-start_label_count\n\nsorted_labels = OrderedDict(sorted(label_counts.items(), key=lambda t: t[1], reverse=True))\nfor label, count in sorted_labels.items():\n    print(label, count)\n\ndef parse_interaction(line):\n    line_split = line.split(\",\")\n    clean_line_split = [line_split[0]]+line_split[4:-1]\n    return (line_split[-1], array([float(x) for x in clean_line_split]))\n\nparsed_data = kddcup_data.map(parse_interaction)\npd_values = parsed_data.values().cache()\n\nkdd_train = pd_values.sample(False, .75, 12345)\nkdd_test = pd_values.sample(False, .25, 12345)\nprint(\"Training set feature count: \" + str(kdd_train.count()))\nprint(\"Test set feature count: \" + str(kdd_test.count()))\n\nstandardizer = StandardScaler(True, True)\nstandardizer_model = standardizer.fit(kdd_train)\ndata_for_cluster = standardizer_model.transform(kdd_train)\n\ninitializationMode=\"random\"\n\nour_k = numpy.arange(10, 31, 10)\nmetrics = []\n\ndef computeError(point):\n    center = clusters.centers[clusters.predict(point)]\n    denseCenter = DenseVector(numpy.ndarray.tolist(center))\n    return sqrt(sum([x**2 for x in (DenseVector(point.toArray()) - denseCenter)]))\n\nfor k in our_k:\n     clusters = KMeans.train(data_for_cluster, k, maxIterations=4, initializationMode=\"random\")\n     WSSSE = data_for_cluster.map(lambda point: computeError(point)).reduce(lambda x, y: x + y)\n     results = (k, WSSSE)\n     metrics.append(results)\nprint(metrics)\n\nmodelk30 = KMeans.train(data_for_cluster, 30, maxIterations=4, initializationMode=\"random\")\ncluster_membership = data_for_cluster.map(lambda x: modelk30.predict(x))\ncluster_idx = cluster_membership.zipWithIndex()\ncluster_idx.take(20)\nprint(\"Final centers: \" + str(modelk30.clusterCenters))\nprint(\"Total Cost: \" + str(modelk30.computeCost(data_for_cluster)))\nWSSSE = data_for_cluster.map(lambda point: computeError(point)).reduce(lambda x, y: x + y)\nprint(\"WSSSE\" + str(WSSSE))\n\n```", "```py\nif (nchar(Sys.getenv(\"SPARK_HOME\")) < 1) { \nSys.setenv(SPARK_HOME = \"/home/spark\") \n} \nlibrary(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"))) \n\n```", "```py\nSPARK_HOME = \"spark-2.1.0-bin-hadoop2.7/R/lib\" \nHADOOP_HOME= \"spark-2.1.0-bin-hadoop2.7/bin\" \nSys.setenv(SPARK_MEM = \"2g\") \nSys.setenv(SPARK_HOME = \"spark-2.1.0-bin-hadoop2.7\") \n.libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"), .libPaths())) \n\n```", "```py\nlibrary(SparkR, lib.loc = SPARK_HOME)\n\n```", "```py\nsparkR.session(appName = \"Hello, Spark!\", master = \"local[*]\")\n\n```", "```py\nsparkR.session(master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"2g\")) \n\n```", "```py\n#Creating R data frame\ndataPath<- \"C:/Exp/nycflights13.csv\"\ndf<- read.csv(file = dataPath, header = T, sep =\",\")\n\n```", "```py\nView(df)\n\n```", "```py\n##Converting Spark DataFrame \n flightDF<- as.DataFrame(df)\n\n```", "```py\nprintSchema(flightDF)\n\n```", "```py\nshowDF(flightDF, numRows = 10)\n\n```", "```py\nflightDF<- read.df(dataPath,  \nheader='true',  \nsource = \"com.databricks.spark.csv\",  \ninferSchema='true') \n\n```", "```py\nprintSchema(flightDF)\n\n```", "```py\nshowDF(flightDF, numRows = 10)\n\n```", "```py\ncolumns(flightDF)\n[1] \"year\" \"month\" \"day\" \"dep_time\" \"dep_delay\" \"arr_time\" \"arr_delay\" \"carrier\" \"tailnum\" \"flight\" \"origin\" \"dest\" \n[13] \"air_time\" \"distance\" \"hour\" \"minute\" \n\n```", "```py\ncount(flightDF)\n[1] 336776\n\n```", "```py\n showDF(flightDF[flightDF$dest == \"MIA\", ], numRows = 10)\n\n```", "```py\ndelay_destination_DF<- select(flightDF, \"flight\", \"dep_delay\", \"origin\", \"dest\") \n delay_IAH_DF<- filter(delay_destination_DF, delay_destination_DF$dest == \"IAH\") showDF(delay_IAH_DF, numRows = 10)\n\n```", "```py\ninstall.packages(c(\"magrittr\")) \nlibrary(magrittr) \ngroupBy(flightDF, flightDF$day) %>% summarize(avg(flightDF$dep_delay), avg(flightDF$arr_delay)) ->dailyDelayDF \n\n```", "```py\nhead(dailyDelayDF)\n\n```", "```py\navg_arr_delay<- collect(select(flightDF, avg(flightDF$arr_delay))) \n head(avg_arr_delay)\navg(arr_delay)\n 1 6.895377\n\n```", "```py\nflight_avg_arrival_delay_by_destination<- collect(agg( \n groupBy(flightDF, \"dest\"), \n NUM_FLIGHTS=n(flightDF$dest), \n AVG_DELAY = avg(flightDF$arr_delay), \n MAX_DELAY=max(flightDF$arr_delay), \n MIN_DELAY=min(flightDF$arr_delay) \n ))\nhead(flight_avg_arrival_delay_by_destination)\n\n```", "```py\n# First, register the flights SparkDataFrame as a table\ncreateOrReplaceTempView(flightDF, \"flight\")\n\n```", "```py\ndestDF<- sql(\"SELECT dest, origin, carrier FROM flight\") \n showDF(destDF, numRows=10)\n\n```", "```py\nselected_flight_SQL<- sql(\"SELECT dest, origin, arr_delay FROM flight WHERE arr_delay>= 120\")\nshowDF(selected_flight_SQL, numRows = 10)\n\n```", "```py\nselected_flight_SQL_complex<- sql(\"SELECT origin, dest, arr_delay FROM flight WHERE dest='IAH' AND arr_delay>= 120 ORDER BY arr_delay DESC LIMIT 20\")\nshowDF(selected_flight_SQL_complex, numRows=20)\n\n```", "```py\nlibrary(ggplot2) \n\n```", "```py\nmy_plot<- ggplot(data=flightDF, aes(x=factor(carrier)))\n>>\nERROR: ggplot2 doesn't know how to deal with data of class SparkDataFrame.\n\n```", "```py\nflight_local_df<- collect(select(flightDF,\"carrier\"))\n\n```", "```py\nstr(flight_local_df)\n\n```", "```py\n'data.frame':  336776 obs. of 1 variable: $ carrier: chr \"UA\" \"UA\" \"AA\" \"B6\" ...\n\n```", "```py\nmy_plot<- ggplot(data=flight_local_df, aes(x=factor(carrier)))\n\n```", "```py\nmy_plot + geom_bar() + xlab(\"Carrier\")\n\n```", "```py\ncarrierDF = sql(\"SELECT carrier, COUNT(*) as cnt FROM flight GROUP BY carrier ORDER BY cnt DESC\")\nshowDF(carrierDF)\n\n```", "```py\n#Configure SparkR\nSPARK_HOME = \"C:/Users/rezkar/Downloads/spark-2.1.0-bin-hadoop2.7/R/lib\"\nHADOOP_HOME= \"C:/Users/rezkar/Downloads/spark-2.1.0-bin-hadoop2.7/bin\"\nSys.setenv(SPARK_MEM = \"2g\")\nSys.setenv(SPARK_HOME = \"C:/Users/rezkar/Downloads/spark-2.1.0-bin-hadoop2.7\")\n.libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"), .libPaths()))\n\n#Load SparkR\nlibrary(SparkR, lib.loc = SPARK_HOME)\n\n# Initialize SparkSession\nsparkR.session(appName = \"Example\", master = \"local[*]\", sparkConfig = list(spark.driver.memory = \"8g\"))\n# Point the data file path:\ndataPath<- \"C:/Exp/nycflights13.csv\"\n\n#Creating DataFrame using external data source API\nflightDF<- read.df(dataPath,\nheader='true',\nsource = \"com.databricks.spark.csv\",\ninferSchema='true')\nprintSchema(flightDF)\nshowDF(flightDF, numRows = 10)\n# Using SQL to select columns of data\n\n# First, register the flights SparkDataFrame as a table\ncreateOrReplaceTempView(flightDF, \"flight\")\ndestDF<- sql(\"SELECT dest, origin, carrier FROM flight\")\nshowDF(destDF, numRows=10)\n\n#And then we can use SparkR sql function using condition as follows:\nselected_flight_SQL<- sql(\"SELECT dest, origin, arr_delay FROM flight WHERE arr_delay>= 120\")\nshowDF(selected_flight_SQL, numRows = 10)\n\n#Bit complex query: Let's find the origins of all the flights that are at least 2 hours delayed where the destiantionn is Iowa. Finally, sort them by arrival delay and limit the count upto 20 and the destinations\nselected_flight_SQL_complex<- sql(\"SELECT origin, dest, arr_delay FROM flight WHERE dest='IAH' AND arr_delay>= 120 ORDER BY arr_delay DESC LIMIT 20\")\nshowDF(selected_flight_SQL_complex)\n\n# Stop the SparkSession now\nsparkR.session.stop()\n\n```"]