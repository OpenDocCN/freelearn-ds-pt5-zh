<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 11.  Building Data Science Applications"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch11" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 11.   Building Data Science Applications  </h1></div></div></div><p class="calibre11">Data science applications are garnering a lot of excitement, mainly because of the promise they hold in harnessing data and extracting consumable results. There are already several successful data products that have had a transformative effect on our daily lives. The ubiquitous recommender systems, e-mail spam filters, and targeted advertisements and news content have become part and parcel of life. Music and movies have become data products streaming from providers such as iTunes and Netflix. Businesses, especially in the domains such as retail, are actively pursuing ways to gain a competitive advantage by studying the market and customer behavior using a data-driven approach.</p><p class="calibre11">We have discussed the data analytics workflow up to the model building phase so far in the previous chapters. But the real value of a model is when it is actually deployed in a production system. The end product, the fruit of a data science workflow, is an operationalized data product. In this chapter, we discuss this culminating stage of the data analytics workflow. We will not get into actual code snippets but take a step back to get the complete picture, including the non-technical aspects.</p><p class="calibre11">The complete picture is not limited to the development process alone. It comprises the user application, developments in Spark itself, as well as rapid changes happening in the big data landscape. We'll start with the development process of the user application first and discuss various options at each stage. Then we'll delve into the features and enhancements in the latest Spark 2.0 release and future plans. Finally, we'll attempt to give a broad overview of the big data trends, especially the Hadoop ecosystem. References and useful links are included in individual sections in addition to the end of the chapter for further information about the specific context.</p><div class="calibre2" title="Scope of development"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch11lvl1sec86" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Scope of development</h1></div></div></div><p class="calibre11">Data analytics workflow can be roughly divided into two phases, the build phase and the operationalization phase. The first phase is usually a one-time exercise, with heavy human intervention. Once we've attained reasonable end results, we are ready to operationalize the product. The second phase starts with the models generated in the first phase and makes them available as a part of some production workflow. In this section, we'll discuss the following:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Expectations</li><li class="listitem">Presentation options</li><li class="listitem">Development and testing</li><li class="listitem">Data quality management</li></ul></div><div class="calibre2" title="Expectations"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch11lvl2sec116" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Expectations</h2></div></div></div><p class="calibre11">The primary goal of data science applications is to build "actionable" insights, actionable being the keyword. Many use cases such as fraud detection need the insights to be generated and made available in a consumable fashion in near real time, if you expect any action-ability at all. The end users of the data product vary with the use case. They may be customers of an e-commerce site or a decision maker of a major conglomerate. The end user need not always be a human being. It could be a risk assessment software tool in a financial institution. A one-size-fits-all approach does not fit in with many software products, and data products are no exception. However, there are some common expectations for data products, as listed here:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The first and foremost expectation is that the insight generation time frame based on real-world data should be within "actionable" timeframes. The actual time frame varies based on the use case.</li><li class="listitem">The data product should integrate into some (often already existing) production workflow.</li><li class="listitem">The insights should be translated into something that people can use instead of obscure numbers or hard-to-interpret charts. The presentation should be unobtrusive.</li><li class="listitem">The data product should have the ability to fine-tune itself (self-adapting) based on the incoming data inputs.</li><li class="listitem">Ideally, there has to be some way to receive human feedback, which can be used as a source for self-tuning.</li></ul></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">There should be a mechanism that quantitatively assesses its effectiveness periodically and automatically.</li></ul></div></div><div class="calibre2" title="Presentation options"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch11lvl2sec117" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Presentation options</h2></div></div></div><p class="calibre11">The varied nature of data products calls for varied modes of presentation. Sometimes the end result of a data analytics exercise is to publish a research paper. Sometimes it could be a part of a dashboard, where this becomes one of several sources publishing results on a single web page. They may be overt and targeted for human consumption, or covert and feeding into some other software application. You may use a general-purpose engine such as Spark to build your solution, but the presentation must be highly aligned to the targeted user base.</p><p class="calibre11">Sometimes all you need to do is write an e-mail with your findings or just export a CSV file of insights. Or you may have to develop a dedicated web application around your data product. Some other common options are discussed here, and you have to choose the right one that fits the problem on hand.</p><div class="calibre2" title="Interactive notebooks"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch11lvl3sec74" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Interactive notebooks</h3></div></div></div><p class="calibre11">Interactive notebooks are web applications that allow you to create and share documents that contain code chunks, results, equations, images, videos, and explanation text. They may be viewed as executable documents or REPL shells with visualization and equation support. These documents can be exported as PDFs, Markdown, or HTML. Notebooks contain several "kernels" or "computational engines" that execute code chunks.</p><p class="calibre11">Interactive notebooks are the most suitable choice if the end goal of your data analytics workflow is to generate a written report. There are several notebooks and many of them have Spark support. These notebooks are useful tools during the exploration phase also. We have already introduced IPython and Zeppelin notebooks in previous chapters.</p><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title7"><a id="ch11lvl4sec4" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h4></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The IPython Notebook: A Comprehensive Tool for Data Science: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233">http://conferences.oreilly.com/strata/strata2013/public/schedule/detail/27233</a></li><li class="listitem">Sparkly Notebook: Interactive Analysis and Visualization with Spark: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark">http://www.slideshare.net/felixcss/sparkly-notebook-interactive-analysis-and-visualization-with-spark</a></li></ul></div></div></div><div class="calibre2" title="Web API"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch11lvl3sec75" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Web API</h3></div></div></div><p class="calibre11">An <span class="strong"><strong class="calibre19">Application Programming Interface</strong></span> (<span class="strong"><strong class="calibre19">API</strong></span>) is a software-to-software interface; a specification that describes the available functionality, how it must be used, and what the inputs and outputs are. The software (service) provider exposes some of its functionality as an API. A developer may develop a software component that consumes this API. For example, Twitter offers APIs to get or post data onto Twitter or to query data programmatically. A Spark enthusiast may write a software component that automatically collects all tweets on #Spark, categorizes according to their requirements, and publishes that data on their personal website. Web APIs are a type of APIs where the interface is defined as a set of <span class="strong"><strong class="calibre19">Hypertext Transfer Protocol</strong></span> (<span class="strong"><strong class="calibre19">HTTP</strong></span>) request messages along with a definition of the structure of response messages. Nowadays REST-ful (Representational State Transfer) have become the de facto standard.</p><p class="calibre11">You can implement your data product as an API, and perhaps this is the most powerful option. It can then be plugged into one or more applications, say the management dashboard as well as the marketing analytics workflow. You may develop a domain specific "insights-as-a-service" as a public Web API with a subscription model. The simplicity and ubiquity of Web APIs make them the most compelling choice for building data products.</p><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title7"><a id="ch11lvl4sec5" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h4></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Application programming interface: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://en.wikipedia.org/wiki/Application_programming_interface">https://en.wikipedia.org/wiki/Application_programming_interface</a></li><li class="listitem">Ready for APIs? Three steps to unlock the data economy's most promising channel:<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5">http://www.forbes.com/sites/mckinsey/2014/01/07/ready-for-apis-three-steps-to-unlock-the-data-economys-most-promising-channel/#61e7103b89e5</a></li><li class="listitem">How Insights-as-a-service is growing based on big data: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html">http://www.kdnuggets.com/2015/12/insights-as-a-service-big-data.html</a></li></ul></div></div></div><div class="calibre2" title="PMML and PFA"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch11lvl3sec76" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>PMML and PFA</h3></div></div></div><p class="calibre11">Sometimes you may have to expose your model in a way that other data mining tools can understand. The model and the complete pre- and post-processing steps should be converted into a standard format. PMML and PFA are two such standard formats in the data mining domain.</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Predictive Model Markup Language </strong></span>(<span class="strong"><strong class="calibre19">PMML</strong></span>) is an XML-based predictive model interchange format and Apache Spark API convert models into PMML out of the box. A PMML message may contain a myriad of data transformations as well as one or more predictive models. Different data mining tools can export or import PMML messages without the need for custom code.</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Portable Format for Analytics </strong></span>(<span class="strong"><strong class="calibre19">PFA</strong></span>) is the next generation of predictive model interchange format. It exchanges JSON documents and straightaway inherits all advantages of JSON documents as against XML documents. In addition, PFA is more flexible than PMML.</p><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title7"><a id="ch11lvl4sec6" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h4></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">PMML FAQ: Predictive Model Markup Language: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html">http://www.kdnuggets.com/2013/01/pmml-faq-predictive-model-markup-language.html</a></li><li class="listitem">Portable Format for Analytics: moving models to production: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html">http://www.kdnuggets.com/2016/01/portable-format-analytics-models-production.html</a></li><li class="listitem">What is PFA for?: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://dmg.org/pfa/docs/motivation/">http://dmg.org/pfa/docs/motivation/</a></li></ul></div></div></div></div><div class="calibre2" title="Development and testing"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch11lvl2sec118" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Development and testing</h2></div></div></div><p class="calibre11">Apache Spark is a general-purpose cluster computing system that can run both by itself or over several existing cluster managers such as Apache Mesos, Hadoop, Yarn, and Amazon EC2. In addition, several big data and enterprise software companies have already integrated Spark into their offerings: Microsoft Azure HDInsight, Cloudera, IBM Analytics for Apache Spark, SAP HANA, and the list goes on. Databricks, a company founded by the creators of Apache Spark, have their own product for data science workflow, from ingestion to production. Your responsibility is to understand your organizational requirements and existing talent pool and decide which option is the best for you.</p><p class="calibre11">Regardless of the option chosen, follow the usual best practices in any software development life cycle, such as version control and peer reviews. Try to use high-level APIs wherever applicable. The data transformation pipelines used in production should be the same as the ones used in building the model. Document any questions that arise during the data analytics workflow. Often these may result in business process improvements.</p><p class="calibre11">As always, testing is extremely important for the success of your product. You have to maintain a set of automated scripts that give easy-to-understand results. The test cases should cover the following at the minimum:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Adherence to timeframe and resource consumption requirements</li><li class="listitem">Resilience to bad data (for example, data type violations)</li><li class="listitem">New value in a categorical feature that was not encountered during the model building phase</li><li class="listitem">Very little data or too heavy data that is expected in the target production system</li></ul></div><p class="calibre11">Monitor logs, resource utilization, and so on to uncover any performance bottlenecks. The Spark UI provides a wealth of information to monitor Spark applications. The following are some common tips that will help you improve performance:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Cache any input or intermediate data that might be used multiple times.</li><li class="listitem">Look at the Spark UI and identify jobs that are causing a lot of shuffle. Check the code and see whether you can reduce the shuffles.</li><li class="listitem">Actions may transfer the data from workers to the driver. See that you are not transferring any data that is not absolutely necessary.</li><li class="listitem">Stragglers; that run slower than others; may increase the overall job completion time. There may be several reasons for a straggler. If a job is running slow due to a slow node, you may set <code class="literal">spark.speculation</code> to <code class="literal">true</code>. Then Spark automatically relaunches such a task on a different node. Otherwise, you may have to revisit the logic and see whether it can be improved.</li></ul></div><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch11lvl3sec77" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h3></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Investigating Spark's performance: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://radar.oreilly.com/2015/04/investigating-sparks-performance.html">http://radar.oreilly.com/2015/04/investigating-sparks-performance.html</a></li><li class="listitem">Tuning and Debugging in Apache Spark by Patrick Wendell: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/">https://sparkhub.databricks.com/video/tuning-and-debugging-apache-spark/</a> </li><li class="listitem">How to tune your Apache Spark jobs: http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/ and part 2 </li></ul></div></div></div><div class="calibre2" title="Data quality management"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch11lvl2sec119" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data quality management</h2></div></div></div><p class="calibre11">At the outset, let's not forget that we are trying to build fault-tolerant software data products from unreliable, often unstructured, and uncontrolled data sources. So data quality management gains even more importance in a data science workflow. Sometimes the data may solely come from controlled data sources, such as automated internal process workflows in an organization. But in all other cases, you need to carefully craft your data cleansing processes to protect the subsequent processing.</p><p class="calibre11">Metadata consists of the structure and meaning of data, and obviously the most critical repository to work with. It is the information about the structure of individual data sources and what each component in that structure means. You may not always be able to write some script and extract this data. A single data source may contain data with different structures or an individual component (column) may mean different things during different times. A label such as owner or high may mean different things in different data sources. Collecting and understanding all such nuances and documenting is a tedious, iterative task. Standardization of metadata is a prerequisite to data transformation development.</p><p class="calibre11">Some broad guidelines that are applicable to most use cases are listed here:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">All data sources must be versioned and timestamped</li><li class="listitem">Data quality management processes often require involvement of the highest authorities</li><li class="listitem">Mask or anonymize sensitive data</li><li class="listitem">One important step that is often missed out is to maintain traceability; a link between each data element (say a row) and its original source</li></ul></div></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The Scala advantage"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch11lvl1sec87" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The Scala advantage</h1></div></div></div><p class="calibre11">Apache Spark allows you to write applications in Python, R, Java, or Scala. With this flexibility comes the responsibility of choosing the right language for your requirements. But regardless of your usual language of choice, you may want to consider Scala for your Spark-powered application. In this section, we will explain why.</p><p class="calibre11">Let's digress to gain a high-level understanding of imperative and functional programming paradigms first. Languages such as C, Python, and Java belong to the imperative programming paradigm. In the imperative programming paradigm, a program is a sequence of instructions and it has a program state. The program state is usually represented as a set of variables and their values at any given point in time. Assignments and reassignments are fairly common. Variable values are expected to change over the period of execution by one or more functions. Variable value modification in a function is not limited to local variables. Global variables and public class variables are some examples of such variables.</p><p class="calibre11">In contrast, programs written in functional programming languages such as Erlang can be viewed as stateless expression evaluators. Data is immutable. If a function is called with the same set of input arguments, then it is expected to produce the same result (that is, referential transparency). This is possible due to the absence of interference from a variable context in the form of global variables and the like. This implies that the sequence of function evaluation is of little importance. Functions can be passed as arguments to other functions. Recursive calls replace loops. The absence of state makes parallel programming much easier because it eliminates the need for locking and possible deadlocks. Coordination gets simplified when the execution order is less important. These factors make the functional programming paradigm a neat fit for parallel programming.</p><p class="calibre11">Pure functional programming languages are hard to work with because most of the programs require state changes. Most functional programming languages, including good old Lisp, do allow storing of data in variables (side-effects). Some languages such as Scala draw from multiple programming paradigms.</p><p class="calibre11">Returning to Scala, it is a JVM-based, statically typed multi-paradigm programming language. Its built-in-type inference mechanism allows programmers to omit some redundant type information. This gives a feel of the flexibility offered by dynamic languages while retaining the robustness of better compile time checks and fast runtime. Scala is an object-oriented language in the sense that every value is an object, including numerical values. Functions are first-class objects, which can be used as any data type, and they can be passed as arguments to other functions. Scala interoperates well with Java and its tools because Scala runs on JVM. Java and Scala classes can be freely mixed. That implies that Scala can easily interact with the Hadoop ecosystem.</p><p class="calibre11">All of these factors should be taken into account when you choose the right programming language for your application.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Spark development status"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch11lvl1sec88" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Spark development status</h1></div></div></div><p class="calibre11">Apache Spark has become the most currently active project in the Hadoop ecosystem in terms of the number of contributors by the end of 2015. Having started as a research project at UC Berkeley AMPLAB in 2009, Spark is still relatively young when compared to projects such as Apache Hadoop and is still in active development. There were three releases in the year 2015, from 1.3 through 1.5, packed with features such as DataFrames API, SparkR, and Project Tungsten respectively. Version 1.6 was released in early 2016 and included the new Dataset API and expansion of data science functionality. Spark 2.0 was released in July 2016, and this being a major release has a lot of new features and enhancements that deserve a section of their own.</p><div class="calibre2" title="Spark 2.0's features and enhancements"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch11lvl2sec120" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Spark 2.0's features and enhancements</h2></div></div></div><p class="calibre11">Apache Spark 2.0 included three major new features and several other performance improvements and under-the-hood changes. This section attempts to give a high-level overview yet step into the details to give a conceptual understanding wherever required.</p><div class="calibre2" title="Unifying Datasets and DataFrames"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch11lvl3sec78" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Unifying Datasets and DataFrames</h3></div></div></div><p class="calibre11">DataFrames are high-level APIs that support a data abstraction conceptually equivalent to a table in a relational database or a DataFrame in R and Python (the pandas library). Datasets are an extension of the DataFrame API that provide a type-safe, object-oriented programming interface. Datasets add static types to DataFrames. Defining a structure on top of DataFrames provides information to the core that enables optimizations. It also helps in catching analysis errors early on, even before a distributed job starts.</p><p class="calibre11">RDDs, Datasets, and DataFrames are interchangeable. RDDs continue to be the low-level API. DataFrames, Datasets, and SQL share the same optimization and execution pipeline. Machine learning libraries take either DataFrames or Datasets. Both DataFrames and Datasets run on Tungsten, an initiative to improve runtime performance. They leverage Tungsten's fast in-memory encoding, which is responsible for converting between JVM objects and Spark's internal representation. The same APIs work on streams also, introducing the concept of continuous DataFrames.</p></div><div class="calibre2" title="Structured Streaming"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch11lvl3sec79" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Structured Streaming</h3></div></div></div><p class="calibre11">Structure Streaming APIs are high-level APIs that are built on the Spark SQL engine and extend DataFrames and Datasets. Structured Streaming unifies streaming, interactive, and batch queries. In most use cases, streaming data needs to be combined with batch and interactive queries to form continuous applications. These APIs are designed to address that requirement. Spark takes care of running the query incrementally and continuously on streaming data.</p><p class="calibre11">The first release of structured streaming will be focusing on ETL workloads. Users will be able to specify the input, query, trigger, and type of output. An input stream is logically equivalent to an append-only table. Users define queries just the way they would on a traditional SQL table. The trigger is a timeframe, say one second. The output modes offered are complete output, deltas, or updates in place (for example, a DB table).</p><p class="calibre11">Take this example: you can aggregate the data in a stream, serve it using the Spark SQL JDBC server, and pass it to a database such as MySQL for downstream applications. Or you could run ad hoc SQL queries that act on the latest data. You can also build and apply machine learning models.</p></div><div class="calibre2" title="Project Tungsten phase 2"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title4"><a id="ch11lvl3sec80" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Project Tungsten phase 2</h3></div></div></div><p class="calibre11">The central idea behind project Tungsten is to bring Spark's performance closer to bare metal through native memory management and runtime code generation. It was first included in Spark 1.4 and enhancements were added in 1.5 and 1.6. It focuses on substantially improving the efficiency of memory and CPU for Spark applications, primarily by the following ways:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Managing memory explicitly and eliminating the overhead of JVM object model and garbage collection. For example, a four-byte string would occupy around 48 bytes in the JVM object model. Since Spark is not a general-purpose application and has more knowledge about the life cycle of memory blocks than the garbage collector, it can manage memory more efficiently than JVM.</li><li class="listitem">Designing cache-friendly algorithms and data structures.</li><li class="listitem">Spark performs code generation to compile parts of queries to Java bytecode. This is being broadened to cover most built-in expressions.</li></ul></div><p class="calibre11">Spark 2.0 rolls out phase 2, which is an order of magnitude faster and includes:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Whole stage code generation by removing expensive iterator calls and fusing across multiple operators so that the generated code looks like hand-optimized code</li><li class="listitem">Optimized input and output</li></ul></div></div></div><div class="calibre2" title="What's in store?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch11lvl2sec121" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>What's in store?</h2></div></div></div><p class="calibre11">Apache Spark 2.1 is expected to have the following:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Continuous SQL</strong></span> (<span class="strong"><strong class="calibre19">CSQL</strong></span>)</li><li class="listitem">BI application integration</li><li class="listitem">Support for more streaming sources and sinks</li><li class="listitem">Inclusion of additional operators and libraries for structured streaming</li><li class="listitem">Enhancements to a machine learning package</li><li class="listitem">Columnar in-memory support in Tungsten</li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The big data trends"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch11lvl1sec89" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The big data trends</h1></div></div></div><p class="calibre11">Big data processing has been an integral part of the IT industry, more so in the past decade. Apache Hadoop and other similar endeavors are focused on building the infrastructure to store and process massive amounts of data. After being around for over 10 years, the Hadoop platform is considered mature and almost synonymous with big data processing. Apache Spark, a general computing engine that works well with is and not limited to the Hadoop ecosystem, was quite successful in the year 2015.</p><p class="calibre11">Building data science applications requires knowledge of the big data landscape and what software products are available out of that box. We need to carefully map the right blocks that fit our requirements. There are several options with overlapping functionality, and picking the right tools is easier said than done. The success of the application very much depends on assembling the right mix of technologies and processes. The good news is that there are several open source options that drive down the cost of doing big data analytics; and at the same time, you have enterprise-quality end-to-end platforms backed by companies such as Databricks. In addition to the use case on hand, keeping track of the industry trends in general is equally important.</p><p class="calibre11">The recent surge in NOSQL data stores with their own interfaces are adding SQL-based interfaces even though they are not relational data stores and may not adhere to ACID properties. This is a welcome trend because converging to a single, age-old interface across relational and non-relational data stores improves programmer productivity.</p><p class="calibre11">The operational (OLTP) and analytical (OLAP) systems were being maintained as separate systems over the past couple of decades, but that's one more place where convergence is happening. This convergence brings us to near-real-time use cases such as fraud prevention. Apache Kylin is one open source distributed analytics engine in the Hadoop ecosystem that offers an extremely fast OLAP engine at scale.</p><p class="calibre11">The advent of the Internet of Things is accelerating real-time and streaming analytics, bringing in a whole lot of new use cases. The cloud frees up organizations from the operations and IT management overheads so that they can concentrate on their core competence, especially in big data processing. Cloud-based analytic engines, self-service data preparation tools, self-service BI, just-in-time data warehousing, advanced analytics, rich media analytics, and agile analytics are some of the commonly used buzzwords. The term big data itself is slowly evaporating or becoming implicit.</p><p class="calibre11">There are plenty of software products and libraries in the big data landscape with overlapping functionalities, as shown in this infographic (http://mattturck.com/wp-content/uploads/2016/02/matt_turck_big_data_landscape_v11.png). Choosing the right blocks for your application is a daunting but very important task. Here is a short list of projects to get you started. The list excludes popular names such as Cassandra and tries to include blocks with complementing functionality and mostly from Apache Software Foundation:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Apache Arrow</strong></span> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://arrow.apache.org/">https://arrow.apache.org/</a>) is an in-memory columnar layer used to accelerate analytical processing and interchange. It is a high-performance, cross-system, and in-memory data representation that is expected to bring in 100 times the performance improvements.</li><li class="listitem"><span class="strong"><strong class="calibre19">Apache Parquet</strong></span> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://parquet.apache.org/">https://parquet.apache.org/</a>) is a columnar storage format. Spark SQL provides support for both reading and writing parquet files while automatically capturing the structure of the data.</li><li class="listitem"><span class="strong"><strong class="calibre19">Apache Kafka</strong></span> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://kafka.apache.org/">http://kafka.apache.org/</a>) is a popular, high-throughput distributed messaging system. Spark streaming has a direct API to support streaming data ingestion from Kafka.</li></ul></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Alluxio</strong></span> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://alluxio.org/">http://alluxio.org/</a>), formerly called Tachyon, is a memory-centric, virtual distributed storage system that enables data sharing across clusters at memory speed. It aims to become the de facto storage unification layer for big data. Alluxio sits between computation frameworks such as Spark and storage systems such as Amazon S3, HDFS, and others.</li><li class="listitem"><span class="strong"><strong class="calibre19">GraphFrames</strong></span> (https://databricks.com/blog/2016/03/03/introducing-graphframes.html) is a graph processing library for Apache spark that is built on top of DataFrames API.</li><li class="listitem"><span class="strong"><strong class="calibre19">Apache Kylin</strong></span> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://kylin.apache.org/">http://kylin.apache.org/</a>) is a distributed analytics engine designed to provide SQL interface and multidimensional analysis (OLAP) on Hadoop, supporting extremely large datasets.</li><li class="listitem"><span class="strong"><strong class="calibre19">Apache Sentry</strong></span> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://sentry.apache.org/">http://sentry.apache.org/</a>) is a system for enforcing fine-grained role-based authorization to data and metadata stored on a Hadoop cluster. It is in the incubation stage at the time of writing this book.</li><li class="listitem"><span class="strong"><strong class="calibre19">Apache Solr</strong></span> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://lucene.apache.org/solr/">http://lucene.apache.org/solr/</a>) is a blazing fast search platform. Check this <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://spark-summit.org/2015/events/integrating-spark-and-solr/">presentation</a> for integrating Solr and Spark.</li><li class="listitem"><span class="strong"><strong class="calibre19">TensorFlow</strong></span> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>) is a machine learning library with extensive built-in support for deep learning. Check out this <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://databricks.com/blog/2016/01/25/deep-learning-with-spark-and-tensorflow.html">blog</a> to learn how it can be used with Spark.</li><li class="listitem"><span class="strong"><strong class="calibre19">Zeppelin</strong></span> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://zeppelin.incubator.apache.org/">http://zeppelin.incubator.apache.org/</a>) is a web-based notebook that enables interactive data analytics. It is covered in the data visualization chapter.</li></ul></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch11lvl1sec90" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this final chapter, we discussed how to build real-world applications using Spark. We discussed the big picture consisting of technical and non-technical aspects of data analytics workflows.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch11lvl1sec91" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The Spark Summit site has a wealth of information on Apache Spark and related projects from completed events</li><li class="listitem">Interview with <span class="strong"><em class="calibre22">Matei Zaharia</em></span> by KDnuggets</li><li class="listitem"><span class="strong"><em class="calibre22">Why Spark Reached the Tipping Point</em></span> in 2015 from KDnuggets by <span class="strong"><em class="calibre22">Matthew Mayo</em></span></li><li class="listitem">Going Live: Preparing your first Spark production deployment is a very good starting point</li><li class="listitem"><span class="strong"><em class="calibre22">What is Scala?</em></span> from the Scala home page</li><li class="listitem"><span class="strong"><em class="calibre22">Martin Odersky</em></span>, creator of Scala, explains the reasons why Scala fuses together imperative and functional programming</li></ul></div></div></div>



  </body></html>