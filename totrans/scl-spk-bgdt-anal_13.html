<html><head></head><body>
        <section id="C9ROA1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">My Name is Bayes, Naive Bayes</h1>
                
            
            <article>
                
<div class="packt_quote">"Prediction is very difficult, especially if it's about the future"</div>
<p class="cdpalignright">-Niels Bohr</p>
<p class="mce-root"><strong class="calibre1">Machine learning (ML)</strong> in combination with big data is a radical combination that has created some great impacts in the field of research in Academia and Industry. Moreover, many research areas are also entering into big data since datasets are being generated and produced in an unprecedented way from diverse sources and technologies, commonly referred as the <strong class="calibre1">Data Deluge</strong>. This imposes great challenges on ML, data analytics tools, and algorithms to find the real <strong class="calibre1">VALUE</strong> out of big data criteria such as volume, velocity, and variety. However, making predictions from these huge dataset has never been easy.</p>
<p class="mce-root">Considering this challenge, in this chapter we will dig deeper into ML and find out how to use a simple yet powerful method to build a scalable classification model and even more. In a nutshell, the following topics will be covered throughout this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Multinomial classification</li>
<li class="mce-root1">Bayesian inference</li>
<li class="mce-root1">Na<span>i</span>ve Bayes</li>
<li class="mce-root1">Decision trees</li>
<li class="mce-root1">Na<span>i</span>ve Bayes versus decision trees</li>
</ul>


            </article>

            
        </section>
    

        <section id="CAQ8S1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Multinomial classification</h1>
                
            
            <article>
                
<p class="mce-root">In ML, <strong class="calibre1">multinomial</strong> (also known as multiclass) classification is the task of classifying data objects or instances into more than two classes, that is, having more than two labels or classes. Classifying data objects or instances into two classes is called <strong class="calibre1">binary classification</strong>. More technically, in multinomial classification, each training instance belongs to one of N different classes subject to <kbd class="calibre11">N &gt;=2</kbd>. The goal is then to construct a model that correctly predicts the classes to which the new instances belong. There may be numerous scenarios having multiple categories in which the data points belong. However, if a given point belongs to multiple categories, this problem decomposes trivially into a set of unlinked binary problems, which can be solved naturally using a binary classification algorithm.</p>
<div class="packt_tip">Readers are suggested not be confused distinguishing the multiclass classification with multilabel classification, where multiple labels are to be predicted for each instance. For more on Spark-based implementation for the multilabel classification, interested readers should refer to <a href="https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#multilabel-classification" class="calibre21">https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#multilabel-classification</a>.</div>
<p class="mce-root">Multiclass classification techniques can be divided into several categories as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Transformation to binary</li>
<li class="mce-root1">Extension from binary</li>
<li class="mce-root1">Hierarchical classification</li>
</ul>


            </article>

            
        </section>
    

        <section id="CBOPE1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Transformation to binary</h1>
                
            
            <article>
                
<p class="mce-root">Using the transformation to binary technique, a multiclass classification problem can be transformed into an equivalent strategy for multiple binary classification problems. In other words, this technique can be referred to as a <em class="calibre8">problem transformation techniques</em>. A detailed discussion from the theoretical and practical perspectives is out of the scope of this chapter. Therefore, here we will discuss only one example of the problem transformation technique called <strong class="calibre1">One-Vs-The-Rest</strong> (<strong class="calibre1">OVTR</strong>) algorithm as the representative of this category.</p>


            </article>

            
        </section>
    

        <section id="CCNA01-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Classification using One-Vs-The-Rest approach</h1>
                
            
            <article>
                
<p class="mce-root">In this subsection, we will describe an example of performing multiclass classification using the OVTR algorithm by converting the problem into equivalent multiple binary classification problems. The OVTR strategy breaks down the problem and trains each binary classifier per class. In other words, the OVTR classifier strategy consists of fitting one binary classifier per class. It then treats all the samples of the current class as positive samples, and consequently other samples of other classifiers are treated as negatives samples.</p>
<p class="mce-root">This is a modular machine learning technique no doubt. However, on the downside, this strategy requires a base classifier from the multiclass family. The reason is that the classifier must produce a real value also called <em class="calibre8">confidence scores</em> instead of a prediction of the actual labels. The second disadvantage of this strategy is that if the dataset (aka training set) contains discrete class labels, these eventually lead to vague prediction results. In that case, multiple classes can be predicted for a single sample. To make the preceding discussion clearer, now let's see an example as follows.</p>
<p class="mce-root">Suppose that we have a set of 50 observations divided into three classes. Thus, we will use the same logic as before for selecting the negative examples too. For the training phase, let's have the following setting:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Classifier 1</strong> has 30 positive examples and 20 negative examples</li>
<li class="mce-root1"><strong class="calibre1">Classifier 2</strong> has 36 positive examples and 14 negative examples</li>
<li class="mce-root1"><strong class="calibre1">Classifier 3</strong> has 14 positive examples and 24 negative examples</li>
</ul>
<p class="mce-root">On the other hand, for the testing phase, suppose I have a new instance that need to be classified into one of the previous classes. Each of the three classifiers, of course, produces a probability with respect to the estimation This is an estimation of how low an instance belongs to the negative or positive examples in the classifier? In this case, we should always compare the probabilities of positive class in one versus the rest. Now that for <em class="calibre8">N</em> classes, we will have <em class="calibre8">N</em> probability estimates of the positive class for one test sample. Compare them, and whichever probability is the maximum of <em class="calibre8">N</em> probabilities belongs to that particular class. Spark provides multiclass to binary reduction with the <span>OVTR</span> algorithm, where the <strong class="calibre1">Logistic Regression</strong> algorithm is used as the base classifier.</p>
<p class="mce-root">Now let's see another example of a real dataset to demonstrate how Spark classifies all the features using OVTR algorithm. The OVTR classifier eventually predicts handwritten characters from the <strong class="calibre1">Optical Character Reader</strong> (<strong class="calibre1">OCR</strong>) dataset. However, before diving into the demonstration, let's explore the OCR dataset first to get the exploratory nature of the data. It is to be noted that when OCR software first processes a document, it divides the paper or any object into a matrix such that each cell in the grid contains a single glyph (also known different graphical shapes), which is just an elaborate way of referring to a letter, symbol, or number or any contextual information from the paper or the object.</p>
<p class="mce-root">To demonstrate the OCR pipeline, let's assume that the document contains only alpha characters in English that match glyphs to one of the 26 capital letters, that is, <em class="calibre8">A</em> to <em class="calibre8">Z</em>. We will use the OCR letter dataset from the <em class="calibre8">UCI Machine Learning Data Repository</em>. The dataset was denoted by W<em class="calibre8">. Frey</em> and <em class="calibre8">D. J. Slate.</em> While exploring the dataset, you should observe 20,000 examples of 26 English capital letters. Letter written in capital letters are available as printed using 20 different, randomly reshaped and distorted black and white fonts as glyphs of different shapes. In short, predicting all the characters from 26 alphabets turns the problem itself into a multiclass classification problem with 26 classes. Consequently, a binary classifier will not be able to serve our purpose.</p>
<div class="cdpaligncenter"><img class="image-border176" src="../images/00362.gif"/></div>
<div class="packt_figref"><strong class="calibre1">Figure 1</strong>: Some of the printed glyphs (Source: Letter recognition using Holland-style adaptive classifiers, ML, V. 6, p. 161-182, by W. Frey and D.J. Slate [1991])</div>
<p class="mce-root">The preceding figure shows the images that I explained earlier.<em class="calibre8">The dataset</em> provides an example of some of the printed glyphs distorted in this way; therefore, the letters are computationally challenging for a computer to identify. Yet, these glyphs are easily recognized by a human being. The following figure shows the statistical attributes of the top 20 rows:</p>
<div class="cdpaligncenter"><img class="image-border177" src="../images/00020.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 2:</strong> The snapshot of the dataset shown as the data frame</div>


            </article>

            
        </section>
    

        <section>

                            <header id="CDLQI2-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">Exploration and preparation of the OCR dataset</h1>
                
            
            <article>
                
<p class="mce-root">According to the dataset description, glyphs are scanned using an OCR reader on to the computer then they are automatically converted into pixels. Consequently, all the 16 statistical attributes (in <strong class="calibre1">figure 2</strong>) are recorded to the computer too. The the concentration of black pixels across various areas of the box provide a way to differentiate 26 letters using OCR or a machine learning algorithm to be trained.</p>
<div class="packt_infobox"><span class="field">Recall that <strong class="calibre27">support vector machines</strong> (<strong class="calibre27">SVM</strong>), Logistic Regression, Naive Bayesian-based classifier, or any other classifier algorithms (along with their associated learners) require all the features to be numeric.</span> LIBSVM allows you to use a sparse training dataset in an unconventional format. While transforming the normal training dataset to the LIBSVM format. Only the nonzero values that are also included in the dataset are stored in a sparse array/matrix form. The index specifies the column of the instance data (feature index). However, any missing data is taken as holding zero value too. The index serves as a way to distinguish between the features/parameters. For example, for three features, indices 1, 2, and 3 would correspond to the <em class="calibre25">x</em>, <em class="calibre25">y</em>, and <em class="calibre25">z</em> coordinates, respectively. The correspondence between the same index values of different data instances is merely mathematical when constructing the hyperplane; these serve as coordinates. If you skip any index in between, it should be assigned a default value of zero.</div>
<p class="mce-root"><span>In most practical cases, we might need to normalize the data against all the features points. In short, we need to convert the current tab-separated OCR data into LIBSVM format to make the training step easier.</span> Thus, I'm assuming you have downloaded the data and converted into LIBSVM format using their own script. The resulting dataset that is transformed into LIBSVM format consisting of labels and features is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border178" src="../images/00223.gif"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 3:</strong> A snapshot of 20 rows of the OCR dataset in LIBSVM format</div>
<div class="packt_tip">Interested readers can refer to the following research article for gaining in-depth knowledge: <em class="calibre25">Chih-Chung Chang</em> and <em class="calibre25">Chih-Jen Lin</em>, <em class="calibre25">LIBSVM: a library for support vector machines</em>, <em class="calibre25">ACM Transactions on Intelligent Systems and Technology</em>, 2:27:1--27:27, 2011. You can also refer to a public script provided on my GitHub repository at <a href="https://github.com/rezacsedu/RandomForestSpark/" class="calibre21">https://github.com/rezacsedu/RandomForestSpark/</a> that directly converts the OCR data in CSV into LIBSVM format. I read the data about all the letters and assigned a unique numeric value to each. All you need is to show the input and output file path and run the script.</div>
<p class="mce-root">Now let's dive into the example. The example that I will be demonstrating has 11 steps including data parsing, Spark session creation, model building, and model evaluation.</p>
<p class="mce-root"><strong class="calibre1">Step 1. Creating Spark session</strong> - Create a Spark session by specifying master URL, Spark SQL warehouse, and application name as follows:</p>
<pre class="calibre19">
val spark = SparkSession.builder<br class="title-page-name"/>                     .master("local[*]") //change acordingly<br class="title-page-name"/>                     .config("spark.sql.warehouse.dir", "/home/exp/")<br class="title-page-name"/>                     .appName("OneVsRestExample") <br class="title-page-name"/>                     .getOrCreate()
</pre>
<p class="mce-root"><strong class="calibre1">Step 2. Loading, parsing, and creating the data frame</strong> - Load the data file from the HDFS or local disk and create a data frame, and finally show the data frame structure as follows:</p>
<pre class="calibre19">
val inputData = spark.read.format("libsvm")<br class="title-page-name"/>                     .load("data/Letterdata_libsvm.data")<br class="title-page-name"/>inputData.show()
</pre>
<p class="mce-root"><strong class="calibre1">Step 3. Generating training and test set to train the model</strong> - Let's generate the training and test set by splitting 70% for training and 30% for the test:</p>
<pre class="calibre19">
val Array(train, test) = inputData.randomSplit(Array(0.7, 0.3))
</pre>
<p class="mce-root"><strong class="calibre1">Step 4. Instantiate the base classifier</strong> - Here the base classifier acts as the multiclass classifier. For this case, it is the Logistic Regression algorithm that can be instantiated by specifying parameters such as the number of max iterations, tolerance, regression parameter, and Elastic Net parameters.</p>
<p class="mce-root">Note that Logistic Regression is an appropriate regression analysis to conduct when the dependent variable is dichotomous (binary). Like all regression analyses, Logistic Regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval, or ratio level independent variables.</p>
<div class="packt_infobox1">For a a Spark-based implementation of the Logistic Regression algorithm, interested readers can refer to <a href="https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression" class="calibre21">https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression</a>.</div>
<p class="calibre46">In brief, the following parameters are used to training a Logistic Regression classifier:</p>
<div class="calibre47">
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">MaxIter</kbd>: This specifies the number of maximum iterations. In general, more is better.</li>
<li class="mce-root1"><kbd class="calibre11">Tol</kbd>: This is the tolerance for the stopping criteria. In general, less is better, which helps the model to be trained more intensively. The default value is 1E-4.</li>
<li class="mce-root1"><kbd class="calibre11">FirIntercept</kbd>: This signifies if you want to intercept the decision function while generating the probabilistic interpretation.</li>
<li class="mce-root1"><kbd class="calibre11">Standardization</kbd>: This signifies a Boolean value depending upon if would like to standardize the training or not.</li>
<li class="mce-root1"><kbd class="calibre11">AggregationDepth</kbd>: More is better.</li>
<li class="mce-root1"><kbd class="calibre11">RegParam</kbd>: This signifies the regression params. Less is better for most cases.</li>
<li class="mce-root1"><kbd class="calibre11">ElasticNetParam</kbd>: This signifies more advanced regression params. Less is better for most cases.</li>
</ul>
</div>
<p class="mce-root">Nevertheless, you can specify the fitting intercept as a <kbd class="calibre11">Boolean</kbd> value as true or false depending upon your problem type and dataset properties:</p>
<pre class="calibre19">
<strong class="calibre1"> val</strong> classifier = <strong class="calibre1">new</strong> LogisticRegression()<br class="title-page-name"/>                        .setMaxIter(500)          <br class="title-page-name"/>                        .setTol(1E-4)                                                                                                  <br class="title-page-name"/>                        .setFitIntercept(<strong class="calibre1">true</strong>)<br class="title-page-name"/>                        .setStandardization(true) <br class="title-page-name"/>                        .setAggregationDepth(50) <br class="title-page-name"/>                        .setRegParam(0.0001) <br class="title-page-name"/>                        .setElasticNetParam(0.01)
</pre>
<p class="mce-root"><strong class="calibre1">Step 5. Instantiate the OVTR classifier</strong> - Now instantiate an OVTR classifier to convert the multiclass classification problem into multiple binary classifications as follows:</p>
<pre class="calibre19">
val ovr = new OneVsRest().setClassifier(classifier)
</pre>
<p class="mce-root">Here <kbd class="calibre11">classifier</kbd> is the Logistic Regression estimator. Now it's time to train the model.</p>
<p class="mce-root"><strong class="calibre1">Step 6. Train the multiclass model</strong> - Let's train the model using the training set as follows:</p>
<pre class="calibre19">
val ovrModel = ovr.fit(train)
</pre>
<p class="mce-root"><strong class="calibre1">Step 7. Score the model on the test set</strong> - We can score the model on test data using the transformer (that is, <kbd class="calibre11">ovrModel</kbd>) as follows:</p>
<pre class="calibre19">
val predictions = ovrModel.transform(test)
</pre>
<p class="mce-root"><strong class="calibre1">Step 8. Evaluate the model</strong> - In this step, we will predict the labels for the characters in the first column. But before that we need instantiate an <kbd class="calibre11">evaluator</kbd> to compute the classification performance metrics such as accuracy, precision, recall, and <kbd class="calibre11">f1</kbd> measure as follows:</p>
<pre class="calibre19">
val evaluator = new MulticlassClassificationEvaluator()<br class="title-page-name"/>                           .setLabelCol("label")<br class="title-page-name"/>                           .setPredictionCol("prediction")    <br class="title-page-name"/>val evaluator1 = evaluator.setMetricName("accuracy")<br class="title-page-name"/>val evaluator2 = evaluator.setMetricName("weightedPrecision")<br class="title-page-name"/>val evaluator3 = evaluator.setMetricName("weightedRecall")<br class="title-page-name"/>val evaluator4 = evaluator.setMetricName("f1")
</pre>
<p class="mce-root"><strong class="calibre1">Step 9. Compute performance metrics</strong> - Compute the classification accuracy, precision, recall, <kbd class="calibre11">f1</kbd> measure, and error on test data as follows:</p>
<pre class="calibre19">
val accuracy = evaluator1.evaluate(predictions)<br class="title-page-name"/>val precision = evaluator2.evaluate(predictions)<br class="title-page-name"/>val recall = evaluator3.evaluate(predictions)<br class="title-page-name"/>val f1 = evaluator4.evaluate(predictions)
</pre>
<p class="mce-root"><strong class="calibre1">Step 10.</strong> Print the performance metrics:</p>
<pre class="calibre19">
println("Accuracy = " + accuracy)<br class="title-page-name"/>println("Precision = " + precision)<br class="title-page-name"/>println("Recall = " + recall)<br class="title-page-name"/>println("F1 = " + f1)<br class="title-page-name"/>println(s"Test Error = ${1 - accuracy}")
</pre>
<p class="mce-root">You should observe the value as follows:</p>
<pre class="calibre19">
<strong class="calibre1">Accuracy = 0.5217246545696688</strong><br class="title-page-name"/><strong class="calibre1">Precision = 0.488360500637862</strong><br class="title-page-name"/><strong class="calibre1">Recall = 0.5217246545696688</strong><br class="title-page-name"/><strong class="calibre1">F1 = 0.4695649096879411</strong><br class="title-page-name"/><strong class="calibre1">Test Error = 0.47827534543033123</strong>
</pre>
<p class="mce-root"><strong class="calibre1">Step 11.</strong> Stop the Spark session:</p>
<pre class="calibre19">
spark.stop() // Stop Spark session
</pre>
<p class="mce-root">This way, we can convert a multinomial classification problem into multiple binary classifications problem without sacrificing the problem types. However, from step 10, we can observe that the classification accuracy is not good at all. It might be because of several reasons such as the nature of the dataset we used to train the model. Also even more importantly, we did not tune the hyperparameters while training the Logistic Regression model. Moreover, while performing the transformation, the OVTR had to sacrifice some accuracy.</p>


            </article>

            
        </section>
    

        <section id="CEKB41-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Hierarchical classification</h1>
                
            
            <article>
                
<p class="mce-root">In a hierarchical classification task, the classification problem can be resolved by dividing the output space into a tree. In that tree, parent nodes are divided into multiple child nodes. The process persists until each child node depicts a single class. Several methods have been proposed based on the hierarchical classification technique. Computer vision is an example of such areas where recognizing pictures or written text are something that use hierarchical processing does. An extensive discussion on this classifier is out of the scope of this chapter.</p>


            </article>

            
        </section>
    

        <section id="CFIRM1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Extension from binary</h1>
                
            
            <article>
                
<p class="mce-root">This is a technique for extending existing binary classifiers to solve multiclass classification problems. To address multiclass classification problems, several algorithms have been proposed and developed based on neural networks, DTs, Random forest, k-nearest neighbors, Na<span>i</span>ve Bayes, and SVM. In the following sections, we will discuss the Na<span>i</span>ve Bayes and the DT algorithm as two representatives of this category.</p>
<p class="mce-root"><span>Now, before starting to solve multiclass classification problems using Naive</span> <span>Bayes algorithms, let's have a brief overview of Bayesian inference in the next section.</span></p>


            </article>

            
        </section>
    

        <section id="CGHC81-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Bayesian inference</h1>
                
            
            <article>
                
<p class="mce-root">In this section, we will briefly discuss <strong class="calibre1">Bayesian inference</strong> (<strong class="calibre1">BI</strong>) and its underlying theory. Readers will be familiar with this concept from the theoretical and computational viewpoints.</p>


            </article>

            
        </section>
    

        <section id="CHFSQ1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">An overview of Bayesian inference</h1>
                
            
            <article>
                
<p class="mce-root">Bayesian inference is a statistical method based on Bayes theorem. It is used to update the probability of a hypothesis (as a strong statistical proof) so that statistical models can repeatedly update towards more accurate learning. In other words, all types of uncertainty are revealed in terms of statistical probability in the Bayesian inference approach. This is an important technique in theoretical as well as mathematical statistics. We will discuss the Bayes theorem broadly in a later section.</p>
<p class="mce-root">Furthermore, Bayesian updating is predominantly foremost in the incremental learning and dynamic analysis of the sequence of the dataset. For example time series analysis, genome sequencing in biomedical data analytics, science, engineering, philosophy, and law are some example where Bayesian inference is used widely. From the philosophical perspective and decision theory, Bayesian inference is strongly correlated to predictive probability. This theory, however, is more formally known as the <strong class="calibre1">Bayesian probability</strong>.</p>


            </article>

            
        </section>
    

        <section id="CIEDC1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">What is inference?</h1>
                
            
            <article>
                
<p class="mce-root">Inference or model evaluation is the process of updating probabilities of the <span>denouement</span> derived from the model at the end. As a result, all the probabilistic evidence is eventually known against the observation at hand so that observations can be updated while using the Bayesian model for classification analysis. Later on, this information is fetched to the Bayesian model by instantiating the consistency against all the observations in the dataset. The rules that are fetched to the model are referred to as prior probabilities where <span>a probability is assessed before making reference to certain relevant observations, especially subjectively or on the assumption that all possible outcomes be given the same probability.</span> Then beliefs are computed when all the evidence is known as posterior probabilities. These posterior probabilities reflect the levels of hypothesis computed based on updated evidence.</p>
<p class="mce-root">The Bayes theorem is used to compute the posterior probabilities that signify a consequence of two antecedents. Based on these antecedents, a prior probability and a likelihood function are derived from a statistical model for the new data for model adaptability. We will further discuss the Bayes theorem in a later section.</p>


            </article>

            
        </section>
    

        <section id="CJCTU1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How does it work?</h1>
                
            
            <article>
                
<p class="mce-root">Here we discuss a general setup for a statistical inference problem. At the first place, from the data, we estimate the desired quantity and there might be unknown quantities too that we would like to estimate. It could be simply a response variable or predicted variable, a class, a label, or simply a number. If you are familiar with the <em class="calibre8">frequentist</em> approach, you might know that in this approach the unknown quantity say <kbd class="calibre11">θ</kbd> is assumed to be a fixed (nonrandom) quantity that is to be estimated by the observed data.</p>
<p class="mce-root">However, in the Bayesian framework, an unknown quantity say <kbd class="calibre11">θ</kbd> is treated as a random variable. More specifically, it is assumed that we have an initial guess about the distribution of <kbd class="calibre11">θ</kbd>, which is commonly referred to as the <strong class="calibre1">prior distribution</strong>. Now, after observing some data, the distribution of <kbd class="calibre11">θ</kbd> is updated. This step is usually performed using Bayes' rule (for more details, refer to the next section). This is why this approach is called the Bayesian approach. However, in short, from the prior distribution, we can compute predictive distributions for future observations.</p>
<p class="mce-root">This unpretentious process can be justified as the appropriate methodology to uncertain inference with the help of numerous arguments. However, the consistency is maintained with the clear principles of the rationality of these arguments. In spite of this strong mathematical evidence, many machine learning practitioners are uncomfortable with, and a bit reluctant of, using the Bayesian approach. The reason behind this is that often they view the selection of a posterior probability or prior as being arbitrary and subjective; however, in reality, this is subjective but not arbitrary.</p>
<div class="packt_tip">Inappropriately, many Bayesians don't really think in true Bayesian terms. One can, therefore, find many pseudo-Bayesian procedures in the literature, in which models and priors are used that cannot be taken seriously as expressions of prior belief. There may also be computational difficulties with the Bayesian approach. Many of these can be addressed using <strong class="calibre27">Markov chain Monte Carlo</strong> methods, which are another main focus of my research. The details of this approach will be clearer as you go through this chapter.</div>


            </article>

            
        </section>
    

        <section id="CKBEG1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Naive Bayes</h1>
                
            
            <article>
                
<p class="mce-root">In ML, <strong class="calibre1">Naive Bayes</strong> (<strong class="calibre1">NB</strong>) is an example of the probabilistic classifier based on the well-known Bayes' theorem with strong independence assumptions between the features. We will discuss Na<span>i</span>ve Bayes in detail in this section.</p>


            </article>

            
        </section>
    

        <section id="CL9V21-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">An overview of Bayes' theorem</h1>
                
            
            <article>
                
<p class="mce-root">In probability theory, <strong class="calibre1">Bayes' theorem</strong> describes the probability of an event based on a prior knowledge of conditions that is related to that certain event. This is a theorem of probability originally stated by the Reverend Thomas Bayes. In other words, it can be seen as a way of understanding how the probability theory is true and affected by a new piece of information. For example, if cancer is related to age, the information about <em class="calibre8">age</em> can be used to assess the probability that a person might have cancer more accurately<em class="calibre8">.</em></p>
<p class="mce-root">Bayes' theorem is stated mathematically as the following equation:</p>
<div class="cdpaligncenter"><img class="image-border179" src="../images/00241.gif"/></div>
<p class="cdpalignleft1">In the preceding equation, <em class="calibre8">A</em> and <em class="calibre8">B</em> are events with <em class="calibre8">P (B) ≠ 0,</em> and the other terms can be described as follows:</p>
<ul class="calibre9">
<li class="mce-root1"><em class="calibre8">P</em>(<em class="calibre8">A</em>) and <em class="calibre8">P</em>(<em class="calibre8">B</em>) are the probabilities of observing <em class="calibre8">A</em> and <em class="calibre8">B</em> without regard to each other (that is, independence)</li>
<li class="mce-root1"><em class="calibre8">P</em>(<em class="calibre8">A</em> | <em class="calibre8">B</em>) is the conditional probability of observing event <em class="calibre8">A</em> given that <em class="calibre8">B</em> is true</li>
<li class="mce-root1"><em class="calibre8">P</em>(<em class="calibre8">B</em>| <em class="calibre8">A</em>) is the conditional probability of observing event <em class="calibre8">B</em> given that <em class="calibre8">A</em> is true</li>
</ul>
<p class="mce-root">As you probably know, a well-known Harvard study shows that only 10% of happy people are rich. However, you might think that this statistic is very compelling but you might be somewhat interested in knowing the percentage of rich people are also really happy<em class="calibre8">.</em> Bayes' theorem helps you out on how to calculate this reserving statistic using two additional clues:</p>
<ol class="calibre14">
<li value="1" class="mce-root1">The percentage of people overall who are happy, that is, <em class="calibre8">P(A).</em></li>
<li value="2" class="mce-root1">The percentage of people overall who are rich, that is <em class="calibre8">P(B).</em></li>
</ol>
<p class="mce-root">The key idea behind Bayes' theorem is reversing the statistic considering the overall rates<strong class="calibre1">.</strong> Suppose that the following pieces of information are available as a prior:<strong class="calibre1"><br class="title-page-name"/></strong></p>
<ol class="calibre14">
<li value="1" class="mce-root1">40% of people are happy and <em class="calibre8">=&gt; P(A).</em></li>
<li value="2" class="mce-root1">5% of people are rich <em class="calibre8">=&gt; P(B).</em></li>
</ol>
<p class="mce-root">Now let's consider that the Harvard study is correct, that is, <em class="calibre8">P(B|A) = 10%</em>. Now the fraction of rich people who are happy, that is, <em class="calibre8">P(A | B),</em> can be calculated as follows:</p>
<p class="mce-root"><em class="calibre8">P(A|B) = {P(A)* P(B| A)}/ P(B) = (40%*10%)/5% = 80%</em></p>
<p class="mce-root">Consequently, a majority of the people are also happy! Nice. To make it clearer, now let's assume the population of the whole world is 1,000 for simplicity. Then, according to our calculation, there are two facts that exist:</p>
<ul class="calibre9">
<li class="mce-root1">Fact 1: This tells us 400 people are happy, and the Harvard study tells us that 40 of these happy people are also rich.</li>
<li class="mce-root1">Fact 2: There are 50 rich people altogether, and so the fraction who are happy is 40/50 = 80%.</li>
</ul>
<div class="packt_tip">This proves the Bayes theorem and its effectiveness. However, more comprehensive examples can be found at <a href="https://onlinecourses.science.psu.edu/stat414/node/43" class="calibre21">https://onlinecourses.science.psu.edu/stat414/node/43</a>.</div>


            </article>

            
        </section>
    

        <section id="CM8FK1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">My name is Bayes, Naive Bayes</h1>
                
            
            <article>
                
<p class="mce-root">I'm Bayes, Naive Bayes (NB). I'm a successful classifier based upon the principle of <strong class="calibre1">maximum a posteriori</strong> (<strong class="calibre1">MAP</strong>). As a classifier, I am highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. I have several properties, for example, I am computationally faster, if you can hire me to classify something I'm simple to implement, and I can work well with high-dimensional datasets. Moreover, I can handle missing values in your dataset. Nevertheless, I'm adaptable since the model can be modified with new training data without rebuilding the model.</p>
<div class="packt_infobox">In Bayesian statistics, a MAP estimate is an estimate of an unknown quantity that equals the mode of the posterior distribution. The MAP estimate can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data.</div>
<p class="mce-root">Sounds something similar to James Bond movies? Well, you/we can think a classifer as agent 007, right? Just kidding. I believe I am not as the <span><span>parameters of the Naive Bayes classifier such as priori and conditional probabilities are learned or rather determined using a deterministic set of steps: this involves two very trivial operations that can be blindingly fast on modern computers, that is, counting and dividing. There is no <em class="calibre8">iteration</em>. There is no <em class="calibre8">epoch</em>. There is <em class="calibre8">no optimization of a cost equation</em> (which can be complex, of cubic order on an average or at least of square order complexity). There is no <em class="calibre8">error back-propagation</em>. There is no operation(s) involving <em class="calibre8">solving a matrix equation</em>. These make Naive Bayes and its overall training faster.<br class="title-page-name"/></span></span></p>
<p class="mce-root">However, before hiring this agent, you/we can discover his pros and cons so that we can use this agent like a trump card by utilizing it's best only. Well, here's table summarizing the pros and cons of this agent:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Agent</strong></td>
<td class="calibre7"><strong class="calibre1">Pros</strong></td>
<td class="calibre7"><strong class="calibre1">Cons</strong></td>
<td class="calibre7"><strong class="calibre1">Better at</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root"> </p>
<p class="mce-root"> </p>
<p class="mce-root"><strong class="calibre1">Naive Bayes (NB)</strong></p>
</td>
<td class="calibre7">
<p class="mce-root">- Computationally fast</p>
<p class="mce-root">- Simple to implement</p>
<p class="mce-root">- Works well with high dimensions</p>
<p class="mce-root">- Can handle missing values</p>
<p class="mce-root">- Requires a small amount of data to train the model<br class="title-page-name"/>
- It is scalable</p>
<p class="mce-root">- Is adaptable since the model can be modified with new training data without rebuilding the model</p>
</td>
<td class="calibre7">
<p class="mce-root">- Relies on independence assumptions and so performs badly if the assumption does not meet</p>
<p class="mce-root">- Relatively low accuracy</p>
<p class="mce-root">- If you have no occurrences of a class label and a certain attribute value together then the frequency-based probability estimate will be zero</p>
</td>
<td class="calibre7">
<p class="mce-root">- When data has lots of missing values</p>
<p class="mce-root">- When dependencies of features from each other are similar between features</p>
<p class="mce-root">- Spam filtering and classification</p>
<p class="mce-root">- Classifying a news article about technology, politics, sports, and so on.</p>
<p class="mce-root">- Text mining</p>
</td>
</tr>
</tbody>
</table>
<div class="cdpaligncenter1"><strong class="calibre1">Table 1:</strong> Pros and the cons of the Naive Bayes algorithm</div>


            </article>

            
        </section>
    

        <section>

                            <header id="CN7062-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">Building a scalable classifier with NB</h1>
                
            
            <article>
                
<p class="mce-root">In this section, we will see a step-by-step example using <strong class="calibre1">Naive Bayes</strong> (<strong class="calibre1">NB</strong>) algorithm. As already stated, NB is highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. This scalability has enabled the Spark community to make predictive analytics on large-scale datasets using this algorithm. The current implementation of NB in Spark MLlib supports both the multinomial NB and Bernoulli NB.</p>
<div class="packt_infobox">Bernoulli NB is useful if the feature vectors are binary. One application would be text classification with a bag of words (BOW) approach. On the other hand, multinomial NB is typically used for discrete counts. For example, if we have a text classification problem, we can take the idea of Bernoulli trials one step further and instead of BOW in a document we can use the frequency count in a document.</div>
<p class="mce-root">In this section, we will see how to predict the digits from the <strong class="calibre1">Pen-Based Recognition of Handwritten Digits</strong> dataset by incorporating Spark machine learning APIs including Spark MLlib, Spark ML, and Spark SQL:</p>
<p class="mce-root"><strong class="calibre1">Step 1. Data collection, preprocessing, and exploration</strong> - The Pen-based recognition of handwritten digits dataset was downloaded from the UCI Machine Learning Repository at <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits" class="calibre10">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/pendigits.</a> This dataset was generated after collecting around 250 digit samples each from 44 writers, correlated to the location of the pen at fixed time intervals of 100 milliseconds. Each digit was then written inside a 500 x 500 pixel box. Finally, those images were scaled to an integer value between 0 and 100 to create consistent scaling between each observation. A well-known spatial resampling technique was used to obtain 3 and 8 regularly spaced points on an arc trajectory. A sample image along with the lines from point to point can be visualized by plotting the 3 or 8 sampled points based on their (x, y) coordinates; it looks like what is shown in the following table:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7">Set</td>
<td class="calibre7">'0'</td>
<td class="calibre7">'1'</td>
<td class="calibre7">'2'</td>
<td class="calibre7">'3'</td>
<td class="calibre7">'4'</td>
<td class="calibre7">'5'</td>
<td class="calibre7">'6'</td>
<td class="calibre7">'7'</td>
<td class="calibre7">'8'</td>
<td class="calibre7">'9'</td>
<td class="calibre7">Total</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Training</td>
<td class="calibre7">780</td>
<td class="calibre7">779</td>
<td class="calibre7">780</td>
<td class="calibre7">719</td>
<td class="calibre7">780</td>
<td class="calibre7">720</td>
<td class="calibre7">720</td>
<td class="calibre7">778</td>
<td class="calibre7">718</td>
<td class="calibre7">719</td>
<td class="calibre7">7493</td>
</tr>
<tr class="calibre6">
<td class="calibre7">Test</td>
<td class="calibre7">363</td>
<td class="calibre7">364</td>
<td class="calibre7">364</td>
<td class="calibre7">336</td>
<td class="calibre7">364</td>
<td class="calibre7">335</td>
<td class="calibre7">336</td>
<td class="calibre7">364</td>
<td class="calibre7">335</td>
<td class="calibre7">336</td>
<td class="calibre7">3497</td>
</tr>
</tbody>
</table>
<div class="cdpaligncenter1">Table 2: Number of digits used for the training and the test set</div>
<p class="mce-root">As shown in the preceding table, the training set consists of samples written by 30 writers and the testing set consists of samples written by 14 writers.</p>
<div class="cdpaligncenter"><img class="image-border180" src="../images/00130.jpeg"/></div>
<div class="cdpaligncenter1">Figure 4: Example of digit 3 and 8 respectively</div>
<p class="mce-root">More on this dataset can be found at <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.names" class="calibre10">http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits-orig.names</a>. A digital representation of a sample snapshot of the dataset is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border181" src="../images/00149.gif"/></div>
<div class="cdpaligncenter1">Figure 5: A snap of the 20 rows of the hand-written digit dataset</div>
<p class="mce-root">Now to predict the dependent variable (that is, label) using the independent variables (that is, features), we need to train a multiclass classifier since, as shown previously, the dataset now has nine classes, that is, nine handwritten digits. For the prediction, we will use the Naive Bayes classifier and evaluate the model's performance.</p>
<p class="mce-root"><strong class="calibre1">Step 2.</strong> Load the required library and packages:</p>
<pre class="calibre19">
import org.apache.spark.ml.classification.NaiveBayes<br class="title-page-name"/>import org.apache.spark.ml.evaluation<br class="title-page-name"/>                                 .MulticlassClassificationEvaluator<br class="title-page-name"/>import org.apache.spark.sql.SparkSession
</pre>
<div class="cdpalignleft">
<p class="mce-root"><strong class="calibre1">Step 3.</strong> Create an active Spark session:</p>
<pre class="calibre19">
val spark = SparkSession<br class="title-page-name"/>              .builder<br class="title-page-name"/>              .master("local[*]")<br class="title-page-name"/>              .config("spark.sql.warehouse.dir", "/home/exp/")<br class="title-page-name"/>              .appName(s"NaiveBayes")<br class="title-page-name"/>              .getOrCreate()
</pre></div>
<div class="cdpalignleft">
<p class="mce-root">Note that here the master URL has been set as <kbd class="calibre11">local[*]</kbd>, which means all the cores of your machine will be used for processing the Spark job. You should set SQL warehouse accordingly and other configuration parameter based on the requirements.</p>
<p class="mce-root"><strong class="calibre1">Step 4. Create the DataFrame</strong> - Load the data stored in LIBSVM format as a DataFrame:</p>
<pre class="calibre19">
val data = spark.read.format("libsvm")<br class="title-page-name"/>                     .load("data/pendigits.data")
</pre>
<p class="mce-root">For digits classification, the input feature vectors are usually sparse, and sparse vectors should be supplied as input to take advantage of sparsity. Since the training data is only used once, and moreover the size of the dataset is relatively smaller (that is, few MBs), we can cache it if you use the DataFrame more than once.</p>
</div>
<div class="cdpalignleft">
<p class="mce-root"><strong class="calibre1">Step 5. Prepare the training and test set</strong> - Split the data into training and test sets (25% held out for testing):</p>
<pre class="calibre19">
val Array(trainingData, testData) = data<br class="title-page-name"/>                  .randomSplit(Array(0.75, 0.25), seed = 12345L)
</pre>
<p class="mce-root"><strong class="calibre1">Step 6. Train the Naive Bayes model</strong> - Train a Naive Bayes model using the training set as follows:</p>
<pre class="calibre19">
val nb = new NaiveBayes()<br class="title-page-name"/>val model = nb.fit(trainingData)
</pre>
<p class="mce-root"><strong class="calibre1">Step 7. Calculate the prediction on the test set</strong> - Calculate the prediction using the model transformer and finally show the prediction against each label as follows:</p>
<pre class="calibre19">
val predictions = model.transform(testData)<br class="title-page-name"/>predictions.show()
</pre></div>
<div class="cdpaligncenter1"><img class="image-border182" src="../images/00189.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 6:</strong> Prediction against each label (that is, each digit)</div>
<div class="cdpalignleft">
<p class="mce-root">As you can see in the preceding figure, some labels were predicted accurately and some of them were wrongly. Again we need to know the weighted accuracy, precision, recall and f1 measures without evaluating the model naively.</p>
<p class="mce-root"><strong class="calibre1">Step 8. Evaluate the model</strong> - Select the prediction and the true label to compute test error and classification performance metrics such as accuracy, precision, recall, and f1 measure as follows:</p>
<pre class="calibre19">
val evaluator = new MulticlassClassificationEvaluator()<br class="title-page-name"/>                           .setLabelCol("label")<br class="title-page-name"/>                           .setPredictionCol("prediction")    <br class="title-page-name"/>val evaluator1 = evaluator.setMetricName("accuracy")<br class="title-page-name"/>val evaluator2 = evaluator.setMetricName("weightedPrecision")<br class="title-page-name"/>val evaluator3 = evaluator.setMetricName("weightedRecall")<br class="title-page-name"/>val evaluator4 = evaluator.setMetricName("f1")
</pre>
<p class="mce-root"><strong class="calibre1">Step 9. Compute the performance metrics</strong> - Compute the classification accuracy, precision, recall, f1 measure, and error on test data as follows:</p>
<pre class="calibre19">
val accuracy = evaluator1.evaluate(predictions)<br class="title-page-name"/>val precision = evaluator2.evaluate(predictions)<br class="title-page-name"/>val recall = evaluator3.evaluate(predictions)<br class="title-page-name"/>val f1 = evaluator4.evaluate(predictions)
</pre>
<p class="mce-root"><strong class="calibre1">Step 10.</strong> Print the performance metrics:</p>
<pre class="calibre19">
println("Accuracy = " + accuracy)<br class="title-page-name"/>println("Precision = " + precision)<br class="title-page-name"/>println("Recall = " + recall)<br class="title-page-name"/>println("F1 = " + f1)<br class="title-page-name"/>println(s"Test Error = ${1 - accuracy}")
</pre>
<p class="mce-root">You should observe values as follows:</p>
<pre class="calibre19">
<strong class="calibre1">Accuracy = 0.8284365162644282</strong><br class="title-page-name"/><strong class="calibre1">Precision = 0.8361211320692463</strong><br class="title-page-name"/><strong class="calibre1">Recall = 0.828436516264428</strong><br class="title-page-name"/><strong class="calibre1">F1 = 0.8271828540349192</strong><br class="title-page-name"/><strong class="calibre1">Test Error = 0.17156348373557184</strong>
</pre>
<p class="mce-root">The performance is not that bad. However, you can still increase the classification accuracy by performing hyperparameter tuning. There are further opportunities to improve the prediction accuracy by selecting appropriate algorithms (that is, classifier or regressor) through cross-validation and train split, which will be discussed in the following section.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header id="CO5GO2-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">Tune me up!</h1>
                
            
            <article>
                
<p class="mce-root">You already know my pros and cons, I have a con that is, my classification accuracy is relatively low. However, if you tune me up, I can perform much better. Well, should we trust Naive Bayes? If so, shouldn't we look at how to increase the prediction performance of this guy? Let's say using the WebSpam dataset. At first, we should observe the performance of the NB model, and after that we will see how to increase the performance using the cross-validation technique.</p>
<p class="mce-root">The WebSpam dataset that downloaded from <a href="http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2" class="calibre10">http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2</a> contains features and corresponding labels, that is, spam or ham. Therefore, this is a supervised machine learning problem, and the task here is to predict whether a given message is spam or ham (that is, not spam). The original dataset size is 23.5 GB, where the classes are labeled as +1 or -1 (that is, a binary classification problem). Later on, we replaced -1 with 0.0 and +1 with 1.0 since Naive Bayes does not permit using signed integers. The modified dataset is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border183" src="../images/00054.gif"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 7:</strong> A snapshot of the 20 rows of the WebSpam dataset</div>
<p class="mce-root">At first, we need to import necessary packages as follows:</p>
<pre class="calibre19">
import org.apache.spark.ml.classification.NaiveBayes<br class="title-page-name"/>import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator<br class="title-page-name"/>import org.apache.spark.sql.SparkSession<br class="title-page-name"/>import org.apache.spark.ml.Pipeline;<br class="title-page-name"/>import org.apache.spark.ml.PipelineStage;<br class="title-page-name"/>import org.apache.spark.ml.classification.LogisticRegression<br class="title-page-name"/>import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator<br class="title-page-name"/>import org.apache.spark.ml.feature.{HashingTF, Tokenizer}<br class="title-page-name"/>import org.apache.spark.ml.linalg.Vector<br class="title-page-name"/>import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}
</pre>
<p class="mce-root">Now create the Spark Session as the entry point to the code as follows:</p>
<pre class="calibre19">
val spark = SparkSession<br class="title-page-name"/>      .builder<br class="title-page-name"/>      .master("local[*]")<br class="title-page-name"/>      .config("spark.sql.warehouse.dir", "/home/exp/")<br class="title-page-name"/>      .appName("Tuned NaiveBayes")<br class="title-page-name"/>      .getOrCreate()
</pre>
<p class="mce-root">Let's load the WebSpam dataset and prepare the training set to train the Naive Bayes model as follows:</p>
<pre class="calibre19">
// Load the data stored in LIBSVM format as a DataFrame.<br class="title-page-name"/> val data = spark.read.format("libsvm").load("hdfs://data/ webspam_wc_normalized_trigram.svm")<br class="title-page-name"/> // Split the data into training and test sets (30% held out for testing)<br class="title-page-name"/> val Array(trainingData, testData) = data.randomSplit(Array(0.75, 0.25), seed = 12345L)<br class="title-page-name"/> // Train a NaiveBayes model with using the training set<br class="title-page-name"/> val nb = new NaiveBayes().setSmoothing(0.00001)<br class="title-page-name"/> val model = nb.fit(trainingData)
</pre>
<p class="mce-root">In the preceding code, setting the seed is required for reproducibility. Now let's make the prediction on the validation set as follows:</p>
<pre class="calibre19">
val predictions = model.transform(testData)<br class="title-page-name"/>predictions.show()
</pre>
<p class="mce-root">Now let's obtain <kbd class="calibre11">evaluator</kbd> and compute the classification performance metrics like accuracy, precision, recall, and <kbd class="calibre11">f1</kbd> measure as follows:</p>
<pre class="calibre19">
val evaluator = new MulticlassClassificationEvaluator()<br class="title-page-name"/>                    .setLabelCol("label")<br class="title-page-name"/>                    .setPredictionCol("prediction")    <br class="title-page-name"/>val evaluator1 = evaluator.setMetricName("accuracy")<br class="title-page-name"/>val evaluator2 = evaluator.setMetricName("weightedPrecision")<br class="title-page-name"/>val evaluator3 = evaluator.setMetricName("weightedRecall")<br class="title-page-name"/>val evaluator4 = evaluator.setMetricName("f1")
</pre>
<p class="mce-root">Now let's compute and print the performance metrics:</p>
<pre class="calibre19">
val accuracy = evaluator1.evaluate(predictions)<br class="title-page-name"/>val precision = evaluator2.evaluate(predictions)<br class="title-page-name"/>val recall = evaluator3.evaluate(predictions)<br class="title-page-name"/>val f1 = evaluator4.evaluate(predictions)   <br class="title-page-name"/>// Print the performance metrics<br class="title-page-name"/>println("Accuracy = " + accuracy)<br class="title-page-name"/>println("Precision = " + precision)<br class="title-page-name"/>println("Recall = " + recall)<br class="title-page-name"/>println("F1 = " + f1)<br class="title-page-name"/>println(s"Test Error = ${1 - accuracy}")
</pre>
<p class="mce-root">You should receive the following output:</p>
<pre class="calibre19">
<strong class="calibre1">Accuracy = 0.8839357429715676</strong><br class="title-page-name"/><strong class="calibre1">Precision = 0.86393574297188752</strong><br class="title-page-name"/><strong class="calibre1">Recall = 0.8739357429718876</strong><br class="title-page-name"/><strong class="calibre1">F1 = 0.8739357429718876</strong><br class="title-page-name"/><strong class="calibre1">Test Error = 0.11606425702843237</strong>
</pre>
<p class="mce-root">Although the accuracy is at a satisfactory level, we can further improve it by applying the cross-validation technique. The technique goes as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Create a pipeline by chaining an NB estimator as the only stage of the pipeline</li>
<li class="mce-root1">Now prepare the param grid for tuning</li>
<li class="mce-root1">Perform the 10-fold cross-validation</li>
<li class="mce-root1">Now fit the model using the training set</li>
<li class="mce-root1">Compute the prediction on the validation set</li>
</ul>
<p class="mce-root">The first step in model tuning techniques such as cross-validation is pipeline creation. A pipeline can be created by chaining a transformer, an estimator, and related parameters.</p>
<p class="mce-root"><strong class="calibre1">Step 1. Pipeline creation</strong> - Let's create a Naive Bayes estimator (<kbd class="calibre11">nb</kbd> is an estimator in the following case) and create a pipeline by chaining the estimator as follows:</p>
<pre class="calibre19">
val nb = new NaiveBayes().setSmoothing(00001)<br class="title-page-name"/>val pipeline = new Pipeline().setStages(Array(nb))
</pre>
<div class="packt_tip1">A pipeline can be considered as the data workflow system for training and prediction using the model. ML pipelines provide a uniform set of high-level APIs built on top of <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html" class="calibre21">DataFrames</a> that help users create and tune practical machine learning pipelines. DataFrame, transformer, estimator, pipeline, and parameter are the five most important components in Pipeline creation. For more on Pipeline, interested readers should refer to <a href="https://spark.apache.org/docs/latest/ml-pipeline.html" class="calibre21">https://spark.apache.org/docs/latest/ml-pipeline.html</a></div>
<p class="mce-root">In the earlier case, the only stage in our pipeline is an estimator that is an algorithm for fitting on a DataFrame to produce a transformer to make sure the training is carried out successfully.</p>
<p class="mce-root"><strong class="calibre1">Step 2. Creating grid parameters</strong> - Let's use <kbd class="calibre11">ParamGridBuilder</kbd> to construct a grid of parameters to search over:</p>
<pre class="calibre19">
val paramGrid = new ParamGridBuilder()<br class="title-page-name"/>              .addGrid(nb.smoothing, Array(0.001, 0.0001))<br class="title-page-name"/>              .build()
</pre>
<p class="mce-root"><strong class="calibre1">Step 3. Performing 10-fold cross-validation</strong> - We now treat the pipeline as an estimator, wrapping it in a cross-validator instance. This will allow us to jointly choose parameters for all Pipeline stages. A <kbd class="calibre11">CrossValidator</kbd> requires an estimator, a set of estimator <kbd class="calibre11">ParamMaps</kbd>, and an evaluator. Note that the evaluator here is a <kbd class="calibre11">BinaryClassificationEvaluator</kbd>, and its default metric is <kbd class="calibre11">areaUnderROC</kbd>. However, if you use the evaluator as <kbd class="calibre11">MultiClassClassificationEvaluator</kbd>, you will be able to use the other performance metrics as well:</p>
<pre class="calibre19">
val cv = new CrossValidator()<br class="title-page-name"/>            .setEstimator(pipeline)<br class="title-page-name"/>            .setEvaluator(new BinaryClassificationEvaluator)<br class="title-page-name"/>            .setEstimatorParamMaps(paramGrid)<br class="title-page-name"/>            .setNumFolds(10)  // Use 3+ in practice
</pre>
<p class="mce-root"><strong class="calibre1">Step 4.</strong> Fit the cross-validation model with the training set as follows:</p>
<pre class="calibre19">
val model = cv.fit(trainingData)
</pre>
<p class="mce-root"><strong class="calibre1">Step 5.</strong> Compute performance as follows:</p>
<pre class="calibre19">
val predictions = model.transform(validationData)<br class="title-page-name"/>predictions.show()
</pre>
<p class="mce-root"><strong class="calibre1">Step 6.</strong> Obtain the evaluator, compute the performance metrics, and display the results. Now let's obtain <kbd class="calibre11">evaluator</kbd> and compute the classification performance metrics such as accuracy, precision, recall, and f1 measure. Here <kbd class="calibre11">MultiClassClassificationEvaluator</kbd> will be used for accuracy, precision, recall, and f1 measure:</p>
<pre class="calibre19">
val evaluator = new MulticlassClassificationEvaluator()<br class="title-page-name"/>                            .setLabelCol("label")<br class="title-page-name"/>                            .setPredictionCol("prediction")    <br class="title-page-name"/>val evaluator1 = evaluator.setMetricName("accuracy")<br class="title-page-name"/>val evaluator2 = evaluator.setMetricName("weightedPrecision")<br class="title-page-name"/>val evaluator3 = evaluator.setMetricName("weightedRecall")<br class="title-page-name"/>val evaluator4 = evaluator.setMetricName("f1")
</pre>
<p class="mce-root">Now compute the classification accuracy, precision, recall, f1 measure, and error on test data as follows:</p>
<pre class="calibre19">
val accuracy = evaluator1.evaluate(predictions)<br class="title-page-name"/>val precision = evaluator2.evaluate(predictions)<br class="title-page-name"/>val recall = evaluator3.evaluate(predictions)<br class="title-page-name"/>val f1 = evaluator4.evaluate(predictions)
</pre>
<p class="mce-root">Now let's print the performance metrics:</p>
<pre class="calibre19">
println("Accuracy = " + accuracy)<br class="title-page-name"/>println("Precision = " + precision)<br class="title-page-name"/>println("Recall = " + recall)<br class="title-page-name"/>println("F1 = " + f1)<br class="title-page-name"/>println(s"Test Error = ${1 - accuracy}")
</pre>
<p class="mce-root">You should now receive the results as follows:</p>
<pre class="calibre19">
<strong class="calibre1">Accuracy = 0.9678714859437751</strong><br class="title-page-name"/><strong class="calibre1">Precision = 0.9686742518830365</strong><br class="title-page-name"/><strong class="calibre1">Recall = 0.9678714859437751</strong><br class="title-page-name"/><strong class="calibre1">F1 = 0.9676697179934564</strong><br class="title-page-name"/><strong class="calibre1">Test Error = 0.032128514056224855</strong>
</pre>
<p class="mce-root">Now this is much better compared to the previous one, right? Please note that you might receive a slightly different result due to the random split of the dataset and your platform.</p>


            </article>

            
        </section>
    

        <section id="CP41A1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The decision trees</h1>
                
            
            <article>
                
<p class="mce-root">In this section, we will discuss the DT algorithm in detail. A comparative analysis of Naive Bayes and DT will be discussed too. DTs are commonly considered as a supervised learning technique used for solving classification and regression tasks. <span>A DT is simply a decision support tool that uses a tree-like graph (or a model of decisions) and their possible consequences, including chance event outcomes, resource costs, and utility</span>. More technically, e<span>ach branch in a DT represents a possible decision, occurrence, or reaction in terms of statistical probability.</span></p>
<p class="mce-root">Compared to Naive Bayes, DT is a far more robust classification technique. The reason is that at first DT splits the features into training and test set. Then it produces a good generalization to infer the predicted labels or classes. Most interestingly, DT algorithm can handle both binary and multiclass classification problems.</p>
<div class="cdpaligncenter"><img class="image-border184" src="../images/00081.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 8:</strong> A sample decision tree on the admission test dataset using the Rattle package of R</div>
<p class="mce-root">For instance, in the preceding example figure, DTs learn from the admission data to approximate a sine curve with a set of <kbd class="calibre11">if...else</kbd> decision rules. The dataset contains the record of each student who applied for admission, say to an American university. Each record contains the graduate record exam score, CGPA score, and the rank of the column. Now we will have to predict who is competent based on these three features (variables). DTs can be used to solve this kind of problem after training the DT model and pruning unwanted branches of the tree. In general, a deeper tree signifies more complex decision rules and a better fitted model. Therefore, the deeper the tree, the more complex the decision rules and the more fitted the model.</p>
<div class="packt_tip">If you would like to draw the preceding figure, just run my R script, execute it on RStudio, and feed the admission dataset. The script and the dataset can be found in my GitHub repository at <a href="https://github.com/rezacsedu/AdmissionUsingDecisionTree" class="calibre21">https://github.com/rezacsedu/AdmissionUsingDecisionTree</a>.</div>


            </article>

            
        </section>
    

        <section id="CQ2HS1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Advantages and disadvantages of using DTs</h1>
                
            
            <article>
                
<p class="mce-root">Before hiring me, you can discover my pros and cons and when I work best from Table 3 so that you don't have any late regrets!</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Agent</strong></td>
<td class="calibre7"><strong class="calibre1">Pros</strong></td>
<td class="calibre7"><strong class="calibre1">Cons</strong></td>
<td class="calibre7"><strong class="calibre1">Better at</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Decision trees (DTs)</strong></td>
<td class="calibre7">
<p class="mce-root">-Simple to implement, train, and interpret</p>
<p class="mce-root">-Trees can be visualized</p>
<p class="mce-root">-Requires little data preparation</p>
<p class="mce-root">-Less model building and prediction time</p>
<p class="mce-root">-Can handle both numeric and categorical data</p>
<p class="mce-root">-Possible of validating the model using the statistical tests</p>
<p class="mce-root">-Robust against noise and missing values</p>
<p class="mce-root">-High accuracy</p>
</td>
<td class="calibre7">
<p class="mce-root">-Interpretation is hard with large and complex trees</p>
<p class="mce-root">-Duplication may occur within the same subtree</p>
<p class="mce-root">-Possible issues with diagonal decision boundaries</p>
<p class="mce-root">-DT learners can create overcomplex trees that do not generalize data well</p>
<p class="mce-root">-Sometimes DTs can be unstable because of small variants in the data</p>
<p class="mce-root">-Learning the DTs itself an NP-complete problem (aka. <span>nondeterministic polynomial time</span> -complete problem)</p>
<p class="mce-root">-DTs learners create biased trees if some classes dominate</p>
</td>
<td class="calibre7">
<p class="mce-root">-Targeting highly accurate classification</p>
<p class="mce-root">-Medical diagnosis and prognosis</p>
<p class="mce-root">-Credit risk analytics</p>
</td>
</tr>
</tbody>
</table>
<div class="cdpaligncenter1"><strong class="calibre1">Table 3:</strong> Pros and cons of the decision tree</div>


            </article>

            
        </section>
    

        <section id="CR12E1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Decision tree versus Naive Bayes</h1>
                
            
            <article>
                
<p class="mce-root">As stated in the preceding table, DTs are very easy to understand and debug because of their flexibility for training datasets. They will work with both classification as well as regression problems.</p>
<p class="mce-root">If you are trying to predict values out of categorical or continuous values, DTs will handle both problems. Consequently, if you just have tabular data, feed it to the DT and it will build the model toward classifying your data without any additional requirement for upfront or manual interventions. In summary, DTs are very simple to implement, train, and interpret. With very little data preparation, DTs can build the model with much less prediction time. As said earlier, they can handle both numeric and categorical data and are very robust against noise and missing values. They are very easy to validate the model using statistical tests. More interestingly, the constructed trees can be visualized. Overall, they provide very high accuracy.</p>
<p class="mce-root">However, on the downside, DTs sometimes tend to the overfitting problem for the training data. This means that you generally have to prune the tree and find an optimal one for better classification or regression accuracy. Moreover, duplication may occur within the same subtree. Sometimes it also creates issues with diagonal decision boundaries towards overfitting and underfitting. Furthermore, DT learners can create over-complex trees that do not generalize the data well this makes overall interpretation hard. DTs can be unstable because of small variants in the data, and as a result learning DT is itself an NP-complete problem. Finally, DT learners create biased trees if some classes dominate over others.</p>
<div class="packt_tip">Readers are suggested to refer to <em class="calibre25">Tables 1</em> and <em class="calibre25">3</em> to get a comparative summary between Naive Bayes and DTs.</div>
<p class="mce-root">On the other hand, there is a saying while using Naive Bayes: <em class="calibre8">NB requires you build a classification by hand</em>. There's no way to feed a bunch of tabular data to it, and it picks the best features for the classification. In this case, however, choosing the right features and features that matter is up to the user, that is, you. On the other hand, DTs will pick the best features from tabular data. Given this fact, you probably need to combine Naive Bayes with other statistical techniques to help toward best feature extraction and classify them later on. Alternatively, use DTs to get better accuracy in terms of precision, recall, and f1 measure. Another positive thing about Naive Bayes is that it will answer as a continuous classifier. However, the downside is that they are harder to debug and understand. Naive Bayes does quite well when the training data doesn't have good features with low amounts of data.</p>
<p class="mce-root">In summary, if you are trying to choose the better classifier from these two often times it is best to test each one to solve a problem. My recommendation would be to build a DT as well as a Naive Bayes classifier using the training data you have and then compare the performance using available performance metrics and then decide which one best solves your problem subject to the dataset nature.</p>


            </article>

            
        </section>
    

        <section>

                            <header id="CRVJ02-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">Building a scalable classifier with DT algorithm</h1>
                
            
            <article>
                
<p class="mce-root">As you have already seen, using the OVTR classifier we observed the following values of the performance metrics on the OCR dataset:</p>
<pre class="calibre19">
<strong class="calibre1">Accuracy = 0.5217246545696688</strong><br class="title-page-name"/><strong class="calibre1">Precision = 0.488360500637862</strong><br class="title-page-name"/><strong class="calibre1">Recall = 0.5217246545696688</strong><br class="title-page-name"/><strong class="calibre1">F1 = 0.4695649096879411</strong><br class="title-page-name"/><strong class="calibre1">Test Error = 0.47827534543033123</strong>
</pre>
<p class="mce-root">This signifies that the accuracy of the model on that dataset is very low. In this section, we will see how we could improve the performance using the DT classifier. An example with Spark 2.1.0 will be shown using the same OCR dataset. The example will have several steps including data loading, parsing, model training, and, finally, model evaluation.</p>
<p class="mce-root">Since we will be using the same dataset, to avoid redundancy, we will escape the dataset exploration step and will enter into the example:</p>
<p class="mce-root"><strong class="calibre1">Step 1.</strong> Load the required library and packages as follows:</p>
<pre class="calibre19">
import org.apache.spark.ml.Pipeline // for Pipeline creation<br class="title-page-name"/>import org.apache.spark.ml.classification<br class="title-page-name"/>                         .DecisionTreeClassificationModel <br class="title-page-name"/>import org.apache.spark.ml.classification.DecisionTreeClassifier <br class="title-page-name"/>import org.apache.spark.ml.evaluation<br class="title-page-name"/>                         .MulticlassClassificationEvaluator <br class="title-page-name"/>import org.apache.spark.ml.feature<br class="title-page-name"/>                         .{IndexToString, StringIndexer, VectorIndexer} <br class="title-page-name"/>import org.apache.spark.sql.SparkSession //For a Spark session
</pre>
<p class="mce-root"><strong class="calibre1">Step 2.</strong> Create an active Spark session as follows:</p>
<pre class="calibre19">
val spark = SparkSession<br class="title-page-name"/>              .builder<br class="title-page-name"/>              .master("local[*]")<br class="title-page-name"/>              .config("spark.sql.warehouse.dir", "/home/exp/")<br class="title-page-name"/>              .appName("DecisionTreeClassifier")<br class="title-page-name"/>              .getOrCreate()
</pre>
<p class="mce-root">Note that here the master URL has been set as <kbd class="calibre11">local[*]</kbd>, which means all the cores of your machine will be used for processing the Spark job. You should set SQL warehouse accordingly and other configuration parameter based on requirements.</p>
<p class="mce-root"><strong class="calibre1">Step 3. Create the DataFrame</strong> - Load the data stored in LIBSVM format as a DataFrame as follows:</p>
<pre class="calibre19">
val data = spark.read.format("libsvm").load("datab<br class="title-page-name"/>                             /Letterdata_libsvm.data")
</pre>
<p class="mce-root">For the classification of digits, the input feature vectors are usually sparse, and sparse vectors should be supplied as input to take advantage of the sparsity. Since the training data is only used once, and moreover the size of the dataset is relatively small (that is, a few MBs), we can cache it if you use the DataFrame more than once.</p>
<p class="mce-root"><strong class="calibre1">Step 4. Label indexing</strong> - Index the labels, adding metadata to the label column. Then let's fit on the whole dataset to include all labels in the index:</p>
<pre class="calibre19">
val labelIndexer = new StringIndexer()<br class="title-page-name"/>               .setInputCol("label")<br class="title-page-name"/>               .setOutputCol("indexedLabel")<br class="title-page-name"/>               .fit(data)
</pre>
<p class="mce-root"><strong class="calibre1">Step 5. Identifying categorical features</strong> - The following code segment automatically identifies categorical features and indexes them:</p>
<pre class="calibre19">
val featureIndexer = new VectorIndexer()<br class="title-page-name"/>              .setInputCol("features")<br class="title-page-name"/>              .setOutputCol("indexedFeatures")<br class="title-page-name"/>              .setMaxCategories(4)<br class="title-page-name"/>              .fit(data)
</pre>
<p class="mce-root">For this case, if the number of features is more than four distinct values, they will be treated as continuous.</p>
<p class="mce-root"><strong class="calibre1">Step 6. Prepare the training and test sets</strong> - Split the data into training and test sets (25% held out for testing):</p>
<pre class="calibre19">
val Array(trainingData, testData) = data.randomSplit<br class="title-page-name"/>                                      (Array(0.75, 0.25), 12345L)
</pre>
<p class="mce-root"><strong class="calibre1">Step 7.</strong> Train the DT model as follows:</p>
<pre class="calibre19">
val dt = new DecisionTreeClassifier()<br class="title-page-name"/>                     .setLabelCol("indexedLabel")<br class="title-page-name"/>                     .setFeaturesCol("indexedFeatures")
</pre>
<p class="mce-root"><strong class="calibre1">Step 8.</strong> Convert the indexed labels back to original labels as follows:</p>
<pre class="calibre19">
val labelConverter = new IndexToString()<br class="title-page-name"/>                .setInputCol("prediction")<br class="title-page-name"/>                .setOutputCol("predictedLabel")<br class="title-page-name"/>                .setLabels(labelIndexer.labels)
</pre>
<p class="mce-root"><strong class="calibre1">Step 9. Create a DT pipeline</strong> - Let's create a DT pipeline by changing the indexers, label converter and tree together:</p>
<pre class="calibre19">
val pipeline = new Pipeline().setStages(Array(labelIndexer,<br class="title-page-name"/>                              featureIndexer, dt, labelconverter))
</pre>
<p class="mce-root"><strong class="calibre1">Step 10. Running the indexers</strong> - Train the model using the transformer and run the indexers:</p>
<pre class="calibre19">
<strong class="calibre1">val</strong> model = pipeline.fit(trainingData)
</pre>
<p class="mce-root"><strong class="calibre1">Step 11. Calculate the prediction on the test set</strong> - Calculate the prediction using the model transformer and finally show the prediction against each label as follows:</p>
<pre class="calibre19">
val predictions = model.transform(testData)<br class="title-page-name"/>predictions.show()
</pre>
<div class="cdpaligncenter"><img class="image-border185" src="../images/00344.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 9:</strong> Prediction against each label (that is, each letter)</div>
<p class="mce-root">As you can see from the preceding figure, some labels were predicted accurately and some of them were predicted wrongly. However, we know the weighted accuracy, precision, recall, and f1 measures, but we need to evaluate the model first.</p>
<p class="mce-root"><strong class="calibre1">Step 12. Evaluate the model</strong> - Select the prediction and the true label to compute test error and classification performance metrics such as accuracy, precision, recall, and f1 measure as follows:</p>
<pre class="calibre19">
val evaluator = new MulticlassClassificationEvaluator()<br class="title-page-name"/>                             .setLabelCol("label")<br class="title-page-name"/>                             .setPredictionCol("prediction")    <br class="title-page-name"/>val evaluator1 = evaluator.setMetricName("accuracy")<br class="title-page-name"/>val evaluator2 = evaluator.setMetricName("weightedPrecision")<br class="title-page-name"/>val evaluator3 = evaluator.setMetricName("weightedRecall")<br class="title-page-name"/>val evaluator4 = evaluator.setMetricName("f1")
</pre>
<p class="mce-root"><strong class="calibre1">Step 13. Compute the performance metrics</strong> - Compute the classification accuracy, precision, recall, f1 measure, and error on test data as follows:</p>
<pre class="calibre19">
val accuracy = evaluator1.evaluate(predictions)<br class="title-page-name"/>val precision = evaluator2.evaluate(predictions)<br class="title-page-name"/>val recall = evaluator3.evaluate(predictions)<br class="title-page-name"/>val f1 = evaluator4.evaluate(predictions)
</pre>
<p class="mce-root"><strong class="calibre1">Step 14.</strong> Print the performance metrics:</p>
<pre class="calibre19">
println("Accuracy = " + accuracy)<br class="title-page-name"/>println("Precision = " + precision)<br class="title-page-name"/>println("Recall = " + recall)<br class="title-page-name"/>println("F1 = " + f1)<br class="title-page-name"/>println(s"Test Error = ${1 - accuracy}")
</pre>
<p class="mce-root">You should observe values as follows:</p>
<pre class="calibre19">
<strong class="calibre1">Accuracy = 0.994277821625888</strong><br class="title-page-name"/><strong class="calibre1">Precision = 0.9904583933020722</strong><br class="title-page-name"/><strong class="calibre1">Recall = 0.994277821625888</strong><br class="title-page-name"/><strong class="calibre1">F1 = 0.9919966504321712</strong><br class="title-page-name"/><strong class="calibre1">Test Error = 0.005722178374112041</strong>
</pre>
<p class="mce-root">Now the performance is excellent, right? However, you can still increase the classification accuracy by performing hyperparameter tuning. There are further opportunities to improve the prediction accuracy by selecting appropriate algorithms (that is, classifier or regressor) through cross-validation and train split.</p>
<p class="mce-root"><strong class="calibre1">Step 15.</strong> Print the DT nodes:<strong class="calibre1"><br class="title-page-name"/></strong></p>
<pre class="calibre19">
val treeModel = model.stages(2).asInstanceOf<br class="title-page-name"/>                                [DecisionTreeClassificationModel]<br class="title-page-name"/>println("Learned classification tree model:\n" + treeModel<br class="title-page-name"/>                 .toDebugString)
</pre>
<p class="cdpalignleft1">Finally, we will print a few nodes in the DT, as shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border186" src="../images/00199.gif"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 10:</strong> A few decision tree nodes that were generated during the model building</div>


            </article>

            
        </section>
    

        <section id="CSU3I1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="mce-root">In this chapter, we discussed some advanced algorithms in ML and found out how to use a simple yet powerful method of Bayesian inference to build another kind of classification model, multinomial classification algorithms. Moreover, the Naive Bayes algorithm was discussed broadly from the theoretical and technical perspectives. At the last pace, a comparative analysis between the DT and Naive Bayes algorithms was discussed and a few guidelines were provided.</p>
<p class="mce-root">In the next chapter<em class="calibre8">,</em> we will dig even deeper into ML and find out how we can take advantage of ML to cluster records belonging to a dataset of unsupervised observations.</p>


            </article>

            
        </section>
    </body></html>