<html><head></head><body>
        

                            
                    <h1 class="header-title">Enterprise Data Science</h1>
                
            
            
                
<p>We have thus far discussed various topics regarding both data mining and machine learning. Most of the examples shown were designed so that anyone with a standard computer would be able to run them and complete the exercises. In real-world situations, datasets would be much larger than those encountered in general home use.</p>
<p>Traditionally, we have relied on well-known database technologies such as SQL Server, Oracle, and others for organizational data warehouse and data management. The advent of NoSQL and Hadoop-based solutions made a significant change to this model of operation. Although companies were at first reluctant, the popular appeal of these tools became too large to ignore, and today, most, if not all, large organizations leverage one or more non-traditional contemporary solution for their enterprise data requirements.</p>
<p>Furthermore, the advent of cloud computing has transformed most businesses, and in-house data centers are being rapidly replaced by cloud-based infrastructures. The primary market leaders in the cloud space are Amazon (Amazon Web Services), Microsoft (Azure), and, to a lesser extent, Google (Google Compute Engine).</p>
<p>Data warehousing, data science, and machine learning needs are being delivered primarily on such platforms.</p>
<p>In this section, we will look at the various technical platforms that are prevalent in the corporate/enterprise market, their strengths, use cases, and potential pitfalls. In addition, we will also complete a tutorial using AWS to launch new instances on-demand using a trial account.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Enterprise data science overview</li>
<li>Enterprise data mining</li>
<li>Enterprise AI and machine learning</li>
<li>Enterprise infrastructure</li>
<li>Other considerations, such as data strategy, governance, and tool selection</li>
<li>Amazon Web Services tutorial</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Enterprise data science overview</h1>
                
            
            
                
<p>Data science is a relatively new topic in terms of enterprise IT and analytics. Traditionally, researchers and analysts belonged broadly to one of two categories:</p>
<ul>
<li>Highly technical researchers who used complex computing languages and/or hardware for their professional tasks</li>
<li>Analysts who could use tools such as Excel and BI platforms in order to perform both simple and complex data analysis</li>
</ul>
<p>Organizations started looking into <strong>Big Data</strong> and, more generally, data science platforms in the late 2000s. It had gained immense momentum by 2013, when solutions such as Hadoop and NoSQL platforms were released. The following table shows the developments in data science:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Year</strong></p>
</td>
<td>
<p><strong>Developments</strong></p>
</td>
</tr>
<tr>
<td>
<p>1970s to late 1990s</p>
</td>
<td>
<p>Widespread use of relational database management systems. Entity relationship model, structured query language (SQL), and other developments eventually led to a rapid expansion of databases in the late 90s.</p>
</td>
</tr>
<tr>
<td>
<p>Early 2000s</p>
</td>
<td>
<p>The anti-climatic, yet expensive, non-event of Y2K, coupled with the collapse of the dot-com <kbd>bubble</kbd> led to a period of stagnation. In terms of databases, or more generally, data mining platforms, this meant that companies were less focused on new innovations than they were on keeping the business running.</p>
</td>
</tr>
<tr>
<td>
<p>2005-2010</p>
</td>
<td>
<p>The industry slowly recovered, but it was not until 2005 that newer developments began to emerge. Some notable events included:</p>
<ul>
<li>2006: GoogleBigTable paper published</li>
<li>2006: Amazon Web Services cloud platform launched</li>
<li>2007: Amazon Dynamo paper published</li>
<li>2008: Facebook makes Cassandra open source</li>
<li>2009: MongoDB released</li>
<li>2009: Redis released</li>
</ul>
</td>
</tr>
<tr>
<td>
<p>2010-2012</p>
</td>
<td>
<p>2010: NoSQL conferences and related events start gaining popularity and <em>NoSQL</em> becomes a commonly accepted technical term. At the same time, Hadoop becomes widely popular, and nearly all major companies begin the process of implementing Hadoop-related technologies.</p>
<p>2011: Market leaders start adopting Big Data and forming Big Data strategies. Numerous articles and research papers claiming the huge potential of Big Data makes it very popular. McKinsey publishes a paper on Big Data and calls it the next frontier of <em>innovation, competition, and productivity</em>. The October 2012 edition of, <em>Harvard Business Review</em> includes a very positive outlook on data scientists, which becomes immediately popular.</p>
</td>
</tr>
<tr>
<td>
<p>2013-2015</p>
</td>
<td>
<p>The growth of Big Data technologies leads to the development of a concept called data science, which moves the focus from just the data to the value of the data. Coupled with developments in machine learning and the rise of the popularity of R, Python, and other data science-oriented platforms, the industry shifts attention to getting insights from data as opposed to merely managing data. Machine learning is the new buzzphrase.</p>
</td>
</tr>
<tr>
<td>
<p>2016-</p>
</td>
<td>
<p>The evolution of smart devices, wearables, AI-enabled cell phones, autonomous driving cars, and other such innovative solutions adds a new component of artificial intelligence to the existing trend of Big Data and machine learning. Manufacturers start broadly advertising the intelligent capabilities, as opposed to merely the machine learning capabilities, of technical solutions.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The responsibility for implementing a Big Data platform or, more generally, a Big Data Initiative, is generally delegated to the IT or Analytics Department of a company, if such a department exists.</p>
<p>In a general survey of Big Data and data science delegations in organizations, we observed that, in most cases, the Chief Information Officer or the Chief Data/Digital Officer was responsible for the <strong>Enterprise Big Data Strategy</strong>, as shown in the following figure:</p>
<div><img height="213" width="372" src="img/d6322402-7a21-4d28-8865-5a2f9453c557.png"/></div>
<p>Although analytics and IT teams played a significant role, not surprisingly, the final responsibility was delegated to the C-level management of the company.</p>
<p>Investment in Big Data within the enterprise was also varied, with most organizations in the $100k to $1M range. An analysis of mid/large-scale organizations produced an expected result. What was evident, though, was that nearly <em>all respondents had made at least some investment in Big Data</em>:</p>
<div><img height="220" width="383" src="img/4425642a-c9be-4dd1-99cd-66e6a8872cb8.png"/></div>
<p>Most organizations also reported having a corporate mandate for <strong>Advanced Analytics</strong>. This helped in securing the required budget to implement and advance the state of analytics within the organization.</p>
<p>Furthermore, the predicted forecast of the revenue potential of data science greatly helped in making the case to senior management that a suitable investment in Big Data was essential to the future growth of a company.</p>
<p>With a current Big Data and Business Analytics revenue that has grown exponentially to more than $150 billion, the pressure on corporations to implement such capabilities, at least at a preliminary level, has been immense.</p>
<p>Another aspect of organizational awareness and acceptance of Big Data as a corporate mandate is the cultural perception of the utility of such tools. In a survey conducted with C-level management at large companies, most respondents stated that analytics was being used by managers in their departments, but there wasn't a uniform level of engagement across all departments, as shown in the following figure:</p>
<div><img height="285" width="497" src="img/b4978e01-1aed-45c0-88a6-44e624bc627b.png"/></div>
<p>Furthermore, it appeared that the partnership strategies for Big Data across the organization were also not structured to the extent needed for commercial success. Respondents to the survey indicated that the partnership, as in, the cross-functional collaboration of analytics initiatives, was loosely defined:</p>
<div><img height="229" width="400" src="img/686fd5cb-7dbe-4acf-ae1b-853413bc5548.png"/></div>
<p>While the responses to some of the questions were skewed, with one category being the overwhelming majority, the feedback on organizational challenges with Big Data and analytics in general had a broad uniform consensus. The following chart shows the feedback on analytics challenges from each participant in the survey:</p>
<div><img height="234" width="409" src="img/ef198cfa-17c6-4049-9889-55400a56e12a.png"/></div>
<p>All of this leads to an interesting but paradoxical analytics dilemma in the enterprise. Although the merit of Big Data and analytics is widely understood and accepted, there is a sense of ambiguity regarding the appropriate approach, as shown in the following diagram:</p>
<div><img height="181" width="556" src="img/adfd631c-ebf5-4f65-8972-cd946165e092.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">A roadmap to enterprise analytics success</h1>
                
            
            
                
<p>In our experience, analytics, which is a fairly recent term compared to well-established terms such as data warehouse and others, requires a careful approach in order to ensure both immediate success and the consequent longevity of the initiative.</p>
<p>Projects that prematurely attempt to complete an initial analytics project with large-scale, high-budget engagement run the risk of jeopardizing the entire initiative if the project does not turn out as expected.</p>
<p>Moreover, in such projects, the outcome measures are not clearly defined. In other words, measuring the value of the outcome is ambiguous. Sometimes, it cannot be quantified either. This arises because the success of an analytics initiative has benefits beyond simply the immediate monetary or technical competencies. A successful analytics project often helps to foster executive confidence in the department's ability to conduct said projects, which in turn may lead to bigger endeavors.</p>
<p>The <strong>general challenges</strong> associated with Big Data analytics are as follows:<strong><br/></strong></p>
<ul>
<li>Nearly every company is investing in Big Data, machine learning, and AI</li>
<li>Often, the company has a corporate mandate</li>
<li>Finding the right use cases can be challenging</li>
<li>Even after you <em>find</em> them, the outcome may be uncertain (will this resonate, how long will it take, and so on)</li>
<li>Even after you <em>achieve</em> them, whether or not the optimal targets have been identified can be elusive (for example, when using HDFS for storing only data)</li>
</ul>
<p>Now, let's look at some <strong>general guidelines</strong> for data science and analytics initiatives:</p>
<ul>
<li><strong>Conduct meetings and one-on-one reviews with business partners</strong> in the organization to review their workflows and get feedback on where analytics and/or data mining would provide the most value</li>
<li><strong>Identify specific aspects of business operations</strong> that are important and related to the firm's <em>revenue stream; </em>the use case would have a measurable impact once completed</li>
<li>The use cases do not have to be <em>complex</em>; they can be simple tasks, such as ML or Data Mining</li>
<li>Intuitive, easily understood, you can explain it to friends and family</li>
<li>Ideally the use case takes effort to accomplish today using conventional means. The solution should not only benefit a <strong>range of users</strong>, but also have <strong>executive visibility</strong></li>
<li>Identify <strong>Low Difficulty - High Value</strong> (<strong>Short</strong>) vs <strong>High Difficulty - High Value</strong> (<strong>Long</strong>) use cases</li>
<li>Educate business sponsors, share ideas, show <strong>enthusiasm</strong> (like a long job interview)</li>
<li>Score <strong>early wins</strong> for <strong>Low Difficulty - High Value</strong>, create <strong>Minimum Viable Solutions</strong>, and get management to buy in before further enhancing the use solutions developed. (takes time)</li>
</ul>
<p>Early wins act as a <strong>catalyst</strong> to a) foster executive confidence, and b) also makes it easier to justify budgets, which then makes it easier to move onto High Difficulty - High Value tasks.</p>
<p>The last two points are important as it is essential to identify Low Difficulty - High Value projects. This could be a task that appears <em>basic</em> to an experienced practitioner but is very valuable to the end user.</p>
<p>One of the executives of an analytics group in a large enterprise organization once remarked that the most successful project of the year was the <em>change of timing of an email report</em>. Instead of sending the report in the morning, the timing was changed to late afternoon. It appeared that engagement with the report became more active after the timing was changed. Morning schedules tend to be very busy and afternoon reports, on the other hand, provide recipients with the time to review the report at a more relaxed pace.</p>
<p>A few examples of <em>low</em> <em>difficulty</em> but <em>p</em><em>otentially h</em><em>igh value</em> projects could be:</p>
<ul>
<li>Automating manual tasks conducted on a frequent basis by a business group; for instance, reports that are created in Excel may be easily automated using a combination of open source tools and databases.</li>
<li>Converting manual stock analytics to automated versions using programming scripts. This could involve tasks such as creating regular tables, pivot tables, and charts that are created in Excel but can be converted into automated processes.</li>
<li>Creating web interfaces using R Shiny for business applications and implementing predictive analytics functionalities.</li>
<li>Moving certain parts of the IT infrastructure to a cloud platform. This may seem counter-intuitive, especially if the organization is not used to working in cloud environments. However, the ease and simplicity of managing cloud deployments can mean an overall reduction in the total cost of ownership and operational overhead.</li>
</ul>
<p>The ultimate choice of the use case would depend on various factors, and the previous ones have been mentioned to set an approximate idea of the type of projects that may be attempted, and the workflows that may yield positive results. In the next section, we will look at some of the specific software and hardware solutions used in the industry for data science.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Data science solutions in the enterprise</h1>
                
            
            
                
<p>As discussed before, in general, we can broadly categorize data science into two primary sections:</p>
<ul>
<li>Enterprise data warehouse and data mining</li>
<li>Enterprise data science: machine learning, artificial intelligence</li>
</ul>
<p>In this section, we will look at each of these individually and discuss both the software and hardware solutions used in the industry for delivering these capabilities.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Enterprise data warehouse and data mining</h1>
                
            
            
                
<p>Today, there are scores of databases available in the industry that are marketed as NoSQL systems capable of running complex analytical queries. Most of them have one or more features of typical NoSQL systems, such as columnar, in-memory, key-value, document-oriented, graph-based, and so on. The next section highlights some of the key enterprise NoSQL systems in use today.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Traditional data warehouse systems</h1>
                
            
            
                
<p>Traditional data warehouses might be a misnomer, since most of the <em>traditional</em> systems have also incorporated core concepts of NoSQL. However, in this case, the term is intended to indicate databases that existed well before the advent of NoSQL systems, and that have also added features that make them aligned with the requirements of Enterprise data science.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Oracle Exadata, Exalytics, and TimesTen</h1>
                
            
            
                
<p>Oracle Exadata, Exalytics, and Exalogic belong to Oracle's Exa family of products. <strong>Exadata</strong> is Oracle's high performance <em>Engineered Database Platform</em> that is designed for resource intensive queries. In general, it is expected to significantly improve query performance over non-Exadata systems, and supports advanced software features such as in-memory computing, independent row and column-based filtering, and other hardware features such as support for the latest storage devices, including NVMe, in-memory fault tolerance, and others.</p>
<p><strong>Exalytics</strong> is a complementary solution that is intended primarily for <strong>BI</strong> workloads. The Exa family of products are considered <em>engineered systems</em> as opposed to <em>appliances</em>. Whereas the latter may indicate preset and pre-loaded software-hardware stacks, an engineered system is expected to support a higher level of flexibility regarding the choice of installed components which are installed selectively depending on client needs. One of the key components of Exalytics commonly found in enterprise installations is <strong>OBIEE</strong> (<strong>Oracle Business Intelligence Enterprise Edition</strong>). This is a complete BI suite and benefits from an underlying in-memory database called <strong>Times Ten</strong>, which is also a part of the Exalytics ecosystem.</p>
<p><strong>Business use case</strong>: Oracle Exadata is used for OLTP transactional workloads where speed and performance is critical.</p>
<p>Exalytics, on the other hand, is used for analytical workloads. The integrated OBIEE interface together with TimesTen provides a strongly coupled analytics environment. Oracle Exadata is also available as a cloud-based service.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">HP Vertica</h1>
                
            
            
                
<p>Vertica from HP is a column-oriented, massively parallel processing database system with key software features such as support for in-database machine learning, and native integration to open source systems such as Apache Kafka and Apache Spark, and is generally deployed on a multi-node hardware architecture. Vertica is supported on popular cloud environments such as <strong>Amazon Web Services</strong> (<strong>AWS</strong>), Google, and Azure. Vertica supports a standard interactive SQL interface, thus making it readily compatible with most contemporary BI tools.</p>
<p>HP Vertica, interestingly, is one of the few enterprise databases that is also available as a Community Edition. This means that users who are interested in trying out Vertica (within the scope of its licensing), or who are simply learning more about the platform can leverage the Community Edition, which can be downloaded from HP Vertica's website at no charge.</p>
<p><strong>Business use case</strong>: Similar to the other databases indicated in this section, Vertica incorporates several notable features such as in-database processing, parallel processing capabilities, and others. Vertica supports a wide range of analytical workloads and comes with associated commercial licensing fees (as do all the other commercial database products). The availability of a Community Edition, along with HP's willingness, in most cases, to engage in proof of concept for large deployments provides ample opportunities for business to try and test the platform with company-specific use cases prior to making a decision.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Teradata</h1>
                
            
            
                
<p>Teradata is widely recognized as a leader in enterprise database technology. Its database, which also goes by the same name, shares several of the same features as other competitor products. Some key features of Teradata include native integration with many open source solutions, such as R, RStudio, Jupyter, and SAS; time series support; built-in analytic functions for machine learning and AI; support for a wide range of data types, such as CSV, JSON, and text, and spatial/temporal data.</p>
<p>The analytics platform, also known as Aster Analytics, is available as a Community Edition from <a href="https://aster-community.teradata.com/community/download" target="_blank">https://aster-community.teradata.com/community/download</a>.</p>
<p>While, traditionally, Teradata was available as an appliance solution, as in both the database as well as the hardware were available as a single integrated unit, today, it is also possible to use Teradata in the cloud using Teradata Everywhere. The software can be deployed in a hybrid architecture (both on-premises as well as in the cloud), as well as in public cloud environments such as AWS and Microsoft Azure. Bundled services, subscription-based services, and as-a-service options are available. Teradata Intellicloud is a subscription-based cloud offering from Teradata that includes several products from the Teradata ecosystem in a managed environment.</p>
<p><strong>Business use case</strong>: Teradata has been a popular enterprise database for several decades and has strong credibility with large organizations. In recent years, Teradata's proactive integration with open source systems such as R, Jupyter, and other products made it more appealing and arguably helped increase its visibility. Teradata appliances can be relatively expensive and, as with other commercial options, require proper POCs to assess suitability for use cases specific to the organization.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">IBM data warehouse systems (formerly Netezza appliances)</h1>
                
            
            
                
<p>IBM Netezza used to be sold as a separate data warehouse appliance, in some ways similar to how Teradata was also marketed. Recently, Netezza has been moved under the broader categorization of IBM Data Warehouse systems which is more aligned with the contemporary Big Data requirements for managing very large volumes of data. IBM Pure Systems, PureSystems for Analytics, and IBM Integrated Analytics System are some of the newer solutions that provide essentially the same functionalities of Netezza in an integrated ecosystem.</p>
<p>The Integrated Analytics environment includes embedded Apache Spark for machine learning, Jupyter Notebooks for data science workloads, a common SQL engine that connects to other NoSQL and Hadoop implementations, and support for deployments on high performance architecture with the option of managed, cloud-based environments</p>
<p><strong>Business use case</strong>: Netezza has been favored by firms that have traditionally had strong dependency on IBM-related technologies, such as DB2, AIX, and other products from IBM. The new integrated product environment provides an opportunity to continue using whilst adding data science capabilities to existing IBM investments in the organization.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">PostgreSQL</h1>
                
            
            
                
<p>PostgreSQL is an interesting choice in this section because, technically, there is no separate NoSQL version of PostgreSQL, but rather PostgreSQL has added various features in recent releases that have added NoSQL capabilities to the existing Postgres implementation.</p>
<p>Proponents of PostgreSQL rightly point out that it is a much older, and, by extension tested technology, having been first released in the mid-1990s. <strong>Postgres</strong> now supports hierarchical document data storage, JSON, a key-value store (called <strong>HStore</strong>), and sharding, and includes interfaces for various programming languages as well as diverse data sources. In other words, PostgreSQL has been extended to support NoSQL-like functionalities while maintaining its existing capabilities as a traditional RDBMS.</p>
<p>PostgreSQL is available as a fully-functional, open source product.</p>
<p><strong>Business use cases</strong>: While most of the technologies in this section are available under commercial licensing (to get access to all their capabilities), PostgreSQL, being open source, is a very cost-effective way to try out a mature database without making large initial investments. It can also serve as a testing platform for trying out NoSQL features, such as handling JSON data prior to making a final decision. In either case, PostgreSQL is a formidable platform and can support enterprise needs. There are also commercial derivatives of PostgreSQL—databases that build on top of PostgreSQL such as Greenplum, which is also available as an open source product.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Greenplum</h1>
                
            
            
                
<p><strong>Greenplum Database</strong>® is built on top of PostgreSQL, and adds a number of significant analytic capabilities. These include an innovative cost-based query optimizer, integration with Apache MADlib, and choices for row or columnar storage. It has native interfaces for popular programming languages such as R, Python, and Java, and supports massively-parallel architectures. Greenplum is available for download at no charge from <a href="http://greenplum.org/download/" target="_blank">http://greenplum.org/download/</a>.</p>
<p><strong>Business use cases</strong>: A commercial distribution of Greenplum with full support is available from <strong>Pivotal</strong>. Greenplum has been very successful, not least because of its proven performance for large enterprise workloads. The availability of commercial support has been beneficial to organizations who require dedicated support and service-level agreements (<strong>SLA</strong>) that guarantee critical business operations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">SAP Hana</h1>
                
            
            
                
<p>SAP Hana is a columnar, in-memory database from SAP with support for NoSQL features. Hana supports multicore parallel operations, multi-tenancy, and is fully <strong>ACID</strong> compliant and can handle a diverse range of analytical workloads including predictive modelling, streaming analytics, time series analysis, and spatial, text, and graph-based analysis. You can also manage JSON based unstructured data within an SAP Hana system.</p>
<p>Hana also works natively with other SAP products such as SAP Fiori, which includes a wide range of SAP UX applications used in HR, finance, accounting, and other departments.</p>
<p><strong>Business use cases</strong>: SAP has been a mainstay for enterprise organizations for several decades and is used for a wide range of applications, most notably perhaps for manufacturing and financial/accounting requirements. SAP Hana adds a formidable high-performance database to existing SAP installations. In general, due to the high cost involved with enterprise-grade deployments, SAP is used mainly for business-critical needs. The benefits of Hana for large organizations that are dependent on SAP may outweigh the costs. Furthermore, while Hana can deliver a wide range of NoSQL capabilities, companies may find that they will end up with two or more different solutions based on budget, performance needs, and other factors.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Enterprise and open source NoSQL Databases</h1>
                
            
            
                
<p>The prior section outlined some of the well-known traditional database/RDBMS solutions that have added enterprise-grade NoSQL capabilities. This upcoming sections looks at some of the more niche business use cases specific to database solutions.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Kdb+</h1>
                
            
            
                
<p>The <strong>kdb+</strong> from Kx Systems is one of the fastest, most efficient and lightweight databases that has been used in high-frequency trading and other similar environments for almost two decades. Its popularity outside of finance has been much less pronounced, but nevertheless, it is arguably one of the most efficiently designed and optimized systems in the world of databases.</p>
<p>Kdb+ supports in-memory columnar storage from the outset and is technically an extension of the <strong>q</strong> programming language. A table in kdb+ is in essence a data structure in the q language. However, unlike similar concepts in other programming languages, a kdb+ table is enterprise-grade and can easily handle terabytes and petabytes of data.</p>
<p>Due to its inherent programming language, code that is written in q can be run against data stored in kdb+, so a custom <em>function</em> can be run in-database with very minimal effort.</p>
<p>Additionally, the size of the kdb+ binary is about 500 to 600KB, small enough to fit in the L3 cache of most modern CPUs.</p>
<p>Kdb+ also includes built-in MapReduce capabilities so that queries are automatically executed in parallel across muticore CPUs.</p>
<p><strong>Business use case</strong>: Kdb+ is one of the most formidable databases to have existed in an enterprise setting. It was traditionally available only for perpetual core-based licensing, but in recent days, the company has added support for subscription-based and on-demand licensing. Its low footprint and simplicity of use makes it well suited for enterprise needs. However, this comes with a caveat. The q language is very terse and can appear cryptic to new users. The language, arguably, has a slightly steeper learning curve than others and requires practice and first-hand experience. That said, there are ample online resources to learn and utilize the features of the database. Native interfaces for R, Python, C, Java, and other programming languages, along with libraries used for machine learning, make it particularly well suited for data science workloads involving large datasets.</p>
<p>While kdb+ is not available as an open source product, it is generally available for personal use at no charge from: <a href="https://kx.com/download/" target="_blank">https://kx.com/download/</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">MongoDB</h1>
                
            
            
                
<p><strong>MongoDB</strong> is a market leader in the space of document-oriented databases for the storage of data in JSON format. It supports on-demand querying, indexing, and aggregations, and has a rich interface for Python, Java, and JavaScript, among other languages. Other features, such as horizontal scaling and sharding, high availability, an integrated data exploration tool called Compass, and others, add to the existing capabilities of the database.</p>
<p><strong>Business use cases</strong>: Companies considering databases for storing unstructured or semi-structured data may find the features of MongoDB well suited for querying such datasets. The database does not require a fixed schema to be defined at the onset, making it flexible and extensible to support new attributes that are added to existing data. MongoDB is available as a free open source download from <a href="https://www.mongodb.com/download-center" target="_blank">https://www.mongodb.com/download-center</a> and can also be implemented as a managed and hosted cloud solution via MongoDB Atlas. An enterprise version that supports features such as in-memory and encrypted storage is also available on a subscription basis.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cassandra</h1>
                
            
            
                
<p><strong>Cassandra</strong> is one of the most successful and widely used enterprise NoSQL systems. It incorporates both columnar and key-value concepts and stores data in row-based partitions. Each partition is in turn a primary key. Rows can have multiple columns and the number of columns may differ from one row to another.</p>
<p>Cassandra databases can be queries done via CQL, which uses a SQL-like syntax and makes the process of data querying, saving, and other common tasks much easier. Cassandra also uses a decentralized architecture; it does not have any single point of failure and supports multi-node architecture.</p>
<p>In addition to the standard horizontal scalability of Cassandra DBMS, the platform also supports Elastic scalability and is able to transparently allocate and de-allocate nodes depending upon needs. On the whole, Cassandra is one of the most formidable options for enterprise NoSQL systems and is used in production environment across multiple large firms globally.</p>
<p><strong>Business use case</strong>: Cassandra is a fully open source solution and implements multiple key features of NoSQL systems. It is used in production workloads globally and has matured into a stable, enterprise-grade, open source platform. In other words, Cassandra is well suited for managing large organizational needs and does not incur any additional licensing costs. A commercial, licensed, and paid version of Cassandra is also available from Datastax: <a href="https://www.datastax.com/products/datastax-enterprise" target="_blank">https://www.datastax.com/products/datastax-enterprise</a>. It is known as <strong>DSE</strong> (<strong>Datastax</strong> <strong>Enterprise</strong>). DSE incorporates various enterprise features such as security and search, and can also be accessed via the Datastax Managed Cloud environment using popular cloud providers such as AWS.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Neo4j</h1>
                
            
            
                
<p><strong>Neo4j</strong> is a graph-based database that is used to model relationships between different entities. The database uses familiar concepts in graph theory to create tree-based representations (with nodes and relationships) of interconnected subjects. It is used most commonly in conjunction with recommendation engines. Conceptually, a Neo4j graph database could represent individuals as nodes who are connected to one another by, say, their degree of separation. This would hypothetically allow an end user to trace the degrees of separation between any arbitrary node or one individual to another.</p>
<p>Various graph-based representations such as weighted, directed, unidirectional, and labelled are available in the Neo4j platform.</p>
<p><strong>Business use case</strong>: Companies that require deep customer-level or user-level analysis such as social networks or recommendation systems (such as Netflix), stand to gain an immense benefit from deploying graph-based databases such as Neo4j. Today, the platform supports AI and machine learning, iOT, real-time recommendations, and many other useful characteristics used in enterprise.</p>
<p>Although Neo4j is available as open source software from <a href="https://neo4j.com/download/?ref=hro" target="_blank">https://neo4j.com/download/?ref=hro</a>, there is also a commercial licensed version known as Neo4j Enterprise Edition.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cloud databases</h1>
                
            
            
                
<p>Cloud databases, as the name suggests refers to data warehouse or database systems available from cloud vendors such as Amazon, Google, and Microsoft.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Amazon Redshift, Redshift Spectrum, and Athena databases</h1>
                
            
            
                
<p>One of the most commonly used cloud-based data warehouse platforms is <strong>Amazon Redshift</strong>. It is the most prominent platform for data management in the cloud-based ecosystem. It is based on PostgreSQL and is intended mainly for analytical workloads. Redshift is highly scalable and requires significantly less effort relative to on-premises databases with similar characteristics. It can be deployed directly from the AWS console (after signing up for an AWS account). Nodes can be added or removed seamlessly via the AWS Console to increase and/or decrease capacities respectively.</p>
<p>A more recent release of Redshift known as <strong>Redshift Spectrum</strong> permits the querying of data that has been stored in Amazon S3, the standard storage layer in AWS. This means that users can directly query data stored on disk without having to load it into a Redshift-specific instance. Overall, Redshift is relatively fast, inexpensive, and more importantly easy to use and deploy. Redshift Spectrum uses a pay-per-query model—users pay only for the queries that are executed at a nominal charge for each terabyte of data scanned.</p>
<p><strong>Amazon Athena</strong> is in many respects similar to Amazon Redshift Spectrum, in that it is also used to query data that is stored on S3. However, while the features of Amazon Redshift Spectrum cannot be used without first purchasing Amazon Redshift, users can leverage Amazon Athena on-demand and do not need to reserve any additional hardware. On the other hand, because Amazon Redshift Spectrum is closely integrated with the Redshift ecosystem, users can distribute their workload on either of the two solutions. Data that needs faster processing can remain on Amazon Redshift, whereas less frequently used/less critical data can be stored on S3 and queried using Redshift Spectrum:</p>
<div><img src="img/9d66f408-9391-48c2-bf53-db4b4cfb1510.png"/></div>
<p>You can learn more about Amazon Redshift Spectrum at <a href="https://aws.amazon.com/redshift/spectrum/" target="_blank">https://aws.amazon.com/redshift/spectrum/</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Google BigQuery and other cloud services</h1>
                
            
            
                
<p><strong>Google BigQuery</strong> is similar to Amazon Redshift, in that it is also a large-scale data warehouse system that is fully cloud-based. However, while Redshift requires separate provisioning (of an AWS cluster and Redshift resources), Google BigQuery is the <em>plug-and-play</em> equivalent of the same. To use BigQuery, the user simply needs to create an account at <a href="https://bigquery.cloud.google.com" target="_blank">https://bigquery.cloud.google.com</a> and begin running queries after loading their datasets.</p>
<p>The charging method of BigQuery is also quite different in comparison to Redshift. Users can query a cumulative of 1 terabyte of data at no charge per month. BigQuery uses a pay-per-use model whereby queries have allocated costs. In essence, BigQuery abstracts the complexity of setting up a database and allows the end user to dedicate time to writing queries and/or performing analytics without the overhead of setting up an infrastructure. Scaling queries, allocating resources, and tasks that may have otherwise required manual intervention (by a DBA for example), hence become redundant.</p>
<p>Google also has a set of other NoSQL products on its cloud platform, including Google Cloud Datastore, a NoSQL document-based database; Google BigTable; Google Spanner; and several others. The following figure shows the Google BigQuery database:</p>
<div><img src="img/b4680c31-ff0b-4125-8dec-08423dae22ab.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Azure CosmosDB</h1>
                
            
            
                
<p>The <strong>Azure CosmosDB</strong> is one of Microsoft's NoSQL cloud-based databases. Other NoSQL systems in Azure include Table Storage, Azure Redis Cache, and others. CosmosDB is considered a <em>multi-model</em> database; it can support key-value pairs, document-based queries, graph-based models, and also relational database queries.</p>
<p>Traditional Microsoft databases, such as, SQL Server are also available and are supported as fully managed and hosted solutions on the Azure platform. You can learn more about the Azure platform at <a href="https://azure.microsoft.com/en-in/services/cosmos-db/" target="_blank">https://azure.microsoft.com/en-in/services/cosmos-db/</a>. The following figure shows the Microsoft Azure platform's Solutions window:</p>
<div><img src="img/51d4d481-2a5e-4be5-bd01-a20636a11c9c.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">GPU databases</h1>
                
            
            
                
<p><strong>GPU databases</strong> are a more recent development that came with the growth of Graphics Processing Unit cards for data science related tasks, such as machine learning. GPUs work best when the query can be parallelized. This is due to the fact that GPUs contain thousands of cores. By delegating each core to work on a small subset of the data, a GPU can often calculate at an impressively fast rate that far exceeds the CPU-based query performance.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Brytlyt</h1>
                
            
            
                
<p><strong>Brytlyt</strong> is a recent entrant in the space of GPU databases. It is complemented by a visualization product called <strong>Spotlyt</strong>. Early testing has shown that Brytlyt surpasses several challenging benchmarks. However, how well it generalizes in other use cases remains to be seen.</p>
<p>Brytlyt is available in the Amazon AWS Marketplace (<a href="https://aws.amazon.com/marketplace/pp/B074JZNSWZ?qid=1513342415797&amp;sr=0-1&amp;ref_=srh_res_product_title" target="_blank">https://aws.amazon.com/marketplace/pp/B074JZNSWZ?qid=1513342415797&amp;sr=0-1&amp;ref_=srh_res_product_title</a>) for those wish to try it.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">MapD</h1>
                
            
            
                
<p><strong>MapD</strong> was one of the early developers of a commercial GPU database platform. Similarly to Brytlyt, it has also shown impressive early results. Nevertheless, as GPU-based databases are still in their early stages, popular use and adoption will ultimately determine whether they will become commonplace in enterprise.</p>
<p>One of the primary challenges of GPU-based databases is the need to configure a GPU-based system properly. This can require specialized skills, as using GPU cards for computation is quite different than using GPU cards for common tasks such as rendering images. Due to this, users wishing to try out GPU-based databases prior to adopting a formal version would find it easier to leverage a pre-configured image in AWS (AMI Image), which would require minimal system configuration.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Other common databases</h1>
                
            
            
                
<p>There are various other types of databases, such as ones that are used for analyzing streaming data (Amazon Kinesis), and those that process data using specialized Accelerator Cards using FPGAs from Baidu.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Enterprise data science – machine learning and AI</h1>
                
            
            
                
<p><strong>Data science solutions</strong> have matured rapidly over the past 4 - 5 years, similar to the movement in other areas of data science such as NoSQL, Hadoop, and other data mining solutions. Although many of the prior database systems also incorporate key features of <em>data science</em>, such as machine learning and others, this section highlights some of the solutions at a high level that are primarily used for machine learning and/or AI, as opposed to data management.</p>
<p>Indeed, the distinction between <em>Big Data</em> products and <em>data science</em> products has become blurred, since products that were originally intended for Big Data handling have incorporated key features of data science, and vice versa.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The R programming language</h1>
                
            
            
                
<p><strong>R</strong>, as we have seen in prior chapters, is an environment originally designed for statistical programming. It emerged out of a project at the University of New Zealand, where <em>Ross Ihanka</em> and <em>Robert Gentleman</em> developed R as a variation of the S programming language developed by John Chambers in Bell Labs. Although R was initially intended for <em>statistical programming</em>, over the last 7 to 8 years it has evolved into a mature, multifaceted language with enhanced support for a diverse range of related disciplines such as machine learning, high performance computing, visualization, econometrics, TimeSeries analysis, and much more. Some of these areas are also described with accompanying information at <a href="https://cran.r-project.org/web/views/" target="_blank">https://cran.r-project.org/web/views/</a>.</p>
<p>A commercial version of R with enterprise support was available from Revolution Analytics. In 2015, it was rebranded as <strong>Microsoft R Open</strong> (open-source version) and <strong>Microsoft R Server</strong> (commercial version).</p>
<p>Although marketed under the Microsoft brand, note that Microsoft R is also available for Linux and Mac OS.</p>
<p>Popular machine learning packages in R include <kbd>e1071</kbd>, <kbd>randomForest</kbd>, <kbd>gbm</kbd>, <kbd>kernlab</kbd>, <kbd>arules</kbd>, and many more. These are listed at <a href="https://cran.r-project.org/web/views/MachineLearning.html" target="_blank">https://cran.r-project.org/web/views/MachineLearning.html</a>. Another popular package, called caret, acts as a wrapper around various algorithm packages and provides a useful unified interface to run algorithms without having to conform to the nuances of the packages individually.</p>
<p>R also supports multicore programming via packages such as <kbd>multicore</kbd>, <kbd>doMC</kbd>, and others. These are listed at <a href="https://cran.r-project.org/web/views/HighPerformanceComputing.html" target="_blank">https://cran.r-project.org/web/views/HighPerformanceComputing.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Python</h1>
                
            
            
                
<p>The <kbd>scikit-learn</kbd> package in Python is arguably the most comprehensive machine learning package among all platforms that incorporates an extensive list of machine learning algorithms. It is also considered to be faster compared to R, and is the tool of choice for various enterprise organizations. The following screenshot shows the web page from which we can download the <kbd>scikit-learn</kbd> package:</p>
<div><img src="img/da0de066-9015-40f0-997b-f340cac4dcf1.png"/></div>
<p>A commercially supported enterprise version of Python that comes pre-configured with useful machine learning and data mining packages is <kbd>Anaconda</kbd>, available from Continuum Analytics. A cloud version of Anaconda called <strong>Anaconda Cloud</strong> allows new users to start leveraging the features of Anaconda Python without the overhead of downloading and installing it separately.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">OpenCV, Caffe, and others</h1>
                
            
            
                
<p>Image recognition is one of the more successful areas of machine learning. While most machine learning tasks require a relatively long period of time before their true benefits can be measured and quantified, image recognition is a familiar subject area that can be readily understood. In essence, it involves identifying objects and correctly categorizing them. It has several applications, ranging from identifying license plate numbers to face recognition, and is available in mobile devices and robotics.</p>
<p>OpenCV provides a standard interface for various image recognition tasks, and can also leverage hardware acceleration features to optimize performance.</p>
<p>Other well-known machine learning software for image processing include Caffe, cuDNN, TensorFlow, and others. Note that these packages are not limited to simply image recognition, but can be also used for other deep learning use cases.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Spark</h1>
                
            
            
                
<p>The MLlib library in Spark provides a formal implementation of various machine learning algorithms that can be used in a Spark platform. The availability of pySpark makes the process of using the functionality easier for those with Python programming knowledge. If the organization had an existing Spark platform, it would be worth exploring the machine learning capabilities in MLlib.</p>
<p>The following screenshot gives you a brief overview of MLlib:</p>
<div><img src="img/44661322-2958-4356-8f9a-012617e4ca8f.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Deep learning</h1>
                
            
            
                
<p>Neural Networks with multiple hidden layers (generally more than two) and/or nodes are generally categorized as <strong>deep learning</strong>. Several contemporary advances in machine learning, such as autonomously driving cars, are a direct result of the use of deep learning for practical day-to-day tasks.</p>
<p>There are various deep learning frameworks/packages, and some notable ones include:</p>
<ul>
<li>TensorFlow</li>
<li>cuDNN</li>
<li>Theano</li>
<li>Torch</li>
<li>PaddlePaddle, from Baidu</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">H2O and Driverless AI</h1>
                
            
            
                
<p>A popular platform for Kaggle competitions, <strong>H2O</strong> provides a massively scalable, real-time machine learning interface with native integration for R, Python, Spark, and much more. It is available for download, at no charge, from <a href="https://www.h2o.ai/h2o/" target="_blank">https://www.h2o.ai/h2o/</a>.</p>
<p><strong>Driverless AI</strong> is a recent addition to the H2O line of products. It aims to make machine learning easier for practitioners by implementing an automated interface that attempts to create models and optimize accuracy by building and evaluating multiple models in an automated manner. The following screenshot shows the homepage of the H2O platform:</p>
<div><img src="img/c14b6afd-caa7-46dd-acdb-bc040231968e.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Datarobot</h1>
                
            
            
                
<p>Conceptually, Dataro<strong>b</strong>ot is similar to H2O's Driverless AI in that it also attempts to build machine learning models in an automated manner by creating and evaluating multiple models against a given dataset.</p>
<p>However, unlike H2O, although it can be very powerful, Datarobot requires a licensing fee and can be expensive for smaller firms.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Command-line tools</h1>
                
            
            
                
<p>There are multiple machine learning tools that are executed at the Unix command-line. There are existing interfaces for some of these tools in R, Python, and other languages that permit users to leverage their capabilities without having to use them from the Unix terminal. Some of the popular command-line utilities include:</p>
<ul>
<li>LIBSVM</li>
<li>LIBLINEAR</li>
<li>Vowpal Wabbit</li>
<li>MLPACK</li>
<li>libFM</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Apache MADlib</h1>
                
            
            
                
<p>One of the lesser-known but feature-rich platforms is <strong>Apache MADlib</strong>, which aims to perform analytics and run algorithms <em>in-database</em>, as in, it can execute functions locally without requiring an external programming interface. It supports parallel processing and can work seamlessly with multiple data sources such as Greenplum, PostgreSQL, and others.</p>
<p>As an example, an apriori model can be created by simply running an SQL command, as shown here, from <a href="http://madlib.apache.org/docs/latest/group__grp__assoc__rules.html" target="_blank">http://madlib.apache.org/docs/latest/group__grp__assoc__rules.html:</a></p>
<pre>SELECT * FROM madlib.assoc_rules(.25,            -- Support 
                                  .5,             -- Confidence 
                                  'trans_id',     -- id col 
                                  'product',      -- Product col 
                                  'test_data',    -- Input data 
NULL,           -- Output schema 
                                  TRUE            -- Verbose output 
); </pre>
<p>Further information about Apache MADlib (screenshot of site shown below) is available at <a href="http://madlib.apache.org" target="_blank">http://madlib.apache.org.</a></p>
<div><img src="img/5201727e-95e6-438f-90eb-fa83a92ef87f.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Machine learning as a service</h1>
                
            
            
                
<p>Cloud-based machine learning platforms that integrate with other cloud resources have also proliferated. Some of the well-known platforms include AzureML, BigML, IBM Watson, and others.</p>
<p>The screenshot below is from IBM Watson, one of the most well-known platforms for machine learning and artificial intelligence. The platform gained prominence after it won the Jeopardy championship in 2011 [Source: <a href="https://www.techrepublic.com/article/ibm-watson-the-inside-story-of-how-the-jeopardy-winning-supercomputer-was-born-and-what-it-wants-to-do-next/">https://www.techrepublic.com/article/ibm-watson-the-inside-story-of-how-the-jeopardy-winning-supercomputer-was-born-and-what-it-wants-to-do-next/</a>]. At the time, the Machine Learning trend was in a nascent state and Watson was one of the first AI technologies that took the world by surprise. It proved that AI can be powerful and capable asset. Users can today leverage some of the same computing capabilities of IBM Watson by signing up for an account on the site <a href="https://www.ibm.com/watson/">https://www.ibm.com/watson/</a>.</p>
<div><img src="img/3cd0d41a-b5d0-4825-8a94-3d4700617fcf.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Enterprise infrastructure solutions</h1>
                
            
            
                
<p>The proper choice of infrastructure also plays a key role in determining the efficiency of the organization's data science platform. Too little, and the algorithms will take too long to execute; too much and you may have a lot of resources remaining unutilized. As such, the latter is preferable to having too little, which thwarts progress and the ability of any machine learning researcher to efficiently perform his or her tasks.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cloud computing</h1>
                
            
            
                
<p>Over the past 5 - 7 years, organizations have gradually shifted their resources to cloud-based platforms such as Amazon Web Services, Microsoft Azure, and Google Compute Engine. Today, all of these contain extremely sophisticated and extensive architecture to support machine learning, data mining, and in general <em>data science</em> at an enterprise level to meet the needs of organizations of all sizes.</p>
<p>In addition, the concept of <em>images</em>, such as AMI images in Amazon's AWS, allows users to initiate a pre-built snapshot of an OS with pre-installed components. As a result, users can almost entirely avoid the setup overhead prior to trying out new platforms.</p>
<p>Hadoop and map-reduce operations in general are also supported extensively in AWS. The <strong>EMR</strong>, or <strong>Elastic Map Reduce </strong>in AWS, and HDInsight in Azure, are two well-known and very popular Big Data frameworks.</p>
<p>The tutorial at the end of this chapter will demonstrate how to set up an AWS account and start using a sample AMI Image.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Virtualization</h1>
                
            
            
                
<p><strong>Virtualization</strong>—the process of creating isolated, self-contained environments within a larger host, has allowed organizations to consolidate servers and dramatically reduce data center footprints. If, say, an organization leverages six servers for their websites, and of those, two get utilized frequently whereas the others have relatively lower loads most of the time, it may be possible to consolidate all the servers into one or two servers at most. In this regard, technologies from Dell EMC, such as VxBLOCK, are well-known enterprise virtualization hardware used in physical data centers. This also allows companies to create their own private cloud infrastructure. However, it can be fairly expensive and requires the proper assessment of the cost-to-benefit ratio.</p>
<p>An open source software used for creating public and private clouds is Openstack. It is an enterprise-grade ecosystem with multiple products that works seamlessly within the Openstack platform. Further details about Openstack are available at <a href="https://www.openstack.org" target="_blank">https://www.openstack.org:</a></p>
<div><img src="img/13b666d0-dff7-45ec-92ea-c7e3a7d54b3d.png"/></div>
<p>Users may be familiar with Oracle Virtualbox, which, in essence, is also a type of virtualization software that permits users to create isolated environments. This allows users to run Linux within Windows, and vice versa.</p>
<p>Specialized software or hardware, known as hypervisors, are used to manage and administer virtual machines.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Containers – Docker, Kubernetes, and Mesos</h1>
                
            
            
                
<p>Containers, like virtualization, create isolated guest systems, but, while Virtual Machines create a completely separate environment, containers share the same kernel as the host system and hence are considered to be closer to the hardware. Both virtualization and containers incur performance penalties due to multiple layers of abstraction—the translation of functionalities between a host and guest OS. However, containers in general have a higher level of performance because they rely on and directly use features of the guest OS instead of creating a separate OS ecosystem.</p>
<p>Popular containers include Docker, CoreOS, and many others. Today, containers are used for the large-scale management of mainly web-related services. Containers can be started up and shut down on demand much more readily than VMs, and popular cloud providers have added dedicated support for containers, making it easy to start up thousands of containers to service web requests with simply a few lines of code. Orchestration software such as Kubernetes provide enterprise-grade capabilities for managing containers. Furthermore, platforms, such as Mesos, not only provide support for managing containers, but also add the capability of managing other legacy hardware for application-aware scheduling and other services:</p>
<div><img src="img/db50058b-25b9-4515-9e3d-9af6f877efc5.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">On-premises hardware</h1>
                
            
            
                
<p>Finally, on-premises hardware, such as the traditional data center, still has a place in modern-day computing. With a physical data center, users do not have to pay recurring fees for cloud-based services. For small to mid-sized organizations that do not have large administrative overhead, or for organizations that do not require high-performance/specialized computing capabilities, on-premises systems are fully capable of delivering cost-efficient, permanent solutions.</p>
<p>Companies such as ScaleMP provide specialized hardware that is used for high-performance computing. Consumers of such hardware usually have specific requirements that cannot be provided by cloud-based vendors:</p>
<div><img src="img/e4320051-00a4-434e-bc8a-d36f5fc84a68.png"/></div>
<p>A summary of some of the differences between on-premises and cloud-based systems is shown below:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>On-premises</strong></p>
</td>
<td>
<p><strong>Cloud</strong></p>
</td>
</tr>
<tr>
<td>
<p>You own the hardware</p>
</td>
<td>
<p>You lease the hardware</p>
</td>
</tr>
<tr>
<td>
<p>Requires full maintenance</p>
</td>
<td>
<p>Maintenance is managed by a cloud-hosting provider</p>
</td>
</tr>
<tr>
<td>
<p>Requires IT resources for managing computing hardware resources</p>
</td>
<td>
<p>Much less overhead in terms of managing computing hardware resources, as they can be added on-demand in the cloud</p>
</td>
</tr>
<tr>
<td>
<p>Cost efficient for small to mid-sized environments with low or no data center operation cost</p>
</td>
<td>
<p>Cost efficient for large organizations that are looking to simplify data center operation costs</p>
</td>
</tr>
<tr>
<td>
<p>No recurring cost for using hardware other than resources required to manage them</p>
</td>
<td>
<p>Recurring cost to use the hardware; uses a subscription model for pricing</p>
</td>
</tr>
<tr>
<td>
<p>Mainly static architecture; new requirements for Hadoop will require a complete range of new purchases</p>
</td>
<td>
<p>Extremely flexible; companies can provision thousands of servers in multiple operating systems on-demand</p>
</td>
</tr>
<tr>
<td>
<p>Are readily accepted by organizational, legal, and associated departments</p>
</td>
<td>
<p>Faces obstacles, in particular from legal departments, due to the delegation of management to a third-party/cloud-hosting provider</p>
</td>
</tr>
</tbody>
</table>


            

            
        
    

        

                            
                    <h1 class="header-title">Enterprise Big Data</h1>
                
            
            
                
<p>The overall strategy of Big Data implementation in large organizations depends on the particular needs of the organization. Today, there are hundreds of options to choose from between Big Data, data science, machine learning and, of late, AI providers.</p>
<p>As such, there are two main considerations while implementing Big Data in large organizations:</p>
<ul>
<li><strong>Technical</strong>: The selection of the proper software and hardware stack</li>
<li><strong>Operational</strong>: Management of the organizational data, creating a formal data governance strategy, and creating an adequate data management framework</li>
</ul>
<p>Apart from these, hiring the right talent and possibly creating well-defined roles for the company's Big Data/data science implementations are additional but equally essential tasks.</p>
<p>Some key questions in the creation of such a strategy include:</p>
<ul>
<li>Is the software/hardware licensing based on size or cores? If it is based on the size of data and my data size increases, what will be my 3 year/5 year cost?</li>
<li>Does the solution have enterprise support?</li>
<li>Do we need to hire external resources?</li>
<li>What business questions will the new capabilities answer?</li>
<li>Have we done short and long-term cost-benefit analysis?</li>
<li>What are the present unmet needs of the organization that the new solutions can answer?</li>
<li>Is the solution scalable enough to meet my potential future needs?</li>
</ul>
<p>In terms of technical needs, although there are many solutions in the marketplace, it is extremely essential in practice to conduct testing or proof of concept using real-world/actual data that the solution will be used for. It is not uncommon to find solutions that claim grand capabilities but do not deliver expectations. <strong>In other words, it is crucial to gather thorough empirical results and not purchase solely on the basis of a marketing pitch.</strong></p>
<p>Lastly, as Big Data/data science is constantly evolving, the long-term scalability and adaptability of the solution needs to be properly evaluated. The cloud-based option should be considered in light of the fact that it provides an efficient medium to access and use new and emerging solutions in an easy and affordable manner.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Tutorial – using RStudio in the cloud</h1>
                
            
            
                
<p>The following tutorial will demonstrate how to create an account on <strong>AWS</strong> (Amazon Web Services), load an AMI Image for RStudio, and thereafter use RStudio, all at <em>no charge</em>. Readers who are experienced in using cloud platforms may find the instructions quite basic. For other users, the tutorial should provide helpful initial guidance on using AWS.</p>
<p>Please read the <strong>Warning</strong> message below prior to proceeding.</p>
<p><em><strong>Warning</strong>: Note that AWS requires a credit card for signup. Users must be careful and select only the options for the FREE TIER. The AWS agreement permits Amazon to bill users for incurred charges. Due to this reason, users should use the platform judiciously to avoid potentially expensive unexpected charges from servers or services that are left running</em><strong>.</strong></p>
<div><strong><br/></strong>As of this time, Azure and Google Cloud offer user signups with provisions to avoid inadvertent charges. However, AWS has the highest market share among all cloud vendors, and users are likely to encounter AWS in most workplace situations. Hence, this tutorial focuses on AWS rather than the alternatives.</div>
<p>Instructions on how to close your account have also been provided at the end of the tutorial, should you wish to discontinue your use of AWS (and thus also prevent any charges):</p>
<ol>
<li>Go to <a href="https://aws.amazon.com/" target="_blank">https://aws.amazon.com/</a>and click on the <strong>Create an AWS Account</strong> button at the top-right:</li>
</ol>
<div><img src="img/db605921-3e54-4530-9ee5-307af2704617.png"/></div>
<ol start="2">
<li>An AWS account generally includes 12 months of initial free tier access. Enter your information and click on Continue:</li>
</ol>
<div><img src="img/a5ea69aa-1d38-4fb7-a7f3-be6f10812805.png"/></div>
<ol start="3">
<li>Select your <strong>Account type</strong> (such as Personal) and enter your contact information:</li>
</ol>
<div><img src="img/870f14c0-4125-45d4-b82a-b9cec57db03c.png"/></div>
<ol start="4">
<li>Enter the payment information. AWS requires a credit card for signup. Note that users must utilize AWS resources very carefully and judiciously in order to ensure that there are no inadvertent charges:</li>
</ol>
<div><img src="img/7ac30d10-8413-4be2-8f37-7245d004fee7.png"/></div>
<ol start="5">
<li> You'll receive a confirmation once the payment information has been verified:</li>
</ol>
<div><img height="282" width="482" src="img/3648ca3b-044c-43b2-88f4-e5d096ddf622.png"/></div>
<ol start="6">
<li>Select the Basic Plan (Free):</li>
</ol>
<div><img src="img/d1f2b414-286f-4a4f-8e23-33db8af59332.png"/></div>
<p style="padding-left: 60px">The confirmation page after selecting the <strong>Basic Plan</strong> is as follows:</p>
<div><img src="img/fa0ef62e-36db-4cb2-ab65-6cc84e89a4d7.png"/></div>
<ol start="7">
<li>Log in to AWS with your credentials:</li>
</ol>
<div><strong><img src="img/7c88a880-7e82-4015-b46f-1c8c96067d86.png"/><br/></strong></div>
<ol start="8">
<li>The first page shows some of the services in AWS. The top-right shows the region of your instance. AWS supports multiple regions, and users can select from a range of geographically-dispersed locations:</li>
</ol>
<div><img src="img/91261eb1-5550-42c5-be3f-d821080b605d.png"/></div>
<ol start="9">
<li>Clicking on the top-left drop-down menu for <strong>Services</strong> will bring up the different services available. Some of the important ones are highlighted here:</li>
</ol>
<div><img src="img/5b6d713e-dc10-4839-915e-2cabfb2d357d.png"/></div>
<ol start="10">
<li>Click on EC2:</li>
</ol>
<div><img src="img/0f53486c-da5e-4eb7-98d5-471da09f36dc.png"/></div>
<ol start="11">
<li>AWS provides the option to launch multiple different OSs. While we can start a new instance afresh with a selected OS, we will be instead using an AMI image. AMIs are preconfigured images with installed software. Note that using an AMI image is <strong>not</strong> required, but one is being used here for the tutorial:</li>
</ol>
<div><strong><br/>
<img src="img/e6cef2d9-33da-42c1-b9f9-dd361a3fb11e.png"/></strong></div>
<ol start="12">
<li>Click on <strong>Community AMIs</strong> on the left menu bar and search for <strong>RStudio</strong>. Select the first option and click on the Select button:</li>
</ol>
<div><img src="img/23a8a6e5-a9f3-4853-84d4-a55775c978a6.png"/></div>
<ol start="13">
<li>Select the free tier option (t2.micro) and click Next: Configure Instance Details:</li>
</ol>
<div><img height="468" width="662" src="img/9da49526-d171-49f3-ac57-b008a05340ce.png"/></div>
<ol start="14">
<li>Select the default options on the next page and click <strong>Next:</strong>Add Storage:</li>
</ol>
<div><strong><img height="403" width="692" src="img/93604a2c-0b31-4db1-b763-68b54b728221.png"/></strong></div>
<ol start="15">
<li>Select the default storage options and click on Add Tags:</li>
</ol>
<div><strong><img height="446" width="776" src="img/40244e19-6e3b-4284-805f-a4ae1646e9d8.png"/></strong></div>
<ol start="16">
<li>Click <strong>Next:</strong>Configure Security Group:</li>
</ol>
<div><strong><img src="img/fcd90f09-627e-4686-888a-c079e729fc38.png"/></strong></div>
<ol start="17">
<li>Security groups specify the network access rules for the server. For our tutorial, we will select All TCP and click Review and Launch:</li>
</ol>
<div><img src="img/e66665c3-a7d9-4a4b-b925-b74fd4ac6db3.png"/></div>
<ol start="18">
<li>Click Launch:</li>
</ol>
<div><img src="img/715bf058-18f3-4ff5-9542-873dfdcd7c18.png"/></div>
<ol start="19">
<li>Select Create a new key pair, and click Download Key Pair. Once the key finishes downloading, click on the Launch Instances button:</li>
</ol>
<div><img height="388" width="553" src="img/c0d8e61d-eb02-4db2-95c9-7e7787798247.png"/></div>
<ol start="20">
<li>Click on the instance ID:</li>
</ol>
<div><img height="444" width="656" src="img/d8ba5b7e-5da6-4eaf-b9dc-4c1d8ec44844.png"/></div>
<ol start="21">
<li>Once the status of the instance ID is displayed as running, copy the name of the server, which can be viewed in the bottom panel:</li>
</ol>
<div><img src="img/872d860f-47a9-4570-a61e-102b36d18f7f.png"/></div>
<ol start="22">
<li>Open a new browser, enter the name of the server as the URL, and hit Enter. This will bring up RStudio. Log in with the ID <strong>rstudio</strong> and password <strong>rstudio</strong>:</li>
</ol>
<div><img height="352" width="539" src="img/b27e31e8-6033-4c00-add7-2dea324aaf4c.png"/></div>
<ol start="23">
<li>This will bring up the RStudio console. This is a complete R environment and you can execute R code just as you would in a local installation of R and RStudio:</li>
</ol>
<div><img src="img/3edebd0b-3abd-4851-ade0-3afd02918e99.png"/></div>
<ol start="24">
<li>Once you have finished using R Studio, make sure that you <kbd>terminate</kbd> the instance. Termination stops the billing process. Even though we are using the free tier account, it is good practice to stop or terminate the instance once you have finished your work:</li>
</ol>
<div><img src="img/2ac27a33-994d-4e0e-80ed-9ad936ccc048.png"/></div>
<ol start="25">
<li>Click Sign Out to log out of the AWS console:</li>
</ol>
<div><img height="188" width="380" src="img/9ea90e86-5ad4-4b13-8e49-31659d81cfc0.png"/></div>
<p>Azure also offers free account signups at <a href="https://azure.microsoft.com/en-us/free/" target="_blank">https://azure.microsoft.com/en-us/free/</a>, as shown in the following screenshot:<br/></p>
<div><img src="img/d516e3b3-725a-4b0b-9769-0c4e9932df38.png"/></div>
<p>Google's free cloud signup form is available at <a href="https://cloud.google.com/free/" target="_blank">https://cloud.google.com/free/</a>, as shown in the following screenshot:</p>
<div><img src="img/e1d4fb13-1e0f-4c6e-9c0d-805ed107499a.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we discussed the requirements for deploying enterprise-scale data science infrastructures, both at a software as well as a hardware level. We shared key common questions around such initiatives at a management level. This was followed by an extensive section on key enterprise solutions that are being used for data mining and machine learning in large organizations.</p>
<p>The tutorial involved launching an RStudio Server on Amazon Web Services (a cloud-based system). AWS has become the leading provider of cloud services in the world today, and the exercise showed how simple it can be to launch entire machines in a few seconds. Appropriate pros and cons about the judicious and careful use of AWS to prevent very expensive charges were mentioned.</p>
<p>The next and final chapter will include some closing thoughts, the next steps, and links to useful resources you can use to learn more about the topics that have been discussed in this book.</p>


            

            
        
    </body></html>