["```py\n//Load tab delimited file \nscala> val fp = \"<YourPath>/Oscars.txt\" \nscala> val init_data = spark.read.options(Map(\"header\"->\"true\", \"sep\" -> \"\\t\",\"inferSchema\"->\"true\")).csv(fp) \n//Select columns of interest and ignore the rest \n>>> val awards = init_data.select(\"birthplace\", \"date_of_birth\", \n        \"race_ethnicity\",\"year_of_award\",\"award\").toDF( \n         \"birthplace\",\"date_of_birth\",\"race\",\"award_year\",\"award\") \nawards: org.apache.spark.sql.DataFrame = [birthplace: string, date_of_birth: string ... 3 more fields] \n//register temporary view of this dataset \nscala> awards.createOrReplaceTempView(\"awards\") \n\n//Explore data \n>>> awards.select(\"award\").distinct().show(10,false) //False => do not truncate \n+-----------------------+                                                        \n|award                  | \n+-----------------------+ \n|Best Supporting Actress| \n|Best Director          | \n|Best Actress           | \n|Best Actor             | \n|Best Supporting Actor  | \n+-----------------------+ \n//Check DOB quality. Note that length varies based on month name \nscala> spark.sql(\"SELECT distinct(length(date_of_birth)) FROM awards \").show() \n+---------------------+                                                          \n|length(date_of_birth)| \n+---------------------+ \n|                   15| \n|                    9| \n|                    4| \n|                    8| \n|                   10| \n|                   11| \n+---------------------+ \n\n//Look at the value with unexpected length 4 Why cant we show values for each of the length type ?  \nscala> spark.sql(\"SELECT date_of_birth FROM awards WHERE length(date_of_birth) = 4\").show() \n+-------------+ \n|date_of_birth| \n+-------------+ \n|         1972| \n+-------------+ \n//This is an invalid date. We can either drop this record or give some meaningful value like 01-01-1972 \n\n```", "```py\n    //Load tab delimited file\n    >>> init_data = spark.read.csv(\"<YOURPATH>/Oscars.txt\",sep=\"\\t\",header=True)\n    //Select columns of interest and ignore the rest\n    >>> awards = init_data.select(\"birthplace\", \"date_of_birth\",\n            \"race_ethnicity\",\"year_of_award\",\"award\").toDF(\n             \"birthplace\",\"date_of_birth\",\"race\",\"award_year\",\"award\")\n    //register temporary view of this dataset\n    >>> awards.createOrReplaceTempView(\"awards\")\n    scala>\n    //Explore data\n    >>> awards.select(\"award\").distinct().show(10,False) //False => do not truncate\n    +-----------------------+                                                       \n    |award                  |\n    +-----------------------+\n    |Best Supporting Actress|\n    |Best Director          |\n    |Best Actress           |\n    |Best Actor             |\n    |Best Supporting Actor  |\n    +-----------------------+\n    //Check DOB quality\n    >>> spark.sql(\"SELECT distinct(length(date_of_birth)) FROM awards \").show()\n    +---------------------+                                                         \n    |length(date_of_birth)|\n    +---------------------+\n    |                   15|\n    |                    9|\n    |                    4|\n    |                    8|\n    |                   10|\n    |                   11|\n    +---------------------+\n    //Look at the value with unexpected length 4\\. Note that length varies based on month name\n    >>> spark.sql(\"SELECT date_of_birth FROM awards WHERE length(date_of_birth) = 4\").show()\n    +-------------+\n    |date_of_birth|\n    +-------------+\n    |         1972|\n    +-------------+\n    //This is an invalid date. We can either drop this record or give some meaningful value like 01-01-1972\n\nMost of the datasets contain a date field and unless they come from a single, controlled data source, it is highly likely that they will differ in their formats and are almost always a candidate for cleaning.\n```", "```py\n//UDF to clean date \n//This function takes 2 digit year and makes it 4 digit \n// Any exception returns an empty string \nscala> def fncleanDate(s:String) : String = {  \n  var cleanedDate = \"\" \n  val dateArray: Array[String] = s.split(\"-\") \n  try{    //Adjust year \n     var yr = dateArray(2).toInt \n     if (yr < 100) {yr = yr + 1900 } //make it 4 digit \n     cleanedDate = \"%02d-%s-%04d\".format(dateArray(0).toInt, \n                dateArray(1),yr) \n     } catch { case e: Exception => None } \n     cleanedDate } \nfncleanDate: (s: String)String \n\n```", "```py\n    //This function takes 2 digit year and makes it 4 digit\n    // Any exception returns an empty string\n    >>> def fncleanDate(s):\n          cleanedDate = \"\"\n          dateArray = s.split(\"-\")\n          try:    //Adjust year\n             yr = int(dateArray[2])\n             if (yr < 100):\n                  yr = yr + 1900 //make it 4 digit\n             cleanedDate = \"{0}-{1}-{2}\".format(int(dateArray[0]),\n                      dateArray[1],yr)\n          except :\n              None\n          return cleanedDate\n\n```", "```py\n//UDF to clean birthplace \n// Data explorartion showed that  \n// A. Country is omitted for USA \n// B. New York City does not have State code as well \n//This function appends country as USA if \n// A. the string contains New York City  (OR) \n// B. if the last component is of length 2 (eg CA, MA) \nscala> def fncleanBirthplace(s: String) : String = { \n        var cleanedBirthplace = \"\" \n        var strArray : Array[String] =  s.split(\" \") \n        if (s == \"New York City\") \n           strArray = strArray ++ Array (\"USA\") \n        //Append country if last element length is 2 \n        else if (strArray(strArray.length-1).length == 2) \n            strArray = strArray ++ Array(\"USA\") \n        cleanedBirthplace = strArray.mkString(\" \") \n        cleanedBirthplace } \n\n```", "```py\n    >>> def fncleanBirthplace(s):\n            cleanedBirthplace = \"\"\n            strArray = s.split(\" \")\n            if (s == \"New York City\"):\n                strArray += [\"USA\"]  //Append USA\n            //Append country if last element length is 2\n            elif (len(strArray[len(strArray)-1]) == 2):\n                strArray += [\"USA\"]\n            cleanedBirthplace = \" \".join(strArray)\n            return cleanedBirthplace\n\n```", "```py\n//Register UDFs \nscala> spark.udf.register(\"fncleanDate\",fncleanDate(_:String)) \nres10: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType))) \nscala> spark.udf.register(\"fncleanBirthplace\", fncleanBirthplace(_:String)) \nres11: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType))) \n\n```", "```py\n    >>> from pyspark.sql.types import StringType\n    >>> sqlContext.registerFunction(\"cleanDateUDF\",fncleanDate, StringType())\n    >>> sqlContext.registerFunction( \"cleanBirthplaceUDF\",fncleanBirthplace, StringType())\n\n```", "```py\n//Create cleaned data frame \nscala> var cleaned_df = spark.sql ( \n            \"\"\"SELECT fncleanDate (date_of_birth) dob, \n               fncleanBirthplace(birthplace) birthplace, \n               substring_index(fncleanBirthplace(birthplace),' ',-1)  \n                               country, \n               (award_year - substring_index(fncleanDate( date_of_birth),'-',-1)) age, race, award FROM awards\"\"\") \ncleaned_df: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 4 more fields] \n\n```", "```py\n//Create cleaned data frame \n>>> from pyspark.sql.functions import substring_index>>> cleaned_df = spark.sql (            \"\"\"SELECT cleanDateUDF (date_of_birth) dob,               cleanBirthplaceUDF(birthplace) birthplace,               substring_index(cleanBirthplaceUDF(birthplace),' ',-1) country,               (award_year - substring_index(cleanDateUDF( date_of_birth),               '-',-1)) age, race, award FROM awards\"\"\")\n```", "```py\ncleaned_df = cleaned_df.na.drop //Drop rows with missing values \ncleaned_df.groupBy(\"award\",\"country\").count().sort(\"country\",\"award\",\"count\").show(4,False) \n+-----------------------+---------+-----+                                        \n|award                  |country  |count| \n+-----------------------+---------+-----+ \n|Best Actor             |Australia|1    | \n|Best Actress           |Australia|1    | \n|Best Supporting Actor  |Australia|1    | \n|Best Supporting Actress|Australia|1    | \n+-----------------------+---------+-----+ \n//Re-register data as table \ncleaned_df.createOrReplaceTempView(\"awards\") \n//Find out levels (distinct values) in each categorical variable \nspark.sql(\"SELECT count(distinct country) country_count, count(distinct race) race_count, count(distinct award) award_count from awards\").show() \n+-------------+----------+-----------+                                           \n|country_count|race_count|award_count| \n+-------------+----------+-----------+ \n|           34|         6|          5| \n+-------------+----------+-----------+ \n\n```", "```py\n//Country has too many values. Retain top ones and bundle the rest \n//Check out top 6 countries with most awards. \nscala> val top_countries_df = spark.sql(\"SELECT country, count(*) freq FROM awards GROUP BY country ORDER BY freq DESC LIMIT 6\") \ntop_countries_df: org.apache.spark.sql.DataFrame = [country: string, freq: bigint] \nscala> top_countries_df.show() \n+-------+----+                                                                   \n|country|freq| \n+-------+----+ \n|    USA| 289| \n|England|  57| \n| France|   9| \n| Canada|   8| \n|  Italy|   7| \n|Austria|   7| \n+-------+----+ \n//Prepare top_countries list \nscala> val top_countries = top_countries_df.select(\"country\").collect().map(x => x(0).toString) \ntop_countries: Array[String] = Array(USA, England, New York City, France, Canada, Italy) \n//UDF to fix country. Retain top 6 and bundle the rest into \"Others\" \nscala> import org.apache.spark.sql.functions.udf \nimport org.apache.spark.sql.functions.udf \nscala > val setCountry = udf ((s: String) => \n        { if (top_countries.contains(s)) {s} else {\"Others\"}}) \nsetCountry: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType))) \n//Apply udf to overwrite country \nscala> cleaned_df = cleaned_df.withColumn(\"country\", setCountry(cleaned_df(\"country\"))) \ncleaned_df: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 4 more fields] \n\n```", "```py\n    //Check out top 6 countries with most awards.\n    >>> top_countries_df = spark.sql(\"SELECT country, count(*) freq FROM awards GROUP BY country ORDER BY freq DESC LIMIT 6\")\n    >>> top_countries_df.show()\n    +-------+----+                                                                  \n    |country|freq|\n    +-------+----+\n    |    USA| 289|\n    |England|  57|\n    | France|   9|\n    | Canada|   8|\n    |  Italy|   7|\n    |Austria|   7|\n    +-------+----+\n    >>> top_countries = [x[0] for x in top_countries_df.select(\"country\").collect()]\n    //UDF to fix country. Retain top 6 and bundle the rest into \"Others\"\n    >>> from pyspark.sql.functions import udf\n    >>> from pyspark.sql.types import StringType\n    >>> setCountry = udf(lambda s: s if s in top_countries else \"Others\", StringType())\n    //Apply UDF\n    >>> cleaned_df = cleaned_df.withColumn(\"country\", setCountry(cleaned_df[\"country\"]))\n\n```", "```py\nscala> val splits = Array(Double.NegativeInfinity, 35.0, 45.0, 55.0, \n          Double.PositiveInfinity) \nsplits: Array[Double] = Array(-Infinity, 35.0, 45.0, 55.0, Infinity) \nscala> val bucketizer = new Bucketizer().setSplits(splits). \n                 setInputCol(\"age\").setOutputCol(\"age_buckets\") \nbucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_a25c5d90ac14 \n\n```", "```py\n    >>> splits = [-float(\"inf\"), 35.0, 45.0, 55.0,\n                   float(\"inf\")]\n    >>> bucketizer = Bucketizer(splits = splits, inputCol = \"age\",\n                        outputCol = \"age_buckets\")\n\n```", "```py\n//Define pipeline to convert categorical labels to numerical labels \nscala> import org.apache.spark.ml.feature.{StringIndexer, Bucketizer, VectorAssembler} \nimport org.apache.spark.ml.feature.{StringIndexer, Bucketizer, VectorAssembler} \nscala> import org.apache.spark.ml.Pipeline \nimport org.apache.spark.ml.Pipeline \n//Race \nscala> val raceIdxer = new StringIndexer(). \n           setInputCol(\"race\").setOutputCol(\"raceIdx\") \nraceIdxer: org.apache.spark.ml.feature.StringIndexer = strIdx_80eddaa022e6 \n//Award (prediction target) \nscala> val awardIdxer = new StringIndexer(). \n         setInputCol(\"award\").setOutputCol(\"awardIdx\") \nawardIdxer: org.apache.spark.ml.feature.StringIndexer = strIdx_256fe36d1436 \n//Country \nscala> val countryIdxer = new StringIndexer(). \n         setInputCol(\"country\").setOutputCol(\"countryIdx\") \ncountryIdxer: org.apache.spark.ml.feature.StringIndexer = strIdx_c73a073553a2 \n\n//Convert continuous variable age to buckets \nscala> val splits = Array(Double.NegativeInfinity, 35.0, 45.0, 55.0, \n          Double.PositiveInfinity) \nsplits: Array[Double] = Array(-Infinity, 35.0, 45.0, 55.0, Infinity) \n\nscala> val bucketizer = new Bucketizer().setSplits(splits). \n                 setInputCol(\"age\").setOutputCol(\"age_buckets\") \nbucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_a25c5d90ac14 \n\n//Prepare numerical feature vector by clubbing all individual features \nscala> val assembler = new VectorAssembler().setInputCols(Array(\"raceIdx\", \n          \"age_buckets\",\"countryIdx\")).setOutputCol(\"features\") \nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_8cf17ee0cd60 \n\n//Define data preparation pipeline \nscala> val dp_pipeline = new Pipeline().setStages( \n          Array(raceIdxer,awardIdxer, countryIdxer, bucketizer, assembler)) \ndp_pipeline: org.apache.spark.ml.Pipeline = pipeline_06717d17140b \n//Transform dataset \nscala> cleaned_df = dp_pipeline.fit(cleaned_df).transform(cleaned_df) \ncleaned_df: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 9 more fields] \n//Split data into train and test datasets \nscala> val Array(trainData, testData) = \n        cleaned_df.randomSplit(Array(0.7, 0.3)) \ntrainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [dob: string, birthplace: string ... 9 more fields] \ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [dob: string, birthplace: string ... 9 more fields] \n\n```", "```py\n    //Define pipeline to convert categorical labels to numcerical labels\n    >>> from pyspark.ml.feature import StringIndexer, Bucketizer, VectorAssembler\n    >>> from pyspark.ml import Pipelin\n    //Race\n    >>> raceIdxer = StringIndexer(inputCol= \"race\", outputCol=\"raceIdx\")\n    //Award (prediction target)\n    >>> awardIdxer = StringIndexer(inputCol = \"award\", outputCol=\"awardIdx\")\n    //Country\n    >>> countryIdxer = StringIndexer(inputCol = \"country\", outputCol = \"countryIdx\")\n\n    //Convert continuous variable age to buckets\n    >>> splits = [-float(\"inf\"), 35.0, 45.0, 55.0,\n                   float(\"inf\")]\n    >>> bucketizer = Bucketizer(splits = splits, inputCol = \"age\",\n                        outputCol = \"age_buckets\")\n    >>>\n    //Prepare numerical feature vector by clubbing all individual features\n    >>> assembler = VectorAssembler(inputCols = [\"raceIdx\", \n              \"age_buckets\",\"countryIdx\"], outputCol = \"features\")\n\n    //Define data preparation pipeline\n    >>> dp_pipeline = Pipeline(stages = [raceIdxer,\n             awardIdxer, countryIdxer, bucketizer, assembler])\n    //Transform dataset\n    >>> cleaned_df = dp_pipeline.fit(cleaned_df).transform(cleaned_df)\n    >>> cleaned_df.columns\n    ['dob', 'birthplace', 'country', 'age', 'race', 'award', 'raceIdx', 'awardIdx', 'countryIdx', 'age_buckets', 'features']\n\n    //Split data into train and test datasets\n    >>> trainData, testData = cleaned_df.randomSplit([0.7, 0.3])\n\n```", "```py\nscala> import org.apache.spark.ml.Pipeline \nimport org.apache.spark.ml.Pipeline \nscala> import org.apache.spark.ml.classification.DecisionTreeClassifier \nimport org.apache.spark.ml.classification.DecisionTreeClassifier \n\n//Use Decision tree classifier \nscala> val dtreeModel = new DecisionTreeClassifier(). \n           setLabelCol(\"awardIdx\").setFeaturesCol(\"features\"). \n           fit(trainData) \ndtreeModel: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_76c9e80680a7) of depth 5 with 39 nodes \n\n//Run predictions using testData \nscala> val dtree_predictions = dtreeModel.transform(testData) \ndtree_predictions: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 12 more fields] \n\n//Examine results. Your results may vary due to randomSplit \nscala> dtree_predictions.select(\"award\",\"awardIdx\",\"prediction\").show(4) \n+--------------------+--------+----------+ \n|               award|awardIdx|prediction| \n+--------------------+--------+----------+ \n|       Best Director|     1.0|       1.0| \n|        Best Actress|     0.0|       0.0| \n|        Best Actress|     0.0|       0.0| \n|Best Supporting A...|     4.0|       3.0| \n+--------------------+--------+----------+ \n\n//Compute prediction mismatch count \nscala> dtree_predictions.filter(dtree_predictions(\"awardIdx\") =!= dtree_predictions(\"prediction\")).count() \nres10: Long = 88 \nscala> testData.count \nres11: Long = 126 \n//Predictions match with DecisionTreeClassifier model is about 30% ((126-88)*100/126) \n\n//Train Random forest \nscala> import org.apache.spark.ml.classification.RandomForestClassifier \nimport org.apache.spark.ml.classification.RandomForestClassifier \nscala> import org.apache.spark.ml.classification.RandomForestClassificationModel \nimport org.apache.spark.ml.classification.RandomForestClassificationModel \nscala> import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer} \nimport org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer} \n\n//Build model \nscala> val RFmodel = new RandomForestClassifier(). \n        setLabelCol(\"awardIdx\"). \n        setFeaturesCol(\"features\"). \n        setNumTrees(6).fit(trainData) \nRFmodel: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_c6fb8d764ade) with 6 trees \n//Run predictions on the same test data using Random Forest model \nscala> val RF_predictions = RFmodel.transform(testData) \nRF_predictions: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 12 more fields] \n//Check results \nscala> RF_predictions.filter(RF_predictions(\"awardIdx\") =!= RF_predictions(\"prediction\")).count() \nres29: Long = 87 //Roughly the same as DecisionTreeClassifier \n\n//Try OneVsRest Logistic regression technique \nscala> import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest} \nimport org.apache.spark.ml.classification.{LogisticRegression, OneVsRest} \n//This model requires a base classifier \nscala> val classifier = new LogisticRegression(). \n            setLabelCol(\"awardIdx\"). \n            setFeaturesCol(\"features\"). \n            setMaxIter(30). \n            setTol(1E-6). \n            setFitIntercept(true) \nclassifier: org.apache.spark.ml.classification.LogisticRegression = logreg_82cd24368c87 \n\n//Fit OneVsRest model \nscala> val ovrModel = new OneVsRest(). \n           setClassifier(classifier). \n           setLabelCol(\"awardIdx\"). \n           setFeaturesCol(\"features\"). \n           fit(trainData) \novrModel: org.apache.spark.ml.classification.OneVsRestModel = oneVsRest_e696c41c0bcf \n//Run predictions \nscala> val OVR_predictions = ovrModel.transform(testData) \npredictions: org.apache.spark.sql.DataFrame = [dob: string, birthplace: string ... 10 more fields] \n//Check results \nscala> OVR_predictions.filter(OVR_predictions(\"awardIdx\") =!= OVR_predictions(\"prediction\")).count()          \nres32: Long = 86 //Roughly the same as other models \n\n```", "```py\n    >>> from pyspark.ml import Pipeline\n    >>> from pyspark.ml.classification import DecisionTreeClassifier\n\n    //Use Decision tree classifier\n    >>> dtreeModel = DecisionTreeClassifier(labelCol = \"awardIdx\", featuresCol=\"features\").fit(trainData)\n\n    //Run predictions using testData\n    >>> dtree_predictions = dtreeModel.transform(testData)\n\n    //Examine results. Your results may vary due to randomSplit\n    >>> dtree_predictions.select(\"award\",\"awardIdx\",\"prediction\").show(4)\n    +--------------------+--------+----------+\n    |               award|awardIdx|prediction|\n    +--------------------+--------+----------+\n    |       Best Director|     1.0|       4.0|\n    |       Best Director|     1.0|       1.0|\n    |       Best Director|     1.0|       1.0|\n    |Best Supporting A...|     4.0|       3.0|\n    +--------------------+--------+----------+\n\n    >>> dtree_predictions.filter(dtree_predictions[\"awardIdx\"] != dtree_predictions[\"prediction\"]).count()\n    92\n    >>> testData.count()\n    137\n    >>>\n    //Predictions match with DecisionTreeClassifier model is about 31% ((133-92)*100/133)\n\n    //Train Random forest\n    >>> from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n    >>> from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer\n    >>> from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n    //Build model\n    >>> RFmodel = RandomForestClassifier(labelCol = \"awardIdx\", featuresCol = \"features\", numTrees=6).fit(trainData)\n\n    //Run predictions on the same test data using Random Forest model\n    >>> RF_predictions = RFmodel.transform(testData)\n    //Check results\n    >>> RF_predictions.filter(RF_predictions[\"awardIdx\"] != RF_predictions[\"prediction\"]).count()\n    94     //Roughly the same as DecisionTreeClassifier\n\n    //Try OneVsRest Logistic regression technique\n    >>> from pyspark.ml.classification import LogisticRegression, OneVsRest\n\n    //This model requires a base classifier\n    >>> classifier = LogisticRegression(labelCol = \"awardIdx\", featuresCol=\"features\",\n                  maxIter = 30, tol=1E-6, fitIntercept = True)\n    //Fit OneVsRest model\n    >>> ovrModel = OneVsRest(classifier = classifier, labelCol = \"awardIdx\",\n                    featuresCol = \"features\").fit(trainData)\n    //Run predictions\n    >>> OVR_predictions = ovrModel.transform(testData)\n    //Check results\n    >>> OVR_predictions.filter(OVR_predictions[\"awardIdx\"] != OVR_predictions[\"prediction\"]).count()\n    90  //Roughly the same as other models\n\n```", "```py\nscala> import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n//F1 \nscala> val f1_eval = new MulticlassClassificationEvaluator(). \n                     setLabelCol(\"awardIdx\") //Default metric is F1 \nf1_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_e855a949bb0e \n\n//WeightedPrecision \nscala> val wp_eval = new MulticlassClassificationEvaluator(). \n                     setMetricName(\"weightedPrecision\").setLabelCol(\"awardIdx\") \nwp_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_44fd64e29d0a \n\n//WeightedRecall \nscala> val wr_eval = new MulticlassClassificationEvaluator(). \n                     setMetricName(\"weightedRecall\").setLabelCol(\"awardIdx\") \nwr_eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_aa341966305a \n//Compute measures for all models \nscala> val f1_eval_list = List (dtree_predictions, RF_predictions, OVR_predictions) map ( \n           x => f1_eval.evaluate(x)) \nf1_eval_list: List[Double] = List(0.2330854098674473, 0.2330854098674473, 0.2330854098674473) \nscala> val wp_eval_list = List (dtree_predictions, RF_predictions, OVR_predictions) map ( \n           x => wp_eval.evaluate(x)) \nwp_eval_list: List[Double] = List(0.2661599224979506, 0.2661599224979506, 0.2661599224979506) \n\nscala> val wr_eval_list = List (dtree_predictions, RF_predictions, OVR_predictions) map ( \n           x => wr_eval.evaluate(x)) \nwr_eval_list: List[Double] = List(0.31746031746031744, 0.31746031746031744, 0.31746031746031744) \n\n```", "```py\n    >>> from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n    //F1\n    >>> f1_eval = MulticlassClassificationEvaluator(labelCol=\"awardIdx\") //Default metric is F1\n    //WeightedPrecision\n    >>> wp_eval = MulticlassClassificationEvaluator(labelCol=\"awardIdx\", metricName=\"weightedPrecision\")\n    //WeightedRecall\n    >>> wr_eval = MulticlassClassificationEvaluator(labelCol=\"awardIdx\", metricName=\"weightedRecall\")\n    //Accuracy\n    >>> acc_eval = MulticlassClassificationEvaluator(labelCol=\"awardIdx\", metricName=\"Accuracy\")\n    //Compute measures for all models\n    >>> f1_eval_list = [ f1_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]\n    >>> wp_eval_list = [ wp_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]\n    >>> wr_eval_list = [ wr_eval.evaluate(x) for x in [dtree_predictions, RF_predictions, OVR_predictions]]\n    //Print results for DecisionTree, Random Forest and OneVsRest\n    >>> f1_eval_list\n    [0.2957949866055487, 0.2645186821042419, 0.2564967990214734]\n    >>> wp_eval_list\n    [0.3265407181548341, 0.31914852065228005, 0.25295826631254753]\n    >>> wr_eval_list\n    [0.3082706766917293, 0.2932330827067669, 0.3233082706766917]\n\n```"]