<html><head></head><body>
        

                            
                    <h1 class="header-title">Big Data Mining for the Masses</h1>
                
            
            
                
<p>Implementing a big data mining platform in an enterprise environment that serves specific business requirements is non-trivial. While it is relatively simple to build a big data platform, the novel nature of the tools present a challenge in terms of adoption by business-facing users used to traditional methods of data mining. This, ultimately, is a measure of how successful the platform becomes within an organization.</p>
<p>This chapter introduces some of the salient characteristics of big data analytics relevant for both practitioners and end users of analytics tools. This will include the following topics:</p>
<ul>
<li>What is big data mining?</li>
<li>Big data mining in the enterprise:
<ul>
<li>Building a use case</li>
<li>Stakeholders of the solution</li>
<li>Implementation life cycle</li>
</ul>
</li>
<li>Key technologies in big data mining:
<ul>
<li>Selecting the hardware stack:
<ul>
<li>Single/multinode architecture</li>
<li>Cloud-based environments</li>
</ul>
</li>
<li>Selecting the software stack:
<ul>
<li>Hadoop, Spark, and NoSQL</li>
<li>Cloud-based environments</li>
</ul>
</li>
</ul>
</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">What is big data mining?</h1>
                
            
            
                
<p>Big data mining forms the first of two broad categories of big data analytics, the other being Predictive Analytics, which we will cover in later chapters. In simple terms, big data mining refers to the entire life cycle of processing large-scale datasets, from procurement to implementation of the respective tools to analyze them.</p>
<p>The next few chapters will illustrate some of the high-level characteristics of any big data project that is undertaken in an organization.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Big data mining in the enterprise</h1>
                
            
            
                
<p>Implementing a big data solution in a medium to large size enterprise can be a challenging task due to the extremely dynamic and diverse range of considerations, not the least of which is determining what specific business objectives the solution will address.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Building the case for a Big Data strategy</h1>
                
            
            
                
<p>Perhaps the most important aspect of big data mining is determining the appropriate use cases and needs that the platform would address. The success of any big data platform depends largely on finding relevant problems in business units that will deliver measurable value for the department or organization. The hardware and software stack for a solution that collects large volumes of sensor or streaming data will be materially different from one that is used to analyze large volumes of internal data.</p>
<p>The following are some suggested steps that, in my experience, have been found to be particularly effective in building and implementing a corporate big data strategy:</p>
<ul>
<li><strong>Who needs big data mining</strong>: Determining which business groups will benefit most significantly from a big data mining solution is the first step in this process. This would typically entail groups that are already working with large datasets, are important to the business, and have a direct revenue impact, and optimizing their processes in terms of data access or time to analyze information would have an impact on the daily work processes.<br/>
As an example, in a pharmaceutical organization, this could include Commercial Research, Epidemiology, Health Economics, and Outcomes. At a financial services organization, this could include Algorithmic Trading Desks, Quantitative Research, and even Back Office.</li>
</ul>
<ul>
<li><strong>Determining the use cases</strong>: The departments identified in the preceding step might already have a platform that delivers the needs of the group satisfactorily. Prioritizing among multiple use cases and departments (or a collection of them) requires personal familiarity with the work being done by the respective business groups.<br/>
Most organizations follow a hierarchical structure where the interaction among business colleagues is likely to be mainly along <strong>rank lines</strong>. Determining impactful analytics use cases requires a close collaboration between both the practitioner as well as the stakeholder; namely, both the management who has oversight of a department as well as the staff members who perform the hands-on analysis. The business stakeholder can shed light on which aspects of his or her business will benefit the most from more efficient data mining and analytics environment. The practitioners provide insight on the challenges that exist at the hands-on operational level. Incremental improvements that consolidate both the operational as well as the managerial aspects to determine an optimal outcome are bound to deliver faster and better results.</li>
</ul>
<ul>
<li><strong>Stakeholders' buy-in</strong>: The buy-in of the stakeholders—in other words, a consensus among decision-makers and those who can make independent budget decisions—should be established prior to commencing work on the use case(s). In general, multiple buy-ins should be secured for redundancy such that there is a pool of primary and secondary sources that can provide appropriate support and funding for an extension of any early-win into a broader goal. The buy-in process does not have to be deterministic and this may not be possible in most circumstances. Rather, a general agreement on the value that a certain use case will bring is helpful in establishing a baseline that can be leveraged on the successful execution of the use case.</li>
<li><strong>Early-wins and the effort-to-reward ratio</strong>: Once the appropriate use cases have been identified, finding the ones that have an optimal effort-to-reward ratio is critical. A relatively small use case that can be implemented in a short time within a smaller budget to optimize a specific business-critical function helps in showcasing early-wins, thus adding credibility to the big data solution in question. We cannot precisely quantify these intangible properties, but we can hypothesize:</li>
</ul>
<div><img height="60" width="400" src="img/a4c6d708-32ca-4b4a-8832-43f32e9567ab.png"/></div>
<p style="padding-left: 90px">In this case, <em>effort</em> is the time and work required to implement the use case. This includes aspects such as how long it would take to procure the relevant hardware and/or software that is part of the solution, the resources or equivalent <em>man-hours</em> it will take to implement the solution, and the overall operational overhead. An open source tool might have a lower barrier to entry relative to implementing a commercial solution that may involve lengthy procurement and risk analysis by the organization. Similarly, a project that spans across departments and would require time from multiple resources who are already engaged in other projects is likely to have a longer duration than one that can be executed by the staff of a single department. If the net effort is low enough, one can also run more than one exercise in parallel as long as it doesn’t compromise the quality of the projects.</p>
<ul>
<li><strong>Leveraging the early-wins</strong>: The successful implementation of one or more of the projects in the early-wins phase often lays the groundwork to develop a bigger strategy for the big data analytics platform that goes far beyond the needs of just a single department and has a broader organizational-level impact. As such, the early-win serves as a first, but crucial, step in establishing the value of big data to an audience, who may or may not be skeptical of its viability and relevance.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementation life cycle</h1>
                
            
            
                
<p>As outlined earlier, the implementation process can span multiple steps. These steps are often iterative in nature and require a trial-and-error approach. This will require a fair amount of perseverance and persistence as most undertakings will be characterized by varying degrees of successes and failures.</p>
<p>In practice, a Big Data strategy will include multiple stakeholders and a collaborative approach often yields the best results. Business sponsors, business support and IT &amp;amp; Analytics are three broad categories of stakeholders that together create a proper unified solution, catering to the needs of the business to the extent that budget and IT capabilities will permit.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Stakeholders of the solution</h1>
                
            
            
                
<p>The exact nature of the stakeholders of a big data solution is subjective and would vary depending on the use case and problem domain. In general, the following can be considered a high-level representation of this:</p>
<ul>
<li><strong>Business sponsor</strong>: The individual or department that provides the support and/or funding for the project. In most cases, this entity would also be the beneficiary of the solution.</li>
<li><strong>Implementation group</strong>: The team that implements the solution from a hands-on perspective. This is usually the IT or Analytics department of most companies that is responsible for the design and deployment of the platform.</li>
<li><strong>IT procurement</strong>: The procurement department in most organizations is responsible for vetting a solution to evaluate its competitive pricing and viability from an organizational perspective. Compliance with internal IT policies and assessment of other aspects such as licensing costs are some of services provided by procurement, especially for commercial products.</li>
<li><strong>Legal</strong>: All products, unless developed in-house, will most certainly have associated terms and conditions of use. Open source products can have a wide range of properties that defines the permissibility and restrictiveness of use. Open source software licenses such as Apache 2.0, MIT, and BSD are generally more permissible relative to GNU <strong>GPL</strong> (<strong>General Purpose License</strong>). For commercial solutions, the process is more involved as it requires the analysis of vendor-specific agreements and can take a long time to evaluate and get approved depending on the nature of the licensing terms and conditions.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing the solution</h1>
                
            
            
                
<p>The final implementation of the solution is the culmination of the collaboration between the implementation group, business beneficiaries, and auxiliary departments. The time to undertake projects from start to end can vary anywhere from 3-6 months for most small-sized projects as explained in the section on early-wins. Larger endeavors can take several months to years to accomplish and are marked by an agile framework of product management where capabilities are added incrementally during the implementation and deployment period.</p>
<p>The following screenshot gives us a good understanding of the concept:</p>
<div><img height="241" width="299" src="img/20ad7f91-d721-447f-8465-010285dd4f34.png"/></div>
<p>High level image showing the workflow</p>
<p>The images and icons have been taken from:</p>
<ul>
<li><a href="https://creativecommons.org/licenses/by/3.0/us/" target="_blank">https://creativecommons.org/licenses/by/3.0/us/</a></li>
<li>Icons made by Freepik (<a href="http://www.freepik.com" target="_blank">http://www.freepik.com</a>) from www.flaticon.com is licensed by CC 3.0 BY</li>
<li>Icons made by Vectors Market (<a href="http://www.flaticon.com/authors/vectors-market" target="_blank">http://www.flaticon.com/authors/vectors-market</a>) from <a href="http://www.flaticon.com" target="_blank">www.flaticon.com</a> is licensed by CC 3.0 BY</li>
<li>Icons made by Prosymbols (<a href="http://www.flaticon.com/authors/prosymbols" target="_blank">http://www.flaticon.com/authors/prosymbols</a>) from <a href="http://www.flaticon.com" target="_blank">www.flaticon.com</a> is licensed by CC 3.0 BY</li>
<li>Vectors by Vecteezy (<a href="https://www.vecteezy.com" target="_blank">https://www.vecteezy.com</a>)</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical elements of the big data platform</h1>
                
            
            
                
<p>Our discussion, so far, has been focused on the high-level characteristics of design and deployment of big data solutions in an enterprise environment. We will now shift attention to the technical aspects of such undertakings. From time to time, we’ll incorporate high-level messages where appropriate in addition to the technical underpinnings of the topics in discussion.</p>
<p>At the technical level, there are primarily two main considerations:</p>
<ul>
<li>Selection of the hardware stack</li>
<li>Selection of the software and <strong>BI</strong> (<strong>business intelligence</strong>) platform</li>
</ul>
<p>Over the recent 2-3 years, it has become increasingly common for corporations to move their processes to cloud-based environments as a complementary solution for in-house infrastructures. As such, cloud-based deployments have become exceedingly common and hence, an additional section on on-premises versus cloud-based has been added. Note that the term <em>On-premises</em> can be used interchangeably with <strong>In-house</strong>, <strong>On-site</strong>, and other similar terminologies.</p>
<p>You’d often hear the term <strong>On-premise</strong> being used as an alternative for <em>On-premises.</em> The correct term is <strong>On-premises</strong>. The term <strong>premise</strong> is defined by the Chambers Dictionary as <em>premise noun 1 (also premises) something assumed to be true as a basis for stating something further.</em> <strong>Premises,</strong> on the other hand, is a term used to denote buildings (among others) and arguably makes a whole lot more sense.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Selection of the hardware stack</h1>
                
            
            
                
<p>The choice of hardware often depends on the type of solution that is chosen and where the hardware would be located. The proper choice depends on several key metrics such as the type of data (structured, unstructured, or semi-structured), the size of data (gigabytes versus terabytes versus petabytes), and, to an extent, the frequency with which the data will be updated. The optimal choice requires a formal assessment of these variables and will be discussed later on in the book. At a high-level, we can surmise three broad models of hardware architecture:</p>
<ul>
<li><strong>Multinode architecture</strong>: This would typically entail multiple nodes (or servers) that are interconnected and work on the principle of multinode or distributed computing. A classic example of a multinode architecture is Hadoop, where multiple servers maintain bi-directional communication to coordinate a job. Other technologies such as a NoSQL database like Cassandra and search and analytics platform like Elasticsearch also run on the principle of multinode computing architecture. Most of them leverage <em>commodity servers</em>, another name for relatively low-end machines by enterprise standards that work in tandem to provide large-scale data mining and analytics capabilities. Multinode architectures are suitable for hosting data that is in the range of terabytes and above.</li>
<li><strong>Single-node architecture</strong>: Single-node refers to computation done on a single server. This is relatively uncommon with the advent of multinode computing tools, but still retains a huge advantage over distributed computing architectures. The <em>Fallacy of Distributed Computing</em> outlines a set of assertions, or assumptions, related to the implementation of distributed systems such as the reliability of the network, cost of latency, bandwidth, and other considerations.<br/>
If the dataset is structured, contains primarily textual data, and is in the order of 1-5 TB, in today’s computing environment, it is entirely possible to host such datasets on single-node machines using specific technologies as has been demonstrated in later chapters.</li>
</ul>
<ul>
<li><strong>Cloud-based architecture</strong>: Over the past few years, numerous cloud-based solutions have appeared in the industry. These solutions have greatly reduced the barrier to entry in big data analytics by providing a platform that makes it incredibly easy to provision hardware resources on demand based on the needs of the task at hand. This materially reduces the significant overhead in procuring, managing, and maintaining physical hardware and hosting them at in-house data center facilities.</li>
</ul>
<p>Cloud platforms such as Amazon Web Services, Azure from Microsoft, and the Google Compute Environment permit enterprises to provision 10s to 1000s of nodes at costs starting as low as 1 cent per hour per instance.</p>
<p>In the wake of the growing dominance of cloud vendors over traditional brick-and-mortar hosting facilities, several complementary services to manage client cloud environments have come into existence.</p>
<p>Examples include cloud management companies, such as Altiscale that provides big data as a service solutions and IBM Cloud Brokerage that facilitates selection and management of multiple cloud-based solutions.</p>
<div><strong>The exponential decrease in the cost of hardware</strong>: The cost of hardware has gone down exponentially over the past few years. As a case in point, per Statistic Brain’s research, the cost of hard drive storage in 2013 was approximately 4 cents per GB. Compare that with $7 per GB as recent as 2000 and over $100,000 per GB in the early 80’s. Given the high cost of licensing commercial software, which can often exceed the cost of the hardware, it makes sense to allocate enough budget toward procuring capable hardware solutions. Software needs appropriate hardware to provide optimal performance and providing level importance toward hardware selection is just as important as selecting the appropriate software.</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Selection of the software stack</h1>
                
            
            
                
<p>The selection of the software stack for data mining varies based on individual circumstances. The most popular options specific to data mining are shown along with a couple of alternatives which, although not as well-known, are just as capable of managing large-scale datasets:</p>
<ul>
<li><strong>The Hadoop ecosystem</strong>: The big data terms arguably got their start in the popular domain with the advent of Hadoop. The Hadoop ecosystem consists of multiple projects run under the auspices of the Apache Software Foundation. Hadoop supports nearly all the various types of datasets—such as structured, unstructured, and semi-structured—well-known in the big data space. Its thriving ecosystem of auxiliary tools that add new functionalities as well as a rapidly evolving marketplace where companies are vying to demonstrate the next-big-thing-in-Big-Data means that Hadoop will be here for the foreseeable future. There are four primary components of Hadoop, apart from the projects present in the large ecosystem. They are as follows:
<ul>
<li><strong>Hadoop Common</strong>: The common utilities that support the other Hadoop modules</li>
<li><strong>Hadoop Distributed File System (HDFS™)</strong>: A distributed filesystem that provides high-throughput access to application data</li>
<li><strong>Hadoop YARN</strong>: A framework for job scheduling and cluster resource management</li>
<li><strong>Hadoop MapReduce</strong>: A YARN-based system for parallel processing of large datasets</li>
</ul>
</li>
<li><strong>Apache Spark™</strong>: Apache Spark was a project for a multinode computing framework first conceived at University of California at Berkeley’s AMPLab as a platform that provided a seamless interface to run parallel computations and overcome limitations in the Hadoop MapReduce framework. In particular, Spark internally leverages a concept known as <strong>DAG</strong>—<strong>directed acyclic graphs</strong>—which indicates a functionality that optimizes a set of operations into a smaller, or more computationally efficient, set of operations. In addition, Spark exposes several <strong>APIs</strong>—<strong>application programming interfaces</strong>—to commonly used languages such as Python (PySpark) and Scala (natively available interface). This removes one of the barriers of entry into the Hadoop space where a knowledge of Java is essential.</li>
</ul>
<p style="padding-left: 60px">Finally, Spark introduces a data structure called <strong>Resilient Distributed Datasets</strong> (<strong>RDD</strong>), which provides a mechanism to store data in-memory, thus improving data retrieval and subsequently processing times dramatically:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Cluster manager</strong>: The nodes constituting a Spark cluster communicate using cluster managers, which manage the overall coordination among the nodes that are part of the cluster. As of writing this, the cluster manager can be the standalone Spark cluster manager, Apache Mesos, or YARN. There is also an additional facility of running Spark on AWS EC2 instances using spark-ec2 that automatically sets up an environment to run Spark programs.</li>
</ul>
</li>
</ul>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Distributed</strong> <strong>storage</strong>: Spark can access data from a range of underlying distributed storage systems such as HDFS, S3 (AWS Storage), Cassandra, HBase, Hive, Tachyon, and any Hadoop data source. It should be noted that Spark can be used as a standalone product and does <em>not</em> require Hadoop for operations. Newcomers to Spark are often under the impression that Hadoop, or more concretely an HDFS filesystem, is needed for Spark operations. This is not true. Spark can support multiple types of cluster managers as well as backend storage systems, as shown in this section.</li>
</ul>
</li>
</ul>
<ul>
<li><strong>NoSQL and traditional databases</strong>: A third consideration in terms of selecting the software stack are NoSQL databases. The term NoSQL came into existence recently and is meant to distinguish databases that do not follow the traditional relational-database models. There are both open source and commercial variations of NoSQL databases and indeed even cloud-based options that have become increasingly common. There are various broad classifications of NoSQL databases and some of the more common paradigms are as follows:
<ul>
<li><strong>Key-value</strong>: These NoSQL databases store data on a principle of hashing—a unique key identifies a set of properties about the key. An example of a key in this parlance could be the national ID number of an individual (such as the Social Security Number or SSN in the US and Aadhaar in India). This could be associated with various aspects relating to the individual such as name, address, phone number, and other variables. The end user of the database would query by the ID number to directly access information about the individual. Open source Key-Value databases such as Redis and commercial ones such as Riak are very popular.</li>
<li><strong>In-memory</strong>: While databases that have used in-memory facilities, such as storing caches in the memory to provide faster access relative to storing on disk, have always existed, they were adopted more broadly with the advent of big data. Accessing data in-memory is orders of magnitude faster (~ 100 nanoseconds) than accessing the same information from disk (1-10 milliseconds or 100,000 times slower). Several NoSQL databases, such as Redis and KDB+, leverage temporary in-memory <strong>storage</strong> in order to provide faster access to frequently used data.</li>
<li><strong>Columnar</strong>: These databases append multiple columns of data as opposed to rows to create a table. The primary advantage of columnar storage over row-based storage is that a columnar layout provides the means to access data faster with reduced I/O overhead and is particularly well-suited for analytics use cases. By segregating data into individual columns, the database query can retrieve data by scanning the appropriate columns instead of scanning a table on a row-by-row basis and can leverage parallel processing facilities extremely well. Well-known columnar databases include Cassandra, Google BigTable, and others.</li>
<li><strong>Document-oriented</strong>: In many ways considered a step up from pure key-value stores, document-oriented databases store data that do not conform to any specific schema such as unstructured text like news articles. These databases provide ways to encapsulate the information in multiple key-value pairs that do not have to be necessarily consistent in structure across all other entries. As a consequence, document databases such as MongoDB are used widely in media-related organizations such as NY Times and Forbes in addition to other mainstream companies.</li>
</ul>
</li>
<li><strong>Cloud-based solutions:</strong> Finally, cloud-based solutions for large-scale data mining such as AWS Redshift, Azure SQL Data Warehouse, and Google Bigquery permit users to query datasets directly on the cloud-vendor’s platform without having to create their own architecture. Although the end user can choose to have their own in-house specialists such as Redshift System Administrators, the management of the infrastructure, maintenance, and day-to-day routine tasks are mostly carried out by the vendor, thus reducing the operational overhead on the client side.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we got a high-level overview of Big Data and some of the components of implementing a Big Data solution in the Enterprise. Big Data requires selection of an optimal software and hardware stack, an effort that is non-trivial, not least because of the hundreds of solutions in the industry. Although the topic of a Big Data strategy may be deemed as a subject best left for management rather than a technical audience, it is essential to understand the nuances.</p>
<p>Note that without a proper, well-defined strategy and corresponding high level support, IT departments will remain limited in the extent to which they can provide successful solutions. Further, the solution, including the hardware-software stack should be such that it can be adequately managed and supported by existing IT resources. Most companies will find that it would be essential to recruit new hires for the Big Data implementation. Since such implementations require evaluation of various elements - business needs, budget, resources and other variables, a lead time, often of a few months to an year and more would be needed depending on the scale and scope.</p>
<p>These topics will be discussed in depth in later chapters and this section serves as a preliminary introduction to the subject.</p>


            

            
        
    </body></html>