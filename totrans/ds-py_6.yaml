- en: '*Chapter 7*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processing Human Language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Create machine learning models for textual data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the NLTK library to preprocess text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize regular expressions to clean and analyze strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create word embedding using the Word2Vec model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter shall cover the concepts on processing human language.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important goals of artificial intelligence (AI) is to understand
    the human language to perform tasks. Spellcheck, sentiment analysis, question
    answering, chat bots, and virtual assistants (such as Siri and Google Assistant)
    all have a natural language processing (NLP) module. The NLP module enables virtual
    assistants to process human language and perform actions based on it. For example,
    when we say, "OK Google, set an alarm for 7 A.M.", the speech is first converted
    to text and then this text is processed by the NLP module. After this processing,
    the virtual assistant calls the appropriate API of the Alarm/Clock application.
    Processing human language has its own set of challenges because it is ambiguous,
    with words meaning different things depending on the context in which they are
    used. This is the biggest pain point of language for AI.
  prefs: []
  type: TYPE_NORMAL
- en: Another big reason is the unavailability of complete information. We tend to
    leave out most of the information while communicating; information that is common
    sense or things that are universally true or false. For example, the sentence
    "I saw a man on a hill with a telescope" can have different meanings depending
    on the contextual information. For example, it could mean that "I saw a man who
    had a telescope on a hill," but it could also mean that "I saw a man on a hill
    through a telescope." It is very difficult for computers to keep track of this
    information as most of it is contextual. Due to the advances in deep learning,
    NLP today works much better than when we used traditional methods such as clustering
    and linear models. This is the reason we will use deep learning on text corpora
    to solve NLP problems. NLP, like any other machine learning problem, has two main
    parts, data processing and model creation. In the next topic, we will learn how
    to process textual data, and later, we will learn how to use this processed data
    to create machine learning models to solve our problems.
  prefs: []
  type: TYPE_NORMAL
- en: Text Data Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we start building machine learning models for our textual data, we need
    to process the data. First, we will learn the different ways in which we can understand
    what the data comprises. This helps us get a sense of what the data really is
    and decide on the preprocessing techniques to be used in the next step. Next,
    we will move on to learn the techniques that will help us preprocess the data.
    This step helps reduce the size of the data, thus reducing the training time,
    and also helps us transform the data into a form that would be easier for machine
    learning algorithms to extract information from. Finally, we will learn how to
    convert the textual data to numbers so that machine learning algorithms can actually
    use it to create models. We do this using word embedding, much like the entity
    embedding we performed in *Chapter 5*: *Mastering Structured Data*.'
  prefs: []
  type: TYPE_NORMAL
- en: Regular Expressions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we start working on textual data, we need to learn about regular expressions
    (RegEx). RegEx is not really a preprocessing technique, but a sequence of characters
    that defines a search pattern in a string. RegEx is a powerful tool when dealing
    with textual data as it helps us find sequences in a collection of text. A RegEx
    consists of metacharacters and regular characters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: Tables containing metacharacters used in RegEx, and some examples'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.1: Tables containing metacharacters used in RegEx, and some examples'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Using RegEx, we can search for complex patterns in a text. For example, we
    can use it to remove URLs from a text. We can use the `re` module in Python to
    remove a URL as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`re.sub` accepts three parameters: the first is RegEx, the second is the expression
    you want to substitute in place of the matched pattern, and the third is the text
    in which it should search for the pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: Output command'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.2: Output command'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It is difficult to remember all the RegEx conventions, so when working with
    RegEx, refer to a cheat sheet, such as: (http://www.pyregex.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 54: Using RegEx for String Cleaning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will use the `re` module of Python to modify and analyze
    a string. We will simply learn how to use RegEx in this exercise, and in the following
    section, we will see how we can use RegEx to preprocess our data. We will use
    a single review from the IMDB Movie Review dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter07),
    which we shall also work on later in the chapter to create sentiment analysis
    models. This dataset is already processed, and some words have been removed. This
    will be the case sometimes when dealing with prebuilt datasets, so it is important
    to analyze the dataset you are working on before you start working.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will use a movie review from IMDB. Save the review text
    into a variable, as in the following code. You can use any other paragraph of
    text for this exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the length of the review to know by how much we should reduce the
    size. We will use `len(string)` and get the output, as shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output length is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.3: Length of the string'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.3: Length of the string'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Sometimes, when you scrape data from websites, hyperlinks get recorded as well.
    Most of the times, hyperlinks do not provide us any information. Remove any hyperlink
    from the data using a complex regex string, as in "`https?\://\S+`". This selects
    any substring with `https://` in it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The string with hyperlinks is removed as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.4: The string with hyperlinks removed'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.4: The string with hyperlinks removed'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, we will remove the `br` HTML tags from the text, which we observed while
    reading the string. Sometimes, these HTML tags get added to the scrapped data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The string without the `br` tags is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.5: The string without br tags'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.5: The string without br tags'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, we will remove all the digits from the text. This helps us reduce the
    size of the dataset when digits are of no significance to us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The string without digits is shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.6: The string without digits'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.6: The string without digits'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, we will remove all special characters and punctuations. Depending on
    your problem, these could just be taking up space and not providing relevant information
    for the machine learning algorithms. So, we remove them with the following regex
    pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The string without special characters and punctuations is shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.7: The string without special characters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.7: The string without special characters'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, we will substitute `can''t` with `cannot` and `it''s` with `it is`. This
    helps us reduce the training time as the number of unique words reduces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final string is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.8: The final string'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.8: The final string'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Finally, we will calculate the length of the cleaned string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output size of the string is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.9: The length of cleaned string'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.9: The length of cleaned string'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We reduced the size of the review by 14%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will use RegEx to analyze the data and get all the words that start
    with a capital letter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The words are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.10: Words starting with capital letters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.10: Words starting with capital letters'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To find all the one- and two-letter words in the text, use the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.11: One and two letter words'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.11: One and two letter words'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You successfully modified and analyzed a review string using
    RegEx with the `re` module.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Feature Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Basic feature extraction helps us understand what our data consists of. This
    helps us select the steps to take to preprocess the dataset. Basic feature extraction
    consists of actions such as calculation of the average number of words and count
    of special characters. We will make use of the IMDB movie review dataset in this
    section as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what our dataset consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12: SentimentText data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.12: SentimentText data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `SentimentText` variable contains the actual review and the `Sentiment`
    variable contains the sentiment of the review. `1` represents a positive sentiment
    and `0` represents a negative sentiment. Let''s print the first review to get
    a sense of the data we are dealing with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The first review is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13: First review'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.13: First review'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, we will try to understand the kind of data we are working with by getting
    the key statistics of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of words**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the number of words in each review with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, the `word_count` variable in the DataFrame contains the total number of
    words in the review. The `apply` function applies the `split` function to each
    row of the dataset iteratively. Now, we can get the average number of words for
    each class to see if positive reviews have more words than negative reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mean()` function calculates the average of a column in pandas. For negative
    reviews, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The average number of words for a negative sentiment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14: Total number of words for negative sentiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.14: Total number of words for negative sentiment'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For positive reviews, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The average number of words for a positive sentiment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15: Total number of words for positive sentiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.15: Total number of words for positive sentiment'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that there isn't much difference in the average number of words for
    either class.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stop words**'
  prefs: []
  type: TYPE_NORMAL
- en: Stop words are the most common words in a language â€“ for example, "I", "me",
    "my", "yours", and "the." Most of the time, these words provide no real information
    about the sentence, so we remove these words from our dataset to reduce the size.
    The `nltk` library has a list of stop words for the English language that we can
    access.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the count of these stop words, we can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can see the average number of stop words for each class, by using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The average number of stop words for a negative sentiment is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C13322_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.16: Average number of stop words for a negative sentiment'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, to get the number of stop words for a positive sentiment, we use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output average number of stop words for a positive sentiment is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17: Average number of stop words for a positive sentiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.17: Average number of stop words for a positive sentiment'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Number of special characters**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the kind of problem you are dealing with, you will want to either
    keep special characters such as `@`, `#`, `$`, and `*`, or remove them. To be
    able to do that, you first must figure out how many special characters occur in
    your dataset. To get the count of `^`, `&`, `*`, `$`, `@`, and `#` in your dataset,
    use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Text Preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we know what our data comprises, we need to preprocess it so that our
    machine learning algorithms can easily find patterns in the text. In this section,
    we will go over some of the techniques used to clean and reduce the dimensionality
    of the data we feed into our machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Lowercase**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first preprocessing step we perform is converting all the data into lowercase.
    This prevents multiple copies of the same word. You can easily convert all text
    to lowercase using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The apply function applies the `lower` function to each row of the dataset iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stop word removal**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed previously, stop words should be removed from the dataset as they
    add very little useful information. Stop words do not affect the sentiments of
    a sentence. We perform this step to remove any bias that stop words might introduce:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Frequent word removal**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stop words are more general words such as ''a'', ''an,'' and ''the.'' However,
    in this step, you will remove the most frequent word from the dataset you are
    working with. For example, the words that can be removed from a tweet dataset
    are `RT`, `@username`, and `DM`. First, find the most frequent words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The most frequent words are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18: Most frequent words in tweet dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.18: Most frequent words in tweet dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From the output, we get a hint: the text contains HTML tags, which can be removed
    to considerably reduce the dataset size. So, let''s first remove all `<br />`
    HTML tags and then remove words such as ''movie'' and ''film,'' which will not
    have much impact on the sentiment detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Punctuation and special character removal**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we remove all the punctuations and special characters from the text as
    they add very little information to the text. To remove punctuations and special
    characters, use this regex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The regex selects all alphanumerical characters and spaces.
  prefs: []
  type: TYPE_NORMAL
- en: '**Spellcheck**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, incorrect spellings of the same word causes us to have copies of
    the same word. This can be corrected by performing a spellcheck using the autocorrect
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**Stemming**'
  prefs: []
  type: TYPE_NORMAL
- en: '`nltk` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Spellcheck, stemming, and lemmatization can take a lot of time to complete depending
    on the size of the dataset, so make sure that you do need to perform this step
    by looking into the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Lemmatization**'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You should prefer lemmatization to stemming as it is more effective.
  prefs: []
  type: TYPE_NORMAL
- en: '`nltk` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We are working to reduce the dimensionality of the dataset because of the "curse
    of dimensionality." Datasets become sparse as their dimensionality (dependent
    variables) increases. This causes data science techniques to fail. This is due
    to the difficulty in modelling the high number of features (dependent variables)
    to get the correct output. As the number of features of the dataset increases,
    we need more data points to model them. So, to get around the curse of high-dimensional
    data, we need to obtain a lot more data, which in turn would increase the time
    required to process it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokenization**'
  prefs: []
  type: TYPE_NORMAL
- en: '`nltk` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The tokenized list is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19: Tokenized list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.19: Tokenized list'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, it separates punctuations from words and detects complex words
    such as "Dr."
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 55: Preprocessing the IMDB Movie Review Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will preprocess the IMDB Movie Review dataset to make it
    ready for any machine learning algorithm. The dataset consists of 25,000 movie
    reviews along with the sentiment (positive or negative) of the review. We want
    to predict sentiments using the review, so we need to keep that in mind while
    performing preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the IMDB movie review dataset using pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, we will convert all characters in the dataset into lowercase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will write a `clean_str` function, in which we will clean the reviews
    using the `re` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use the apply function of pandas to perform review cleaning on the complete
    dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, check the word distribution in the dataset using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The occurrence of the top 10 words is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.20: Top 10 words'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.20: Top 10 words'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Remove stop words from the reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: This will be done by first tokenizing the reviews and then removing the stop
    word loaded from the `nltk` library.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We add ''`movie`,'' ''`film`,'' and ''`time`'' to the stop words as they occur
    very frequently in the reviews and don''t really contribute much to understanding
    what the review sentiment is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we convert the tokens back into sentences and drop the reviews where
    all the text was stop words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to convert the text into tokens and then numbers. We will
    be using the Keras Tokenizer as it performs both the steps for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get the size of the vocabulary, use the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The number of unique tokens is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13322_07_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.21: Number of unique tokens'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: To reduce the training time of our model, we will cap the length of our reviews
    at 200 words. You can play around with this number to find out what gives you
    the best accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should save the tokenizer so that you can convert the reviews back to text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To preview a cleaned review, run the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A cleaned review looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.22: A cleaned review'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.22: A cleaned review'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To get the actual input to the next step of the process, run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The input to the next step for the `reviews` command will look something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23: Input for next step to the cleaned review'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_23.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.23: Input for next step to the cleaned review'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You have successfully preprocessed your first text dataset.
    The review data is now a matrix of 25,000 rows, or reviews, and 200 columns, or
    words. Next, we will learn how we can convert this data into embedding to make
    it easier to predict the sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: Text Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have cleaned our dataset, we will convert it into a form that machine
    learning models can work with. Recall *Chapter 5*, *Mastering Structured Data*,
    where we discussed how neural networks cannot process words, so we need to represent
    words as numbers to be able to process them. Therefore, to be able to perform
    tasks such as sentiment analysis, we need to convert text into numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the very first method we discussed was one-hot encoding, which performs
    poorly in the case of words, because words have certain relationships between
    them and one-hot encoding makes it so that the words are computed as if they were
    independent of each other. For example, let us assume we have three words: ''car,''
    ''truck,'' and ''ship.'' Now, ''car'' is closer to ''truck'' in terms of similarity,
    but it still has some similarity to ''ship.'' One-hot encoding fails to capture
    that relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings too are vector representations of words, but they capture the
    relationship of each word with another word. The different ways of getting word
    embedding are explained in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Count Embedding**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Count embedding** is a simple vector representation of the word depending
    on the amount of times it appears in a piece of text. Assume a dataset where you
    have *n* unique words and *M* different records. To get the count embedding, you
    create an *N x M* matrix, where each row is a word and each column is a record.
    The values of any *(n,m)* location in the matrix will contain a count of the number
    of times a word *n* occurs in a record *m*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TF-IDF Embedding**'
  prefs: []
  type: TYPE_NORMAL
- en: '**TF-IDF** is a way to obtain the importance of each word in a collection of
    words or document. It stands for term frequency-inverse document frequency. In
    TF-IDF, the importance of a word increases proportionally to the frequency of
    the word, but this importance is offset by the number of documents that have that
    word, thus helping to adjust for certain words that are used more frequently.
    In other words, the importance of a word is calculated using the frequency of
    the word in one data point of the training set. This importance is increased or
    decreased depending on the occurrence of the word in other data points of the
    training set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Weights generated by TF-IDF consist of two terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Term Frequency** (**TF**): The frequency of a word in the document, as shown
    in the following figure:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.24: The term frequency equation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.24: The term frequency equation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: where w is the word.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverse Document Frequency** (**IDF**): The amount of information the word
    provides, as shown in the following figure:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.25: The inverse document frequency equation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.25: The inverse document frequency equation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The weight is the product of these two terms. In case of TF-IDF, we replace
    the count of the word with this weight in the *N x M* matrix that we used in the
    count embedding section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous bag-of-words embedding**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous bag-of-words** (**CBOW**) works by using neural networks. It predicts
    a word when the input is its surrounding words. The input to the neural network
    is the one-hot vector of the surrounding words. The count of input words is selected
    using the window parameter. The network has only one hidden layer and the output
    layer of the network is activated using the softmax activation function to get
    the probability. The activation function between the layers is linear, but the
    method of updating the gradients is the same as normal neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding matrix of the corpus is the weight between the hidden layer and
    the output layer. Thus, this embedding matrix will be of the *N x H* dimension,
    where *N* is the number of unique words in the corpus and *H* is the number of
    hidden layer nodes. CBOW works better than the two methods discussed previously
    due to its probabilistic nature and low memory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26: A representation of CBOW network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.26: A representation of CBOW network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Skip-gram embedding**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a neural network, skip-gram predicts the surrounding words given an input
    word. The input here is the one-hot vector of the word and the output is the probability
    of the surrounding words. The number of output words is decided by the window
    parameter. Much like CBOW, this method uses a neural network with a single hidden
    layer and the activations are all linear except for the output layer, where we
    use the softmax function. One big difference though is how the error gets calculated:
    different errors are calculated for the different words being predicted and then
    all are added together to get the final error. The error for each individual word
    is calculated by subtracting the output probability vector with the target one-hot
    vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The embedding matrix here is the weight matrix between the input layer and
    the hidden layer. Thus, this embedding matrix will be of the *H x N* dimension,
    where *N* is the number of unique words in the corpus and *H* is the number of
    hidden layer nodes. Skip-gram works much better than CBOW for less frequent words,
    but is generally slower:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.27: A representation of skip-gram network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.27: A representation of skip-gram network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use CBOW for datasets with less words but a high number of samples. Use skip-gram
    when working with a dataset with more words and a low number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Word2Vec**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `gensim` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: To train the model, we need to pass the tokenized sentences as arguments to
    the `Word2Vec` class of `gensim`. `iter` is the number of epochs to train for,
    and `size` refers to the number of nodes in the hidden layer and decides the size
    of the embedding layer. `window` refers to the number of surrounding words that
    are considered when training the neural network. `min_count` refers to the minimum
    frequency required for a word to be considered. `workers` is the number of threads
    to use while training and `sg` refers to the training algorithm to be used, *0*
    for CBOW and *1* for skip-gram.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the number of unique words in the trained embedding, you can use the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we use these embeddings, we need to make sure that they are correct.
    To do that, we find the similar words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28: Similar words'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.28: Similar words'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To save your embeddings to a file, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To load a pretrained embedding, you can use this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The function first reads the embedding file `filename` and gets all the embedding
    vectors present in the file. Then, it creates an embedding matrix that stacks
    the embedding vectors together. The `num_words` parameter limits the size of the
    vocabulary and can be helpful in cases where the training time of the NLP algorithm
    is too high. `word_index` is a dictionary with the key as unique words of the
    corpus and the value as the index of the word. `embedding_dim` is the size of
    the embedding vectors as specified while training.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are a lot of really good pretrained embeddings available. Some of the
    popular ones are GloVe: https://nlp.stanford.edu/projects/glove/ and fastText:
    https://fasttext.cc/docs/en/english-vectors.html'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 56: Creating Word Embeddings Using Gensim'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will create our own Word2Vec embedding using the `gensim`
    library. The word embedding will be created for the IMDB movie review dataset
    that we have been working with. We will take off from where we left in *Exercise
    55*.
  prefs: []
  type: TYPE_NORMAL
- en: The reviews variable has reviews in the token form but they have been converted
    into numbers. The `gensim` Word2Vec requires tokens in the string form, so we
    backtrack to where we converted the tokens back to sentences in step 6 of *Exercise
    55*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The tokens of the first review are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.29: Tokens of first review'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.29: Tokens of first review'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, we convert the lists in each row into a single list using the `apply`
    function of pandas, using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we feed this preprocessed data into Word2Vec to create the word embedding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us check how well the model performs by viewing some similar words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The most similar words to ''`insight`'' in the dataset are:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.30: Similar words to ''insight'''
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.30: Similar words to ''insight'''
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To obtain the similarity between two words, use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output similarity is shown here:![Figure 7.31: Similarity output'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13322_07_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.31: Similarity output'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The similarity score ranges from `0` to `1`, where `1` means that both words
    are the same, and `0` means that both words are completely different and not related
    in any way.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Plot the embedding on a 2D space to understand what words are found to be similar
    to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, convert the embedding into two dimensions using PCA. We will plot only
    the first 200 words. (You can plot more if you like.)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, plot the result on a scatter plot using `matplotlib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your output should look like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.32: Representation of embedding of first 200 words using PCA'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.32: Representation of embedding of first 200 words using PCA'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The axes do not mean anything in the representation of word embedding. The representation
    simply shows the closeness of different words.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Save the embedding to a file so that you can retrieve it later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You just created your first word embedding. You can play around
    with the embedding and view the similarity between different words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 19: Predicting Sentiments of Movie Reviews'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will attempt to predict sentiments of movie reviews. The
    dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter07)
    comprises 25,000 movie reviews sourced from IMDB with their sentiment (positive
    or negative). Let''s look at the following scenario: You work at a DVD rental
    company, which has to predict the number of DVDs to create of a certain movie
    depending on how it is being perceived by the reviewers. To do this, you create
    a machine learning model that can analyze reviews to figure out how the movie
    is being perceived.'
  prefs: []
  type: TYPE_NORMAL
- en: Read and preprocess the movie reviews.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the word embedding of the reviews.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a fully connected neural network to predict sentiments, much like the
    neural network models we created in *Chapter 5*: *Mastering Structured Data*.
    The input will be the word embedding of the reviews and the output of the model
    will be either `1` (positive sentiment) or `0` (negative sentiment).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 378.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The output is a little cryptic because stop words and punctuations have been
    removed, but you can still understand the general sense of the review.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You just created your first NLP module. You should find that
    the model gives an accuracy of of around 76% which is quite low. This is because
    it is predicting sentiments based on individual words; it has no way of figuring
    out the context of the review. For example, it will predict "not good" as a positive
    sentiment as it sees the word 'good.' If it could look at multiple words, it would
    understand that this is a negative sentiment. In the next section, we will learn
    how to create neural networks that can retain information of the past.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, none of the problems we discussed had a temporal dependence, which
    means that the prediction depends not only on the current input but also on the
    past inputs. For example, in the case of the dog vs. cat classifier, we only needed
    the picture of the dog to classify it as a dog. No other information or images
    were required. Instead, if you want to make a classifier that predicts if a dog
    is walking or standing, you will require multiple images in a sequence or a video
    to figure out what the dog is doing. RNNs are like the fully connected networks
    that we talked about. The only change is that an RNN has memory that stores information
    about the previous inputs as states. The outputs of the hidden layers are fed
    in as inputs for the next input.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.33: Representation of recurrent neural network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.33: Representation of recurrent neural network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From the image, you can understand how the outputs of the hidden layers are
    used as inputs for the next input. This acts as a memory element in the neural
    network. Another thing to keep in mind is that the output of a normal neural network
    is a function of the input and weights of the network.
  prefs: []
  type: TYPE_NORMAL
- en: his allows us to randomly input any data point to get the right output. However,
    this is not the case with RNNs. In the case of RNNs, our output depends on the
    previous inputs, so we need to feed in the input in the correct sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.34: Representation of recurrent layer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_34.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.34: Representation of recurrent layer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the preceding image, you can see a single RNN layer on the left in the "folded"
    model. U is the input weight, V is the output weight, and W is the weight associated
    with the memory input. The memory of the RNN is also referred to as state. The
    "unfolded" model on the right shows how the RNN works for the input sequence [xt-1,
    xt, xt+1]. The model differs based on the kind of application. For example, in
    case of sentiment analysis, the input sequence will require only one output in
    the end. The unfolded model for this problem is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.35: Unfolded representation of a recurrent layer used to perform
    sentiment analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_35.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.35: Unfolded representation of a recurrent layer used to perform sentiment
    analysis'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LSTMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Long short-term memory** (**LSTM**) cell is a special kind of RNN cell, capable
    of retaining information over long-term periods of time. Hochreiter and Schmidhuber
    introduced LSTMs in 1997\. RNNs suffer from the vanishing gradient problem. They
    lose information detected over long periods of time. For example, if we are performing
    sentiment analysis on a text and the first sentence says "I am happy today" and
    then the rest of the text is devoid of any sentiments, the RNN will not do a good
    job of detecting that the sentiment of the text is happy. Long short-term memory
    (LSTM) cells overcome this issue by storing certain inputs for a longer time without
    forgetting them. Most real-world recurrent machine learning implementations are
    done using LSTMs. The only difference between RNN cells and LSTM cells is the
    memory states. Every RNN layer takes an input of the memory state and outputs
    a memory state, whereas every LSTM layer takes a long-term and a short-term memory
    as the input and outputs both the long and the short-term memories. The long-term
    memory allows the network to retain information for a longer time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM cells are implemented in Keras, and you easily can add an LSTM layer into
    your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Here, `units` is the number of nodes in the layer, `activation` is the activation
    function to use for the layer. `recurrent_dropout` and `dropout` are the dropout
    probability for the recurrent state and input respectively. `return_sequences`
    specifies if the output should contain the sequence or not; this is made `True`
    when you plan to use another recurrent layer after the current layer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LSTMs almost always work better than RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 57: Performing Sentiment Analysis Using LSTM'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will modify the model we created for the previous activity,
    to make it use an LSTM cell. We will use the same IMDB movie review dataset that
    we have been working with. Most of the preprocessing steps are like those in *Activity
    19*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the IMDB movie review dataset using pandas in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the tweets to lowercase to reduce the number of unique words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Clean the reviews using RegEx with the `clean_str` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, remove stop words and other frequently occurring unnecessary words from
    the reviews. This step converts strings into tokens (which will be helpful in
    the next step):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Combine the tokens to get a string and then drop any review that does not have
    anything in it after the stop-word removal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the reviews using the Keras Tokenizer and convert them into numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, pad the tweets to have a maximum of 100 words. This will remove any
    words after the 100-word limit and add 0s if the number of words is less than
    100:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the previously created embedding to get the embedding matrix using the
    `load_embedding` function discussed in the *Text Processing* section, by using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and testing sets with an 80:20 split. This can
    be modified to find the best split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create and compile the Keras model with one LSTM layer. You can experiment
    with different layers and hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model on the data for 10 epochs to see if it performs better than
    the one in *Activity 1*, by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the accuracy of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy of the LSTM model is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.36: LSTM model accuracy'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.36: LSTM model accuracy'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the confusion matrix of the model to get a proper sense of the model''s
    prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 7.37: Confusion matrix of the model (0 = negative sentiment, 1 = positive
    sentiment)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_07_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.37: Confusion matrix of the model (0 = negative sentiment, 1 = positive
    sentiment)'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Check the performance of the model by seeing the sentiment predictions on random
    reviews using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.38: A negative review from the IMDB dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_07_38.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.38: A negative review from the IMDB dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Congratulations! You just implemented an RNN to predict sentiments of a movie
    review. This network works a little better than the previous network we created.
    Play around with the architecture and hyperparameters of the network to improve
    the accuracy of the model. You can also try using pretrained word embedding from
    either fastText or GloVe to improve the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 20: Predicting Sentiments from Tweets'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will attempt to predict sentiments of a tweet. The provided
    dataset (https://github.com/TrainingByPackt/Data-Science-with-Python/tree/master/Chapter07)
    contains 1.5 million tweets and their sentiments (positive or negative). Let''s
    look at the following scenario: You work at a big consumer organization, which
    recently created a Twitter account. Some of the customers who have had a bad experience
    with your company are taking to Twitter to express their sentiments, which is
    causing a decline in the reputation of the company. You have been tasked to identify
    these tweets so that the company can get in touch with them to provide better
    support. You do this by creating a sentiment predictor, which can determine whether
    the sentiment of a tweet is positive or negative. Before using your new sentiment
    predictor on actual tweets about your company, you will test it on the provided
    tweets dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Read the data and remove all unnecessary information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clean the tweets, tokenize them, and finally convert them into numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load GloVe Twitter embedding and create the embedding matrix (https://nlp.stanford.edu/projects/glove/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an LSTM model to predict the sentiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 383.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congratulations! You just created a machine learning module to predict sentiments
    from tweets. You can now deploy this using Twitter API to perform real-time sentiment
    analysis on tweets. You can play around with different embeddings from GloVe and
    fastText and see how much improvement you can get on your model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we learned how computers understand human language. We first
    learned what RegEx is and how it helps data scientists analyze and clean text
    data. Next, we learned about stop words, what they are, and why they are removed
    from the data to reduce the dimensionality. Next, we next learned about sentence
    tokenization and its importance, followed by word embedding. Embedding is a topic
    that we covered in *Chapter 5*: *Mastering Structured Data*; here, we learned
    how to create word embedding to boost our NLP model''s performance. To create
    better models, we looked at a RNNs, a special type of neural network that retains
    memory of past inputs. Finally, we learned about LSTM cells and how they are better
    than normal RNN cells.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have completed this chapter, you are capable of handling textual
    data and creating machine learning models for NLP. In the next chapter, you will
    learn how to make models faster using transfer learning and a few tricks of the
    craft.
  prefs: []
  type: TYPE_NORMAL
