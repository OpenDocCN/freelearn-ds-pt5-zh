["```py\n## For downloading the protocol buffer 2.4.1\nwget http://protobuf.googlecode.com/files/protobuf-2.4.1.tar.gz\n\n## To extracting the protocol buffer\ntar -xzf protobuf-2.4.1.tar.gz\n\n## To get in to the extracted protocol buffer directory\ncd protobuf-2.4.1\n\n## For making install the protocol buffer\n./configure # --prefix=...\nmake\nmake install\n\n```", "```py\nexport PKG_CONFIG_PATH = /usr/local/lib\nexport LD_LIBRARY_PATH = /usr/local/lib\n\n```", "```py\nSys.setenv(HADOOP_HOME=\"/usr/local/hadoop/\")\nSys.setenv(HADOOP_BIN=\"/usr/local/hadoop/bin\")\nSys.setenv(HADOOP_CONF_DIR=\"/usr/local/hadoop/conf\")\n\n```", "```py\n## For installing the rJava Package will be used for calling java libraries from R.\ninstall.packages(\"rJava\")\n\n```", "```py\n## Downloading RHIPE package from RHIPE repository\nWget http://ml.stat.purdue.edu/rhipebin/Rhipe_0.73.1-2.tar.gz\n\n## Installing the RHIPE package in R via CMD command\nR CMD INSTALL Rhipe_0.73.1.tar.gz\n\n```", "```py\n## Loading the RHIPE library\nlibrary(Rhipe)\n\n## initializing the RHIPE subsystem, which is used for everything. RHIPE will not work if rhinit is not called.\nrhinit()\n\n```", "```py\n## Defining the Map phase\n\nMap(function(k,v){\n\n## for generating the random deviates\n X  runif(v)\n\n## for emitting the key-value pairs with key – k and\n## value – min and max of generated random deviates.\n rhcollect(k, c(Min=min(x),Max=max(x))\n}\n\n```", "```py\n## Create and running a MapReduce job by following\njob = rhwatch(map=map,input=10,reduce=0,\noutput=\"/app/Hadoop/RHIPE/test\",jobname='test')\n\n```", "```py\n## Read the results of job from HDFS\nresult <- rhread(job)\n\n```", "```py\n## Displaying the result\noutputdata  <- do.call('rbind', lapply(result, \"[[\", 2))\n\n```", "```py\n## Loading the RHIPE Library\nlibrary(Rhipe)\n\n```", "```py\nrhput(\"/usr/local/hadoop/CHANGES.txt\",\"/RHIPE/input/\")\n\n```", "```py\n## Defining the Map functionw_map<-expression({ words_vector<-unlist(strsplit(unlist(map.values),\" \")) lapply(words_vector,function(i) rhcollect(i,1))\n})\n\n```", "```py\n## For reference, RHIPE provides a canned version\nReduce = rhoptions()$templates$scalarsummer\n\n## Defining the Reduce functionw_reduce<-expression(pre={total=0},reduce={total<-sum(total,unlist(reduce.values))},post={rhcollect(reduce.key,total)})\n\n```", "```py\n## defining and executing a MapReduce job object\nJob1 <- rhwatch(map=w_map,reduce=w_reduce, ,input=\"/RHIPE/input/\",output=\"/RHIPE/output/\", jobname=\"word_count\")\n\n```", "```py\n## for reading the job output data from HDFS\nOutput_data <- rhread(Job1)results <- data.frame(words=unlist(lapply(Output_data,\"[[\",1)), count =unlist(lapply(Output_data,\"[[\",2)))\n\n```", "```py\n    # Syntax:\n    rhwatch(map, reduce, combiner, input, output, mapred,partitioner,mapred, jobname)\n\n    ## to prepare and submit MapReduce job:\n\n    z=rhwatch(map=map,reduce=0,input=5000,output=\"/tmp/sort\",mapred=mapred,read=FALSE)\n\n    results <- rhread(z)\n\n    ```", "```py\n    ## Submit the job\n    rhex(job)\n\n    ```", "```py\n    rhstatus(job, mon.sec = 5, autokill = TRUE,showErrors = TRUE, verbose = FALSE, handler = NULL)\n\n    ```", "```py\n    install.packages( c('rJava','RJSONIO', 'itertools', 'digest','Rcpp','httr','functional','devtools', 'plyr','reshape2'))\n\n    ```", "```py\n    ## Setting HADOOP_CMD\n    Sys.setenv(HADOOP_CMD=\"/usr/local/hadoop/bin/hadoop\")\n\n    ## Setting up HADOOP_STREAMING\n    Sys.setenv(HADOOP_STREAMING=\"/usr/local/hadoop/contrib/streaming/hadoop-streaming-1.0.3.jar\")\n\n    ```", "```py\n    export HADOOP_CMD=/usr/local/Hadoop\n    export HADOOP_STREAMING=/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.1.1.jar\n\n    ```", "```py\n            R CMD INSTALL rmr-2.2.2.tar.gz\n\n            ```", "```py\n            R CMD INSTALL rmr-2.2.2.tar.gz\n\n            ```", "```py\n            R CMD INSTALL rhbase-1.2.0.tar.gz\n\n            ```", "```py\n## loading the libraries\nlibrary(rhdfs')\nlibrary('rmr2')\n\n## initializing the RHadoop\nhdfs.init()\n\n# defining the input data\nsmall.ints = to.dfs(1:10)\n\n## Defining the MapReduce job\nmapreduce(\n# defining input parameters as small.ints hdfs object, map parameter as function to calculate the min and max for generated random deviates.\n  input = small.ints, \n  map = function(k, v)\n  {\nlapply(seq_along(v), function(r){\n\n  x <- runif(v[[r]])\n    keyval(r,c(max(x),min(x))) \n  })}) \n\n```", "```py\nwordcount = function(input, output = NULL, pattern = \" \"){\n\n```", "```py\nwc.map = function(., lines) {\n keyval(\n unlist(\n strsplit(\n x = lines,\n split = pattern)),\n 1)}\n\n```", "```py\nwc.reduce = function(word, counts ) {\n keyval(word, sum(counts))}\n\n```", "```py\n# To execute the defined Mapper and Reducer functions\n# by specifying the input, output, map, reduce and input.format as parameters.\n\n# Syntax:\n# mapreduce(input, output, input.format, map,reduce,\n# combine)\n\nmapreduce(input = input ,\n output = output,\n input.format = \"text\",\n map = wc.map,\n reduce = wc.reduce,\n combine = T)}\n\n```", "```py\nwordcount('/RHadoop/1/')\n\n```", "```py\nfrom.dfs(\"/tmp/RtmpRMIXzb/file2bda5e10e25f\")\n\n```", "```py\n        hdfs.put('/usr/local/hadoop/README.txt','/RHadoop/1/')\n\n        ```", "```py\n        hdfs.put('/RHadoop/1/','/RHadoop/2/')\n\n        ```", "```py\n        hdfs.move('/RHadoop/1/README.txt','/RHadoop/2/')\n\n        ```", "```py\n        hdfs.rename('/RHadoop/README.txt','/RHadoop/README1.txt')\n\n        ```", "```py\n        hdfs.delete(\"/RHadoop\")\n\n        ```", "```py\n        hdfs.rm(\"/RHadoop\")\n\n        ```", "```py\n        hdfs.chmod('/RHadoop', permissions= '777')\n\n        ```", "```py\n        f = hdfs.file(\"/RHadoop/2/README.txt\",\"r\",buffersize=104857600)\n\n        ```", "```py\n        f = hdfs.file(\"/RHadoop/2/README.txt\",\"r\",buffersize=104857600)\n        hdfs.write(object,con,hsync=FALSE)\n\n        ```", "```py\n        hdfs.close(f)\n\n        ```", "```py\n        hdfs.mkdir(\"/RHadoop/2/\")\n\n        ```", "```py\n        hdfs.rm(\"/RHadoop/2/\")\n\n        ```", "```py\n        Hdfs.ls('/')\n\n        ```", "```py\n        hdfs.file.info(\"/RHadoop\")\n\n        ```"]