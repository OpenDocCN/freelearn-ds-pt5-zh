- en: '1'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Fundamentals
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Describe Jupyter Notebooks and how they are used for data analysis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the features of Jupyter Notebooks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Python data science libraries
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform simple exploratory data analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will learn and implement the fundamental features of the
    Jupyter notebook by completing several hands-on erxercises.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jupyter Notebooks are one of the most important tools for data scientists using
    Python. This is because they're an ideal environment for developing reproducible
    data analysis pipelines. Data can be loaded, transformed, and modeled all inside
    a single Notebook, where it's quick and easy to test out code and explore ideas
    along the way. Furthermore, all of this can be documented "inline" using formatted
    text, so you can make notes for yourself or even produce a structured report.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Other comparable platforms - for example, RStudio or Spyder - present the user
    with multiple windows, which promote arduous tasks such as copy and pasting code
    around and rerunning code that has already been executed. These tools also tend
    to involve **Read Eval Prompt Loops** (**REPLs**) where code is run in a terminal
    session that has saved memory. This type of development environment is bad for
    reproducibility and not ideal for development either. Jupyter Notebooks solve
    all these issues by giving the user a single window where code snippets are executed
    and outputs are displayed inline. This lets users develop code efficiently and
    allows them to look back at previous work for reference, or even to make alterations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: We'll start the chapter by explaining exactly what Jupyter Notebooks are and
    continue to discuss why they are so popular among data scientists. Then, we'll
    open a Notebook together and go through some exercises to learn how the platform
    is used. Finally, we'll dive into our first analysis and perform an exploratory
    analysis in
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Basic Functionality and Features
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first demonstrate the usefulness of Jupyter Notebooks with
    examples and through discussion. Then, in order to cover the fundamentals of Jupyter
    Notebooks for beginners, we'll see the basic usage of them in terms of launching
    and interacting with the platform. For those who have used Jupyter Notebooks before,
    this will be mostly a review; however, you will certainly see new things in this
    topic as well.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: What is a Jupyter Notebook and Why is it Useful?
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Jupyter Notebooks are locally run web applications which contain live code,
    equations, figures, interactive apps, and **Markdown** text. The standard language
    is Python, and that''s what we''ll be using for this book; however, note that
    a variety of alternatives are supported. This includes the other dominant data
    science language, R:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1: Jupyter Notebook sample workbook](img/C13018_01_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Jupyter Notebook sample workbook'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Those familiar with R will know about R Markdown. `README.md` **Markdown** file.
    This format is useful for basic text formatting. It's comparable to HTML but allows
    for much less customization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉 R 的人应该知道 R Markdown。`README.md` **Markdown** 文件。这种格式适用于基本的文本格式化。它类似于 HTML，但允许的自定义选项较少。
- en: 'Commonly used symbols in **Markdown** include hashes (#) to make text into
    a heading, square and round brackets to insert hyperlinks, and stars to create
    italicized or bold text:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**Markdown** 中常用的符号包括井号（#）用来创建标题，方括号和圆括号用来插入超链接，星号用来创建斜体或粗体文本：'
- en: '![Figure 1.2: Sample Markdown document](img/C13018_01_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2：示例 Markdown 文档](img/C13018_01_02.jpg)'
- en: 'Figure 1.2: Sample Markdown document'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.2：示例 Markdown 文档
- en: Having seen the basics of Markdown, let's come back to R Markdown, where **Markdown**
    text can be written alongside executable code. Jupyter Notebooks offer the equivalent
    functionality for Python, although, as we'll see, they function quite differently
    than R **Markdown** documents. For example, R **Markdown** assumes you are writing
    **Markdown** unless otherwise specified, whereas Jupyter Notebooks assume you
    are inputting code. This makes it more appealing to use Jupyter Notebooks for
    rapid development and testing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了 Markdown 的基础知识后，让我们回到 R Markdown，其中 **Markdown** 文本可以与可执行代码一起编写。Jupyter
    Notebooks 为 Python 提供了等效的功能，尽管正如我们将看到的，它们的工作方式与 R **Markdown** 文档有很大不同。例如，R **Markdown**
    假设你是在写 **Markdown**，除非另有说明，而 Jupyter Notebooks 假设你输入的是代码。这使得 Jupyter Notebooks
    更适合用于快速开发和测试。
- en: 'From a data science perspective, there are two primary types for a Jupyter
    Notebook depending on how they are used: lab-style and deliverable.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据科学的角度来看，Jupyter Notebook 主要有两种类型，取决于其使用方式：实验室风格和交付式。
- en: Lab-style Notebooks are meant to serve as the programming analog of research
    journals. These should contain all the work you've done to load, process, analyze,
    and model the data. The idea here is to document everything you've done for future
    reference, so it's usually not advisable to delete or alter previous lab-style
    Notebooks. It's also a good idea to accumulate multiple date-stamped versions
    of the Notebook as you progress through the analysis, in case you want to look
    back at previous states.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 实验室风格 Notebooks 是用来作为编程版的研究期刊。这些 Notebooks 应包含你加载、处理、分析和建模数据的所有工作。其理念是记录下你做过的每一步，以便将来参考，因此通常不建议删除或修改之前的实验室风格
    Notebooks。同时，随着分析的推进，积累多个带有时间戳的 Notebook 版本也是一个好主意，这样你就可以在需要时回顾之前的状态。
- en: Deliverable Notebooks are intended to be presentable and should contain only
    select parts of the lab-style Notebooks. For example, this could be an interesting
    discovery to share with your colleagues, an in-depth report of your analysis for
    a manager, or a summary of the key findings for stakeholders.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 交付式 Notebooks 旨在呈现内容，应该仅包含实验室风格 Notebooks 的一部分内容。例如，这可以是一个有趣的发现，供你与同事分享；也可以是为经理准备的深入分析报告，或是为利益相关者总结的关键发现。
- en: In either case, an important concept is reproducibility. If you've been diligent
    in documenting your software versions, anyone receiving the reports will be able
    to rerun the Notebook and compute the same results as you did. In the scientific
    community, where reproducibility is becoming increasingly difficult, this is a
    breath of fresh air.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是哪种情况，一个重要的概念是可重现性。如果你在记录软件版本时很细心，那么任何收到报告的人都可以重新运行 Notebook 并计算出与你相同的结果。在科学界，可重现性变得越来越困难，这无疑是一个令人耳目一新的做法。
- en: Navigating the Platform
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导航平台
- en: Now, we are going to open up a Jupyter Notebook and start to learn the interface.
    Here, we will assume you have no prior knowledge of the platform and go over the
    basic usage.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将打开一个 Jupyter Notebook，开始学习其界面。在这里，我们假设你对该平台没有任何先前的了解，并将讲解基本的使用方法。
- en: 'Exercise 1: Introducing Jupyter Notebooks'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 1：介绍 Jupyter Notebooks
- en: Navigate to the companion material directory in the terminal
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端中导航到配套材料目录
- en: Note
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Unix machines such as Mac or Linux, command-line navigation can be done using
    `ls` to display directory contents and `cd` to change directories. On Windows
    machines, use `dir` to display directory contents and use cd to change directories
    instead. If, for example, you want to change the drive from C: to D:, you should
    execute d: to change drives.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在 Unix 系统如 Mac 或 Linux 上，可以使用 `ls` 显示目录内容，使用 `cd` 切换目录。在 Windows 系统上，使用 `dir`
    显示目录内容，使用 `cd` 切换目录。如果你想将驱动器从 C: 切换到 D:，可以执行 d: 来切换驱动器。'
- en: 'Start a new local Notebook server here by typing the following into the terminal:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A new window or tab of your default browser will open the Notebook Dashboard
    to the working directory. Here, you will see a list of folders and files contained
    therein.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on a folder to navigate to that particular path and open a file by clicking
    on it. Although its main use is editing IPYNB Notebook files, Jupyter functions
    as a standard text editor as well.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reopen the terminal window used to launch the app. We can see the `NotebookApp`
    being run on a local server. In particular, you should see a line like this:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Going to that HTTP address will load the app in your browser window, as was
    done automatically when starting the app. Closing the window does not stop the
    app; this should be done from the terminal by typing *Ctrl* + *C*.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Close the app by typing *Ctrl* + *C* in the terminal. You may also have to confirm
    by entering `y`. Close the web browser window as well.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the list of available options by running the following code:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Open the `NotebookApp` at local port `9000` by running the following:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Click **New** in the upper-right corner of the Jupyter Dashboard and select
    a kernel from the drop-down menu (that is, select something in the **Notebooks**
    section):![Figure 1.3: Selecting a kernel from the drop down menu](img/C13018_01_03.jpg)'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 1.3: Selecting a kernel from the drop down menu'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This is the primary method of creating a new Jupyter Notebook.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Kernels provide programming language support for the Notebook. If you have installed
    Python with Anaconda, that version should be the default kernel. Conda virtual
    environments will also be available here.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Virtual environments are a great tool for managing multiple projects on the
    same machine. Each virtual environment may contain a different version of Python
    and external libraries. Python has built-in virtual environments; however, the
    Conda virtual environment integrates better with Jupyter Notebooks and boasts
    other nice features. The documentation is available at: [https://conda.io/docs/user-guide/tasks/manage-environments.html](https://conda.io/docs/user-guide/tasks/manage-environments.html).'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With the newly created blank Notebook, click the top cell and type `print('hello
    world')`, or any other code snippet that writes to the screen.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the cell and press *Shift* + *Enter* or select `stdout` or `stderr` output
    from the code will be displayed beneath as the cell runs. Furthermore, the string
    representation of the object written in the final line will be displayed as well.
    This is very handy, especially for displaying tables, but sometimes we don't want
    the final object to be displayed. In such cases, a semicolon (;) can be added
    to the end of the line to suppress the display. New cells expect and run code
    input by default; however, they can be changed to render **Markdown** instead.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click an empty cell and change it to accept the Markdown-formatted text. This
    can be done from the drop-down menu icon in the toolbar or by selecting **Markdown**
    from the **Cell** menu. Write some text in here (any text will do), making sure
    to utilize Markdown formatting symbols such as #.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll to the **Play** icon in the tool bar:![Figure 1.4: Jupyter Notebook
    tool bar](img/C13018_01_04.jpg)'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 1.4: Jupyter Notebook tool bar'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This can be used to run cells. As we'll see later, however, it's handier to
    use the keyboard shortcut *Shift* + *Enter* to run cells.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Right next to this is a **Stop** icon, which can be used to stop cells from
    running. This is useful, for example, if a cell is taking too long to run:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.5: Stop icon in Jupyter Notebooks'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_01_05_2.jpg)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.5: Stop icon in Jupyter Notebooks'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'New cells can be manually added from the **Insert** menu:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.6: Adding new cells from the Insert menu in Jupyter Notebooks](img/C13018_01_06.jpg)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.6: Adding new cells from the Insert menu in Jupyter Notebooks'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Cells can be copied, pasted, and deleted using icons or by selecting options
    from the **Edit** menu:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.7: Edit Menu in the Jupyter Notebooks](img/C13018_01_07.jpg)'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.7: Edit Menu in the Jupyter Notebooks'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 1.8: Cutting and copying cells in Jupyter Notebooks](img/C13018_01_08.jpg)'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.8: Cutting and copying cells in Jupyter Notebooks'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Cells can also be moved up and down this way:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.9: Moving cells up and down in Jupyter Notebooks](img/C13018_01_09.jpg)'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.9: Moving cells up and down in Jupyter Notebooks'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are useful options under the **Cell** menu to run a group of cells or
    the entire Notebook:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.10: Running Cells in Jupyter Notebooks](img/C13018_01_10.jpg)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.10: Running cells in Jupyter Notebooks'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Experiment with the toolbar options to move cells up and down, insert new cells,
    and delete cells. An important thing to understand about these Notebooks is the
    shared memory between cells. It''s quite simple: every cell existing on the sheet
    has access to the global set of variables. So, for example, a function defined
    in one cell could be called from any other, and the same applies to variables.
    As one would expect, anything within the scope of a function will not be a global
    variable and can only be accessed from within that specific function.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Open the **Kernel** menu to see the selections. The **Kernel** menu is useful
    for stopping script executions and restarting the Notebook if the kernel dies.
    Kernels can also be swapped here at any time, but it is unadvisable to use multiple
    kernels for a single Notebook due to reproducibility concerns.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the **File** menu to see the selections. The **File** menu contains options
    for downloading the Notebook in various formats. In particular, it's recommended
    to save an HTML version of your Notebook, where the content is rendered statically
    and can be opened and viewed "as you would expect" in web browsers.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Notebook name will be displayed in the upper-left corner. New Notebooks
    will automatically be named **Untitled**.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Change the name of your IPYNB Notebook file by clicking on the current name
    in the upper-left corner and typing the new name. Then, save the file.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Close the current tab in your web browser (exiting the Notebook) and go to the
    **Jupyter Dashboard** tab, which should still be open. (If it's not open, then
    reload it by copy and pasting the HTTP link from the terminal.)
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since we didn't shut down the Notebook, and we just saved and exited, it will
    have a green book symbol next to its name in the **Files** section of the Jupyter
    Dashboard and will be listed as **Running** on the right side next to the last
    modified date. Notebooks can be shut down from here.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Quit the Notebook you have been working on by selecting it (checkbox to the
    left of the name), and then click the orange **Shutdown** button:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Read through the basic keyboard shortcuts and test them.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.11: Shutting down the Jupyter notebook'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_11.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.11: Shutting down the Jupyter notebook'
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you plan to spend a lot of time working with Jupyter Notebooks, it's worthwhile
    to learn the keyboard shortcuts. This will speed up your workflow considerably.
    Particularly useful commands to learn are the shortcuts for manually adding new
    cells and converting cells from code to Markdown formatting. Click on **Keyboard
    Shortcuts** from the **Help** menu to see how.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Features
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jupyter has many appealing features that make for efficient Python programming.
    These include an assortment of things, from methods for viewing docstrings to
    executing Bash commands. We will explore some of these features in this section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The official IPython documentation can be found here: [http://ipython.readthedocs.io/en/stable/](http://ipython.readthedocs.io/en/stable/).
    It has details on the features we will discuss here and others.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2: Implementing Jupyter''s Most Useful Features'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Navigate to the `lesson-1` directory from the Jupyter Dashboard and open `lesson-1-workbook.ipynb`
    by selecting it.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The standard file extension for Jupyter Notebooks is `.ipynb`, which was introduced
    back when they were called IPython Notebooks.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scroll down to `Subtopic C: Jupyter Features` in the Jupyter Notebook.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We start by reviewing the basic keyboard shortcuts. These are especially helpful
    to avoid having to use the mouse so often, which will greatly speed up the workflow.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can get help by adding a question mark to the end of any object and running
    the cell. Jupyter finds the docstring for that object and returns it in a pop-out
    window at the bottom of the app.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the **Getting Help** cell and check how Jupyter displays the docstrings
    at the bottom of the Notebook. Add a cell in this section and get help on the
    object of your choice:![Figure 1.12: Getting help in Jupyter Notebooks](img/C13018_01_12.jpg)'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 1.12: Getting help in Jupyter Notebooks'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Click an empty code cell in the **Tab Completion** section. Type import (including
    the space after) and then press the **Tab** key:![Figure 1.13: Tab completion
    in Jupyter Notebooks](img/C13018_01_13.jpg)'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 1.13: Tab completion in Jupyter Notebooks'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The above action listed all the available modules for import.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Tab completion can be used for the following: **list available modules when
    importing external libraries**; **list available modules of imported external
    libraries**; **function and variable completion**. This can be especially useful
    when you need to know the available input arguments for a module, when exploring
    a new library, to discover new modules, or simply to speed up workflow. They will
    save time writing out variable names or functions and reduce bugs from typos.
    The tab completion works so well that you may have difficulty coding Python in
    other editors after today!'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scroll to the Jupyter Magic Functions section and run the cells containing
    `%lsmagic` and `%matplotlib` inline:![Figure 1.14: Jupyter Magic functions](img/C13018_01_14.jpg)'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 1.14: Jupyter Magic functions'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The percent signs, % and %%, are one of the basic features of Jupyter Notebook
    and are called magic commands. Magics starting with `%%` will apply to the entire
    cell, and magics starting with `%` will only apply to that line.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`%lsmagic` lists the available options. We will discuss and show examples of
    some of the most useful ones. The most common magic command you will probably
    see is `%matplotlib` inline, which allows matplotlib figures to be displayed in
    the Notebook without having to explicitly use `plt.show()`.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The timing functions are very handy and come in two varieties: a standard timer
    (`%time` or `%%time`) and a timer that measures the average runtime of many iterations
    (`%timeit` and `%%timeit`).'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Notice how list comprehensions are quicker than loops in Python. This can be
    seen by comparing the wall time for the first and second cell, where the same
    calculation is done significantly faster with the list comprehension.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the cells in the `pwd`), what's in the directory (`ls`), make new folders
    (`mkdir`), and write file contents (`cat`/`head`/`tail`).
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the first cell in the **Using bash** in the notebook section.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This cell writes some text to a file in the working directory, prints the directory
    contents, prints an empty line, and then writes back the contents of the newly
    created file before removing it:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.15: Using Bash in Jupyter Notebooks](img/C13018_01_15.jpg)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.15: Using Bash in Jupyter Notebooks'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Run the cells containing only `ls` and `pwd`.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note how we did not have to explicitly use the Bash magic command for these
    to work. There are plenty of external magic commands that can be installed. A
    popular one is `ipython-sql`, which allows for SQL code to be executed in cells.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open a new terminal window and execute the following code to install ipython-sql:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 1.16: Installing ipython-sql using pip'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_01_16.jpg)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.16: Installing ipython-sql using pip'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Run the `%load_ext sql` cell to load the external command into the Notebook:![Figure
    1.17: Loading sql in Jupyter Notebooks'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_17.jpg)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.17: Loading sql in Jupyter Notebooks'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This allows for connections to remote databases so that queries can be executed
    (and thereby documented) right inside the Notebook.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell containing the SQL sample query:![Figure 1.18: Running a sample
    SQL query'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_18.jpg)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.18: Running a sample SQL query'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, we first connect to the local sqlite source; however, this line could
    instead point to a specific database on a local or remote server. Then, we execute
    a simple `SELECT` to show how the cell has been converted to run SQL code instead
    of Python.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Install the version documentation tool now from the terminal using `pip`. Open
    up a new window and run the following code:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once installed, it can then be imported into any Notebook using `%load_ext version_information`.
    Finally, once loaded, it can be used to display the versions of each piece of
    software in the Notebook.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `%version_information commands helps with documentation`, but it does not
    come as standard with Jupyter. Like the SQL example we just saw, it can be installed
    from the command line with `pip`.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell that loads and calls the `version_information` command:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.19: Version Information in Jupyter'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_19.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.19: Version Information in Jupyter'
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Converting a Jupyter Notebook to a Python Script
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can convert a Jupyter Notebook to a Python script. This is equivalent to
    copying and pasting the contents of each code cell into a single `.py` file. The
    Markdown sections are also included as comments.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'The conversion can be done from the `NotebookApp` or in the command line as
    follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 1.20: Converting a Jupyter Notebook into a Python Script'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_20.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.20: Converting a Jupyter Notebook into a Python Script'
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is useful, for example, when you want to determine the library requirements
    for a Notebook using a tool such as `pipreqs`. This tool determines the libraries
    used in a project and exports them into a `requirements.txt` file (and it can
    be installed by running pip install `pipreqs`).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'The command is called from outside the folder containing your `.py` files.
    For example, if the `.py` files are inside a folder called `lesson-1`, you could
    do the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 1.21: Determining library requirements using pipreqs'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_21.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.21: Determining library requirements using pipreqs'
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The resulting `requirements.txt` file for `lesson-1-workbook.ipynb` looks like
    this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Python Libraries
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having now seen all the basics of Jupyter Notebooks, and even some more advanced
    features, we'll shift our attention to the Python libraries we'll be using in
    this book. Libraries, in general, extend the default set of Python functions.
    Examples of commonly used standard libraries are `datetime`, `time`, and `os`.
    These are called standard libraries because they come standard with every installation
    of Python.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: For data science with Python, the most important libraries are external, which
    means they do not come standard with Python.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The external data science libraries we'll be using in this book are NumPy, Pandas,
    Seaborn, matplotlib, scikit-learn, Requests, and Bokeh.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A word of caution: It''s a good idea to import libraries using industry standards,
    for example, import numpy as np; this way, your code is more readable. Try to
    avoid doing things such as from numpy import *, as you may unwittingly overwrite
    functions. Furthermore, it''s often nice to have modules linked to the library
    via a dot (.) for code readability.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Let's briefly introduce each.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy** offers multi-dimensional data structures (arrays) on which operations
    can be performed far quicker than standard Python data structures (for example,
    lists). This is done in part by performing operations in the background using
    C. NumPy also offers various mathematical and data manipulation functions.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NaN` entries and computing statistical descriptions of the data. Working with
    Pandas DataFrames will be a big focus of this book.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matplotlib** is a plotting tool inspired by the MATLAB platform. Those familiar
    with R can think of it as Python''s version of ggplot. It''s the most popular
    Python library for plotting figures and allows for a high level of customization.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seaborn** works as an extension to matplotlib, where various plotting tools
    useful for data science are included. Generally speaking, this allows for analysis
    to be done much faster than if you were to create the same things *manually* with
    libraries such as matplotlib and scikit-learn.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn** is the most commonly used machine learning library. It offers
    top-of-the-line algorithms and a very elegant API where models are instantiated
    and then *fit* with data. It also provides data processing modules and other tools
    useful for predictive analytics.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Requests** is the go-to library for making HTTP requests. It makes it straightforward
    to get HTML from web pages and interface with APIs. For parsing the HTML, many
    choose BeautifulSoup4, which we will also cover in this book.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bokeh** is an interactive visualization library. It functions similar to
    matplotlib, but allows us to add hover, zoom, click, and use other interactive
    tools to our plots. It also allows us to render and play with the plots inside
    our Jupyter Notebook.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having introduced these libraries, let's go back to our Notebook and load them,
    by running the `import` statements. This will lead us into our first analysis,
    where we finally start working with a dataset.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3: Importing the External Libraries and Setting Up the Plotting Environment'
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Open up the `lesson 1` Jupyter Notebook and scroll to the `Subtopic D: Python
    Libraries` section.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just like for regular Python scripts, libraries can be imported into the Notebook
    at any time. It's best practice to put the majority of the packages you use at
    the top of the file. Sometimes it makes sense to load things midway through the
    Notebook and that is completely fine.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cells to import the external libraries and set the plotting options:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.22: Importing Python libraries'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_22.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.22: Importing Python libraries'
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For a nice Notebook setup, it''s often useful to set various options along
    with the imports at the top. For example, the following can be run to change the
    figure appearance to something more aesthetically pleasing than the matplotlib
    and Seaborn defaults:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: So far in this book, we've gone over the basics of using Jupyter Notebooks for
    data science. We started by exploring the platform and finding our way around
    the interface. Then, we discussed the most useful features, which include tab
    completion and magic functions. Finally, we introduced the Python libraries we'll
    be using in this book.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The next section will be very interactive as we perform our first analysis together
    using the Jupyter Notebook.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Our First Analysis - The Boston Housing Dataset
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, this chapter has focused on the features and basic usage of Jupyter.
    Now, we'll put this into practice and do some data exploration and analysis.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we'll look at in this section is the so-called Boston housing dataset.
    It contains US census data concerning houses in various areas around the city
    of Boston. Each sample corresponds to a unique area and has about a dozen measures.
    We should think of samples as rows and measures as columns. The data was first
    published in 1978 and is quite small, containing only about 500 samples.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know something about the context of the dataset, let's decide on
    a rough plan for the exploration and analysis. If applicable, this plan would
    accommodate the relevant question(s) under study. In this case, the goal is not
    to answer a question but to instead show Jupyter in action and illustrate some
    basic data analysis methods.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Our general approach to this analysis will be to do the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Load the data into Jupyter using a Pandas DataFrame
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantitatively understand the features
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look for patterns and generate questions
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer the questions to the problems
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the Data into Jupyter Using a Pandas DataFrame
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Oftentimes, data is stored in tables, which means it can be saved as a **comma-separated
    variable** (**CSV**) file. This format, and many others, can be read into Python
    as a DataFrame object, using the Pandas library. Other common formats include
    **tab-separated variable** (**TSV**), SQL tables, and JSON data structures. Indeed,
    Pandas has support for all of these. In this example, however, we are not going
    to load the data this way because the dataset is available directly through scikit-learn.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An important part after loading data for analysis is ensuring that it's clean.
    For example, we would generally need to deal with missing data and ensure that
    all columns have the correct datatypes. The dataset we use in this section has
    already been cleaned, so we will not need to worry about this. However, we'll
    see messier data in the second chapter and explore techniques for dealing with
    it.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4: Loading the Boston Housing Dataset'
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Scroll to `Subtopic A` of `Topic B: Our first Analysis: the Boston Housing
    Dataset` in chapter 1 of the Jupyter Notebook.'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Boston housing dataset can be accessed from the `sklearn.datasets` module
    using the `load_boston` method.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the first two cells in this section to load the Boston dataset and see
    the `datastructures` type:![Figure 1.23: Loading the Boston dataset'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_23.jpg)'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.23: Loading the Boston dataset'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The output of the second cell tells us that it's a scikit-learn `Bunch` object.
    Let's get some more information about that to understand what we are dealing with.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the next cell to import the base object from scikit-learn `utils` and print
    the docstring in our Notebook:![Figure 1.24: Importing base objects and printing
    the docstring'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_24.jpg)'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.24: Importing base objects and printing the docstring'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the field names (that is, the keys to the dictionary) by running the
    next cell. We find these fields to be self-explanatory: `[''DESCR'', ''target'',
    ''data'', ''feature_names'']`.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the next cell to print the dataset description contained in `boston['DESCR']`.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that in this call, we explicitly want to print the field value so that
    the Notebook renders the content in a more readable format than the string representation
    (that is, if we just type `boston[''DESCR'']` without wrapping it in a `print`
    statement). We then see the dataset information as we''ve previously summarized:'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Briefly read through the feature descriptions and/or describe them yourself.
    For the purposes of this tutorial, the most important fields to understand are
    `Attribute` `Information`). We will use this as reference during our analysis.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the complete code, refer to the following: [https://bit.ly/2EL11cW](https://bit.ly/2EL11cW)'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we are going to create a Pandas DataFrame that contains the data. This
    is beneficial for a few reasons: all of our data will be contained in one object,
    there are useful and computationally efficient DataFrame methods we can use, and
    other libraries such as Seaborn have tools that integrate nicely with DataFrames.'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this case, we will create our DataFrame with the standard constructor method.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell where Pandas is imported and the docstring is retrieved for `pd.DataFrame:`![Figure
    1.25: Retrieving the docstring for pd.DataFrame'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_25.jpg)'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.25: Retrieving the docstring for pd.DataFrame'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The docstring reveals the DataFrame input parameters. We want to feed in `boston['data']`
    for the data and use `boston['feature_names']` for the headers.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the next few cells to print the data, its shape, and the feature names:![Figure
    1.26: Printing data, shape, and feature names'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_26.jpg)'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.26: Printing data, shape, and feature names'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking at the output, we see that our data is in a 2D NumPy array. Running
    the command `boston['data'].shape` returns the length (number of samples) and
    the number of features as the first and second outputs, respectively.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the data into a Pandas DataFrame `df` by running the following:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In machine learning, the variable that is being modeled is called the target
    variable; it's what you are trying to predict given the features. For this dataset,
    the suggested target is **MEDV**, the median house value in 1,000s of dollars.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the next cell to see the shape of the target:![Figure 1.27: Code for viewing
    the shape of the target'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_27.jpg)'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.27: Code for viewing the shape of the target'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We see that it has the same length as the features, which is what we expect.
    It can therefore be added as a new column to the DataFrame.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add the target variable to df by running the cell with the following:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Move the target variable to the front of `df` by running the cell with the
    following code:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is done to distinguish the target from our features by storing it to the
    front of our DataFrame.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we introduce a dummy variable `y` to hold a copy of the target column
    before removing it from the DataFrame. We then use the Pandas concatenation function
    to combine it with the remaining DataFrame along the 1st axis (as opposed to the
    0th axis, which combines rows).
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You will often see dot notation used to reference DataFrame columns. For example,
    previously we could have done `y = df.MEDV.copy()`. This does not work for deleting
    columns, however; `del df.MEDV` would raise an error.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Implement `df.head()` or `df.tail()` to glimpse the data and `len(df)` to verify
    that number of samples is what we expect. Run the next few cells to see the head,
    tail, and length of `df`:![Figure 1.28: Printing the head of the data frame df'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_28.jpg)'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.28: Printing the head of the data frame df'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 1.29: Printing the tail of data frame df'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_01_29.jpg)'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.29: Printing the tail of data frame df'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Each row is labeled with an index value, as seen in bold on the left side of
    the table. By default, these are a set of integers starting at 0 and incrementing
    by one for each row.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Printing `df.dtypes` will show the datatype contained within each column. Run
    the next cell to see the datatypes of each column. For this dataset, we see that
    every field is a float and therefore most likely a continuous variable, including
    the target. This means that predicting the target variable is a regression problem.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run `df.isnull()` to clean the dataset as Pandas automatically sets missing
    data as `NaN` values. To get the number of `NaN` values per column, we can do
    `df.isnull().sum()`:![Figure 1.30: Cleaning the dataset by identifying NaN values'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_30.jpg)'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.30: Cleaning the dataset by identifying NaN values'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '`df.isnull()` returns a Boolean frame of the same length as `df`.'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For this dataset, we see there are no `NaN` values, which means we have no immediate
    work to do in cleaning the data and can move on.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Remove some columns by running the cell that contains the following code:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is done to simplify the analysis. We will focus on the remaining columns
    in more detail.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Data Exploration
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since this is an entirely new dataset that we've never seen before, the first
    goal here is to understand the data. We've already seen the textual description
    of the data, which is important for qualitative understanding. We'll now compute
    a quantitative description.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5: Analyzing the Boston Housing Dataset'
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Navigate to `Subtopic B: Data exploration` in the Jupyter Notebook and run
    the cell containing `df.describe()`:![Figure 1.31: Computation and output of statistical
    properties'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_31.jpg)'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.31: Computation and output of statistical properties'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This computes various properties including the mean, standard deviation, minimum,
    and maximum for each column. This table gives a high-level idea of how everything
    is distributed. Note that we have taken the transform of the result by adding
    a `.T` to the output; this swaps the rows and columns.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Going forward with the analysis, we will specify a set of columns to focus on.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cell where these "focus columns" are defined:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Display the aforementioned subset of columns of the DataFrame by running `df[cols].head()`:![Figure
    1.32: Displaying focus columns'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_32.jpg)'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.32: Displaying focus columns'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As a reminder, let''s recall what each of these columns is. From the dataset
    documentation, we have the following:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To look for patterns in this data, we can start by calculating the pairwise
    correlations using `pd.DataFrame.corr`.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the pairwise correlations for our selected columns by running the
    cell containing the following code:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure 1.33: Pairwise calculation of correlation'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_01_33.jpg)'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.33: Pairwise calculation of correlation'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This resulting table shows the correlation score between each set of values.
    Large positive scores indicate a strong positive (that is, in the same direction)
    correlation. As expected, we see maximum values of 1 on the diagonal.
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By default, Pandas calculates the standard correlation coefficient for each
    pair, which is also called the Pearson coefficient. This is defined as the covariance
    between two variables, divided by the product of their standard deviations:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13018_01_56.jpg)'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The covariance, in turn, is defned as follows:'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13018_01_57.jpg)'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Here, n is the number of samples, xi and yi are the individual samples being
    summed over, and X and Y are the means of each set.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Instead of straining our eyes to look at the preceding table, it's nicer to
    visualize it with a heatmap. This can be done easily with Seaborn.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the next cell to initialize the plotting environment, as discussed earlier
    in the chapter. Then, to create the heatmap, run the cell containing the following
    code:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure 1.34: Plot of the heat map for all variables'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_34.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.34: Plot of the heat map for all variables'
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We call `sns.heatmap` and pass the pairwise correlation matrix as input. We
    use a custom color palette here to override the Seaborn default. The function
    returns a `matplotlib.axes` object which is referenced by the variable `ax`.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: The final figure is then saved as a high resolution PNG to the `figures` folder.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: For the final step in our dataset exploration exercise, we'll visualize our
    data using Seaborn's `pairplot` function.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualize the DataFrame using Seaborn''s `pairplot` function. Run the cell
    containing the following code:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Figure 1.35: Data visualization using Seaborn'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_35.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.35: Data visualization using Seaborn'
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Note that unsupervised learning techniques are outside the scope of this book.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the histograms on the diagonal, we see the following:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '**a**: **RM** and **MEDV** have the closest shape to normal distributions.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '**b**: **AGE** is skewed to the left and **LSTAT** is skewed to the right (this
    mayseem counterintuitive but skew is defined in terms of where the mean is positioned
    in relation to the max).'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '**c**: For **TAX**, we find a large amount of the distribution is around 700\.
    This is also evident from the scatter plots.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Taking a closer look at the `df.describe()`, the min and max of **MDEV** was
    5k and 50k, respectively. This suggests that median house values in the dataset
    were capped at 50k.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Predictive Analytics with Jupyter Notebooks
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Continuing our analysis of the Boston housing dataset, we can see that it presents
    us with a regression problem where we predict a continuous target variable given
    a set of features. In particular, we'll be predicting the median house value (**MEDV**).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: We'll train models that take only one feature as input to make this prediction.
    This way, the models will be conceptually simple to understand and we can focus
    more on the technical details of the scikit-learn API. Then, in the next chapter,
    you'll be more comfortable dealing with the relatively complicated models.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6: Applying Linear Models With Seaborn and Scikit-learn'
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Scroll to `Subtopic C: Introduction to predictive analytics` in the Jupyter
    Notebook and look just above at the pairplot we created in the previous section.
    In particular, look at the scatter plots in the bottom-left corner:![Figure 1.36:
    Scatter plots for MEDV and LSTAT'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_36.jpg)'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.36: Scatter plots for MEDV and LSTAT'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note how the number of rooms per house (**RM**) and the % of the population
    that is lower class (**LSTAT**) are highly correlated with the median house value
    (**MDEV**). Let''s pose the following question: how well can we predict **MDEV**
    given these variables?'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To help answer this, let's first visualize the relationships using Seaborn.
    We will draw the scatter plots along with the line of best fit linear models.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Draw scatter plots along with the linear models by running the cell that contains
    the following:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Figure 1.37: Drawing scatter plots using linear models'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_01_37.jpg)'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.37: Drawing scatter plots using linear models'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The line of best fit is calculated by minimizing the ordinary least squares
    error function, something Seaborn does automatically when we call the `regplot`
    function. Also note the shaded areas around the lines, which represent 95% confidence
    intervals.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: These 95% confidence intervals are calculated by taking the standard deviation
    of data in bins perpendicular to the line of best fit, effectively determining
    the confidence intervals at each point along the line of best fit. In practice,
    this involves Seaborn bootstrapping the data, a process where new data is created
    through random sampling with replacement. The number of bootstrapped samples is
    automatically determined based on the size of the dataset, but can be manually
    set as well by passing the `n_boot` argument.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the residuals using Seaborn by running the cell containing the following:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Figure 1.38: Plotting residuals using Seaborn'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_01_38.jpg)'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.38: Plotting residuals using Seaborn'
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Each point on these residual plots is the difference between that sample (`y`)
    and the linear model prediction (`ŷ`). Residuals greater than zero are data points
    that would be underestimated by the model. Likewise, residuals less than zero
    are data points that would be overestimated by the model.
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Patterns in these plots can indicate suboptimal modeling. In each preceding
    case, we see diagonally arranged scatter points in the positive region. These
    are caused by the $50,000 cap on **MEDV**. The **RM** data is clustered nicely
    around 0, which indicates a good fit. On the other hand, **LSTAT** appears to
    be clustered lower than 0.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a function using sci-kit learn that calculates the line of best fit
    and mean squared error, by running the cell that contains the following:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For complete code, refer to the following: [https://bit.ly/2JgPZdU](https://bit.ly/2JgPZdU)'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the `get_mse` function, we first assign the variables `y` and `x` to the
    target MDEV and the dependent feature, respectively. These are cast as NumPy arrays
    by calling the `values` attribute. The dependent features array is reshaped to
    the format expected by scikit-learn; this is only necessary when modeling a one-dimensional
    feature space. The model is then instantiated and fitted on the data. For linear
    regression, the fitting consists of computing the model parameters using the ordinary
    least squares method (minimizing the sum of squared errors for each sample). Finally,
    after determining the parameters, we predict the target variable and use the results
    to calculate the **MSE**.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `get_mse` function for both **RM** and **LSTAT**, by running the cell
    containing the following:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Figure 1.39: Calling the get_mse function for RM and LSTAT'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_39.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.39: Calling the get_mse function for RM and LSTAT'
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Comparing the **MSE**, it turns out the error is slightly lower for **LSTAT**.
    Looking back to the scatter plots, however, it appears that we might have even
    better success using a polynomial model for **LSTAT**. In the next activity, we
    will test this by computing a third-order polynomial model with scikit-learn.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Forgetting about our Boston housing dataset for a minute, consider another
    real-world situation where you might employ polynomial regression. The following
    example is modeling weather data. In the following plot, we see temperatures (lines)
    and precipitations (bars) for Vancouver, BC, Canada:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.40: Visualizing weather data for Vancouver, Canada'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_40.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.40: Visualizing weather data for Vancouver, Canada'
  id: totrans-341
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Any of these fields are likely to be fit quite well by a fourth-order polynomial.
    This would be a very valuable model to have, for example, if you were interested
    in predicting the temperature or precipitation for a continuous range of dates.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the data source for this here: [http://climate.weather.gc.ca/climate_normals/results_e.](http://climate.weather.gc.ca/climate_normals/results_e.html?stnID=888)html?stnID=888.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 1: Building a Third-Order Polynomial Model'
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Shifting our attention back to the Boston housing dataset, we would like to
    build a third-order polynomial model to compare against the linear one. Recall
    the actual problem we are trying to solve: predicting the median house value,
    given the lower class population percentage. This model could benefit a prospective
    Boston house purchaser who cares about how much of their community would be lower
    class.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: 'Our aim is to use scikit-learn to fit a polynomial regression model to predict
    the median house value (**MEDV**), given the **LSTAT** values. We are hoping to
    build a model that has a lower mean-squared error (**MSE**). In order to achieve
    this, the following steps have to be executed:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Scroll to the empty cells at the bottom of `Subtopic C` in your Jupyter Notebook.
    These will be found beneath the linear-model `Activity` heading.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You should fill these empty cells in with code as we complete the activity.
    You may need to insert new cells as these become filled up; please do so as needed.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Pull out our dependent feature from and target variable from `df`.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify what `x` looks like by printing the first three samples.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform `x` into "polynomial features" by importing the appropriate transformation
    tool from scikit-
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the `x`) by running the `fit_transform` method and build the polynomial
    feature set.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify what `x_poly` looks like by printing the first few samples.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the `LinearRegression` class and build our linear classification model
    the same way as done while calculating the MSE.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the coefficients and print the polynomial model.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the predicted values for each sample and calculate the residuals.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print some of the residual values.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the MSE for the third-order polynomial model.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the polynomial model along with the samples.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the residuals.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The detailed steps along with the solutions are presented in the *Appendix A*
    (pg. no. 144).
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Having successfully modeled the data using a polynomial model, let's finish
    up this chapter by looking at categorical features. In particular, we are going
    to build a set of categorical features and use them to explore the dataset in
    more detail.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Using Categorical Features for Segmentation Analysis
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often, we find datasets where there are a mix of continuous and categorical
    fields. In such cases, we can learn about our data and find patterns by segmenting
    the continuous variables with the categorical fields.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: As a specific example, imagine you are evaluating the return on investment from
    an ad campaign. The data you have access to contain measures of some calculated
    **return on investment** (**ROI**) metric. These values were calculated and recorded
    daily and you are analyzing data from the previous year. You have been tasked
    with finding data-driven insights on ways to improve the ad campaign. Looking
    at the ROI daily time series, you see a weekly oscillation in the data. Segmenting
    by day of the week, you find the following ROI distributions (where 0 represents
    the first day of the week and 6 represents the last).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.48: A sample violin plot for return on investment'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_48.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.41: A sample violin plot for return on investment'
  id: totrans-371
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since we don't have any categorical fields in the Boston housing dataset we
    are working with, we'll create one by effectively discretizing a continuous field.
    In our case, this will involve binning the data into "low", "medium", and "high"
    categories. It's important to note that we are not simply creating a categorical
    data field to illustrate the data analysis concepts in this section. As will be
    seen, doing this can reveal insights from the data that would otherwise be difficult
    to notice or altogether unavailable.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7: Creating Categorical Fields From Continuous Variables and Make
    Segmented Visualizations'
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Scroll up to the pairplot in the Jupyter Notebook where we compared **MEDV**,
    **LSTAT**, **TAX**, **AGE**, and **RM**:![Figure 1.49: A comparison of plots for
    MEDV, LSTAT, TAX, AGE, and RM'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_49.jpg)'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.42: A comparison of plots for MEDV, LSTAT, TAX, AGE, and RM'
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Take a look at the panels containing **AGE**. As a reminder, this feature is
    defined as the proportion of *owner-occupied units built prior to 1940*. We are
    going to convert this feature to a categorical variable. Once it's been converted,
    we'll be able to replot this figure with each panel segmented by color according
    to the age category.
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scroll down to `Subtopic D: Building and exploring categorical features` and
    click into the first cell. Type and execute the following to plot the `kde_kws={''lw'':
    0}` in order to bypass plotting the kernel density estimate in the preceding figure.'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking at the plot, there are very few samples with low **AGE**, whereas there
    are far more with a very large **AGE**. This is indicated by the steepness of
    the distribution on the far right-hand side.
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The red lines indicate 1/3 and 2/3 points in the distribution. Looking at the
    places where our distribution intercepts these horizontal lines, we can see that
    only about 33% of the samples have **AGE** less than 55 and 33% of the samples
    have **AGE** greater than 90! In other words, a third of the housing communities
    have less than 55% of homes built prior to 1940\. These would be considered relatively
    new communities. On the other end of the spectrum, another third of the housing
    communities have over 90% of homes built prior to 1940\. These would be considered
    very old. We''ll use the places where the red horizontal lines intercept the distribution
    as a guide to split the feature into categories: **Relatively New**, **Relatively
    Old**, and **Very Old**.'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a new categorical feature and set the segmentation points by running
    the following code:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, we are using the very handy Pandas method apply, which applies a function
    to a given column or set of columns. The function being applied, in this case
    `get_age_category`, should take one argument representing a row of data and return
    one value for the new column. In this case, the row of data being passed is just
    a single value, the `pd.Series.str` can accomplish the same thing much faster.
    Therefore, it's advised to avoid using it if possible, especially when working
    with large datasets. We'll see some examples of vectorized methods in the upcoming
    chapter.
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Verify the number of samples we''ve grouped into each age category by typing
    `df.groupby(''AGE_category'').size()` into a new cell and running it:![Figure
    1.51: Verifying the grouping of variables'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_51.jpg)'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.44: Verifying the grouping of variables'
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking at the result, it can be seen that two class sizes are fairly equal,
    and the `AGE_category`.
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Construct a violin plot by running the following code:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Figure 1.52: Violin plot for AGE_category and MEDV'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_01_52.jpg)'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.45: Violin plot for AGE_category and MEDV'
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The violin plot shows a kernel density estimate of the median house value distribution
    for each age category. We see that they all resemble a normal distribution. The
    Very Old group contains the lowest median house value samples and has a relatively
    large width, whereas the other groups are more tightly centered around their average.
    The young group is skewed to the high end, which is evident from the enlarged
    right half and position of the white dot in the thick black line within the body
    of the distribution.
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This white dot represents the mean and the thick black line spans roughly 50%
    of the population (it fills to the first quantile on either side of the white
    dot). The thin black line represents boxplot whiskers and spans 95% of the population.
    This inner visualization can be modified to show the individual data points instead
    by passing `inner='point'` to `sns.violinplot()`. Let's do that now.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Re-construct the violin plot adding the `inner=''point''` argument to the `sns.violinplot`
    call:![Figure 1.53: Violin plot for AGE_category and MEDV with the inner = ''point''
    argument'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13018_01_53.jpg)'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.46: Violin plot for AGE_category and MEDV with the inner = ''point''
    argument'
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: It's good to make plots like this for test purposes in order to see how the
    underlying data connects to the visual. We can see, for example, how there are
    no median house values lower than roughly $16,000 for the **Relatively New** segment,
    and therefore the distribution tail actually contains no data. Due to the small
    size of our dataset (only about 500 rows), we can see this is the case for each
    segment.
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Re-construct the pairplot from earlier, but now include color labels for each
    `hue` argument, as follows:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Figure 1.54: Re-constructing pairplot for all variables using color labels
    for AGE'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13018_01_54.jpg)'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.47: Re-constructing pairplot for all variables using color labels
    for AGE'
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking at the histograms, the underlying distributions of each segment appear
    similar for **RM** and **TAX**. The **LSTAT** distributions, on the other hand,
    look more distinct. We can focus on them in more detail by again using a violin plot.
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Re-construct a violin plot comparing the LSTAT distributions for each `AGE_category`
    segment:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.55: Re-constructed violin plots for comparing LSTAT distributions
    for the AGE_category'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13018_01_55.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.48: Re-constructed violin plots for comparing LSTAT distributions
    for the AGE_category'
  id: totrans-408
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unlike the **MEDV** violin plot, where each distribution had roughly the same
    width, here we see the width increasing along with **AGE**. Communities with primarily
    old houses (the **Very Old** segment) contain anywhere from very few to many lower
    class residents, whereas **Relatively New** communities are much more likely to
    be predominantly higher class, with over 95% of samples having less lower class
    percentages than the **Very Old** communities. This makes sense, because **Relatively
    New** neighborhoods would be more expensive.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you have seen the fundamentals of data analysis in Jupyter.
    We began with usage instructions and features of Jupyter such as magic functions
    and tab completion. Then, transitioning to data-science-specific material, we
    introduced the most important libraries for data science with Python.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: In the latter half of the chapter, we ran an exploratory analysis in a live
    Jupyter Notebook. Here, we used visual assists such as scatter plots, histograms,
    and violin plots to deepen our understanding of the data. We also performed simple
    predictive modeling, a topic which will be the focus of the following chapter
    in this book.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how to approach predictive analytics, what
    things to consider when preparing the data for modeling, and how to implement
    and compare a variety of models using Jupyter Notebooks.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
