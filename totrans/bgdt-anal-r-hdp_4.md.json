["```py\n bin/hadoop command [generic Options] [streaming Options]\n\n```", "```py\n    -conf configuration_file\n\n    ```", "```py\n    -D dfs.temp.dir=/app/tmp/Hadoop/\n\n    ```", "```py\n    -D mapred.reduce.tasks=0\n\n    ```", "```py\n    -fs localhost:port\n\n    ```", "```py\n    -jt localhost:port\n\n    ```", "```py\n    -files hdfs://host:port/directory/txtfile.txt\n\n    ```", "```py\n    -libjars  /opt/ current/lib/a.jar, /opt/ current/lib/b.jar\n\n    ```", "```py\n    -archives hdfs://host:fs_port/user/testfile.jar\n\n    ```", "```py\n$ head -5 gadata_mr.csv\n20120301,India,Ahmedabad,/\n20120302,India,Ahmedabad,/gtuadmissionhelpline-team\n20120302,India,Mumbai,/\n20120302,India,Mumbai,/merit-calculator\n20120303,India,Chennai,/\n\n```", "```py\n# To identify the type of the script, here it is RScript\n#! /usr/bin/env Rscript\n# To disable the warning massages to be printed\noptions(warn=-1)\n# To initiating the connection to standard input\ninput <- file(\"stdin\", \"r\")\nEach line has these four fields (date, country, city, and pagePath) in the same order. We split the line by a comma. The result is a vector which has the date, country, city, and pathPath in the indexes 1,2,3, and 4 respectively.\n\n```", "```py\n# Running while loop until all the lines are read\nwhile(length(currentLine <- readLines(input, n=1, warn=FALSE)) > 0) {\n\n# Splitting the line into vectors by \",\" separator \n fields <- unlist(strsplit(currentLine, \",\"))\n\n# Capturing the city and pagePath from fields\n city <- as.character(fields[3])\n pagepath <- as.character(fields[4])\n\n# Printing both to the standard output\nprint(paste(city, pagepath,sep=\"\\t\"),stdout())\n\n}\n\n# Closing the connection to that input stream\nclose(input)\n\n```", "```py\n# To identify the type of the script, here it is RScript\n#! /usr/bin/env Rscript\n\n# Defining the variables with their initial values\ncity.key <- NA\npage.value <- 0.0\n\n# To initiating the connection to standard input\ninput <- file(\"stdin\", open=\"r\")\n\n# Running while loop until all the lines are read\nwhile (length(currentLine <- readLines(input, n=1)) > 0) {\n\n# Splitting the Mapper output line into vectors by \n# tab(\"\\t\") separator\n fields <- strsplit(currentLine, \"\\t\")\n\n# capturing key and value form the fields\n# collecting the first data element from line which is city\n key <- fields[[1]][1]\n# collecting the pagepath value from line \n value <- as.character(fields[[1]][2])\n\n```", "```py\n# setting up key and values\n\n# if block will check whether key attribute is \n# initialized or not. If not initialized then it will be # assigned from collected key attribute with value from # mapper output. This is designed to run at initial time.\n if (is.na(city.key)) {\n city.key <- key\n page.value <- value\n }\n else {\n\n# Once key attributes are set, then will match with the previous key attribute value. If both of them matched then they will combined in to one.\n if (city.key == key) {\n page.value <- c(page.value, value)\n\n }\n else {\n\n# if key attributes are set already but attribute value # is other than previous one then it will emit the store #p agepath values along with associated key attribute value of city,\n\n page.value <- unique(page.value)\n# printing key and value to standard output\nprint(list(city.key, page.value),stdout())\n city.key <- key\n page.value <- value\n }\n }\n}\n\nprint(list(city.key, page.value), stdout())\n\n# closing the connection\nclose(input)\n\n```", "```py\n$ cat gadata_sample.csv | ga_mapper.R |sort | ga_reducer.R\n\n```", "```py\n$ bin/hadoop jar {HADOOP_HOME}/contrib/streaming/hadoop-streaming-1.0.3.jar \n -input /ga/gadaat_mr.csv \n -output /ga/output1 \n -file /usr/local/hadoop/ga/ga_mapper.R  \n -mapper ga_mapper.R \n -file /usr/local/hadoop/ga/ga_ reducer.R \n -reducer ga_reducer.R\n\n```", "```py\nsystem(paste(\"bin/hadoop jar”, “{HADOOP_HOME}/contrib/streaming/hadoop-streaming-1.0.3.jar\",\n \"-input /ga/gadata_mr.csv\", \n \"-output /ga/output2\", \n \"-file /usr/local/hadoop/ga/ga_mapper.R\",\n\"-mapper ga_mapper.R\", \n \"-file /usr/local/hadoop/ga/ga_reducer.R\", \n \"-reducer ga_reducer.R\")) \n\n```", "```py\n$ bin/hadoop dfs -cat /ga/output/part-* > temp.txt\n$ head -n 40 temp.txt\n\n```", "```py\ndir <- system(\"bin/hadoop dfs -ls /ga/output\",intern=TRUE)\nout <- system(\"bin/hadoop dfs -cat /ga/output2/part-00000\",intern=TRUE)\n\n```", "```py\n    Con <- file(\"stdin\", \"r\")\n\n    ```", "```py\n    write(paste(city,pagepath,sep=\"\\t\"),stdout())\n\n    ```", "```py\n    print(paste(city,pagepath,sep=\"\\t\"),stdout())\n\n    ```", "```py\n    conn <- file(\"stdin\", open=\"r\")\n\n    ```", "```py\n    print(list(city.key, page.value),stdout())\n\n    ## where city.key is key and page.value is value of that key\n\n    ```", "```py\n    sink(\"log.txt\")\n    k <- 1:5\n    for(i in 1:k){\n    print(paste(\"value of k\",k))\n    }sink()\n    unlink(\"log.txt\")\n    ```", "```py\n$ bin/hadoop job –history /output/location \n\n```", "```py\n$ bin/hadoop job -history all /output/location \n\n```", "```py\nhsTableReader(file=\"\", cols='character',\n chunkSize=-1, FUN=print,\n ignoreKey=TRUE, singleKey=TRUE, skip=0,\n sep='\\t', keyCol='key',\n FUN=NULL, ,carryMemLimit=512e6,\n carryMaxRows=Inf,\n stringsAsFactors=FALSE)\n\n```", "```py\n# Loading libraries\nLibrary(\"HadoopStreaming\")\n# Input data String with collection of key and values\nstr <- \" key1\\t1.91\\nkey1\\t2.1\\nkey1\\t20.2\\nkey1\\t3.2\\nkey2\\t1.2\\nkey2\\t10\\nkey3\\t2.5\\nkey3\\t2.1\\nkey4\\t1.2\\n\"cat(str)\n\n```", "```py\n# A list of column names, as'what' arg to scan\ncols = list(key='',val=0)\n\n# To make a text connection\ncon <- textConnection(str, open = \"r\")\n# To read the data with chunksize 3\nhsTableReader(con,cols,chunkSize=3,FUN=print,ignoreKey=TRUE)\n\n```", "```py\nhsKeyValReader(file = \"\", chunkSize = -1, skip = 0, sep = \"\\t\",FUN = function(k, v) cat(paste(k, v))\n\n```", "```py\n# Function for reading chunkwise dataset\nprintkeyval <- function(k,v) {cat('A chunk:\\n')cat(paste(k,v,sep=': '),sep='\\n')}\nstr <- \"key1\\tval1\\nkey2\\tval2\\nkey3\\tval3\\n\"\ncon <- textConnection(str, open = \"r\")\n\nhsKeyValReader(con, chunkSize=1, FUN=printFn)\n\n```", "```py\nhsLineReader(file = \"\", chunkSize = 3, skip = 0, FUN = function(x) cat(x, sep = \"\\n\"))\n\n```", "```py\nstr <- \" This is HadoopStreaming!!\\n here are,\\n examples for chunk dataset!!\\n in R\\n  ?\"\n\n#  For defining the string as data source\ncon <- textConnection(str, open = \"r\")\n\n# read from the con object\nhsLineReader(con,chunkSize=2,FUN=print)\n\n```", "```py\n## Loading the library\nlibrary(HadoopStreaming)\n\n## Additional command line arguments for this script (rest are default in hsCmdLineArgs)\nspec = c('printDone','D',0,\"logical\",\"A flag to write DONE at the end.\",FALSE)\n\nopts = hsCmdLineArgs(spec, openConnections=T)\n\nif (!opts$set) {\n quit(status=0)\n}\n\n# Defining the Mapper columns names\nmapperOutCols = c('word','cnt')\n\n# Defining the Reducer columns names\nreducerOutCols = c('word','cnt')\n\n# printing the column header for Mapper output\nif (opts$mapcols) {\n cat( paste(mapperOutCols,collapse=opts$outsep),'\\n', file=opts$outcon )\n} \n\n# Printing the column header for Reducer output \nif (opts$reducecols) {\n cat( paste(reducerOutCols,collapse=opts$outsep),'\\n', file=opts$outcon )\n}\n\n## For running the Mapper\nif (opts$mapper) {\n mapper <- function(d) {\n    words <- strsplit(paste(d,collapse=' '),'[[:punct:][:space:]]+')[[1]] # split on punctuation and spaces\n    words <- words[!(words=='')]  # get rid of empty words caused by whitespace at beginning of lines\n    df = data.frame(word=words)\n    df[,'cnt']=1\n\n# For writing the output in the form of key-value table format\nhsWriteTable(df[,mapperOutCols],file=opts$outcon,sep=opts$outsep)\n  }\n\n## For chunk wise reading the Mapper output, to be feeded to Reducer hsLineReader(opts$incon,chunkSize=opts$chunksize,FUN=mapper)\n\n## For running the Reducer\n} else if (opts$reducer) {\n\n  reducer <- function(d) {\n    cat(d[1,'word'],sum(d$cnt),'\\n',sep=opts$outsep)\n  }\n  cols=list(word='',cnt=0)  # define the column names and types (''-->string 0-->numeric)\n  hsTableReader(opts$incon,cols,chunkSize=opts$chunksize,skip=opts$skip,sep=opts$insep,keyCol='word',singleKey=T, ignoreKey= F, FUN=reducer)\n  if (opts$printDone) {\n    cat(\"DONE\\n\");\n  }\n}\n\n# For closing the connection corresponding to input\nif (!is.na(opts$infile)) {\n  close(opts$incon)\n}\n\n# For closing the connection corresponding to input\nif (!is.na(opts$outfile)) {\n  close(opts$outcon)\n}\n\n```", "```py\n#! /usr/bin/env bash\nHADOOP=\"$HADOOP_HOME/bin/hadoop\"   # Hadoop command\n\nHADOOPSTREAMING=\"$HADOOP jar\n$HADOOP_HOME/contrib/streaming/hadoop-streaming-1.0.3.jar\" # change version number as appropriate\n\nRLIBPATH=/usr/local/lib/R/site-library  # can specify additional R Library paths here\n\n```", "```py\nINPUTFILE=\"anna.txt\"\nHFSINPUTDIR=\"/HadoopStreaming\"\nOUTDIR=\"/HadoopStreamingRpkg_output\"\n\nRFILE=\" home/hduser/Desktop/HadoopStreaming/inst/wordCntDemo/ hsWordCnt.R\"\n#LOCALOUT=\"/home/hduser/Desktop/HadoopStreaming/inst/wordCntDemo/annaWordCnts.out\"\n# Put the file into the Hadoop file system\n#$HADOOP fs -put $INPUTFILE $HFSINPUTDIR\n\n```", "```py\n# Remove the directory if already exists (otherwise, won't run)\n#$HADOOP fs -rmr $OUTDIR\n\n```", "```py\nMAPARGS=\"--mapper\" \nREDARGS=\"--reducer\"\nJOBARGS=\"-cmdenv R_LIBS=$RLIBPATH\" # numReduceTasks 0\n# echo $HADOOPSTREAMING -cmdenv R_LIBS=$RLIBPATH  -input $HFSINPUTDIR/$INPUTFILE -output $OUTDIR -mapper \"$RFILE $MAPARGS\" -reducer \"$RFILE $REDARGS\" -file $RFILE \n\n$HADOOPSTREAMING $JOBARGS   -input $HFSINPUTDIR/$INPUTFILE -output $OUTDIR -mapper \"$RFILE $MAPARGS\" -reducer \"$RFILE $REDARGS\" -file $RFILE \n\n```", "```py\n# Extract output\n./$RFILE --reducecols > $LOCALOUT\n$HADOOP fs -cat $OUTDIR/part* >> $LOCALOUT\n\n```", "```py\nsudo chmod +x runHadoop.sh\n\n```", "```py\n./runHadoop.sh\n\n```"]