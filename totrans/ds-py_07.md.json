["```py\n    ipython kernel install --prefix /tmp\n\n    ```", "```py\n    \"env\": {\n        \"PYTHONPATH\": \"<<spark_root_path>>/python/:<<spark_root_path>>/python/lib/py4j-<<py4j_version>>-src.zip\",\n        \"SPARK_HOME\": \"<<spark_root_path>>\",\n        \"PYSPARK_SUBMIT_ARGS\": \"--master local[10] pyspark-shell\",\n        \"SPARK_DRIVER_MEMORY\": \"10G\",\n        \"SPARK_LOCAL_IP\": \"127.0.0.1\",\n        \"PYTHONSTARTUP\": \"<<spark_root_path>>/python/pyspark/shell.py\"\n    }\n    ```", "```py\n    jupyter kernelspec list\n\n    ```", "```py\n     Available kernels:\n     pixiedustspark16\n     /Users/dtaieb/Library/Jupyter/kernels/pixiedustspark16\n     pixiedustspark21\n     /Users/dtaieb/Library/Jupyter/kernels/pixiedustspark21\n     pixiedustspark22\n     /Users/dtaieb/Library/Jupyter/kernels/pixiedustspark22\n     pixiedustspark23\n     /Users/dtaieb/Library/Jupyter/kernels/pixiedustspark23\n\n    ```", "```py\n    jupyter kernelspec install /tmp/share/jupyter/kernels/python3\n\n    ```", "```py\n     jupyter kernelspec install -h\n\n    ```", "```py\njupyter pixiedust install\n\n```", "```py\n!pip install tweepy\n\n```", "```py\nfrom tweepy import OAuthHandler\n# Go to http://apps.twitter.com and create an app.\n# The consumer key and secret will be generated for you after\nconsumer_key=\"XXXX\"\nconsumer_secret=\"XXXX\"\n\n# After the step above, you will be redirected to your app's page.\n# Create an access token under the \"Your access token\" section\naccess_token=\"XXXX\"\naccess_token_secret=\"XXXX\"\n\nauth = OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\n```", "```py\nfrom six import iteritems\nimport json\nimport csv\nfrom tweepy.streaming import StreamListener\nclass RawTweetsListener(StreamListener):\n    def __init__(self):\n        self.buffered_data = []\n        self.counter = 0\n\n    def flush_buffer_if_needed(self):\n        \"Check the buffer capacity and write to a new file if needed\"\n        length = len(self.buffered_data)\n        if length > 0 and length % 10 == 0:\n            with open(os.path.join( output_dir,\n                \"tweets{}.csv\".format(self.counter)), \"w\") as fs:\n                self.counter += 1\n                csv_writer = csv.DictWriter( fs,\n                    fieldnames = fieldnames)\n                for data in self.buffered_data:\n csv_writer.writerow(data)\n            self.buffered_data = []\n\n    def on_data(self, data):\n        def transform(key, value):\n            return transforms[key](value) if key in transforms else value\n\n        self.buffered_data.append(\n            {key:transform(key,value) \\\n                 for key,value in iteritems(json.loads(data)) \\\n                 if key in fieldnames}\n        )\n        self.flush_buffer_if_needed()\n        return True\n\n    def on_error(self, status):\n        print(\"An error occured while receiving streaming data: {}\".format(status))\n        return False\n```", "```py\n    from pyspark.sql.types import StringType, DateType\n    from bs4 import BeautifulSoup as BS\n    fieldnames = [f[\"name\"] for f in field_metadata]\n    transforms = {\n        item['name']:item['transform'] for item in field_metadata if \"transform\" in item\n    }\n    field_metadata = [\n        {\"name\": \"created_at\",\"type\": DateType()},\n        {\"name\": \"text\", \"type\": StringType()},\n        {\"name\": \"source\", \"type\": StringType(),\n             \"transform\": lambda s: BS(s, \"html.parser\").text.strip()\n        }\n    ]\n    ```", "```py\n    import shutil\n    def ensure_dir(dir, delete_tree = False):\n        if not os.path.exists(dir):\n            os.makedirs(dir)\n        elif delete_tree:\n            shutil.rmtree(dir)\n            os.makedirs(dir)\n        return os.path.abspath(dir)\n\n    root_dir = ensure_dir(\"output\", delete_tree = True)\n    output_dir = ensure_dir(os.path.join(root_dir, \"raw\"))\n\n    ```", "```py\n    lambda s: BS(s, \"html.parser\").text.strip()\n    ```", "```py\nfrom tweepy import Stream\ndef start_stream(queries):\n    \"Asynchronously start a new Twitter stream\"\n    stream = Stream(auth, RawTweetsListener())\n stream.filter(track=queries, async=True)\n    return stream\n```", "```py\nstream = start_stream([\"baseball\"])\n```", "```py\nstream.disconnect()\n```", "```py\nschema = StructType(\n[StructField(f[\"name\"], f[\"type\"], True) for f in field_metadata]\n)\ncsv_sdf = spark.readStream\\\n    .format(\"csv\")\\\n .option(\"schema\", schema)\\\n    .option(\"multiline\", True)\\\n .option(\"dateFormat\", 'EEE MMM dd kk:mm:ss Z y')\\\n    .option(\"ignoreTrailingWhiteSpace\", True)\\\n    .option(\"ignoreLeadingWhiteSpace\", True)\\\n    .load(output_dir)\n```", "```py\ncsv_sdf = spark.readStream \\\n    .csv(\n        output_dir,\n        schema=schema,\n        multiLine = True,\n        dateFormat = 'EEE MMM dd kk:mm:ss Z y',\n        ignoreTrailingWhiteSpace = True,\n        ignoreLeadingWhiteSpace = True\n    )\n```", "```py\nprint(csv_sdf.isStreaming)\ncsv_sdf.printSchema()\n```", "```py\nroot\n |-- created_at: date (nullable = true)\n |-- text: string (nullable = true)\n |-- source: string (nullable = true)\n```", "```py\ntweet_streaming_query = csv_sdf \\\n    .writeStream \\\n    .format(\"parquet\") \\\n .option(\"path\", os.path.join(root_dir, \"output_parquet\")) \\\n .trigger(processingTime=\"2 seconds\") \\\n .option(\"checkpointLocation\", os.path.join(root_dir, \"output_chkpt\")) \\\n    .start()\n```", "```py\ntweet_streaming_query.stop()\n```", "```py\ntweet_streaming_query = csv_sdf.writeStream\\\n    .outputMode(\"append\")\\\n    .format(\"console\")\\\n    .trigger(processingTime='2 seconds')\\\n    .start()\n```", "```py\n-------------------------------------------\nBatch: 17\n-------------------------------------------\n+----------+--------------------+-------------------+\n|created_at|                text|             source|\n+----------+--------------------+-------------------+\n|2018-04-12|RT @XXXXXXXXXXXXX...|Twitter for Android|\n|2018-04-12|RT @XXXXXXX: Base...| Twitter for iPhone|\n|2018-04-12|That's my roommat...| Twitter for iPhone|\n|2018-04-12|He's come a long ...| Twitter for iPhone|\n|2018-04-12|RT @XXXXXXXX: U s...| Twitter for iPhone|\n|2018-04-12|Baseball: Enid 10...|   PushScoreUpdates|\n|2018-04-12|Cubs and Sox aren...| Twitter for iPhone|\n|2018-04-12|RT @XXXXXXXXXX: T...|          RoundTeam|\n|2018-04-12|@XXXXXXXX that ri...| Twitter for iPhone|\n|2018-04-12|RT @XXXXXXXXXX: S...| Twitter for iPhone|\n+----------+--------------------+-------------------+\n```", "```py\nprint(spark.streams.active)\n```", "```py\n[<pyspark.sql.streaming.StreamingQuery object at 0x12d7db6a0>, <pyspark.sql.streaming.StreamingQuery object at 0x12d269c18>]\n```", "```py\nimport json\nfor query in spark.streams.active:\n    print(\"-----------\")\n    print(\"id: {}\".format(query.id))\n    print(json.dumps(query.lastProgress, indent=2, sort_keys=True))\n```", "```py\n-----------\nid: b621e268-f21d-4eef-b6cd-cb0bc66e53c4\n{\n  \"batchId\": 18,\n  \"durationMs\": {\n    \"getOffset\": 4,\n    \"triggerExecution\": 4\n  },\n  \"id\": \"b621e268-f21d-4eef-b6cd-cb0bc66e53c4\",\n  \"inputRowsPerSecond\": 0.0,\n  \"name\": null,\n  \"numInputRows\": 0,\n  \"processedRowsPerSecond\": 0.0,\n  \"runId\": \"d2459446-bfad-4648-ae3b-b30c1f21be04\",\n  \"sink\": {\n    \"description\": \"org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@586d2ad5\"\n  },\n  \"sources\": [\n    {\n      \"description\": \"FileStreamSource[file:/Users/dtaieb/cdsdev/notebookdev/Pixiedust/book/Chapter7/output/raw]\",\n      \"endOffset\": {\n        \"logOffset\": 17\n      },\n      \"inputRowsPerSecond\": 0.0,\n      \"numInputRows\": 0,\n      \"processedRowsPerSecond\": 0.0,\n      \"startOffset\": {\n        \"logOffset\": 17\n      }\n    }\n  ],\n  \"stateOperators\": [],\n  \"timestamp\": \"2018-04-12T21:40:10.004Z\"\n}\n```", "```py\n    parquet_batch_df = spark.read.parquet(os.path.join(root_dir, \"output_parquet\"))\n    ```", "```py\n    parquet_batch_df = spark.sql(\n    \"select * from parquet.'{}'\".format(\n    os.path.join(root_dir, \"output_parquet\")\n    )\n    )\n    ```", "```py\nimport pixiedust\ndisplay(parquet_batch_df)\n```", "```py\n!pip install Watson_developer_cloud\n\n```", "```py\nfrom watson_developer_cloud import NaturalLanguageUnderstandingV1\nfrom watson_developer_cloud.natural_language_understanding_v1 import Features, SentimentOptions, EntitiesOptions\n\nnlu = NaturalLanguageUnderstandingV1(\n    version='2017-02-27',\n    username='XXXX',\n    password='XXXX'\n)\n```", "```py\n[[RawTweetsListener]]\ndef enrich(self, data):\n    try:\n        response = nlu.analyze(\n text = data['text'],\n features = Features(\n sentiment=SentimentOptions(),\n entities=EntitiesOptions()\n )\n )\n        data[\"sentiment\"] = response[\"sentiment\"][\"document\"][\"label\"]\n        top_entity = response[\"entities\"][0] if len(response[\"entities\"]) > 0 else None\n        data[\"entity\"] = top_entity[\"text\"] if top_entity is not None else \"\"\n        data[\"entity_type\"] = top_entity[\"type\"] if top_entity is not None else \"\"\n        return data\n    except Exception as e:\n self.warn(\"Error from Watson service while enriching data: {}\".format(e))\n\n```", "```py\nfield_metadata = [\n    {\"name\": \"created_at\", \"type\": DateType()},\n    {\"name\": \"text\", \"type\": StringType()},\n    {\"name\": \"source\", \"type\": StringType(),\n         \"transform\": lambda s: BS(s, \"html.parser\").text.strip()\n    },\n {\"name\": \"sentiment\", \"type\": StringType()},\n {\"name\": \"entity\", \"type\": StringType()},\n {\"name\": \"entity_type\", \"type\": StringType()}\n]\n```", "```py\ndef on_data(self, data):\n    def transform(key, value):\n        return transforms[key](value) if key in transforms else value\n    data = self.enrich(json.loads(data))\n if data is not None:\n        self.buffered_data.append(\n            {key:transform(key,value) \\\n                for key,value in iteritems(data) \\\n                if key in fieldnames}\n        )\n        self.flush_buffer_if_needed()\n    return True\n\n```", "```py\nschema = StructType(\n    [StructField(f[\"name\"], f[\"type\"], True) for f in field_metadata]\n)\ncsv_sdf = spark.readStream \\\n    .csv(\n        output_dir,\n        schema=schema,\n        multiLine = True,\n        dateFormat = 'EEE MMM dd kk:mm:ss Z y',\n        ignoreTrailingWhiteSpace = True,\n        ignoreLeadingWhiteSpace = True\n    )\ncsv_sdf.printSchema()\n```", "```py\nroot\n |-- created_at: date (nullable = true)\n |-- text: string (nullable = true)\n |-- source: string (nullable = true)\n |-- sentiment: string (nullable = true)\n |-- entity: string (nullable = true)\n |-- entity_type: string (nullable = true)\n\n```", "```py\n-------------------------------------------\nBatch: 2\n-------------------------------------------\n+----------+---------------+---------------+---------+------------+-------------+\n|created_at|           text|         source|sentiment|      entity|  entity_type|\n+----------+---------------+---------------+---------+------------+-------------+\n|2018-04-14|Some little ...| Twitter iPhone| positive|        Drew|       Person|d\n|2018-04-14|RT @XXXXXXXX...| Twitter iPhone|  neutral| @XXXXXXXXXX|TwitterHandle|\n|2018-04-14|RT @XXXXXXXX...| Twitter iPhone|  neutral|    baseball|        Sport|\n|2018-04-14|RT @XXXXXXXX...| Twitter Client|  neutral| @XXXXXXXXXX|TwitterHandle|\n|2018-04-14|RT @XXXXXXXX...| Twitter Client| positive| @XXXXXXXXXX|TwitterHandle|\n|2018-04-14|RT @XXXXX: I...|Twitter Android| positive| Greg XXXXXX|       Person|\n|2018-04-14|RT @XXXXXXXX...| Twitter iPhone| positive| @XXXXXXXXXX|TwitterHandle|\n|2018-04-14|RT @XXXXX: I...|Twitter Android| positive| Greg XXXXXX|       Person|\n|2018-04-14|Congrats to ...|Twitter Android| positive|    softball|        Sport|\n|2018-04-14|translation:...| Twitter iPhone|  neutral|        null|         null|\n+----------+---------------+---------------+---------+------------+-------------+\n```", "```py\ndef start_stream(queries):\n    \"Asynchronously start a new Twitter stream\"\n    stream = Stream(auth, RawTweetsListener())\n    stream.filter(track=queries, languages=[\"en\"], async=True)\n    return stream\n```", "```py\ndef start_streaming_dataframe(output_dir):\n    \"Start a Spark Streaming DataFrame from a file source\"\n    schema = StructType(\n        [StructField(f[\"name\"], f[\"type\"], True) for f in field_metadata]\n    )\n    return spark.readStream \\\n        .csv(\n            output_dir,\n            schema=schema,\n            multiLine = True,\n            timestampFormat = 'EEE MMM dd kk:mm:ss Z yyyy',\n            ignoreTrailingWhiteSpace = True,\n            ignoreLeadingWhiteSpace = True\n        )\n```", "```py\ndef start_parquet_streaming_query(csv_sdf):\n    \"\"\"\n    Create and run a streaming query from a Structured DataFrame\n    outputing the results into a parquet database\n    \"\"\"\n    streaming_query = csv_sdf \\\n      .writeStream \\\n      .format(\"parquet\") \\\n      .option(\"path\", os.path.join(root_dir, \"output_parquet\")) \\\n      .trigger(processingTime=\"2 seconds\") \\\n      .option(\"checkpointLocation\", os.path.join(root_dir, \"output_chkpt\")) \\\n      .start()\n    return streaming_query\n```", "```py\nclass StreamsManager():\n    def __init__(self):\n        self.twitter_stream = None\n        self.csv_sdf = None\n\n    def reset(self, search_query = None):\n        if self.twitter_stream is not None:\n            self.twitter_stream.disconnect()\n        #stop all the active streaming queries and re_initialize the directories\n        for query in spark.streams.active:\n            query.stop()\n        # initialize the directories\n        self.root_dir, self.output_dir = init_output_dirs()\n        # start the tweepy stream\n        self.twitter_stream = start_stream([search_query]) if search_query is not None else None\n        # start the spark streaming stream\n        self.csv_sdf = start_streaming_dataframe(output_dir) if search_query is not None else None\n\n def __del__(self):\n # Automatically called when the class is garbage collected\n self.reset()\n\nstreams_manager = StreamsManager()\n```", "```py\nfrom pixiedust.display.app import *\n@PixieApp\nclass TweetInsightApp():\n    @route()\n    def main_screen(self):\n        return \"\"\"\n<style>\n    div.outer-wrapper {\n        display: table;width:100%;height:300px;\n    }\n    div.inner-wrapper {\n        display: table-cell;vertical-align: middle;height: 100%;width: 100%;\n    }\n</style>\n<div class=\"outer-wrapper\">\n    <div class=\"inner-wrapper\">\n        <div class=\"col-sm-3\"></div>\n        <div class=\"input-group col-sm-6\">\n          <input id=\"query{{prefix}}\" type=\"text\" class=\"form-control\"\n              value=\"\"\n              placeholder=\"Enter a search query (e.g. baseball)\">\n          <span class=\"input-group-btn\">\n            <button class=\"btn btn-default\" type=\"button\"\n pd_options=\"search_query=$val(query{{prefix}})\">\n                Go\n            </button>\n          </span>\n        </div>\n    </div>\n</div>\n        \"\"\"\n\nTweetInsightApp().run()\n```", "```py\nimport time\n[[TweetInsightApp]]\n@route(search_query=\"*\")\n    def do_search_query(self, search_query):\n        streams_manager.reset(search_query)\n        start_parquet_streaming_query(streams_manager.csv_sdf)\n while True:\n try:\n parquet_dir = os.path.join(root_dir,\n \"output_parquet\")\n self.parquet_df = spark.sql(\"select * from parquet.'{}'\".format(parquet_dir))\n break\n except:\n time.sleep(5)\n        return \"\"\"\n<div class=\"container\">\n <div id=\"header{{prefix}}\" class=\"row no_loading_msg\"\n pd_refresh_rate=\"5000\" pd_target=\"header{{prefix}}\">\n <pd_script>\nprint(\"Number of tweets received: {}\".format(streams_manager.twitter_stream.listener.tweet_count))\n </pd_script>\n </div>\n    <div class=\"row\" style=\"min-height:300px\">\n        <div class=\"col-sm-5\">\n            <div id=\"metric1{{prefix}}\" pd_refresh_rate=\"10000\"\n                class=\"no_loading_msg\"\n                pd_options=\"display_metric1=true\"\n                pd_target=\"metric1{{prefix}}\">\n            </div>\n        </div>\n        <div class=\"col-sm-5\">\n            <div id=\"metric2{{prefix}}\" pd_refresh_rate=\"12000\"\n                class=\"no_loading_msg\"\n                pd_options=\"display_metric2=true\"\n                pd_target=\"metric2{{prefix}}\">\n            </div>\n        </div>\n    </div>\n\n    <div class=\"row\" style=\"min-height:400px\">\n        <div class=\"col-sm-offset-1 col-sm-10\">\n            <div id=\"word_cloud{{prefix}}\" pd_refresh_rate=\"20000\"\n                class=\"no_loading_msg\"\n                pd_options=\"display_wc=true\"\n                pd_target=\"word_cloud{{prefix}}\">\n            </div>\n        </div>\n    </div>\n        \"\"\"\n```", "```py\n    [[TweetInsightApp]]\n    def on_data(self, data):\n            def transform(key, value):\n                return transforms[key](value) if key in transforms else value\n            data = self.enrich(json.loads(data))\n            if data is not None:\n     self.tweet_count += 1\n                self.buffered_data.append(\n                    {key:transform(key,value) \\\n                         for key,value in iteritems(data) \\\n                         if key in fieldnames}\n                )\n                self.flush_buffer_if_needed()\n            return True\n    ```", "```py\n[[TweetInsightApp]]\n@route(display_metric1=\"*\")\n    def do_display_metric1(self, display_metric1):\n        parquet_dir = os.path.join(root_dir, \"output_parquet\")\n        self.parquet_df = spark.sql(\"select * from parquet.'{}'\".format(parquet_dir))\n        return \"\"\"\n<div class=\"no_loading_msg\" pd_render_onload pd_entity=\"parquet_df\">\n    <pd_options>\n    {\n      \"legend\": \"true\",\n      \"keyFields\": \"sentiment\",\n      \"clusterby\": \"entity_type\",\n      \"handlerId\": \"barChart\",\n      \"rendererId\": \"bokeh\",\n      \"rowCount\": \"10\",\n      \"sortby\": \"Values DESC\",\n      \"noChartCache\": \"true\"\n    }\n    </pd_options>\n</div>\n        \"\"\"\n```", "```py\n!pip install wordcloud\n\n```", "```py\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\n[[TweetInsightApp]]\n@route(display_wc=\"*\")\n@captureOutput\ndef do_display_wc(self):\n    text = \"\\n\".join(\n [r['entity'] for r in self.parquet_df.select(\"entity\").collect() if r['entity'] is not None]\n )\n    plt.figure( figsize=(13,7) )\n    plt.axis(\"off\")\n    plt.imshow(\n        WordCloud(width=750, height=350).generate(text),\n        interpolation='bilinear'\n    )\n```", "```py\n@PixieApp\nclass StreamingQueriesApp():\n    @route()\n    def main_screen(self):\n        return \"\"\"\n<div class=\"no_loading_msg\" pd_refresh_rate=\"5000\" pd_options=\"show_progress=true\">\n</div>\n        \"\"\"\n```", "```py\n@route(show_progress=\"true\")\n    def do_show_progress(self):\n        return \"\"\"\n{%for query in this.spark.streams.active%}\n    <div>\n    <div class=\"page-header\">\n        <h1>Progress Report for Spark Stream: {{query.id}}</h1>\n    <div>\n    <table>\n        <thead>\n          <tr>\n             <th>metric</th>\n             <th>value</th>\n          </tr>\n        </thead>\n        <tbody>\n {%for key, value in query.lastProgress.items()%}\n <tr>\n <td>{{key}}</td>\n <td>{{value}}</td>\n </tr>\n {%endfor%}\n        </tbody>\n    </table>\n{%endfor%}\n        \"\"\"\n```", "```py\nfrom pixiedust.display.app import *\nfrom pixiedust.apps.template import TemplateTabbedApp\n\n@PixieApp\nclass TwitterSentimentApp(TemplateTabbedApp):\n    def setup(self):\n self.apps = [\n {\"title\": \"Tweets Insights\", \"app_class\": \"TweetInsightApp\"},\n {\"title\": \"Streaming Queries\", \"app_class\": \"StreamingQueriesApp\"}\n ]\n\napp = TwitterSentimentApp()\napp.run()\n```", "```py\nmessage_hub_creds = {\n  \"instance_id\": \"XXXXX\",\n  \"mqlight_lookup_url\": \"https://mqlight-lookup-prod02.messagehub.services.us-south.bluemix.net/Lookup?serviceId=XXXX\",\n  \"api_key\": \"XXXX\",\n  \"kafka_admin_url\": \"https://kafka-admin-prod02.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_rest_url\": \"https://kafka-rest-prod02.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_brokers_sasl\": [\n    \"kafka03-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka01-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka02-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka05-prod02.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka04-prod02.messagehub.services.us-south.bluemix.net:9093\"\n  ],\n  \"user\": \"XXXX\",\n  \"password\": \"XXXX\"\n}\n```", "```py\n!pip install kafka-python\n\n```", "```py\n[[RawTweetsListener]]\ncontext = ssl.create_default_context()\ncontext.options &= ssl.OP_NO_TLSv1\ncontext.options &= ssl.OP_NO_TLSv1_1\nkafka_conf = {\n    'sasl_mechanism': 'PLAIN',\n    'security_protocol': 'SASL_SSL',\n    'ssl_context': context,\n    \"bootstrap_servers\": message_hub_creds[\"kafka_brokers_sasl\"],\n    \"sasl_plain_username\": message_hub_creds[\"user\"],\n    \"sasl_plain_password\": message_hub_creds[\"password\"],\n    \"api_version\":(0, 10, 1),\n    \"value_serializer\" : lambda v: json.dumps(v).encode('utf-8')\n}\nself.producer = KafkaProducer(**kafka_conf)\n\n```", "```py\nimport requests\nimport json\n\ndef ensure_topic_exists(topic_name):\n    response = requests.post(\n message_hub_creds[\"kafka_rest_url\"] +\n \"/admin/topics\",\n data = json.dumps({\"name\": topic_name}),\n headers={\"X-Auth-Token\": message_hub_creds[\"api_key\"]}\n )\n    if response.status_code != 200 and \\\n       response.status_code != 202 and \\\n       response.status_code != 422 and \\\n       response.status_code != 403:\n        raise Exception(response.json())\n```", "```py\n[[RawTweetsListener]]\ndef on_data(self, data):\n    self.tweet_count += 1\n self.producer.send(\n self.topic,\n {key:transform(key,value) \\\n for key,value in iteritems(json.loads(data)) \\\n if key in fieldnames}\n )\n    return True\n```", "```py\nimport sys\nfrom watson_developer_cloud import NaturalLanguageUnderstandingV1\nfrom watson_developer_cloud.natural_language_understanding_v1 import Features, SentimentOptions, EntitiesOptions\n\n# init() function will be called once on pipeline initialization\n# @state a Python dictionary object for keeping state. The state object is passed to the process function\ndef init(state):\n    # do something once on pipeline initialization and save in the state object\n state[\"nlu\"] = NaturalLanguageUnderstandingV1(\n version='2017-02-27',\n username='XXXX',\n password='XXXX'\n )\n\n```", "```py\n# @event a Python dictionary object representing the input event tuple as defined by the input schema\n# @state a Python dictionary object for keeping state over subsequent function calls\n# return must be a Python dictionary object. It will be the output of this operator.\n# Returning None results in not submitting an output tuple for this invocation.\n# You must declare all output attributes in the Edit Schema window.\ndef process(event, state):\n    # Enrich the event, such as by:\n    # event['wordCount'] = len(event['phrase'].split())\n    try:\n        event['text'] = event['text'].replace('\"', \"'\")\n response = state[\"nlu\"].analyze(\n text = event['text'],\n features=Features(sentiment=SentimentOptions(), entities=EntitiesOptions())\n )\n        event[\"sentiment\"] = response[\"sentiment\"][\"document\"][\"label\"]\n        top_entity = response[\"entities\"][0] if len(response[\"entities\"]) > 0 else None\n        event[\"entity\"] = top_entity[\"text\"] if top_entity is not None else \"\"\n        event[\"entity_type\"] = top_entity[\"type\"] if top_entity is not None else \"\"\n    except Exception as e:\n        return None\n return event\n\n```", "```py\ndef start_streaming_dataframe():\n    \"Start a Spark Streaming DataFrame from a Kafka Input source\"\n    schema = StructType(\n        [StructField(f[\"name\"], f[\"type\"], True) for f in field_metadata]\n    )\n kafka_options = {\n \"kafka.ssl.protocol\":\"TLSv1.2\",\n \"kafka.ssl.enabled.protocols\":\"TLSv1.2\",\n \"kafka.ssl.endpoint.identification.algorithm\":\"HTTPS\",\n 'kafka.sasl.mechanism': 'PLAIN',\n 'kafka.security.protocol': 'SASL_SSL'\n }\n    return spark.readStream \\\n        .format(\"kafka\") \\\n .option(\"kafka.bootstrap.servers\", \",\".join(message_hub_creds[\"kafka_brokers_sasl\"])) \\\n .option(\"subscribe\", \"enriched_tweets\") \\\n .load(**kafka_options)\n\n```", "```py\n--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0\n```", "```py\nKafkaClient {\n    org.apache.kafka.common.security.plain.PlainLoginModule required\n username=\"XXXX\"\n password=\"XXXX\";\n};\n```", "```py\n--driver-java-options=-Djava.security.auth.login.config=<<jaas.conf path>>\n```", "```py\n{\n \"language\": \"python\",\n \"env\": {\n  \"SCALA_HOME\": \"/Users/dtaieb/pixiedust/bin/scala/scala-2.11.8\",\n  \"PYTHONPATH\": \"/Users/dtaieb/pixiedust/bin/spark/spark-2.3.0-bin-hadoop2.7/python/:/Users/dtaieb/pixiedust/bin/spark/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip\",\n  \"SPARK_HOME\": \"/Users/dtaieb/pixiedust/bin/spark/spark-2.3.0-bin-hadoop2.7\",\n  \"PYSPARK_SUBMIT_ARGS\": \"--driver-java-options=-Djava.security.auth.login.config=/Users/dtaieb/pixiedust/jaas.conf --jars /Users/dtaieb/pixiedust/bin/cloudant-spark-v2.0.0-185.jar --driver-class-path /Users/dtaieb/pixiedust/data/libs/* --master local[10] --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0 pyspark-shell\",\n  \"PIXIEDUST_HOME\": \"/Users/dtaieb/pixiedust\",\n  \"SPARK_DRIVER_MEMORY\": \"10G\",\n  \"SPARK_LOCAL_IP\": \"127.0.0.1\",\n  \"PYTHONSTARTUP\": \"/Users/dtaieb/pixiedust/bin/spark/spark-2.3.0-bin-hadoop2.7/python/pyspark/shell.py\"\n },\n \"display_name\": \"Python with Pixiedust (Spark 2.3)\",\n \"argv\": [\n  \"python\",\n  \"-m\",\n  \"ipykernel\",\n  \"-f\",\n  \"{connection_file}\"\n ]\n}\n\n```"]