<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Advanced Machine Learning Best Practices</h1>
                
            
            
                
<p>"Hyperparameter optimization or model selection is the problem of choosing a set of hyperparameters [when defined as?] for a learning algorithm, usually with the goal of optimizing a measure of the algorithm's performance on an independent dataset."</p>
<p class="cdpalignright">- Machine learning model tuning quote</p>
<p class="mce-root">In this chapter, we will provide some theoretical and practical aspects of some advanced topics of machine learning (ML) with Spark. We will see how to tune machine learning models for better and optimized performance using grid search, cross-validation and hyperparameter tuning. In the later section, we will cover how to develop a scalable recommendation system using the ALS, which is an example of a model-based recommendation algorithm. Finally, a topic modeling application as a text clustering technique will be demonstrated.</p>
<p class="mce-root">In a nutshell, we will cover the following topics in this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Machine learning best practice</li>
<li class="mce-root1">Hyperparameter tuning of ML models</li>
<li class="mce-root1">Topic modeling using latent dirichlet allocation (LDA)</li>
<li class="mce-root1">A recommendation system using collaborative filtering</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Machine learning best practices</h1>
                
            
            
                
<p class="mce-root">Sometimes, it is recommended to consider the error rate rather than only the accuracy. For example, suppose an ML system with 99% accuracy and 50% errors is worse than the one with 90% accuracy but 25% errors. So far, we have discussed the following machine learning topics:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Regression</strong>: This is for predicting values that are linearly separable</li>
<li class="mce-root1"><strong class="calibre1">Anomaly detection</strong>: This is for finding unusual data points often done using a clustering algorithm</li>
<li class="mce-root1"><strong class="calibre1">Clustering</strong>: This is for discovering the hidden structure in the dataset for clustering homogeneous data points</li>
<li class="mce-root1"><strong class="calibre1">Binary classification</strong>: This is for predicting two categories</li>
<li class="mce-root1"><strong class="calibre1">Multi-class classification</strong>: This is for predicting three or more categories</li>
</ul>
<p class="mce-root">Well, we have also seen that there are some good algorithms for these tasks. However, choosing the right algorithm for your problem type is a tricky task for achieving higher and outstanding accuracy in your ML algorithms. For this, we need to adopt some good practices through the stages, that is, from data collection, feature engineering, model building, evaluating, tuning, and deployment. Considering these, in this section, we will provide some practical recommendation while developing your ML application using Spark.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Beware of overfitting and underfitting</h1>
                
            
            
                
<p class="mce-root">A straight line cutting across a curving scatter plot would be a good example of under-fitting, as we can see in the diagram here. However, if the line fits the data too well, there evolves an opposite problem called <strong class="calibre1">overfitting</strong>. When we say a model overfits a dataset, we mean it may have a low error rate for the training data, but it does not generalize well to the overall population in the data.</p>
<div><img class="image-border161" src="img/00148.jpeg"/></div>
<div><strong class="calibre1">Figure 1</strong>: Overfitting-underfitting trade-off (source: The book, "Deep Learning" by Adam Gibson, Josh Patterson)</div>
<p class="mce-root">More technically, if you evaluate your model on the training data instead of test or validated data, you probably won't be able to articulate whether your model is overfitting or not. The common symptoms are as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Predictive accuracy of the data used for training can be over accurate (that is, sometimes even 100%).</li>
<li class="mce-root1">The model might show better performance compared to the random prediction for new data.</li>
<li class="mce-root1">We like to fit a dataset to a distribution because if the dataset is reasonably close to the distribution, we can make assumptions based on the theoretical distribution of how we operate with the data. Consequently, the normal distribution in the data allows us to assume that sampling distributions of statistics are normally distributed under specified conditions. The normal distribution is defined by its mean and standard deviation and has generally the same shape across all variations.</li>
</ul>
<div><img class="image-border162" src="img/00012.jpeg"/></div>
<div><strong class="calibre1">Figure 2</strong>: Normal distribution in the data helps overcoming both the over-fitting and underfitting (source: The book, "Deep Learning" by Adam Gibson, Josh Patterson)</div>
<p class="mce-root">Sometimes, the ML model itself becomes underfit for a particular tuning or data point, which means the model become too simplistic. Our recommendation (like that of others, we believe) is as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Splitting the dataset into two sets to detect overfitting situations--the first one is for training and model selection called the training set, and the second one is the test set for evaluating the model started in place of the ML workflow section.</li>
<li class="mce-root1">Alternatively, you also could avoid the overfitting by consuming simpler models (for example, linear classifiers in preference to Gaussian kernel SVM) or by swelling the regularization parameters of your ML model (if available).</li>
<li class="mce-root1">Tune the model with a correct data value of parameters to avoid both the overfitting as well as underfitting.</li>
<li class="mce-root1">Thus, solving underfitting is the priority, but most of the machines learning practitioners suggest spending more time and effort attempting not to overfit the line to the data. Also, many machine learning practitioners, on the other hand, have recommended splitting the large-scale dataset into three sets: a training set (50%), validation set (25%), and test set (25%). They also suggested to build the model using the training set and to calculate the prediction errors using the validation set. The test set was recommended to be used to assess the generalization error of the final model. If the amount of labeled data available on the other hand during the supervised learning is smaller, it is not recommended to split the datasets. In that case, use cross validation. More specifically, divide the dataset into 10 parts of (roughly) equal size; after that, for each of these 10 parts, train the classifier iteratively and use the 10th part to test the model.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Stay tuned with Spark MLlib and Spark ML</h1>
                
            
            
                
<p class="mce-root">The first step of the pipeline designing is to create the building blocks (as a directed or undirected graph consisting of nodes and edges) and make a linking between those blocks. Nevertheless, as a data scientist, you should be focused on scaling and optimizing nodes (primitives) too so that you are able to scale up your application for handling large-scale datasets in the later stage to make your ML pipeline performing consistently. The pipeline process will also help you to make your model adaptive for new datasets. However, some of these primitives might be explicitly defined to particular domains and data types (for example, text, image, and video, audio, and spatiotemporal).</p>
<p class="mce-root">Beyond these types of data, the primitives should also be working for general purpose domain statistics or mathematics. The casting of your ML model in terms of these primitives will make your workflow more transparent, interpretable, accessible, and explainable.</p>
<p class="mce-root">A recent example would be the ML-matrix, which is a distributed matrix library that can be used on top of Spark. Refer to the JIRA issue at <a href="https://issues.apache.org/jira/browse/SPARK-3434" class="calibre10">https://issues.apache.org/jira/browse/SPARK-3434</a>.</p>
<div><img class="image-border163" src="img/00109.jpeg"/></div>
<div><strong class="calibre1">Figure 3:</strong> Stay tune and interoperate ML and MLlib</div>
<p class="mce-root">As we already stated in the previous section, as a developer, you can seamlessly combine the implementation techniques in Spark MLlib along with the algorithms developed in Spark ML, Spark SQL, GraphX, and Spark Streaming as hybrid or interoperable ML applications on top of RDD, DataFrame, and datasets as shown in <em class="calibre8">Figure 3</em>. Therefore, the recommendation here is to stay in tune or synchronized with the latest technologies around you for the betterment of your ML application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Choosing the right algorithm for your application</h1>
                
            
            
                
<p class="mce-root">"What machine learning algorithm should I use?" is a very frequently asked question for the naive machine learning practitioners but the answer is always <em class="calibre8">it depends</em>. More elaborately:</p>
<ul class="calibre9">
<li class="mce-root1">It depends on the volume, quality, complexity, and the nature of the data you have to be tested/used</li>
<li class="mce-root1">It depends on external environments and parameters like your computing system's configuration or underlying infrastructures</li>
<li class="mce-root1">It depends on what you want to do with the answer</li>
<li class="mce-root1">It depends on how the mathematical and statistical formulation of the algorithm was translated into machine instructions for the computer</li>
<li class="mce-root1">It depends on how much time do you have</li>
</ul>
<p class="mce-root">The reality is, even the most experienced data scientists or data engineers can't give a straight recommendation about which ML algorithm will perform the best before trying them all together. Most of the statements of agreement/disagreement begins with "It depends...hmm..." Habitually, you might wonder if there are cheat sheets of machine learning algorithms, and if so, how should you use that cheat sheet? Several data scientists have said that the only sure way to find the very best algorithm is to try all of them; therefore, there is no shortcut dude! Let's make it clearer; suppose you do have a set of data and you want to do some clustering. Technically, this could be either a classification or regression problem that you want o apply on your dataset if your data is labeled. However, if you have a unlabeled dataset, it is the clustering technique that you will be using. Now, the concerns that evolve in your mind are as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Which factors should I consider before choosing an appropriate algorithm? Or should I just choose an algorithm randomly?</li>
<li class="mce-root1">How do I choose any data pre-processing algorithm or tools that can be applied to my data?</li>
<li class="mce-root1">What sort of feature engineering techniques should I be using to extract the useful features?</li>
<li class="mce-root1">What factors can improve the performance of my ML model?</li>
<li class="mce-root1">How can I adopt my ML application for new data types?</li>
<li class="mce-root1">Can I scale up my ML application for large-scale datasets? And so on.</li>
</ul>
<p class="mce-root">In this section, we will try to answer these questions with our little machine learning knowledge.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Considerations when choosing an algorithm</h1>
                
            
            
                
<p class="mce-root">The recommendation or suggestions we provide here are for the novice data scientist who is just learning machine learning. These will use useful to expert the data scientists too, who is trying to choose an optimal algorithm to start with Spark ML APIs. Don't worry, we will guide you to the direction! We also recommend going with the following algorithmic properties when choosing an algorithm:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Accuracy</strong>: Whether getting the best score is the goal or an approximate solution (<em class="calibre8">good enough</em>) in terms of precision, recall, f1 score or AUC and so on, while trading off overfitting.</li>
<li class="mce-root1"><strong class="calibre1">Training time</strong>: The amount of time available to train the model (including the model building, evaluation, and tanning time).</li>
<li class="mce-root1"><strong class="calibre1">Linearity</strong>: An aspect of model complexity in terms of how the problem is modeled. Since most of the non-linear models are often more complex to understand and tune.</li>
<li class="mce-root1"><strong class="calibre1">Number of parameters</strong></li>
<li class="mce-root1"><strong class="calibre1">Number of features</strong>: The problem of having more attributes than instances, the <em class="calibre8">p&gt;&gt;n</em> problem. This often requires specialized handling or specialized techniques using dimensionality reduction or better feature engineering approach.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Accuracy</h1>
                
            
            
                
<p class="mce-root">Getting the most accurate results from your ML application isn't always indispensable. Depending on what you want to use it for, sometimes an approximation is adequate enough. If the situation is something like this, you may be able to reduce the processing time drastically by incorporating the better-estimated methods. When you become familiar of the workflow with the Spark machine learning APIs, you will enjoy the advantage of having more approximation methods, because these approximation methods will tend to avoid the overfitting problem of your ML model automatically. Now, suppose you have two binary classification algorithms that perform as follows:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Classifier</strong></p>
</td>
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Precision</strong></p>
</td>
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Recall</strong></p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">X</p>
</td>
<td class="calibre7">
<p class="mce-root">96%</p>
</td>
<td class="calibre7">
<p class="mce-root">89%</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">Y</p>
</td>
<td class="calibre7">
<p class="mce-root">99%</p>
</td>
<td class="calibre7">
<p class="mce-root">84%</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">Here, none of the classifiers is obviously superior, so it doesn't immediately guide you toward picking the optimal one. F1-score which is the harmonic mean of precision and recall helps you. Let's calculate it and place it in the table:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Classifier</strong></p>
</td>
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Precision</strong></p>
</td>
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Recall</strong></p>
</td>
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">F1 score</strong></p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">X</p>
</td>
<td class="calibre7">
<p class="mce-root">96%</p>
</td>
<td class="calibre7">
<p class="mce-root">89%</p>
</td>
<td class="calibre7">
<p class="mce-root">92.36%</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">Y</p>
</td>
<td class="calibre7">
<p class="mce-root">99%</p>
</td>
<td class="calibre7">
<p class="mce-root">84%</p>
</td>
<td class="calibre7">
<p class="mce-root">90.885%</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">Therefore, having an F1-score helps make a decision for selecting from a large number of classifiers. It gives a clear preference ranking among all of them, and therefore a clear direction for progress--that is classifier <strong class="calibre1">X</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Training time</h1>
                
            
            
                
<p class="mce-root">Training time is often closely related to the model training and the accuracy. In addition, often you will discover that some of the algorithms are elusive to the number of data points compared to others. However, when your time is inadequate but the training set is large with a lot of features, you can choose the simplest one. In this case, you might have to compromise with the accuracy. But it will fulfill your minimum requirements at least.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Linearity</h1>
                
            
            
                
<p class="mce-root">There are many machine learning algorithms developed recently that make use of linearity (also available in the Spark MLlib and Spark ML). For example, linear classification algorithms undertake that classes can be separated by plotting a differentiating straight line or using higher-dimensional equivalents. Linear regression algorithms, on the other hand, assume that data trends simply follow a straight line. This assumption is not naive for some machine learning problems; however, there might be some other cases where the accuracy will be down. Despite their hazards, linear algorithms are very popular with data engineers and data scientists as the first line of the outbreak. Moreover, these algorithms also tend to be simple and fast, to train your models during the whole process.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Inspect your data when choosing an algorithm</h1>
                
            
            
                
<p class="mce-root">You will find many machine learning datasets available at the UC Irvine Machine Learning Repository. The following data properties should also be prioritized:</p>
<ul class="calibre9">
<li class="mce-root1">Number of parameters</li>
<li class="mce-root1">Number of features</li>
<li class="mce-root1">Size of the training dataset</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Number of parameters</h1>
                
            
            
                
<p class="mce-root">Parameters or data properties are the handholds for a data scientist when setting up an algorithm. They are numbers that affect the algorithm's performance, such as error tolerance or a number of iterations, or options between variants of how the algorithm acts. The training time and accuracy of the algorithm can sometimes be quite sensitive making it difficult get the right settings. Typically, algorithms with a large number of parameters require more trial and error to find an optimal combination.</p>
<p class="mce-root">Despite the fact that this is a great way to span the parameter space, the model building or train time increases exponentially with the increased number of parameters. This is a dilemma as well as a time-performance trade-off. The positive sides are:</p>
<ul class="calibre9">
<li class="mce-root1">Having many parameters characteristically indicates the greater flexibility of the ML algorithms</li>
<li class="mce-root1">Your ML application achieves much better accuracy</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">How large is your training set?</h1>
                
            
            
                
<p class="mce-root">If your training set is smaller, high bias with low variance classifiers, such as Naive Bayes have an advantage over low bias with high variance classifiers (also can be used for regression) such as the <strong class="calibre1">k-nearest neighbors algorithm</strong> (<strong class="calibre1">kNN</strong>).</p>
<div><strong class="calibre27">Bias, Variance, and the kNN model:</strong> In reality, <em class="calibre25">increasing k</em> will <em class="calibre25">decrease the variance,</em> but <em class="calibre25">increase the bias</em>. On the other hand, <em class="calibre25">decreasing k</em> will <em class="calibre25">increase variance</em> and <em class="calibre25">decrease bias</em>. As <em class="calibre25">k</em> increases, this variability is reduced. But if we increase <em class="calibre25">k</em> too much, then we no longer follow the true boundary line and we observe high bias. This is the nature of the Bias-Variance Trade-off.</div>
<p class="mce-root">We have seen the over and underfitting issue already. Now, you can assume that dealing with bias and variance is like dealing with over- and underfitting. Bias is reduced and variance is increased in relation to model complexity. As more and more parameters are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls. In other words, bias has a negative first-order derivative in response to model complexity, while variance has a positive slope. Refer to the following figure for a better understanding:</p>
<div><img class="image-border164" src="img/00077.jpeg"/></div>
<div><strong class="calibre1">Figure 4:</strong> Bias and variance contributing to total error</div>
<p class="mce-root">Therefore, the latter will overfit. But low bias with high variance classifiers, on the other hand, starts to win out as your training set grows linearly or exponentially, since they have lower asymptotic errors. High bias classifiers aren't powerful enough to provide accurate models.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Number of features</h1>
                
            
            
                
<p class="mce-root">For certain types of experimental dataset, the number of extracted features can be very large compared to the number of data points itself. This is often the case with genomics, biomedical, or textual data. A large number of features can swamp down some learning algorithms, making training time ridiculously higher. <strong class="calibre1">Support Vector Machines</strong> (<strong class="calibre1">SVMs</strong>) are particularly well suited in this case for its high accuracy, nice theoretical guarantees regarding overfitting, and with an appropriate kernel.</p>
<div><strong class="calibre27">The SVM and the Kernel</strong>: The task is to find a set of weight and bias such that the margin can maximize the function:<br class="calibre23"/>y = w*¥(x) +b,<br class="calibre23"/>Where <em class="calibre25">w</em> is the weight, <em class="calibre25">¥</em> is the feature vector, and <em class="calibre25">b</em> is the bias. Now if <em class="calibre25">y&gt; 0</em>, then we classify datum to class <em class="calibre25">1</em>, else to class <em class="calibre25">0</em>, whereas, the feature vector <em class="calibre25">¥(x)</em> makes the data linearly separable. However, using the kernel makes the calculation process faster and easier, especially when the feature vector <em class="calibre25">¥</em> consisting of very high dimensional data. Let's see a concrete example. Suppose we have the following value of <em class="calibre25">x</em> and <em class="calibre25">y</em>: <em class="calibre25">x = (x1, x2, x3)</em> and <em class="calibre25">y = (y1, y2, y3)</em>, then for the function <em class="calibre25">f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1, x3x2, x3x3)</em>, the kernel is <em class="calibre25">K(x, y ) = (&lt;x, y&gt;)<sup class="calibre45">2</sup></em>. Following the above, if <em class="calibre25">x</em> <em class="calibre25">= (1, 2, 3)</em> and <em class="calibre25">y = (4, 5, 6)</em>, then we have the following values:<br class="calibre23"/>
f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)<br class="calibre23"/>
f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36)<br class="calibre23"/>
&lt;f(x), f(y)&gt; = 16 + 40 + 72 + 40 + 100+ 180 + 72 + 180 + 324 = 1024<br class="calibre23"/>This is a simple linear algebra that maps a 3-dimensional space to a 9 dimensional. On the other hand, the kernel is a similarity measure used for SVMs. Therefore, choosing an appropriate kernel value based on the prior knowledge of invariances is suggested. The choice of the kernel and kernel and regularization parameters can be automated by optimizing a cross-validation based model selection.<br class="calibre23"/>
Nevertheless, an automated choice of kernels and kernel parameters is a tricky issue, as it is very easy to overfit the model selection criterion. This might result in a worse model than you started with. Now, if we use the kernel function <em class="calibre25">K(x, y), this gives the same value but with much simpler calculation -i.e. (4 + 10 + 18) ^2 = 32^2 = 1024</em>.</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Hyperparameter tuning of ML models</h1>
                
            
            
                
<p class="mce-root">Tuning an algorithm is simply a process that one goes through in order to enable the algorithm to perform optimally in terms of runtime and memory usage. In Bayesian statistics, a hyperparameter is a parameter of a prior distribution. In terms of machine learning, the term hyperparameter refers to those parameters that cannot be directly learned from the regular training process. Hyperparameters are usually fixed before the actual training process begins. This is done by setting different values for those hyperparameters, training different models, and deciding which ones work best by testing them. Here are some typical examples of such parameters:</p>
<ul class="calibre9">
<li class="mce-root1">Number of leaves, bins, or depth of a tree</li>
<li class="mce-root1">Number of iterations</li>
<li class="mce-root1">Number of latent factors in a matrix factorization</li>
<li class="mce-root1">Learning rate</li>
<li class="mce-root1">Number of hidden layers in a deep neural network</li>
<li class="mce-root1">Number of clusters in a k-means clustering and so on.</li>
</ul>
<p class="mce-root">In this section, we will discuss how to perform hyperparameter tuning using the cross-validation technique and grid searching.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Hyperparameter tuning</h1>
                
            
            
                
<p class="mce-root">Hyperparameter tuning is a technique for choosing the right combination of hyperparameters based on the performance of presented data. It is one of the fundamental requirements to obtain meaningful and accurate results from machine learning algorithms in practice. The following figure shows the model tuning process, consideration, and workflow:</p>
<div><img class="image-border165" src="img/00343.jpeg"/></div>
<div><strong class="calibre1">Figure 5</strong>: The model tuning process, consideration, and workflow</div>
<p class="mce-root">For example, suppose we have two hyperparameters to tune for a pipeline presented in <em class="calibre8">Figure 17</em> from <a href="part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 11</a>, <em class="calibre8">Learning Machine Learning - Spark MLlib and Spark ML</em>, a Spark ML pipeline model using a logistic regression estimator (dash lines only happen during pipeline fitting). We can see that we have put three candidate values for each. Therefore, there would be nine combinations in total. However, only four are shown in the diagram, namely, Tokenizer, HashingTF, Transformer, and Logistic Regression (LR). Now, we want to find the one that will eventually lead to the model with the best evaluation result. The fitted model consists of the Tokenizer, the HashingTF feature extractor, and the fitted logistic regression model:</p>
<p class="mce-root">If you recall <em class="calibre8">Figure 17</em> from <a href="part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 11</a>, <em class="calibre8">Learning Machine Learning - Spark MLlib and Spark ML</em>, the dashed line, however, happens only during the pipeline fitting. As mentioned earlier, the fitted pipeline model is a Transformer. The Transformer can be used for prediction, model validation, and model inspection. In addition, we also argued that one ill-fated distinguishing characteristic of the ML algorithms is that typically they have many hyperparameters that need to be tuned for better performance. For example, the degree of regularizations in these hyperparameters is distinctive from the model parameters optimized by the Spark MLlib.</p>
<p class="mce-root">As a consequence, it is really hard to guess or measure the best combination of hyperparameters without expert knowledge of the data and the algorithm to use. Since the complex dataset is based on the ML problem type, the size of the pipeline and the number of hyperparameters may grow exponentially (or linearly); the hyperparameter tuning becomes cumbersome even for an ML expert, not to mention that the result of the tuning parameters may become unreliable.</p>
<p class="mce-root">According to Spark API documentation, a unique and uniform API is used for specifying Spark ML estimators and Transformers. A <kbd class="calibre11">ParamMap</kbd> is a set of (parameter, value) pairs with a Param as a named parameter with self-contained documentation provided by Spark. Technically, there are two ways for passing the parameters to an algorithm as specified in the following options:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Setting parameters</strong>: If an LR is an instance of Logistic Regression (that is, Estimator), you can call the <kbd class="calibre11">setMaxIter()</kbd> method as follows: <kbd class="calibre11">LR.setMaxIter(5)</kbd>. It essentially fits the model pointing the regression instance as follows: <kbd class="calibre11">LR.fit()</kbd>. In this particular example, there would be at most five iterations.</li>
<li class="mce-root1"><strong class="calibre1">The second option</strong>: This one involves passing a <kbd class="calibre11">ParamMaps</kbd> to <kbd class="calibre11">fit()</kbd> or <kbd class="calibre11">transform()</kbd> (refer <em class="calibre8">Figure 5</em> for details). In this circumstance, any parameters will be overridden by the <kbd class="calibre11">ParamMaps</kbd> previously specified via setter methods in the ML application-specific codes or algorithms.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Grid search parameter tuning</h1>
                
            
            
                
<p class="mce-root">Suppose you selected your hyperparameters after necessary feature engineering. In this regard, a full grid search of the space of hyperparameters and features is computationally too intensive. Therefore, you need to perform a fold of the K-fold cross-validation instead of a full-grid search:</p>
<ul class="calibre9">
<li class="mce-root1">Tune the required hyperparameters using cross validation on the training set of the fold, using all the available features</li>
<li class="mce-root1">Select the required features using those hyperparameters</li>
<li class="mce-root1">Repeat the computation for each fold in K</li>
<li class="mce-root1">The final model is constructed on all the data using the N most prevalent features that were selected from each fold of CV</li>
</ul>
<p class="mce-root">The interesting thing is that the hyperparameters would also be tuned again using all the data in a cross-validation loop. Would there be a large downside from this method as compared to a full-grid search? In essence, I am doing a line search in each dimension of free parameters (finding the best value in one dimension, holding that constant, then finding the best in the next dimension), rather than every single combination of parameter settings. The most important downside for searching along single parameters instead of optimizing them all together, is that you ignore interactions.</p>
<p class="mce-root">It is quite common that, for instance, more than one parameter influences model complexity. In that case, you need to look at their interaction in order to successfully optimize the hyperparameters. Depending on how large your dataset is and how many models you compare, optimization strategies that return the maximum observed performance may run into trouble (this is true for both grid search and your strategy).</p>
<p class="mce-root">The reason is that searching through a large number of performance estimates for the maximum skims the variance of the performance estimate: you may just end up with a model and training/test split combination that accidentally happens to look good. Even worse, you may get several perfect-looking combinations, and the optimization then cannot know which model to choose and thus becomes unstable.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Cross-validation</h1>
                
            
            
                
<p class="mce-root">Cross-validation (also called the <strong class="calibre1">rotation estimation</strong> (<strong class="calibre1">RE</strong>)) is a model validation technique for assessing the quality of the statistical analysis and results. The target is to make the model generalize toward an independent test set. One of the perfect uses of the cross-validation technique is making a prediction from a machine learning model. It will help if you want to estimate how a predictive model will perform accurately in practice when you deploy it as an ML application. During the cross-validation process, a model is usually trained with a dataset of a known type. Conversely, it is tested using a dataset of unknown type.</p>
<p class="mce-root">In this regard, cross-validation helps to describe a dataset to test the model in the training phase using the validation set. There are two types of cross-validation that can be typed as follows:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Exhaustive cross-validation</strong>: This includes leave-p-out cross-validation and leave-one-out cross-validation.</li>
<li class="mce-root1"><strong class="calibre1">Non-exhaustive cross-validation</strong>: This includes K-fold cross-validation and repeated random sub-sampling cross-validation.</li>
</ul>
<p class="mce-root">In most of the cases, the researcher/data scientist/data engineer uses 10-fold cross-validation instead of testing on a validation set. This is the most widely used cross-validation technique across the use cases and problem type as explained by the following figure:</p>
<div><img class="image-border166" src="img/00372.gif"/></div>
<div><strong class="calibre1">Figure 6:</strong> Cross-validation basically splits your complete available training data into a number of folds. This parameter can be specified. Then the whole pipeline is run once for every fold and one machine learning model is trained for each fold. Finally, the different machine learning models obtained are joined by a voting scheme for classifiers or by averaging for regression</div>
<p class="mce-root">Moreover, to reduce the variability, multiple iterations of cross-validation are performed using different partitions; finally, the validation results are averaged over the rounds. The following figure shows an example of hyperparameter tuning using the logistic regression:</p>
<div><img class="image-border167" src="img/00046.jpeg"/></div>
<div><strong class="calibre1">Figure 7:</strong> An example of hyperparameter tuning using the logistic regression</div>
<p class="mce-root">Using cross-validation instead of conventional validation has two main advantages outlined as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Firstly, if there is not enough data available to partition across the separate training and test sets, there's the chance of losing significant modeling or testing capability.</li>
<li class="mce-root1">Secondly, the K-fold cross-validation estimator has a lower variance than a single hold-out set estimator. This low variance limits the variability and is again very important if the amount of available data is limited.</li>
</ul>
<p class="mce-root">In these circumstances, a fair way to properly estimate the model prediction and related performance are to use cross-validation as a powerful general technique for model selection and validation. If we need to perform manual features and a parameter selection for the model tuning, after that, we can perform a model evaluation with a 10-fold cross-validation on the entire dataset. What would be the best strategy? We would suggest you go for the strategy that provides an optimistic score as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Divide the dataset into training, say 80%, and testing 20% or whatever you chose</li>
<li class="mce-root1">Use the K-fold cross-validation on the training set to tune your model</li>
<li class="mce-root1">Repeat the CV until you find your model optimized and therefore tuned.</li>
</ul>
<p class="mce-root">Now, use your model to predict on the testing set to get an estimate of out of model errors.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Credit risk analysis – An example of hyperparameter tuning</h1>
                
            
            
                
<p class="mce-root">In this section, we will show a practical example of machine learning hyperparameter tuning in terms of grid searching and cross-validation technique. More specifically, at first, we will develop a credit risk pipeline that is commonly used in financial institutions such as banks and credit unions. Later on, we will look at how to improve the prediction accuracy by hyperparameter tuning. Before diving into the example, let's take a quick overview of what credit risk analysis is and why it is important?</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">What is credit risk analysis? Why is it important?</h1>
                
            
            
                
<p class="mce-root">When an applicant applies for loans and a bank receives that application, based on the applicant's profile, the bank has to make a decision whether to approve the loan application or not. In this regard, there are two types of risk associated with the bank's decision on the loan application:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">The applicant is a good credit risk</strong>: That means the client or applicant is more likely to repay the loan. Then, if the loan is not approved, the bank can potentially suffer the loss of business.</li>
<li class="mce-root1"><strong class="calibre1">The applicant is a bad credit risk</strong>: That means that the client or applicant is most likely not to repay the loan. In that case, approving the loan to the client will result in financial loss to the bank.</li>
</ul>
<p class="mce-root">The institution says that the second one is riskier than that of the first one, as the bank has a higher chance of not getting reimbursed the borrowed amount. Therefore, most banks or credit unions evaluate the risks associated with lending money to a client, applicant, or customer. In business analytics, minimizing the risk tends to maximize the profit to the bank itself.</p>
<p class="mce-root">In other words, maximizing the profit and minimizing the loss from a financial perspective is important. Often, the bank makes a decision about approving a loan application based on different factors and parameters of an applicant, such as demographic and socio-economic conditions regarding their loan application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">The dataset exploration</h1>
                
            
            
                
<p class="mce-root">The German credit dataset was downloaded from the UCI Machine Learning Repository at <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/" class="calibre10">https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/</a>. Although a detailed description of the dataset is available in the link, we provide some brief insights here in <strong class="calibre1">Table 3</strong>. The data contains credit-related data on 21 variables and the classification of whether an applicant is considered a good or a bad credit risk for 1000 loan applicants (that is, binary classification problem).</p>
<p class="mce-root">The following table shows details about each variable that was considered before making the dataset available online:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Entry</strong></p>
</td>
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Variable</strong></p>
</td>
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Explanation</strong></p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">1</p>
</td>
<td class="calibre7">
<p class="mce-root">creditability</p>
</td>
<td class="calibre7">
<p class="mce-root">Capable of repaying: has value either 1.0 or 0.0</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">2</p>
</td>
<td class="calibre7">
<p class="mce-root">balance</p>
</td>
<td class="calibre7">
<p class="mce-root">Current balance</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">3</p>
</td>
<td class="calibre7">
<p class="mce-root">duration</p>
</td>
<td class="calibre7">
<p class="mce-root">Duration of the loan being applied for</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">4</p>
</td>
<td class="calibre7">
<p class="mce-root">history</p>
</td>
<td class="calibre7">
<p class="mce-root">Is there any bad loan history?</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">5</p>
</td>
<td class="calibre7">
<p class="mce-root">purpose</p>
</td>
<td class="calibre7">
<p class="mce-root">Purpose of the loan</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">6</p>
</td>
<td class="calibre7">
<p class="mce-root">amount</p>
</td>
<td class="calibre7">
<p class="mce-root">Amount being applied for</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">7</p>
</td>
<td class="calibre7">
<p class="mce-root">savings</p>
</td>
<td class="calibre7">
<p class="mce-root">Monthly saving</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">8</p>
</td>
<td class="calibre7">
<p class="mce-root">employment</p>
</td>
<td class="calibre7">
<p class="mce-root">Employment status</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">9</p>
</td>
<td class="calibre7">
<p class="mce-root">instPercent</p>
</td>
<td class="calibre7">
<p class="mce-root">Interest percent</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">10</p>
</td>
<td class="calibre7">
<p class="mce-root">sexMarried</p>
</td>
<td class="calibre7">
<p class="mce-root">Sex and marriage status</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">11</p>
</td>
<td class="calibre7">
<p class="mce-root">guarantors</p>
</td>
<td class="calibre7">
<p class="mce-root">Are there any guarantors?</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">12</p>
</td>
<td class="calibre7">
<p class="mce-root">residenceDuration</p>
</td>
<td class="calibre7">
<p class="mce-root">Duration of residence at the current address</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">13</p>
</td>
<td class="calibre7">
<p class="mce-root">assets</p>
</td>
<td class="calibre7">
<p class="mce-root">Net assets</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">14</p>
</td>
<td class="calibre7">
<p class="mce-root">age</p>
</td>
<td class="calibre7">
<p class="mce-root">Age of the applicant</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">15</p>
</td>
<td class="calibre7">
<p class="mce-root">concCredit</p>
</td>
<td class="calibre7">
<p class="mce-root">Concurrent credit</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">16</p>
</td>
<td class="calibre7">
<p class="mce-root">apartment</p>
</td>
<td class="calibre7">
<p class="mce-root">Residential status</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">17</p>
</td>
<td class="calibre7">
<p class="mce-root">credits</p>
</td>
<td class="calibre7">
<p class="mce-root">Current credits</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">18</p>
</td>
<td class="calibre7">
<p class="mce-root">occupation</p>
</td>
<td class="calibre7">
<p class="mce-root">Occupation</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">19</p>
</td>
<td class="calibre7">
<p class="mce-root">dependents</p>
</td>
<td class="calibre7">
<p class="mce-root">Number of dependents</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">20</p>
</td>
<td class="calibre7">
<p class="mce-root">hasPhone</p>
</td>
<td class="calibre7">
<p class="mce-root">If the applicant uses a phone</p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">21</p>
</td>
<td class="calibre7">
<p class="mce-root">foreign</p>
</td>
<td class="calibre7">
<p class="mce-root">If the applicant is a foreigner</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">Note that, although <em class="calibre8">Table 3</em> describes the variables with an associated header, there is no associated header in the dataset. In <em class="calibre8">Table 3</em>, we have shown the variable, position, and associated significance of each variable.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step-by-step example with Spark ML</h1>
                
            
            
                
<p class="mce-root">Here, we will provide a step-by-step example of credit risk prediction using the Random Forest classifier. The steps include from data ingestion, some statistical analysis, training set preparation, and finally model evaluation:</p>
<p class="mce-root"><strong class="calibre1">Step 1.</strong> Load and parse the dataset into RDD:</p>
<pre class="calibre19">
val creditRDD = parseRDD(sc.textFile("data/germancredit.csv")).map(parseCredit) 
</pre>
<p class="mce-root">For the preceding line, the <kbd class="calibre11">parseRDD()</kbd> method is used to split the entry with <kbd class="calibre11">,</kbd> and then converted all of them as <kbd class="calibre11">Double</kbd> value (that is, numeric). This method goes as follows:</p>
<pre class="calibre19">
def parseRDD(rdd: RDD[String]): RDD[Array[Double]] = { <br class="title-page-name"/>rdd.map(_.split(",")).map(_.map(_.toDouble)) <br class="title-page-name"/>  } 
</pre>
<p class="mce-root">On the other hand, the <kbd class="calibre11">parseCredit()</kbd> method is used to parse the dataset based on the <kbd class="calibre11">Credit</kbd> case class:</p>
<pre class="calibre19">
def parseCredit(line: Array[Double]): Credit = { <br class="title-page-name"/>Credit( <br class="title-page-name"/>line(0), line(1) - 1, line(2), line(3), line(4), line(5), <br class="title-page-name"/>line(6) - 1, line(7) - 1, line(8), line(9) - 1, line(10) - 1, <br class="title-page-name"/>line(11) - 1, line(12) - 1, line(13), line(14) - 1, line(15) - 1, <br class="title-page-name"/>line(16) - 1, line(17) - 1, line(18) - 1, line(19) - 1, line(20) - 1) <br class="title-page-name"/>  } 
</pre>
<p class="mce-root">The <kbd class="calibre11">Credit</kbd> case class goes as follows:</p>
<pre class="calibre19">
case class Credit( <br class="title-page-name"/>creditability: Double, <br class="title-page-name"/>balance: Double, duration: Double, history: Double, purpose: Double, amount: Double, <br class="title-page-name"/>savings: Double, employment: Double, instPercent: Double, sexMarried: Double, guarantors: Double, <br class="title-page-name"/>residenceDuration: Double, assets: Double, age: Double, concCredit: Double, apartment: Double, <br class="title-page-name"/>credits: Double, occupation: Double, dependents: Double, hasPhone: Double, foreign: Double) 
</pre>
<p class="mce-root"><strong class="calibre1">Step 2. Prepare the DataFrame for the ML pipeline</strong> - Get the DataFrame for the ML pipeline</p>
<pre class="calibre19">
val sqlContext = new SQLContext(sc) <br class="title-page-name"/>import sqlContext._ <br class="title-page-name"/>import sqlContext.implicits._ <br class="title-page-name"/>val creditDF = creditRDD.toDF().cache() 
</pre>
<p class="mce-root">Save them as a temporary view for making the query easier:</p>
<pre class="calibre19">
creditDF.createOrReplaceTempView("credit") 
</pre>
<p class="mce-root">Let's take a snap of the DataFrame:</p>
<pre class="calibre19">
creditDF.show
</pre>
<p class="mce-root">The preceding <kbd class="calibre11">show()</kbd> method prints the credit DataFrame:</p>
<div><img class="image-border31" src="img/00124.gif"/></div>
<div><strong class="calibre1">Figure 8:</strong> A snap of the credit dataset</div>
<p class="mce-root"><strong class="calibre1">Step 3. Observing related statistics</strong> - First, let's see some aggregate values:</p>
<pre class="calibre19">
sqlContext.sql("SELECT creditability, avg(balance) as avgbalance, avg(amount) as avgamt, avg(duration) as avgdur  FROM credit GROUP BY creditability ").show 
</pre>
<p class="mce-root">Let's see the statistics about the balance:</p>
<pre class="calibre19">
creditDF.describe("balance").show 
</pre>
<p class="mce-root">Now, let's see the creditability per average balance:</p>
<pre class="calibre19">
creditDF.groupBy("creditability").avg("balance").show 
</pre>
<p class="mce-root">The output of the three lines:</p>
<div><img class="image-border168" src="img/00030.gif"/></div>
<div><strong class="calibre1">Figure 9:</strong> Some statistics of the dataset</div>
<p class="mce-root"><strong class="calibre1">Step 4. Feature vectors and labels creation</strong> - As you can see, the credibility column is the response column, and, for the result, we need to create the feature vector without considering this column. Now, let's create the feature column as follows:</p>
<pre class="calibre19">
val featureCols = Array("balance", "duration", "history", "purpose", "amount", "savings", "employment", "instPercent", "sexMarried",<br class="title-page-name"/>"guarantors", "residenceDuration", "assets", "age", "concCredit",<br class="title-page-name"/>"apartment", "credits", "occupation", "dependents", "hasPhone",<br class="title-page-name"/>"foreign") 
</pre>
<p class="mce-root">Let's assemble all the features of these selected columns using <kbd class="calibre11">VectorAssembler()</kbd> API as follows:</p>
<pre class="calibre19">
val assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol("features") <br class="title-page-name"/>val df2 = assembler.transform(creditDF) 
</pre>
<p class="mce-root">Now let's see what the feature vectors look like:</p>
<pre class="calibre19">
df2.select("features").show
</pre>
<p class="mce-root">The preceding line shows the features created by the VectorAssembler transformer:</p>
<div><img class="image-border169" src="img/00360.gif"/></div>
<div><strong class="calibre1">Figure 10:</strong> Generating features for ML models using VectorAssembler</div>
<p class="mce-root">Now, let's create a new column as a label from the old response column creditability using <kbd class="calibre11">StringIndexer</kbd> as follows:</p>
<pre class="calibre19">
val labelIndexer = new StringIndexer().setInputCol("creditability").setOutputCol("label") <br class="title-page-name"/>val df3 = labelIndexer.fit(df2).transform(df2) <br class="title-page-name"/>df3.select("label", "features").show
</pre>
<p class="mce-root">The preceding line shows the features and labels created by the <kbd class="calibre11">VectorAssembler</kbd> transformer:</p>
<div><img class="image-border170" src="img/00274.gif"/></div>
<div><strong class="calibre1">Figure 11:</strong> Corresponding labels and feature for ML models using VectorAssembler</div>
<p class="mce-root"><strong class="calibre1">Step 5.</strong> Prepare the training and test set:</p>
<pre class="calibre19">
val splitSeed = 5043 <br class="title-page-name"/>val Array(trainingData, testData) = df3.randomSplit(Array(0.80, 0.20), splitSeed) 
</pre>
<p class="mce-root"><strong class="calibre1">Step 6. Train the random forest model</strong> - At first, instantiate the model:</p>
<pre class="calibre19">
val classifier = new RandomForestClassifier() <br class="title-page-name"/>      .setImpurity("gini") <br class="title-page-name"/>      .setMaxDepth(30) <br class="title-page-name"/>      .setNumTrees(30) <br class="title-page-name"/>      .setFeatureSubsetStrategy("auto") <br class="title-page-name"/>      .setSeed(1234567) <br class="title-page-name"/>      .setMaxBins(40) <br class="title-page-name"/>      .setMinInfoGain(0.001) 
</pre>
<p class="mce-root">For an explanation of the preceding parameters, refer to the random forest algorithm section in this chapter. Now, let's train the model using the training set:</p>
<pre class="calibre19">
val model = classifier.fit(trainingData)
</pre>
<p class="mce-root"><strong class="calibre1">Step 7.</strong> Compute the raw prediction for the test set:</p>
<pre class="calibre19">
val predictions = model.transform(testData) 
</pre>
<p class="mce-root">Let's see the top 20 rows of this DataFrame:</p>
<pre class="calibre19">
predictions.select("label","rawPrediction", "probability", "prediction").show()
</pre>
<p class="mce-root">The preceding line shows the DataFrame containing the label, raw prediction, probablity, and actual prediciton:</p>
<div><img class="image-border171" src="img/00040.gif"/></div>
<div><strong class="calibre1">Figure 12:</strong> The DataFrame containing raw and actual prediction for test set</div>
<p class="mce-root">Now after seeing the prediction from the last column, a bank can make a decision about the applications for which the application should be accepted.</p>
<p class="mce-root"><strong class="calibre1">Step 8. Model evaluation before tuning</strong> - Instantiate the binary evaluator:</p>
<pre class="calibre19">
val binaryClassificationEvaluator = new BinaryClassificationEvaluator() <br class="title-page-name"/>      .setLabelCol("label") <br class="title-page-name"/>      .setRawPredictionCol("rawPrediction") 
</pre>
<p class="mce-root">Calculate the accuracy of the prediction for the test set as follows:</p>
<pre class="calibre19">
val accuracy = binaryClassificationEvaluator.evaluate(predictions) <br class="title-page-name"/>println("The accuracy before pipeline fitting: " + accuracy) 
</pre>
<p class="mce-root">The accuracy before pipeline fitting: <kbd class="calibre11">0.751921784149243</kbd></p>
<p class="mce-root">This time, the accuracy is 75%, which is not that good. Let's compute other important performance metrics for the binary classifier like <strong class="calibre1">area under receiver operating characteristic</strong> (<strong class="calibre1">AUROC</strong>) and <strong class="calibre1">area under precision recall curve</strong> (<strong class="calibre1">AUPRC</strong>):</p>
<pre class="calibre19">
println("Area Under ROC before tuning: " + printlnMetric("areaUnderROC"))         <br class="title-page-name"/>println("Area Under PRC before tuning: "+  printlnMetric("areaUnderPR")) <br class="title-page-name"/><strong class="calibre1"><strong class="calibre1"><br class="title-page-name"/></strong>Area Under ROC before tuning: 0.8453079178885631<br class="title-page-name"/></strong><strong class="calibre1">Area Under PRC before tuning: 0.751921784149243</strong>
</pre>
<p class="mce-root">The <kbd class="calibre11">printlnMetric()</kbd> method goes as follows:</p>
<pre class="calibre19">
def printlnMetric(metricName: String): Double = { <br class="title-page-name"/>  val metrics = binaryClassificationEvaluator.setMetricName(metricName)<br class="title-page-name"/>                                             .evaluate(predictions) <br class="title-page-name"/>  metrics <br class="title-page-name"/>} 
</pre>
<p class="mce-root">Finally, let's compute a few more performance metrics using the <kbd class="calibre11">RegressionMetrics ()</kbd> API for the random forest model we used during training:</p>
<pre class="calibre19">
val rm = new RegressionMetrics( <br class="title-page-name"/>predictions.select("prediction", "label").rdd.map(x =&gt; <br class="title-page-name"/>        (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))) <br class="title-page-name"/> 
</pre>
<p class="mce-root">Now, let's see how our model is:</p>
<pre class="calibre19">
println("MSE: " + rm.meanSquaredError) <br class="title-page-name"/>println("MAE: " + rm.meanAbsoluteError) <br class="title-page-name"/>println("RMSE Squared: " + rm.rootMeanSquaredError) <br class="title-page-name"/>println("R Squared: " + rm.r2) <br class="title-page-name"/>println("Explained Variance: " + rm.explainedVariance + "\n") 
</pre>
<p class="mce-root">We get the following output:</p>
<pre class="calibre19">
<strong class="calibre1">MSE: 0.2578947368421053</strong><br class="title-page-name"/><strong class="calibre1">MAE: 0.2578947368421053</strong><br class="title-page-name"/><strong class="calibre1">RMSE Squared: 0.5078333750770082</strong><br class="title-page-name"/><strong class="calibre1">R Squared: -0.13758553274682295</strong><br class="title-page-name"/><strong class="calibre1">Explained Variance: 0.16083102493074794</strong>
</pre>
<p class="mce-root">Not that bad! However, not satisfactory either, right? Let's tune the model using grid search and cross-validation techniques.</p>
<p class="mce-root"><strong class="calibre1">Step 9. Model tuning using grid search and cross-validation</strong> - First, let's use the <kbd class="calibre11">ParamGridBuilder</kbd> API to construct a grid of parameters to search over the param grid consisting of 20 to 70 trees with <kbd class="calibre11">maxBins</kbd> between 25 and 30, <kbd class="calibre11">maxDepth</kbd> between 5 and 10, and impurity as entropy and gini:</p>
<pre class="calibre19">
val paramGrid = new ParamGridBuilder()<br class="title-page-name"/>                    .addGrid(classifier.maxBins, Array(25, 30))<br class="title-page-name"/>                    .addGrid(classifier.maxDepth, Array(5, 10))<br class="title-page-name"/>                    .addGrid(classifier.numTrees, Array(20, 70))<br class="title-page-name"/>                    .addGrid(classifier.impurity, Array("entropy", "gini"))<br class="title-page-name"/>                    .build()
</pre>
<p class="mce-root">Let's train the cross-validator model using the training set as follows:</p>
<pre class="calibre19">
val cv = new CrossValidator()<br class="title-page-name"/>             .setEstimator(pipeline)<br class="title-page-name"/>             .setEvaluator(binaryClassificationEvaluator)<br class="title-page-name"/>             .setEstimatorParamMaps(paramGrid)<br class="title-page-name"/>             .setNumFolds(10)<br class="title-page-name"/>val pipelineFittedModel = cv.fit(trainingData)
</pre>
<p class="mce-root">Compute the raw prediction for the test set as follows:</p>
<pre class="calibre19">
val predictions2 = pipelineFittedModel.transform(testData) 
</pre>
<p class="mce-root"><strong class="calibre1">Step 10. Evaluation of the model after tuning</strong> - Let's see the accuracy:</p>
<pre class="calibre19">
val accuracy2 = binaryClassificationEvaluator.evaluate(predictions2)<br class="title-page-name"/>println("The accuracy after pipeline fitting: " + accuracy2)
</pre>
<p class="mce-root">We get the following output:</p>
<pre class="calibre19">
<strong class="calibre1">The accuracy after pipeline fitting: 0.8313782991202348</strong>
</pre>
<p class="mce-root">Now, it's above 83%. Good improvement indeed! Let's see the two other metrics computing AUROC and AUPRC:</p>
<pre class="calibre19">
def printlnMetricAfter(metricName: String): Double = { <br class="title-page-name"/>val metrics = binaryClassificationEvaluator.setMetricName(metricName).evaluate(predictions2) <br class="title-page-name"/>metrics <br class="title-page-name"/>    } <br class="title-page-name"/>println("Area Under ROC after tuning: " + printlnMetricAfter("areaUnderROC"))     <br class="title-page-name"/>println("Area Under PRC after tuning: "+  printlnMetricAfter("areaUnderPR"))
</pre>
<p class="mce-root">We get the following output:</p>
<pre class="calibre19">
<strong class="calibre1">Area Under ROC after tuning: 0.8313782991202345<br class="title-page-name"/> Area Under PRC after tuning: 0.7460301367852662</strong>
</pre>
<p class="mce-root">Now based on the <kbd class="calibre11">RegressionMetrics</kbd> API, compute the other metrics:</p>
<pre class="calibre19">
val rm2 = new RegressionMetrics(predictions2.select("prediction", "label").rdd.map(x =&gt; (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))) <br class="title-page-name"/> println("MSE: " + rm2.meanSquaredError) <br class="title-page-name"/>println("MAE: " + rm2.meanAbsoluteError) <br class="title-page-name"/>println("RMSE Squared: " + rm2.rootMeanSquaredError) <br class="title-page-name"/>println("R Squared: " + rm2.r2) <br class="title-page-name"/>println("Explained Variance: " + rm2.explainedVariance + "\n")  
</pre>
<p class="mce-root">We get the following output:</p>
<pre class="calibre19">
<strong class="calibre1">MSE: 0.268421052631579<br class="title-page-name"/> MAE: 0.26842105263157895<br class="title-page-name"/> RMSE Squared: 0.5180936716768301<br class="title-page-name"/> R Squared: -0.18401759530791795<br class="title-page-name"/> Explained Variance: 0.16404432132963992</strong>
</pre>
<p class="mce-root"><strong class="calibre1">Step 11. Finding the best cross-validated model</strong> - Finally, let's find the best cross-validated model information:</p>
<pre class="calibre19">
pipelineFittedModel <br class="title-page-name"/>      .bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel] <br class="title-page-name"/>      .stages(0) <br class="title-page-name"/>      .extractParamMap <br class="title-page-name"/>println("The best fitted model:" + pipelineFittedModel.bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel].stages(0)) 
</pre>
<p class="mce-root">We get the following output:</p>
<pre class="calibre19">
<strong class="calibre1">The best fitted model:RandomForestClassificationModel (uid=rfc_1fcac012b37c) with 70 trees</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">A recommendation system with Spark</h1>
                
            
            
                
<p class="mce-root">A recommender system tries to predict potential items a user might be interested in based on a history from other users. Model-based collaborative filtering is commonly used in many companies such as Netflix. It is to be noted that Netflix is an American entertainment company founded by Reed Hastings and Marc Randolph on August 29, 1997, in Scotts Valley, California. It specializes in and provides streaming media and video-on-demand online and DVD by mail. In 2013, Netflix expanded into film and television production, as well as online distribution. As of 2017, the company has its headquarters in Los Gatos, California (source Wikipedia). Netflix is a recommender system for a real-time movie recommendation. In this section, we will see a complete example of how it works toward recommending movies for new users.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Model-based recommendation with Spark</h1>
                
            
            
                
<p class="mce-root">The implementation in Spark MLlib supports model-based collaborative filtering. In the model based collaborative filtering technique, users and products are described by a small set of factors, also called the <strong class="calibre1">latent factors</strong> (<strong class="calibre1">LFs</strong>). From the following figure, you can get some idea of a different recommender system. <em class="calibre8">Figure 13</em> justifies why are going to use model-based collaborative filtering for the movie recommendation example:</p>
<div><img class="image-border172" src="img/00284.gif"/></div>
<div><strong class="calibre1">Figure 13</strong>: A comparative view of a different recommendation system</div>
<p class="mce-root">The LFs are then used for predicting the missing entries. Spark API provides the implementation of the alternating least squares (also known as the ALS widely) algorithm, which is used to learn these latent factors by considering six parameters, including:</p>
<ul class="calibre9">
<li class="mce-root1"><em class="calibre8">numBlocks</em>: This is the number of blocks used to parallelize computation (set to -1 to auto-configure).</li>
<li class="mce-root1"><em class="calibre8">rank</em>: This is the number of latent factors in the model.</li>
<li class="mce-root1"><em class="calibre8">iterations</em>: This is the number of iterations of ALS to run. ALS typically converges to a reasonable solution in 20 iterations or less.</li>
<li class="mce-root1"><em class="calibre8">lambda</em>: This specifies the regularization parameter in ALS.</li>
<li class="mce-root1"><em class="calibre8">implicitPrefs</em>: This specifies whether to use the <em class="calibre8">explicit feedback</em> ALS variant or one adapted for <em class="calibre8">implicit feedback</em> data.</li>
<li class="mce-root1"><em class="calibre8">alpha</em>: This is a parameter applicable to the implicit feedback variant of ALS that governs the <em class="calibre8">baseline</em> confidence in preference observations.</li>
</ul>
<p class="mce-root">Note that to construct an ALS instance with default parameters; you can set the value based on your requirements. The default values are as follows: <kbd class="calibre11">numBlocks: -1</kbd>, <kbd class="calibre11">rank: 10</kbd>, <kbd class="calibre11">iterations: 10</kbd>, <kbd class="calibre11">lambda: 0.01</kbd>, <kbd class="calibre11">implicitPrefs: false</kbd>, and <kbd class="calibre11">alpha: 1.0</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Data exploration</h1>
                
            
            
                
<p class="mce-root">The movie and the corresponding rating dataset were downloaded from the MovieLens Website (<a href="https://movielens.org" class="calibre10">https://movielens.org</a>). According to the data description on the MovieLens Website, all the ratings are described in the <kbd class="calibre11">ratings.csv</kbd> file. Each row of this file followed by the header represents one rating for one movie by one user.</p>
<p class="mce-root">The CSV dataset has the following columns: <strong class="calibre1">userId</strong>, <strong class="calibre1">movieId</strong>, <strong class="calibre1">rating</strong>, and <strong class="calibre1">timestamp</strong>, as shown in <em class="calibre8">Figure 14</em>. The rows are ordered first by the <strong class="calibre1">userId</strong>, and within the user, by <strong class="calibre1">movieId</strong>. Ratings are made on a five-star scale, with half-star increments (0.5 stars up to 5.0 stars). The timestamps represent the seconds since midnight Coordinated Universal Time (UTC) on January 1, 1970, where we have 105,339 ratings from the 668 users on 10,325 movies:</p>
<div><img class="alignnone2" src="img/00244.gif"/></div>
<div><strong class="calibre1">Figure 14:</strong> A snap of the ratings dataset</div>
<p class="mce-root">On the other hand, the movie information is contained in the <kbd class="calibre11">movies.csv</kbd> file. Each row apart from the header information represents one movie containing the columns: movieId, title, and genres (see <em class="calibre8">Figure 14</em>). Movie titles are either created or inserted manually or imported from the website of the movie database at <a href="https://www.themoviedb.org/" class="calibre10">https://www.themoviedb.org/</a>. The release year, however, is shown in the bracket. Since movie titles are inserted manually, some errors or inconsistencies may exist in these titles. Readers are, therefore, recommended to check the IMDb database (<a href="https://www.ibdb.com/" class="calibre10">https://www.ibdb.com/</a>) to make sure if there are no inconsistencies or incorrect titles with their corresponding release year.</p>
<p class="mce-root">Genres are a separated list, and are selected from the following genre categories:</p>
<ul class="calibre9">
<li class="mce-root1">Action, Adventure, Animation, Children's, Comedy, Crime</li>
<li class="mce-root1">Documentary, Drama, Fantasy, Film-Noir, Horror, Musical</li>
<li class="mce-root1">Mystery, Romance, Sci-Fi, Thriller, Western, War</li>
</ul>
<div><img class="alignnone3" src="img/00356.gif"/></div>
<div><strong class="calibre1">Figure 15</strong>: The title and genres for the top 20 movies</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Movie recommendation using ALS</h1>
                
            
            
                
<p class="mce-root">In this subsection, we will show you how to recommend the movie for other users through a step-by-step example from data collection to movie recommendation.</p>
<p class="mce-root"><strong class="calibre1">Step 1. Load, parse and explore the movie and rating Dataset</strong> - Here is the code illustrated:</p>
<pre class="calibre19">
val ratigsFile = "data/ratings.csv"<br class="title-page-name"/>val df1 = spark.read.format("com.databricks.spark.csv").option("header", true).load(ratigsFile)<br class="title-page-name"/>val ratingsDF = df1.select(df1.col("userId"), df1.col("movieId"), df1.col("rating"), df1.col("timestamp"))<br class="title-page-name"/>ratingsDF.show(false)
</pre>
<p class="mce-root">This code segment should return you the DataFrame of the ratings. On the other hand, the following code segment shows you the DataFrame of movies:</p>
<pre class="calibre19">
val moviesFile = "data/movies.csv"<br class="title-page-name"/>val df2 = spark.read.format("com.databricks.spark.csv").option("header", "true").load(moviesFile)<br class="title-page-name"/>val moviesDF = df2.select(df2.col("movieId"), df2.col("title"), df2.col("genres"))
</pre>
<p class="mce-root"><strong class="calibre1">Step 2. Register both DataFrames as temp tables to make querying easier</strong> - To register both Datasets, we can use the following code:</p>
<pre class="calibre19">
ratingsDF.createOrReplaceTempView("ratings")<br class="title-page-name"/>moviesDF.createOrReplaceTempView("movies")
</pre>
<p class="mce-root">This will help to make the in-memory querying faster by creating a temporary view as a table in min-memory. The lifetime of the temporary table using the <kbd class="calibre11">createOrReplaceTempView ()</kbd> method is tied to the <kbd class="calibre11">[[SparkSession]]</kbd> that was used to create this DataFrame.</p>
<p class="mce-root"><strong class="calibre1">Step 3. Explore and query for related statistics</strong> - Let's check the ratings-related statistics. Just use the following code lines:</p>
<pre class="calibre19">
val numRatings = ratingsDF.count()<br class="title-page-name"/>val numUsers = ratingsDF.select(ratingsDF.col("userId")).distinct().count()<br class="title-page-name"/>val numMovies = ratingsDF.select(ratingsDF.col("movieId")).distinct().count()<br class="title-page-name"/>println("Got " + numRatings + " ratings from " + numUsers + " users on " + numMovies + " movies.")
</pre>
<p class="mce-root">You should find 105,339 ratings from 668 users on 10,325 movies. Now, let's get the maximum and minimum ratings along with the count of users who have rated a movie. However, you need to perform a SQL query on the ratings table we just created in-memory in the previous step. Making a query here is simple, and it is similar to making a query from a MySQL database or RDBMS. However, if you are not familiar with SQL-based queries, you are recommended to look at the SQL query specification to find out how to perform a selection using <kbd class="calibre11">SELECT</kbd> from a particular table, how to perform the ordering using <kbd class="calibre11">ORDER</kbd>, and how to perform a joining operation using the <kbd class="calibre11">JOIN</kbd> keyword.</p>
<p class="mce-root">Well, if you know the SQL query, you should get a new dataset by using a complex SQL query as follows:</p>
<pre class="calibre19">
// Get the max, min ratings along with the count of users who have rated a movie.<br class="title-page-name"/>val results = spark.sql("select movies.title, movierates.maxr, movierates.minr, movierates.cntu "<br class="title-page-name"/>       + "from(SELECT ratings.movieId,max(ratings.rating) as maxr,"<br class="title-page-name"/>       + "min(ratings.rating) as minr,count(distinct userId) as cntu "<br class="title-page-name"/>       + "FROM ratings group by ratings.movieId) movierates "<br class="title-page-name"/>       + "join movies on movierates.movieId=movies.movieId "<br class="title-page-name"/>       + "order by movierates.cntu desc") <br class="title-page-name"/>results.show(false) 
</pre>
<p class="mce-root">We get the following output:</p>
<div><img class="alignnone4" src="img/00073.gif"/></div>
<div><strong class="calibre1">Figure 16:</strong> max, min ratings along with the count of users who have rated a movie</div>
<p class="mce-root">To get an insight, we need to know more about the users and their ratings. Now, let's find the top most active users and how many times they rated a movie:</p>
<pre class="calibre19">
// Show the top 10 mostactive users and how many times they rated a movie<br class="title-page-name"/>val mostActiveUsersSchemaRDD = spark.sql("SELECT ratings.userId, count(*) as ct from ratings "<br class="title-page-name"/>               + "group by ratings.userId order by ct desc limit 10")<br class="title-page-name"/>mostActiveUsersSchemaRDD.show(false)
</pre>
<div><img class="alignnone5" src="img/00262.jpeg"/></div>
<div><strong class="calibre1">Figure 17:</strong> top 10 most active users and how many times they rated a movie</div>
<p class="mce-root">Let's take a look at a particular user, and find the movies that, say user 668, rated higher than 4:</p>
<pre class="calibre19">
// Find the movies that user 668 rated higher than 4<br class="title-page-name"/>val results2 = spark.sql(<br class="title-page-name"/>"SELECT ratings.userId, ratings.movieId,"<br class="title-page-name"/>         + "ratings.rating, movies.title FROM ratings JOIN movies"<br class="title-page-name"/>         + "ON movies.movieId=ratings.movieId"<br class="title-page-name"/>         + "where ratings.userId=668 and ratings.rating &gt; 4")<br class="title-page-name"/>results2.show(false)
</pre>
<div><img class="alignnone6" src="img/00035.gif"/></div>
<div><strong class="calibre1">Figure 18:</strong> movies that user 668 rated higher than 4</div>
<p class="mce-root"><strong class="calibre1">Step 4. Prepare training and test rating data and see the counts</strong> - The following code splits ratings RDD into training data RDD (75%) and test data RDD (25%). Seed here is optional but required for the reproducibility purpose:</p>
<pre class="calibre19">
// Split ratings RDD into training RDD (75%) &amp; test RDD (25%)<br class="title-page-name"/>val splits = ratingsDF.randomSplit(Array(0.75, 0.25), seed = 12345L)<br class="title-page-name"/>val (trainingData, testData) = (splits(0), splits(1))<br class="title-page-name"/>val numTraining = trainingData.count()<br class="title-page-name"/>val numTest = testData.count()<br class="title-page-name"/>println("Training: " + numTraining + " test: " + numTest)
</pre>
<p class="mce-root">You should find that there are 78,792 ratings in the training and 26,547 ratings in the test<br class="title-page-name"/>
DataFrame.<br class="title-page-name"/>
<br class="title-page-name"/>
<strong class="calibre1">Step 5. Prepare the data for building the recommendation model using ALS</strong> - The ALS algorithm takes the RDD of <kbd class="calibre11">Rating</kbd> for the training purpose. The following code illustrates for building the recommendation model using APIs:</p>
<pre class="calibre19">
val ratingsRDD = trainingData.rdd.map(row =&gt; {<br class="title-page-name"/>  val userId = row.getString(0)<br class="title-page-name"/>  val movieId = row.getString(1)<br class="title-page-name"/>  val ratings = row.getString(2)<br class="title-page-name"/>  Rating(userId.toInt, movieId.toInt, ratings.toDouble)<br class="title-page-name"/>})
</pre>
<p class="mce-root">The <kbd class="calibre11">ratingsRDD</kbd> is an RDD of ratings that contains the <kbd class="calibre11">userId</kbd>, <kbd class="calibre11">movieId</kbd>, and corresponding ratings from the training dataset that we prepared in the previous step. On the other hand, a test RDD is also required for evaluating the model. The following <kbd class="calibre11">testRDD</kbd> also contains the same information coming from the test DataFrame we prepared in the previous step:</p>
<pre class="calibre19">
val testRDD = testData.rdd.map(row =&gt; {<br class="title-page-name"/>  val userId = row.getString(0)<br class="title-page-name"/>  val movieId = row.getString(1)<br class="title-page-name"/>  val ratings = row.getString(2)<br class="title-page-name"/>  Rating(userId.toInt, movieId.toInt, ratings.toDouble)<br class="title-page-name"/>}) 
</pre>
<p class="mce-root"><strong class="calibre1">Step 6. Build an ALS user product matrix</strong> - Build an ALS user matrix model based on the <kbd class="calibre11">ratingsRDD</kbd> by specifying the maximal iteration, a number of blocks, alpha, rank, lambda, seed, and <kbd class="calibre11">implicitPrefs</kbd>. Essentially, this technique predicts missing ratings for specific users for specific movies based on ratings for those movies from other users who did similar ratings for other movies:</p>
<pre class="calibre19">
val rank = 20<br class="title-page-name"/>val numIterations = 15<br class="title-page-name"/>val lambda = 0.10<br class="title-page-name"/>val alpha = 1.00<br class="title-page-name"/>val block = -1<br class="title-page-name"/>val seed = 12345L<br class="title-page-name"/>val implicitPrefs = false<br class="title-page-name"/>val model = new ALS()<br class="title-page-name"/>           .setIterations(numIterations)<br class="title-page-name"/>           .setBlocks(block)<br class="title-page-name"/>           .setAlpha(alpha)<br class="title-page-name"/>           .setLambda(lambda)<br class="title-page-name"/>           .setRank(rank)<br class="title-page-name"/>           .setSeed(seed)<br class="title-page-name"/>           .setImplicitPrefs(implicitPrefs)<br class="title-page-name"/>           .run(ratingsRDD) 
</pre>
<p class="mce-root">Finally, we iterated the model for learning 15 times. With this setting, we got good prediction accuracy. Readers are suggested to apply the hyperparameter tuning to get to know the optimum values for these parameters. Furthermore, set the number of blocks for both user blocks and product blocks to parallelize the computation into a pass -1 for an auto-configured number of blocks. The value is -1.</p>
<p class="mce-root"><strong class="calibre1">Step 7. Making predictions</strong> - Let's get the top six movie predictions for user 668. The following source code can be used to make the predictions:</p>
<pre class="calibre19">
// Making Predictions. Get the top 6 movie predictions for user 668<br class="title-page-name"/>println("Rating:(UserID, MovieID, Rating)")<br class="title-page-name"/>println("----------------------------------")<br class="title-page-name"/>val topRecsForUser = model.recommendProducts(668, 6)<br class="title-page-name"/>for (rating &lt;- topRecsForUser) {<br class="title-page-name"/>  println(rating.toString())<br class="title-page-name"/>}<br class="title-page-name"/>println("----------------------------------")
</pre>
<p class="mce-root">The preceding code segment produces the following output containing the rating prediction with <kbd class="calibre11">UserID</kbd>, <kbd class="calibre11">MovieID</kbd>, and corresponding <kbd class="calibre11">Rating</kbd> for that movie:</p>
<div><img class="image-border173" src="img/00376.gif"/></div>
<div><strong class="calibre1">Figure 19</strong>: top six movie predictions for user 668</div>
<p class="mce-root"><strong class="calibre1">Step 8. Evaluating the model</strong> - In order to verify the quality of the models, <strong class="calibre1">Root Mean Squared Error</strong> (<strong class="calibre1">RMSE</strong>) is used to measure the differences between values predicted by a model and the values actually observed. By default, the smaller the calculated error, the better the model. In order to test the quality of the model, the test data is used (which was split above in step 4). According to many machine learning practitioners, the RMSE is a good measure of accuracy, but only to compare forecasting errors of different models for a particular variable and not between variables, as it is scale-dependent. The following line of code calculates the RMSE value for the model that was trained using the training set:</p>
<pre class="calibre19">
var rmseTest = computeRmse(model, testRDD, true)<br class="title-page-name"/>println("Test RMSE: = " + rmseTest) //Less is better 
</pre>
<p class="mce-root">It is to be noted that the <kbd class="calibre11">computeRmse()</kbd> is a UDF, that goes as follows:</p>
<pre class="calibre19">
  def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating], implicitPrefs: Boolean): Double = {<br class="title-page-name"/>    val predictions: RDD[Rating] = model.predict(data.map(x =&gt; (x.user, x.product)))<br class="title-page-name"/>    val predictionsAndRatings = predictions.map { x =&gt; ((x.user, x.product), x.rating)<br class="title-page-name"/>  }.join(data.map(x =&gt; ((x.user, x.product), x.rating))).values<br class="title-page-name"/>  if (implicitPrefs) {<br class="title-page-name"/>    println("(Prediction, Rating)")<br class="title-page-name"/>    println(predictionsAndRatings.take(5).mkString("\n"))<br class="title-page-name"/>  }<br class="title-page-name"/>  math.sqrt(predictionsAndRatings.map(x =&gt; (x._1 - x._2) * (x._1 - x._2)).mean())<br class="title-page-name"/>}
</pre>
<p class="mce-root">The preceding method computes the RMSE to evaluate the model. Less the RMSE, the better the model and its prediction capability.</p>
<p class="mce-root">For the earlier setting, we got the following output:</p>
<pre class="calibre19">
<strong class="calibre1">Test RMSE: = 0.9019872589764073</strong>
</pre>
<p class="mce-root">The performance of the preceding model could be increased further we believe. Interested readers should refer to this URL for more on tuning the ML-based ALS models <a href="https://spark.apache.org/docs/preview/ml-collaborative-filtering.html" class="calibre10">https://spark.apache.org/docs/preview/ml-collaborative-filtering.html</a>.</p>
<p class="mce-root">The topic modeling technique is widely used in the task of mining text from a large collection of documents. These topics can then be used to summarize and organize documents that include the topic terms and their relative weights. In the next section, we will show an example of topic modeling using the <strong class="calibre1">latent dirichlet allocation</strong> (<strong class="calibre1">LDA</strong>) algorithm.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Topic modelling - A best practice for text clustering</h1>
                
            
            
                
<p class="mce-root">The topic modeling technique is widely used in the task of mining text from a large collection of documents. These topics can then be used to summarize and organize documents that include the topic terms and their relative weights. The dataset that will be used for this example is just in plain text, however, in an unstructured format. Now the challenging part is finding useful patterns about the data using LDA called topic modeling.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">How does LDA work?</h1>
                
            
            
                
<p class="mce-root">LDA is a topic model which infers topics from a collection of text documents. LDA can be thought of as a clustering algorithm where topics correspond to cluster centers, and documents correspond to examples (rows) in a dataset. Topics and documents both exist in a feature space, where feature vectors are vectors of word counts (bag of words). Instead of estimating a clustering using a traditional distance, LDA uses a function based on a statistical model of how text documents are generated.</p>
<p class="mce-root">LDA supports different inference algorithms via <kbd class="calibre11">setOptimizer</kbd> function. <kbd class="calibre11">EMLDAOptimizer</kbd> learns clustering using expectation-maximization on the likelihood function and yields comprehensive results, while <kbd class="calibre11">OnlineLDAOptimizer</kbd> uses iterative mini-batch sampling for online variational inference and is generally memory friendly. LDA takes in a collection of documents as vectors of word counts and the following parameters (set using the builder pattern):</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">k</kbd>: Number of topics (that is, cluster centers).</li>
<li class="mce-root1"><kbd class="calibre11">optimizer</kbd>: Optimizer to use for learning the LDA model, either <kbd class="calibre11">EMLDAOptimizer</kbd> or <kbd class="calibre11">OnlineLDAOptimizer</kbd>.</li>
<li class="mce-root1"><kbd class="calibre11">docConcentration</kbd>: Dirichlet parameter for prior over documents' distributions over topics. Larger values encourage smoother inferred distributions.</li>
<li class="mce-root1"><kbd class="calibre11">topicConcentration</kbd>: Dirichlet parameter for prior over topics' distributions over terms (words). Larger values encourage smoother inferred distributions.</li>
<li class="mce-root1"><kbd class="calibre11">maxIterations</kbd>: Limit on the number of iterations.</li>
<li class="mce-root1"><kbd class="calibre11">checkpointInterval</kbd>: If using checkpointing (set in the Spark configuration), this parameter specifies the frequency with which checkpoints will be created. If <kbd class="calibre11">maxIterations</kbd> is large, using checkpointing can help reduce shuffle file sizes on disk and help with failure recovery.</li>
</ul>
<p class="mce-root">Particularly, we would like to discuss the topics people talk about most from the large collection of texts. Since the release of Spark 1.3, MLlib supports the LDA, which is one of the most successfully used topic modeling techniques in the area of text mining and <strong class="calibre1">Natural Language Processing</strong> (<strong class="calibre1">NLP</strong>). Moreover, LDA is also the first MLlib algorithm to adopt Spark GraphX.</p>
<p>To get more information about how the theory behind the LDA works, please refer to David M. Blei, Andrew Y. Ng and Michael I. Jordan, Latent, Dirichlet Allocation, <em class="calibre25">Journal of Machine Learning Research 3</em> (2003) 993-1022.</p>
<p class="mce-root">The following figure shows the topic distribution from randomly generated tweet text:</p>
<div><img class="image-border174" src="img/00165.jpeg"/><br class="title-page-name"/>
<br class="title-page-name"/>
<strong class="calibre1">Figure 20</strong>: The topic distribution and how it looks like</div>
<p class="mce-root">In this section, we will look at an example of topic modeling using the LDA algorithm of Spark MLlib with unstructured raw tweets datasets. Note that here we have used LDA, which is one of the most popular topic modeling algorithms commonly used for text mining. We could use more robust topic modeling algorithms such as <strong class="calibre1">Probabilistic Latent Sentiment Analysis</strong> (<strong class="calibre1">pLSA</strong>), <strong class="calibre1">pachinko allocation model</strong> (<strong class="calibre1">PAM</strong>), or <strong class="calibre1">hierarchical dirichlet process</strong> (<strong class="calibre1">HDP</strong>) algorithms.</p>
<p class="mce-root">However, pLSA has the overfitting problem. On the other hand, both HDP and PAM are more complex topic modeling algorithms used for complex text mining such as mining topics from high dimensional text data or documents of unstructured text. Moreover, to this date, Spark has implemented only one topic modeling algorithm, that is LDA. Therefore, we have to use LDA reasonably.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Topic modeling with Spark MLlib</h1>
                
            
            
                
<p class="mce-root">In this subsection, we represent a semi-automated technique of topic modeling using Spark. Using other options as defaults, we train LDA on the dataset downloaded from the GitHub URL at <a href="https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test" class="calibre10">https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test</a>. The following steps show the topic modeling from data reading to printing the topics, along with their term-weights. Here's the short workflow of the topic modeling pipeline:</p>
<pre class="calibre19">
object topicModellingwithLDA {<br class="title-page-name"/>  def main(args: Array[String]): Unit = {<br class="title-page-name"/>    val lda = new LDAforTM() // actual computations are done here<br class="title-page-name"/>    val defaultParams = Params().copy(input = "data/docs/") <br class="title-page-name"/>    // Loading the parameters<br class="title-page-name"/>    lda.run(defaultParams) // Training the LDA model with the default<br class="title-page-name"/>                              parameters.<br class="title-page-name"/>  }<br class="title-page-name"/>} 
</pre>
<p class="mce-root">The actual computation on topic modeling is done in the <kbd class="calibre11">LDAforTM</kbd> class. The <kbd class="calibre11">Params</kbd> is a case class, which is used for loading the parameters to train the LDA model. Finally, we train the LDA model using the parameters setting via the <kbd class="calibre11">Params</kbd> class. Now, we will explain each step broadly with step-by-step source code:<br class="title-page-name"/>
<br class="title-page-name"/>
<strong class="calibre1">Step 1. Creating a Spark session</strong> - Let's create a Spark session by defining number of computing core, SQL warehouse, and application name as follows:</p>
<pre class="calibre19">
val spark = SparkSession<br class="title-page-name"/>    .builder<br class="title-page-name"/>    .master("local[*]")<br class="title-page-name"/>    .config("spark.sql.warehouse.dir", "E:/Exp/")<br class="title-page-name"/>    .appName("LDA for topic modelling")<br class="title-page-name"/>    .getOrCreate() 
</pre>
<p class="mce-root"><strong class="calibre1">Step 2. Creating vocabulary, tokens count to train LDA after text pre-processing</strong> - At first, load the documents, and prepare them for LDA as follows:</p>
<pre class="calibre19">
// Load documents, and prepare them for LDA.<br class="title-page-name"/><br class="title-page-name"/>val preprocessStart = System.nanoTime()<br class="title-page-name"/>val (corpus, vocabArray, actualNumTokens) = preprocess(params.input, params.vocabSize, params.stopwordFile)  
</pre>
<p class="mce-root">The pre-process method is used to process the raw texts. At first, let's read the whole texts using the <kbd class="calibre11">wholeTextFiles()</kbd> method as follows:</p>
<pre class="calibre19">
val initialrdd = spark.sparkContext.wholeTextFiles(paths).map(_._2)<br class="title-page-name"/>initialrdd.cache()  
</pre>
<p class="mce-root">In the preceding code, paths are the path of the text files. Then, we need to prepare a morphological RDD from the raw text based on the lemma texts as follows:</p>
<pre class="calibre19">
val rdd = initialrdd.mapPartitions { partition =&gt;<br class="title-page-name"/>  val morphology = new Morphology()<br class="title-page-name"/>  partition.map { value =&gt; helperForLDA.getLemmaText(value, morphology) }<br class="title-page-name"/>}.map(helperForLDA.filterSpecialCharacters)
</pre>
<p class="mce-root">Here the <kbd class="calibre11">getLemmaText()</kbd> method from the <kbd class="calibre11">helperForLDA</kbd> class supplies the lemma texts after filtering the special characters such as <kbd class="calibre11">("""[! @ # $ % ^ &amp; * ( ) _ + - − , " ' ; : . ` ? --]</kbd> as regular expressions using the <kbd class="calibre11">filterSpaecialChatacters()</kbd> method.</p>
<p class="mce-root">It is to be noted that the <kbd class="calibre11">Morphology()</kbd> class computes the base form of English words, by removing just inflections (not derivational morphology). That is, it only does noun plurals, pronoun case, and verb endings, and not things like comparative adjectives or derived nominals. This comes from the Stanford NLP group. To use this, you should have the following import in the main class file: <kbd class="calibre11">edu.stanford.nlp.process.Morphology</kbd>. In the <kbd class="calibre11">pom.xml</kbd> file, you will have to include the following entries as dependencies:</p>
<pre class="calibre19">
&lt;dependency&gt;<br class="title-page-name"/>    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;<br class="title-page-name"/>    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;<br class="title-page-name"/>    &lt;version&gt;3.6.0&lt;/version&gt;<br class="title-page-name"/>&lt;/dependency&gt;<br class="title-page-name"/>&lt;dependency&gt;<br class="title-page-name"/>    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;<br class="title-page-name"/>    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;<br class="title-page-name"/>    &lt;version&gt;3.6.0&lt;/version&gt;<br class="title-page-name"/>    &lt;classifier&gt;models&lt;/classifier&gt;<br class="title-page-name"/>&lt;/dependency&gt;
</pre>
<p class="mce-root">The method goes as follows:</p>
<pre class="calibre19">
def getLemmaText(document: String, morphology: Morphology) = {<br class="title-page-name"/>  val string = new StringBuilder()<br class="title-page-name"/>  val value = new Document(document).sentences().toList.flatMap { a =&gt;<br class="title-page-name"/>  val words = a.words().toList<br class="title-page-name"/>  val tags = a.posTags().toList<br class="title-page-name"/>  (words zip tags).toMap.map { a =&gt;<br class="title-page-name"/>    val newWord = morphology.lemma(a._1, a._2)<br class="title-page-name"/>    val addedWoed = if (newWord.length &gt; 3) {<br class="title-page-name"/>      newWord<br class="title-page-name"/>    } else { "" }<br class="title-page-name"/>      string.append(addedWoed + " ")<br class="title-page-name"/>    }<br class="title-page-name"/>  }<br class="title-page-name"/>  string.toString()<br class="title-page-name"/>} 
</pre>
<p class="mce-root">The <kbd class="calibre11">filterSpecialCharacters()</kbd> goes as follows:<br class="title-page-name"/>
<kbd class="calibre11">def filterSpecialCharacters(document: String) = document.replaceAll("""[! @ # $ % ^ &amp; * ( ) _ + - − , " ' ; : . ` ? --]""", " ")</kbd>. Once we have the RDD with special characters removed in hand, we can create a DataFrame for building the text analytics pipeline:</p>
<pre class="calibre19">
rdd.cache()<br class="title-page-name"/>initialrdd.unpersist()<br class="title-page-name"/>val df = rdd.toDF("docs")<br class="title-page-name"/>df.show() 
</pre>
<p class="mce-root">So, the DataFrame consist of only of the documents tag. A snapshot of the DataFrame is as follows:</p>
<div><img class="image-border175" src="img/00332.gif"/></div>
<div><strong class="calibre1">Figure 21</strong>: Raw texts</div>
<p class="mce-root">Now if you examine the preceding DataFrame carefully, you will see that we still need to tokenize the items. Moreover, there are stop words in a DataFrame such as this, so we need to remove them as well. At first, let's tokenize them using the <kbd class="calibre11">RegexTokenizer</kbd> API as follows:</p>
<pre class="calibre19">
val tokenizer = new RegexTokenizer().setInputCol("docs").setOutputCol("rawTokens") 
</pre>
<p class="mce-root">Now, let's remove all the stop words as follows:</p>
<pre class="calibre19">
val stopWordsRemover = new StopWordsRemover().setInputCol("rawTokens").setOutputCol("tokens")<br class="title-page-name"/>stopWordsRemover.setStopWords(stopWordsRemover.getStopWords ++ customizedStopWords)
</pre>
<p class="mce-root">Furthermore, we also need to apply count victories to find only the important features from the tokens. This will help make the pipeline chained at the pipeline stage. Let's do it as follows:</p>
<pre class="calibre19">
val countVectorizer = new CountVectorizer().setVocabSize(vocabSize).setInputCol("tokens").setOutputCol("features") 
</pre>
<p class="mce-root">Now, create the pipeline by chaining the transformers (<kbd class="calibre11">tokenizer</kbd>, <kbd class="calibre11">stopWordsRemover</kbd>, and <kbd class="calibre11">countVectorizer</kbd>) as follows:</p>
<pre class="calibre19">
val pipeline = new Pipeline().setStages(Array(tokenizer, stopWordsRemover, countVectorizer))
</pre>
<p class="mce-root">Let's fit and transform the pipeline towards the vocabulary and number of tokens:</p>
<pre class="calibre19">
val model = pipeline.fit(df)<br class="title-page-name"/>val documents = model.transform(df).select("features").rdd.map {<br class="title-page-name"/>  case Row(features: MLVector) =&gt;Vectors.fromML(features)<br class="title-page-name"/>}.zipWithIndex().map(_.swap)
</pre>
<p class="mce-root">Finally, return the vocabulary and token count pairs as follows:</p>
<pre class="calibre19">
(documents, model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary, documents.map(_._2.numActives).sum().toLong)
</pre>
<p class="mce-root">Now, let's see the statistics of the training data:</p>
<pre class="calibre19">
println()<br class="title-page-name"/>println("Training corpus summary:")<br class="title-page-name"/>println("-------------------------------")<br class="title-page-name"/>println("Training set size: " + actualCorpusSize + " documents")<br class="title-page-name"/>println("Vocabulary size: " + actualVocabSize + " terms")<br class="title-page-name"/>println("Number of tockens: " + actualNumTokens + " tokens")<br class="title-page-name"/>println("Preprocessing time: " + preprocessElapsed + " sec")<br class="title-page-name"/>println("-------------------------------")<br class="title-page-name"/>println()<br class="title-page-name"/><br class="title-page-name"/>
</pre>
<p class="mce-root">We get the following output:</p>
<pre class="calibre19">
<strong class="calibre1">Training corpus summary:<br class="title-page-name"/> -------------------------------<br class="title-page-name"/> Training set size: 18 documents<br class="title-page-name"/> Vocabulary size: 21607 terms<br class="title-page-name"/> Number of tockens: 75758 tokens<br class="title-page-name"/> Preprocessing time: 39.768580981 sec<br class="title-page-name"/> <strong class="calibre1">-------------------------------</strong></strong>
</pre>
<p class="mce-root"><strong class="calibre1"><strong class="calibre1">Step 4. Instantiate the LDA model before training</strong></strong></p>
<pre class="calibre19">
val lda = new LDA()
</pre>
<p class="mce-root"><strong class="calibre1">Step 5: Set the NLP optimizer</strong></p>
<p class="mce-root">For better and optimized results from the LDA model, we need to set the optimizer for the LDA model. Here we use the <kbd class="calibre11">EMLDAOPtimizer</kbd> optimizer. You can also use the <kbd class="calibre11">OnlineLDAOptimizer()</kbd> optimizer. However, you need to add (1.0/actualCorpusSize) to <kbd class="calibre11">MiniBatchFraction</kbd> to be more robust on tiny datasets. The whole operation goes as follows. First, instantiate the <kbd class="calibre11">EMLDAOptimizer</kbd> as follows:</p>
<pre class="calibre19">
val optimizer = params.algorithm.toLowerCase match {<br class="title-page-name"/>  case "em" =&gt; new EMLDAOptimizer<br class="title-page-name"/>  case "online" =&gt; new OnlineLDAOptimizer().setMiniBatchFraction(0.05 + 1.0 / actualCorpusSize)<br class="title-page-name"/>  case _ =&gt; throw new IllegalArgumentException("Only em is supported, got ${params.algorithm}.")<br class="title-page-name"/>}
</pre>
<p class="mce-root">Now set the optimizer using the <kbd class="calibre11">setOptimizer()</kbd> method from the LDA API as follows:</p>
<pre class="calibre19">
lda.setOptimizer(optimizer)<br class="title-page-name"/>  .setK(params.k)<br class="title-page-name"/>  .setMaxIterations(params.maxIterations)<br class="title-page-name"/>  .setDocConcentration(params.docConcentration)<br class="title-page-name"/>  .setTopicConcentration(params.topicConcentration)<br class="title-page-name"/>  .setCheckpointInterval(params.checkpointInterval)
</pre>
<p class="mce-root">The <kbd class="calibre11">Params</kbd> case class is used to define the parameters to training the LDA model. This goes as follows:</p>
<pre class="calibre19">
 //Setting the parameters before training the LDA model<br class="title-page-name"/>case class Params(input: String = "",<br class="title-page-name"/>                  k: Int = 5,<br class="title-page-name"/>                  maxIterations: Int = 20,<br class="title-page-name"/>                  docConcentration: Double = -1,<br class="title-page-name"/>                  topicConcentration: Double = -1,<br class="title-page-name"/>                  vocabSize: Int = 2900000,<br class="title-page-name"/>                  stopwordFile: String = "data/stopWords.txt",<br class="title-page-name"/>                  algorithm: String = "em",<br class="title-page-name"/>                  checkpointDir: Option[String] = None,<br class="title-page-name"/>                  checkpointInterval: Int = 10)
</pre>
<p class="mce-root">For a better result you can set these parameters in a naive way. Alternatively, you should go with the cross-validation for even better performance. Now if you want to checkpoint the current parameters, use the following line of codes:</p>
<pre class="calibre19">
if (params.checkpointDir.nonEmpty) {<br class="title-page-name"/>  spark.sparkContext.setCheckpointDir(params.checkpointDir.get)<br class="title-page-name"/>}
</pre>
<p class="mce-root"><strong class="calibre1">Step 6.</strong> Training the LDA model:</p>
<pre class="calibre19">
val startTime = System.nanoTime()<br class="title-page-name"/>//Start training the LDA model using the training corpus <br class="title-page-name"/>val ldaModel = lda.run(corpus)<br class="title-page-name"/>val elapsed = (System.nanoTime() - startTime) / 1e9<br class="title-page-name"/>println(s"Finished training LDA model.  Summary:") <br class="title-page-name"/>println(s"t Training time: $elapsed sec")
</pre>
<p class="mce-root">For the texts we have, the LDA model took 6.309715286 sec to train. Note that these timing codes are optional. Here we provide them for reference purposes, only to get an idea of the training time.</p>
<p class="mce-root"><strong class="calibre1">Step 7. Measuring the likelihood of the data</strong> - Now, of to get some more statistics about the data such as maximum likelihood or log-likelihood, we can use the following code:</p>
<pre class="calibre19">
if (ldaModel.isInstanceOf[DistributedLDAModel]) {<br class="title-page-name"/>  val distLDAModel = ldaModel.asInstanceOf[DistributedLDAModel]<br class="title-page-name"/>  val avgLogLikelihood = distLDAModel.logLikelihood / actualCorpusSize.toDouble<br class="title-page-name"/>  println("The average log likelihood of the training data: " +  avgLogLikelihood)<br class="title-page-name"/>  println()<br class="title-page-name"/>}
</pre>
<p class="mce-root">The preceding code calculates the average log likelihood if the LDA model is an instance of the distributed version of the LDA model. We get the following output:</p>
<pre class="calibre19">
<strong class="calibre1">The average log-likelihood of the training data: -208599.21351837728</strong>  
</pre>
<p class="mce-root">The likelihood is used after data are available to describe a function of a parameter (or parameter vector) for a given outcome. This helps especially for estimating a parameter from a set of statistics. For more information on the likelihood measurement, interested readers should refer to <a href="https://en.wikipedia.org/wiki/Likelihood_function" class="calibre10">https://en.wikipedia.org/wiki/Likelihood_function</a>.</p>
<p class="mce-root"><strong class="calibre1">Step 8. Prepare the topics of interests</strong> - Prepare the top five topics with each topic having 10 terms. Include the terms and their corresponding weights.</p>
<pre class="calibre19">
val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 10)<br class="title-page-name"/>println(topicIndices.length)<br class="title-page-name"/>val topics = topicIndices.map {case (terms, termWeights) =&gt; terms.zip(termWeights).map { case (term, weight) =&gt; (vocabArray(term.toInt), weight) } }
</pre>
<p class="mce-root"><strong class="calibre1">Step 9. Topic modeling</strong> - Print the top ten topics, showing the top-weighted terms for each topic. Also, include the total weight in each topic as follows:</p>
<pre class="calibre19">
var sum = 0.0<br class="title-page-name"/>println(s"${params.k} topics:")<br class="title-page-name"/>topics.zipWithIndex.foreach {<br class="title-page-name"/>  case (topic, i) =&gt;<br class="title-page-name"/>  println(s"TOPIC $i")<br class="title-page-name"/>  println("------------------------------")<br class="title-page-name"/>  topic.foreach {<br class="title-page-name"/>    case (term, weight) =&gt;<br class="title-page-name"/>    println(s"$termt$weight")<br class="title-page-name"/>    sum = sum + weight<br class="title-page-name"/>  }<br class="title-page-name"/>  println("----------------------------")<br class="title-page-name"/>  println("weight: " + sum)<br class="title-page-name"/>  println()
</pre>
<p class="mce-root">Now, let's see the output of our LDA model toward topics modeling:</p>
<pre class="calibre19">
    <strong class="calibre1">5 topics:</strong><br class="title-page-name"/>    <strong class="calibre1">TOPIC 0</strong><br class="title-page-name"/>    <strong class="calibre1">------------------------------</strong><br class="title-page-name"/>    <strong class="calibre1">think 0.0105511077762379</strong><br class="title-page-name"/>    <strong class="calibre1">look  0.010393384083882656</strong><br class="title-page-name"/>    <strong class="calibre1">know  0.010121680765600402</strong><br class="title-page-name"/>    <strong class="calibre1">come  0.009999416569525854</strong><br class="title-page-name"/>    <strong class="calibre1">little      0.009880422850906338</strong><br class="title-page-name"/>    <strong class="calibre1">make  0.008982740529851225</strong><br class="title-page-name"/>    <strong class="calibre1">take  0.007061048216197747</strong><br class="title-page-name"/>    <strong class="calibre1">good  0.007040301924830752</strong><br class="title-page-name"/>    <strong class="calibre1">much  0.006273732732002744</strong><br class="title-page-name"/>    <strong class="calibre1">well  0.0062484438391950895</strong><br class="title-page-name"/>    <strong class="calibre1">----------------------------</strong><br class="title-page-name"/>    <strong class="calibre1">weight: 0.0865522792882307</strong><br class="title-page-name"/>    <br class="title-page-name"/>    <strong class="calibre1">TOPIC 1</strong><br class="title-page-name"/>    <strong class="calibre1">------------------------------</strong><br class="title-page-name"/>    <strong class="calibre1">look  0.008658099588372216</strong><br class="title-page-name"/>    <strong class="calibre1">come  0.007972622171954474</strong><br class="title-page-name"/>    <strong class="calibre1">little      0.007596460821298818</strong><br class="title-page-name"/>    <strong class="calibre1">hand  0.0065409990798624565</strong><br class="title-page-name"/>    <strong class="calibre1">know  0.006314616294309573</strong><br class="title-page-name"/>    <strong class="calibre1">lorry 0.005843633203040061</strong><br class="title-page-name"/>    <strong class="calibre1">upon  0.005545300032552888</strong><br class="title-page-name"/>    <strong class="calibre1">make  0.005391780686824741</strong><br class="title-page-name"/>    <strong class="calibre1">take  0.00537353581562707</strong><br class="title-page-name"/>    <strong class="calibre1">time  0.005030870790464942</strong><br class="title-page-name"/>    <strong class="calibre1">----------------------------</strong><br class="title-page-name"/>    <strong class="calibre1">weight: 0.15082019777253794</strong><br class="title-page-name"/>    <br class="title-page-name"/>    <strong class="calibre1">TOPIC 2</strong><br class="title-page-name"/>    <strong class="calibre1">------------------------------</strong><br class="title-page-name"/>    <strong class="calibre1">captain     0.006865463831587792</strong><br class="title-page-name"/>    <strong class="calibre1">nautilus    0.005175561004431676</strong><br class="title-page-name"/>    <strong class="calibre1">make  0.004910586984657019</strong><br class="title-page-name"/>    <strong class="calibre1">hepzibah    0.004378298053191463</strong><br class="title-page-name"/>    <strong class="calibre1">water 0.004063096964497903</strong><br class="title-page-name"/>    <strong class="calibre1">take  0.003959626037381751</strong><br class="title-page-name"/>    <strong class="calibre1">nemo  0.0037687537789531005</strong><br class="title-page-name"/>    <strong class="calibre1">phoebe      0.0037683642100062313</strong><br class="title-page-name"/>    <strong class="calibre1">pyncheon    0.003678496229955977</strong><br class="title-page-name"/>    <strong class="calibre1">seem  0.0034594205003318193</strong><br class="title-page-name"/>    <strong class="calibre1">----------------------------</strong><br class="title-page-name"/>    <strong class="calibre1">weight: 0.19484786536753268</strong><br class="title-page-name"/>    <br class="title-page-name"/>    <strong class="calibre1">TOPIC 3</strong><br class="title-page-name"/>    <strong class="calibre1">------------------------------</strong><br class="title-page-name"/>    <strong class="calibre1">fogg  0.009552022075897986</strong><br class="title-page-name"/>    <strong class="calibre1">rodney      0.008705705501603078</strong><br class="title-page-name"/>    <strong class="calibre1">make  0.007016635545801613</strong><br class="title-page-name"/>    <strong class="calibre1">take  0.00676049232003675</strong><br class="title-page-name"/>    <strong class="calibre1">passepartout      0.006295907851484774</strong><br class="title-page-name"/>    <strong class="calibre1">leave 0.005565220660514245</strong><br class="title-page-name"/>    <strong class="calibre1">find  0.005077555215275536</strong><br class="title-page-name"/>    <strong class="calibre1">time  0.004852923943330551</strong><br class="title-page-name"/>    <strong class="calibre1">luke  0.004729546554304362</strong><br class="title-page-name"/>    <strong class="calibre1">upon  0.004707181805179265</strong><br class="title-page-name"/>    <strong class="calibre1">----------------------------</strong><br class="title-page-name"/>    <strong class="calibre1">weight: 0.2581110568409608</strong><br class="title-page-name"/>    <br class="title-page-name"/>    <strong class="calibre1">TOPIC 4</strong><br class="title-page-name"/>    <strong class="calibre1">------------------------------</strong><br class="title-page-name"/>    <strong class="calibre1">dick  0.013754147765988699</strong><br class="title-page-name"/>    <strong class="calibre1">thus  0.006231933402776328</strong><br class="title-page-name"/>    <strong class="calibre1">ring  0.0052746290878481926</strong><br class="title-page-name"/>    <strong class="calibre1">bear  0.005181637978658836</strong><br class="title-page-name"/>    <strong class="calibre1">fate  0.004739983892853129</strong><br class="title-page-name"/>    <strong class="calibre1">shall 0.0046221874997173906</strong><br class="title-page-name"/>    <strong class="calibre1">hand  0.004610810387565958</strong><br class="title-page-name"/>    <strong class="calibre1">stand 0.004121100025638923</strong><br class="title-page-name"/>    <strong class="calibre1">name  0.0036093879729237</strong><br class="title-page-name"/>    <strong class="calibre1">trojan      0.0033792362039766505</strong><br class="title-page-name"/>    <strong class="calibre1">----------------------------</strong><br class="title-page-name"/>    <strong class="calibre1">weight: 0.31363611105890865</strong>
</pre>
<p class="mce-root">From the preceding output, we can see that the topic of the input documents is topic 5 having the most weight of <kbd class="calibre11">0.31363611105890865</kbd>. This topic discusses the terms love, long, shore, shower, ring, bring, bear and so on. Now, for a better understanding of the flow, here's the complete source code:</p>
<pre class="calibre19">
package com.chapter11.SparkMachineLearning<br class="title-page-name"/><br class="title-page-name"/>import edu.stanford.nlp.process.Morphology<br class="title-page-name"/>import edu.stanford.nlp.simple.Document<br class="title-page-name"/>import org.apache.log4j.{ Level, Logger }<br class="title-page-name"/>import scala.collection.JavaConversions._<br class="title-page-name"/>import org.apache.spark.{ SparkConf, SparkContext }<br class="title-page-name"/>import org.apache.spark.ml.Pipeline<br class="title-page-name"/>import org.apache.spark.ml.feature._<br class="title-page-name"/>import org.apache.spark.ml.linalg.{ Vector =&gt; MLVector }<br class="title-page-name"/>import org.apache.spark.mllib.clustering.{ DistributedLDAModel, EMLDAOptimizer, LDA, OnlineLDAOptimizer }<br class="title-page-name"/>import org.apache.spark.mllib.linalg.{ Vector, Vectors }<br class="title-page-name"/>import org.apache.spark.rdd.RDD<br class="title-page-name"/>import org.apache.spark.sql.{ Row, SparkSession }<br class="title-page-name"/><br class="title-page-name"/>object topicModellingwithLDA {<br class="title-page-name"/>  def main(args: Array[String]): Unit = {<br class="title-page-name"/>    val lda = new LDAforTM() // actual computations are done here<br class="title-page-name"/>    val defaultParams = Params().copy(input = "data/docs/") <br class="title-page-name"/>    // Loading the parameters to train the LDA model<br class="title-page-name"/>    lda.run(defaultParams) // Training the LDA model with the default<br class="title-page-name"/>                              parameters.<br class="title-page-name"/>  }<br class="title-page-name"/>}<br class="title-page-name"/>//Setting the parameters before training the LDA model<br class="title-page-name"/>caseclass Params(input: String = "",<br class="title-page-name"/>                 k: Int = 5,<br class="title-page-name"/>                 maxIterations: Int = 20,<br class="title-page-name"/>                 docConcentration: Double = -1,<br class="title-page-name"/>                 topicConcentration: Double = -1,<br class="title-page-name"/>                 vocabSize: Int = 2900000,<br class="title-page-name"/>                 stopwordFile: String = "data/docs/stopWords.txt",<br class="title-page-name"/>                 algorithm: String = "em",<br class="title-page-name"/>                 checkpointDir: Option[String] = None,<br class="title-page-name"/>                 checkpointInterval: Int = 10)<br class="title-page-name"/><br class="title-page-name"/>// actual computations for topic modeling are done here<br class="title-page-name"/>class LDAforTM() {<br class="title-page-name"/>  val spark = SparkSession<br class="title-page-name"/>              .builder<br class="title-page-name"/>              .master("local[*]")<br class="title-page-name"/>              .config("spark.sql.warehouse.dir", "E:/Exp/")<br class="title-page-name"/>              .appName("LDA for topic modelling")<br class="title-page-name"/>              .getOrCreate()<br class="title-page-name"/><br class="title-page-name"/>  def run(params: Params): Unit = {<br class="title-page-name"/>    Logger.getRootLogger.setLevel(Level.WARN)<br class="title-page-name"/>    // Load documents, and prepare them for LDA.<br class="title-page-name"/>    val preprocessStart = System.nanoTime()<br class="title-page-name"/>    val (corpus, vocabArray, actualNumTokens) = preprocess(params<br class="title-page-name"/>                      .input, params.vocabSize, params.stopwordFile)<br class="title-page-name"/>    val actualCorpusSize = corpus.count()<br class="title-page-name"/>    val actualVocabSize = vocabArray.length<br class="title-page-name"/>    val preprocessElapsed = (System.nanoTime() - preprocessStart) / 1e9<br class="title-page-name"/>    corpus.cache() //will be reused later steps<br class="title-page-name"/>    println()<br class="title-page-name"/>    println("Training corpus summary:")<br class="title-page-name"/>    println("-------------------------------")<br class="title-page-name"/>    println("Training set size: " + actualCorpusSize + " documents")<br class="title-page-name"/>    println("Vocabulary size: " + actualVocabSize + " terms")<br class="title-page-name"/>    println("Number of tockens: " + actualNumTokens + " tokens")<br class="title-page-name"/>    println("Preprocessing time: " + preprocessElapsed + " sec")<br class="title-page-name"/>    println("-------------------------------")<br class="title-page-name"/>    println()<br class="title-page-name"/>    // Instantiate an LDA model<br class="title-page-name"/>    val lda = new LDA()<br class="title-page-name"/>    val optimizer = params.algorithm.toLowerCase match {<br class="title-page-name"/>      case "em" =&gt; new EMLDAOptimizer<br class="title-page-name"/>      // add (1.0 / actualCorpusSize) to MiniBatchFraction be more<br class="title-page-name"/>         robust on tiny datasets.<br class="title-page-name"/>     case "online" =&gt; new OnlineLDAOptimizer()<br class="title-page-name"/>                  .setMiniBatchFraction(0.05 + 1.0 / actualCorpusSize)<br class="title-page-name"/>      case _ =&gt; thrownew IllegalArgumentException("Only em, online are<br class="title-page-name"/>                             supported but got ${params.algorithm}.")<br class="title-page-name"/>    }<br class="title-page-name"/>    lda.setOptimizer(optimizer)<br class="title-page-name"/>      .setK(params.k)<br class="title-page-name"/>      .setMaxIterations(params.maxIterations)<br class="title-page-name"/>      .setDocConcentration(params.docConcentration)<br class="title-page-name"/>      .setTopicConcentration(params.topicConcentration)<br class="title-page-name"/>      .setCheckpointInterval(params.checkpointInterval)<br class="title-page-name"/>    if (params.checkpointDir.nonEmpty) {<br class="title-page-name"/>      spark.sparkContext.setCheckpointDir(params.checkpointDir.get)<br class="title-page-name"/>    }<br class="title-page-name"/>    val startTime = System.nanoTime()<br class="title-page-name"/>    //Start training the LDA model using the training corpus<br class="title-page-name"/>    val ldaModel = lda.run(corpus)<br class="title-page-name"/>    val elapsed = (System.nanoTime() - startTime) / 1e9<br class="title-page-name"/>    println("Finished training LDA model. Summary:")<br class="title-page-name"/>    println("Training time: " + elapsed + " sec")<br class="title-page-name"/>    if (ldaModel.isInstanceOf[DistributedLDAModel]) {<br class="title-page-name"/>      val distLDAModel = ldaModel.asInstanceOf[DistributedLDAModel]<br class="title-page-name"/>      val avgLogLikelihood = distLDAModel.logLikelihood /<br class="title-page-name"/>                             actualCorpusSize.toDouble<br class="title-page-name"/>      println("The average log likelihood of the training data: " +<br class="title-page-name"/>              avgLogLikelihood)<br class="title-page-name"/>      println()<br class="title-page-name"/>    }<br class="title-page-name"/>    // Print the topics, showing the top-weighted terms for each topic.<br class="title-page-name"/>    val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 10)<br class="title-page-name"/>    println(topicIndices.length)<br class="title-page-name"/>    val topics = topicIndices.map {case (terms, termWeights) =&gt;<br class="title-page-name"/>                 terms.zip(termWeights).map { case (term, weight) =&gt;<br class="title-page-name"/>                 (vocabArray(term.toInt), weight) } }<br class="title-page-name"/>    var sum = 0.0<br class="title-page-name"/>    println(s"${params.k} topics:")<br class="title-page-name"/>    topics.zipWithIndex.foreach {<br class="title-page-name"/>      case (topic, i) =&gt;<br class="title-page-name"/>      println(s"TOPIC $i")<br class="title-page-name"/>      println("------------------------------")<br class="title-page-name"/>      topic.foreach {<br class="title-page-name"/>        case (term, weight) =&gt;<br class="title-page-name"/>        term.replaceAll("\\s", "")<br class="title-page-name"/>        println(s"$term\t$weight")<br class="title-page-name"/>        sum = sum + weight<br class="title-page-name"/>      }<br class="title-page-name"/>      println("----------------------------")<br class="title-page-name"/>      println("weight: " + sum)<br class="title-page-name"/>      println()<br class="title-page-name"/>    }<br class="title-page-name"/>    spark.stop()<br class="title-page-name"/>  }<br class="title-page-name"/>  //Pre-processing of the raw texts<br class="title-page-name"/>import org.apache.spark.sql.functions._<br class="title-page-name"/>def preprocess(paths: String, vocabSize: Int, stopwordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {<br class="title-page-name"/>  import spark.implicits._<br class="title-page-name"/>  //Reading the Whole Text Files<br class="title-page-name"/>  val initialrdd = spark.sparkContext.wholeTextFiles(paths).map(_._2)<br class="title-page-name"/>  initialrdd.cache()<br class="title-page-name"/>  val rdd = initialrdd.mapPartitions { partition =&gt;<br class="title-page-name"/>    val morphology = new Morphology()<br class="title-page-name"/>    partition.map {value =&gt; helperForLDA.getLemmaText(value,<br class="title-page-name"/>                                                      morphology)}<br class="title-page-name"/>  }.map(helperForLDA.filterSpecialCharacters)<br class="title-page-name"/>    rdd.cache()<br class="title-page-name"/>    initialrdd.unpersist()<br class="title-page-name"/>    val df = rdd.toDF("docs")<br class="title-page-name"/>    df.show()<br class="title-page-name"/>    //Customizing the stop words<br class="title-page-name"/>    val customizedStopWords: Array[String] = if(stopwordFile.isEmpty) {<br class="title-page-name"/>      Array.empty[String]<br class="title-page-name"/>    } else {<br class="title-page-name"/>      val stopWordText = spark.sparkContext.textFile(stopwordFile)<br class="title-page-name"/>                            .collect()<br class="title-page-name"/>      stopWordText.flatMap(_.stripMargin.split(","))<br class="title-page-name"/>    }<br class="title-page-name"/>    //Tokenizing using the RegexTokenizer<br class="title-page-name"/>    val tokenizer = new RegexTokenizer().setInputCol("docs")<br class="title-page-name"/>                                       .setOutputCol("rawTokens")<br class="title-page-name"/>    //Removing the Stop-words using the Stop Words remover<br class="title-page-name"/>    val stopWordsRemover = new StopWordsRemover()<br class="title-page-name"/>                       .setInputCol("rawTokens").setOutputCol("tokens")<br class="title-page-name"/>    stopWordsRemover.setStopWords(stopWordsRemover.getStopWords ++<br class="title-page-name"/>                                  customizedStopWords)<br class="title-page-name"/>    //Converting the Tokens into the CountVector<br class="title-page-name"/>    val countVectorizer = new CountVectorizer().setVocabSize(vocabSize)<br class="title-page-name"/>                        .setInputCol("tokens").setOutputCol("features")<br class="title-page-name"/>    val pipeline = new Pipeline().setStages(Array(tokenizer,<br class="title-page-name"/>                                    stopWordsRemover, countVectorizer))<br class="title-page-name"/>    val model = pipeline.fit(df)<br class="title-page-name"/>    val documents = model.transform(df).select("features").rdd.map {<br class="title-page-name"/>      case Row(features: MLVector) =&gt; Vectors.fromML(features)<br class="title-page-name"/>    }.zipWithIndex().map(_.swap)<br class="title-page-name"/>    //Returning the vocabulary and tocken count pairs<br class="title-page-name"/>    (documents, model.stages(2).asInstanceOf[CountVectorizerModel]<br class="title-page-name"/>     .vocabulary, documents.map(_._2.numActives).sum().toLong)<br class="title-page-name"/>    }<br class="title-page-name"/>  }<br class="title-page-name"/>  object helperForLDA {<br class="title-page-name"/>    def filterSpecialCharacters(document: String) = <br class="title-page-name"/>      document.replaceAll("""[! @ # $ % ^ &amp; * ( ) _ + - − ,<br class="title-page-name"/>                          " ' ; : . ` ? --]""", " ")<br class="title-page-name"/>    def getLemmaText(document: String, morphology: Morphology) = {<br class="title-page-name"/>      val string = new StringBuilder()<br class="title-page-name"/>      val value =new Document(document).sentences().toList.flatMap{a =&gt;<br class="title-page-name"/>      val words = a.words().toList<br class="title-page-name"/>      val tags = a.posTags().toList<br class="title-page-name"/>      (words zip tags).toMap.map { a =&gt;<br class="title-page-name"/>        val newWord = morphology.lemma(a._1, a._2)<br class="title-page-name"/>        val addedWoed = if (newWord.length &gt; 3) {<br class="title-page-name"/>          newWord<br class="title-page-name"/>        } else { "" }<br class="title-page-name"/>        string.append(addedWoed + " ")<br class="title-page-name"/>      }<br class="title-page-name"/>    }<br class="title-page-name"/>    string.toString()<br class="title-page-name"/>  }<br class="title-page-name"/>}
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Scalability of LDA</h1>
                
            
            
                
<p class="mce-root">The previous example shows how to perform topic modeling using the LDA algorithm as a standalone application. The parallelization of LDA is not straightforward, and there have been many research papers proposing different strategies. The key obstacle in this regard is that all methods involve a large amount of communication. According to the blog on the Databricks website (<a href="https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html" class="calibre10">https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html</a>), here are the statistics of the dataset and related training and test sets that were used during the experimentation:</p>
<ul class="calibre9">
<li class="mce-root1">Training set size: 4.6 million documents</li>
<li class="mce-root1">Vocabulary size: 1.1 million terms</li>
<li class="mce-root1">Training set size: 1.1 billion tokens (~239 words/document)</li>
<li class="mce-root1">100 topics</li>
<li class="mce-root1">16-worker EC2 cluster, for example, M4.large or M3.medium depending upon budget and requirements</li>
</ul>
<p class="mce-root">For the preceding setting, the timing result was 176 secs/iteration on average over 10 iterations. From these statistics, it is clear that LDA is quite scalable for a very large number of the corpus as well.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, we provided theoretical and practical aspects of some advanced topics of machine learning with Spark. We also provided some recommendations about the best practice in machine learning. Following that, we have seen how to tune machine learning models for better and optimized performance using grid search, cross-validation, and hyperparameter tuning. In the later section, we have seen how to develop a scalable recommendation system using the ALS, which is an example of a model-based recommendation system using a model-based collaborative filtering approach. Finally, we have seen how to develop a topic modeling application as a text clustering technique.</p>
<p class="mce-root">For additional aspects and topics on machine learning best practice, interested readers can refer to the book titled <em class="calibre8">Large Scale Machine Learning with Spark</em> at <a href="https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark" class="calibre10">https://www.packtpub.com/big-data-and-business-intelligence/large-scale-machine-learning-spark.</a></p>
<p class="mce-root">In the next chapter, we will enter into more advanced use of Spark. Although we have discussed and provided a comparative analysis on binary and multiclass classification, we will get to know more about other multinomial classification algorithms with Spark such as Naive Bayes, decision trees, and the One-vs-Rest classifier.</p>


            

            
        
    </body></html>