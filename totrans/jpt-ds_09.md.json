["```py\n#install.packages(\"e1071\", repos=\"http://cran.r-project.org\") \nlibrary(e1071) \nlibrary(caret) \nset.seed(7317) \ndata(iris) \n```", "```py\nmodel <- naiveBayes(response ~ ., data=training) \nprediction <- predict(model, test, type=\"class\") \n```", "```py\ntrainingIndices <- createDataPartition(iris$Species, p=0.75, list=FALSE) \ntraining <- iris[trainingIndices,] \ntesting <- iris[-trainingIndices,] \nnrow(training) \nnrow(testing) \n114 \n36 \n```", "```py\nmodel <- naiveBayes(Species ~ ., data=training) \nmodel \n```", "```py\nprediction <- predict(model, testing, type=\"class\") \n```", "```py\nresults <- data.frame(testing$Species, prediction) \nresults[\"accurate\"] <- results['testing.Species'] == results['prediction'] \nnrow(results) \nnrow(results[results$accurate == TRUE,]) \n36 \n35 \n```", "```py\nfrom sklearn import datasets \nirisb = datasets.load_iris() \niris = irisb['data'] \niris.shape \n```", "```py\nfrom sklearn.naive_bayes import GaussianNB \ngnb = GaussianNB() \ny_pred = gnb.fit(irisb.data, irisb.target).predict(irisb.data) \n```", "```py\nprint(\"Number of errors out of a total %d points : %d\"  \n      % (irisb.data.shape[0],(irisb.target != y_pred).sum())) \nNumber of errors out of a total 150 points : 6\n```", "```py\nhousing <- read.table(\"http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\") \ncolnames(housing) <- c(\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PRATIO\", \"B\", \"LSTAT\", \"MDEV\") \nsummary(housing) \n```", "```py\nhousing <- housing[order(housing$MDEV),] \n```", "```py\n#install.packages(\"caret\") \nlibrary(caret) \nset.seed(5557) \nindices <- createDataPartition(housing$MDEV, p=0.75, list=FALSE) \ntraining <- housing[indices,] \ntesting <- housing[-indices,] \nnrow(training) \nnrow(testing) \n381 \n125 \n```", "```py\nlibrary(class) \nknnModel <- knn(train=training, test=testing, cl=training$MDEV) \nknnModel \n10.5 9.7 7 6.3 13.1 16.3 16.1 13.3 13.3... \n```", "```py\nplot(knnModel) \n```", "```py\npredicted <- read.table(\"housing-knn-predicted.csv\") \ncolnames(predicted) <- c(\"predicted\") \npredicted\n```", "```py\nresults <- data.frame(testing$MDEV, predicted) \n```", "```py\nresults[\"accuracy\"] <- results['testing.MDEV'] / results['predicted'] \nhead(results) \nmean(results$accuracy) \n1.01794816307793\n```", "```py\nfrom sklearn.neighbors import NearestNeighbors \nimport numpy as np \nimport pandas as pd \n```", "```py\nhousing = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n                       header=None, sep='\\s+') \nhousing.columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \\ \n\"DIS\", \"RAD\", \"TAX\", \"PRATIO\", \\ \n\"B\", \"LSTAT\", \"MDEV\"] \nhousing.head(5) \n```", "```py\nlen(housing) \n506 \n```", "```py\nmask = np.random.rand(len(housing)) < 0.8 \ntraining = housing[mask] \ntesting = housing[~mask] \nlen(training) \n417 \nlen(testing) \n89 \n```", "```py\nnbrs = NearestNeighbors().fit(housing) \n```", "```py\ndistances, indices = nbrs.kneighbors(housing) \nindices \narray([[  0, 241,  62,  81,   6], \n       [  1,  47,  49,  87,   2], \n       [  2,  85,  87,  84,   5], \n       ...,  \n       [503, 504, 219,  88, 217], \n       [504, 503, 219,  88, 217], \n       [505, 502, 504, 503,  91]], dtype=int32) \ndistances \narray([[  0\\.        ,  16.5628085 , 17.09498324,18.40127391, \n         19.10555821], \n       [  0\\.        ,  16.18433277, 20.59837827, 22.95753545, \n         23.05885288] \n       [  0\\.        ,  11.44014392, 15.34074743, 19.2322435 , \n         21.73264817], \n       ...,  \n       [  0\\.        ,   4.38093898,  9.44318468, 10.79865973, \n         11.95458848], \n       [  0\\.        ,   4.38093898,  8.88725757, 10.88003717, \n         11.15236419], \n       [  0\\.        ,   9.69512304, 13.73766871, 15.93946676, \n         15.94577477]]) \n```", "```py\nfrom sklearn.neighbors import KNeighborsRegressor \nknn = KNeighborsRegressor(n_neighbors=5) \nx_columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PRATIO\", \"B\", \"LSTAT\"] \ny_column = [\"MDEV\"] \nknn.fit(training[x_columns], training[y_column]) \nKNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', \n          metric_params=None, n_jobs=1, n_neighbors=5, p=2, \n          weights='uniform') \n```", "```py\npredictions = knn.predict(testing[x_columns]) predictions array([[ 20.62],\n [ 21.18], [ 23.96], [ 17.14], [ 17.24], [ 18.68], [ 28.88], \n```", "```py\ncolumns = [\"testing\",\"prediction\",\"diff\"] \nindex = range(len(testing)) \nresults = pd.DataFrame(index=index, columns=columns) \n\nresults['prediction'] = predictions \n\nresults = results.reset_index(drop=True) \ntesting = testing.reset_index(drop=True) \nresults['testing'] = testing[\"MDEV\"] \n\nresults['diff'] = results['testing'] - results['prediction'] \nresults['pct'] = results['diff'] / results['testing'] \nresults.mean() \ntesting       22.159551 \nprediction    22.931011 \ndiff          -0.771461 \npct           -0.099104 \n```", "```py\nlibrary(rpart) \nlibrary(caret) \nset.seed(3277) \n```", "```py\ncarmpg <- read.csv(\"car-mpg.csv\") \nindices <- createDataPartition(carmpg$mpg, p=0.75, list=FALSE) \ntraining <- carmpg[indices,] \ntesting <- carmpg[-indices,] \nnrow(training) \nnrow(testing) \n33 \n9 \n```", "```py\nfit <- rpart(mpg ~ cylinders + displacement + horsepower + weight + acceleration +\n             modelyear + maker, method=\"anova\", data=training) \nfit \nn= 33  \n\nnode), split, n, deviance, yval \n      * denotes terminal node \n\n1) root 33 26.727270 1.909091   \n2) weight>=3121.5 10  0.000000 1.000000 * \n3) weight< 3121.5 23 14.869570 2.304348   \n6) modelyear>=78.5 9  4.888889 1.888889 * \n7) modelyear< 78.5 14  7.428571 2.571429 * \n```", "```py\nplot(fit) \ntext(fit, use.n=TRUE, all=TRUE, cex=.5) \n```", "```py\npredicted <- predict(fit, newdata=testing) \npredicted \ntesting \n```", "```py\nimport pandas as pd \nimport numpy as np \nfrom os import system \nimport graphviz #pip install graphviz  \nfrom sklearn.cross_validation import train_test_split \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import accuracy_score \nfrom sklearn import tree \n```", "```py\ncarmpg = pd.read_csv(\"car-mpg.csv\") \ncarmpg.head(5) \n```", "```py\ncolumns = carmpg.columns \nmask = np.ones(columns.shape, dtype=bool) \ni = 0 #The specified column that you don't want to show \nmask[i] = 0 \nmask[7] = 0 #maker is a string \nX = carmpg[columns[mask]] \nY = carmpg[\"mpg\"] \n```", "```py\nX_train, X_test, y_train, y_test  \n= train_test_split( X, Y, test_size = 0.3,  \nrandom_state = 100) \n```", "```py\nclf_gini = tree.DecisionTreeClassifier(criterion = \"gini\",  \nrandom_state = 100, max_depth=3, min_samples_leaf=5) \n```", "```py\nclf_gini.fit(X_train, y_train) \nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3, \n            max_features=None, max_leaf_nodes=None, \n            min_impurity_split=1e-07, min_samples_leaf=5, \n            min_samples_split=2, min_weight_fraction_leaf=0.0, \n            presort=False, random_state=100, splitter='best') \n```", "```py\n#I could not get this to work on a Windows machine \n#dot_data = tree.export_graphviz(clf_gini, out_file=None,  \n#                         filled=True, rounded=True,   \n#                         special_characters=True)   \n#graph = graphviz.Source(dot_data)   \n#graph \n```", "```py\n#install.packages('neuralnet', repos=\"http://cran.r-project.org\") \nlibrary(\"neuralnet\") \n```", "```py\nfilename = \"http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\" \nhousing <- read.table(filename) \ncolnames(housing) <- c(\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\",  \n                       \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PRATIO\", \n                       \"B\", \"LSTAT\", \"MDEV\") \n```", "```py\nhousing <- housing[order(housing$MDEV),] \n#install.packages(\"caret\") \nlibrary(caret) \nset.seed(5557) \nindices <- createDataPartition(housing$MDEV, p=0.75, list=FALSE) \ntraining <- housing[indices,] \ntesting <- housing[-indices,] \nnrow(training) \nnrow(testing) \ntesting$MDEV \n```", "```py\nnnet <- neuralnet(MDEV ~ CRIM + ZN + INDUS + CHAS + NOX  \n                  + RM + AGE + DIS + RAD + TAX + PRATIO  \n                  + B + LSTAT, \n                  training, hidden=10, threshold=0.01) \nnnet \n```", "```py\n$call \nneuralnet(formula = MDEV ~ CRIM + ZN + INDUS + CHAS + NOX + RM +  \n    AGE + DIS + RAD + TAX + PRATIO + B + LSTAT, data = training,  \n    hidden = 10, threshold = 0.01) \n\n$response \n    MDEV \n399  5.0 \n406  5.0 \n... \n$covariate \n           [,1]  [,2]  [,3] [,4]   [,5]  [,6]  [,7]    [,8] [,9] [,10] [,11] \n  [1,] 38.35180   0.0 18.10    0 0.6930 5.453 100.0  1.4896   24   666  20.2 \n  [2,] 67.92080   0.0 18.10    0 0.6930 5.683 100.0  1.4254   24   666  20.2 \n  [3,]  9.91655   0.0 18.10    0 0.6930 5.852  77.8  1.5004   24   666  20.2 \n.... \n```", "```py\nplot(nnet, rep=\"best\") \n```", "```py\nresults <- compute(nnet, testing[,-14]) \ndiff <- results$net.result - testing$MDEV \nsum( (diff - mean(diff) )^2 ) #sum of squares \n9275.74672 \n```", "```py\ninstall.packages(\"randomForest\", repos=\"http://cran.r-project.org\") \nlibrary(randomForest) \n```", "```py\nfilename = \"http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\" \nhousing <- read.table(filename) \ncolnames(housing) <- c(\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\",  \n                       \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PRATIO\", \n                       \"B\", \"LSTAT\", \"MDEV\") \n```", "```py\nhousing <- housing[order(housing$MDEV),] \n#install.packages(\"caret\") \nlibrary(caret) \nset.seed(5557) \nindices <- createDataPartition(housing$MDEV, p=0.75, list=FALSE) \ntraining <- housing[indices,] \ntesting <- housing[-indices,] \nnrow(training) \nnrow(testing) \n```", "```py\nforestFit <- randomForest(MDEV ~ CRIM + ZN + INDUS + CHAS + NOX  \n                  + RM + AGE + DIS + RAD + TAX + PRATIO  \n                  + B + LSTAT, data=training) \nforestFit \nCall: \n randomForest(formula = MDEV ~ CRIM + ZN + INDUS + CHAS + NOX +      RM + AGE + DIS + RAD + TAX + PRATIO + B + LSTAT, data = training)  \n               Type of random forest: regression \n                     Number of trees: 500 \nNo. of variables tried at each split: 4 \n\n          Mean of squared residuals: 11.16163 \n                    % Var explained: 87.28 \n```", "```py\nforestPredict <- predict(forestFit, newdata=testing) \nSee how well the model worked: \ndiff <- forestPredict - testing$MDEV \nsum( (diff - mean(diff) )^2 ) #sum of squares \n1391.95553131418 \n```"]