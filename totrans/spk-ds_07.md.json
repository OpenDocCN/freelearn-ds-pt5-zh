["```py\n> R  // Start R shell  \n> Sys.getenv(\"SPARK_HOME\") //Confirm SPARK_HOME is set \n  <Your SPARK_HOME path> \n> library(SparkR, lib.loc = \n    c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"))) \n\nAttaching package: 'SparkR' \nThe following objects are masked from 'package:stats': \n\n    cov, filter, lag, na.omit, predict, sd, var, window \n\nThe following objects are masked from 'package:base': \n\n    as.data.frame, colnames, colnames<-, drop, endsWith, intersect, \n    rank, rbind, sample, startsWith, subset, summary, transform, union \n> \n\n> //Try help(package=SparkR) if you want to more information \n//initialize SparkSession object \n>  sparkR.session()  \nJava ref type org.apache.spark.sql.SparkSession id 1  \n> \nAlternatively, you may launch sparkR shell which comes with predefined SparkSession. \n\n> bin/sparkR  // Start SparkR shell  \n>      // For simplicity sake, no Log messages are shown here \n> //Try help(package=SparkR) if you want to more information \n> \n\n```", "```py\n> \n> //Open the shell \n> \n> //Try help(package=SparkR) if you want to more information \n> \n> df <- createDataFrame(iris) //Create a Spark DataFrame \n> df    //Check the type. Notice the column renaming using underscore \nSparkDataFrame[Sepal_Length:double, Sepal_Width:double, Petal_Length:double, Petal_Width:double, Species:string] \n> \n> showDF(df,4) //Print the contents of the Spark DataFrame \n+------------+-----------+------------+-----------+-------+ \n|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species| \n+------------+-----------+------------+-----------+-------+ \n|         5.1|        3.5|         1.4|        0.2| setosa| \n|         4.9|        3.0|         1.4|        0.2| setosa| \n|         4.7|        3.2|         1.3|        0.2| setosa| \n|         4.6|        3.1|         1.5|        0.2| setosa| \n+------------+-----------+------------+-----------+-------+ \n>  \n> head(df,2)  //Returns an R data.frame. Default 6 rows \n  Sepal_Length Sepal_Width Petal_Length Petal_Width Species \n1          5.1         3.5          1.4         0.2  setosa \n2          4.9         3.0          1.4         0.2  setosa \n> //You can use take(df,2) to get the same results \n//Check the dimensions \n> nrow(df) [1] 150 > ncol(df) [1] 5 \n\n```", "```py\n> \n> //Open the SparkR shell \n> df <- createDataFrame(iris) //Create a Spark DataFrame \n> class(df) [1] \"SparkDataFrame\" attr(,\"package\") [1] \"SparkR\" \n> df2 <- head(df,2) //Create an R data frame \n> class(df2) \n [1] \"data.frame\" \n> //Now try running some R command on both data frames \n> unique(df2$Species)   //Works fine as expected [1] \"setosa\" > unique(df$Species)    //Should fail Error in unique.default(df$Species) : unique() applies only to vectors > class(df$Species)   //Each column is a Spark's Column class [1] \"Column\" attr(,\"package\") [1] \"SparkR\" > class(df2$Species) [1] \"character\" \n\n```", "```py\n//First try in R environment, without loading sparkR \n//Try sampling from a column in an R data.frame \n>sample(iris$Sepal.Length,6,FALSE) //Returns any n elements [1] 5.1 4.9 4.7 4.6 5.0 5.4 >sample(head(iris),3,FALSE) //Returns any 3 columns \n//Try sampling from an R data.frame \n//The Boolean argument is for with_replacement \n> sample(head \n> head(sample(iris,3,TRUE)) //Returns any 3 columns\n  Species Species.1 Petal.Width\n1  setosa    setosa         0.2 \n2  setosa    setosa         0.2 \n3  setosa    setosa         0.2 \n4  setosa    setosa         0.2 \n5  setosa    setosa         0.2 \n6  setosa    setosa         0.4 \n\n//Load sparkR, initialize sparkSession and then execute this  \n> df <- createDataFrame(iris) //Create a Spark DataFrame \n> sample_df <- sample(df,TRUE,0.3) //Different signature \n> dim(sample_df)  //Different behavior [1] 44  5 \n> //Returned 30% of the original data frame and all columns \n> //Try with base prefix \n> head(base::sample(iris),3,FALSE)  //Call base package's sample\n  Species Petal.Width Petal.Length \n1  setosa         0.2          1.4\n2  setosa         0.2          1.4 \n3  setosa         0.2          1.3 \n4  setosa         0.2          1.5 \n5  setosa         0.2          1.4 \n6  setosa         0.4          1.7 \n\n```", "```py\n//Subsetting data examples \n> b1 <- createDataFrame(beaver1) \n//Get one column \n> b1$temp \nColumn temp    //Column class and not a vector \n> //Select some columns. You may use positions too \n> select(b1, c(\"day\",\"temp\")) \nSparkDataFrame[day:double, temp:double] \n>//Row subset based on conditions \n> head(subset(b1,b1$temp>37,select= c(2,3))) \n  time  temp \n1 1730 37.07 \n2 1740 37.05 \n3 1940 37.01 \n4 1950 37.10 \n5 2000 37.09 \n6 2010 37.02 \n> //Multiple conditions with AND and OR \n> head(subset(b1, between(b1$temp,c(36.0,37.0)) |  \n        b1$time %in% 900 & b1$activ == 1,c(2:4)),2) \n time  temp activ \n1  840 36.33     0 \n2  850 36.34     0 \n\n```", "```py\n//For example, try on a normal R data.frame \n> beaver1[2:4,] \n  day time  temp activ \n2 346  850 36.34     0 \n3 346  900 36.35     0 \n4 346  910 36.42     0 \n//Now, try on Spark Data frame \n> b1[2:4,] //Throws error \nExpressions other than filtering predicates are not supported in the first parameter of extract operator [ or subset() method. \n> \n\n```", "```py\n> //subset using Column operation using airquality dataset as df \n> head(subset(df,isNull(df$Ozone)),2) \n  Ozone Solar_R Wind Temp Month Day \n1    NA      NA 14.3   56     5   5 \n2    NA     194  8.6   69     5  10 \n> \n> //Add column and drop column examples \n> b1 <- createDataFrame(beaver1) \n\n//Add new column \n> b1$inRetreat <- otherwise(when(b1$activ == 0,\"No\"),\"Yes\") \n head(b1,2) \n  day time  temp activ inRetreat \n1 346  840 36.33     0        No \n2 346  850 36.34     0        No \n> \n//Drop a column.  \n> b1$day <- NULL \n> b1  // Example assumes b1$inRetreat does not exist \nSparkDataFrame[time:double, temp:double, activ:double] \n> //Drop columns using negative subscripts \n> b2 <- b1[,-c(1,4)]  > head(b2) \n   time  temp \n1  840 36.33 \n2  850 36.34 \n3  900 36.35 \n4  910 36.42 \n5  920 36.55 \n6  930 36.69 \n>  \n\n```", "```py\n> //GroupedData example using iris data as df \n> //Open SparkR shell and create df using iris dataset  \n> groupBy(df,\"Species\") \nGroupedData    //Returns GroupedData object \n> library(magrittr)  //Load the required library \n//Get group wise average sepal length \n//Report results sorted by species name \n>df2 <- df %>% groupBy(\"Species\") %>%  \n          avg(\"Sepal_Length\") %>%  \n          withColumnRenamed(\"avg(Sepal_Length)\",\"avg_sepal_len\") %>% \n          orderBy (\"Species\") \n//Format the computed double column \ndf2$avg_sepal_len <- format_number(df2$avg_sepal_len,2) \nshowDF(df2) \n+----------+-------------+ \n|   Species|avg_sepal_len| \n+----------+-------------+ \n|    setosa|         5.01| \n|versicolor|         5.94| \n| virginica|         6.59| \n+----------+-------------+ \n\n```", "```py\n> //Open the R shell and NOT SparkR shell  \n> library(dplyr,warn.conflicts=FALSE)  //Load dplyr first \n//Perform a common, useful operation  \n> iris %>%               \n+   group_by(Species) %>% +   summarise(avg_length = mean(Sepal.Length),  \n+             avg_width = mean(Sepal.Width)) %>% +   arrange(desc(avg_length)) \nSource: local data frame [3 x 3] \n     Species avg_length avg_width \n      (fctr)      (dbl)     (dbl) \n1  virginica      6.588     2.974 \n2 versicolor      5.936     2.770 \n3     setosa      5.006     3.428 \n\n//Remove from R environment \n> detach(\"package:dplyr\",unload=TRUE) \n\n```", "```py\n> //Open SparkR shell and create df using iris dataset  \n> collect(arrange(summarize(groupBy(df,df$Species),  +     avg_sepal_length = avg(df$Sepal_Length), +     avg_sepal_width = avg(df$Sepal_Width)), +     \"avg_sepal_length\", decreasing = TRUE))  \n     Species avg_sepal_length avg_sepal_width \n1     setosa            5.006           3.428 \n2 versicolor            5.936           2.770 \n3  virginica            6.588           2.974 \n\n```", "```py\n> //Register the Spark DataFrame as a table/View \n> createOrReplaceTempView(df,\"iris_vw\")  \n//Look at the table structure and some rows\n> collect(sql(sqlContext, \"SELECT * FROM iris_tbl LIMIT 5\"))\n    Sepal_Length Sepal_Width Petal_Length Petal_Width Species \n1          5.1         3.5          1.4         0.2  setosa \n2          4.9         3.0          1.4         0.2  setosa \n3          4.7         3.2          1.3         0.2  setosa \n4          4.6         3.1          1.5         0.2  setosa \n5          5.0         3.6          1.4         0.2  setosa \n> //Try out the above example using SQL syntax \n> collect(sql(sqlContext, \"SELECT Species,       avg(Sepal_Length) avg_sepal_length,      avg(Sepal_Width) avg_sepal_width       FROM iris_tbl        GROUP BY Species       ORDER BY avg_sepal_length desc\")) \n\n  Species avg_sepal_length avg_sepal_width \n\n1  virginica            6.588           2.974 \n2 versicolor            5.936           2.770 \n3     setosa            5.006           3.428 \n\n```", "```py\n> //Create b1 and b2 DataFrames using beaver1 and beaver2 datasets \n> b1 <- createDataFrame(beaver1) \n> b2 <- createDataFrame(beaver2) \n//Get individual and total counts \n> > c(nrow(b1), nrow(b2), nrow(b1) + nrow(b2)) \n[1] 114 100 214 \n//Try adding both data frames using union operation \n> nrow(unionAll(b1,b2)) \n[1] 214     //Sum of two datsets \n> //intersect example \n//Remove the first column (day) and find intersection \nshowDF(intersect(b1[,-c(1)],b2[,-c(1)])) \n\n+------+-----+-----+ \n|  time| temp|activ| \n+------+-----+-----+ \n|1100.0|36.89|  0.0| \n+------+-----+-----+ \n> //except (minus or A-B) is covered in machine learning examples   \n\n```", "```py\n> //Example illustrating data frames merging using R (Not SparkR) \n> //Create two data frames with a matching column \n//Products df with two rows and two columns \n> products_df <- data.frame(rbind(c(101,\"Product 1\"), \n                    c(102,\"Product 2\"))) \n> names(products_df) <- c(\"Prod_Id\",\"Product\") \n> products_df \n Prod_Id   Product \n1     101 Product 1 \n2     102 Product 2 \n\n//Sales df with sales for each product and month 24x3 \n> sales_df <- data.frame(cbind(rep(101:102,each=12), month.abb, \n                    sample(1:10,24,replace=T)*10)) \n> names(sales_df) <- c(\"Prod_Id\",\"Month\",\"Sales\") \n\n//Look at first 2 and last 2 rows in the sales_df \n> sales_df[c(1,2,23,24),] \n   Prod_Id Month Sales \n1      101   Jan    60 \n2      101   Feb    40 \n23     102   Nov    20 \n24     102   Dec   100 \n\n> //merge the data frames and examine the data \n> total_df <- merge(products_df,sales_df) \n//Look at the column names \n> colnames(total_df) \n> [1] \"Prod_Id\" \"Product\" \"Month\"   \"Sales\" \n\n//Look at first 2 and last 2 rows in the total_df \n> total_df[c(1,2,23,24),]     \n   Prod_Id   Product Month Sales \n1      101 Product 1   Jan    10 \n2      101 Product 1   Feb    20 \n23     102 Product 2   Nov    60 \n24     102 Product 2   Dec    10 \n\n```", "```py\n> //Example illustrating data frames merging using SparkR \n> //Create an R data frame first and then pass it on to Spark \n> //Watch out the base prefix for masked rbind function \n> products_df <- createDataFrame(data.frame( \n    base::rbind(c(101,\"Product 1\"), \n    c(102,\"Product 2\")))) \n> names(products_df) <- c(\"Prod_Id\",\"Product\") \n>showDF(products_df) \n+-------+---------+ \n|Prod_Id|  Product| \n+-------+---------+ \n|    101|Product 1| \n|    102|Product 2| \n+-------+---------+ \n> //Create Sales data frame \n> //Notice the as.data.frame similar to other R functions \n> //No cbind in SparkR so no need for base:: prefix \n> sales_df <- as.DataFrame(data.frame(cbind( \n             \"Prod_Id\" = rep(101:102,each=12), \n\"Month\" = month.abb, \n\"Sales\" = base::sample(1:10,24,replace=T)*10))) \n> //Check sales dataframe dimensions and some random rows  \n> dim(sales_df) \n[1] 24  3 \n> collect(sample(sales_df,FALSE,0.20)) \n  Prod_Id Month Sales \n1     101   Sep    50 \n2     101   Nov    80 \n3     102   Jan    90 \n4     102   Jul   100 \n5     102   Nov    20 \n6     102   Dec    50 \n> //Merge the data frames. The following merge is from SparkR library \n> total_df <- merge(products_df,sales_df) \n// You may try join function for the same purpose \n//Look at the columns in total_df \n> total_df \nSparkDataFrame[Prod_Id_x:string, Product:string, Prod_Id_y:string, Month:string, Sales:string] \n//Drop duplicate column \n> total_df$Prod_Id_y <- NULL    \n> head(total_df) \n  Prod_Id_x   Product Month Sales \n1       101 Product 1   Jan    40 \n2       101 Product 1   Feb    10 \n3       101 Product 1   Mar    90 \n4       101 Product 1   Apr    10 \n5       101 Product 1   May    50 \n6       101 Product 1   Jun    70 \n> //Note: As of Spark 2.0 version, SparkR does not support \n    row sub-setting  \n\n```", "```py\n//Example to train NaÃ¯ve Bayes model \n\n//Read file \n> myFile <- read.csv(\"../work/StudentsPassFail.csv\") //R data.frame \n> df <- createDataFrame(myFile) //sparkDataFrame \n//Look at the data \n> showDF(df,4) \n+---------+---------+----------+------+ \n|StudentId|Avg_Marks|Attendance|Result| \n+---------+---------+----------+------+ \n|     1001|     48.0|      Full|  Pass| \n|     1002|     21.0|    Enough|  Fail| \n|     1003|     24.0|    Enough|  Fail| \n|     1004|      4.0|      Poor|  Fail| \n+---------+---------+----------+------+ \n\n//Make three buckets out of Avg_marks \n// A >60; 40 < B < 60; C > 60 \n> df$marks_bkt <- otherwise(when(df$Avg_marks < 40, \"C\"), \n                           when(df$Avg_marks > 60, \"A\")) \n> df$marks_bkt <- otherwise(when(df$Avg_marks < 40, \"C\"), \n                           when(df$Avg_marks > 60, \"A\")) \n> df <- fillna(df,\"B\",cols=\"marks_bkt\") \n//Split train and test \n> trainDF <- sample(df,TRUE,0.7) \n> testDF <- except(df, trainDF) \n\n//Build model by supplying RFormula, training data \n> model <- spark.naiveBayes(Result ~ Attendance + marks_bkt, data = trainDF) \n> summary(model) \n$apriori \n          Fail      Pass \n[1,] 0.6956522 0.3043478 \n\n$tables \n     Attendance_Poor Attendance_Full marks_bkt_C marks_bkt_B \nFail 0.5882353       0.1764706       0.5882353   0.2941176   \nPass 0.125           0.875           0.125       0.625       \n\n//Run predictions on test data \n> predictions <- predict(model, newData= testDF) \n//Examine results \n> showDF(predictions[predictions$Result != predictions$prediction, \n     c(\"StudentId\",\"Attendance\",\"Avg_Marks\",\"marks_bkt\", \"Result\",\"prediction\")]) \n+---------+----------+---------+---------+------+----------+                     \n|StudentId|Attendance|Avg_Marks|marks_bkt|Result|prediction| \n+---------+----------+---------+---------+------+----------+ \n|     1010|      Full|     19.0|        C|  Fail|      Pass| \n|     1019|    Enough|     45.0|        B|  Fail|      Pass| \n|     1014|      Full|     12.0|        C|  Fail|      Pass| \n+---------+----------+---------+---------+------+----------+ \n//Note that the predictions are not exactly what we anticipate but models are usually not 100% accurate \n\n```", "```py\n> //Example illustrating Gaussian GLM model using SparkR \n> a <- createDataFrame(airquality) \n//Remove rows with missing values \n> b <- na.omit(a) \n> //Inspect the dropped rows with missing values \n> head(except(a,b),2)    //MINUS set operation \n  Ozone Solar_R Wind Temp Month Day \n1    NA     186  9.2   84     6   4 \n2    NA     291 14.9   91     7  14 \n\n> //Prepare train data and test data \ntraindata <- sample(b,FALSE,0.8) //Not base::sample \ntestdata <- except(b,traindata) \n\n> //Build model \n> model <- glm(Temp ~ Ozone + Solar_R + Wind,  \n          data = traindata, family = \"gaussian\") \n> // Get predictions \n> predictions <- predict(model, newData = testdata) \n> head(predictions[,c(predictions$Temp, predictions$prediction)], \n                 5) \n  Temp prediction \n1   90   81.84338 \n2   79   80.99255 \n3   88   85.25601 \n4   87   76.99957 \n5   76   71.75683 \n\n```"]