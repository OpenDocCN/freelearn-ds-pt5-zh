<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 9. Growing Trees"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch09" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 9. Growing Trees</h1></div></div></div><p class="calibre11">In this chapter, we will cover the following recipes:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Going from trees to forest – Random Forest</li><li class="listitem">Growing extremely randomized Trees</li><li class="listitem">Growing a rotation forest</li></ul></div><div class="calibre2" title="Introduction"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch09lvl1sec81" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Introduction</h1></div></div></div><p class="calibre11">In this chapter, we will see some more Bagging methods based on tree-based algorithms. Due to their robustness against noise and universal applicability to a variety of problems, they are very popular among the data science community.</p><p class="calibre11">The claim to fame for most of these methods is that they can obtain very good results with zero data preparation compared to other methods, and they can be provided as black box tools in the hands of software engineers.</p><p class="calibre11">Other than the tall claims made in the previous paragraphs, there are some other advantages as well. </p><p class="calibre11">By design, bagging lends itself nicely to parallelization. Hence, these methods can be easily applied on a very large dataset in a cluster environment.</p><p class="calibre11">Decision Tree algorithms split the input data into various regions at each level of the tree. Thus, they perform implicit feature selection. Feature selection is one of the most important tasks in building a good model. By providing implicit feature selection, Decision Trees are in an advantageous position compared to other techniques. Hence, Bagging with Decision Trees comes with this advantage.</p><p class="calibre11">Almost no data preparation is needed for decision trees. For example, consider scaling of attributes. The attribute scale has no impact on the structure of the decision trees. Moreover, missing values do not affect decision trees. The effect of outliers too is very minimal on a Decision Tree.</p><p class="calibre11">In some of our earlier recipes, we had used Polynomial features retaining only the interaction components. With an ensemble of trees, these interactions are taken care of. We don't have to make explicit feature transformations to accommodate feature interactions.</p><p class="calibre11">Linear Regression-based models fail in the case of the existence of a non-linear relationship in the input data. We saw this effect when we explained the Kernel PCA recipes. Tree-based algorithms are not affected by a non-linear relationship in the data.</p><p class="calibre11">One of the major complaints against the Tree-based method is the difficulty with pruning of trees to avoid overfitting. Big trees tend to fit the noise present in the underlying data as well, and hence, lead to a low bias and high variance. However, when we grow a lot of trees, and the final prediction is an average of the output of all the trees in the ensemble, we avoid the problem of variance.</p><p class="calibre11">In this chapter, we will see three tree-based ensemble methods.</p><p class="calibre11">Our first recipe is about implementing Random Forests for a classification problem. Leo Breiman is the inventor of this algorithm. The Random Forest is an ensemble technique which leverages a lot of trees internally to produce a model for solving any regression or classification problems.</p><p class="calibre11">Our second recipe is about Extremely Randomized trees, an algorithm which varies in a very small way from Random Forests. By introducing more randomization in its procedure as compared to a Random Forest, it claims to address the variance problem more effectively. Moreover, it has a slightly reduced computational complexity.</p><p class="calibre11">Our final recipe is about Rotation Forests. The first two recipes require a large number of trees to be a part of their ensemble for achieving good performance. Rotation forest claim that they can achieve similar or better performance with a fewer number of trees. Furthermore, the authors of this algorithm claim that the underlying estimator can be anything other than a tree. In this way, it is projected as a new framework for building an ensemble similar to Gradient Boosting.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Going from trees to Forest – Random Forest"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch09lvl1sec82" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Going from trees to Forest – Random Forest</h1></div></div></div><p class="calibre11">The Random forest <a id="id709" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>method builds a lot of trees (forest) which are uncorrelated to each other. Given a classification or a regression problem, the method proceeds to build a lot of trees, and the final prediction is either the average of predictions from the entire forest for regression or a majority vote classification.</p><p class="calibre11">This should remind you of Bagging. Random Forests is yet another Bagging methodology. The fundamental idea behind bagging is to use a lot of noisy estimators, handling the noise by averaging, and hence reducing the variance in the final output. Trees are highly affected by even a very small noise in the training dataset.  Hence, being a noisy estimator, they are an ideal candidate for Bagging.</p><p class="calibre11">Let us write down the steps involved in building a Random Forest. The number of trees required in the forest is a parameter specified by the user. Let T be the number of trees required to be<a id="id710" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> built:</p><p class="calibre11">We start with iterating from 1 through T, that is, we build T trees:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">For each tree, draw a bootstrap sample of size D from our input dataset.</li><li class="listitem">We proceed to fit a tree t to the input data:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Randomly select m attributes.</li><li class="listitem">Pick the best attribute to use as a splitting variable using a predefined criterion.</li><li class="listitem">Split the data set into two. Remember, trees are binary in nature. At each level of the tree, the input dataset is split into two.</li><li class="listitem">We proceed to do the preceding three steps recursively on the dataset that we split.</li></ul></div></li><li class="listitem">Finally, we return T trees.</li></ul></div><p class="calibre11">To make a prediction on a new instance, we take a majority vote amongst all the trees in T for a classification; for regression, we take the average value returned by each tree t in T.</p><p class="calibre11">We said earlier that a Random Forest builds non-correlated trees. Let's see how the various trees in the ensemble are not correlated to each other. By taking a bootstrap sample from the dataset for each tree, we ensure that different parts of the data are presented to different trees. This way, each tree tries to model different characteristics of the dataset. Hence, we stick to the ensemble rule of introducing variation in the underlying estimators. But this does not guarantee complete non correlation between the underlying trees. When we do the node splitting, we don't select all attributes; rather, we randomly select a subset of attributes. In this manner, we try to ensure that our trees are not correlated to each other.</p><p class="calibre11">Compared to Boosting, where our ensemble of estimators were weak classifiers, in a Random Forest, we build trees with maximum depth so that they fit the bootstrapped sample perfectly leading to a low bias. The consequence is the introduction of high variance. However, by building a large number of trees and using the averaging principle for the final prediction, we hope to tackle this variance problem.</p><p class="calibre11">Let us proceed to jump into our recipe for a Random Forest.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec292" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">We are going to generate some classification datasets to demonstrate a Random Forest Algorithm. We will leverage scikit-learn's implementation of a Random Forest from the ensemble module.</p></div><div class="calibre2" title="How to do it..."><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec293" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it...</h2></div></div></div><p class="calibre11">We will start <a id="id711" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with loading all the necessary libraries. Let us leverage the <code class="literal">make_classification</code> method from the <code class="literal">sklearn.dataset</code> module for generating the training data to demonstrate a Random Forest:</p><div class="calibre2"><pre class="programlisting">from sklearn.datasets import make_classification
from sklearn.metrics import classification_report, accuracy_score
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.grid_search import RandomizedSearchCV
from operator import itemgetter

import numpy as np

def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y</pre></div><p class="calibre11">We will now write the function <code class="literal">build_forest</code> to build fully grown trees and proceed to evaluate the forest's performance. Then we will write the methods which can be used to search the optimal parameters for our forest:</p><div class="calibre2"><pre class="programlisting">def build_forest(x,y,x_dev,y_dev):
    """
    Build a random forest of fully grown trees
    and evaluate peformance
    """
    no_trees = 100
    estimator = RandomForestClassifier(n_estimators=no_trees)
    estimator.fit(x,y)
    
    train_predcited = estimator.predict(x)
    train_score = accuracy_score(y,train_predcited)
    dev_predicted = estimator.predict(x_dev)
    dev_score = accuracy_score(y_dev,dev_predicted)
    
    print "Training Accuracy = %0.2f Dev Accuracy = %0.2f"%(train_score,dev_score)
    
     
def search_parameters(x,y,x_dev,y_dev):
    """
    Search the parameters of random forest algorithm
    """
    estimator = RandomForestClassifier()
    no_features = x.shape[1]
    no_iterations = 20
    sqr_no_features = int(np.sqrt(no_features))

    parameters = {"n_estimators"      : np.random.randint(75,200,no_iterations),
                 "criterion"         : ["gini", "entropy"],
                 "max_features"      : [sqr_no_features,sqr_no_features*2,sqr_no_features*3,sqr_no_features+10]
                 }

    grid = RandomizedSearchCV(estimator=estimator,param_distributions=parameters,\
    verbose=1, n_iter=no_iterations,random_state=77,n_jobs=-1,cv=5)
    grid.fit(x,y)
    print_model_worth(grid,x_dev,y_dev)

    return grid.best_estimator_

   
def print_model_worth(grid,x_dev,y_dev):    
    # Print the goodness of the models
    # We take the top 5 models
    scores = sorted(grid.grid_scores_, key=itemgetter(1), reverse=True) [0:5]
    
    for model_no,score in enumerate(scores):
        print "Model %d, Score = %0.3f"%(model_no+1,score.mean_validation_score)
        print "Parameters = {0}".format(score.parameters)
    print
    dev_predicted = grid.predict(x_dev)
    
    print classification_report(y_dev,dev_predicted)</pre></div><p class="calibre11">Finally, we write <a id="id712" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a main function for invoking the functions that we have defined previously:</p><div class="calibre2"><pre class="programlisting">if __name__ == "__main__":
    x,y = get_data()    

    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
        
    build_forest(x_train,y_train,x_dev,y_dev)
    model = search_parameters(x,y,x_dev,y_dev)
    get_feature_importance(model)</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec294" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let us start with our main function. We invoke get_data to get our predictor attributes <code class="literal">x</code> and the response attributes <code class="literal">y</code>. Inside <code class="literal">get_data</code>, we leverage the <code class="literal">make_classification</code> dataset to generate our training data for Random Forest:</p><div class="calibre2"><pre class="programlisting">def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y</pre></div><p class="calibre11">Let us look at the parameters passed to the <code class="literal">make_classification</code> method. The first parameter is the <a id="id713" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of instances required; in this case, we say we need 500 instances. The second parameter is about the number of attributes required per instance. We say that we need 30. The third parameter, <code class="literal">flip_y</code>, randomly interchanges 3 percent of the instances. This is done to introduce some noise in to our data. The next parameter specifies the number of features out of those 30 features, which should be informative enough to be used in our classification. We have specified that 60 percent of our features, that is, 18 out of 30 should be informative. The next parameter is about the redundant features. These are generated as a linear combination of the informative features in order to introduce a correlation among the features. Finally, repeated features are the duplicate features, which are drawn randomly from both informative features and redundant features.</p><p class="calibre11">Let us split the data into the training and testing set using <code class="literal">train_test_split</code>. We reserve 30 percent of our data for testing:</p><div class="calibre2"><pre class="programlisting">    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)</pre></div><p class="calibre11">Once again, we leverage <code class="literal">train_test_split</code> to split our test data into dev and test:</p><div class="calibre2"><pre class="programlisting">    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)</pre></div><p class="calibre11">With the data divided for building, evaluating, and testing the model, we proceed to build our models:</p><div class="calibre2"><pre class="programlisting">build_forest(x_train,y_train,x_dev,y_dev)</pre></div><p class="calibre11">We invoke the <code class="literal">build_forest</code> function with our training and dev data to build the random forest model. Let us look inside that function:</p><div class="calibre2"><pre class="programlisting">    no_trees = 100
    estimator = RandomForestClassifier(n_estimators=no_trees)
    estimator.fit(x,y)
    
    train_predcited = estimator.predict(x)
    train_score = accuracy_score(y,train_predcited)
    dev_predicted = estimator.predict(x_dev)
    dev_score = accuracy_score(y_dev,dev_predicted)
    
    print "Training Accuracy = %0.2f Dev Accuracy = %0.2f"%(train_score,dev_score)</pre></div><p class="calibre11">We need 100 trees in our ensemble, so we use the variable <code class="literal">no_trees</code> to define the number of trees. We leverage the <code class="literal">RandomForestClassifier</code> class from scikit-learn check and apply throughout. As you can see, we pass the <a id="id714" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of trees required as a parameter. We then proceed to fit our model.</p><p class="calibre11">Now let us find the model accuracy score for our train and dev data:</p><div class="mediaobject"><img src="Images/B04041_09_01.jpg" alt="How it works…" class="calibre206"/></div><p class="calibre11">Not bad! We have achieved 83 percent accuracy on our dev set. Let us see if we can improve our scores. There are other parameters to the forest which can be tuned to get a better model. For the<a id="id715" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> list of parameters which can be tuned, refer to the following link:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>
</p><p class="calibre11">We invoke the function <code class="literal">search_parameters</code> with the training and dev data to tune the various parameters for our Random Forest model.</p><p class="calibre11">In some of the previous recipes, we used GridSearchCV to search through the parameter space for finding the best parameter combination. GridSearchCV performs a very exhaustive search. However, in this recipe we are going to use RandomizedSearchCV. We provide a distribution of parameter values for each parameter, and specify the number of iterations needed. For each iteration, RandomizedSearchCV will pick a sample value from the parameter distribution and fit the model:</p><div class="calibre2"><pre class="programlisting">parameters = {"n_estimators" : np.random.randint(75,200,no_iterations),
"criterion" : ["gini", "entropy"],
"max_features" : [sqr_no_features,sqr_no_features*2,sqr_no_features*3,sqr_no_features+10]
}</pre></div><p class="calibre11">We provide a dictionary of parameters as we did in GridSearchCV. In our case, we want to experiment with three parameters.</p><p class="calibre11">The first one is the number of trees in the model, represented by the <code class="literal">n_estimators</code> parameter. By invoking the randint function, we get a list of integers between 75 and 200. The size of the trees is defined by <code class="literal">no_iterations</code> parameters:</p><div class="calibre2"><pre class="programlisting">no_iterations = 20</pre></div><p class="calibre11">This is the parameter we will pass to RandomizedSearchCV for the number of iterations we want to perform. From this array of <code class="literal">20</code> elements, RandomizedSearchCV will sample a single value for each iteration.</p><p class="calibre11">Our next parameter is the criterion, we pick randomly between gini and entropy, and use that as a criterion for splitting the nodes during each iteration.</p><p class="calibre11">The most important parameter, <code class="literal">max_features</code>, defines the number of features that the algorithm<a id="id716" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> should pick during the splitting of each node.  In our pseudocode for describing the Random Forest, we have specified that we need to pick m attributes randomly during each split of the node. The parameter <code class="literal">max_features</code> defines that m. Here we give a list of four values. The variable <code class="literal">sqr_no_features</code> is the square root of the number of attributes available in the input dataset:</p><div class="calibre2"><pre class="programlisting">sqr_no_features = int(np.sqrt(no_features))</pre></div><p class="calibre11">Other values in that list are some variations of the square root.</p><p class="calibre11">Let us instantiate RandomizedSearchCV with this parameter distribution:</p><div class="calibre2"><pre class="programlisting">grid = RandomizedSearchCV(estimator=estimator,param_distributions=parameters,\
verbose=1, n_iter=no_iterations,random_state=77,n_jobs=-1,cv=5)</pre></div><p class="calibre11">The first parameter is the underlying estimator whose parameters we are trying to optimize. It's our <code class="literal">RandomForestClassifier</code>:</p><div class="calibre2"><pre class="programlisting">estimator = RandomForestClassifier()</pre></div><p class="calibre11">The second parameter, <code class="literal">param_distributions</code> is the distribution defined by the dictionary parameters. We define the number of iterations, that is, the number of times we want to run the RandomForestClassifier using the parameter <code class="literal">n_iter</code>. With the <code class="literal">cv</code> parameter, we specify the number of cross validations required <code class="literal">5</code> cross validations in our case.</p><p class="calibre11">Let us proceed to fit the model, and see how well the model has turned out:</p><div class="calibre2"><pre class="programlisting">grid.fit(x,y)
print_model_worth(grid,x_dev,y_dev)</pre></div><div class="mediaobject"><img src="Images/B04041_09_02.jpg" alt="How it works…" class="calibre207"/></div><p class="calibre11">As you can see, we have five folds, that is, we want to do a five-fold cross validation on each of our iterations. We have a total of <code class="literal">20</code> iterations, and hence, we will be building 100 models.</p><p class="calibre11">Let us look inside the function <code class="literal">print_model_worth</code>. We pass our grid object and dev dataset to this function. The grid object stores the evaluation metric for each of the models it builds inside an attribute called the <code class="literal">grid_scores_ of type</code> list. Let us sort this list in the descending <a id="id717" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>order to build the best model:</p><div class="calibre2"><pre class="programlisting">scores = sorted(grid.grid_scores_, key=itemgetter(1), reverse=True) [0:5]</pre></div><p class="calibre11">We select the top five models as you can see from the indexing. We proceed to print the details of those models:</p><div class="calibre2"><pre class="programlisting">for model_no,score in enumerate(scores):
print "Model %d, Score = %0.3f"%(model_no+1,score.mean_validation_score)
print "Parameters = {0}".format(score.parameters)
print</pre></div><p class="calibre11">We first print the evaluation score and follow it with the parameters of the model:</p><div class="mediaobject"><img src="Images/B04041_09_03.jpg" alt="How it works…" class="calibre208"/></div><p class="calibre11">We have ranked the modes by their scores in the descending order thus showing the best model parameters in the beginning. We will choose these parameters as our model parameters. The attribute <code class="literal">best_estimator_ will</code> return the model with these parameters.</p><p class="calibre11">Let us use these parameters and test our dev data:</p><div class="calibre2"><pre class="programlisting">dev_predicted = grid.predict(x_dev)
print classification_report(y_dev,dev_predicted)</pre></div><p class="calibre11">The predict function will use <code class="literal">best_estimtor</code> internally:</p><div class="mediaobject"><img src="Images/B04041_09_04.jpg" alt="How it works…" class="calibre209"/></div><p class="calibre11">Great! We have a perfect model with a classification accuracy of 100 percent.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec295" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">Internally, the <code class="literal">RandomForestClassifier</code> uses the <code class="literal">DecisionTreeClassifier</code>. Refer to the following link<a id="id718" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> for all the parameters that are passed for building a <a id="id719" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>decision tree:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a>
</p><p class="calibre11">One parameter that is of some interest to us is splitter. The default value of splitter is set to best. Based on the <code class="literal">max_features</code> attribute, the implementation will choose the splitting mechanism internally. The available splitting mechanisms include the following: </p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">best: Chooses the best possible split from the given set of attributes defined by the <code class="literal">max_features</code> parameter</li><li class="listitem">random: Randomly chooses a splitting attribute</li></ul></div><p class="calibre11">You would have noticed that this parameter is not available while instantiating a <code class="literal">RandomForestClassifier</code>. The only way to control is to give a value to the <code class="literal">max_features</code> parameter which is less than the number of attributes available in the dataset.</p><p class="calibre11">In the industry, Random Forests are extensively used for variable selection. In Scikit learn, variable importance is calculated using gini impurity. Both the gini and entropy criteria used for node splitting identify the best attribute for splitting the node by its ability to split the dataset into subsets with high impurity so that subsequent splitting leads to good classification. The importance of a variable is decided by the amount of impurity it can induce into the split dataset. Refer to the following book for more details:</p><div class="calibre2"><blockquote class="blockquote"><p class="calibre17"><span class="strong1"><em class="calibre15">Breiman, Friedman, "Classification and regression trees", 1984.</em></span>
</p></blockquote></div><p class="calibre11">We can write a small function to print the important features:</p><div class="calibre2"><pre class="programlisting">def get_feature_importance(model):
    feature_importance = model.feature_importances_
    fm_with_id = [(i,importance) for i,importance in enumerate(feature_importance)]
    fm_with_id = sorted(fm_with_id, key=itemgetter(1),reverse=True)[0:10]
    print "Top 10 Features"
    for importance in fm_with_id:
        print "Feature %d importance = %0.3f"%(importance[0],importance[1])
    print</pre></div><p class="calibre11">A Random Forest<a id="id720" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> object has a variable called <code class="literal">feature_importances_</code>. We use this variable and create a list of tuples with the feature number and importance:</p><div class="calibre2"><pre class="programlisting">    feature_importance = model.feature_importances_
    fm_with_id = [(i,importance) for i,importance in enumerate(feature_importance)]</pre></div><p class="calibre11">We proceed to sort it in descending order of importance, and select only the top 10 features:</p><div class="calibre2"><pre class="programlisting">    fm_with_id = sorted(fm_with_id, key=itemgetter(1),reverse=True)[0:10]</pre></div><p class="calibre11">We then print the top 10 features:</p><div class="mediaobject"><img src="Images/B04041_09_05.jpg" alt="There's more…" class="calibre210"/></div><p class="calibre11">Another interesting aspect of Random Forests is the <code class="literal">Out-of-Bag estimation (OOB)</code>. Remember that we<a id="id721" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> bootstrap from the dataset initially for every tree grown in the forest. Because of bootstrapping, some records will not be used in some trees. Let us say record 1 is used in 100 trees and not used in 150 trees in our forest. We can then use those 150 trees to predict the class label for that record to figure out the classification error for that record. Out-of-bag estimation can be used to effectively assess the quality of our forest. The following URL gives an example of how the <a id="id722" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>OOB can be used effectively:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/dev/auto_examples/ensemble/plot_ensemble_oob.html">http://scikit-learn.org/dev/auto_examples/ensemble/plot_ensemble_oob.html</a>
</p><p class="calibre11">The RandomForestClassifier class in Scikit learn is derived from <code class="literal">ForestClassifier</code>.The source code <a id="id723" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>for the same can be found at the following link:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/scikit-learn/scikit-learn/blob/a95203b/sklearn/ensemble/forest.py#L318">https://github.com/scikit-learn/scikit-learn/blob/a95203b/sklearn/ensemble/forest.py#L318</a>
</p><p class="calibre11">When we call the predict method in RandomForestClassifier, it internally calls the <code class="literal">predict_proba</code> method defined in ForestClassifier. Here, the final prediction is done not on the basis of <a id="id724" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>voting but by averaging the probabilities for each of the classes from different trees inside the forest and deciding the final class based on the highest probability.</p><p class="calibre11">The original paper by Leo Breiman on Random Forests is available for download at the following link:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://link.springer.com/article/10.1023%2FA%3A1010933404324">http://link.springer.com/article/10.1023%2FA%3A1010933404324</a>
</p><p class="calibre11">You can also refer to the website maintained by Leo Breiman and Adele Cutler:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a>
</p></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec296" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Building Decision Trees to solve Multi Class Problems</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6. Machine Learning 1">Chapter 6</a>, <span class="strong1"><em class="calibre15">Machine Learning I</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Understanding Ensemble, Gradient Boosting</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch08.xhtml" title="Chapter 8. Ensemble Methods">Chapter 8</a>, <span class="strong1"><em class="calibre15">Model Selection and Evaluation</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Understanding Ensemble, Bagging Method</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch08.xhtml" title="Chapter 8. Ensemble Methods">Chapter 8</a>, <span class="strong1"><em class="calibre15">Model Selection and Evaluation</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Growing Extremely Randomized Trees"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch09lvl1sec83" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Growing Extremely Randomized Trees</h1></div></div></div><p class="calibre11">Extremely Randomized Trees, also known as the Extra trees algorithm differs from the Random Forest<a id="id725" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> described in the previous recipe in two ways:</p><div class="calibre2"><ol class="orderedlist"><li class="listitem1">It does not use bootstrapping to select instances for every tree in the ensemble; instead, it uses the complete training dataset.</li><li class="listitem1">Given K as the number of attributes to be randomly selected at a given node, it selects a random cut-point without considering the target variable.</li></ol></div><p class="calibre11">As you saw in<a id="id726" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the previous recipe, Random Forests used randomization in two places. First, in selecting the instances to be used for the training trees in the forest; bootstrap was used to select the training instances. Secondly, at every node a random set of attributes were selected. One attribute among them was selected based on either the gini impurity or entropy criterion. Extremely randomized trees go one step further and select the splitting attribute randomly.</p><p class="calibre11">Extremely Randomized Trees were proposed in the following paper:</p><p class="calibre11">
<span class="strong1"><em class="calibre15">P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees", Machine Learning, 63(1), 3-42, 2006</em></span>.</p><p class="calibre11">According to this paper, there are two aspects, other than the technical aspects listed earlier, which make an Extremely Randomized Tree more suitable:</p><p class="calibre11">The rationale behind the Extra-Trees method is that the explicit randomization of the cut-point and attribute combined with ensemble averaging should be able to reduce variance more strongly than the weaker randomization schemes used by other methods.</p><p class="calibre11">Compared to a<a id="id727" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> Random Forest, randomization of the cut-point ( the attribute selected to split the dataset at each node) combined with the randomization of cut-point, that is, ignoring any criteria, and finally, averaging the results from each of the tree, will result in a much superior performance on an unknown dataset.</p><p class="calibre11">The second advantage is regarding the compute complexity:</p><p class="calibre11">From the computational point of view, the complexity of the tree growing procedure is, assuming balanced trees, on the order of N log N with respect to learning the sample size, like most other tree growing procedures. However, given the simplicity of the node splitting procedure we expect the constant factor to be much smaller than in other ensemble based methods which locally optimize cut-points</p><p class="calibre11">Since no computation time is spent in identifying the best attribute to split, this method is more computationally efficient than Random Forests.</p><p class="calibre11">Let us write down the steps involved in building Extremely Random trees. The number of trees required in the forest is typically specified by the user. Let T be the number of trees required to be built.</p><p class="calibre11">We start with iterating from 1 through T, that is, we build T trees:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">For each tree, we select the complete input dataset.</li><li class="listitem">We then proceed to fit a tree t to the input data:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Select m attributes randomly.</li><li class="listitem">Pick an attribute randomly as the splitting variable.</li><li class="listitem">Split the data set into two. Remember that trees are binary in nature. At each level of the tree, the input dataset is split into two.</li><li class="listitem">Perform the preceding three steps recursively on the dataset that we split.</li></ul></div></li><li class="listitem">Finally, we return T trees.</li><li class="listitem">Let us take a look at the recipe for Extremely Randomized Trees.</li></ul></div><div class="calibre2" title="Getting ready…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec297" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready…</h2></div></div></div><p class="calibre11">We are<a id="id728" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> going to generate some classification datasets to demonstrate Extremely Randomized Trees. For that, we will leverage Scikit Learn's implementation of the Extremely Randomized Trees ensemble module.</p></div><div class="calibre2" title="How to do it..."><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec298" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it...</h2></div></div></div><p class="calibre11">We start by loading all the necessary libraries. Let us leverage the <code class="literal">make_classification</code> method from the <code class="literal">sklearn.dataset</code> module to generate the training data:</p><div class="calibre2"><pre class="programlisting">
<code class="literal">from sklearn.datasets import make_classification</code>
<code class="literal">from sklearn.metrics import classification_report, accuracy_score</code>
<code class="literal">from sklearn.cross_validation import train_test_split, cross_val_score</code>
<code class="literal">from sklearn.ensemble import ExtraTreesClassifier</code>
<code class="literal">from sklearn.grid_search import RandomizedSearchCV</code>
<code class="literal">from operator import itemgetter</code>

<code class="literal">def get_data():</code>
<code class="literal">    """</code>
<code class="literal">    Make a sample classification dataset</code>
<code class="literal">    Returns : Independent variable y, dependent variable x</code>
<code class="literal">    """</code>
<code class="literal">    no_features = 30</code>
<code class="literal">    redundant_features = int(0.1*no_features)</code>
<code class="literal">    informative_features = int(0.6*no_features)</code>
<code class="literal">    repeated_features = int(0.1*no_features)</code>
<code class="literal">    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\</code>
<code class="literal">            n_informative = informative_features, n_redundant = redundant_features \</code>
<code class="literal">            ,n_repeated = repeated_features,random_state=7)</code>
    return x,y</pre></div><p class="calibre11">We write the function <code class="literal">build_forest</code>, where we will build fully grown trees, and proceed to evaluate the forest's performance:</p><div class="calibre2"><pre class="programlisting">def build_forest(x,y,x_dev,y_dev):
    """
    Build a Extremely random tress
    and evaluate peformance
    """
    no_trees = 100
    estimator = ExtraTreesClassifier(n_estimators=no_trees,random_state=51)
    estimator.fit(x,y)
    
    train_predcited = estimator.predict(x)
    train_score = accuracy_score(y,train_predcited)
    dev_predicted = estimator.predict(x_dev)
    dev_score = accuracy_score(y_dev,dev_predicted)
    
    print "Training Accuracy = %0.2f Dev Accuracy = %0.2f"%(train_score,dev_score)
    print "cross validated score"
    print cross_val_score(estimator,x_dev,y_dev,cv=5)

def search_parameters(x,y,x_dev,y_dev):
    """
    Search the parameters 
    """
    estimator = ExtraTreesClassifier()
    no_features = x.shape[1]
    no_iterations = 20
    sqr_no_features = int(np.sqrt(no_features))

    parameters = {"n_estimators"      : np.random.randint(75,200,no_iterations),
                 "criterion"         : ["gini", "entropy"],
                 "max_features"      : [sqr_no_features,sqr_no_features*2,sqr_no_features*3,sqr_no_features+10]
                 }

    grid = RandomizedSearchCV(estimator=estimator,param_distributions=parameters,\
    verbose=1, n_iter=no_iterations,random_state=77,n_jobs=-1,cv=5)
    grid.fit(x,y)
    print_model_worth(grid,x_dev,y_dev)
    
    return grid.best_estimator_</pre></div><p class="calibre11">Finally, we write a main function for invoking the functions that we have defined:</p><div class="calibre2"><pre class="programlisting">if __name__ == "__main__":
    x,y = get_data()    

    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
    
    build_forest(x_train,y_train,x_dev,y_dev)
    model = search_parameters(x,y,x_dev,y_dev)</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec299" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let us start <a id="id729" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with our main function. We invoke <code class="literal">get_data</code> to get our predictor attributes in the response attributes. Inside <code class="literal">get_data</code>, we leverage the make_classification dataset to generate the training data for our recipe as follows:</p><div class="calibre2"><pre class="programlisting">def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y</pre></div><p class="calibre11">Let us look at the parameters that are passed to the <code class="literal">make_classification</code> method. The first parameter is the number of instances required; in this case, we say we need 500 instances. The second parameter is about the number of attributes required per instance. We say that we need 30. The third parameter, <code class="literal">flip_y</code>, randomly interchanges 3 percent of the instances. This is done to introduce some noise in our data. The next parameter specifies the number of features out of those 30 features should be informative enough to be used in our classification. We have specified that 60 percent of our features, that is, 18 out of 30 should be informative. The next parameter is about redundant features. These are generated as a linear combination of the informative features in order to introduce a correlation among the features. Finally, repeated features are the duplicate features, which are drawn randomly from both informative features and redundant features.</p><p class="calibre11">Let us split the <a id="id730" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data into the training and testing set using <code class="literal">train_test_split</code>. We reserve 30 percent of our data for testing:</p><div class="calibre2"><pre class="programlisting">    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)</pre></div><p class="calibre11">Once again, we leverage train_test_split to split our test data into dev and test:</p><div class="calibre2"><pre class="programlisting">    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)</pre></div><p class="calibre11">With the data divided for building, evaluating, and testing the model, we proceed to build our models:</p><div class="calibre2"><pre class="programlisting">build_forest(x_train,y_train,x_dev,y_dev)</pre></div><p class="calibre11">We invoke the <code class="literal">build_forest</code> function with our training and dev data to build our Extremely Randomized trees model. Let us look inside that function:</p><div class="calibre2"><pre class="programlisting">no_trees = 100
    estimator = ExtraTreesClassifier(n_estimators=no_trees,random_state=51)
    estimator.fit(x,y)
    
    train_predcited = estimator.predict(x)
    train_score = accuracy_score(y,train_predcited)
    dev_predicted = estimator.predict(x_dev)
    dev_score = accuracy_score(y_dev,dev_predicted)
    
    print "Training Accuracy = %0.2f Dev Accuracy = %0.2f"%(train_score,dev_score)
    print "cross validated score"
    print cross_val_score(estimator,x_dev,y_dev,cv=5)</pre></div><p class="calibre11">We need 100 trees in our ensemble, so we use the variable no_trees to define the number of trees. We leverage the <code class="literal">ExtraTreesClassifier</code> class from Scikit learn. As you can see, we pass the number of trees required as a parameter. A point to note here is the parameter bootstrap. Refer to the <a id="id731" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>following URL for the parameters for the ExtraTreesClassifier:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html</a>
</p><p class="calibre11">The parameter bootstrap is set to <code class="literal">False</code> by default. Compare it with the <code class="literal">RandomForestClassifier</code>
<a id="id732" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> bootstrap parameter given at the following URL:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>
</p><p class="calibre11">As explained earlier, every tree in the forest is trained with all the records.</p><p class="calibre11">We proceed to<a id="id733" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> fit our model as follows:</p><div class="calibre2"><pre class="programlisting">    train_predcited = estimator.predict(x)</pre></div><p class="calibre11">We then proceed to find the model accuracy score for our train and dev data:</p><div class="calibre2"><pre class="programlisting">    train_score = accuracy_score(y,train_predcited)
    dev_predicted = estimator.predict(x_dev)
    dev_score = accuracy_score(y_dev,dev_predicted)</pre></div><p class="calibre11">Let us print the scores for the training and dev dataset:</p><div class="calibre2"><pre class="programlisting">    print "Training Accuracy = %0.2f Dev Accuracy = %0.2f"%(train_score,dev_score)</pre></div><div class="mediaobject"><img src="Images/B04041_09_06.jpg" alt="How it works…" class="calibre211"/></div><p class="calibre11">Let us now do a five-fold cross validation to look at the model predictions:</p><div class="mediaobject"><img src="Images/B04041_09_07.jpg" alt="How it works…" class="calibre212"/></div><p class="calibre11">Pretty good results. We almost have a 90 percent accuracy rate for one of the folds. We can do a randomized search across the parameter space as we did for Random Forest. Let us invoke the function <code class="literal">search_parameters</code> with our train and test dataset. Refer to the previous recipe for an explanation of RandomizedSearchCV. We will then print the output of the <code class="literal">search_parameters</code> function:</p><div class="mediaobject"><img src="Images/B04041_09_08.jpg" alt="How it works…" class="calibre213"/></div><p class="calibre11">As in the<a id="id734" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> previous recipe, we have ranked the models by their scores in descending order, thus showing the best model parameters in the beginning. We will choose these parameters as our model parameters. The attribute <code class="literal">best_estimator_ will</code> return the model with these parameters.</p><p class="calibre11">What you see next is the classification report generated for the best estimator. The predict function will use <code class="literal">best_estimator_ internally</code>. The report was generated by the following code:</p><div class="calibre2"><pre class="programlisting">dev_predicted = grid.predict(x_dev)
print classification_report(y_dev,dev_predicted)</pre></div><p class="calibre11">Great! We have a perfect model with a classification accuracy of 100 percent.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec300" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more… </h2></div></div></div><p class="calibre11">Extremely Randomized Trees are very popular with the time series classification problems.  Refer to the following paper for more information:</p><div class="calibre2"><blockquote class="blockquote"><p class="calibre17"><span class="strong1"><em class="calibre15">Geurts, P., Blanco Cuesta A., and Wehenkel, L. (2005a). Segment and combine approach for biological sequence classification. In: Proceedings of IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology, 194–201.</em></span></p></blockquote></div></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec301" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Building Decision Trees to solve Multi Class Problems</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6. Machine Learning 1">Chapter 6</a>, <span class="strong1"><em class="calibre15">Machine Learning I</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Understanding Ensemble, Bagging Method</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch08.xhtml" title="Chapter 8. Ensemble Methods">Chapter 8</a>, <span class="strong1"><em class="calibre15">Model Selection and Evaluation</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Growing from trees to Forest, Random Forest</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch09.xhtml" title="Chapter 9. Growing Trees">Chapter 9</a>, <span class="strong1"><em class="calibre15">Machine Learning III</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Growing Rotational Forest"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch09lvl1sec84" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Growing Rotational Forest</h1></div></div></div><p class="calibre11">Random forests <a id="id735" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and Bagging give impressive results with very large ensembles; having a large number of estimators results in an improvement in the accuracy of these methods. On the contrary, a Rotational forest is designed to work with a smaller number of ensembles.</p><p class="calibre11">Let us write down the steps involved in building a Rotational Forest. The number of trees required in the forest is<a id="id736" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> typically specified by the user. Let T be the number of trees required to be built.</p><p class="calibre11">We start with iterating from 1 through T, that is, we build T trees.</p><p class="calibre11">For each tree t, perform the following steps:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Split the attributes in the training set into K non-overlapping subsets of equal size.</li><li class="listitem">We have K datasets, each with K attributes. For each of the K datasets, we proceed to do the following: Bootstrap 75 percent of the data from each K dataset, and use the bootstrapped sample for further steps:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Run a Principal Component analysis on the ith subset in K. Retain all the principal components. For every feature j in the Kth subset, we have a principal component a. Let us denote it as aij, which is the principal component for the jth attribute in the ith subset.</li><li class="listitem">Store the principal components for the subset.</li></ul></div></li><li class="listitem">Create a rotation matrix of size n X n, where n is the total number of attributes. Arrange the principal components in the matrix such that the components match the position of the features in the original training dataset.</li><li class="listitem">Project the training dataset on the Rotation matrix using matrix multiplication.</li><li class="listitem">Build a decision tree with the projected dataset.</li><li class="listitem">Store the tree and the rotational matrix.</li></ul></div><p class="calibre11">With this knowledge, let us jump to our recipe.</p><div class="calibre2" title="Getting ready…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec302" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready…</h2></div></div></div><p class="calibre11">We are going to generate some classification datasets to demonstrate a Rotational Forest. To our knowledge, there is no Python implementation available for Rotational forests. Hence, we will write our own code. We will leverage Scikit Learn's implementation of a Decision Tree Classifier and use the <code class="literal">train_test_split</code> method for bootstrapping.</p></div><div class="calibre2" title="How to do it..."><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec303" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it...</h2></div></div></div><p class="calibre11">We will start with <a id="id737" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>loading all the necessary libraries. Let us leverage the <code class="literal">make_classification</code> method from the <code class="literal">sklearn.dataset</code> module to generate the training data. We follow it with a method to select a random subset of attributes called <code class="literal">gen_random_subset</code>:</p><div class="calibre2"><pre class="programlisting">from sklearn.datasets import make_classification
from sklearn.metrics import classification_report
from sklearn.cross_validation import train_test_split
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
import numpy as np

def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 50
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y

def get_random_subset(iterable,k):
    subsets = []
    iteration = 0
    np.random.shuffle(iterable)
    subset = 0
    limit = len(iterable)/k
    while iteration &lt; limit:
        if k &lt;= len(iterable):
            subset = k
        else:
            subset = len(iterable)
        subsets.append(iterable[-subset:])
        del iterable[-subset:]
        iteration+=1
    return subsets</pre></div><p class="calibre11">We now write<a id="id738" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> a function <code class="literal">build_rotationtree_model</code>, where we will build fully grown trees, and proceed to evaluate the forest's performance using the function <code class="literal">model_worth</code>:</p><div class="calibre2"><pre class="programlisting">def build_rotationtree_model(x_train,y_train,d,k):
    models = []
    r_matrices = []
    feature_subsets = []
    for i in range(d):
        x,_,_,_ = train_test_split(x_train,y_train,test_size=0.3,random_state=7)
        # Features ids
        feature_index = range(x.shape[1])
        # Get subsets of features
        random_k_subset = get_random_subset(feature_index,k)
        feature_subsets.append(random_k_subset)
        # Rotation matrix
        R_matrix = np.zeros((x.shape[1],x.shape[1]),dtype=float)
        for each_subset in random_k_subset:
            pca = PCA()
            x_subset = x[:,each_subset]
            pca.fit(x_subset)
            for ii in range(0,len(pca.components_)):
                for jj in range(0,len(pca.components_)):
                    R_matrix[each_subset[ii],each_subset[jj]] = pca.components_[ii,jj]
                
        x_transformed = x_train.dot(R_matrix)
        
        model = DecisionTreeClassifier()
        model.fit(x_transformed,y_train)
        models.append(model)
        r_matrices.append(R_matrix)
    return models,r_matrices,feature_subsets
    
def model_worth(models,r_matrices,x,y):
    
    predicted_ys = []
    for i,model in enumerate(models):
        x_mod =  x.dot(r_matrices[i])
        predicted_y = model.predict(x_mod)
        predicted_ys.append(predicted_y)
    
    predicted_matrix = np.asmatrix(predicted_ys)
    final_prediction = []
    for i in range(len(y)):
        pred_from_all_models = np.ravel(predicted_matrix[:,i])
        non_zero_pred = np.nonzero(pred_from_all_models)[0]  
        is_one = len(non_zero_pred) &gt; len(models)/2
        final_prediction.append(is_one)
    
    print classification_report(y, final_prediction)</pre></div><p class="calibre11">Finally, we write <a id="id739" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a main function for invoking the functions that we have defined earlier:</p><div class="calibre2"><pre class="programlisting">if __name__ == "__main__":
    x,y = get_data()    
#    plot_data(x,y)

    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
        
    # Build a bag of models
    models,r_matrices,features = build_rotationtree_model(x_train,y_train,25,5)
    model_worth(models,r_matrices,x_train,y_train)
    model_worth(models,r_matrices,x_dev,y_dev)</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec304" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let us start with our main function. We invoke <code class="literal">get_data</code> to get our predictor attributes in the response attributes. Inside get_data, we leverage the make_classification dataset to generate the training data for our recipe as follows:</p><div class="calibre2"><pre class="programlisting">def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y</pre></div><p class="calibre11">Let us look at the parameters passed to the make_classification method. The first parameter is the <a id="id740" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of instances required; in this case we say we need 500 instances. The second parameter is about the number of attributes required per instance. We say that we need 30. The third parameter, <code class="literal">flip_y</code>, randomly interchanges 3 percent of the instances. This is done to introduce some noise in our data. The next parameter is about the number of features out of those 30 features, which should be informative enough to be used in our classification. We have specified that 60 percent of our features, that is, 18 out of 30 should be informative. The next parameter is about redundant features. These are generated as a linear combination of the informative features in order to introduce a correlation among the features. Finally, repeated features are the duplicate features which are drawn randomly from both informative features and redundant features.</p><p class="calibre11">Let us split the data into the training and testing set using train_test_split. We reserve 30 percent of our data for testing:</p><div class="calibre2"><pre class="programlisting">    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)</pre></div><p class="calibre11">Once again, we leverage train_test_split to split our test data into dev and test as follows:</p><div class="calibre2"><pre class="programlisting">    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)</pre></div><p class="calibre11">With the data divided for building, evaluating, and testing the model, we proceed to build our models:</p><div class="calibre2"><pre class="programlisting">    models,r_matrices,features = build_rotationtree_model(x_train,y_train,25,5)</pre></div><p class="calibre11">We invoke the <code class="literal">build_rotationtree_model</code> function to build our Rotational forest. We pass our training data, predictors <code class="literal">x_train</code> and response variable <code class="literal">y_train</code>, the total number of trees to be built (<code class="literal">25</code> in this case), and finally, the subset of features to be used (<code class="literal">5</code> in this case).</p><p class="calibre11">Let us jump to that function:</p><div class="calibre2"><pre class="programlisting">    models = []
    r_matrices = []
    feature_subsets = []</pre></div><p class="calibre11">We begin with<a id="id741" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> declaring three lists to store each of the decision tree, the rotation matrix for that tree, and finally, the subset of features used in that iteration. We proceed to build each tree in our ensemble.</p><p class="calibre11">As a first order of business, we bootstrap to retain only 75 percent of the data:</p><div class="calibre2"><pre class="programlisting">        x,_,_,_ = train_test_split(x_train,y_train,test_size=0.3,random_state=7)</pre></div><p class="calibre11">We leverage the <code class="literal">train_test_split</code> function from Scikit learn for bootstrapping. We then decide the feature subsets as follows:</p><div class="calibre2"><pre class="programlisting">        # Features ids
        feature_index = range(x.shape[1])
        # Get subsets of features
        random_k_subset = get_random_subset(feature_index,k)
        feature_subsets.append(random_k_subset)</pre></div><p class="calibre11">The function <code class="literal">get_random_subset</code> takes the feature index and the number of subsets that require k as parameter, and returns K subsets.</p><p class="calibre11">Inside that function, we shuffle the feature index. The feature index is an array of numbers that starts from 0 and ends with the number of features in our training set:</p><div class="calibre2"><pre class="programlisting">    np.random.shuffle(iterable)</pre></div><p class="calibre11">Let us say we have 10 features and our k value is 5 indicating that we need subsets with 5 non-overlapping feature indices; we need to then do two iterations. We store the number of iterations needed in the limit variable:</p><div class="calibre2"><pre class="programlisting">    limit = len(iterable)/k
    while iteration &lt; limit:
        if k &lt;= len(iterable):
            subset = k
        else:
            subset = len(iterable)
        iteration+=1</pre></div><p class="calibre11">If our required subset is less than the total number of attributes, then we can proceed to use the first k entries in our iterable. Since we have shuffled our iterables, we will be returning different volumes at different times:</p><div class="calibre2"><pre class="programlisting">        subsets.append(iterable[-subset:])</pre></div><p class="calibre11">On selecting a subset, we remove it from the iterable as we need non-overlapping sets:</p><div class="calibre2"><pre class="programlisting">        del iterable[-subset:]</pre></div><p class="calibre11">With all the subsets ready, we declare our rotation matrix as follows:</p><div class="calibre2"><pre class="programlisting">        # Rotation matrix
        R_matrix = np.zeros((x.shape[1],x.shape[1]),dtype=float)</pre></div><p class="calibre11">As you can see, our rotational matrix is of size n x n, where n is the number of attributes in our dataset. You can see that we have used the shape attribute to declare this matrix filled with zeros:</p><div class="calibre2"><pre class="programlisting">        for each_subset in random_k_subset:
            pca = PCA()
            x_subset = x[:,each_subset]
            pca.fit(x_subset)</pre></div><p class="calibre11">For each of the K subsets of data having only K features, we proceed to perform the principal<a id="id742" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> component analysis.</p><p class="calibre11">We fill our rotational matrix with the component values as follows:</p><div class="calibre2"><pre class="programlisting">            for ii in range(0,len(pca.components_)):
                for jj in range(0,len(pca.components_)):
                    R_matrix[each_subset[ii],each_subset[jj]] = pca.components_[ii,jj]</pre></div><p class="calibre11">For example, let us say that we have three attributes in our subset, in a total of six attributes. For illustration, let us say our subsets are:</p><div class="calibre2"><pre class="programlisting">2,4,6 and 1,3,5</pre></div><p class="calibre11">Our rotational matrix R is of size 6 x 6. Assume that we want to fill the rotation matrix for the first subset of features. We will have three principal components, one each for 2, 4, and 6 of size 1 x 3.</p><p class="calibre11">The output of the PCA from Scikit learn is a matrix of the size component's X features. We go through each component value in the for loop. At the first run, our feature of interest is 2, and the cell (<code class="literal">0</code>,<code class="literal">0</code>) in the component matrix output from PCA gives the value of the contribution of feature 2 to component 1. We have to find the right place in the rotational matrix for this value. We use the index from the component matrix ii and jj with the subset list to get the right place in the rotation matrix:</p><div class="calibre2"><pre class="programlisting">                    R_matrix[each_subset[ii],each_subset[jj]] = pca.components_[ii,jj]</pre></div><p class="calibre11">
<code class="literal">each_subset[0]</code> and <code class="literal">each_subset[0]</code> will put us in cell (2,2) in the rotation matrix. As we go through the loop, the next component value in cell (0,1) in the component matrix will be placed in cell (2,4) of the rotational matrix, and the last one in cell (2,6) of the rotational matrix. This is done for all the attributes in the first subset. Let us go to the second subset; here the first attribute is 1. Cell (0,0) of the component matrix corresponds to cell (1,1) in the rotation matrix.</p><p class="calibre11">Proceeding this way, you will notice that the attribute component values are arranged in the same order as the attributes themselves.</p><p class="calibre11">With our rotation <a id="id743" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix ready, let us project our input onto the rotation matrix:</p><div class="calibre2"><pre class="programlisting">        x_transformed = x_train.dot(R_matrix)</pre></div><p class="calibre11">It's time now to fit our decision tree:</p><div class="calibre2"><pre class="programlisting">model = DecisionTreeClassifier()
model.fit(x_transformed,y_train)</pre></div><p class="calibre11">Finally, we store our models and the corresponding rotation matrices:</p><div class="calibre2"><pre class="programlisting">models.append(model)
r_matrices.append(R_matrix)</pre></div><p class="calibre11">With our model built, let us proceed to see how good our model is with both the train and the dev data, using the <code class="literal">model_worth</code> function:</p><div class="calibre2"><pre class="programlisting">model_worth(models,r_matrices,x_train,y_train)
model_worth(models,r_matrices,x_dev,y_dev)</pre></div><p class="calibre11">Let us take a look at our model_worth function:</p><div class="calibre2"><pre class="programlisting">    for i, model in enumerate(models):
        x_mod =  x.dot(r_matrices[i])
        predicted_y = model.predict(x_mod)
        predicted_ys.append(predicted_y)</pre></div><p class="calibre11">Inside the function with perform prediction using each of the tree we have built. However, before the prediction, we project our input using the rotation matrix. We store all our prediction output in a list called <code class="literal">predicted_ys</code>. Let us say we have 100 instances to predict, and we have 10 models in our tree. For each instance, we have 10 predictions. We store those as a matrix for convenience:</p><div class="calibre2"><pre class="programlisting">predicted_matrix = np.asmatrix(predicted_ys)</pre></div><p class="calibre11">Now we proceed to give a final classification for each of our input records:</p><div class="calibre2"><pre class="programlisting">    final_prediction = []
    for i in range(len(y)):
        pred_from_all_models = np.ravel(predicted_matrix[:,i])
        non_zero_pred = np.nonzero(pred_from_all_models)[0]  
        is_one = len(non_zero_pred) &gt; len(models)/2
        final_prediction.append(is_one)</pre></div><p class="calibre11">We will store our final prediction in a list called <code class="literal">final_prediction</code>. We go through each of the predictions for our instance. Say we are in the first instance (i=0 in our for loop); <code class="literal">pred_from_all_models</code> stores the output from all the trees in our model. It's an array of 0s and 1s indicating the class which has the model classified at that instance.</p><p class="calibre11">We make another array out of it <code class="literal">non_zero_pred</code> which has only those entries from the parent arrays which are non-zero.</p><p class="calibre11">Finally, if the <a id="id744" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>length of this non-zero array is greater than half the number of models that we have, we say our final prediction is 1 for the instance of interest. What we have accomplished here is the classic voting scheme.</p><p class="calibre11">Let us look at how good our models are now by calling <code class="literal">classification_report</code>:</p><div class="calibre2"><pre class="programlisting">    print classification_report(y, final_prediction)</pre></div><p class="calibre11">The following is the performance of our model on the training set:</p><div class="mediaobject"><img src="Images/B04041_09_09.jpg" alt="How it works…" class="calibre214"/></div><p class="calibre11">Let us look at our model's performance on the dev dataset:</p><div class="mediaobject"><img src="Images/B04041_09_10.jpg" alt="How it works…" class="calibre215"/></div></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec305" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">More information about Rotational forests can be gathered from the following paper:</p><div class="calibre2"><blockquote class="blockquote"><p class="calibre17"><span class="strong1"><em class="calibre15">Rotation Forest: A New Classifier Ensemble Method, Juan J. Rodriguez, Member, IEEE Computer Society, Ludmila I. Kuncheva, Member, IEEE, and Carlos J. Alonso</em></span></p><p class="calibre17"><span class="strong1"><em class="calibre15">The paper also claims that when Extremely Randomized Trees was compared to Bagging, AdBoost, and Random Forest on 33 datasets, Extremely Randomized Trees outperformed all the other three algorithms.</em></span></p><p class="calibre17"><span class="strong1"><em class="calibre15">Similar to Gradient Boosting, the authors of the paper claim that the Extremely Randomized method is an overall framework, and the underlying ensemble does not necessarily have to be a Decision Tree. Work is in progress on testing other algorithms like Naïve Bayes, Neural Networks, and others.</em></span></p></blockquote></div></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch09lvl2sec306" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Extracting Principal Components</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Data Analysis – Deep Dive">Chapter 4</a>, <span class="strong1"><em class="calibre15">Analyzing Data - Deep Dive</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Reducing data dimension by Random Projection</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Data Analysis – Deep Dive">Chapter 4</a>, <span class="strong1"><em class="calibre15">Analyzing Data - Deep Dive</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Building Decision Trees to solve Multi Class Problems</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6. Machine Learning 1">Chapter 6</a>, <span class="strong1"><em class="calibre15">Machine Learning I</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Understanding Ensemble, Gradient Boosting</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch08.xhtml" title="Chapter 8. Ensemble Methods">Chapter 8</a>, <span class="strong1"><em class="calibre15">Model Selection and Evaluation</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Growing from trees to Forest, Random Forest</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch09.xhtml" title="Chapter 9. Growing Trees">Chapter 9</a>, <span class="strong1"><em class="calibre15">Machine Learning III</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Growing Extremely Randomized Trees</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch09.xhtml" title="Chapter 9. Growing Trees">Chapter 9</a>, <span class="strong1"><em class="calibre15">Machine Learning III</em></span></li></ul></div></div></div></div>



  </body></html>