- en: Making Predictive Models in Healthcare
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is intended for all audiences and is an integral part of this book.
    We will demonstrate how to build predictive models for healthcare using example
    data and an example machine learning problem. We will preprocess the data one
    feature at a time. By the end of this chapter, you will understand how to prepare
    and fit a machine learning model to a clinical dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to predictive analytics in healthcare
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 1](b15b2b73-d2bb-410f-ab55-5f0f1e91730e.xhtml), *Introduction to
    Healthcare Analytics,* we discussed the three subcomponents of analytics: descriptive
    analytics, predictive analytics, and prescriptive analytics. Predictive and prescriptive
    analytics form the heart of healthcare''s mission to improve care, cost, and outcomes.
    That is because if we can predict that an adverse event is likely in the future,
    we can divert our scarce resources toward preventing the adverse event from occurring.'
  prefs: []
  type: TYPE_NORMAL
- en: What are some of the adverse events we can predict (and then prevent) in healthcare?
  prefs: []
  type: TYPE_NORMAL
- en: '**Deaths**: Obviously, any death that is preventable or foreseeable should
    be avoided. Once a death is predicted to occur, preventative actions may include
    directing more nurses toward that patient, hiring more consultants for the case,
    or speaking to the family about options earlier rather than later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adverse clinical events**: These are events that are not synonymous with
    deaths, but highly increase the chances of morbidity and mortality. Morbidity
    refers to complications, while mortality refers to death. Examples of adverse
    clinical events include heart attacks, heart failure exacerbations, COPD exacerbations,
    pneumonia, and falls. Patients in which adverse events are likely could be candidates
    for more nursing care or for prophylactic therapies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Readmissions**: Readmissions don''t present an obvious danger to patients;
    however, they are costly, so preventable readmissions should, therefore, be avoided.
    Furthermore, readmission reduction is highly incentivized by the Centers for Medicare
    and Medicaid Services, as we saw in [Chapter 6](023c1d7e-f3f0-42e6-a2be-64bd5ba4ab80.xhtml), *Measuring
    Healthcare Quality*. Preventative actions include assigning social workers and
    case managers to high-risk patients to assure that they are following up with
    outpatient providers and buying needed prescriptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High utilization**: Predicting patients who are likely to incur high amounts
    of medical spending again could potentially reduce costs by assigning more care
    members to their team and ensuring frequent outpatient check-ins and follow-ups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we've answered the "What?" question, the next question is, "How?" In
    other words, how do we make predictions about which care providers can act?
  prefs: []
  type: TYPE_NORMAL
- en: '**First, we need data**: The provider should send you their historical patient
    data. The data can be claims data, clinical transcripts, a dump of EHR records,
    or some combination of these. Whatever the type of data, it should eventually
    be able to be molded into a tabular format, in which each row represents a patient/encounter
    and each column represents a particular feature of that patient/encounter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using some of the data, we train a predictive model**: In [Chapter 3](46c83498-cb6e-45b4-ac39-6875a8d32400.xhtml),
    *Machine Learning Foundations*, we learned about what exactly we are doing when
    we train predictive models, and how the general modeling pipeline works.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using some of the data, we test our model''s performance**: Assessing the
    performance of our model is important for setting the expectations of the provider
    as to how accurate the model is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**We then deploy the model into a production environment and provide live predictions
    for patients on a routine basis**: At this stage, there should be a periodic flow
    of data from the provider to the analytics firm. The firm then responds with regularly
    scheduled predictions on those patients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the remainder of the chapter, we will go through the "How?" of building a
    predictive model for healthcare. First, we will describe our mock modeling task.
    Then, we will describe and obtain the publicly available dataset. After that,
    we will preprocess the dataset and train predictive models using different machine
    learning algorithms. Finally, we will assess the performance of our model. While
    we will not be using our models to make actual predictions on live data, we will
    describe the steps necessary for doing so.
  prefs: []
  type: TYPE_NORMAL
- en: Our modeling task – predicting discharge statuses for ED patients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every year, millions of patients use emergency department facilities across
    the nation. The resources of these facilities have to be managed properly—if there
    is a large influx of patients at any given time, the staff and the available rooms
    should be increased accordingly. The mismatch between resources and patient influx
    could lead to wasted money and suboptimal care.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, we introduce our example modeling task —predicting discharge
    statuses for patients presenting to the emergency room. The discharge status refers
    to whether patients are admitted to the hospital or sent home. Usually, the more
    serious cases are admitted to the hospital. Therefore, we are attempting to predict
    the outcome of the ED visit early on in the patient stay.
  prefs: []
  type: TYPE_NORMAL
- en: With such a model, the workflow of the hospital could be greatly improved, as
    well as resource flow. Many previous academic studies have looked at this problem
    (for an example, see Cameron et al., 2015).
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering why we didn't pick a different modeling task, such as readmission
    modeling or predicting CHF exacerbations. For one thing, the publicly available
    clinical data is very limited. The dataset that we chose is an ED dataset; there
    are no publicly available inpatient datasets available that are free to download
    without registering. Nevertheless, the task that we have chosen will serve our
    purposes in demonstrating how predictive healthcare models can be built.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will provide step-by-step instructions for obtaining the
    data and its associated documentation.
  prefs: []
  type: TYPE_NORMAL
- en: The NHAMCS dataset at a glance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset we have chosen for this book is part of the **National Hospital
    Ambulatory Medical Care Survey** (**NHAMCS**) public use data. It is survey data
    published and maintained by the US **Center for Disease Control and Prevention**
    (**CDC**). The home page for this data set is [https://www.cdc.gov/nchs/ahcd/ahcd_questionnaires.htm](https://www.cdc.gov/nchs/ahcd/ahcd_questionnaires.htm).
  prefs: []
  type: TYPE_NORMAL
- en: The NHAMCS data is survey-based data; it is populated by surveys sent to patients
    and healthcare providers that were seen in the hospital for encounters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data files are in fixed-width format. In other words, they are text files
    in which each row is on a distinct line, and columns are each a set number of
    characters long. Information about the character length of each feature is available
    in the corresponding NHAMCS documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are different sets of files depending on whether the data is from outpatient
    encounters or emergency department visits. We will be using the ED format in this
    chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data comes with detailed documentation about the content of each feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row of the data represents a distinct ED patient encounter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See the following table for a summary of the emergency department data files
    from NHAMCS that we will be using throughout this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Filename** | **Data type and year** | **Number of rows (encounters)** |
    **Number of columns (features)** | **Broad feature categories** |'
  prefs: []
  type: TYPE_TB
- en: '| ED2013 | ED Encounters; 2013 | 24,777 | 579 | Visit date and information,
    Demographics, Tobacco, Arrival means, Payment, Vital signs, Triage, ED relationship,
    Reason for visit, Injury, Diagnoses, Chronic conditions, Services performed, Providers
    seen, Disposition, Hospital admission, Imputed data, ED information, Socioeconomic
    data |'
  prefs: []
  type: TYPE_TB
- en: Downloading the NHAMCS data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The raw data files and supporting documentation are accessible from the CDC
    NHAMCS home page: [https://www.cdc.gov/nchs/ahcd/ahcd_questionnaires.htm](https://www.cdc.gov/nchs/ahcd/ahcd_questionnaires.htm)
    (the following screenshot). We recommend downloading all of the files into a directory
    dedicated to this book and its associated files. Also, remember which directories
    where they are downloaded to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9ddefe9-bdc0-4a36-970d-4274b7783b58.png)'
  prefs: []
  type: TYPE_IMG
- en: Downloading the ED2013 file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ED2013 file contains the raw data.  To download it:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the CDC NHAMCS homepage: [https://www.cdc.gov/nchs/ahcd/ahcd_questionnaires.htm](https://www.cdc.gov/nchs/ahcd/ahcd_questionnaires.htm).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll to the middle of the page, to the Public-use Data Files: Downloadable
    Data Files heading.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the link for NHAMCS. It should take you to the CDC's FTP website ([ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NHAMCS](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NHAMCS)).
    This website is pictured in the following screenshot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the file named `ED2013.zip`. Click on it. The file will start to download.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the file in your File Explorer and unzip it. In the unzipped directory,
    you should see a file named `ED2013` with no extension. This is the data file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Move the ED2013 data file to the directory associated with the book-related
    files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d5e8aec1-9f4a-443b-8e4f-9fe145de903b.png)'
  prefs: []
  type: TYPE_IMG
- en: Downloading the list of survey items – body_namcsopd.pdf
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Navigate to the CDC NHAMCS home page: [https://www.cdc.gov/nchs/ahcd/ahcd_questionnaires.htm](https://www.cdc.gov/nchs/ahcd/ahcd_questionnaires.htm).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll to the middle of the page, to the List of Survey Items, 1973-2012 heading.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the link labeled NAMCS and NHAMCS Survey Content Brochure [Revised
    11/2012].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The link should take you to a PDF page at the [https://www.cdc.gov/nchs/data/ahcd/body_namcsopd.pdf](https://www.cdc.gov/nchs/data/ahcd/body_namcsopd.pdf) URL.
    This is the list of survey items.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your browser to download the file. Then use your File Explorer to move it
    to the desired directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Downloading the documentation file – doc13_ed.pdf
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Navigate to the CDC NHAMCS home page: [https://www.cdc.gov/nchs/ahcd/ahcd_questionnaires.htm](https://www.cdc.gov/nchs/ahcd/ahcd_questionnaires.htm).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll to the middle of the page, to the Downloadable Documentation heading.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the link for NHAMCS (1992-2014). It should take you to the CDC's documentation
    FTP website ([ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/NHAMCS](ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/NHAMCS)).
    This website is pictured in the following screenshot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the file named `doc13_ed.pdf`. Click on it. A PDF should open in your browser.
    This PDF file contains the documentation for the ED2013 data file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use your browser to download the file. Then use your File Explorer to move
    it to the desired directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/afd9e727-52c1-4415-8ffa-4cdd7004fb3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Starting a Jupyter session
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will start a Jupyter session so that we can import our data into Python
    and make a machine learning model. A detailed example of creating a new Jupyter
    Notebook was presented in [Chapter 1](b15b2b73-d2bb-410f-ab55-5f0f1e91730e.xhtml),
    *Introduction to Healthcare Analytics*. Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Locate the Jupyter application on your computer and start it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the new Jupyter tab that was opened in your default browser, navigate to
    the directory where you wish to save the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate the New drop-down menu on the upper right of the console, click it, and
    select Python 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should see a new notebook, named Untitled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To rename your notebook, click on the name of the notebook in the upper left.
    A cursor should appear. Type in the desired name. We have named our notebook `ED_predict`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You are now ready to import the dataset into Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we load the dataset, there are some important facts about the data that
    must be acknowledged:'
  prefs: []
  type: TYPE_NORMAL
- en: The data is in a fixed-width format, meaning that there is no delimiter. Column
    widths will have to be specified manually.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no header row that has column names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you were to open the data file using a text editor, you would see rows of
    data simply containing numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because column widths are necessary for importing `.fwf` files, we must import
    those *first* into our session. We have therefore made a helper `.csv` file, titled
    `ED_metadata.csv`, that contains the width, name, and variable type of each column.
    Our data only has 579 columns, so making such a file only took a couple of hours.
    If you have a bigger dataset, you may have to rely on automated width detection
    methods and/or more team members to do the grunt work of creating a schema for
    your data.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the metadata
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our first cell, let''s import the metadata and print a small preview of
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So the `ED_metadata.csv` file simply is a comma-separated values file containing
    the width, column name, and variable type as specified in the documentation. This
    file can be downloaded from the code repository for this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next cell, we convert the columns of the pandas DataFrame we imported
    into separate lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Loading the ED dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we import the contents of the fixed-width data file into Python as a
    pandas DataFrame composed of string columns, using the `widths` list created in
    the previous cell. We then name the columns using the `col_names` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print a preview of our dataset to confirm it was imported correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the column values and their meanings in the documentation confirm
    that the data has been imported correctly. The `nan` values correspond to blank
    spaces in the data file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, as another check, let''s count the dimensions of the data file and
    confirm that there are 24,777 rows and 579 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that the data has been imported correctly, let's set up our response variable.
  prefs: []
  type: TYPE_NORMAL
- en: Making the response variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some cases, the response variable that we are trying to predict may already
    be a separate well-defined column. In those cases, simply converting the response
    from a string to a numeric type before splitting the data into train and test
    sets will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our specific modeling task, we are trying to predict which patients presenting
    to the ED will eventually be hospitalized. In our case, hospitalization encompasses:'
  prefs: []
  type: TYPE_NORMAL
- en: Those admitted to an inpatient ward for further evaluation and treatment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those transferred to a different hospital (either psychiatric or non-psychiatric)
    for further treatment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those admitted to the observation unit for further evaluation (whether they
    are eventually admitted or discharged after their observation unit stay)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Accordingly, we must do some data wrangling to assemble all of these various
    outcomes into a single response variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s discuss the previous code example in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line identifies the columns we would like to include in our final
    target variable by name. The target should equal `1` if the values for any of
    those columns is `1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Line 2, we convert the columns from the string to the numeric type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Lines 3-5, we create a column called `ADMITTEMP` that contains the row-wise
    sum of the five target columns. We then create our final target column, `ADMITFINAL`,
    and set it equal to `1` when `ADMITTEMP` is `>= 1`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Lines 6-7, we drop the five original response columns as well as the `ADMITTEMP`
    column since we now have our final response column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting the data into train and test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our response variable, the next step is to split the dataset
    into train and test sets. In data science, the **training set** is the data that
    is used to determine the model coefficients. In the training phase, the model
    takes into account the predictor variable values together with the response value
    to "discover" the rules and the weights that will guide the prediction of new
    data. The **testing set** is then used to measure our model performance, as we
    discussed in [Chapter 3](46c83498-cb6e-45b4-ac39-6875a8d32400.xhtml), *Machine
    Learning Foundations*. Typical splits use 70-80% for the training data and 20-30%
    for the testing data (unless the dataset is very large, in which case a smaller
    percentage can be allotted toward the testing set).
  prefs: []
  type: TYPE_NORMAL
- en: Some practitioners also have a validation set that is used to train model parameters,
    such as the tree size in the random forest model or the lasso parameter in regularized
    logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the `scikit-learn` library has a handy function called `train_test_split()`
    that takes care of the random splitting for us, when given the test set percentage.
    To use this function, we must first separate the target variable from the rest
    of the data, we do so as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, `y` holds our response variable and `X` holds
    our dataset. We feed these two variables to the `train_test_split()` function,
    along with `0.25` for the `test_size` and a random state for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a 2 x 2 split: `X_train`, `X_test`, `y_train`, and `y_test`.
    We can now use `X_train` and `y_train` to train the model, and `X_test` and `y_test`
    to test the model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: An important thing to remember is that during the preprocessing phase, any transformation
    made to the training set must also be performed on the testing set at test time,
    or otherwise, the model's output for the new data will be incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a sanity check and also to detect any target variable imbalance, let''s
    check the number of positive and negative responses in the response variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our result indicates that approximately 1 out of 7 observations in the test
    set have a positive response. While it is not a perfectly balanced dataset (in
    which case the ratio would be 1 out of 2), it is not so imbalanced that we need
    to do any upsampling or downsampling of the data. Let's proceed with preprocessing
    the predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the predictor variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a look at specific groups of predictor variables that commonly pop
    up in healthcare data.
  prefs: []
  type: TYPE_NORMAL
- en: Visit information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first feature category in the ED2013 dataset contains information about
    the timing of the visit. Variables such as month, day of week, and arrival time
    are included here. Also included are the waiting time and length of visit variables
    (both in minutes).
  prefs: []
  type: TYPE_NORMAL
- en: Month
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s analyze the `VMONTH` predictor in more detail. The following code prints
    all the values in the training set and their counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can now see that the months are numbered from `01` to `12`, as it says in
    the documentation, and that each month has representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'One part of preprocessing the data is performing **feature engineering **– that
    is, combining or transforming the features in some way to come up with new features
    that are more predictive than the previous ones. For example, suppose we had a
    hypothesis that ED visitors tend to be admitted more often during the winter months.
    We could make a predictor called `WINTER` that is a combination of the `VMONTH`
    predictor such that the value is `1` only if the patient came during December,
    January, February, or March. We have done that in the following cell. Later on,
    we can test this hypothesis when we assess variable importance while making our
    machine learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As an informal test, let''s print out the distribution of the `WINTER` variable
    and confirm that it is the sum of the preceding four winter months:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Sure enough, we get `6113 = 1551 + 1757 + 1396 + 1409`, confirming that we engineered
    the feature correctly. We will see other examples of feature engineering throughout
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Day of the week
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a sanity check that the data was imported correctly, let''s also explore
    the `VDAYR` variable, which indicates the day of the week that the patient visit
    occurred:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As we would expect, there are seven possible values, and the observations are
    relatively uniformly distributed across the possible values. We could get fancy
    and engineer a `WEEKEND` feature, but engineering additional features can be very
    time-consuming and memory-consuming, often for minimal gain. We'll leave that
    exercise up to the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Arrival time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The arrival time is another visit information variable included in the data.
    However, in its raw form, it will probably be unhelpful, since it can be an integer
    between 0 and 2,359\. Let''s make a `NIGHT` variable that is only positive when
    the patient comes in between 8 PM and 8 AM. Our reasoning behind creating this
    variable is the hypothesis that patients arriving at the ED outside of regular
    hours have more serious illnesses and will, therefore, be admitted more often
    to the hospital. We can use the following code to make the `NIGHT` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we first code a function that returns `1` if the patient
    has arrived between 8 PM and 8 AM, and returns `0` otherwise. We then use the
    `apply()` function of pandas to "apply" this function to the `ARRTIME` column
    and make the `NIGHT` column. We then drop the original `ARRTIME` column since
    it is not useful in its raw form.
  prefs: []
  type: TYPE_NORMAL
- en: Wait time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The wait time spent in the ED is yet another visit information variable that
    could reasonably be correlated with the target variable. Hypothetically, patients
    with more serious illnesses could appear to be more symptomatic to the triage
    nurse and therefore assigned more critical triage scores, causing them to have
    smaller waiting times than people with less serious illnesses.
  prefs: []
  type: TYPE_NORMAL
- en: In the documentation, it states that the `WAITTIME` variable may take values
    of `-9` and `-7` when blank and not applicable, respectively. Whenever a continuous
    variable has a placeholder value like this, we *must* do some sort of imputation
    to remove the placeholder values. Otherwise, the model will think that the patient
    had a wait time of `-7` minutes, and the whole model will be adjusted adversely.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, mean imputation is the appropriate action. **Mean imputation**
    replaces those negative values with the mean of the rest of the dataset, so that
    during modeling time those observations will have no effect in determining the
    coefficient for this variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform mean imputation, we first convert the columns to the numeric type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we write a function, called `mean_impute_values()`, that removes values
    of `-7` and `-9` from the column and replaces them with the mean of the column.
    We make the function generalizable so that it may be used later on in our preprocessing
    for other columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We then call the function on the data, and we are finished. Following that,
    we will confirm that this function has been applied correctly, but first, let's
    go over a few more variables.
  prefs: []
  type: TYPE_NORMAL
- en: Other visit information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final visit information variable in this dataset is the length of the visit
    variable (`LOV`). However, the length of visit is determined only after the entire
    ED visit, and by that time, the decision whether to admit or discharge will have
    already been made. It is important to drop variables that won''t be available
    during the time of the prediction, and for that reason, we must drop `LOV`. We
    do so as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've finished tackling the visit information, let's move on to demographic
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: Demographic variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In healthcare, demographic variables are usually associated with outcomes. Age,
    sex, and race are major demographic variables in healthcare. In this dataset,
    ethnicity and residence type have also been included. Let's sort out these variables
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Age
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As people get older, one can expect them to be sicker and to be admitted to
    the hospital more frequently. This hypothesis will be tested once we see the variable
    importance results of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three variables that reflect the age in the dataset. `AGE` is an
    integer value that gives the age in years. `AGEDAYS` is an integer value that
    gives the age in days if the patient is less than 1 year old. `AGER` is the age
    variable, except that it has been converted to a categorical variable. Let''s
    convert the `AGE` variable to a numeric type, leave the `AGER` variable as is,
    and remove the `AGEDAYS` variable since it will be not applicable in the vast
    majority of cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Sex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In healthcare, women have often been found to have longer life expectancies
    and be healthier overall than men, so let's include the `SEX` variable in our
    model. It is already categorical, so we can leave it as is.
  prefs: []
  type: TYPE_NORMAL
- en: Ethnicity and race
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ethnicity (Hispanic/Latino versus non-Hispanic/Latino) and race are also included
    in the data. Often, races that are prone to poor socioeconomic status have worse
    outcomes in healthcare. Let''s leave the unimputed ethnicity and race variables
    (`ETHUN` and `RACEUN`) as is. We can remove the redundant `RACER` variable as
    well as the imputed versions of ethnicity and race (`ETHIM` and `RACERETH`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Other demographic information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The patient residence is included in the data. Since it is categorical, there
    is no need to alter it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what we have so far and print the first five rows using the `head()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Scrolling horizontally through the output, you should confirm that all of our
    transformations and variable drops have been done correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Triage variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Triage variables are important for emergency department modeling tasks. Triage
    encompasses assigning a risk score to a patient based on their initial presentation
    and vital signs. It is usually completed by a nurse specialized to perform triage
    and encompasses both subjective and objective information. Triage scores usually
    range from 1 (critical) to 5 (non-urgent). The `IMMEDR` variable (item number
    34 in the documentation) is the triage score in this dataset. We will certainly
    include it.
  prefs: []
  type: TYPE_NORMAL
- en: Other variables we can categorize as triage variables include whether or not
    the patient arrived via EMS (`ARREMS`; usually correlated with worse outcomes)
    and whether or not the patient has been seen and discharged within the last 72
    hours (`SEEN72`). We will also include these variables in our model.
  prefs: []
  type: TYPE_NORMAL
- en: Financial variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The method of payment of the patient is commonly included in healthcare datasets
    and usually, certain payment types are associated with better or worse outcomes.
    Patients with no expected source of payment (`NOPAY`), or with Medicaid (`PAYMCAID`)
    or Medicare (`PAYMCARE`), typically are less healthy than patients with private
    insurance (`PAYPRIV`) or who are paying on their own (`PAYSELF`). Let''s include
    all of the financial variables except for the `PAYTYPER` variable, which is just
    a nonbinary expansion of the other payment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Vital signs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vital signs are an important source of information for patients in healthcare
    modeling, for many reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: They are easy to collect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are typically available at the beginning of the clinical encounter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are objective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are numerical indicators of patient health.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vital signs included in this dataset are temperature, pulse, respiratory
    rate, blood pressure (systolic and diastolic), oxygen saturation percentage, and
    whether they are on oxygen. Height and weight are commonly also categorized as
    vital signs, but they are not included in our data. Let's take a look at each
    vital sign in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Temperature** is usually measured using a thermometer early during the patient
    encounter and can be recorded in degrees Celsius or Fahrenheit. A temperature
    of 98.6° F (37.1° C) is usually considered a normal body temperature. Temperatures
    markedly above this range can be termed as **fever** or **hyperthermia** and usually
    reflect infection, inflammation, or environmental overexposure to the sun. Temperatures
    below normal by a certain amount are termed **hypothermia** and usually reflect
    environmental exposure to cold. The more the temperature deviates from normal,
    usually the more serious the illness is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our dataset, the `TEMPF` temperature has been multiplied by 10 and stored
    as an integer. Also, some values are blank (indicated by `-9`) and we must impute
    those, since temperature is a continuous variable. Following that, we first convert
    the temperature to a numeric type, use our previously written `mean_impute_values()`
    function to impute the missing values in `TEMPF`, and then use a lambda function
    to divide all temperatures by `10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print out 30 values of just this column to confirm that our processing
    was performed correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the temperatures are now of the float type and that they are
    not multiplied by 10\. Also, we see that the mean value, `98.282103`, has been
    substituted where values were previously blank. Let's move on to the next variable.
  prefs: []
  type: TYPE_NORMAL
- en: Pulse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pulse** measures the frequency of the heartbeat in the patient. The normal
    range is 60-100\. Having a pulse faster than `100` is termed **tachycardia** and
    usually indicates some underlying cardiac dysfunction, volume depletion, or infection
    (sepsis). A pulse lower than `60` is termed **bradycardia**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We must use mean imputation to impute the missing values. First, we convert
    the pulse to a numeric type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we write a `mean_impute_vitals()` function that is similar to our `mean_impute_values()`
    function, except that the placeholder values have been changed from `-7` and `-9`
    to `-998` and `-9`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Respiratory rate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The respiratory rate indicates the rate at which the person takes breaths. 18-20
    is considered normal. Tachypnea (abnormally elevated respiratory rate) is seen
    commonly in clinical practice and indicates an oxygen shortage in the body, usually
    due to either a cardiac or pulmonary cause. Bradypnea is an abnormally low respiratory
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we convert the `RESPR` variable to a numeric type and
    then perform a mean imputation of the missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Blood pressure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The blood pressure measures the amount of force per unit area that the blood
    exerts on the blood vessel walls. Blood pressure consists of two numbers – the
    **systolic blood pressure** (blood pressure during the systolic phase of the heartbeat)
    and the **diastolic blood pressure** (blood pressure during the diastolic phase).
    Normal blood pressure is usually somewhere between 110 to 120 mmHg for the systolic
    blood pressure and 70 to 80 mmHg for the diastolic blood pressure. Elevated blood
    pressure is called **hypertension**. The most common cause of elevated blood pressure
    is essential hypertension, which is primarily genetic (but multifactorial). Low
    blood pressure is called **hypotension**. Both hypertension and hypotension have
    complex etiologies that are often difficult to identify.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our dataset, systolic and diastolic blood pressure are in separate columns
    (`BPSYS` and `BPDIAS`, respectively). First, we process the systolic blood pressure
    by converting it to a numeric type and mean imputing the missing values as we
    have done already for other columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Diastolic blood pressure is a bit more complex. The value `998` means that
    the pressure was `PALP`, meaning that it was too low to be detected by a sphygmamometer
    but high enough to feel by touch (palpation). After we convert it to a numeric
    type, we will substitute a numeric value of `40` for the `PALP` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We write a new function called `mean_impute_bp_diast()` that does the conversion
    of `PALP` values to `40` and the missing values to the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Oxygen saturation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Oxygen saturation measures the oxygen level in the blood. It is reported as
    a percentage, with higher values being more healthy. We convert it to a numeric
    type and perform mean imputation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s examine the vital sign transformations we''ve done so far by selecting
    those columns and using the `head()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | TEMPF | PULSE | RESPR | BPSYS | BPDIAS |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 15938 | 98.200000 | 101.000000 | 22.0 | 159.000000 | 72.000000 | 98.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5905 | 98.100000 | 70.000000 | 18.0 | 167.000000 | 79.000000 | 96.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 4636 | 98.200000 | 85.000000 | 20.0 | 113.000000 | 70.000000 | 98.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 9452 | 98.200000 | 84.000000 | 20.0 | 146.000000 | 72.000000 | 98.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 7558 | 99.300000 | 116.000000 | 18.0 | 131.000000 | 82.000000 | 96.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 17878 | 99.000000 | 73.000000 | 16.0 | 144.000000 | 91.000000 | 99.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 21071 | 97.800000 | 88.000000 | 18.0 | 121.000000 | 61.000000 | 98.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 20990 | 98.600000 | 67.000000 | 16.0 | 112.000000 | 65.000000 | 95.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 4537 | 98.200000 | 85.000000 | 20.0 | 113.000000 | 72.000000 | 99.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 7025 | 99.300000 | 172.000000 | 40.0 | 124.000000 | 80.000000 | 100.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2134 | 97.500000 | 91.056517 | 18.0 | 146.000000 | 75.000000 | 94.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5212 | 97.400000 | 135.000000 | 18.0 | 125.000000 | 71.000000 | 99.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 9213 | 97.900000 | 85.000000 | 18.0 | 153.000000 | 96.000000 | 99.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2306 | 97.000000 | 67.000000 | 20.0 | 136.000000 | 75.000000 | 99.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 6106 | 98.600000 | 90.000000 | 18.0 | 109.000000 | 70.000000 | 98.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2727 | 98.282103 | 83.000000 | 17.0 | 123.000000 | 48.000000 | 92.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 4098 | 99.100000 | 147.000000 | 20.0 | 133.483987 | 78.127013 | 100.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5233 | 98.800000 | 81.000000 | 16.0 | 114.000000 | 78.000000 | 97.311242
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5107 | 100.000000 | 95.000000 | 24.0 | 133.000000 | 75.000000 | 94.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '| 18327 | 98.900000 | 84.000000 | 16.0 | 130.000000 | 85.000000 | 98.000000
    |'
  prefs: []
  type: TYPE_TB
- en: Examining the preceding table, it looks like we are in good shape. We can see
    the imputed mean values for each column (values having extra precision). Let's
    move onto the last vital sign we have in our data, the pain level.
  prefs: []
  type: TYPE_NORMAL
- en: Pain level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pain** is a common indication that something is wrong with the human body,
    and pain level is usually asked in every medical interview, whether it is the
    initial history and physical or the daily SOAP note. Pain levels are usually reported
    on a scale from 0 (non-existent) to 10 (unbearable). Let''s first convert the
    `PAINSCALE` column to the numeric type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to write a separate function for mean-imputing pain values, since
    it uses `-8` as a placeholder value instead of `-7`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Together, vital signs provide an important picture of the health of the patient.
    In the end, we will see how important a role these variables play when we do the
    variable importance.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can move on to the next variable category.
  prefs: []
  type: TYPE_NORMAL
- en: Reason-for-visit codes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The reason-for-visit variables encode the reason for the patient visit, which
    can be seen as the chief complaint of the visit (we talked about chief complaints
    in [Chapter 2](71c31b0a-fa9e-4b31-8b58-f563a815e338.xhtml), *Healthcare Foundations*).
    In this dataset, these reasons are coded using a code set called *A Reason for
    Visit Classification for Ambulatory Care* (refer to *Page 16* and *Appendix II*
    of the 2011 documentation for further information; a screenshot of the first page
    of the *Appendix* is provided at the end of the chapter). While the exact code
    may not be determined early during the patient encounter, we include it here because:'
  prefs: []
  type: TYPE_NORMAL
- en: It reflects information available early during the patient encounter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We would like to demonstrate how to process a coded variable (all the other
    coded variables occur too late in the patient encounter to be of use for this
    modeling task):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c468e8f7-db28-46d9-8d7a-814054403ffc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Coded variables require special attention for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Often there are multiple entries designated in the table for more than one code,
    and the reason-for-visit codes are no exception. Notice that this dataset contains
    three RFV columns (`RFV1`, `RFV2`, and `RFV3`). A code for asthma, for example,
    may appear in any of these columns. Therefore, it is not enough to do one-hot
    encoding for these columns. We must detect the presence of each code in *any*
    of the three columns, and we must write a special function to do that.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Codes are categorical, but the numbers themselves usually carry no meaning.
    For easier interpretation, we must name the columns accordingly, using suitable
    descriptions. To do this, we have put together a special `.csv` file that contains
    the typed description for each code (available for download at the book's GitHub
    repository).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One output format possibility is a column for each code, where a `1` indicates
    the presence of that code and a `0` indicates its absence (as done in Futoma et
    al., 2015). Any desired combinations/transformations can then be performed. We
    have used that format here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Without further ado, let''s start transforming our reason-for-visit variables.
    First, we import the RFV code descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now we will do our RFV code processing.
  prefs: []
  type: TYPE_NORMAL
- en: First, to name the columns properly, we import the `sub()` function from the
    `re` module (`re` stands for regular expression).
  prefs: []
  type: TYPE_NORMAL
- en: Then we write a function that scans any given RFV columns for the presence of
    an indicated code, and returns the dataset with a new column, with a `1` if the
    code is present and a `0` if the code is absent.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use a `for` loop to iterate through every code in the `.csv` file,
    effectively adding a binary column for every possible code. We do this for both
    the training and testing sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we drop the original RFV columns, since we no longer need them. The
    full code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at our transformed dataset with the `head()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Notice that there are now 1,264 columns. While the full DataFrame has been truncated,
    if you scroll horizontally, you should see some of the new `rfv_` columns appended
    to the end of the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Injury codes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Injury codes are also included in the data. While the reason-for-visit codes
    apply to all visits, injury codes only apply if the patient has undergone either
    physical injury, poisoning, or adverse effects of medical treatment (including
    suicide attempts). Because the exact reason for injury may not be known until
    a full workup has been performed, and that workup usually occurs after a decision
    to admit has already been made. Therefore, we will remove the injury code variables,
    since they potentially contain future information that will not be available at
    prediction time. However, if you wish to use such codes for your modeling task,
    remember that coded data can be processed in a manner similar to that shown earlier.
    Refer to the documentation for additional details on injury variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Diagnostic codes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset also contains ICD-9-DM codes to classify diagnoses associated with
    each visit. Notice that there are three diagnostic code columns. This is consistent
    with what we said about coded variables in the *Reason-for-Visit codes* section.
    Because ICD-9 codes are usually assigned to visits after the workup has been performed
    and the cause of the symptoms determined, we will have to omit them from this
    modeling task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Medical history
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 2](71c31b0a-fa9e-4b31-8b58-f563a815e338.xhtml), *Healthcare
    Foundations*, individuals that have chronic conditions are usually less healthy
    and have poorer health outcomes than those who do not have chronic health conditions.
    The dataset includes information on the presence of 11 common chronic conditions
    for each visit. These conditions are cancer, cerebrovascular disease, chronic
    obstructive pulmonary disease, a condition requiring dialysis, congestive heart
    failure, dementia, diabetes, history of myocardial infarction, history of pulmonary
    embolism or deep vein thrombosis, and HIV/AIDS. Because past medical history is
    often available electronically for previously seen patients and is usually established
    early during patient triage, we have decided to include these variables here.
    Because they are already binary, no processing of these variables is needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a continuous variable, called `TOTCHRON`, that tallies the total
    number of chronic disease for each patient, which we mean-impute as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Medical tests, while important, occur post-prediction time and must be omitted
    for this use case. They may be used for other modeling tasks, such as readmission
    prediction or mortality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Procedures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We omit procedures because, similar to the tests, they often occur post-prediction
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Medication codes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data includes ample information on medications given in the ED and/or prescribed
    at discharge. In fact, information on up to 12 medications is allotted in various
    columns. Obviously, medication administration occurs after the decision to admit
    the patient has been made, so we cannot use these columns for this use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, we encourage you to peruse the documentation and read about the
    coding systems used for medications if you wish to use such information in your
    own predictive modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Provider information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Provider columns indicate which type(s) of medical providers participated in
    the medical encounter. We have omitted these variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Disposition information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because disposition variables are directly related to the outcome, we cannot
    leave them in the data. We omit them here (recall that we previously removed several
    of the disposition variables right after we created our final target column):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Imputed columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These columns indicate those that contain imputed data. For the most part,
    we have included the unimputed counterparts in our data and therefore do not need
    the imputed columns, so we remove them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Identifying variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When combined, the identifying variables provide a unique key for each encounter.
    While this may come in handy in many situations, fortunately, `pandas` DataFrames
    already uniquely assign an integer to each row, so we can remove the ID variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Electronic medical record status columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset includes dozens of columns that indicate the technological level
    of the facility at which the patient was seen. We discussed this in the *EHR technology
    and meaningful use* section in [Chapter 2](71c31b0a-fa9e-4b31-8b58-f563a815e338.xhtml),
    *Healthcare Foundations*. We omit these columns since they are valued on a per-hospital
    basis rather than a per-encounter basis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Detailed medication information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More detailed drug information is available in these columns. These columns
    include information on drug categories, which is coded. We must omit these columns
    because they represent future information. However, this information could be
    immensely useful in other machine learning problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Miscellaneous information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, there are several columns at the end that are irrelevant for our purposes,
    so we remove them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Final preprocessing steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have gone through all of the variable groups, we are almost ready
    to build our predictive models. But first, we must expand all of our categorical
    variables into binary variables (also known as one-hot encoding or a 1-of-K representation)
    and convert our data into a format suitable for input into the `scikit-learn` methods.
    Let's do that next.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many classifiers of the scikit-learn library require categorical variables to
    be one-hot encoded. **One-hot encoding**, or a **1-of-K representation**, is when
    a categorical variable that has more than two possible values is recorded as multiple
    variables each having two possible values.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say that we have five patients in our dataset and we wish
    to one-hot encode a column that encodes the primary visit diagnosis. Before one-hot
    encoding, the column looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `patient_id` | `primary_dx` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | copd |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | hypertension |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | copd |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | chf |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | asthma |'
  prefs: []
  type: TYPE_TB
- en: 'After one-hot encoding, this column would be split into *K* columns, where
    *K* is the number of possible values, and each column takes a value of 0 or 1
    depending on whether the observation takes the value corresponding to that column:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `patient_id` | `primary_dx_copd` | `primary_dx_hypertension` | `primary_dx_chf`
    | `primary_dx_asthma` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Note that we have converted the strings of the previous column into an integer
    representation. This makes sense since machine learning algorithms are trained
    on numbers, not words! This is why one-hot encoding is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn` has a `OneHotEncoder` class in its `preprocessing` module. However,
    `pandas` has a `get_dummies()` function that accomplishes one-hot encoding in
    a single line. Let''s use the `pandas` function. Before we do that, we must identify
    the columns that are categorical in our dataset to be passed to the function.
    We do this by using the metadata to identify the categorical columns and seeing
    which of those columns intersect with the columns that remain in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We must also one-hot encode the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: As a final note, we should mention that there is the possibility that the testing
    set will contain categorical values that haven't been seen in the training data.
    This may cause an error when assessing the performance of the model using the
    testing set. To prevent this, you may have to write some extra code that sets
    any missing columns in the testing set to zero. Fortunately, we do not have to
    worry about that with our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Numeric conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now convert all of the columns into numeric format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: NumPy array conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final step is taking the NumPy array of the pandas DataFrame that will
    be passed directly into the machine learning algorithm. First, we save the final
    column names, which will assist us when we assess variable importance later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we use the `values` attribute of the `pandas` DataFrames to access the
    underlying NumPy array for each DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready for to build the models.
  prefs: []
  type: TYPE_NORMAL
- en: Building the models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll build three types of classifiers and assess their performance:
    a logistic regression classifier, a random forest, and a neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We discussed the intuition behind and basics of logistic regression models
    in [Chapter 3](46c83498-cb6e-45b4-ac39-6875a8d32400.xhtml), *Machine Learning
    Foundations*. To build a model on our training set, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Prior to the `for` loop, we import the `LogisticRegression` class and set `clf`
    equal to a `LogisticRegression` instance. The training and testing occur in the
    `for` loop. First, we use the `fit()` method to fit the model (for example, to
    determine the optimal coefficients) using the training data. Next, we use the
    `score()` method to assess the model performance on both the training data and
    the testing data.
  prefs: []
  type: TYPE_NORMAL
- en: In the second half of the `for` loop, we print out the coefficient values of
    each feature. In general, features with coefficients that are farther from zero
    are the most positively/negatively correlated with the outcome. However, we did
    not scale the data prior to training, so it is possible that more important predictors
    that are not scaled appropriately will have lower coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the code should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: First, let's discuss the performance of the training and testing sets. They
    are close together, which indicates the model did not overfit to the training
    set. The accuracy is approximately 88%, which is on par with performance in research
    studies (Cameron et al., 2015) for predicting emergency department status.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the coefficients, we can confirm that they make intuitive sense.
    The feature with the highest coefficient is related to the onset of labor in pregnancy;
    we all know that labor results in a hospital admission. Many of the features pertaining
    to severe psychiatric disease, which almost always result in an admission due
    to the risk the patient poses to themselves or to others. The `IMMEDR_1` feature
    also has a high coefficient; remember that this feature corresponds to a value
    of 1 on the triage scale, which is the most critical value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: In contrast, scrolling to the bottom reveals some of the features that are negatively
    correlated with an admission. Having a toothache and needing sutures to be removed
    show up here, and they are not likely to result in admissions since they are not
    urgent complaints.
  prefs: []
  type: TYPE_NORMAL
- en: We've trained our first model. Let's see whether some of the more complex models
    will improve.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A convenient feature of `scikit-learn` is that many of the classifiers have
    identical methods so that models can be built interchangeably. We see that in
    the following code, when we use the `fit()` and `score()` methods of the `RandomForestClassifier`
    class to train the model and assess its performance, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: This time the validation accuracy was similar to that of the logistic regression
    model, approximately 88%. However, the accuracy on the training data was 100%,
    indicating that we overfit the model to the training data. We'll discuss potential
    ways of improving our models at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the feature importance, again the results make sense. This time the
    vital signs seem to be the most important predictors, along with the age predictor.
    Scrolling to the bottom reveals those features that had no impact on predictability.
    Note that in contrast to regression coefficients, with the variable importance
    of random forest, the frequency with which the variable was positive plays a role
    in the importance; this may explain why `IMMEDR_02` is ranked as more important
    than `IMMEDR_01`.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we arrive at the neural network model. Note that our training set
    only has approximately 18,000 observations; the most successful neural network
    models (for example, "deep learning" models) typically use millions or even billions
    of observations. Nevertheless, let''s see how our neural network model fares.
    For neural networks, it is recommended that the data is scaled appropriately (for
    example, having a **standard distribution** with a mean equal to 0 and a standard
    deviation equal to 1). We use the `StandardScaler` class to accomplish this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Once you run the preceding cell, you will see iterations being completed, and
    once the iterations fail to result in improvements to the model, the training
    will stop and the accuracies will be printed. In our run, the validation accuracy
    of the model having 150 cells in its hidden layer was 87%, higher than the other
    hidden layer sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Using the models to make predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have finished preprocessing the data, and making and scoring the model. The
    AUC is similar to that reported in previous academic studies that predict ED outcomes
    (see Cameron et al., 2015 for an example).
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step would be to save and deploy the model and use it to make live
    predictions. Fortunately, all of the classifiers in the scikit-learn library include
    several functions for making predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: For most classifiers, the `predict()` function takes a matrix, *X*, that contains
    unlabeled data as input and simply returns the class predictions with no further
    information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `predict_proba()` function takes a matrix, *X*, that contains unlabeled
    data as input and returns the probabilities with which the observations belong
    to each class. These should add up to `1` for each observation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `predict_log_proba()` function is similar to the `predict_proba()` function
    except that it returns the log probabilities with which the observations belong
    to each class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keep the following important fact in mind: *When making predictions, the unlabeled
    data must be preprocessed identically to the manner in which the training data
    was preprocessed*. This includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Column additions and deletions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Column transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputation of missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling and centering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just one column that is not preprocessed properly can have an extremely negative
    impact on the model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Improving our models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although in this chapter we have built a rudimentary model that matches the
    performance of academic research studies, there is certainly room for improvement.
    The following are some ideas for how the model can be improved, and we leave it
    to the reader to implement these suggestions and any other tricks or techniques
    the reader might know to improve performance. How high will your performance go?
  prefs: []
  type: TYPE_NORMAL
- en: 'First and foremost, the current training data has a large number of columns.
    Some sort of feature selection is almost always performed, particularly for logistic
    regression and random forest models. For logistic regression, common methods of
    performing feature selection include:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a certain number of predictors that have the highest coefficients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a certain number of predictors that have the lowest *p*-values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using lasso regularization and removing predictors whose coefficients become
    zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a greedy algorithm such as forward- or backward-stepwise logistic regression
    that removes/adds predictors systematically according to rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a brute-force algorithm, such as best subset logistic regression, that
    tests every predictor permutation/combination for a given number of predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For random forests, using the variable importance and selecting a certain number
    of predictors with the highest importance is very common, as is performing grid
    searches.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks have their own unique improvement techniques.
  prefs: []
  type: TYPE_NORMAL
- en: For one thing, more data is always good, particularly with neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specific optimization algorithm used can factor into the performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, our neural networks only had one hidden layer. In the industry,
    models with multiple hidden layers are becoming increasingly common (although
    they may take a long time to train).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specific nonlinear activation function used may also impact the model's
    performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have built a predictive model for predicting outcomes in
    the emergency department. While there are many machine learning problems in healthcare,
    this exercise has demonstrated the issues typically faced as one preprocesses
    healthcare data, trains and scores models, and makes predictions with unlabeled
    data. This chapter marks the end of the coding portion of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen the construction of a predictive model firsthand, the
    next logical question to ask is how predictive models have fared when compared
    with traditional statistical risk scores in predicting clinical outcomes. We explore
    that question in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: References and further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cameron A, Rodgers K, Ireland A, et al. (2015). A simple tool to predict admission
    at the time of triage. *Emerg Med J* 2015;32:174-179.
  prefs: []
  type: TYPE_NORMAL
- en: 'Futoma J, Morris J, Lucas J (2015). A comparison of models for predicting early
    hospital readmissions. Journal of Biomedical Informatics 56: 229-238.'
  prefs: []
  type: TYPE_NORMAL
