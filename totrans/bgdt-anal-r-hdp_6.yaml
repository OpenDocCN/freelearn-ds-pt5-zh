- en: Chapter 6. Understanding Big Data Analysis with Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to learn about different machine learning techniques
    that can be used with R and Hadoop to perform Big Data analytics with the help
    of the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of machine-learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised machine-learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised machine-learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a branch of artificial intelligence that allows us to make
    our application intelligent without being explicitly programmed. Machine learning
    concepts are used to enable applications to take a decision from the available
    datasets. A combination of machine learning and data mining can be used to develop
    spam mail detectors, self-driven cars, speech recognition, face recognition, and
    online transactional fraud-activity detection.
  prefs: []
  type: TYPE_NORMAL
- en: There are many popular organizations that are using machine-learning algorithms
    to make their service or product understand the need of their users and provide
    services as per their behavior. Google has its intelligent web search engine,
    which provides a number one search, spam classification in Google Mail, news labeling
    in Google News, and Amazon for recommender systems. There are many open source
    frameworks available for developing these types of applications/frameworks, such
    as R, Python, Apache Mahout, and Weka.
  prefs: []
  type: TYPE_NORMAL
- en: Types of machine-learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three different types of machine-learning algorithms for intelligent
    system development:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine-learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised machine-learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommender systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we are going to discuss well-known business problems with classification,
    regression, and clustering, as well as how to perform these machine-learning techniques
    over Hadoop to overcome memory issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you load a dataset that won''t be able to fit into your machine memories
    and you try to run it, the predictive analysis will throw an error related to
    machine memory, such as **Error: cannot allocate vector of size 990.1 MB**. The
    solution is to increase the machine configuration or parallelize with commodity
    hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine-learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be learning about supervised machine-learning algorithms.
    The algorithms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear regression is mainly used for predicting and forecasting values based
    on historical information. Regression is a supervised machine-learning technique
    to identify the linear relationship between target variables and explanatory variables.
    We can say it is used for predicting the target variable values in numeric form.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will be learning about linear regression with R
    and linear regression with R and Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the variables that are going to be predicted are considered as target
    variables and the variables that are going to help predict the target variables
    are called explanatory variables. With the linear relationship, we can identify
    the impact of a change in explanatory variables on the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematics, regression can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = ax +e
  prefs: []
  type: TYPE_NORMAL
- en: 'Other formulae include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The slope of the regression line is given by:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a= (NΣxy - (Σx)(Σy)) / (NΣx² - (Σx)²)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The intercept point of regression is given by:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: e = (Σy - b(Σx)) / N
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, *x* and *y* are variables that form a dataset and *N* is the total numbers
    of values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have the data shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| x | y |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 63 | 3.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 66 | 4 |'
  prefs: []
  type: TYPE_TB
- en: If we have a new value of *x*, we can get the value of *y* with it with the
    help of the regression formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications of linear regression include:'
  prefs: []
  type: TYPE_NORMAL
- en: Sales forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting optimum product price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the next online purchase from various sources and campaigns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at the statistical technique to implement the regression model for
    the provided dataset. Assume that we have been given n number of statistical data
    units.
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/3282OS_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Its formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Y = e[0] + a[0]x[0] + a[1]x[1] + a[2]x[2] +a[3]x[3] + a[4]x[4]
  prefs: []
  type: TYPE_NORMAL
- en: Here, *Y* is the target variable (response variable), *xi* are explanatory variables,
    and *e[0]* is the sum of the squared error term, which can be considered as noise.
    To get a more accurate prediction, we need to reduce this error term as soon as
    possible with the help of the `call` function.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we will see how to perform linear regression in R. We can use the in-built
    `lm()` method to build a linear regression model with R.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It will build a regression model based on the property of the provided dataset
    and store all of the variables' coefficients and model parameters used for predicting
    and identifying of data pattern from the model variable values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the various model parameters that can be displayed with the
    preceding `summary` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RSS**: This is equal to ∑(yactual - y)².'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Degrees of Freedom** (**DOF**): This is used for identifying the degree of
    fit for the prediction model, which should be as small as possible (logically,
    the value 0 means perfect prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual standard error** (**RSS/DF**): This is used for identifying the
    goodness of fit for the prediction model, which should be as small as possible
    (logically, the value 0 means perfect prediction).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pr**: This is the probability for a variable to be included into the model;
    it should be less than 0.05 for a variable to be included.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t-value**: This is equal to 15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**f**: This is the statistic that checks whether R square is a value other
    than zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Linear regression with R](img/3282OS_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Linear regression with R and Hadoop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assume we have a large dataset. How will we perform regression data analysis
    now? In such cases, we can use R and Hadoop integration to perform parallel linear
    regression by implementing Mapper and Reducer. It will divide the dataset into
    chunks among the available nodes and then they will process the distributed data
    in parallel. It will not fire memory issues when we run with an R and Hadoop cluster
    because the large dataset is going to be distributed and processed with R among
    Hadoop computation nodes. Also, keep in mind that this implemented method does
    not provide higher prediction accuracy than the `lm()` model.
  prefs: []
  type: TYPE_NORMAL
- en: RHadoop is used here for integration of R and Hadoop, which is a trusted open
    source distribution of **Revolution Analytics**. For more information on RHadoop,
    visit [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki).
    Among the packages of RHadoop, here we are using only the `rmr` and `rhdfs` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to perform regression analysis with R and Hadoop data technologies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `Sum()` function is re-usable as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The outline of the linear regression algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the `Xtx` value with MapReduce job1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculating the `Xty` value with MapReduce job2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deriving the coefficient values with `Solve (Xtx, Xty)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's understand these steps one by one.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to calculate the `Xtx` value with MapReduce job 1.
  prefs: []
  type: TYPE_NORMAL
- en: The big matrix is passed to the Mapper in chunks of complete rows. Smaller cross-products
    are computed for these submatrices and passed on to a single Reducer, which sums
    them together. Since we have a single key, a Combiner is mandatory and since the
    matrix sum is associative and commutative, we certainly can use it here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we have a large amount of data stored in **Hadoop Distributed File System**
    (**HDFS**), we need to pass its path value to the input parameters in the `MapReduce`
    method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the preceding code, we saw that `X` is the design matrix, which has been
    created with the following function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Its output will look as shown in the following screenshot:![Linear regression
    with R and Hadoop](img/3282OS_06_10.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, here all the columns will be considered as explanatory variables and their
    standard errors can be calculated in a similar manner to how we calculated them
    with normal linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the `Xty` value with MapReduce job 2 is pretty much the same as
    for the vector `y`, which is available to the nodes according to normal scope
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To derive the coefficient values with `solve (Xtx, Xty)`, use the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we just need to call the following line of code to get the coefficient
    values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command will be as shown in the following screenshot:![Linear
    regression with R and Hadoop](img/3282OS_06_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In statistics, logistic regression or logit regression is a type of probabilistic
    classification model. Logistic regression is used extensively in numerous disciplines,
    including the medical and social science fields. It can be binomial or multinomial.
  prefs: []
  type: TYPE_NORMAL
- en: Binary logistic regression deals with situations in which the outcome for a
    dependent variable can have two possible types. Multinomial logistic regression
    deals with situations where the outcome can have three or more possible types.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression can be implemented using logistic functions, which are listed
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'To predict the log odds ratios, use the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: logit(p) = β0 + β1 × x1 + β2 × x2 + ... + βn × xn
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The probability formula is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p = e^(logit(p)) ⁄ 1 + e^(logit(p))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`logit(p)` is a linear function of the explanatory variable, X (x1,x2,x3..xn),
    which is similar to linear regression. So, the output of this function will be
    in the range 0 to 1\. Based on the probability score, we can set its probability
    range from 0 to 1\. In a majority of the cases, if the score is greater than 0.5,
    it will be considered as 1, otherwise 0\. Also, we can say it provides a classification
    boundary to classify the outcome variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/3282OS_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure is of a training dataset. Based on the training dataset
    plot, we can say there is one classification boundary generated by the `glm` model
    in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications of logistic regression include:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the likelihood of an online purchase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting the presence of diabetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression with R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To perform logistic regression with R, we will use the `iris` dataset and the
    `glm` model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Logistic regression with R and Hadoop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To perform logistic regression with R and Hadoop, we will use RHadoop with `rmr2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The outline of the logistic regression algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the `lr.map` Mapper function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the `lr.reducer` Reducer function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the `logistic.regression` MapReduce function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's understand them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: We will first define the logistic regression function with gradient decent.
    Multivariate regression can be performed by forming the nondependent variable
    into a matrix data format. For factorial variables, we can translate them to binary
    variables for fitting the model. This function will ask for `input`, `iterations`,
    `dims`, and `alpha` as input parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '`lr.map`: This stands for the logistic regression Mapper, which will compute
    the contribution of subset points to the gradient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`lr.reducer`: This stands for the logistic regression Reducer, which is performing
    just a big sum of all the values of key 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`logistic.regression`: This will mainly define the `logistic.regression` MapReduce
    function with the following input parameters. Calling this function will start
    executing logistic regression of the MapReduce function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input`: This is an input dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iterations`: This is the fixed number of iterations for calculating the gradient'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dims`: This is the dimension of input variables'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: This is the learning rate'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see how to develop the logistic regression function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run this logistic regression function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression with R and Hadoop](img/3282OS_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Unsupervised machine learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, unsupervised learning is used for finding the hidden structure
    from the unlabeled dataset. Since the datasets are not labeled, there will be
    no error while evaluating for potential solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised machine learning includes several algorithms, some of which are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector quantization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will consider popular clustering algorithms here.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering is the task of grouping a set of object in such a way that similar
    objects with similar characteristics are grouped in the same category, but other
    objects are grouped in other categories. In clustering, the input datasets are
    not labeled; they need to be labeled based on the similarity of their data structure.
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised machine learning, the classification technique performs the
    same procedure to map the data to a category with the help of the provided set
    of input training datasets. The corresponding procedure is known as clustering
    (or cluster analysis), and involves grouping data into categories based on some
    measure of inherent similarity; for example, the distance between data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the following figure, we can identify clustering as grouping objects based
    on their similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering](img/3282OS_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are several clustering techniques available within R libraries, such as
    k-means, k-medoids, hierarchical, and density-based clustering. Among them, k-means
    is widely used as the clustering algorithm in data science. This algorithm asks
    for a number of clusters to be the input parameters from the user side.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications of clustering are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Market segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social network analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing computer network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Astronomical data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering with R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are considering the `k-means` method here for implementing the clustering
    model over the `iris` input dataset, which can be achieved by just calling its
    in-built R dataset – the `iris` data (for more information, visit [http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html](http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html)).
    Here we will see how k-means clustering can be performed with R.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Deriving clusters for small datasets is quite simple, but deriving it for huge
    datasets requires the use of Hadoop for providing computation power.
  prefs: []
  type: TYPE_NORMAL
- en: Performing clustering with R and Hadoop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the k-means clustering algorithm is already developed in RHadoop, we are
    going to use and understand it. You can make changes in their Mappers and Reducers
    as per the input dataset format. As we are dealing with Hadoop, we need to develop
    the Mappers and Reducers to be run on nodes in a parallel manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'The outline of the clustering algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the `dist.fun` distance function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the `k-means.map` k-means Mapper function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the `k-means.reduce` k-means Reducer function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the `k-means.mr` k-means MapReduce function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining input data points to be provided to the clustering algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we will run `k-means.mr` (the k-means MapReduce job) by providing the required
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: '`dist.fun`: First, we will see the `dist.fun` function for calculating the
    distance between a matrix of center `C` and a matrix of point `P`, which is tested.
    It can produce 10⁶ points and 10² centers in five dimensions in approximately
    16 seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`k-means.map`: The Mapper of the k-means MapReduce algorithm will compute the
    distance between points and all the centers and return the closest center for
    each point. This Mapper will run in iterations based on the following code. With
    the first iteration, the cluster center will be assigned randomly and from the
    next iteration, it will calculate these cluster centers based on the minimum distance
    from all the points of the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`k-means.reduce`: The Reducer of the k-means MapReduce algorithm will compute
    the column average of matrix points as key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`kmeans.mr`: Defining the k-means MapReduce function involves specifying several
    input parameters, which are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`P`: This denotes the input data points'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num.clusters`: This is the total number of clusters'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num.iter`: This is the total number of iterations to be processed with datasets'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`combine`: This will decide whether the Combiner should be enabled or disabled
    (`TRUE` or `FALSE`)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Defining the input data points to be provided to the clustering algorithms:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running `kmeans.mr` (the k-means MapReduce job) by providing it with the required
    parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command is shown in the following screenshot:![Performing
    clustering with R and Hadoop](img/3282OS_06_06.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recommendation is a machine-learning technique to predict what new items a user
    would like based on associations with the user's previous items. Recommendations
    are widely used in the field of e-commerce applications. Through this flexible
    data and behavior-driven algorithms, businesses can increase conversions by helping
    to ensure that relevant choices are automatically suggested to the right customers
    at the right time with cross-selling or up-selling.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when a customer is looking for a Samsung Galaxy S IV/S4 mobile
    phone on Amazon, the store will also suggest other mobile phones similar to this
    one, presented in the **Customers Who Bought This Item Also Bought** window.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two different types of recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User-based recommendations**: In this type, users (customers) similar to
    current user (customer) are determined. Based on this user similarity, their interested/used
    items can be recommended to other users. Let''s learn it through an example.![Recommendation
    algorithms](img/3282OS_06_07.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assume there are two users named Wendell and James; both have a similar interest
    because both are using an iPhone. Wendell had used two items, iPad and iPhone,
    so James will be recommended to use iPad. This is user-based recommendation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Item-based recommendations**: In this type, items similar to the items that
    are being currently used by a user are determined. Based on the item-similarity
    score, the similar items will be presented to the users for cross-selling and
    up-selling type of recommendations. Let''s learn it through an example.![Recommendation
    algorithms](img/3282OS_06_08.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, a user named Vaibhav likes and uses the following books:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Apache Mahout Cookbook*, *Piero Giacomelli*, *Packt Publishing*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hadoop MapReduce Cookbook*, *Thilina Gunarathne* and *Srinath Perera*, *Packt
    Publishing*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hadoop Real-World Solutions Cookbook*, *Brian Femiano*, *Jon Lentz*, and *Jonathan
    R. Owens*, *Packt Publishing*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Big Data For Dummies*, *Dr. Fern Halper*, *Judith Hurwitz*, *Marcia Kaufman*,
    and *Alan Nugent*, *John Wiley & Sons Publishers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on the preceding information, the recommender system will predict which
    new books Vaibhav would like to read, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Big Data Analytics with R and Hadoop*, *Vignesh Prajapati*, *Packt Publishing*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we will see how to generate recommendations with R and Hadoop. But before
    going towards the R and Hadoop combination, let us first see how to generate it
    with R. This will clear the concepts to translate your generated recommender systems
    to MapReduce recommendation algorithms. In case of generating recommendations
    with R and Hadoop, we will use the RHadoop distribution of **Revolution Analytics**.
  prefs: []
  type: TYPE_NORMAL
- en: Steps to generate recommendations in R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To generate recommendations for users, we need to have datasets in a special
    format that can be read by the algorithm. Here, we will use the collaborative
    filtering algorithm for generating the recommendations rather than content-based
    algorithms. Hence, we will need the user's rating information for the available
    item sets. So, the `small.csv` dataset is given in the format `user ID, item ID,
    item's ratings`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code and datasets are reproduced from the book *Mahout in Action,
    Robin Anil, Ellen Friedman, Ted Dunning,* and *Sean Owen, Manning Publications*
    and the website is [http://www.fens.me/](http://www.fens.me/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recommendations can be derived from the matrix-factorization technique as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To generate the recommenders, we will follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Computing the co-occurrence matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establishing the user-scoring matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating recommendations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the next section, we will see technical details for performing the preceding
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: In the first section, computing the co-occurrence matrix, we will be able to
    identify the co-occurred item sets given in the dataset. In simple words, we can
    call it counting the pair of items from the given dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To establish the user-scoring matrix based on the user's rating information,
    the user-item rating matrix can be generated for users.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, the recommendations as output can be generated by the product operations
    of both matrix items: co-occurrence matrix and user''s scoring matrix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generating recommendations via **Myrrix** and R interface is quite easy. For
    more information, refer to [https://github.com/jwijffels/Myrrix-R-interface](https://github.com/jwijffels/Myrrix-R-interface).
  prefs: []
  type: TYPE_NORMAL
- en: Generating recommendations with R and Hadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To generate recommendations with R and Hadoop, we need to develop an algorithm
    that will be able to run and perform data processing in a parallel manner. This
    can be implemented using Mappers and Reducers. A very interesting part of this
    section is how we can use R and Hadoop together to generate recommendations from
    big datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here are the steps that are similar to generating recommendations with
    R, but translating them to the Mapper and Reducer paradigms is a little tricky:'
  prefs: []
  type: TYPE_NORMAL
- en: Establishing the co-occurrence matrix items.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establishing the user scoring matrix to articles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating recommendations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will use the same concepts as our previous operation with R to generate recommendations
    with R and Hadoop. But in this case, we need to use a key-value paradigm as it's
    the base of parallel operations. Therefore, every function will be implemented
    by considering the key-value paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first section, establishment of the co-occurrence matrix items, we will
    establish co-occurrence items in steps: grouped by user, locate each user-selected
    items appearing alone counting, and counting in pairs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The key points to note are:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`train.mr`: This is the MapReduce job''s key-value paradigm information'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**key**: This is the list of items vector'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**value**: This is the item combination vector'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The co-occurrence matrix items will be combined to count them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To define a MapReduce job, `step2.mr` is used for calculating the frequency
    of the combinations of items.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Step2.mr`: This is the MapReduce job''s key value paradigm information'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**key**: This is the list of items vector'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**value**: This is the co-occurrence matrix dataframe value (`item`, `item`,
    `Freq`)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To establish the user-scoring matrix to articles, let us define the `Train2.mr`
    MapReduce job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Train2.mr`: This is the MapReduce job''s key value paradigm information'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**key**: This is the list of items'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**value**: This is the value of the user goods scoring matrix'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the consolidation and co-occurrence scoring matrix:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`eq.hdfs`: This is the MapReduce job''s key value paradigm information'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**key**: The key here is null'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**value**: This is the merged dataframe value'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the section of generating recommendations, we will obtain the recommended
    list of results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Cal.mr`: This is the MapReduce job''s key value paradigm information'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**key**: This is the list of items'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**value**: This is the recommended result dataframe value'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By defining the result for getting the list of recommended items with preference
    value, the sorting process will be applied on the recommendation result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`result.mr`: This is the MapReduce job''s key value paradigm information'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**key**: This is the user ID'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**value**: This is the recommended outcome dataframe value'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we have designed the collaborative algorithms for generating item-based
    recommendation. Since we have tried to make it run on parallel nodes, we have
    focused on the Mapper and Reducer. They may not be optimal in some cases, but
    you can make them optimal by using the available code.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how we can perform Big Data analytics with machine
    learning with the help of R and Hadoop technologies. In the next chapter, we will
    learn how to enrich datasets in R by integrating R to various external data sources.
  prefs: []
  type: TYPE_NORMAL
