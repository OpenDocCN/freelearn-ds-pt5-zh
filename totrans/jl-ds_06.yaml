- en: Chapter 6. Supervised Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is often believed that data science is machine learning, which means in data
    science, we only train models of machine learning. But data science is much more
    than that. Data science involves understanding data, gathering data, munging data,
    taking the meaning out of that data, and then machine learning if needed.
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, machine learning is the most exciting field that exists today.
    With huge amounts of data that is readily available, we can gather invaluable
    knowledge. Lots of companies have made their machine learning libraries accessible
    and there are lots of open source alternatives that exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will study the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is machine learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is overfitting and underfitting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias-variance trade-off
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction and selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is machine learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, when we talk about machine learning, we get into the idea of us fighting
    wars with intelligent machines that we created but went out of control. These
    machines are able to outsmart the human race and become a threat to human existence.
    These theories are just created for our entertainment. We are still very far away
    from such machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the question is: what is machine learning? Tom M. Mitchell gave a formal
    definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P if its performance at tasks in T, as
    measured by P, improves with experience E."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This implies that machine learning is teaching computers to generate algorithms
    using data without programming them explicitly. It transforms data into actionable
    knowledge. Machine learning has close association with statistics, probability,
    and mathematical optimization.
  prefs: []
  type: TYPE_NORMAL
- en: As technology grows, there is one thing that grows with it exponentially—data.
    We have huge amounts of unstructured and structured data growing at a very great
    pace. Lots of data is generated by space observatories, meteorologists, biologists,
    fitness sensors, surveys, and so on. It is not possible to manually go through
    this much amount of data and find patterns or gain insights. This data is very
    important for scientists, domain experts, governments, health officials, and even
    businesses. To gain knowledge out of this data, we need self-learning algorithms
    that can help us in decision making.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning evolved as a subfield of artificial intelligence, which eliminates
    the need to manually analyze large amounts of data. Instead of using machine learning,
    we make data-driven decisions by gaining knowledge using self-learning predictive
    models. Machine learning has become important in our daily lives. Some common
    use cases include search engines, games, spam filters, and image recognition.
    Self-driving cars also uses machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some basic terminologies used in machine learning include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Features**: Distinctive characteristics of the data point or record'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training set**: This is the dataset that we feed to train the algorithm that
    helps us to find relationships or build a model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing set**: The algorithm generated using the training dataset is tested
    on the testing dataset to find the accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature vector**: An n-dimensional vector that contains the features defining
    an object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample**: An item from the dataset or the record'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses of machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning in one way or another is used everywhere. Its applications
    are endless. Let''s discuss some very common use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**E-mail spam filtering**: Every major e-mail service provider uses machine
    learning to filter out spam messages from the Inbox to the Spam folder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicting storms and natural disasters**: Machine learning is used by meteorologists
    and geologists to predict the natural disasters using weather data, which can
    help us to take preventive measures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Targeted promotions/campaigns and advertising**: On social sites, search
    engines, and maybe in mailboxes, we see advertisements that somehow suit our tastes.
    This is made feasible using machine learning on the data from our past searches,
    our social profile, or e-mail contents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-driving cars**: Technology giants are currently working on self-driving
    cars. This is made possible using machine learning on the feed of the actual data
    from human drivers, image and sound processing, and various other factors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning is also used by businesses to predict the market.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can also be used to predict the outcomes of elections and the sentiment of
    voters towards a particular candidate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning is also being used to prevent crime. By understanding the pattern
    of different criminals, we can predict a crime that can happen in the future and
    can prevent it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One case that got a huge amount of attention was of a big retail chain in the
    United States using machine learning to identify pregnant women. The retailer
    thought of a strategy to give discounts on multiple maternity products, so that
    the women would become loyal customers and would baby purchase items with a high
    profit margin.
  prefs: []
  type: TYPE_NORMAL
- en: The retailer worked on the algorithm to predict the pregnancy using useful patterns
    in purchases of different products which are useful for pregnant women.
  prefs: []
  type: TYPE_NORMAL
- en: Once a man approached the retailer and asked for the reason that his teenage
    daughter is receiving discount coupons for maternity items. The retail chain offered
    an apology but later the father himself apologized when he got to know that his
    daughter was indeed pregnant.
  prefs: []
  type: TYPE_NORMAL
- en: This story may or may not be completely true, but retailers do analyze their
    customers' data routinely to find out patterns for targeted promotions, campaigns,
    and inventory management.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning and ethics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s see where machine learning is used very frequently:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retailers**: In the previous example, we mentioned how retail chains use
    data for machine learning to increase their revenue as well as to retain their
    customers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spam filtering**: E-mails are processed using various machine learning algorithms
    for spam filtering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Targeted advertisements**: In our mailbox, social sites, or search engines,
    we see advertisements of our liking'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are only some of the actual use cases that are implemented in the world
    today. One thing that is common between them is the user data.
  prefs: []
  type: TYPE_NORMAL
- en: In the first example, retailers are using the history of transactions done by
    the user for targeted promotions and campaigns and for inventory management, among
    other things. Retail giants do this by providing users a loyalty or sign-up card.
  prefs: []
  type: TYPE_NORMAL
- en: In the second example, the e-mail service provider uses trained machine learning
    algorithms to detect and flag spam. It does so by going through the contents of
    e-mail/attachments and classifying the sender of the e-mail.
  prefs: []
  type: TYPE_NORMAL
- en: In the third example, again the e-mail provider, social network, or search engine
    will go through our cookies, our profiles, or our mails to do the targeted advertising.
  prefs: []
  type: TYPE_NORMAL
- en: In all of these examples, it is mentioned in the terms and conditions of the
    agreement when we sign up with the retailer, e-mail provider, or social network
    that the user's data will be used but privacy will not be violated.
  prefs: []
  type: TYPE_NORMAL
- en: It is really important that before using data that is not publicly available,
    we take the required permissions. Also, our machine learning models shouldn't
    discriminate on the basis of region, race, and sex, or of any other kind. The
    data provided should not be used for purposes not mentioned in the agreement or
    if it is illegal in the region or country of existence.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning – the process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning algorithms are trained in keeping with the idea of how the
    human brain works. They are somewhat similar. Let's discuss the whole process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The machine learning process can be described in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Abstraction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generalization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These three steps are the core of how the machine learning algorithm works.
    Although the algorithm may or may not be divided or represented in such a way,
    this explains the overall approach:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step concentrates on what data should be there and what shouldn't.
    On the basis of that, it gathers, stores, and cleans the data as per the requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second step entails the data being translated to represent the bigger class
    of data. This is required as we cannot capture everything and our algorithm should
    not be applicable for only the data that we have.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third step focuses on the creation of the model or an action that will use
    this abstracted data, which will be applicable for the broader mass.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, what should be the flow of approaching a machine learning problem?
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning – the process](img/B05321_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this particular figure, we see that the data goes through the abstraction
    process before it can be used to create the machine learning algorithm. This process
    itself is cumbersome. We studied this process in the chapter related to data munging.
  prefs: []
  type: TYPE_NORMAL
- en: The process follows the training of the model, which is fitting the model into
    the dataset that we have. The computer does not pick up the model on its own,
    but it is dependent on the learning task. The learning task also includes generalizing
    the knowledge gained on the data that we don't have yet.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, training the model is based on the data that we currently have and
    the learning task includes generalization of the model for future data.
  prefs: []
  type: TYPE_NORMAL
- en: It depends on how our model deduces knowledge from the dataset that we currently
    have. We need to make such a model that can gather insights into something that
    wasn't known to us before can be useful and can be linked to future data.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning is divided mainly into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In supervised learning, the model/machine is presented with inputs and the outputs
    corresponding to those inputs. The machine learns from these inputs and applies
    this learning in further unseen data to generate outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning doesn't have the required outputs; therefore it is up
    to the machine to learn and find patterns that were previously unseen.
  prefs: []
  type: TYPE_NORMAL
- en: In reinforcement learning, the machine continuously interacts with the environment
    and learns through this process. This includes a feedback loop.
  prefs: []
  type: TYPE_NORMAL
- en: What is bias-variance trade-off?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s understand what bias and variance are. First we will go through bias
    in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: Bias is the difference between the predictions that have been generated by the
    model and the correct value that was expected or we should have received.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we get the new data, the model will work out and give predictions. Therefore,
    it means our model has a range of predictions it can generate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias is the correctness of this range of predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s understand variance and how it affects the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Variance is the variability of the model when the data points are changed or
    new data is introduced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It shouldn't be required to tweak the model every time new data is introduced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As per our understanding of bias and variance, we can conclude that these affect
    each other. Therefore, while creating the model, we keep this trade-off in consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Effects of overfitting and underfitting on a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overfitting happens when the model that we have created also starts considering
    the outliers or noise in our dataset. Therefore, it means our model is fitting
    the dataset rather too well.
  prefs: []
  type: TYPE_NORMAL
- en: The drawback of such a model is that it will not be able to generalize well.
    Such models have low bias and high variance.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting happens when the model that we have created is not able to find
    out the patterns or trend of the data as is desired. Therefore, it means the model
    is not fitting to the dataset well.
  prefs: []
  type: TYPE_NORMAL
- en: The drawback of such a model is that it is not able to give good predictions.
    Such models have high bias and low variance.
  prefs: []
  type: TYPE_NORMAL
- en: We should try to reduce both underfitting and overfitting. This is done through
    various techniques. Ensemble models are very good in avoiding underfitting and
    overfitting. We will study ensemble models in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is a very good example of "divide and conquer". It is one of
    the most practical and widely used methods for inductive inference. It is a supervised
    learning method that can be used for both classification and regression. It is
    non-parametric and its aim is to learn by inferring simple decision rules from
    the data and create such a model that can predict the value of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Before taking a decision, we analyze the probability of the pros and cons by
    weighing the different options that we have. Let's say we want to purchase a phone
    and we have multiple choices in the price segment. Each of the phones has something
    really good, and maybe better than the other. To make a choice, we start by considering
    the most important feature that we want. And as such, we create a series of features
    that it has to pass to become the ultimate choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entropy measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also learn about famous decision tree learning algorithms such as ID3
    and C5.0.
  prefs: []
  type: TYPE_NORMAL
- en: Building decision trees – divide and conquer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A heuristic called recursive partitioning is used to build decision trees. In
    this approach, our data is split into similar classes of smaller and smaller subsets
    as we move along.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree is actually an inverted tree. It starts from the root and ends
    up at the leaf nodes, which are the terminal nodes. The splitting of the node
    into branches is based on logical decisions. The whole dataset is represented
    at the root node. A feature is chosen by the algorithm that is most predictive
    of the target class. Then it partitions the examples into distinct value groups
    of this particular feature. This represents the first set of the branches of our
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: The divide-and-conquer approach is followed until the end point is reached.
    At each step, the algorithm continues to choose the best candidate feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'The end point is defined when:'
  prefs: []
  type: TYPE_NORMAL
- en: At a particular node, nearly all the examples belong to the same class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature list is exhausted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A predefined size limit of the tree is reached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Building decision trees – divide and conquer](img/B05321_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding image is a very famous example of decision tree. Here, a decision
    tree is made to find out whether to go out or not:'
  prefs: []
  type: TYPE_NORMAL
- en: Outlook is the root node. This refers to all the possible classes of the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sunny, overcast, and Rain are the branches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Humidity and Wind are the leaf nodes, which are again split into branches, and
    a decision is taken depending on the favorable environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These trees can also be re-represented as if-then rules, which would be easily
    understandable. Decision trees are one of the very successful and popular algorithms,
    with a broad range of applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the applications of decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision of credit card / loan approval**: Credit scoring models are based
    on decision trees, where every applicant''s information is fed to decide whether
    a credit card / loan should be approved or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnosis**: Various diseases are diagnosed using well-defined and
    tested decision trees based on symptoms, measurements, and tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where should we use decision tree learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although there are various decision tree learning methods that can be used
    for a variety of problems, decision trees are best suited for the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Attribute-value pairs are scenarios where instances are described by attributes
    from a fixed set and values. In the previous example, we had the attribute as
    "Wind" and the values as "Strong" and "Weak". These disjoint possible values make
    it easy to create decision tree learning, although attributes with real values
    can also be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final output of the target function has a discreet value, like the previous
    example, where we had "Yes" or "No". The decision tree algorithm can be extended
    to have more than two possible target values. Decision trees can also be extended
    to have real values as outputs, but this is rarely used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision tree algorithm is robust to errors in the training dataset. These
    errors can be in the attribute values of the examples or the classification of
    the examples or both.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree learning is also suited for missing values in a dataset. If the
    values are missing in some examples where they are available for attributes in
    other examples, then decision trees can be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is easy to understand and interpret decision trees. Visualizing decision
    trees is easy too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other algorithms, data normalization needs to be done before they can be
    applied. Normalization refers to the creation of dummy variables and removing
    blank values. Decision trees, on the other hand, require very less data preparation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost involved in prediction using a decision tree is logarithmic with respect
    to the number of examples used in training the tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees, unlike other algorithms, can be applied to both numerical and
    categorical data. Other algorithms are generally specialized to be used for only
    one type of variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees can easily take care of the problems where multiple outputs is
    a possibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees follow the white box model, which means a condition is easily
    explained using Boolean logic if the situation is observable in the model. On
    the other hand, results are comparatively difficult to interpret in a black box
    model, such as artificial neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical tests can be used to validate the model. Therefore, we can test
    the reliability of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is able to perform well even if there is a violation in the assumptions from
    the true model that was the source of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages of decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve covered where decision trees are suited and their advantages. Now we
    will go through the disadvantages of decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: There is always a possibility of overfitting the data in decision trees. This
    generally happens when we create trees that are over-complex and are not possible
    to generalize well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To avoid this, various steps can be taken. One method is pruning. As the name
    suggests, it is a method where we set the maximum depth to which the tree can
    grow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instability is always a concern with decision trees as a small variation in
    the data can result in generation of a different tree altogether.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution to such a scenario is ensemble learning, which we will study in
    the next chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree learning may sometimes lead to the creation of biased trees, where
    some classes are dominant over others. The solution to such a scenario is balancing
    the dataset prior to fitting it to the decision tree algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree learning is known to be NP-complete, considering several aspects
    of optimality. This holds even for the basic concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, heuristic algorithms like greedy algorithms are used where a locally
    optimal decision is made at each node. This doesn't guarantee that we will have
    a decision tree that is globally optimal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning can be hard for the concepts such as Parity, XOR, and multiplexer problems,
    where decision trees cannot represent them easily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are various decision tree learning algorithms that are actually variations
    of the core algorithm. The core algorithm is actually a top-down, greedy search
    through all possible trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to discuss two algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: ID3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C4.5 and C5.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first algorithm, **ID3** (**Iterative Dichotomiser 3**), was developed by
    Ross Quinlan in 1986\. The algorithm proceeds by creating a multiway tree, where
    it uses a greedy search to find each node and the features that can yield maximum
    information gain for the categorical targets. As trees can grow to the maximum
    size, which can result in over-fitting of data, pruning is used to make the generalized
    model.
  prefs: []
  type: TYPE_NORMAL
- en: C4.5 came after ID3 and eliminated the restriction that all features must be
    categorical. It does this by defining dynamically a discrete attribute based on
    the numerical variables. This partitions into a discrete set of intervals from
    the continuous attribute value. C4.5 creates sets of if-then rules from the trained
    trees of the ID3 algorithm. C5.0 is the latest version; it builds smaller rule
    sets and uses comparatively lesser memory.
  prefs: []
  type: TYPE_NORMAL
- en: How a decision tree algorithm works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The decision tree algorithm constructs the top-down tree. It follows these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To know which element should come at the root of the tree, a statistical test
    is done on each instance of the attribute to determine how well the training examples
    can be classified using this attribute alone.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This leads to the selection of the best attribute at the root node of the tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now at this root node, for each possible value of the attribute, the descendants
    are created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The examples in our training dataset are sorted to each of these descendant
    nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now for these individual descendant nodes, all the previous steps are repeated
    for the remaining examples in our training dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This leads to the creation of an acceptable tree for our training dataset using
    a greedy search. The algorithm never backtracks, which means it never reconsiders
    the previous choices and follows the tree downwards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding and measuring purity of node
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decision tree is built top-down. It can be tough to decide on which attribute
    to split on each node. Therefore, we find the feature that best splits the target
    class. Purity is the measure of a node containing only one class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Purity in C5.0 is measured using entropy. The entropy of the sample of the
    examples is the indication of how class values are mixed across the examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '0: The minimum value is an indication of the homogeneity in the class values
    in the sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1: The maximum value is an indication that there is maximum amount of disorder
    in the class values in the sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Entropy is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding and measuring purity of node](img/image_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, *S* refers to the dataset that we have and *c* refers
    to the class levels. For a given class *i*, *p* is the proportion of the values.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the purity measure is determined, the algorithm has to decide on which
    feature the data should be split. To decide this, the entropy measure is used
    by the algorithm to calculate how the homogeneity differs on splitting on each
    possible feature. This particular calculation done by the algorithm is the information
    gain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding and measuring purity of node](img/image_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The difference between the entropy before splitting the dataset (*S1*) and the
    resulting partitions from splitting (*S2*) is called information gain (*F*).
  prefs: []
  type: TYPE_NORMAL
- en: An example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's apply what we've learned to create a decision tree using Julia. We will
    be using the example available for Python on [http://scikit-learn.org/](http://scikit-learn.org/)
    and Scikitlearn.jl by Cedric St-Jean.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first have to add the required packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'ScikitLearn provides the interface to the much-famous library of machine learning
    for Python to Julia:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After adding the required packages, we will create the dataset that we will
    be using in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will generate a 16-element `Array{Float64,1}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will create instances of two different models. One model is where we
    will not limit the depth of the tree, and in other model, we will prune the decision
    tree on the basis of purity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An example](img/image_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will now fit the models to the dataset that we have. We will fit both the
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '![An example](img/image_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the first model. Here our decision tree has `25` leaf nodes and a depth
    of `8`.
  prefs: []
  type: TYPE_NORMAL
- en: '![An example](img/image_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the second model. Here we prune our decision tree. This has `6` leaf
    nodes and a depth of `4`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will use the models to predict on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This creates a 501-element `Array{Float64,1}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the results, let''s plot both the models on the dataset
    that we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Decision trees can tend to overfit data. It is required to prune the decision
    tree to make it more generalized. But if we do more pruning than required, then
    it may lead to an incorrect model. So, it is required that we find the most optimized
    pruning level.
  prefs: []
  type: TYPE_NORMAL
- en: '![An example](img/image_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is quite evident that the first decision tree overfits to our dataset, whereas
    the second decision tree model is comparatively more generalized.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning using Naïve Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naïve Bayes is one of most famous machine learning algorithms to date. It is
    widely used in text classification techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes methods come under the set of supervised learning algorithms. It
    is a probabilistic classifier and is based on Bayes' theorem. It takes the "naïve"
    assumption that every pair of features is independent of one another.
  prefs: []
  type: TYPE_NORMAL
- en: And in spite of these assumptions, Naïve Bayes classifiers work really well.
    Their most famous use case is spam filtering. The effectiveness of this algorithm
    is justified by the requirement of quite a small amount of training data for estimating
    the required parameters.
  prefs: []
  type: TYPE_NORMAL
- en: These classifiers and learners are quite fast when compared to other methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Supervised learning using Naïve Bayes](img/image_06_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this given formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A* and *B* are events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(A)* and *P(B)* are probabilities of *A* and *B*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are prior probabilities and are independent of each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(A | B)* is the probability of *A* with the condition that *B* is true. It
    is the posterior probability of class (*A*, target) given predictor (*B*, attributes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(B | A)* is the probability of *B* with the condition that *A* is true. It
    is the likelihood, which is the probability of the predictor given class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of Naïve Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following are some of the advantages of Naïve Bayes:'
  prefs: []
  type: TYPE_NORMAL
- en: It is relatively simple to build and understand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be trained easily and doesn't require a huge dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is comparatively fast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is not affected by irrelevant features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages of Naïve Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The disadvantage of Naïve Bayes is the "naïve" assumption that every feature
    is independent. This is not always true.
  prefs: []
  type: TYPE_NORMAL
- en: Uses of Naïve Bayes classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are a few uses of Naïve Bayes classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Naïve Bayes text classification**: This is used as a probabilistic learning
    method and is actually one of the most successful algorithms to classify documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spam filtering**: This is the best known use case of Naïve Bayes. Naïve Bayes
    is used to identify spam e-mail from legitimate e-mail. Many server-side e-mail
    filtering mechanisms use this with other algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommender systems**: Naïve Bayes can also be used to build recommender
    systems. Recommender systems are used to predict and suggest products the user
    may like in the future. It is based on unseen data and is used with collaborative
    filtering to do so. This method is more scalable and generally performs better
    than other algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand how Naïve Bayes classifiers actually work, we should understand
    the Bayesian rule. It was formed by Thomas Bayes in the 18^(th) century. He developed
    various mathematical principles, which are known to us as Bayesian methods. These
    very effectively describe probabilities of events and how the probabilities should
    be revised when we have additional information.
  prefs: []
  type: TYPE_NORMAL
- en: Classifiers, based on Bayesian methods, use the training dataset to find out
    the observed probability of every class based on the values of all the features.
    So, when this classifier is used on unlabeled or unseen data, it makes use of
    the observed probabilities to predict to which class the new features belong.
    Although it is a very simple algorithm, its performance is comparable or better
    than most other algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian classifiers are best used for these cases:'
  prefs: []
  type: TYPE_NORMAL
- en: A dataset containing numerous attributes, where all of them should be considered
    simultaneously to calculate the probability of an outcome.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features with weak effects are generally ignored, but Bayesian classifiers use
    them too to generate predictions. Many such weak features can lead to a big change
    in the decision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Bayesian methods work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bayesian methods are dependent on the concept that the estimation of likelihood
    of an event is based on the evidence at hand. The possible outcome of the situation
    is the event; for example, in a coin toss, we get heads or tails. Similarly, a
    mail can be "ham" or "spam". The trial refers to a single opportunity in which
    an event occurs. In our previous example, the coin toss is the trial.
  prefs: []
  type: TYPE_NORMAL
- en: Posterior probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*posterior probability = conditional probability * prior probability/evidence*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In terms of classification, posterior probability refers to the probability
    that a particular object belongs to a class x when the observed feature values
    are given. For example, "what is the probability that it will rain given the temperature
    and percentage of humidity?"
  prefs: []
  type: TYPE_NORMAL
- en: '*P(rain | xi), xi = [45degrees, 95%humidity]*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let *xi* be the feature vector of the sample *i*, where *i*" belongs to *{1,2,3,.....n*}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let *wj* be the notation of the class *j*, where *j* belongs to *{1,2,3,......n}*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(xi | wi)* is the probability of the observing sample *xi* when it is given
    that it belongs to class *wj*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The general notation of posterior probabilities is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(*wj *| xi) = P(xi | wj) * P(wj)/P(xi)*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The main objective of Naïve Bayes is to maximize the probability of the posterior
    probability on the given training data so that a decision rule can be formed.
  prefs: []
  type: TYPE_NORMAL
- en: Class-conditional probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bayesian classifiers assume that all the samples in the dataset are independent
    and identically distributed. Here, independence means that the probability of
    one observation is not affected by the probability of the other observation.
  prefs: []
  type: TYPE_NORMAL
- en: One very famous example that we discussed is the coin toss. Here the outcome
    of the first coin toss doesn't affect the subsequent coin tosses. The probability
    of getting the head or tail always remains 0.5 for an unbiased coin.
  prefs: []
  type: TYPE_NORMAL
- en: An added assumption is that the features have conditional independence. This
    is another "naïve" assumption, which means that the estimation of the likelihood
    or the class-conditional probabilities can be done directly from the training
    data without needing to evaluate all the probabilities of x.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand with an example. Suppose we have to create a server-side e-mail
    filtering application to decide if the mails are spam or not. Let's say we have
    around 1,000 e-mails and 100 e-mails are spam.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we received a new mail with the text "Hello Friend". So, how should we
    calculate the class-conditional probability of the new message?
  prefs: []
  type: TYPE_NORMAL
- en: 'The pattern of the text consists of two features: "hello" and "friend". Now,
    we will calculate the class-conditional probability of the new mail.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Class-conditional probability is the probability of encountering "hello" when
    the mail is spam * the probability of encountering "friend" when the mail is spam:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(X=[hello,world] | w=spam) = P(hello | spam) * P(friend | spam)*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can easily find out how many mails contained the word "hello" and how many
    mails contained the word "spam". However, we took the "naïve" assumption that
    one word doesn't influence the occurrence of the other. We know that "hello" and
    "friend" often occur together. Therefore, our assumption is violated.
  prefs: []
  type: TYPE_NORMAL
- en: Prior probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prior probability is the prior knowledge of the occurrence of an event. It is
    the general probability of the occurrence of the particular class. If the priors
    follow a uniform distribution, the posterior probabilities are determined using
    the class-conditional probabilities and also using the evidence term.
  prefs: []
  type: TYPE_NORMAL
- en: Prior knowledge is obtained using the estimation on the training data, when
    the training data is the sample of the entire population.
  prefs: []
  type: TYPE_NORMAL
- en: Evidence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is one more required value to calculate posterior probability, and that
    is "evidence". The evidence P(x) is the probability of occurrence of the particular
    pattern x, which is independent of the class label.
  prefs: []
  type: TYPE_NORMAL
- en: The bag of words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous example, we were doing classification of e-mails. For that,
    we classify a pattern. To classify a pattern, the most important tasks are:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But how are good features recognized? There are some characteristics of good
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: The features must be important to the use case that we are building the classifier
    for
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The selected features should have enough information to distinguish well between
    the different patterns and can be used to train the classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The features should not be susceptible to distortion or scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to first represent the e-mail text document as a feature vector before
    we can fit it to our model and apply machine learning algorithms. The classification
    of the text document uses the bag-of-words model. In this model, we create the
    vocabulary, which is a collection of different words that occur in all the e-mails
    (training set) and then count how many times each word occurred.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of using Naïve Bayes as a spam filter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are the advantages of using Naïve Bayes as a spam filter:'
  prefs: []
  type: TYPE_NORMAL
- en: It can be personalized. It means that it can be trained on a per user basis.
    We sometime subscribe to newsletters or mailing lists or update about products,
    which may be spam to other users. Also, the e-mails that I receive have some words
    related to my work, which may be categorized as spam for other users. So, being
    a legitimate user, I would not like my mails going into spam. We may try to use
    the rules or filters, but Bayesian spam filtering is far more superior than these
    mechanisms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian spam filters are effective in avoiding false positives, by which it
    is very less probable that legitimate e-mail will be classified as spam. For example,
    we all get mails with the word "Nigeria" or claiming to be from Nigeria, which
    are actually phishing scams. But there is the possibility that I have a relative
    or a friend there, or I have some business there; therefore that mail may not
    be illegitimate to me.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages of Naïve Bayes filters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bayesian filters are vulnerable to Bayesian poisoning, which is a technique
    in which a large amount of legitimate text is sent with the spam mail. Therefore,
    the Bayesian filter fails there and marks it as "ham" or legitimate mail.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Naïve Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us create some Naïve Bayes models using Julia:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We added the required `NaiveBayes` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create some dummy datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We created two arrays of `X` and `y`, where an element in `y` represents the
    column in `X`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We loaded an instance of MultinomialNB and fit our dataset to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will use it to predict it on our test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output that I got was:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Which means the first column is `b`, second is `a`, and third is also `a`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example was on a dummy dataset. Let''s apply Naïve Bayes on an actual
    dataset. We will be using the famous iris dataset in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We loaded RDatasets, which contains the iris dataset. We created arrays for
    the feature vectors (sepal length, sepal width, petal length, and petal width).
  prefs: []
  type: TYPE_NORMAL
- en: '![Examples of Naïve Bayes](img/image_06_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we will split the dataset for training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Examples of Naïve Bayes](img/image_06_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is quite straightforward, fitting the dataset to a Naïve Bayes classifier.
    We are also calculating the accuracy to which our model worked. We can see that
    the accuracy was 1.0, which is 100%.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about machine learning and its uses. Providing computers
    the ability to learn and improve has far-reaching uses in this world. It is used
    in predicting disease outbreaks, predicting weather, games, robots, self-driving
    cars, personal assistants, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three different types of machine learning: supervised learning, unsupervised
    learning, and reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned about supervised learning, especially about Naïve
    Bayes and decision trees. In further chapters, we will learn more about ensemble
    learning and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://github.com/JuliaStats/MLBase.jl](https://github.com/JuliaStats/MLBase.jl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://julialang.org/](http://julialang.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/johnmyleswhite/NaiveBayes.jl](https://github.com/johnmyleswhite/NaiveBayes.jl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/bensadeghi/DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/bicycle1885/RandomForests.jl](https://github.com/bicycle1885/RandomForests.jl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
