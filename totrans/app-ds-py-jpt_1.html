<html><head></head><body><div><div><p class="hidden">1</p>
		</div>
		<div><h1 id="_idParaDest-14"><a id="_idTextAnchor014"/>Jupyter Fundamentals</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Describe Jupyter Notebooks and how they are used for data analysis</li>
				<li class="bullets">Describe the features of Jupyter Notebooks</li>
				<li class="bullets">Use Python data science libraries</li>
				<li class="bullets">Perform simple exploratory data analysis</li>
			</ul>
			<p>In this chapter, you will learn and implement the fundamental features of the Jupyter notebook by completing several hands-on erxercises.</p>
		</div>
		<div><h2 id="_idParaDest-15"><a id="_idTextAnchor015"/>Introduction</h2>
			<p>Jupyter Notebooks are one of the most important tools for data scientists using Python. This is because they're an ideal environment for developing reproducible data analysis pipelines. Data can be loaded, transformed, and modeled all inside a single Notebook, where it's quick and easy to test out code and explore ideas along the way. Furthermore, all of this can be documented "inline" using formatted text, so you can make notes for yourself or even produce a structured report.</p>
			<p>Other comparable platforms - for example, RStudio or Spyder - present the user with multiple windows, which promote arduous tasks such as copy and pasting code around and rerunning code that has already been executed. These tools also tend to involve <strong class="keyword">Read Eval Prompt Loops</strong> (<strong class="keyword">REPLs</strong>) where code is run in a terminal session that has saved memory. This type of development environment is bad for reproducibility and not ideal for development either. Jupyter Notebooks solve all these issues by giving the user a single window where code snippets are executed and outputs are displayed inline. This lets users develop code efficiently and allows them to look back at previous work for reference, or even to make alterations.</p>
			<p>We'll start the chapter by explaining exactly what Jupyter Notebooks are and continue to discuss why they are so popular among data scientists. Then, we'll open a Notebook together and go through some exercises to learn how the platform is used. Finally, we'll dive into our first analysis and perform an exploratory analysis in </p>
			<h2 id="_idParaDest-16"><a id="_idTextAnchor016"/>Basic Functionality and Features</h2>
			<p>In this section, we first demonstrate the usefulness of Jupyter Notebooks with examples and through discussion. Then, in order to cover the fundamentals of Jupyter Notebooks for beginners, we'll see the basic usage of them in terms of launching and interacting with the platform. For those who have used Jupyter Notebooks before, this will be mostly a review; however, you will certainly see new things in this topic as well.</p>
			<h3 id="_idParaDest-17"><a id="_idTextAnchor017"/>What is a Jupyter Notebook and Why is it Useful?</h3>
			<p>Jupyter Notebooks are locally run web applications which contain live code, equations, figures, interactive apps, and <strong class="bold">Markdown</strong> text. The standard language is Python, and that's what we'll be using for this book; however, note that a variety of alternatives are supported. This includes the other dominant data science language, R:</p>
			<div><div><img src="img/C13018_01_01.jpg" alt="Figure 1.1: Jupyter Notebook sample workbook" width="1092" height="614"/>
				</div>
			</div>
			<h6>Figure 1.1: Jupyter Notebook sample workbook</h6>
			<p>Those familiar with R will know about R Markdown. <code>README.md</code></a> <strong class="bold">Markdown</strong> file. This format is useful for basic text formatting. It's comparable to HTML but allows for much less customization.</p>
			<p> Commonly used symbols in <strong class="bold">Markdown</strong> include hashes (#) to make text into a heading, square and round brackets to insert hyperlinks, and stars to create italicized or bold text:</p>
			<div><div><img src="img/C13018_01_02.jpg" alt="Figure 1.2: Sample Markdown document" width="926" height="271"/>
				</div>
			</div>
			<h6>Figure 1.2: Sample Markdown document</h6>
			<p>Having seen the basics of Markdown, let's come back to R Markdown, where <strong class="bold">Markdown</strong> text can be written alongside executable code. Jupyter Notebooks offer the equivalent functionality for Python, although, as we'll see, they function quite differently than R <strong class="bold">Markdown</strong> documents. For example, R <strong class="bold">Markdown</strong> assumes you are writing <strong class="bold">Markdown</strong> unless otherwise specified, whereas Jupyter Notebooks assume you are inputting code. This makes it more appealing to use Jupyter Notebooks for rapid development and testing.</p>
			<p>From a data science perspective, there are two primary types for a Jupyter Notebook depending on how they are used: lab-style and deliverable.</p>
			<p>Lab-style Notebooks are meant to serve as the programming analog of research journals. These should contain all the work you've done to load, process, analyze, and model the data. The idea here is to document everything you've done for future reference, so it's usually not advisable to delete or alter previous lab-style Notebooks. It's also a good idea to accumulate multiple date-stamped versions of the Notebook as you progress through the analysis, in case you want to look back at previous states.</p>
			<p>Deliverable Notebooks are intended to be presentable and should contain only select parts of the lab-style Notebooks. For example, this could be an interesting discovery to share with your colleagues, an in-depth report of your analysis for a manager, or a summary of the key findings for stakeholders.</p>
			<p>In either case, an important concept is reproducibility. If you've been diligent in documenting your software versions, anyone receiving the reports will be able to rerun the Notebook and compute the same results as you did. In the scientific community, where reproducibility is becoming increasingly difficult, this is a breath of fresh air.</p>
			<h3 id="_idParaDest-18"><a id="_idTextAnchor018"/>Navigating the Platform</h3>
			<p>Now, we are going to open up a Jupyter Notebook and start to learn the interface. Here, we will assume you have no prior knowledge of the platform and go over the basic usage.</p>
			<h3 id="_idParaDest-19"><a id="_idTextAnchor019"/>Exercise 1: Introducing Jupyter Notebooks</h3>
			<ol>
				<li>Navigate to the companion material directory in the terminal<h4>Note </h4><p class="callout">Unix machines such as Mac or Linux, command-line navigation can be done using <code>ls</code> to display directory contents and <code>cd</code> to change directories. On Windows machines, use <code>dir</code> to display directory contents and use cd to change directories instead. If, for example, you want to change the drive from C: to D:, you should execute d: to change drives.</p></li>
				<li>Start a new local Notebook server here by typing the following into the terminal:<pre>jupyter notebook</pre><p>A new window or tab of your default browser will open the Notebook Dashboard to the working directory. Here, you will see a list of folders and files contained therein.</p></li>
				<li>Click on a folder to navigate to that particular path and open a file by clicking on it. Although its main use is editing IPYNB Notebook files, Jupyter functions as a standard text editor as well.</li>
				<li>Reopen the terminal window used to launch the app. We can see the <code>NotebookApp</code> being run on a local server. In particular, you should see a line like this:<pre>[I 20:03:01.045 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/?token=e915bb06866f19ce462d959a9193a94c7c088e81765f9d8a</pre><p>Going to that HTTP address will load the app in your browser window, as was done automatically when starting the app. Closing the window does not stop the app; this should be done from the terminal by typing <em class="italics">Ctrl</em> + <em class="italics">C</em>.</p></li>
				<li>Close the app by typing <em class="italics">Ctrl</em> + <em class="italics">C</em> in the terminal. You may also have to confirm by entering <code>y</code>. Close the web browser window as well.</li>
				<li>Load the list of available options by running the following code:<pre>jupyter notebook --help</pre></li>
				<li>Open the <code>NotebookApp</code> at local port <code>9000</code> by running the following:<pre>jupyter notebook --port 9000</pre></li>
				<li>Click <strong class="bold">New</strong> in the upper-right corner of the Jupyter Dashboard and select a kernel from the drop-down menu (that is, select something in the <strong class="bold">Notebooks</strong> section):<div><img src="img/C13018_01_03.jpg" alt="Figure 1.3: Selecting a kernel from the drop down menu" width="1011" height="490"/></div><h6>Figure 1.3: Selecting a kernel from the drop down menu</h6><p>This is the primary method of creating a new Jupyter Notebook.</p><p>Kernels provide programming language support for the Notebook. If you have installed Python with Anaconda, that version should be the default kernel. Conda virtual environments will also be available here.</p><h4>Note</h4><p class="callout">Virtual environments are a great tool for managing multiple projects on the same machine. Each virtual environment may contain a different version of Python and external libraries. Python has built-in virtual environments; however, the Conda virtual environment integrates better with Jupyter Notebooks and boasts other nice features. The documentation is available at: <a href="https://conda.io/docs/user-guide/tasks/manage-environments.html">https://conda.io/docs/user-guide/tasks/manage-environments.html</a>.</p></li>
				<li>With the newly created blank Notebook, click the top cell and type <code>print('hello world')</code>, or any other code snippet that writes to the screen. </li>
				<li>Click the cell and press <em class="italics">Shift</em> + <em class="italics">Enter</em> or select <code>stdout</code> or <code>stderr</code> output from the code will be displayed beneath as the cell runs. Furthermore, the string representation of the object written in the final line will be displayed as well. This is very handy, especially for displaying tables, but sometimes we don't want the final object to be displayed. In such cases, a semicolon (;) can be added to the end of the line to suppress the display. New cells expect and run code input by default; however, they can be changed to render <strong class="bold">Markdown</strong> instead.</p></li>
				<li>Click an empty cell and change it to accept the Markdown-formatted text. This can be done from the drop-down menu icon in the toolbar or by selecting <strong class="bold">Markdown</strong> from the <strong class="bold">Cell</strong> menu. Write some text in here (any text will do), making sure to utilize Markdown formatting symbols such as #.</li>
				<li>Scroll to the <strong class="bold">Play</strong> icon in the tool bar:<div><img src="img/C13018_01_04.jpg" alt="Figure 1.4: Jupyter Notebook tool bar" width="1523" height="146"/></div><h6>Figure 1.4: Jupyter Notebook tool bar</h6><p>This can be used to run cells. As we'll see later, however, it's handier to use the keyboard shortcut <em class="italics">Shift</em> + <em class="italics">Enter</em> to run cells. </p><p>Right next to this is a <strong class="bold">Stop</strong> icon, which can be used to stop cells from running. This is useful, for example, if a cell is taking too long to run:</p><div><img src="img/C13018_01_05_2.jpg" alt="Figure 1.5: Stop icon in Jupyter Notebooks&#13;&#10;" width="1020" height="164"/></div><h6>Figure 1.5: Stop icon in Jupyter Notebooks</h6><p>New cells can be manually added from the <strong class="bold">Insert</strong> menu:</p><div><img src="img/C13018_01_06.jpg" alt="Figure 1.6: Adding new cells from the Insert menu in Jupyter Notebooks" width="1255" height="194"/></div><h6>Figure 1.6: Adding new cells from the Insert menu in Jupyter Notebooks</h6><p>Cells can be copied, pasted, and deleted using icons or by selecting options from the <strong class="bold">Edit</strong> menu:</p><div><img src="img/C13018_01_07.jpg" alt="Figure 1.7: Edit Menu in the Jupyter Notebooks" width="983" height="142"/></div><h6>Figure 1.7: Edit Menu in the Jupyter Notebooks</h6><div><img src="img/C13018_01_08.jpg" alt="Figure 1.8: Cutting and copying cells in Jupyter Notebooks" width="1185" height="414"/></div><h6>Figure 1.8: Cutting and copying cells in Jupyter Notebooks</h6><p>Cells can also be moved up and down this way:</p><div><img src="img/C13018_01_09.jpg" alt="Figure 1.9: Moving cells up and down in Jupyter Notebooks" width="1274" height="130"/></div><h6>Figure 1.9: Moving cells up and down in Jupyter Notebooks</h6><p>There are useful options under the <strong class="bold">Cell</strong> menu to run a group of cells or the entire Notebook:</p><div><img src="img/C13018_01_10.jpg" alt="Figure 1.10: Running Cells in Jupyter Notebooks" width="1066" height="534"/></div><h6>Figure 1.10: Running cells in Jupyter Notebooks</h6><p>Experiment with the toolbar options to move cells up and down, insert new cells, and delete cells. An important thing to understand about these Notebooks is the shared memory between cells. It's quite simple: every cell existing on the sheet has access to the global set of variables. So, for example, a function defined in one cell could be called from any other, and the same applies to variables. As one would expect, anything within the scope of a function will not be a global variable and can only be accessed from within that specific function.</p></li>
				<li>Open the <strong class="bold">Kernel</strong> menu to see the selections. The <strong class="bold">Kernel</strong> menu is useful for stopping script executions and restarting the Notebook if the kernel dies. Kernels can also be swapped here at any time, but it is unadvisable to use multiple kernels for a single Notebook due to reproducibility concerns.</li>
				<li>Open the <strong class="bold">File</strong> menu to see the selections. The <strong class="bold">File</strong> menu contains options for downloading the Notebook in various formats. In particular, it's recommended to save an HTML version of your Notebook, where the content is rendered statically and can be opened and viewed "as you would expect" in web browsers.<p>The Notebook name will be displayed in the upper-left corner. New Notebooks will automatically be named <strong class="bold">Untitled</strong>.</p></li>
				<li>Change the name of your IPYNB Notebook file by clicking on the current name in the upper-left corner and typing the new name. Then, save the file.</li>
				<li>Close the current tab in your web browser (exiting the Notebook) and go to the <strong class="bold">Jupyter Dashboard</strong> tab, which should still be open. (If it's not open, then reload it by copy and pasting the HTTP link from the terminal.)<p>Since we didn't shut down the Notebook, and we just saved and exited, it will have a green book symbol next to its name in the <strong class="bold">Files</strong> section of the Jupyter Dashboard and will be listed as <strong class="bold">Running</strong> on the right side next to the last modified date. Notebooks can be shut down from here.</p></li>
				<li>Quit the Notebook you have been working on by selecting it (checkbox to the left of the name), and then click the orange <strong class="bold">Shutdown</strong> button:<h4>Note</h4><p class="callout">Read through the basic keyboard shortcuts and test them.</p></li>
			</ol>
			<div><div><img src="img/C13018_01_11.jpg" alt="Figure 1.11: Shutting down the Jupyter notebook&#13;&#10;" width="845" height="218"/>
				</div>
			</div>
			<h6>Figure 1.11: Shutting down the Jupyter notebook</h6>
			<h4>Note</h4>
			<p class="callout">If you plan to spend a lot of time working with Jupyter Notebooks, it's worthwhile to learn the keyboard shortcuts. This will speed up your workflow considerably. Particularly useful commands to learn are the shortcuts for manually adding new cells and converting cells from code to Markdown formatting. Click on <strong class="bold">Keyboard Shortcuts</strong> from the <strong class="bold">Help</strong> menu to see how.</p>
			<h3 id="_idParaDest-20"><a id="_idTextAnchor020"/>Jupyter Features</h3>
			<p>Jupyter has many appealing features that make for efficient Python programming. These include an assortment of things, from methods for viewing docstrings to executing Bash commands. We will explore some of these features in this section.</p>
			<h4>Note</h4>
			<p class="callout">The official IPython documentation can be found here: <a href="http://ipython.readthedocs.io/en/stable/">http://ipython.readthedocs.io/en/stable/</a>. It has details on the features we will discuss here and others.</p>
			<h3 id="_idParaDest-21"><a id="_idTextAnchor021"/>Exercise 2: Implementing Jupyter's Most Useful Features</h3>
			<ol>
				<li value="1">Navigate to the <code>lesson-1</code> directory from the Jupyter Dashboard and open <code>lesson-1-workbook.ipynb</code> by selecting it. <p>The standard file extension for Jupyter Notebooks is <code>.ipynb</code>, which was introduced back when they were called IPython Notebooks.</p></li>
				<li>Scroll down to <code>Subtopic C: Jupyter Features</code> in the Jupyter Notebook. <p>We start by reviewing the basic keyboard shortcuts. These are especially helpful to avoid having to use the mouse so often, which will greatly speed up the workflow.</p><p>You can get help by adding a question mark to the end of any object and running the cell. Jupyter finds the docstring for that object and returns it in a pop-out window at the bottom of the app.</p></li>
				<li>Run the <strong class="bold">Getting Help</strong> cell and check how Jupyter displays the docstrings at the bottom of the Notebook. Add a cell in this section and get help on the object of your choice:<div><img src="img/C13018_01_12.jpg" alt="Figure 1.12: Getting help in Jupyter Notebooks" width="738" height="339"/></div><h6>Figure 1.12: Getting help in Jupyter Notebooks</h6></li>
				<li>Click an empty code cell in the <strong class="bold">Tab Completion</strong> section. Type import (including the space after) and then press the <strong class="bold">Tab</strong> key: <div><img src="img/C13018_01_13.jpg" alt="Figure 1.13: Tab completion in Jupyter Notebooks" width="1218" height="334"/></div><h6>Figure 1.13: Tab completion in Jupyter Notebooks</h6><p>The above action listed all the available modules for import.</p><p>Tab completion can be used for the following: <strong class="bold">list available modules when importing external libraries</strong>; <strong class="bold">list available modules of imported external libraries</strong>; <strong class="bold">function and variable completion</strong>. This can be especially useful when you need to know the available input arguments for a module, when exploring a new library, to discover new modules, or simply to speed up workflow. They will save time writing out variable names or functions and reduce bugs from typos. The tab completion works so well that you may have difficulty coding Python in other editors after today!</p></li>
				<li>Scroll to the Jupyter Magic Functions section and run the cells containing <code>%lsmagic</code> and <code>%matplotlib</code> inline:<div><img src="img/C13018_01_14.jpg" alt="Figure 1.14: Jupyter Magic functions" width="573" height="217"/></div><h6>Figure 1.14: Jupyter Magic functions</h6><p>The percent signs, % and %%, are one of the basic features of Jupyter Notebook and are called magic commands. Magics starting with <code>%%</code> will apply to the entire cell, and magics starting with <code>%</code> will only apply to that line. </p><p><code>%lsmagic</code> lists the available options. We will discuss and show examples of some of the most useful ones. The most common magic command you will probably see is <code>%matplotlib</code> inline, which allows matplotlib figures to be displayed in the Notebook without having to explicitly use <code>plt.show()</code>.</p><p>The timing functions are very handy and come in two varieties: a standard timer (<code>%time</code> or <code>%%time</code>) and a timer that measures the average runtime of many iterations (<code>%timeit</code> and <code>%%timeit</code>).</p><h4>Note </h4><p class="callout">Notice how list comprehensions are quicker than loops in Python. This can be seen by comparing the wall time for the first and second cell, where the same calculation is done significantly faster with the list comprehension.</p></li>
				<li>Run the cells in the <code>pwd</code>), what's in the directory (<code>ls</code>), make new folders (<code>mkdir</code>), and write file contents (<code>cat</code>/<code>head</code>/<code>tail</code>).</p></li>
				<li>Run the first cell in the <strong class="bold">Using bash</strong> in the notebook section. <p>This cell writes some text to a file in the working directory, prints the directory contents, prints an empty line, and then writes back the contents of the newly created file before removing it:</p><div><img src="img/C13018_01_15.jpg" alt="Figure 1.15: Using Bash in Jupyter Notebooks" width="614" height="313"/></div><h6>Figure 1.15: Using Bash in Jupyter Notebooks</h6></li>
				<li>Run the cells containing only <code>ls</code> and <code>pwd</code>. <p>Note how we did not have to explicitly use the Bash magic command for these to work. There are plenty of external magic commands that can be installed. A popular one is <code>ipython-sql</code>, which allows for SQL code to be executed in cells.</p></li>
				<li>Open a new terminal window and execute the following code to install ipython-sql: <pre>pip install ipython-sql</pre><div><img src="img/C13018_01_16.jpg" alt="Figure 1.16: Installing ipython-sql using pip&#13;&#10;" width="1088" height="142"/></div><h6>Figure 1.16: Installing ipython-sql using pip</h6></li>
				<li>Run the <code>%load_ext sql</code> cell to load the external command into the Notebook:<div><img src="img/C13018_01_17.jpg" alt="Figure 1.17: Loading sql in Jupyter Notebooks&#13;&#10;" width="974" height="142"/></div><h6>Figure 1.17: Loading sql in Jupyter Notebooks</h6><p>This allows for connections to remote databases so that queries can be executed (and thereby documented) right inside the Notebook.</p></li>
				<li>Run the cell containing the SQL sample query:<div><img src="img/C13018_01_18.jpg" alt="Figure 1.18: Running a sample SQL query&#13;&#10;" width="451" height="226"/></div><h6>Figure 1.18: Running a sample SQL query</h6><p>Here, we first connect to the local sqlite source; however, this line could instead point to a specific database on a local or remote server. Then, we execute a simple <code>SELECT</code> to show how the cell has been converted to run SQL code instead of Python.</p></li>
				<li>Install the version documentation tool now from the terminal using <code>pip</code>. Open up a new window and run the following code:<pre>pip install version_information</pre><p>Once installed, it can then be imported into any Notebook using <code>%load_ext version_information</code>. Finally, once loaded, it can be used to display the versions of each piece of software in the Notebook.</p><p>The <code>%version_information commands helps with documentation</code>, but it does not come as standard with Jupyter. Like the SQL example we just saw, it can be installed from the command line with <code>pip</code>.</p></li>
				<li>Run the cell that loads and calls the <code>version_information</code> command:</li>
			</ol>
			<div><div><img src="img/C13018_01_19.jpg" alt="Figure 1.19: Version Information in Jupyter&#13;&#10;" width="857" height="374"/>
				</div>
			</div>
			<h6>Figure 1.19: Version Information in Jupyter</h6>
			<h3 id="_idParaDest-22"><a id="_idTextAnchor022"/>Converting a Jupyter Notebook to a Python Script</h3>
			<p>You can convert a Jupyter Notebook to a Python script. This is equivalent to copying and pasting the contents of each code cell into a single <code>.py</code> file. The Markdown sections are also included as comments.</p>
			<p>The conversion can be done from the <code>NotebookApp</code> or in the command line as follows:</p>
			<pre>jupyter nbconvert --to=python lesson-1-notebook.ipynb</pre>
			<div><div><img src="img/C13018_01_20.jpg" alt="Figure 1.20: Converting a Jupyter Notebook into a Python Script&#13;&#10;" width="1220" height="552"/>
				</div>
			</div>
			<h6>Figure 1.20: Converting a Jupyter Notebook into a Python Script</h6>
			<p>This is useful, for example, when you want to determine the library requirements for a Notebook using a tool such as <code>pipreqs</code>. This tool determines the libraries used in a project and exports them into a <code>requirements.txt</code> file (and it can be installed by running pip install <code>pipreqs</code>).</p>
			<p>The command is called from outside the folder containing your <code>.py</code> files. For example, if the <code>.py</code> files are inside a folder called <code>lesson-1</code>, you could do the following:</p>
			<pre>pipreqs lesson-1/</pre>
			<div><div><img src="img/C13018_01_21.jpg" alt="Figure 1.21: Determining library requirements using pipreqs&#13;&#10;" width="812" height="420"/>
				</div>
			</div>
			<h6>Figure 1.21: Determining library requirements using pipreqs</h6>
			<p>The resulting <code>requirements.txt</code> file for <code>lesson-1-workbook.ipynb</code> looks like this:</p>
			<pre>cat lesson-1/requirements.txt 
matplotlib==2.0.2 numpy==1.13.1
pandas==0.20.3 
requests==2.18.4 
seaborn==0.8 
beautifulsoup4==4.6.0 
scikit_learn==0.19.0</pre>
			<h3 id="_idParaDest-23"><a id="_idTextAnchor023"/>Python Libraries</h3>
			<p>Having now seen all the basics of Jupyter Notebooks, and even some more advanced features, we'll shift our attention to the Python libraries we'll be using in this book. Libraries, in general, extend the default set of Python functions. Examples of commonly used standard libraries are <code>datetime</code>, <code>time</code>, and <code>os</code>. These are called standard libraries because they come standard with every installation of Python.</p>
			<p>For data science with Python, the most important libraries are external, which means they do not come standard with Python.</p>
			<p>The external data science libraries we'll be using in this book are NumPy, Pandas, Seaborn, matplotlib, scikit-learn, Requests, and Bokeh. </p>
			<h4>Note</h4>
			<p class="callout">A word of caution: It's a good idea to import libraries using industry standards, for example, import numpy as np; this way, your code is more readable. Try to avoid doing things such as from numpy import *, as you may unwittingly overwrite functions. Furthermore, it's often nice to have modules linked to the library via a dot (.) for code readability.</p>
			<p>Let's briefly introduce each.</p>
			<ul>
				<li><strong class="bold">NumPy</strong> offers multi-dimensional data structures (arrays) on which operations can be performed far quicker than standard Python data structures (for example, lists). This is done in part by performing operations in the background using C. NumPy also offers various mathematical and data manipulation functions.</li>
				<li><code>NaN</code> entries and computing statistical descriptions of the data. Working with Pandas DataFrames will be a big focus of this book.</li>
				<li><strong class="bold">Matplotlib</strong> is a plotting tool inspired by the MATLAB platform. Those familiar with R can think of it as Python's version of ggplot. It's the most popular Python library for plotting figures and allows for a high level of customization.</li>
				<li><strong class="bold">Seaborn</strong> works as an extension to matplotlib, where various plotting tools useful for data science are included. Generally speaking, this allows for analysis to be done much faster than if you were to create the same things <em class="italics">manually</em> with libraries such as matplotlib and scikit-learn.</li>
				<li><strong class="bold">scikit-learn</strong> is the most commonly used machine learning library. It offers top-of-the-line algorithms and a very elegant API where models are instantiated and then <em class="italics">fit</em> with data. It also provides data processing modules and other tools useful for predictive analytics.</li>
				<li><strong class="bold">Requests</strong> is the go-to library for making HTTP requests. It makes it straightforward to get HTML from web pages and interface with APIs. For parsing the HTML, many choose BeautifulSoup4, which we will also cover in this book.</li>
				<li><strong class="bold">Bokeh</strong> is an interactive visualization library. It functions similar to matplotlib, but allows us to add hover, zoom, click, and use other interactive tools to our plots. It also allows us to render and play with the plots inside our Jupyter Notebook.</li>
			</ul>
			<p>Having introduced these libraries, let's go back to our Notebook and load them, by running the <code>import</code> statements. This will lead us into our first analysis, where we finally start working with a dataset. </p>
			<h3 id="_idParaDest-24"><a id="_idTextAnchor024"/>Exercise 3: Importing the External Libraries and Setting Up the Plotting Environment</h3>
			<ol>
				<li value="1">Open up the <code>lesson 1</code> Jupyter Notebook and scroll to the <code>Subtopic D: Python Libraries</code> section.<p>Just like for regular Python scripts, libraries can be imported into the Notebook at any time. It's best practice to put the majority of the packages you use at the top of the file. Sometimes it makes sense to load things midway through the Notebook and that is completely fine.</p></li>
				<li>Run the cells to import the external libraries and set the plotting options:</li>
			</ol>
			<div><div><img src="img/C13018_01_22.jpg" alt="Figure 1.22: Importing Python libraries&#13;&#10;" width="1170" height="460"/>
				</div>
			</div>
			<h6>Figure 1.22: Importing Python libraries</h6>
			<p>For a nice Notebook setup, it's often useful to set various options along with the imports at the top. For example, the following can be run to change the figure appearance to something more aesthetically pleasing than the matplotlib and Seaborn defaults:</p>
			<pre>import matplotlib.pyplot as plt
%matplotlib inline import 
seaborn as sns
# See here for more options: https://matplotlib.org/users/ customizing.html
%config InlineBackend.figure_format='retina' 
sns.set() # Revert to matplotlib defaults 
plt.rcParams['figure.figsize'] = (9, 6)
plt.rcParams['axes.labelpad'] = 10 sns.set_style("darkgrid")</pre>
			<p>So far in this book, we've gone over the basics of using Jupyter Notebooks for data science. We started by exploring the platform and finding our way around the interface. Then, we discussed the most useful features, which include tab completion and magic functions. Finally, we introduced the Python libraries we'll be using in this book.</p>
			<p>The next section will be very interactive as we perform our first analysis together using the Jupyter Notebook.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor025"/>Our First Analysis - The Boston Housing Dataset</h2>
			<p>So far, this chapter has focused on the features and basic usage of Jupyter. Now, we'll put this into practice and do some data exploration and analysis.</p>
			<p>The dataset we'll look at in this section is the so-called Boston housing dataset. It contains US census data concerning houses in various areas around the city of Boston. Each sample corresponds to a unique area and has about a dozen measures. We should think of samples as rows and measures as columns. The data was first published in 1978 and is quite small, containing only about 500 samples.</p>
			<p>Now that we know something about the context of the dataset, let's decide on a rough plan for the exploration and analysis. If applicable, this plan would accommodate the relevant question(s) under study. In this case, the goal is not to answer a question but to instead show Jupyter in action and illustrate some basic data analysis methods.</p>
			<p>Our general approach to this analysis will be to do the following:</p>
			<ul>
				<li>Load the data into Jupyter using a Pandas DataFrame</li>
				<li>Quantitatively understand the features </li>
				<li>Look for patterns and generate questions </li>
				<li>Answer the questions to the problems</li>
			</ul>
			<h3 id="_idParaDest-26"><a id="_idTextAnchor026"/>Loading the Data into Jupyter Using a Pandas DataFrame</h3>
			<p>Oftentimes, data is stored in tables, which means it can be saved as a <strong class="keyword">comma-separated variable</strong> (<strong class="keyword">CSV</strong>) file. This format, and many others, can be read into Python as a DataFrame object, using the Pandas library. Other common formats include <strong class="keyword">tab-separated variable</strong> (<strong class="keyword">TSV</strong>), SQL tables, and JSON data structures. Indeed, Pandas has support for all of these. In this example, however, we are not going to load the data this way because the dataset is available directly through scikit-learn.</p>
			<h4>Note</h4>
			<p class="callout">An important part after loading data for analysis is ensuring that it's clean. For example, we would generally need to deal with missing data and ensure that all columns have the correct datatypes. The dataset we use in this section has already been cleaned, so we will not need to worry about this. However, we'll see messier data in the second chapter and explore techniques for dealing with it.</p>
			<h3 id="_idParaDest-27"><a id="_idTextAnchor027"/>Exercise 4: Loading the Boston Housing Dataset</h3>
			<ol>
				<li value="1">Scroll to <code>Subtopic A</code> of <code>Topic B: Our first Analysis: the Boston Housing Dataset</code> in chapter 1 of the Jupyter Notebook.<p>The Boston housing dataset can be accessed from the <code>sklearn.datasets</code> module using the <code>load_boston</code> method.</p></li>
				<li>Run the first two cells in this section to load the Boston dataset and see the <code>datastructures</code> type:<div><img src="img/C13018_01_23.jpg" alt="Figure 1.23: Loading the Boston dataset&#13;&#10;" width="1024" height="231"/></div><h6>Figure 1.23: Loading the Boston dataset</h6><p>The output of the second cell tells us that it's a scikit-learn <code>Bunch</code> object. Let's get some more information about that to understand what we are dealing with.</p></li>
				<li>Run the next cell to import the base object from scikit-learn <code>utils</code> and print the docstring in our Notebook:<div><img src="img/C13018_01_24.jpg" alt="Figure 1.24: Importing base objects and printing the docstring&#13;&#10;" width="1085" height="420"/></div><h6>Figure 1.24: Importing base objects and printing the docstring</h6></li>
				<li>Print the field names (that is, the keys to the dictionary) by running the next cell. We find these fields to be self-explanatory: <code>['DESCR', 'target', 'data', 'feature_names']</code>.</li>
				<li>Run the next cell to print the dataset description contained in <code>boston['DESCR']</code>.<p>Note that in this call, we explicitly want to print the field value so that the Notebook renders the content in a more readable format than the string representation (that is, if we just type <code>boston['DESCR']</code> without wrapping it in a <code>print</code> statement). We then see the dataset information as we've previously summarized:</p><pre>Boston House Prices dataset
===========================
Notes
------
Data Set Characteristics:
:Number of Instances: 506
:Number of Attributes: 13 numeric/categorical predictive
:Median Value (attribute 14) is usually the target	
:Attribute Information (in order):
- CRIM	per capita crime rate by town
…
…
- MEDV     Median value of owner-occupied homes in $1000's
:Missing Attribute Values: None</pre><h4>Note</h4><p class="callout">Briefly read through the feature descriptions and/or describe them yourself. For the purposes of this tutorial, the most important fields to understand are <code>Attribute</code> <code>Information</code>). We will use this as reference during our analysis.</p><h4>Note</h4><p class="callout">For the complete code, refer to the following: <a href="https://bit.ly/2EL11cW">https://bit.ly/2EL11cW</a></p><p>Now, we are going to create a Pandas DataFrame that contains the data. This is beneficial for a few reasons: all of our data will be contained in one object, there are useful and computationally efficient DataFrame methods we can use, and other libraries such as Seaborn have tools that integrate nicely with DataFrames.</p><p>In this case, we will create our DataFrame with the standard constructor method.</p></li>
				<li>Run the cell where Pandas is imported and the docstring is retrieved for <code>pd.DataFrame:</code><div><img src="img/C13018_01_25.jpg" alt="Figure 1.25: Retrieving the docstring for pd.DataFrame&#13;&#10;" width="768" height="335"/></div><h6>Figure 1.25: Retrieving the docstring for pd.DataFrame</h6><p>The docstring reveals the DataFrame input parameters. We want to feed in <code>boston['data']</code> for the data and use <code>boston['feature_names']</code> for the headers.</p></li>
				<li>Run the next few cells to print the data, its shape, and the feature names:<div><img src="img/C13018_01_26.jpg" alt="Figure 1.26: Printing data, shape, and feature names&#13;&#10;" width="1361" height="814"/></div><h6>Figure 1.26: Printing data, shape, and feature names</h6><p>Looking at the output, we see that our data is in a 2D NumPy array. Running the command <code>boston['data'].shape</code> returns the length (number of samples) and the number of features as the first and second outputs, respectively.</p></li>
				<li>Load the data into a Pandas DataFrame <code>df</code> by running the following:<pre>df = pd.DataFrame(data=boston['data'], 
columns=boston['feature_names'])</pre><p>In machine learning, the variable that is being modeled is called the target variable; it's what you are trying to predict given the features. For this dataset, the suggested target is <strong class="bold">MEDV</strong>, the median house value in 1,000s of dollars.</p></li>
				<li>Run the next cell to see the shape of the target:<div><img src="img/C13018_01_27.jpg" alt="Figure 1.27: Code for viewing the shape of the target&#13;&#10;" width="1038" height="150"/></div><h6>Figure 1.27: Code for viewing the shape of the target</h6><p>We see that it has the same length as the features, which is what we expect. It can therefore be added as a new column to the DataFrame.</p></li>
				<li>Add the target variable to df by running the cell with the following:<pre>df['MEDV'] = boston['target']</pre></li>
				<li>Move the target variable to the front of <code>df</code> by running the cell with the following code: <pre>y = df['MEDV'].copy() 
del df['MEDV']
df = pd.concat((y, df), axis=1)</pre><p>This is done to distinguish the target from our features by storing it to the front of our DataFrame.</p><p>Here, we introduce a dummy variable <code>y</code> to hold a copy of the target column before removing it from the DataFrame. We then use the Pandas concatenation function to combine it with the remaining DataFrame along the 1st axis (as opposed to the 0th axis, which combines rows).</p><h4>Note</h4><p class="callout">You will often see dot notation used to reference DataFrame columns. For example, previously we could have done <code>y = df.MEDV.copy()</code>. This does not work for deleting columns, however; <code>del df.MEDV</code> would raise an error.</p></li>
				<li>Implement <code>df.head()</code> or <code>df.tail()</code> to glimpse the data and <code>len(df)</code> to verify that number of samples is what we expect. Run the next few cells to see the head, tail, and length of <code>df</code>:<div><img src="img/C13018_01_28.jpg" alt="Figure 1.28: Printing the head of the data frame df&#13;&#10;" width="877" height="283"/></div><h6>Figure 1.28: Printing the head of the data frame df</h6><div><img src="img/C13018_01_29.jpg" alt="Figure 1.29: Printing the tail of data frame df&#13;&#10;" width="1800" height="639"/></div><h6>Figure 1.29: Printing the tail of data frame df</h6><p>Each row is labeled with an index value, as seen in bold on the left side of the table. By default, these are a set of integers starting at 0 and incrementing by one for each row.</p></li>
				<li>Printing <code>df.dtypes</code> will show the datatype contained within each column. Run the next cell to see the datatypes of each column. For this dataset, we see that every field is a float and therefore most likely a continuous variable, including the target. This means that predicting the target variable is a regression problem.</li>
				<li>Run <code>df.isnull()</code> to clean the dataset as Pandas automatically sets missing data as <code>NaN</code> values. To get the number of <code>NaN</code> values per column, we can do <code>df.isnull().sum()</code>:<div><img src="img/C13018_01_30.jpg" alt="Figure 1.30: Cleaning the dataset by identifying NaN values&#13;&#10;" width="1616" height="768"/></div><h6>Figure 1.30: Cleaning the dataset by identifying NaN values</h6><p><code>df.isnull()  </code>returns a Boolean frame of the same length as <code>df</code>. </p><p>For this dataset, we see there are no <code>NaN</code> values, which means we have no immediate work to do in cleaning the data and can move on.</p></li>
				<li>Remove some columns by running the cell that contains the following code:<pre>for col in ['ZN', 'NOX', 'RAD', 'PTRATIO', 'B']:
del df[col]</pre><p>This is done to simplify the analysis. We will focus on the remaining columns in more detail.</p></li>
			</ol>
			<h3 id="_idParaDest-28"><a id="_idTextAnchor028"/>Data Exploration</h3>
			<p>Since this is an entirely new dataset that we've never seen before, the first goal here is to understand the data. We've already seen the textual description of the data, which is important for qualitative understanding. We'll now compute a quantitative description.</p>
			<h3 id="_idParaDest-29"><a id="_idTextAnchor029"/>Exercise 5: Analyzing the Boston Housing Dataset</h3>
			<ol>
				<li value="1">Navigate to <code>Subtopic B: Data exploration</code> in the Jupyter Notebook and run the cell containing <code>df.describe()</code>:<div><img src="img/C13018_01_31.jpg" alt="Figure 1.31: Computation and output of statistical properties&#13;&#10;" width="1554" height="804"/></div><h6>Figure 1.31: Computation and output of statistical properties</h6><p>This computes various properties including the mean, standard deviation, minimum, and maximum for each column. This table gives a high-level idea of how everything is distributed. Note that we have taken the transform of the result by adding a <code>.T</code> to the output; this swaps the rows and columns.</p><p>Going forward with the analysis, we will specify a set of columns to focus on.</p></li>
				<li>Run the cell where these "focus columns" are defined:<pre>cols = ['RM', 'AGE', 'TAX', 'LSTAT', 'MEDV']</pre></li>
				<li>Display the aforementioned subset of columns of the DataFrame by running <code>df[cols].head()</code>:<div><img src="img/C13018_01_32.jpg" alt="Figure 1.32: Displaying focus columns&#13;&#10;" width="1243" height="502"/></div><h6>Figure 1.32: Displaying focus columns</h6><p>As a reminder, let's recall what each of these columns is. From the dataset documentation, we have the following:</p><pre>- RM       average number of rooms per dwelling
        - AGE      proportion of owner-occupied units built prior to 1940
        - TAX      full-value property-tax rate per $10,000
        - LSTAT    % lower status of the population
        - MEDV     Median value of owner-occupied homes in $1000's</pre><p>To look for patterns in this data, we can start by calculating the pairwise correlations using <code>pd.DataFrame.corr</code>.</p></li>
				<li>Calculate the pairwise correlations for our selected columns by running the cell containing the following code:<pre>df[cols].corr()</pre><div><img src="img/C13018_01_33.jpg" alt="Figure 1.33: Pairwise calculation of correlation&#13;&#10;" width="598" height="193"/></div><h6>Figure 1.33: Pairwise calculation of correlation</h6><p>This resulting table shows the correlation score between each set of values. Large positive scores indicate a strong positive (that is, in the same direction) correlation. As expected, we see maximum values of 1 on the diagonal.</p><p>By default, Pandas calculates the standard correlation coefficient for each pair, which is also called the Pearson coefficient. This is defined as the covariance between two variables, divided by the product of their standard deviations:</p><div><img src="img/C13018_01_56.jpg" alt="" width="1565" height="180"/></div><p>The covariance, in turn, is defned as follows: </p><div><img src="img/C13018_01_57.jpg" alt="" width="1800" height="90"/></div><p>Here, n is the number of samples, xi and yi are the individual samples being summed over, and X and Y are the means of each set.</p><p>Instead of straining our eyes to look at the preceding table, it's nicer to visualize it with a heatmap. This can be done easily with Seaborn.</p></li>
				<li>Run the next cell to initialize the plotting environment, as discussed earlier in the chapter. Then, to create the heatmap, run the cell containing the following code:<pre>import matplotlib.pyplot as plt import seaborn as sns
%matplotlib inline
ax = sns.heatmap(df[cols].corr(),
cmap=sns.cubehelix_palette(20, light=0.95,
dark=0.15))
ax.xaxis.tick_top() # move labels to the top
plt.savefig('../figures/lesson-1-boston-housing-corr.png', bbox_inches='tight', dpi=300)</pre></li>
			</ol>
			<div><div><img src="img/C13018_01_34.jpg" alt="Figure 1.34: Plot of the heat map for all variables&#13;&#10;" width="1800" height="843"/>
				</div>
			</div>
			<h6>Figure 1.34: Plot of the heat map for all variables</h6>
			<p>We call <code>sns.heatmap</code> and pass the pairwise correlation matrix as input. We use a custom color palette here to override the Seaborn default. The function returns a <code>matplotlib.axes</code> object which is referenced by the variable <code>ax</code>.</p>
			<p>The final figure is then saved as a high resolution PNG to the <code>figures</code> folder.</p>
			<p>For the final step in our dataset exploration exercise, we'll visualize our data using Seaborn's <code>pairplot</code> function.</p>
			<p>Visualize the DataFrame using Seaborn's <code>pairplot</code> function. Run the cell containing the following code:</p>
			<pre>sns.pairplot(df[cols],
plot_kws={'alpha': 0.6},
diag_kws={'bins': 30})</pre>
			<div><div><img src="img/C13018_01_35.jpg" alt="Figure 1.35: Data visualization using Seaborn&#13;&#10;" width="1800" height="1106"/>
				</div>
			</div>
			<h6>Figure 1.35: Data visualization using Seaborn</h6>
			<h4>Note</h4>
			<p class="callout">Note that unsupervised learning techniques are outside the scope of this book.</p>
			<p>Looking at the histograms on the diagonal, we see the following:</p>
			<p><strong class="bold">a</strong>: <strong class="bold">RM</strong> and <strong class="bold">MEDV</strong> have the closest shape to normal distributions.</p>
			<p><strong class="bold">b</strong>: <strong class="bold">AGE</strong> is skewed to the left and <strong class="bold">LSTAT</strong> is skewed to the right (this mayseem counterintuitive but skew is defined in terms of where the mean is positioned in relation to the max).</p>
			<p><strong class="bold">c</strong>: For <strong class="bold">TAX</strong>, we find a large amount of the distribution is around 700. This is also evident from the scatter plots.</p>
			<p>Taking a closer look at the <code>df.describe()</code>, the min and max of <strong class="bold">MDEV</strong> was 5k and 50k, respectively. This suggests that median house values in the dataset were capped at 50k.</p>
			<h3 id="_idParaDest-30"><a id="_idTextAnchor030"/>Introduction to Predictive Analytics with Jupyter Notebooks</h3>
			<p>Continuing our analysis of the Boston housing dataset, we can see that it presents us with a regression problem where we predict a continuous target variable given a set of features. In particular, we'll be predicting the median house value (<strong class="bold">MEDV</strong>).</p>
			<p>We'll train models that take only one feature as input to make this prediction. This way, the models will be conceptually simple to understand and we can focus more on the technical details of the scikit-learn API. Then, in the next chapter, you'll be more comfortable dealing with the relatively complicated models.</p>
			<h3 id="_idParaDest-31"><a id="_idTextAnchor031"/>Exercise 6: Applying Linear Models With Seaborn and Scikit-learn</h3>
			<ol>
				<li value="1">Scroll to <code>Subtopic C: Introduction to predictive analytics</code> in the Jupyter Notebook and look just above at the pairplot we created in the previous section. In particular, look at the scatter plots in the bottom-left corner:<div><img src="img/C13018_01_36.jpg" alt="Figure 1.36: Scatter plots for MEDV and LSTAT&#13;&#10;" width="1800" height="1106"/></div><h6>Figure 1.36: Scatter plots for MEDV and LSTAT</h6><p>Note how the number of rooms per house (<strong class="bold">RM</strong>) and the % of the population that is lower class (<strong class="bold">LSTAT</strong>) are highly correlated with the median house value (<strong class="bold">MDEV</strong>). Let's pose the following question: how well can we predict <strong class="bold">MDEV</strong> given these variables?</p><p>To help answer this, let's first visualize the relationships using Seaborn. We will draw the scatter plots along with the line of best fit linear models.</p></li>
				<li>Draw scatter plots along with the linear models by running the cell that contains the following:<pre>fig, ax = plt.subplots(1, 2) sns.regplot('RM', 'MEDV', df, ax=ax[0],
scatter_kws={'alpha': 0.4})) sns.regplot('LSTAT', 'MEDV', df, ax=ax[1],
scatter_kws={'alpha': 0.4}))</pre><div><img src="img/C13018_01_37.jpg" alt="Figure 1.37: Drawing scatter plots using linear models&#13;&#10;" width="1800" height="902"/></div><h6>Figure 1.37: Drawing scatter plots using linear models</h6><p>The line of best fit is calculated by minimizing the ordinary least squares error function, something Seaborn does automatically when we call the <code>regplot</code> function. Also note the shaded areas around the lines, which represent 95% confidence intervals.</p><h4>Note</h4><p class="callout">These 95% confidence intervals are calculated by taking the standard deviation of data in bins perpendicular to the line of best fit, effectively determining the confidence intervals at each point along the line of best fit. In practice, this involves Seaborn bootstrapping the data, a process where new data is created through random sampling with replacement. The number of bootstrapped samples is automatically determined based on the size of the dataset, but can be manually set as well by passing the <code>n_boot</code> argument.</p></li>
				<li>Plot the residuals using Seaborn by running the cell containing the following:<pre>fig, ax = plt.subplots(1, 2)
ax[0] = sns.residplot('RM', 'MEDV', df, ax=ax[0],
scatter_kws={'alpha': 0.4}) ax[0].set_ylabel('MDEV residuals $(y-\hat{y})$') ax[1] = sns.residplot('LSTAT', 'MEDV', df, ax=ax[1],
scatter_kws={'alpha': 0.4})
ax[1].set_ylabel('')</pre><div><img src="img/C13018_01_38.jpg" alt="Figure 1.38: Plotting residuals using Seaborn&#13;&#10;" width="1800" height="785"/></div><h6>Figure 1.38: Plotting residuals using Seaborn</h6><p>Each point on these residual plots is the difference between that sample (<code>y</code>) and the linear model prediction (<code>ŷ</code>). Residuals greater than zero are data points that would be underestimated by the model. Likewise, residuals less than zero are data points that would be overestimated by the model.</p><p>Patterns in these plots can indicate suboptimal modeling. In each preceding case, we see diagonally arranged scatter points in the positive region. These are caused by the $50,000 cap on <strong class="bold">MEDV</strong>. The <strong class="bold">RM</strong> data is clustered nicely around 0, which indicates a good fit. On the other hand, <strong class="bold">LSTAT</strong> appears to be clustered lower than 0.</p></li>
				<li>Define a function using sci-kit learn that calculates the line of best fit and mean squared error, by running the cell that contains the following:<pre>def get_mse(df, feature, target='MEDV'): # Get x, y to model
y = df[target].values
x = df[feature].values.reshape(-1,1)
...
...
error = mean_squared_error(y, y_pred) print('mse = {:.2f}'.format(error)) print()</pre><h4>Note</h4><p class="callout">For complete code, refer to the following: <a href="https://bit.ly/2JgPZdU">https://bit.ly/2JgPZdU</a></p><p>In the <code>get_mse</code> function, we first assign the variables <code>y</code> and <code>x</code> to the target MDEV and the dependent feature, respectively. These are cast as NumPy arrays by calling the <code>values</code> attribute. The dependent features array is reshaped to the format expected by scikit-learn; this is only necessary when modeling a one-dimensional feature space. The model is then instantiated and fitted on the data. For linear regression, the fitting consists of computing the model parameters using the ordinary least squares method (minimizing the sum of squared errors for each sample). Finally, after determining the parameters, we predict the target variable and use the results to calculate the <strong class="bold">MSE</strong>.</p></li>
				<li>Call the <code>get_mse</code> function for both <strong class="bold">RM</strong> and <strong class="bold">LSTAT</strong>, by running the cell containing the following:<pre>get_mse(df, 'RM') get_mse(df, 'LSTAT')</pre></li>
			</ol>
			<div><div><img src="img/C13018_01_39.jpg" alt="Figure 1.39: Calling the get_mse function for RM and LSTAT&#13;&#10;" width="1446" height="584"/>
				</div>
			</div>
			<h6>Figure 1.39: Calling the get_mse function for RM and LSTAT</h6>
			<p>Comparing the <strong class="bold">MSE</strong>, it turns out the error is slightly lower for <strong class="bold">LSTAT</strong>. Looking back to the scatter plots, however, it appears that we might have even better success using a polynomial model for <strong class="bold">LSTAT</strong>. In the next activity, we will test this by computing a third-order polynomial model with scikit-learn.</p>
			<p>Forgetting about our Boston housing dataset for a minute, consider another real-world situation where you might employ polynomial regression. The following example is modeling weather data. In the following plot, we see temperatures (lines) and precipitations (bars) for Vancouver, BC, Canada:</p>
			<div><div><img src="img/C13018_01_40.jpg" alt="Figure 1.40: Visualizing weather data for Vancouver, Canada&#13;&#10;" width="1102" height="749"/>
				</div>
			</div>
			<h6>Figure 1.40: Visualizing weather data for Vancouver, Canada</h6>
			<p>Any of these fields are likely to be fit quite well by a fourth-order polynomial. This would be a very valuable model to have, for example, if you were interested in predicting the temperature or precipitation for a continuous range of dates.</p>
			<h4>Note</h4>
			<p class="callout">You can find the data source for this here: <a href="http://climate.weather.gc.ca/climate_normals/results_e.html?stnID=888">http://climate.weather.gc.ca/climate_normals/results_e.</a>html?stnID=888.</p>
			<h3 id="_idParaDest-32"><a id="_idTextAnchor032"/>Activity 1: Building a Third-Order Polynomial Model</h3>
			<p>Shifting our attention back to the Boston housing dataset, we would like to build a third-order polynomial model to compare against the linear one. Recall the actual problem we are trying to solve: predicting the median house value, given the lower class population percentage. This model could benefit a prospective Boston house purchaser who cares about how much of their community would be lower class.</p>
			<p>Our aim is to use scikit-learn to fit a polynomial regression model to predict the median house value (<strong class="bold">MEDV</strong>), given the <strong class="bold">LSTAT</strong> values. We are hoping to build a model that has a lower mean-squared error (<strong class="bold">MSE</strong>). In order to achieve this, the following steps have to be executed:</p>
			<ol>
				<li value="1">Scroll to the empty cells at the bottom of <code>Subtopic C</code> in your Jupyter Notebook. These will be found beneath the linear-model <code>Activity</code> heading.<h4>Note</h4><p class="callout">You should fill these empty cells in with code as we complete the activity. You may need to insert new cells as these become filled up; please do so as needed.</p></li>
				<li>Pull out our dependent feature from and target variable from <code>df</code>. </li>
				<li>Verify what <code>x</code> looks like by printing the first three samples.</li>
				<li>Transform <code>x</code> into "polynomial features" by importing the appropriate transformation tool from scikit-</li>
				<li>Transform the <code>x</code>) by running the <code>fit_transform</code> method and build the polynomial feature set.</li>
				<li>Verify what <code>x_poly</code> looks like by printing the first few samples. </li>
				<li>Import the <code>LinearRegression</code> class and build our linear classification model the same way as done while calculating the MSE.</li>
				<li>Extract the coefficients and print the polynomial model.</li>
				<li>Determine the predicted values for each sample and calculate the residuals.</li>
				<li>Print some of the residual values.</li>
				<li>Print the MSE for the third-order polynomial model.</li>
				<li>Plot the polynomial model along with the samples.</li>
				<li>Plot the residuals.<h4>Note</h4><p class="callout">The detailed steps along with the solutions are presented in the <em class="italics">Appendix A</em> (pg. no. 144).</p></li>
			</ol>
			<p>Having successfully modeled the data using a polynomial model, let's finish up this chapter by looking at categorical features. In particular, we are going to build a set of categorical features and use them to explore the dataset in more detail.</p>
			<h3 id="_idParaDest-33"><a id="_idTextAnchor033"/>Using Categorical Features for Segmentation Analysis</h3>
			<p>Often, we find datasets where there are a mix of continuous and categorical fields. In such cases, we can learn about our data and find patterns by segmenting the continuous variables with the categorical fields.</p>
			<p>As a specific example, imagine you are evaluating the return on investment from an ad campaign. The data you have access to contain measures of some calculated <strong class="keyword">return on investment</strong> (<strong class="keyword">ROI</strong>) metric. These values were calculated and recorded daily and you are analyzing data from the previous year. You have been tasked with finding data-driven insights on ways to improve the ad campaign. Looking at the ROI daily time series, you see a weekly oscillation in the data. Segmenting by day of the week, you find the following ROI distributions (where 0 represents the first day of the week and 6 represents the last).</p>
			<div><div><img src="img/C13018_01_48.jpg" alt="Figure 1.48: A sample violin plot for return on investment&#13;&#10;" width="1279" height="512"/>
				</div>
			</div>
			<h6>Figure 1.41: A sample violin plot for return on investment</h6>
			<p>Since we don't have any categorical fields in the Boston housing dataset we are working with, we'll create one by effectively discretizing a continuous field. In our case, this will involve binning the data into "low", "medium", and "high" categories. It's important to note that we are not simply creating a categorical data field to illustrate the data analysis concepts in this section. As will be seen, doing this can reveal insights from the data that would otherwise be difficult to notice or altogether unavailable.</p>
			<h3 id="_idParaDest-34"><a id="_idTextAnchor034"/>Exercise 7: Creating Categorical Fields From Continuous Variables and Make Segmented Visualizations</h3>
			<ol>
				<li value="1">Scroll up to the pairplot in the Jupyter Notebook where we compared <strong class="bold">MEDV</strong>, <strong class="bold">LSTAT</strong>, <strong class="bold">TAX</strong>, <strong class="bold">AGE</strong>, and <strong class="bold">RM</strong>:<div><img src="img/C13018_01_49.jpg" alt="Figure 1.49: A comparison of plots for MEDV, LSTAT, TAX, AGE, and RM&#13;&#10;" width="1800" height="1289"/></div><h6>Figure 1.42: A comparison of plots for MEDV, LSTAT, TAX, AGE, and RM</h6><p>Take a look at the panels containing <strong class="bold">AGE</strong>. As a reminder, this feature is defined as the proportion of <em class="italics">owner-occupied units built prior to 1940</em>. We are going to convert this feature to a categorical variable. Once it's been converted, we'll be able to replot this figure with each panel segmented by color according to the age category.</p></li>
				<li>Scroll down to <code>Subtopic D: Building and exploring categorical features</code> and click into the first cell. Type and execute the following to plot the <code>kde_kws={'lw': 0}</code> in order to bypass plotting the kernel density estimate in the preceding figure.</p><p>Looking at the plot, there are very few samples with low <strong class="bold">AGE</strong>, whereas there are far more with a very large <strong class="bold">AGE</strong>. This is indicated by the steepness of the distribution on the far right-hand side.</p><p>The red lines indicate 1/3 and 2/3 points in the distribution. Looking at the places where our distribution intercepts these horizontal lines, we can see that only about 33% of the samples have <strong class="bold">AGE</strong> less than 55 and 33% of the samples have <strong class="bold">AGE</strong> greater than 90! In other words, a third of the housing communities have less than 55% of homes built prior to 1940. These would be considered relatively new communities. On the other end of the spectrum, another third of the housing communities have over 90% of homes built prior to 1940. These would be considered very old. We'll use the places where the red horizontal lines intercept the distribution as a guide to split the feature into categories: <strong class="bold">Relatively New</strong>, <strong class="bold">Relatively Old</strong>, and <strong class="bold">Very Old</strong>.</p></li>
				<li>Create a new categorical feature and set the segmentation points by running the following code:<pre>def get_age_category(x): if x &lt; 50:
return 'Relatively New' elif 50 &lt;= x &lt; 85:
return 'Relatively Old' else:
return 'Very Old'
df['AGE_category'] = df.AGE.apply(get_age_category)</pre><p>Here, we are using the very handy Pandas method apply, which applies a function to a given column or set of columns. The function being applied, in this case <code>get_age_category</code>, should take one argument representing a row of data and return one value for the new column. In this case, the row of data being passed is just a single value, the <code>pd.Series.str</code> can accomplish the same thing much faster. Therefore, it's advised to avoid using it if possible, especially when working with large datasets. We'll see some examples of vectorized methods in the upcoming chapter.</p></li>
				<li>Verify the number of  samples we've grouped into each age category by typing <code>df.groupby('AGE_category').size()</code> into a new cell and running it:<div><img src="img/C13018_01_51.jpg" alt="Figure 1.51: Verifying the grouping of variables&#13;&#10;" width="1040" height="302"/></div><h6>Figure 1.44: Verifying the grouping of variables</h6><p>Looking at the result, it can be seen that two class sizes are fairly equal, and the <code>AGE_category</code>.</p></li>
				<li>Construct a violin plot by running the following code:<pre>sns.violinplot(x='MEDV', y='AGE_category', data=df, order=['Relatively New', 'Relatively Old',
'Very Old']);</pre><div><img src="img/C13018_01_52.jpg" alt="Figure 1.52: Violin plot for AGE_category and MEDV&#13;&#10;" width="1644" height="882"/></div><h6>Figure 1.45: Violin plot for AGE_category and MEDV</h6><p>The violin plot shows a kernel density estimate of the median house value distribution for each age category. We see that they all resemble a normal distribution. The Very Old group contains the lowest median house value samples and has a relatively large width, whereas the other groups are more tightly centered around their average. The young group is skewed to the high end, which is evident from the enlarged right half and position of the white dot in the thick black line within the body of the distribution.</p><p>This white dot represents the mean and the thick black line spans roughly 50% of the population (it fills to the first quantile on either side of the white dot). The thin black line represents boxplot whiskers and spans 95% of the population. This inner visualization can be modified to show the individual data points instead by passing <code>inner='point'</code> to <code>sns.violinplot()</code>. Let's do that now.</p></li>
				<li>Re-construct the violin plot adding the <code>inner='point'</code> argument to the <code>sns.violinplot</code> call:<div><img src="img/C13018_01_53.jpg" alt="Figure 1.53: Violin plot for AGE_category and MEDV with the inner = 'point' argument&#13;&#10;" width="1515" height="882"/></div><h6>Figure 1.46: Violin plot for AGE_category and MEDV with the inner = 'point' argument</h6><p>It's good to make plots like this for test purposes in order to see how the underlying data connects to the visual. We can see, for example, how there are no median house values lower than roughly $16,000 for the <strong class="bold">Relatively New</strong> segment, and therefore the distribution tail actually contains no data. Due to the small size of our dataset (only about 500 rows), we can see this is the case for each segment.</p></li>
				<li>Re-construct the pairplot from earlier, but now include color labels for each <code>hue</code> argument, as follows:<pre>cols = ['RM', 'AGE', 'TAX', 'LSTAT', 'MEDV', 'AGE_
category']
sns.pairplot(df[cols], hue='AGE_category',
hue_order=['Relatively New', 'Relatively Old',
'Very Old'],
plot_kws={'alpha': 0.5}, diag_kws={'bins':
30});</pre><div><img src="img/C13018_01_54.jpg" alt="Figure 1.54: Re-constructing pairplot for all variables using color labels for AGE&#13;&#10;" width="1560" height="1287"/></div><h6>Figure 1.47: Re-constructing pairplot for all variables using color labels for AGE</h6><p>Looking at the histograms, the underlying distributions of each segment appear similar for <strong class="bold">RM</strong> and <strong class="bold">TAX</strong>. The <strong class="bold">LSTAT</strong> distributions, on the other hand, look more distinct. We can focus on them in more detail by again using a violin plot.</p></li>
				<li>Re-construct a violin plot comparing the LSTAT distributions for each <code>AGE_category</code> segment:</li>
			</ol>
			<div><div><img src="img/C13018_01_55.jpg" alt="Figure 1.55: Re-constructed violin plots for comparing LSTAT distributions for the AGE_category&#13;&#10;" width="1543" height="873"/>
				</div>
			</div>
			<h6>Figure 1.48: Re-constructed violin plots for comparing LSTAT distributions for the AGE_category</h6>
			<p>Unlike the <strong class="bold">MEDV</strong> violin plot, where each distribution had roughly the same width, here we see the width increasing along with <strong class="bold">AGE</strong>. Communities with primarily old houses (the <strong class="bold">Very Old</strong> segment) contain anywhere from very few to many lower class residents, whereas <strong class="bold">Relatively New</strong> communities are much more likely to be predominantly higher class, with over 95% of samples having less lower class percentages than the <strong class="bold">Very Old</strong> communities. This makes sense, because <strong class="bold">Relatively New</strong> neighborhoods would be more expensive.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor035"/>Summary</h2>
			<p>In this chapter, you have seen the fundamentals of data analysis in Jupyter. We began with usage instructions and features of Jupyter such as magic functions and tab completion. Then, transitioning to data-science-specific material, we introduced the most important libraries for data science with Python.</p>
			<p>In the latter half of the chapter, we ran an exploratory analysis in a live Jupyter Notebook. Here, we used visual assists such as scatter plots, histograms, and violin plots to deepen our understanding of the data. We also performed simple predictive modeling, a topic which will be the focus of the following chapter in this book.</p>
			<p>In the next chapter, we will discuss how to approach predictive analytics, what things to consider when preparing the data for modeling, and how to implement and compare a variety of models using Jupyter Notebooks.</p>
		</div>
	</div></body></html>