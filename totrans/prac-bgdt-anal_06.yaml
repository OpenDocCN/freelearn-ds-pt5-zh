- en: Spark for Big Data Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the use of Hadoop and related technologies in the respective ecosystem gained
    prominence, a few obvious and salient deficiencies of the Hadoop operational model
    became apparent. In particular, the ingrained reliance on the MapReduce paradigm,
    and other facets related to MapReduce, made a truly functional use of the Hadoop
    ecosystem possible only for major firms that were invested deeply in the respective
    technologies.
  prefs: []
  type: TYPE_NORMAL
- en: At the **UC Berkeley Electrical Engineering and Computer Sciences** (**EECS**)
    Annual Research Symposium of 2011, a vision for a new research group at the university
    was announced during a presentation by Prof. Ian Stoica ([https://amplab.cs.berkeley.edu/about/](https://amplab.cs.berkeley.edu/about/)).
    It laid out the foundation of what was to become a pivotal unit that would profoundly
    change the landscape of Big Data. The **AMPLab**, launched in February 2011, aimed
    to deliver a scalable and unified solution by integrating Algorithms, Machines,
    and People that could cater to future needs without requiring any major re-engineering
    efforts.
  prefs: []
  type: TYPE_NORMAL
- en: The most well-known and most widely used project to evolve from the AMPLab initiative
    was Spark, arguably a superior alternative - or more precisely, *extension* -
    of the Hadoop ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will visit some of the salient characteristics of Spark
    and end with a real-world tutorial on how to use Spark. The topics we will cover
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: The advent of Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theoretical concepts in Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core components of Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spark architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark tutorial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advent of Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the first release of Spark became available in 2014, Hadoop had already
    enjoyed several years of growth since 2009 onwards in the commercial space. Although
    Hadoop solved a major hurdle in analyzing large terabyte-scale datasets efficiently,
    using distributed computing methods that were broadly accessible, it still had
    shortfalls that hindered its wider acceptance.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A few of the common limitations with Hadoop were as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**I/O Bound operations**: Due to the reliance on local disk storage for saving
    and retrieving data, any operation performed in Hadoop incurred an I/O overhead.
    The problem became more acute in cases of larger datasets that involved thousands
    of blocks of data across hundreds of servers. To be fair, the ability to co-ordinate
    concurrent I/O operations (via HDFS) formed the foundation of distributed computing
    in Hadoop world. However, leveraging the capability and *tuning* the Hadoop cluster
    in an efficient manner across different use cases and datasets required an immense
    and perhaps disproportionate level of expertise. Consequently, the I/O bound nature
    of workloads became a deterrent factor for using Hadoop against extremely large
    datasets. As an example, machine learning use cases that required hundreds of
    iterative operations meant that the system would incur an I/O overhead for each
    pass of the iteration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MapReduce programming (MR) Model**: As discussed in the earlier parts of
    this book, all operations in Hadoop require expressing problems in terms of the
    MapReduce Programming Model - namely, the user would have to express the problem
    in terms of key-value pairs where each pair can be independently computed. In
    Hadoop, coding efficient MapReduce programs, mainly in Java, was non-trivial,
    especially for those new to Java or to Hadoop (or both).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-MR Use Cases**: Due to the reliance on MapReduce, other more common and
    simpler concepts such as filters, joins, and so on would have to also be expressed
    in terms of a MapReduce program. Thus, a join across two files across a primary
    key would have to adopt a key-value pair approach. This meant that operations,
    both simple and complex, were hard to achieve without significant programming
    efforts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Programming APIs**: The use of Java as the central programming language across
    Hadoop meant that to be able to properly administer and use Hadoop, developers
    had to have a strong knowledge of Java and related topics such as JVM tuning,
    Garbage Collection, and others. This also meant that developers in other popular
    languages such as R, Python, and Scala had very little recourse for re-using or
    at least implementing their solution in the language they knew best.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the whole, even though the Hadoop world had championed the Big Data revolution,
    it fell short of being able to democratize the use of the technology for Big Data
    on a broad scale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The team at AMPLab recognized these shortcomings early on, and set about creating
    Spark to address these and, in the process, hopefully develop a new, superior
    alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming the limitations of Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll now look at some of the limitations discussed in the earlier section and
    understand how Spark addresses these areas, by virtue of which it provides a superior
    alternative to the Hadoop ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: A key difference to bear in mind at the onset is that Spark does NOT need Hadoop
    in order to operate. In fact, the underlying backend from which Spark accesses
    data can be technologies such as HBase, Hive and Cassandra in addition to HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: This means that organizations that wish to leverage a standalone Spark system
    can do so without building a separate Hadoop infrastructure if one does not already
    exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark solutions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**I/O Bound operations**: Unlike Hadoop, Spark can store and access data stored
    in *memory*, namely RAM - which, as discussed earlier, is 1,000+ times faster
    than reading data from a disk. With the emergence of SSD drives, the standard
    in today''s enterprise systems, the difference has gone down significantly. Recent
    NVMe drives can deliver up to 3-5 GB (Giga Bytes) of bandwidth per second. Nevertheless,
    RAM, which averages about 25-30 GB per second in read speed, is still 5-10x faster
    compared to reading from the newer storage technologies. As a result, being able
    to store data in RAM provides a 5x or more improvement to the time it takes to
    read data for Spark operations. This is a significant improvement over the Hadoop
    operating model which relies on disk read for all operations. In particular, tasks
    that involve iterative operations as in machine learning benefit immensely from
    the Spark''s facility to store and read data from memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MapReduce programming (MR) Model**: While MapReduce is the primary programming
    model through which users can benefit from a Hadoop platform, Spark does not have
    the same requirement. This is particularly helpful for more complex use cases
    such as quantitative analysis involving calculations that cannot be easily *parallelized,*
    such as machine learning algorithms. By decoupling the programming model from
    the platform, Spark allows users to write and execute code written in various
    languages without forcing any specific programming model as a pre-requisite.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-MR use cases**: Spark SQL, Spark Streaming and other components of the
    Spark ecosystem provide a rich set of functionalities that allow users to perform
    common tasks such as SQL joins, aggregations, and related database-like operations
    without having to leverage other, external solutions. Spark SQL queries are generally
    executed against data stored in Hive (JSON is another option), and the functionality
    is also available in other Spark APIs such as R and Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Programming APIs**: The most commonly used APIs in Spark are Python, Scala
    and Java. For R programmers, there is a separate package called `SparkR` that
    permits direct access to Spark data from R. This is a major differentiating factor
    between Hadoop and Spark, and by exposing APIs in these languages, Spark becomes
    immediately accessible to a much larger community of developers. In Data Science
    and Analytics, Python and R are the most prominent languages of choice, and hence,
    any Python or R programmer can leverage Spark with a much simpler learning curve
    relative to Hadoop. In addition, Spark also includes an interactive shell for
    ad-hoc analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theoretical concepts in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the core concepts in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: Resilient distributed datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directed acyclic graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkContext
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions and transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark deployment options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resilient distributed datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Resilient distributed datasets**, more commonly known as **RDD**s, are the
    primary data structure used in Spark. RDDs are essentially a collection of records
    that are stored across a Spark cluster in a distributed manner. RDDs are *immutable*,
    which is to say, they cannot be altered once created. RDDs that are stored across
    nodes can be accessed in parallel, and hence support parallel operations natively.'
  prefs: []
  type: TYPE_NORMAL
- en: The user does not need to write separate code to get the benefits of parallelization
    but can get the benefits of *actions and transformations* of data simply by running
    specific commands that are native to the Spark platform. Because RDDs can be also
    stored in memory, as an additional benefit, the parallel operations can act on
    the data directly in memory without incurring expensive I/O access penalties.
  prefs: []
  type: TYPE_NORMAL
- en: Directed acyclic graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In computer science and mathematics parlance, a directed acyclic graph represents
    pairs of nodes (also known as **vertices**) connected with edges (or **lines**)
    that are unidirectional. Namely, given Node A and Node B, the edge can connect
    A à B or B à A but not both. In other words, there isn't a circular relationship
    between any pair of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Spark leverages the concept of DAG to build an internal workflow that delineates
    the different stages of processing in a Spark job. Conceptually, this is akin
    to creating a virtual flowchart of the series of steps needed to obtain a certain
    output. For instance, if the required output involves producing a count of words
    in a document, the intermediary steps map-shuffle-reduce can be represented as
    a series of actions that lead to the final result. By maintaining such a **map**,
    Spark is able to keep track of the dependencies involved in the operation. More
    specifically, RDDs are the **nodes**, and transformations, which are discussed
    later in this section, are the **edges** of the DAG.
  prefs: []
  type: TYPE_NORMAL
- en: SparkContext
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A SparkContext is the entry point for all Spark operations and means by which
    the application connects to the resources of the Spark cluster. It initializes
    an instance of Spark and can thereafter be used to create RDDs, perform actions
    and transformations on the RDDs, and extract data and other Spark functionalities.
    A SparkContext also initializes various properties of the process, such as the
    application name, number of cores, memory usage parameters, and other characteristics.
    Collectively, these properties are contained in the object SparkConf, which is
    passed to SparkContext as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '`SparkSession` is the new abstraction through which users initiate their connection
    to Spark. It is a superset of the functionality provided in `SparkContext` prior
    to Spark 2.0.0\. However, practitioners still use `SparkSession` and `SparkContext`
    interchangeably to mean one and the same entity; namely, the primary mode of interacting
    with `Spark.SparkSession` has essentially combined the functionalities of both
    SparkContext and `HiveContext`.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A DataFrame in Spark is the raw data organized into rows and columns. This is
    conceptually similar to CSV files or SQL tables. Using R, Python and other Spark
    APIs, the user can interact with a DataFrame using common Spark commands used
    for filtering, aggregating, and more generally manipulating the data. The data
    contained in DataFrames are physically located across the multiple nodes of the
    Spark cluster. However, by representing them in a **DataFrame** they appear to
    be a cohesive unit of data without exposing the complexity of the underlying operations.
  prefs: []
  type: TYPE_NORMAL
- en: Note that DataFrames are not the same as Datasets, another common term used
    in Spark. Datasets refer to the actual data that is held across the Spark cluster.
    A DataFrame is the tabular representation of the Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with Spark 2.0, the DataFrame and Dataset APIs were merged and a DataFrame
    in essence now represents a Dataset of Row. That said, DataFrame still remains
    the primary abstraction for users who want to leverage Python and R for interacting
    with Spark data.
  prefs: []
  type: TYPE_NORMAL
- en: Actions and transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are 2 types of Spark operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformations** specify general data manipulation operations such as filtering
    data, joining data, performing aggregations, sampling data, and so on. Transformations
    do not return any result when the line containing the transformation operation
    in the code is executed. Instead, the command, upon execution, supplements Spark''s
    internal DAG with the corresponding operation request. Examples of common transformations
    include: `map`, `filter`, `groupBy`, `union`, `coalesce`, and many others.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Actions**, on the other hand, return results. Namely, they execute the series
    of transformations (if any) that the user may have specified on the corresponding
    RDD and produce an output. In other words, actions trigger the execution of the
    steps in the DAG. Common Actions include: `reduce`, `collect`, `take`, `aggregate`,
    `foreach`, and many others.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that RDDs are immutable. They cannot be changed; transformations and actions
    will always produce new RDDs, but never modify existing ones.
  prefs: []
  type: TYPE_NORMAL
- en: Spark deployment options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark can be deployed in various modes. The most important ones are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standalone mode**: As an independent cluster not dependent upon any external
    cluster manager'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon EC2**: On EC2 instances of Amazon Web Services where it can access
    data from S3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache YARN**: The Hadoop ResourceManager'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other options include **Apache Mesos** and **Kubernetes.**
  prefs: []
  type: TYPE_NORMAL
- en: Further details can be found at the Spark documentation website, [https://spark.apache.org/docs/latest/index.html](https://spark.apache.org/docs/latest/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Spark APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark platform is easily accessible through Spark APIs available in Python,
    Scala, R, and Java. Together they make working with data in Spark simple and broadly
    accessible. During the inception of the Spark project, it only supported Scala/Java
    as the primary API. However, since one of the overarching objectives of Spark
    was to provide an easy interface to a diverse set of developers, the Scala API
    was followed by a Python and R API.
  prefs: []
  type: TYPE_NORMAL
- en: In Python, the PySpark package has become a widely used standard for writing
    Spark applications by the Python developer community. In R, users interact with
    Spark via the SparkR package. This is useful for R developers who may also be
    interested in working with data stored in a Spark ecosystem. Both of these languages
    are very prevalent in the Data Science community, and hence, the introduction
    of the Python and R APIs set the groundwork for democratizing **Big Data** Analytics
    on Spark for analytical use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Core components in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following components are quite important in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark Core
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GraphX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Core
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Core provides fundamental functionalities in Spark, such as working with
    RDDs, performing actions, and transformations, in addition to more administrative
    tasks such as storage, high availability, and other topics.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark SQL provides the user with the ability to query data stored in Apache
    Hive using standard SQL commands. This adds an additional level of accessibility
    by providing developers with a means to interact with datasets via the Spark SQL
    interface using common SQL terminologies. The platform hosting the underlying
    data is not limited to Apache Hive, but can also include JSON, Parquet, and others.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The streaming component of Spark allows users to interact with streaming data
    such as web-related content and others. It also includes enterprise characteristics
    such as high availability. Spark can read data from various middleware and data
    streaming services such as Apache Kafka, Apache Flume, and Cloud based solutions
    from vendors such as Amazon Web Services.
  prefs: []
  type: TYPE_NORMAL
- en: GraphX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GraphX component of Spark supports graph-based operations, similar to technologies
    such as graph databases that support specialized data structures. These make it
    easy to use, access, and represent inter-connected points of data, such as social
    networks. Besides analytics, the Spark GraphX platform supports graph algorithms
    that are useful for business use cases that require relationships to be represented
    at scale. As an example, credit card companies use Graph based databases similar
    to the GraphX component of Spark to build recommendation engines that detect users
    with similar characteristics. These characteristics may include buying habits,
    location, demographics, and other qualitative and quantitative factors. Using
    Graph systems in these cases allows companies to build networks with nodes representing
    individuals and edges representing relationship metrics to find common features
    amongst them.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLlib is one of the flagship components of the Spark ecosystem. It provides
    a scalable, high-performance interface to perform resource intensive machine learning
    tasks in Spark. Additionally, MLlib can natively connect to HDFS, HBase, and other
    underlying storage systems supported in Spark. Due to this versatility, users
    do not need to rely on a pre-existing Hadoop environment to start using the algorithms
    built into MLlib. Some of the supported algorithms in MLlib include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: logistic regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: generalized linear regression, survival regression and others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees, random forests, and gradient-boosted trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation**: Alternating least squares'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: K-means, Gaussian mixtures and others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic modeling**: Latent Dirichlet allocation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apriori**: Frequent Itemsets, Association Rules'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ML workflow utilities include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature transformations**: Standardization, normalization and others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML Pipeline construction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation and hyper-parameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML persistence**: Saving and loading models and Pipelines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark consists of 3 primary architectural components:'
  prefs: []
  type: TYPE_NORMAL
- en: The SparkSession / SparkContext
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Cluster Manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Worker Nodes (that hosts executor processes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **SparkSession/SparkContext**, or more generally the Spark Driver, is the
    entry point for all Spark applications as discussed earlier. The SparkContext
    will be used to create RDDs and perform operations against RDDs. The SparkDriver
    sends instructions to the worker nodes to schedule tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The **Cluster manager** is conceptually similar to Resource Managers in Hadoop
    and indeed, one of the supported solutions is YARN. Other Cluster Managers include
    Mesos. Spark can also operate in a Standalone mode in which case YARN/Mesos are
    not required. Cluster Managers co-ordinate communications between the Worker Nodes,
    manage the nodes (such as starting, stopping, and so on), and perform other administration
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Worker nodes** are servers where Spark applications are hosted. Each application
    gets its own unique **executor process**, namely, processes that perform the actual
    action and transformation tasks. By assigning dedicated executor processes, Spark
    ensures that an issue in any particular application does not impact other applications.
    Worker Nodes consist of the Executor, the JVM, and the Python/R/other application
    process required by the Spark application. Note that in the case of Hadoop, the
    Worker Node and Data Nodes are one and the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/820c15f2-b785-4035-9227-69df5fcfba24.png)'
  prefs: []
  type: TYPE_IMG
- en: Spark solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark is directly available from [spark.apache.org](http://spark.apache.org/)
    as an open-source solution. **Databricks** is the leading provider of the commercial
    solution of Spark. For those who are familiar with programming in Python, R, Java,
    or Scala, the time required to start using Spark is minimal due to efficient interfaces,
    such as the PySpark API that allows users to work in Spark using just Python.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-based Spark platforms, such as the Databricks Community Edition, provide
    an easy and simple means to work on Spark without the additional work of installing
    and configuring Spark. Hence, users who wish to use Spark for programming and
    related tasks can get started much more rapidly without spending time on administrative
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Spark practicals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create an account on Databricks' Community Edition
    and complete a hands-on exercise that will walk the reader through the basics
    of actions, transformations, and RDD concepts in general.
  prefs: []
  type: TYPE_NORMAL
- en: Signing up for Databricks Community Edition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps outline the process of signing up for the **Databricks
    Community Edition**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to [https://databricks.com/try-databricks](https://databricks.com/try-databricks):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b32b5600-60df-4eb1-8bc7-fbef99fc7302.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the START TODAY button and enter your information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/24ce51e3-d0a6-4572-a80c-226886b59f14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Confirm that you have read and agree to the terms in the popup menu (scroll
    down to the bottom for the **Agree** button):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/08c99e13-c3ff-49b6-932a-8786ce3112d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Check your email for a confirmation email from Databricks and click on the
    link to confirm your account:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fa9ccec1-86a7-4b4e-898b-ebcf8fe525a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you click on the link to confirm your account, you''ll be taken to a login
    screen where you can log on using the email address and password you used to sign
    up for the account:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b77a0d6c-f43a-4646-a1f2-6cac2abd0f82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After logging in, click on Cluster to set up a Spark cluster, as shown in the
    following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ec9a4931-343a-4856-8453-81359dc3cdc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Enter `Packt_Exercise` as the Cluster Name and click on the Create Cluster
    button at the top of the page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4c630ae4-a0ae-42e5-8e38-9be37c9b24a5.png)'
  prefs: []
  type: TYPE_IMG
- en: This will initiate the process of starting up a Spark Cluster on which we will
    execute our Spark commands using an iPython notebook. An iPython Notebook is the
    name given to a commonly used IDE - a web-based development application used for
    writing and testing Python code. The notebook can also support other languages
    through the use of kernels, but for the purpose of this exercise, we will focus
    on the Python kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After a while, the Status will change from Pending to Running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c97289e0-9db2-4a45-849f-3062f7f69040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Status changes to Running after a few minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7149a7e-15b2-4c4f-99ce-332c20841de1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on **Workspace** (on the left hand bar) and select **options**, **Users**
    | (`Your userid`) and click on the drop-down arrow next to your email address.
    Select Create | Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/972a70c7-9846-48d0-83bf-c64f87eb3e3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the popup screen, enter `Packt_Exercise` as the name of the notebook and
    click on the Create button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/05754e5e-5029-4729-bff4-41b883ced6c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you click on the **Create** button, you''ll be taken directly to the Notebook
    as shown in the following screenshot. This is the Spark Notebook, where you''ll
    be able to execute the rest of the code given in the next few sections. The code
    should be typed in the cells of the notebook as shown. After entering your code,
    press *Shift + Enter* to execute the corresponding cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fa59787f-7c2f-4ed5-a0e6-62c84d6f76a9.png)'
  prefs: []
  type: TYPE_IMG
- en: For the next few exercises, you can copy-paste the text into the cells of the
    Notebook. Alternatively, you can also import the notebook and load it directly
    in your workspace. If you do so, you'll not need to type in the commands (although
    typing in the commands will provide more hands-on familiarity).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An alternative approach to copy-pasting commands: You can import the notebook
    by clicking on Import as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8e1545cd-bf1c-43a7-9309-4eb40b781d71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Enter the following **URL** in the popup menu (select **URL** as the **Import
    from** option):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/db82dd15-c748-4433-bf0e-96f8a1d820d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The notebook will then show up under your email ID. Click on the name of the
    notebook to load it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e3ab20dd-cf19-40cd-9e8e-e32c3faed7e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Spark exercise - hands-on with Spark (Databricks)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This notebook is based on tutorials conducted by Databricks ([https://databricks.com/](https://databricks.com/)).
    The tutorial will be conducted using the Databricks' Community Edition of Spark,
    available to sign up to at [https://databricks.com/try-databricks](https://databricks.com/try-databricks).
    Databricks is a leading provider of the commercial and enterprise supported version
    of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will introduce a few basic commands used in Spark. Users
    are encouraged to try out more extensive Spark tutorials and notebooks that are
    available on the web for more detailed examples.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation for Spark's Python API can be found at [https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql).
  prefs: []
  type: TYPE_NORMAL
- en: The data for this book was imported into the Databricks' Spark Platform. For
    more information on importing data, go to **Importing Data** - **Databricks**
    ([https://docs.databricks.com/user-guide/importing-data.html](https://docs.databricks.com/user-guide/importing-data.html)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we read about some of the core features of Spark, one of the
    most prominent technologies in the Big Data landscape today. Spark has matured
    rapidly since its inception in 2014, when it was released as a Big Data solution
    that alleviated many of the shortcomings of Hadoop, such as I/O contention and
    others.
  prefs: []
  type: TYPE_NORMAL
- en: Today, Spark has several components, including dedicated ones for streaming
    analytics and machine learning, and is being actively developed. Databricks is
    the leading provider of the commercially supported version of Spark and also hosts
    a very convenient cloud-based Spark environment with limited resources that any
    user can access at no charge. This has dramatically lowered the barrier to entry
    as users do not need to install a complete Spark environment to learn and use
    the platform.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin our discussion on machine learning. Most
    of the text, until this section, has focused on the management of large scale
    data. Making use of the data effectively and gaining *insights* from the data
    is always the final aim. In order to do so, we need to employ the advanced algorithmic
    techniques that have become commonplace today. The next chapter will discuss the
    basic tenets of machine learning, and thereafter we will delve deeper into the
    subject area in the subsequent chapter.
  prefs: []
  type: TYPE_NORMAL
