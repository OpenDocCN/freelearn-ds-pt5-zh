["```py\ntar -xvzf alluxio-1.5.0-hadoop2.7-bin.tar.gz\ncd alluxio-1.5.0-hadoop-2.7\n\n```", "```py\nexport ALLUXIO_MASTER_HOSTNAME=localhost\n\n```", "```py\nfalcon:alluxio-1.5.0-hadoop-2.7 salla$ ./bin/alluxio format\nWaiting for tasks to finish...\nAll tasks finished, please analyze the log at /Users/salla/alluxio-1.5.0-hadoop-2.7/bin/../logs/task.log.\nFormatting Alluxio Master @ falcon\n\n```", "```py\nfalcon:alluxio-1.5.0-hadoop-2.7 salla$ ./bin/alluxio-start.sh local\nWaiting for tasks to finish...\nAll tasks finished, please analyze the log at /Users/salla/alluxio-1.5.0-hadoop-2.7/bin/../logs/task.log.\nWaiting for tasks to finish...\nAll tasks finished, please analyze the log at /Users/salla/alluxio-1.5.0-hadoop-2.7/bin/../logs/task.log.\nKilled 0 processes on falcon\nKilled 0 processes on falcon\nStarting master @ falcon. Logging to /Users/salla/alluxio-1.5.0-hadoop-2.7/logs\nFormatting RamFS: ramdisk 2142792 sectors (1gb).\nStarted erase on disk2\nUnmounting disk\nErasing\nInitialized /dev/rdisk2 as a 1 GB case-insensitive HFS Plus volume\nMounting disk\nFinished erase on disk2 ramdisk\nStarting worker @ falcon. Logging to /Users/salla/alluxio-1.5.0-hadoop-2.7/logs\nStarting proxy @ falcon. Logging to /Users/salla/alluxio-1.5.0-hadoop-2.7/logs\n\n```", "```py\nfalcon:alluxio-1.5.0-hadoop-2.7 salla$ ./bin/alluxio runTests\n2017-06-11 10:31:13,997 INFO type (MetricsSystem.java:startSinksFromConfig) - Starting sinks with config: {}.\n2017-06-11 10:31:14,256 INFO type (AbstractClient.java:connect) - Alluxio client (version 1.5.0) is trying to connect with FileSystemMasterClient master @ localhost/127.0.0.1:19998\n2017-06-11 10:31:14,280 INFO type (AbstractClient.java:connect) - Client registered with FileSystemMasterClient master @ localhost/127.0.0.1:19998\nrunTest Basic CACHE_PROMOTE MUST_CACHE\n2017-06-11 10:31:14,585 INFO type (AbstractClient.java:connect) - Alluxio client (version 1.5.0) is trying to connect with BlockMasterClient master @ localhost/127.0.0.1:19998\n2017-06-11 10:31:14,587 INFO type (AbstractClient.java:connect) - Client registered with BlockMasterClient master @ localhost/127.0.0.1:19998\n2017-06-11 10:31:14,633 INFO type (ThriftClientPool.java:createNewResource) - Created a new thrift client alluxio.thrift.BlockWorkerClientService$Client@36b4cef0\n2017-06-11 10:31:14,651 INFO type (ThriftClientPool.java:createNewResource) - Created a new thrift client alluxio.thrift.BlockWorkerClientService$Client@4eb7f003\n2017-06-11 10:31:14,779 INFO type (BasicOperations.java:writeFile) - writeFile to file /default_tests_files/Basic_CACHE_PROMOTE_MUST_CACHE took 411 ms.\n2017-06-11 10:31:14,852 INFO type (BasicOperations.java:readFile) - readFile file /default_tests_files/Basic_CACHE_PROMOTE_MUST_CACHE took 73 ms.\nPassed the test!\n\n```", "```py\n cd spark-2.2.0-bin-hadoop2.7\n\n```", "```py\ncp ../alluxio-1.5.0-hadoop-2.7/core/common/target/alluxio-core-common-1.5.0.jar .\ncp ../alluxio-1.5.0-hadoop-2.7/core/client/hdfs/target/alluxio-core-client-hdfs-1.5.0.jar .\ncp ../alluxio-1.5.0-hadoop-2.7/core/client/fs/target/alluxio-core-client-fs-1.5.0.jar .\ncp ../alluxio-1.5.0-hadoop-2.7/core/protobuf/target/alluxio-core-protobuf-1.5.0.jar . \n\n```", "```py\n./bin/spark-shell --master local[2] --jars alluxio-core-common-1.5.0.jar,alluxio-core-client-fs-1.5.0.jar,alluxio-core-client-hdfs-1.5.0.jar,alluxio-otobuf-1.5.0.jar\n\n```", "```py\n$ ./bin/alluxio fs copyFromLocal ../spark-2.1.1-bin-hadoop2.7/Sentiment_Analysis_Dataset10k.csv /Sentiment_Analysis_Dataset10k.csv\nCopied ../spark-2.1.1-bin-hadoop2.7/Sentiment_Analysis_Dataset10k.csv to /Sentiment_Analysis_Dataset10k.csv\n\n```", "```py\nscala> sc.hadoopConfiguration.set(\"fs.alluxio.impl\", \"alluxio.hadoop.FileSystem\")\n\n```", "```py\nscala> val alluxioFile = sc.textFile(\"alluxio://localhost:19998/Sentiment_Analysis_Dataset10k.csv\")\nalluxioFile: org.apache.spark.rdd.RDD[String] = alluxio://localhost:19998/Sentiment_Analysis_Dataset10k.csv MapPartitionsRDD[39] at textFile at <console>:24\n\nscala> alluxioFile.count\nres24: Long = 9999\n\n```", "```py\nscala> val localFile = sc.textFile(\"Sentiment_Analysis_Dataset10k.csv\")\nlocalFile: org.apache.spark.rdd.RDD[String] = Sentiment_Analysis_Dataset10k.csv MapPartitionsRDD[41] at textFile at <console>:24\n\nscala> localFile.count\nres23: Long = 9999\n\n```"]