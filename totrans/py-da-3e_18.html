<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Parallel Computing Using Dask
                </header>
            
            <article>
                
<p>Dask is one of the simplest ways to process your data <span>in a parallel manner. The platform is for pandas lovers who struggle with large datasets. Dask offers scalability in a similar manner to Hadoop and Spark and the same flexibility that Airflow and Luigi provide. Dask can be used to work on pandas DataFrames and Numpy arrays that cannot fit into RAM. It splits these data structures and processes them in parallel while making minimal code changes. It utilizes your laptop power and has the ability to run locally. We can also deploy it on large distributed systems as we deploy Python applications. Dask can execute data in parallel and processes it in less time. It also scales the computation power of your workstation without migrating to a larger or distributed environment.</span></p>
<p>The main objective of this chapter is to learn how to perform flexible parallel computation on large datasets using Dask. The platform provides three data types for parallel execution: Dask Arrays, Dask DataFrames, and Dask Bags. The Dask array is like a NumPy array, while Dask DataFrames are like pandas DataFrames. Both can execute data in parallel. A Dask Bag is a wrapper for Python objects so that they can perform operations simultaneously. Another concept we'll cover in this chapter is Dask Delayed, which parallelizes code. Dask also offers data preprocessing and machine learning model development in parallel mode.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Parallel computing using Dask<span> </span></li>
<li>Dask data types</li>
<li>Dask Delayed</li>
<li>Preprocessing <span>data </span>at scale</li>
<li>Machine learning at scale</li>
</ul>
<p>Let's get started!</p>
<h1 id="uuid-2673b023-c721-4070-9b96-aed448420960">Parallel computing using Dask</h1>
<p>Python is one of the most popular programming languages among data professionals. Python data science libraries such as Numpy, Pandas, Scipy, and Scikit-learn can sequentially perform data science tasks. However, with large datasets, these libraries will become very slow due to not being scalable beyond a single machine. This is where Dask comes into the picture. Dask helps data professionals handle datasets that are larger than the RAM size on a single machine. Dask utilizes the multiple cores of a <span><span>processor </span></span>or uses it as a distributed computed environment. Dask has the following qualities:</p>
<ul>
<li>It is familiar with existing Python libraries</li>
<li>It offers flexible task scheduling</li>
<li>It offers a single and distributed environment for parallel computation</li>
<li>It performs fast operations with lower latency and overhead</li>
<li>It can scale up and scale down</li>
</ul>
<p>Dask offers similar concepts to pandas, <span>NumPy, and Scikit-learn, which makes it easier to learn. It is an open source parallel computing Python library that runs on top of pandas, Numpy, and Scikit-learn across multiple cores of a CPU or multiple systems. For example, if a laptop has a quad-core processor, then Dask will use 4 cores for processing the data. If the data won't fit in the RAM, it will be partitioned into chunks before processing. Dask scales up the pandas and NumPy capacity to deal with moderately large datasets. Let's understand how Dask perform operations in parallel by looking at the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/69c5e3db-7f88-419a-9439-f46457b33a4c.png" style=""/></div>
<p><span>Dask creates a task graph to execute a program in parallel mode. In the task graph, nodes represent the task, and the edges between the nodes represent the dependency of one task over another.</span></p>
<p><span>Let's install the Dask library on our local system. By default, Anaconda has Dask installed already, but if you want to reinstall or update Dask, you can use the following command:</span></p>
<pre>conda install dask</pre>
<p><span>We can also install it using the <kbd>pip</kbd> command, as shown here:</span></p>
<pre>pip install dask</pre>
<p>With that, we have learned how to install the <kbd>dask</kbd> library for parallel and fast execution. Now, let's look at the core data types of the Dask library.</p>
<h1 id="uuid-505b10c9-b9c3-4f11-8539-5859eb356f29">Dask data types</h1>
<p>In computer programming, data types are basic building blocks for writing any kind of functionality. They help us work with different types of variables. Data types are the kind of values that are stored in variables. They can be primary and secondary.</p>
<p>Primary data types are the basic data types such as int, float, and char, while secondary data types are developed using primary data types such as lists, arrays, strings, and DataFrames. Dask offers three data structures for parallel operations: DataFrames, Bags, and Arrays. These data structures split data into multiple partitions and distribute them to multiple nodes in the cluster. A Dask DataFrame is a combination of multiple small pandas DataFrames and it operates in a similar manner. Dask Arrays are like NumPy arrays and support all the operations of Numpy. Finally, Dask Bags are used to process large Python objects.</p>
<p>Now, it's time to explore these data types. We'll start with Dask Arrays.</p>
<h2 id="uuid-a016e0d2-3e6f-4c91-b9ea-6f5d9e2dcf2a">Dask Arrays</h2>
<p>A Dask Array is an abstraction of the NumPy n-dimensional array, <span>processed in parallel and partitioned into multiple sub-arrays. These small arrays can be on local or distributed remote machines. Dask Arrays can compute large-sized arraysby utilizing all the available cores in the system. They can be applied to statistics, optimization, bioinformatics, business domains, environmental science, and many more fields. They also support lots of NumPy operations, such as arithmetic and scalar operations, aggregation operations, matrices, and linear algebra operations. However, they do not support unknown shapes. Also, the <kbd>tolist</kbd> and <kbd>sort</kbd> operations are difficult to perform in parallel. Let's understand how Dask Arrays decompose data into a NumPy array and execute them in parallel by taking a look at the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/440d99ec-54de-42e2-9a13-9c42a62657c7.png" style=""/></div>
<p>As we can see, there are multiple blocks of different shapes, all of which represent NumPy arrays. These arrays form a Dask Array and can be executed on multiple machines. Let's create an array using Dask:</p>
<pre># import Dask Array<br/>import dask.array as da<br/><br/># Create Dask Array using arange() function and generate values from 0 to 17<br/>a = da.arange(18, chunks=4)<br/><br/># Compute the array<br/>a.compute()</pre>
<p><span>This results in the following output:</span></p>
<pre>array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,17])</pre>
<p>In the preceding example, we used the <kbd>compute()</kbd> function to <span>get the final output.</span> The <kbd>da.arange()</kbd> function will only create the computational graph, while the <kbd>compute()</kbd> function is used to execute that graph. <span>We have generated 18 values with a chunk size of 4 using the <kbd>da.arange()</kbd> function. Let's also check the chunks in each partition:</span></p>
<pre># Check the chunk size<br/>a.chunks<br/><br/></pre>
<p><span>This results in the following output:</span></p>
<pre>((4, 4, 4, 4, 2),)</pre>
<p>In the preceding example, an array with 18 values was partitioned into five parts with a chunk size of 4, where these initial chunks have 4 values each and the last one has 2 values.</p>
<h2 id="uuid-1721fe9c-1f63-4de3-88c9-13e647c9d9df">Dask DataFrames</h2>
<p class="CDPAlignLeft CDPAlign">Dask DataFrames are abstractions of pandas DataFrames. They are <span>processed in parallel and partitioned into multiple smaller pandas DataFrames, as shown in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><span><span><br/></span></span><img src="assets/3d495ac7-de60-4729-a550-1411cd567081.png" style=""/></div>
<p><span>These small DataFrames can be stored on local or distributed remote machines. Dask DataFrames can compute large-sized DataFrames by utilizing all the available cores in the system. They coordinate the DataFrames using indexing and support standard pandas operations such as <kbd>groupby</kbd>, <kbd>join</kbd>, and <kbd>time series</kbd>. Dask DataFrames perform operations such as element-wise, row-wise, <kbd>isin()</kbd>, and date faster compared to <kbd>set_index()</kbd> and <kbd>join()</kbd> on index operations. Now, let's experiment with the performance or execution speed of Dask:</span></p>
<pre># Read csv file using pandas<br/>import pandas as pd<br/>%time temp = pd.read_csv("HR_comma_sep.csv")</pre>
<p><span>This results in the following output:</span></p>
<pre>CPU times: user 17.1 ms, sys: 8.34 ms, total: 25.4 ms<br/><br/>Wall time: 36.3 ms</pre>
<p>In the preceding code, we tested the read time of a file using the pandas <kbd>read_csv()</kbd> function. Now, let's test the read time for the Dask <kbd>read_csv()</kbd> function:</p>
<pre># Read csv file using Dask<br/><br/>import dask.dataframe as dd<br/><br/>%time df = dd.read_csv("HR_comma_sep.csv")</pre>
<p><span>This results in the following output:</span></p>
<pre>CPU times: user 18.8 ms, sys: 5.08 ms, total: 23.9 ms<br/><br/>Wall time: 25.8 ms</pre>
<p>In both examples, we can <span>observe that the execution time for data reading is reduced when using the Dask <kbd>read_csv()</kbd> function.</span></p>
<h3 id="uuid-0f2f96f6-c755-4a54-a427-d0ce0e89f8f2">DataFrame Indexing</h3>
<p>Dask DataFrames support two types of index: label-based and positional indexing. The main problem with Dask Indexing is that it does not maintain the partition's information. This means it is difficult to perform row indexing; only column indexing is possible. <kbd>DataFrame.iloc</kbd> only supports integer-based indexing, while <kbd>DataFrame.loc</kbd> supports label-based indexing. <kbd>DataFrame.iloc</kbd> only selects columns.</p>
<p>Let's perform these indexing operations on a Dask DataFrame:</p>
<ol>
<li>First, we must create a DataFrame and perform column indexing:</li>
</ol>
<pre style="padding-left: 60px"># Import Dask and Pandas DataFrame<br/>import dask.dataframe as dd<br/>import pandas as pd<br/><br/># Create Pandas DataFrame<br/>df = pd.DataFrame({"P": [10, 20, 30], "Q": [40, 50, 60]},<br/>index=['p', 'q', 'r'])<br/><br/># Create Dask DataFrame<br/>ddf = dd.from_pandas(df, npartitions=2)<br/><br/># Check top records<br/>ddf.head()</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px">P Q<br/><br/>p 10 40<br/><br/>q 20 50<br/><br/>r 30 60</pre>
<p style="padding-left: 60px">In the preceding example, we created a pandas DataFrame (with <kbd>p</kbd>, <kbd>q</kbd>, and <kbd>r</kbd> indexes and <kbd>P</kbd> and <kbd>Q</kbd> columns) and converted it into a Dask DataFrame.</p>
<ol start="2">
<li>The column selection process in Dask is similar to what we do in pandas. Let's select a single column in our Dask DataFrame:</li>
</ol>
<pre style="padding-left: 60px"># Single Column Selection<br/>ddf['P']</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px">Dask Series Structure:<br/><br/>npartitions=1<br/><br/>p int64<br/><br/>r ...<br/>Name: P, dtype: int64<br/>Dask Name: getitem, 2 tasks</pre>
<p style="padding-left: 60px"><span>In the preceding code, we selected a single column by passing the name of the column. For multiple column selection, we need to pass a list of columns. </span></p>
<ol start="3">
<li><span>Let's select multiple columns in our Dask DataFrame:</span></li>
</ol>
<pre style="padding-left: 60px"># Multiple Column Selection<br/>ddf[['Q', 'P']]</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px"><br/>Dask DataFrame Structure:<br/><br/>Q P<br/><br/>npartitions=1<br/><br/>p int64 int64<br/><br/>r ... ...<br/><br/>Dask Name: getitem, 2 tasks</pre>
<p style="padding-left: 60px">Here, we have selected two columns from the list of columns available.</p>
<ol start="4">
<li>Now, let's create a DataFrame with an integer index:</li>
</ol>
<pre style="padding-left: 60px"># Import Dask and Pandas DataFrame<br/>import dask.dataframe as dd<br/>import pandas as pd<br/><br/># Create Pandas DataFrame<br/>df = pd.DataFrame({"X": [11, 12, 13], "Y": [41, 51, 61]})<br/><br/># Create Dask DataFrame<br/>ddf = dd.from_pandas(df, npartitions=2)<br/><br/># Check top records<br/>ddf.head()</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px">X Y<br/><br/>0 11 41<br/><br/>1 12 51<br/><br/>2 13 61</pre>
<p style="padding-left: 60px">In the preceding code, we created a pandas DataFrame and converted it into a Dask DataFrame using the <kbd>from_pandas()</kbd> function.</p>
<ol start="5">
<li><span>Let's select the required column using a positional integer index:</span></li>
</ol>
<pre style="padding-left: 60px">ddf.iloc[:, [1, 0]].compute()</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px">Y X<br/>0 41 11<br/><br/>1 51 12<br/><br/>2 61 13</pre>
<p style="padding-left: 60px">In the preceding code, we swapped the column's location using <kbd>iloc</kbd> while using a positional integer index.</p>
<ol start="6">
<li>If we try to select all the rows, we will get a <kbd>NotImplementedError</kbd>, as shown here:</li>
</ol>
<pre style="padding-left: 60px">ddf.iloc[0:4, [1, 0]].compute()</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px">NotImplementedError: 'DataFrame.iloc' only supports selecting columns. It must be used like 'df.iloc[:, column_indexer]'.</pre>
<p>In the preceding code block, we can see that the <kbd>DataFrame.iloc</kbd> only supports selecting columns. </p>
<h3 id="uuid-f611709b-0f53-4031-a510-39c50fc115f9">Filter data</h3>
<p>We can filter the data from a Dask DataFrame similar to how we would do this for a pandas DataFrame. Let's take a look at the following example:</p>
<pre># Import Dask DataFrame<br/>import dask.dataframe as dd<br/><br/># Read CSV file<br/>ddf = dd.read_csv('HR_comma_sep.csv')<br/><br/># See top 5 records<br/>ddf.head(5)<br/><br/></pre>
<p><span>This results in the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5e709ff8-2afe-406e-8c8f-78b7900d9288.png"/></div>
<p>In the preceding code, we read the human resource CSV file using the <kbd>read_csv()</kbd> function into the Dask DataFrame. This output is only showing some of the columns. However, when you run the notebook for yourself, you will be able to see all the available columns. Let's filter the low-salary employees in the dataset:</p>
<pre># Filter employee with low salary<br/>ddf2 = ddf[ddf.salary == 'low']<br/><br/>ddf2.compute().head()</pre>
<p><span>This results in the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/07172a2b-fe5d-45df-b710-a62a3d81b469.png"/></div>
<p><span>In the preceding code, we filtered the low-salary employees through the condition into the brackets.</span></p>
<h3 id="uuid-c64f9481-122c-4acc-ad97-03a485b54293">Groupby</h3>
<p>The <kbd>groupby</kbd> operation is used to aggregate similar items. First, it splits the data based on the values, finds an aggregate of similar values, and combines the aggregated results. This can be seen in the following code:</p>
<pre># Find the average values of all the columns for employee left or stayed<br/>ddf.groupby('left').mean().compute()</pre>
<p><span>This results in the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b612b1d2-b4e8-4d83-8598-8da093bb00ae.png"/></div>
<p>In the preceding example, we grouped the data based <span>on the left column (it shows an employee who stayed or left the company) and aggregated it by the mean value.</span></p>
<h3 id="uuid-40266b99-7c13-4467-990f-ba1115ac7ae2">Converting a pandas DataFrame into a Dask DataFrame</h3>
<p>Dask DataFrames are implemented based on pandas DataFrames. For data analysts, it is necessary to learn how to convert a Dask DataFrame into a pandas DataFrame. Take a look at the following code:</p>
<pre># Import Dask DataFrame<br/>from dask import dataframe as dd<br/><br/># Convert pandas dataframe to dask dataframe<br/>ddf = dd.from_pandas(pd_df,chunksize=4)<br/><br/>type(ddf)</pre>
<p><span>This results in the following output:</span></p>
<pre>dask.dataframe.core.DataFrame</pre>
<p>Here, we have used the <span><kbd>from_pandas()</kbd> method to convert a pandas DataFrame into a Dask DataFrame.</span></p>
<h3 id="uuid-9f4eac7e-c09f-4955-8f7e-62c5854c9a6e">Converting a Dask DataFrame into a pandas DataFrame</h3>
<p><span>In the previous subsection, we converted a pandas DataFrame into a Dask DataFrame. Similarly, we can convert a Dask DataFrame into a pandas DataFrame using</span> the <kbd>compute()</kbd> method, as shown here:</p>
<pre># Convert dask DataFrame to pandas DataFrame<br/>pd_df = df.compute()<br/><br/>type(pd_df)</pre>
<p><span>This results in the following output:</span></p>
<pre>pandas.core.frame.DataFrame</pre>
<p>Now, let's learn about another important topic: Dask Bags. </p>
<h2 id="uuid-cf5471ff-6fb9-46d3-afe6-83d2039bf394">Dask Bags</h2>
<p>A Dask Bag is an abstraction over generic Python objects. It performs <kbd>map</kbd>, <kbd>filter</kbd>, <kbd>fold</kbd>, and <kbd>groupby</kbd> operations in the parallel interface of smaller Python objects using a Python iterator. This execution is similar to PyToolz or the PySpark RDD. Dask Bags are more suitable for unstructured and <span>semi-structured datasets such as text, JSON, and log files. They perform multiprocessing for computation for faster processing but will not perform well with inter-worker communication. Bags are immutable types of structures that cannot be changed and are slower compared to Dask Arrays and DataFrames. Bags also perform slowly on the <kbd>groupby</kbd> operation, so it is recommended that you use <kbd>foldby</kbd> instead of <kbd>groupby</kbd>. </span></p>
<p><span>Now, let's create various Dask Bag objects and perform operations on them.</span></p>
<h3 id="uuid-d2109cc7-bf76-44a4-af29-fc3f697446f9">Creating a Dask Bag using Python iterable items</h3>
<p>Let's create some Dask Bag objects using Python iterable items:</p>
<pre># Import dask bag<br/>import dask.bag as db<br/><br/># Create a bag of list items<br/>items_bag = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], npartitions=3)<br/><br/># Take initial two items<br/>items_bag.take(2)</pre>
<p><span>This results in the following output:</span></p>
<pre>(1, 2)</pre>
<p><span>In the preceding code, we created a bag of list items using the <kbd>from_sequence()</kbd> method. The <kbd>from_Sequence()</kbd> method takes a list and places it into <kbd>npartitions</kbd> (a number of partitions). Let's filter odd numbers from the list:</span></p>
<pre># Filter the bag of list items<br/>items_square=items_bag.filter(lambda x: x if x % 2 != 0 else None)<br/><br/># Compute the results<br/>items_square.compute()</pre>
<p><span>This results in the following output:</span></p>
<pre>[1, 3, 5, 7, 9]</pre>
<p>In the preceding code, we filtered the odd numbers from the bag of lists using the <kbd>filter()</kbd> method. Now, let's <span>square each item of the bag using the <kbd>map</kbd> function:</span></p>
<pre># Square the bag of list items<br/>items_square=items_b.map(lambda x: x**2)<br/><br/># Compute the results<br/>items_square.compute()</pre>
<p>This results in the following output:</p>
<pre>[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]</pre>
<p>In the preceding code, we used the <kbd>map()</kbd> function to map the bag items. We mapped these items to their square value.</p>
<h3 id="uuid-1e29c49c-8e74-43ae-8cf7-318caaa1220d">Creating a Dask Bag using a text file</h3>
<p>We can <span>create a Dask Bag using a text file by using</span> the <kbd>read_text()</kbd> method, as follows:</p>
<pre># Import dask bag<br/>import dask.bag as db<br/><br/># Create a bag of text file<br/>text = db.read_text('sample.txt')<br/><br/># Show initial 2 items from text<br/>text.take(2)</pre>
<p><span>This results in the following output:</span></p>
<pre>('Hi! how are you? \n', '\n')</pre>
<p>In the preceding code, we read a text file into a <kbd>dask bag</kbd> object by using the <kbd>read_text()</kbd> method. This allowed us to show the two initial items in the Dask Bag.</p>
<h3 id="uuid-d544050b-2ea5-4dc6-bee4-c115cb0395d6">Storing a Dask Bag in a text file</h3>
<p>Let's store a Dask Bag in a text file:</p>
<pre># Convert dask bag object into text file<br/>text.to_textfiles('/path/to/data/*.text.gz')</pre>
<p><span>This results in the following output:</span></p>
<pre>['/path/to/data/0.text.gz']</pre>
<p>In the preceding code, <kbd>to_textfiles()</kbd> converted the <kbd>bag</kbd> object into a text file.</p>
<h3 id="uuid-85bb5ef1-7145-4cf7-9dac-c5d2ea225288">Storing a Dask Bag in a DataFrame</h3>
<p>Let's store a Dask Bag in a DataFrame:</p>
<pre># Import dask bag<br/>import dask.bag as db<br/><br/># Create a bag of dictionary items<br/>dict_bag = db.from_sequence([{'item_name': 'Egg', 'price': 5},<br/>{'item_name': 'Bread', 'price': 20},<br/>{'item_name': 'Milk', 'price': 54}],<br/>npartitions=2)<br/><br/># Convert bag object into dataframe<br/>df = dict_bag.to_dataframe()<br/><br/># Execute the graph results<br/>df.compute()</pre>
<p><span>This results in the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a9775f85-da6d-40b1-8966-54dda0e973a0.png"/></div>
<p>In the preceding example, we created a Dask Bag of dictionary items and converted it into a Dask DataFrame using the <kbd>to_dataframe()</kbd> method. In the next section, we'll look at Dask Delayed.</p>
<h1 id="uuid-60941ead-d771-4cd1-beb1-62508e4c24aa">Dask Delayed</h1>
<p>Dask Delayed is an approach we can use to parallelize code. It can delay the dependent function calls in task <span>graphs and provides complete user control over parallel processes while improving performance. Its lazy computation helps us control the execution of functions. However, this differs from the execution timings of functions for parallel execution.</span></p>
<p><span>Let's understand the concept of Dask Delayed by looking at an example:</span></p>
<pre># Import dask delayed and compute<br/>from dask import delayed, compute<br/><br/># Create delayed function<br/>@delayed<br/>def cube(item):<br/>    return item ** 3<br/><br/># Create delayed function<br/>@delayed<br/>def average(items):<br/>    return sum(items)/len(items)<br/><br/># create a list<br/>item_list = [2, 3, 4]<br/><br/># Compute cube of given item list<br/>cube_list= [cube(i) for i in item_list]<br/><br/># Compute average of cube_list<br/>computation_graph = average(cube_list)<br/><br/># Compute the results<br/>computation_graph.compute()</pre>
<p><span>This results in the following output:</span></p>
<pre><span>33.0</span></pre>
<p>In the preceding example, two methods, <kbd>cube</kbd> and <kbd>average</kbd>, were annotated with <kbd>@dask.delayed</kbd>. A list of three numbers was created and a cube containing every value was computed. After computing the cube of list values, we calculated the average of all the values. All these operations are lazy in nature and are computed later when the output is expected from the programmer and the flow of execution is stored in a computational graph. We executed this using the <kbd>compute()</kbd> method. Here, all the cube operations will execute in a parallel fashion.</p>
<p><span>Now, we will visualize the computational graph. However, before we can do this, we need to install the Graphviz editor.<br/>
<br/>
On Windows, we can install Graphviz using <kbd>pip</kbd>. We must also set the path in an environment variable:</span></p>
<pre><span> pip install graphviz</span></pre>
<p>On Mac, we can install it using <kbd>brew</kbd>, as follows:</p>
<pre>brew install graphviz</pre>
<p>On Ubuntu, we need to install it on a Terminal using the <kbd>sudo apt-get</kbd> command:</p>
<pre>sudo apt-get install graphviz</pre>
<p>Now, let's visualize the computational graph: </p>
<pre># Compute the results<br/>computation_graph.visualize()</pre>
<p>This results in the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8ca62af1-10ff-4937-b676-38a5a916aa17.png" style=""/></div>
<p>In the preceding example, <span>we printed a computational graph using the <kbd>visualize()</kbd> method. In this graph, all the cube operations were executed in a parallel fashion and their result was consumed by the <kbd>average()</kbd> function.</span></p>
<h1 id="uuid-d54bf231-0d25-4a41-ad4d-058464e97a79">Preprocessing data at scale</h1>
<p>Dask preprocessing offers scikit-<span>learn functionalities such as scalers, encoders, and train/test splits. These preprocessing functionalities work well with Dask DataFrames and Arrays since they can fit and transform data in parallel. In this section, we will discuss feature scaling and feature encoding.</span></p>
<h2 id="uuid-d5e51305-d2b3-4d94-a506-deb2891ae5d2">Feature scaling in Dask</h2>
<p>As we discussed in <span>C<span>hapter 7, <em>Cleaning Messy Data</em>, feature scaling, also known as feature normalization, is used to scale the features</span> at the same level. It can handle issues regarding different column ranges and units. Dask also offers scaling methods that have parallel execution capacity. It uses most of the methods that scikit-learn offers:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 24.9786%"><strong>Scaler</strong></td>
<td style="width: 73.0214%"><strong>Description</strong></td>
</tr>
<tr>
<td style="width: 24.9786%">MinMaxScaler</td>
<td style="width: 73.0214%">Transforms features by scaling each feature to a given range</td>
</tr>
<tr>
<td style="width: 24.9786%">RobustScaler</td>
<td style="width: 73.0214%">Scales features using statistics that are robust to outliers</td>
</tr>
<tr>
<td style="width: 24.9786%">StandardScaler</td>
<td style="width: 73.0214%">Standardizes features by removing the mean and scaling them to unit variance</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Let's scale the <kbd>last_evaluation</kbd> (employee performance score) column of the human resource dataset:</p>
<pre># Import Dask DataFrame<br/>import dask.dataframe as dd<br/><br/># Read CSV file<br/>ddf = dd.read_csv('HR_comma_sep.csv')<br/><br/># See top 5 records<br/>ddf.head(5)</pre>
<p><span>This results in the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b1909ce6-1ceb-45ef-a434-838e69870c19.png"/></div>
<p>In the preceding code, we read the human resource CSV file using the <kbd>read_csv()</kbd> function into a Dask DataFrame. The preceding output only shows some of the columns that are available. However, when you run the notebook for yourself, you'll be able to see all the columns in the dataset. Now, let's scale the <kbd>last_evalaution</kbd> column (last evaluated performance score):</p>
<pre># Import MinMaxScaler<br/>from sklearn.preprocessing import MinMaxScaler<br/><br/># Instantiate the MinMaxScaler Object<br/>scaler = MinMaxScaler(feature_range=(0, 100))<br/><br/># Fit the data on Scaler<br/>scaler.fit(ddf[['last_evaluation']])<br/><br/># Transform the data<br/>performance_score=scaler.transform(ddf[['last_evaluation']])<br/><br/># Let's see the scaled performance score<br/>performance_score</pre>
<p><span>This results in the following output:</span></p>
<pre>array([[26.5625],<br/><br/>[78.125 ],<br/><br/>[81.25 ],<br/><br/>...,<br/><br/>[26.5625],<br/><br/>[93.75 ],<br/><br/>[25. ]])</pre>
<p>In the preceding example, we scaled the <kbd>last_evaluation</kbd> (last evaluated performance score) column. We scaled it from a range of 0-1 range to a range of 0-100. Next, we will look at feature encoding in Dask.</p>
<h2 id="uuid-18a97abd-5ffc-426e-93d7-8b43fdfe13ad">Feature encoding in Dask</h2>
<p>As we discussed in <span>Chapter 7, <em>Cleaning Messy Data</em>, feature encoding is a very useful technique for handling categorical features. Dask also offers encoding methods that have parallel execution capacity. It uses most of the methods that scikit-learn offers:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 26.0695%"><strong>Encoder</strong></td>
<td style="width: 72.9947%"><strong>Description</strong></td>
</tr>
<tr>
<td style="width: 26.0695%">LabelEncoder</td>
<td style="width: 72.9947%">Encodes labels with a value between 0 and 1 that's less than the number of classes available.</td>
</tr>
<tr>
<td style="width: 26.0695%">OneHotEncoder</td>
<td style="width: 72.9947%">Encodes categorical integer features as a one-hot encoding.</td>
</tr>
<tr>
<td style="width: 26.0695%">OrdinalEncoder</td>
<td style="width: 72.9947%">Encodes a categorical column as an ordinal variable.</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p class="mce-root">Let's try using these methods:</p>
<pre># Import Dask DataFrame<br/>import dask.dataframe as dd<br/><br/># Read CSV file<br/>ddf = dd.read_csv('HR_comma_sep.csv')<br/><br/># See top 5 records<br/>ddf.head(5)</pre>
<p><span>This results in the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/db24794a-b6d2-4580-b831-673e354e2a22.png"/></div>
<p>In the preceding code, we read the human resource CSV file using the <kbd>read_csv()</kbd> function into a Dask DataFrame. The preceding output only shows some of the columns that are available. However, when you run the notebook for yourself, you'll be able to see all the columns in the dataset. Now, let's scale the <kbd>last_evalaution</kbd> column (last evaluated performance score):</p>
<pre># Import Onehot Encoder<br/>from dask_ml.preprocessing import Categorizer<br/>from dask_ml.preprocessing import OneHotEncoder<br/>from sklearn.pipeline import make_pipeline<br/><br/># Create pipeline with Categorizer and OneHotEncoder<br/>pipe = make_pipeline(Categorizer(), OneHotEncoder())<br/><br/># Fit and transform the Categorizer and OneHotEncoder<br/>pipe.fit(ddf[['salary',]])<br/>result=pipe.transform(ddf[['salary',]])<br/><br/># See top 5 records<br/>result.head()</pre>
<p><span>This results in the following output:</span></p>
<table style="border-collapse: collapse;width: 100%" class="a" border="1">
<tbody>
<tr>
<td style="width: 43.1818px">
<p> </p>
</td>
<td style="width: 55.8182px">
<p><strong>salary_low</strong></p>
</td>
<td style="width: 127px">
<p><strong>salary_medium</strong></p>
</td>
<td style="width: 97px">
<p><strong>salary_high</strong></p>
</td>
</tr>
<tr>
<td style="width: 43.1818px">
<p><strong>0</strong></p>
</td>
<td style="width: 55.8182px">
<p>1.0</p>
</td>
<td style="width: 127px">
<p>0.0</p>
</td>
<td style="width: 97px">
<p>0.0</p>
</td>
</tr>
<tr>
<td style="width: 43.1818px">
<p><strong>1</strong></p>
</td>
<td style="width: 55.8182px">
<p>0.0</p>
</td>
<td style="width: 127px">
<p>1.0</p>
</td>
<td style="width: 97px">
<p>0.0</p>
</td>
</tr>
<tr>
<td style="width: 43.1818px">
<p><strong>2</strong></p>
</td>
<td style="width: 55.8182px">
<p>0.0</p>
</td>
<td style="width: 127px">
<p>1.0</p>
</td>
<td style="width: 97px">
<p>0.0</p>
</td>
</tr>
<tr>
<td style="width: 43.1818px">
<p><strong>3</strong></p>
</td>
<td style="width: 55.8182px">
<p>1.0</p>
</td>
<td style="width: 127px">
<p>0.0</p>
</td>
<td style="width: 97px">
<p>0.0</p>
</td>
</tr>
<tr>
<td style="width: 43.1818px">
<p><strong>4</strong></p>
</td>
<td style="width: 55.8182px">
<p>1.0</p>
</td>
<td style="width: 127px">
<p>0.0</p>
</td>
<td style="width: 97px">
<p>0.0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In the preceding example, <span>the <kbd>scikit-learn</kbd> pipeline was created using  <kbd>Categorizer()</kbd> and <kbd>OneHotEncoder()</kbd>. The Salary column of the Human Resource data was then encoded using the <kbd>fit()</kbd> and <kbd>transform()</kbd> methods. Note that the categorizer will convert the columns of a DataFrame into categorical data types.</span></p>
<p>Similarly, we can also encode the Salary column using the ordinal encoder. Let's take a look at an example:</p>
<pre># Import Onehot Encoder<br/>from dask_ml.preprocessing import Categorizer<br/>from dask_ml.preprocessing import OrdinalEncoder<br/>from sklearn.pipeline import make_pipeline<br/><br/># Create pipeline with Categorizer and OrdinalEncoder<br/>pipe = make_pipeline(Categorizer(), OrdinalEncoder())<br/><br/># Fit and transform the Categorizer and OneHotEncoder<br/>pipe.fit(ddf[['salary',]])<br/>result=pipe.transform(ddf[['salary',]])<br/><br/># Let's see encoded results<br/>result.head()</pre>
<p><span>This results in the following output:</span></p>
<pre>salary<br/><br/>0 0<br/><br/>1 1<br/><br/>2 1<br/><br/>3 0<br/><br/>4 0</pre>
<p>In the preceding example, the scikit-learn pipeline was created using <kbd>Categorizer()</kbd> and <kbd>OrdinalEncoder()</kbd>. The Salary column of the Human Resource data was then encoded using the <kbd>fit()</kbd> and <kbd>transform()</kbd> methods. Note that the categorizer will convert the columns of a DataFrame in categorical data types.</p>
<h1 id="uuid-8a610477-2d32-40bb-a139-3cb33d8c743f">Machine learning at scale</h1>
<p><span>Dask offers Dask-ML services for large-scale machine learning operations using Python. Dask-ML decreases the model training time for medium-sized datasets and experiments with hyperparameter tuning. It offers scikit-learn-like machine learning algorithms for ML operations.</span></p>
<p><span>We can scale scikit-learn in three different ways: parallelize scikit-learn using <kbd>joblib</kbd> by using random forest and SVC; reimplement algorithms using Dask Arrays using generalized linear models, preprocessing, and clustering; and partner it with distributed libraries such as XGBoost and Tensorflow.</span></p>
<p>Let's start by looking at parallel computing using scikit-learn.</p>
<h2 id="uuid-8c90bdd6-8710-4266-8964-f1591c6a41ee">Parallel computing using scikit-learn</h2>
<p>To perform parallel computing using scikit-learn on a single CPU, we need to use <kbd>joblib</kbd>. This makes scikit-learn operations parallel computable. The <kbd>joblib</kbd> library <span>performs parallelization on Python jobs. Dask can help us perform parallel operations on multiple scikit-learn estimators. Let's take a look:</span></p>
<ol>
<li>First, we need to read the dataset. We can load the dataset using a pandas DataFrame, like so:</li>
</ol>
<pre style="padding-left: 60px"># Import Dask DataFrame<br/>import pandas as pd<br/><br/><br/># Read CSV file<br/>df = pd.read_csv('HR_comma_sep.csv')<br/><br/><br/># See top 5 records<br/>df.head(5)</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f8d1eb2a-6eea-4c18-9a03-a2ab565ab35f.png"/></div>
<p style="padding-left: 60px">In the preceding code, we read the human resource CSV file using the <kbd>read_csv()</kbd> function into a Dask DataFrame. The preceding output only shows some of the columns that are available. However, when you run the notebook for yourself, you will be able to see all the columns in the dataset. Now, let's scale the <kbd>last_evalaution</kbd> column (last evaluated performance score).</p>
<ol start="2">
<li>Next, we must select the dependent and independent columns. To do this, select the columns and divide the data into dependent and independent <span>variables, as follows:</span></li>
</ol>
<pre style="padding-left: 60px"># select the feature and target columns<br/>data=df[['satisfaction_level', 'last_evaluation']]<br/><br/>label=df['left']</pre>
<ol start="3">
<li>Create a scheduler and generate the model in parallel. Import the <kbd>dask.distributed</kbd> client to create a scheduler and worker <span>on a local machine:</span></li>
</ol>
<pre style="padding-left: 60px"># Import client<br/>from dask.distributed import Client<br/><br/># Instantiate the Client<br/>client = Client()</pre>
<ol start="4">
<li>The next step is to create a parallel <span>backend using <kbd>sklearn.externals.joblib</kbd> and write the normal <kbd>scikit-learn</kbd> code:</span></li>
</ol>
<pre style="padding-left: 60px"># import dask_ml.joblib<br/>from sklearn.externals.joblib import parallel_backend<br/><br/>with parallel_backend('dask'):<br/>    # Write normal scikit-learn code here<br/>    from sklearn.ensemble import RandomForestClassifier<br/>    from sklearn.metrics import accuracy_score<br/>    from sklearn.model_selection import train_test_split<br/><br/>    # Divide the data into two parts: training and testing set<br/>    X_train, X_test, y_train, y_test = train_test_split(data,label,<br/>    test_size=0.2,<br/>    random_state=0)<br/>    <br/>    # Instantiate RandomForest Model<br/>    model = RandomForestClassifier()<br/><br/>    # Fit the model<br/>    model.fit(X_train,y_train)<br/><br/>    # Predict the classes<br/>    y_pred = model.predict(X_test)<br/><br/>    # Find model accuracy<br/>    print("Accuracy:",accuracy_score(y_test, y_pred))</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px"><span>Accuracy: 0.92</span></pre>
<p>The preceding parallel generated random forest model has given us 92% accuracy, which is very good.</p>
<h2 id="uuid-7228404b-6a60-480e-b108-8e05207a0787">Reimplementing ML algorithms for Dask</h2>
<p>Some machine learning algorithms have been reimplemented by the Dask development team using Dask Arrays and DataFrames. The following algorithms <span>have been reimplemented:</span></p>
<ul>
<li>Linear machine learning models such as <span>linear regression and logistic regression</span></li>
<li>Preprocessing with scalers and encoders</li>
<li>Unsupervised algorithms such as k-means clustering and spectral clustering</li>
</ul>
<p>In the following subsection, we will build a logistic regression model and perform clustering on the dataset.</p>
<h3 id="uuid-3ea4612e-da5a-4c8c-81fe-19f074c15e2a">Logistic regression</h3>
<p>Let's build a classifier using logistic regression:</p>
<ol>
<li>Load the dataset into a Dask DataFrame, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Read <span>CSV file using Dask<br/>import dask.dataframe as dd<br/><br/># Read Human Resource Data<br/>ddf = dd.read_csv("HR_comma_sep.csv")<br/><br/># Let's see top 5 records<br/>ddf.head()</span></pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f287e4e5-469b-41fc-8117-06e8045a37fb.png"/></div>
<p style="padding-left: 60px"><span>In the preceding code, we read the human resource CSV file using the</span> <kbd>read_csv()</kbd> <span>function into a Dask DataFrame. The preceding output only shows some of the columns that are available. However, you run the notebook for yourself, you will be able to see all the columns in the dataset. Now, let's scale the</span> <kbd>last_evalaution</kbd> <span>column (last evaluated performance score).</span></p>
<ol start="2">
<li>Next, select the required column for classification and divide it into dependent and independent variables:</li>
</ol>
<pre style="padding-left: 60px">data=ddf[['satisfaction_level','last_evaluation']].to_dask_array(lengths=True)<br/><br/>label=ddf['left'].to_dask_array(lengths=True)</pre>
<ol start="3">
<li>Now, let's create a <kbd>LogisticRegression</kbd> model. First, import <kbd>LogisticRegression</kbd> and <kbd>train_test_split</kbd>. Once you've imported the required libraries, divide the dataset into two parts; that is, training and testing datasets:</li>
</ol>
<pre style="padding-left: 60px"># Import Dask based LogisticRegression<br/>from dask_ml.linear_model import LogisticRegression<br/><br/># Import Dask based train_test_split<br/>from dask_ml.model_selection import train_test_split<br/><br/># Split data into training and testing set<br/>X_train, X_test, y_train, y_test = train_test_split(data, label)</pre>
<ol start="4">
<li>Instantiate the model and fit it to a training dataset. Now, you can predict the test data and compute the model's accuracy, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span># Create logistic regression model<br/>model = LogisticRegression()<br/><br/># Fit the model<br/>model.fit(X_train,y_train)<br/><br/># Predict the classes<br/>y_pred = model.predict(X_test)<br/><br/># Find model accuracy<br/>print("Accuracy:",accuracy_score(y_test, y_pred))<br/></span></pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px">Accuracy: 0.7753333333333333</pre>
<p style="padding-left: 60px">As we can see, the model is offering an accuracy of 77.5%, which is considered good.</p>
<h3 id="uuid-aa8b0bb0-b726-4db6-8037-7868d06cf85f">Clustering</h3>
<p>The developers of Dask have also reimplemented various k-means clustering algorithms. Let's perform <span>clustering using Dask:</span></p>
<ol>
<li><span>Read the human resource data into a Dask DataFrame, as follows:</span></li>
</ol>
<pre style="padding-left: 60px"># Read <span>CSV file using Dask<br/>import dask.dataframe as dd<br/><br/># Read Human Resource Data<br/>ddf = dd.read_csv("HR_comma_sep.csv")<br/><br/># Let's see top 5 records<br/>ddf.head()</span></pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/297bf3c5-6313-4b48-9d89-c7ca02af7461.png"/></div>
<p style="padding-left: 60px">In the preceding code, we read the human resource CSV file using the <kbd>read_csv()</kbd> function into a Dask DataFrame. The preceding output only shows some of the columns that are available. However, when you run the notebook for yourself, you will be able to see all the columns in the dataset. Now, let's scale the <kbd>last_evalaution</kbd> column (last evaluated performance score).</p>
<ol start="2">
<li class="CDPAlignLeft CDPAlign">Next, select the <span>required column for k-means clustering. We have selected the <kbd>satisfaction_level</kbd> and <kbd>last_evaluation</kbd> columns here:</span></li>
</ol>
<pre style="padding-left: 60px">data=ddf[['satisfaction_level', 'last_evaluation']].to_dask_array(lengths=True)</pre>
<ol start="3">
<li>Now, let's <span>create a k-means clustering model. First, import k-means. Once you've imported the required libraries, fit them onto the dataset and get the necessary labels. We can find these labels by using the <kbd>compute()</kbd> method:</span></li>
</ol>
<pre style="padding-left: 60px"># Import Dask based Kmeans<br/>from dask_ml.cluster import KMeans<br/><br/># Create the Kmeans model<br/>model=KMeans(n_clusters=3)<br/><br/># Fit the model<br/>model.fit(data)<br/><br/># Predict the classes<br/>label=model.labels_<br/><br/># Compute the results<br/>label.compute()</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<pre style="padding-left: 60px">array([0, 1, 2, ..., 0, 2, 0], dtype=int32)</pre>
<p style="padding-left: 60px">In the preceding code, we created the k-means model with three clusters, fitted the model, and predicted the labels for the cluster.</p>
<ol start="4">
<li>Now, we will visualize the k-means results using the <kbd>matplotlib</kbd> library:</li>
</ol>
<pre style="padding-left: 60px"># Import matplotlib.pyplot<br/>import matplotlib.pyplot as plt<br/><br/># Prepare x,y and cluster_labels<br/>x=data[:,0].compute()<br/>y=data[:,1].compute()<br/>cluster_labels=label.compute()<br/><br/># Draw scatter plot<br/>plt.scatter(x,y, c=cluster_labels)<br/><br/># Add label on X-axis<br/>plt.xlabel('Satisfaction Level')<br/><br/># Add label on X-axis<br/>plt.ylabel('Performance Level')<br/><br/># Add a title to the graph<br/>plt.title('Groups of employees who left the Company')<br/><br/># Show the plot<br/>plt.show()</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/90f1c4ae-6f0b-423a-8161-42c3f618b457.png"/></div>
<p>In the preceding code, we visualized the clusters using <kbd>matplotlib.pyplot</kbd>. Here, we have plotted the satisfaction score on the X-axis, the performance score on the Y-axis, and distinguished between the clusters by using different colors.</p>
<h1 id="uuid-77447431-c1f8-4539-8ea6-3bfc62d8c70b">Summary</h1>
<p>In this chapter, we focused on how to perform parallel computation on basic data science Python libraries such as pandas, Numpy, and scikit-learn. Dask provides a complete abstraction for DataFrames and Arrays for processing moderately large <span>datasets over single/multiple core machines or multiple nodes in a cluster.</span></p>
<p><span>We started this chapter by looking at Dask data types such as DataFrames, Arrays, and Bags. After that, we focused on Dask Delayed, preprocessing, and machine learning algorithms in a parallel environment.</span></p>
<p>This was the last chapter of this book, which means our learning journey ends here. We have focused on core Python libraries for data analysis and machine learning such as pandas, Numpy, Scipy, and scikit-learn. We have also focused on Python libraries that can be used for text analytics, image analytics, and parallel<span> computation such as NLTK, spaCy, OpenCV, and Dask</span>. Of course, your learning process doesn't need to stop here; keep learning new things and about the latest changes. Try to explore and change code based on your business or client needs. You can also start private or personal projects for learning purposes. If you are unable to decide <span>on what kind of project you want to start, you can participate in Kaggle competitions at </span><a href="http://www.kaggle.com/">http://www.kaggle.com/</a> and more!</p>


            </article>

            
        </section>
    </body></html>