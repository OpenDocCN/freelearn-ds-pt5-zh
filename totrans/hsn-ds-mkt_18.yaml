- en: What's Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have come a long way. We started this book with the basics of data science
    and its applications in marketing and worked through numerous use cases of data
    science in marketing. Along the way, we have conducted descriptive analysis, where
    we used data science techniques to analyze and visualize data to identify patterns.
    We have also conducted explanatory analysis, where we used machine learning models
    to draw insights from data, such as finding the drivers behind certain customers'
    activities and the correlations between customer attributes and their actions.
    Lastly, we have also looked at predictive analytics, where we trained various
    machine learning algorithms to make forecasts on certain actions of customers.
  prefs: []
  type: TYPE_NORMAL
- en: The topics we have covered throughout this book are not trivial and were geared
    toward the practical usage of data science in marketing. Each chapter was meant
    to showcase how you can use data science and machine learning techniques in actual
    marketing use cases and guide you through how you might be able to apply the concepts
    discussed to your specific business cases. As the field of marketing analytics
    is growing and broadening its reach, we wanted to use this chapter to inform you
    of some potential challenges you might face and look at some other commonly used
    technologies, as well as review the topics that we have discussed in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Recap of the topics covered in this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-life data science challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More machine learning models and packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recap of the topics covered in this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a large amount of material from the beginning of this book,
    from discussing the trends in marketing and how data science and machine learning
    have become a crucial part in building marketing strategies, to building various
    predictive machine learning models for more efficient marketing. It is worth reviewing
    what we have covered so far and refreshing our memory before we close this book.
  prefs: []
  type: TYPE_NORMAL
- en: Trends in marketing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may recall, the first thing we discussed in [Chapter 1](c169428b-e0db-4624-896c-24316e9b29cc.xhtml),
    *Data Science and Marketing*, was the recent trends in marketing. It is important
    to try to understand and keep up with the trends that are occurring in the industry
    that you are working and specializing in. Especially in marketing, there is a
    lot of demand for more data-driven and quantitative marketing, and for the use
    of the latest and most intelligent technologies for developing more cost-effective
    marketing strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the February, 2018, CMO survey ([https://www.forbes.com/sites/christinemoorman/2018/02/27/marketing-analytics-and-marketing-technology-trends-to-watch/#4ec8a8431b8a](https://www.forbes.com/sites/christinemoorman/2018/02/27/marketing-analytics-and-marketing-technology-trends-to-watch/#4ec8a8431b8a)),
    reliance on marketing analytics has gone up from 30% to 42% in the past 5 years.
    The three main trends in marketing that can be easily observed are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rising importance of digital marketing**: Lots of marketing activities are
    now happening more heavily on digital channels, such as search engines, social
    media, email, and websites, rather than on more traditional mass media, such as
    TV, radio, and banners at bus stations. As various digital marketing channels
    are gaining popularity as the choice of marketing channel, it has become more
    important to have a good understanding of how audience targeting works on social
    networks, such as Facebook and Instagram, or how to place advertisements on search
    engines and video streaming services, such as Google and YouTube.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Marketing analytics**: Marketing analytics is a way of monitoring and quantifying
    the results and performances of past marketing efforts. In [Chapter 2](1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml), *Key
    Performance Indicators and Visualizations*, we learned about various **key performance
    indicators** (**KPIs**) that we can use to track and quantify the returns from
    various marketing efforts. Marketing analytics does not just stop at analyzing
    KPIs. It can also be applied to product and customer analytics, which we discussed
    in [Chapter 5](73a716c6-6a84-4785-b04e-87651d0a29d1.xhtml), *Product Analytics*, and
    [Chapter 7](72e8f4ee-7f95-4acc-928d-d33c9fc31bd6.xhtml), *Exploratory Analysis
    for Customer Behavior*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized and target marketing**: As the accessibility of data science
    and machine learning has become easier, another trend in marketing has arisen:
    individual-level targeted marketing. Using predictive analytics, we can now predict
    what types of products that individual customers would like, which we have discussed
    in [Chapter 6](d3ba7047-2873-4b03-9a44-4c1d55b84178.xhtml), *Recommending the
    Right Products*. We have also seen how we can target those customers who are likely
    to churn by building predictive machine learning models in [Chapter 11](3d5c7798-6874-40e9-b7e9-6fe39592cc2a.xhtml),
    *Retaining Customers*. As targeted marketing results in higher ROI, there are
    many **software-as-a-Service** (**SaaS**) companies, such as Sailthru and Oracle,
    that provide platforms for personalized and target marketing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As new strategies and technologies are developed, trends are destined to change.
    The trends that we have discussed in this book might not be applicable in 20-30
    years, time. As a marketing professional, it is critical to follow and understand
    what others in the same industry do and what other approaches or technologies
    are being developed and used to achieve higher ROI.
  prefs: []
  type: TYPE_NORMAL
- en: Data science workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a marketing professional or an aspiring data scientist in marketing, it
    can be challenging to figure out where to start for a data science project. In
    [Chapter 1](c169428b-e0db-4624-896c-24316e9b29cc.xhtml), *Data Science and Marketing*,
    we have discussed a typical workflow for a data science project. It is worth reviewing
    the steps before you embark on your future marketing data science projects. You
    should be familiar with the following workflow diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c76e5d08-c952-4db3-bd8a-7321b80ba75d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s talk a bit more in detail about these six steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem definition**: Any data science and machine learning project should
    have a clear problem definition. You will need to have an in-depth understanding
    of the problem itself, the scope of the project, and approaches to coming up with
    solutions. This is where you brainstorm what types of analyses and data science
    techniques to use.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data collection**: As it is for any data science project, having data is
    key for success. In this step, you will need to gather all the required data for
    your data science project. It is common that you will need to implement data collection
    processes for internal data, purchase third-party data, or scrape data from different
    websites. Depending on the cases, the data collection step can be trivial or it
    can also be tedious.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data preparation**: With the data from the data collection step, the next
    step is to clean and prepare the data. As we have seen throughout this book, our
    programming exercises always started with data cleanup and preparation. In the
    data preparation step, we handled missing values, encoded categorical variables,
    or transformed other variables, so that this data can be understood by machine
    learning algorithms.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data analysis**: As you may recall, we have discovered useful insights from
    this data analysis step in our programming exercises throughout the book. Through
    analyzing data, we gain a better understanding of the overall distributions of
    different variables, and it is often a good idea to visualize data with different
    plots to identify any noticeable patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature engineering**: As we have seen and discussed throughout the book,
    there are many different ways to approach engineering the features for machine
    learning models. For monetary values, we have applied log transformations. In
    some cases, we have normalized the data so that the variables are on the same
    scale. We have also used one-hot encoding to encode categorical variables. Feature
    engineering is one of the most important steps in building machine learning models,
    as the algorithms are going to try to learn from these features to correctly predict
    the target.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model building**: The final step in a typical data science workflow is, of
    course, model building. With the clean data and features that we have built from
    previous steps, this is where you train your machine learning models. Throughout
    this book, we have discussed how to evaluate the models. For classification models,
    we have often used accuracy, precision, recall, the ROC curve, and the AUC. For
    regression models, we have used MSE, *R*², or a scatterplot of predicted and actual
    values for model evaluations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During our programming exercises, our workflow looked almost the same as the
    workflow that we have just discussed. When unsure about what to do next, we hope
    this workflow diagram gives you some hints on the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you may recall, we built a number of machine learning models in this book.
    For example, in [Chapter 8](4f5163a1-c34a-495f-bc5f-e02f9b2a2052.xhtml), *Predicting
    the Likelihood of Marketing Engagement*, we trained a random forest model to predict
    how likely each customer is to engage with marketing calls. In [Chapter 11](3d5c7798-6874-40e9-b7e9-6fe39592cc2a.xhtml),
    *Retaining Customers*, we used an **artificial neural network** (**ANN**) model
    to identify which customers are likely to churn from the business. In this section,
    we will review those machine learning models that we have used in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logistic regression**: In [Chapter 3](ce2c2775-9817-4b18-972c-db8e8c629b74.xhtml),
    *Drivers behind Marketing Engagement*, we have used a logistic regression model
    to extract the insights on which factors make customers more likely to engage
    with marketing campaigns. In Python, we used the `statsmodels` package to build
    a logistic regression model, and the code to train a logistic regression model
    looked like the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'From this trained model, we could look at the details and correlations between
    the features and the target variable by running `logit_fit.summary()`. On the
    other hand, in R, we used the following command to train a logistic regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Similar to how we used the `summary` function in Python, we could run the `summary(logit.fit)`
    command to get the details of the logistic regression fit and the correlations
    between the features and the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random forest**: As you may recall, we used a random forest algorithm in
    [Chapter 8](4f5163a1-c34a-495f-bc5f-e02f9b2a2052.xhtml), *Predicting the Likelihood
    of Marketing Engagement*, to predict which customers are likely to respond to
    marketing calls. In Python, we used the `scikit-learn` package to build random
    forest models. The code to train a random forest model looked like the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may recall, there were numerous hyperparameters you could tune with
    the random forest algorithm. We have discussed how you can fine-tune the number
    of estimators in the forest, `n_estimators`, the maximum depth of the tree, `max_depth`,
    and the minimum of samples needed to be able to split into branches, `min_samples_split`.
    On the other hand, in R, we used the `randomForest` library to build random forest
    models. The code for training a random forest model in R looked like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With the `randomForest` package, you could fine-tune the hyperparameters. You
    could use `ntree` to tune the number of trees in the forest, `sampsize` to tune
    the size of the sample to draw for training each tree, and `maxnodes` to define
    the maximum number of terminal nodes in the tree.
  prefs: []
  type: TYPE_NORMAL
- en: '**ANN**: As you may recall, in [Chapter 11](3d5c7798-6874-40e9-b7e9-6fe39592cc2a.xhtml),
    *Retaining Customers*, we used an ANN model to predict the customers who are likely
    to churn from the business. In order to build an ANN model, we used the `keras`
    package for both Python and R. In Python, training an ANN model looked like the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As you should know already, we first had to add input, hidden, and output layers
    to the model. Then, we could compile and train an ANN model. In R, the concept
    is the same, but the syntax looks a bit different. The R code to train an ANN
    model using the `keras` package looked like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**k-means clustering**: In [Chapter 10](5955002d-2a75-4d5a-aa6a-86710a3bf00e.xhtml),
    *Data-Driven Customer Segmentation*, we used a k-means clustering algorithm to
    programmatically build different customer segments. We have seen how analyzing
    the attributes of these different customer segments can help us understand the
    different behaviors of the customers and find better ways to target different
    groups of customers. In Python, we could use the `scikit-learn` package to build
    a k-means clustering algorithm. The code looked like the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may recall, you needed to define the number of clusters you would like
    to build from the data, using the `n_clusters` parameter. In order to get the
    cluster labels for each record and cluster centroids, we could use `kmeans.labels_`
    and `kmeans.cluster_centers_`. Similarly, in R, we used the `kmeans` function
    to build a clustering model, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In order to get the labels and cluster centroids, we could use `cluster$cluster`
    and `cluster$centers`.
  prefs: []
  type: TYPE_NORMAL
- en: With these algorithms, we were able to easily build various machine learning
    models for different use cases in marketing. We hope these brief reviews of the
    syntax of building these machine learning models helped to refresh your memory.
  prefs: []
  type: TYPE_NORMAL
- en: Real-life data science challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Applying data science and machine learning in marketing would be all glamorous
    and flawless if we were able to just build and use various machine learning models
    for different marketing use cases. However, that normally is not the case. Quite
    often, the end-to-end machine learning model building process can be tedious,
    with lots of barriers and bottlenecks on the way. We are going to discuss some
    of the most frequently appearing data science challenges in real life, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in choosing the right model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most challenging factors in using data science and machine learning
    for marketing is getting the right data. As may sound obvious to you, without
    data, there is no data science or machine learning. Moreover, if the quality of
    the data is not good, then the quality of your trained machine learning is also
    going to be bad.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are going to discuss some of the common challenges that
    many data scientists face in getting the right data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Existence of data**: Sometimes you may come up with a great idea of applying
    data science techniques to solve one of the problems you have in marketing. However,
    the data you need might not even exist. For example, say your idea was to identify
    trending web content, such as which web pages are viewed the most and liked the
    most by your users. However, you might not have the page view data, if the web
    page tracking functionality was not implemented on your websites. In this case,
    you will need to implement tracking functionality in your websites to track which
    users viewed or liked which content. Then, it is only possible to work on your
    idea after some period of time, when you have gathered enough data for your analysis.
    This type of case happens relatively frequently, so it is critical to have a good
    understanding of how well you track user activities and which parts you are missing.
    If possible, obtaining third-party data is also an option, when the data does
    not exist internally. There are lots of data vendors who sell data that you might
    need. If using a third-party data vendor is an option, that can be a good solution
    when there is no data for your project. Also, there is a lot of publicly available
    data that you can use freely. It is always worthwhile to see whether the data
    you need is publicly available or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accessibility of data**: Data accessibility can be a barrier for a data science
    project. Especially in big corporations, access to certain sets of data is strictly
    restricted to selected subgroups of teams. In this case, even if the required
    dataset exists, it can be difficult or even impossible for data scientists or
    marketing professionals to access and use the data. Where the data is being generated
    from can also cause data accessibility problems. For instance, if the data is
    streamed into other applications without being stored or archived, then this data
    can be lost after it has been streamed. The location of the data files can also
    be a barrier to accessing the data you need. If the data cannot be shared through
    a network or if you cannot reach the location that the data lives in, then that
    can also keep you from using this data. This is why the responsibility and importance
    of data engineering and data engineers is rising. Data engineers work with other
    data scientists or software developers to specifically work on building data pipelines
    through which data with accessibility issues can move to other parts of the business.
    If you are facing issues with data accessibility, it is crucial to first find
    out what the barrier is and consider working with data engineers to build data
    pipelines to make the data accessible for your future projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Messy data**: You can assume the majority of the data you will face in real-life
    data science projects will be messy. It may be in a format that you cannot easily
    understand. It may be segmented into smaller parts that cannot easily be joined
    to each other. Or, there may also be too many missing values or too many duplicate
    records in the data. The degree of messiness of datasets can significantly increase
    the amount of time you need to spend on cleaning up the raw data and making it
    usable. Conducting in-depth data analysis on this messy data is crucial in making
    the data usable for future steps. Sometimes, it may be worthwhile to work with
    data engineers to fix the source that causes the messiness in the data and make
    future data more clean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with different datasets for applying data science techniques and
    using machine learning models for different projects in marketing, you may face
    some challenges in the system infrastructure that you use for developments. Quite often,
    datasets are too big to fit into your laptop or computer. As the size of data
    is grows bigger and bigger everyday, it becomes even more likely that sometime
    in the future, you will have issues with developing data science models on your
    laptop, even if you currently do not have this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main things that can slow you down when working on data science
    projects: shortage of CPU or processing power and shortage of RAM or memory. If
    you do not have enough processing power, your analysis could take long time. Especially
    when training machine learning models, it is not uncommon for model training to
    take days, weeks, or even months. On the other hand, if you do not have enough
    memory, you might end up getting `Out of Memory` errors while running your analysis.
    For example, tree-based models, such as decision trees or random forests, can
    take a large amount of memory, and training such models can fail after hours of
    training because of shortage of memory.'
  prefs: []
  type: TYPE_NORMAL
- en: With the emerging popularity of and developments in cloud computing, there are
    solutions to these problems. Using one of the cloud computing service providers,
    such as AWS, Google, or Microsoft Azure, you can, theoretically, get an unlimited
    amount of computing power and memory. Of course, everything comes with a price.
    Running large data science jobs on these cloud platforms can cost a fortune, if
    you do not plan it right. When working with large datasets, it is wise to consider
    the amount of processing power and memory you would need to successfully run your
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in choosing the right model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing a machine learning algorithm for a given data science project is more
    difficult than it sounds. Some algorithms work more like a black box, where you
    do not know how an algorithm makes predictions or decisions. For example, it is
    quite difficult to understand how a trained random forest model makes predictions
    on the output from the input. The decisions are made from hundreds of different
    decision trees, where each tree works differently with different decision-making
    criteria, and this makes it difficult for a data scientist to fully understand
    what happens in between the input and the output.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, linear models, such as logistic regression models, tell us
    exactly how they are making decisions. Once logistic regression models are trained,
    we know the coefficients given to each feature, and from these coefficients, we
    can deduce what the predicted output is going to be. Depending on your use cases,
    you might need to have this kind of explainability, where you need to be able
    to explain how each feature works and affects the prediction output to your business
    partners. Quite often, more advanced models work more like a black box, and you
    will need to make a trade-off between prediction accuracy and explainability.
  prefs: []
  type: TYPE_NORMAL
- en: More machine learning models and packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this book, we have mainly used the following five machine learning algorithms
    that fit into and work the best for our marketing use cases: logistic regression,
    random forests, ANN, k-means clustering, and collaborative filtering. However,
    there are many more readily available machine learning algorithms that you may
    find useful for your future data science and machine learning projects. We will
    be covering some of the other frequently used machine learning algorithms, what
    packages to use in Python and R, and where to find more information on these algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the other machine learning algorithms to consider in your future projects
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nearest neighbors**: This is a machine learning algorithm that finds the
    pre-defined number of closest samples to a new data point. Even though the concept
    of this algorithm sounds simple, the nearest neighbors algorithm has been used
    successfully in various areas, including image recognition. In the `scikit-learn`
    package of Python, you can use the `KNeighborsClassifier` class in the `neighbors`
    module to build classification models, or you can use the `KNeighborsRegressor`
    class to build regression models. For more details on the usage, we recommend
    you take a look at the following documentation page: [https://scikit-learn.org/stable/modules/neighbors.html](https://scikit-learn.org/stable/modules/neighbors.html).
    On the other hand, in R, you can use the `knn` function in the `class` library.
    For the documentation of this function in R, you can refer to this documentation
    page: [https://www.rdocumentation.org/packages/class/versions/7.3-15/topics/knn](https://www.rdocumentation.org/packages/class/versions/7.3-15/topics/knn).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support vector machine** (**SVM**): SVM is another machine learning algorithm
    that you may find useful. The SVM algorithm tries to find a hyperplane that best
    splits the data into classes or groups. It is especially effective in high-dimensional
    space. The `scikit-learn` package has the `SVC` and `SVR` classes implemented
    in Python for classification and regression models. The documentation page can
    be found at the following link: [https://scikit-learn.org/stable/modules/svm.html](https://scikit-learn.org/stable/modules/svm.html).
    In R, the `e1071` library has the `svm` function, which you can use to train SVM
    models. More documentation on its usage can be found here: [https://www.rdocumentation.org/packages/e1071/versions/1.7-0.1/topics/svm](https://www.rdocumentation.org/packages/e1071/versions/1.7-0.1/topics/svm).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient-boosted trees** (**GBT**): GBT is one of the tree-based machine
    learning algorithms. Unlike the random forest algorithm, the GBT algorithm learns
    and trains each tree sequentially, and each tree learns from the mistakes that
    the previous trees made. It is well known and frequently used for its prediction
    accuracy and robustness. In Python, you can use the `GradientBoostingClassifier`
    class in the `scikit-learn` package''s `ensemble` module for classification problems
    and the `GradientBoostingRegressor` class for regression problems. More details
    about GBT in `scikit-learn` can be found here: [https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting).
    Similarly, in R, the `gbm` package has the GBT algorithm implemented for classification
    and regression problems. You can use the `gbm` function within the `gbm` package
    to train a GBT model. More information can be found at the following link: [https://www.rdocumentation.org/packages/gbm/versions/2.1.5/topics/gbm](https://www.rdocumentation.org/packages/gbm/versions/2.1.5/topics/gbm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed the topics that we discussed in this book. We briefly
    went through the trends that are observable in the marketing industry and how
    data science and machine learning are becoming more and more important in marketing.
    Then, we reviewed a typical data science workflow, where you start with problem
    definition, then move onto data collection, preparation, and analysis, and finally
    move to feature engineering and model building. While working on future data science
    projects, it will be worthwhile to keep the workflow diagram we looked at in the
    back of your head and when stuck with what to do next, refer back to this diagram
    for ideas. We have also shared some of the challenges you might face when working
    with real-world datasets. The three main challenges we covered were data issues,
    infrastructure issues, and choosing the right model. More specifically, we discussed
    the trade-off between explainability and model accuracy. We have suggested some
    workarounds and solutions to these challenges, so we hope they help when you face
    similar challenges. Lastly, we have discussed some other frequently used machine
    learning models that you may find useful in your future projects. We have briefly
    showed which Python and R packages to use for each of these models and where you
    can find more information about the usage of those models.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the 13 chapters in this book, we have covered the various data science
    and machine learning techniques you can use in marketing, with a focus on practicality.
    As you have worked through numerous examples for different use cases in marketing
    throughout this book, we hope you have gained more confidence in applying data
    science techniques and building machine learning models for developing more intelligent
    and efficient marketing strategies. We hope your journey throughout this book
    was worthwhile and rewarding and that you have gained many new and useful skills.
  prefs: []
  type: TYPE_NORMAL
