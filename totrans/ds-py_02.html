<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Python and Jupyter Notebooks to Power your Data Analysis</h1></div></div></div><div><blockquote class="blockquote"><p>"The Best Line of Code is the One You Didn't Have to Write!"</p></blockquote></div><p>– <em>Unknown</em>
</p><p>In the previous chapter, I gave a developer's perspective on data science based on real experience and discussed three strategic pillars required for successful deployment with in the enterprise: data, services, and tools. I also discussed the idea that data science is not only the sole purview of data scientists, but rather a team sport with a special role for developers.</p><p>In this chapter, I'll introduce a solution—based on Jupyter Notebooks, Python, and the PixieDust open source library—that focuses on three simple goals:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Democratizing data science by lowering the barrier to entry for non-data scientists</li><li class="listitem" style="list-style-type: disc">Increasing collaboration between developers and data scientists</li><li class="listitem" style="list-style-type: disc">Making it easier to operationalize data science analytics</li></ul></div><div><div><h3 class="title"><a id="note07"/>Note</h3><p>This solution only focuses on the tools pillar and not on data and services, which should be implemented independently, although we'll cover some of it when discussing the sample applications starting in <a class="link" href="ch06.xhtml" title="Chapter 6. Analytics Study: AI and Image Recognition with TensorFlow">Chapter 6</a>, <em>Analytics Study: AI and Image Recognition with TensorFlow</em>.</p></div></div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec25"/>Why choose Python?</h1></div></div></div><p>Like many developers, when <a id="id55" class="indexterm"/>it came to building data-intensive projects, using Python wasn't my first choice. To be honest, having worked with Java for so many years, Scala seemed much more attractive to me at first, even though the learning curve was pretty steep. Scala is a very powerful language that elegantly combines object-oriented and functional programming, which is sorely lacking in Java (at least until Java 8 started to introduce lambda expressions).</p><p>Scala also provides a very concise syntax that translates into fewer lines of code, higher productivity, and ultimately fewer bugs. This comes in very handy, especially when a large part of your work is to manipulate data. Another reason for liking Scala is the better API coverage when using big data frameworks such as Apache Spark, which are themselves written in Scala. There are also plenty of other good reasons to prefer Scala, such as it's a strong typed system and its interoperability with Java, online documentation, and high performance.</p><p>So, for a developer like myself who is starting to get involved in data science, Scala would seem like a more natural choice, but yet, spoiler alert, we ended up focusing on Python instead. There are multiple reasons for this choice:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Python, as a language, has a lot going on for itself too. It is a dynamic programming language with similar benefits to Scala, such as functional programming, and concise syntax, among others.</li><li class="listitem" style="list-style-type: disc">Python has seen, over the last few years, a meteoric rise among data scientists, overtaking longtime rival R as the overall preferred language for data science, as demonstrated by a quick search for the terms <code class="literal">Python Data Science</code>, <code class="literal">Python Machine Learning</code>, <code class="literal">R Data Science</code>, and <code class="literal">R Machine Learning</code> on Google Trends:<div><img src="img/B09699_02_01.jpg" alt="Why choose Python?" width="541" height="402"/><div><p>Interest trends for 2017</p></div></div></li></ul></div><p>In a virtuous circle, Python's rising <a id="id56" class="indexterm"/>popularity fuels a vast and growing ecosystem of wide-ranging libraries that can be easily imported into your projects using the pip Python package installer. Data scientists now have access to many powerful open source Python libraries for data manipulation, data visualization, statistics, mathematics, machine learning, and natural language processing.</p><p>Even beginners can quickly build a machine learning classifier <a id="id57" class="indexterm"/>using the popular scikit-learn package (<a class="ulink" href="http://scikit-learn.org">http://scikit-learn.org</a>) without being a machine learning expert, or quickly plot rich charts using Matplotlib (<a class="ulink" href="https://matplotlib.org">https://matplotlib.org</a>) or Bokeh (<a class="ulink" href="https://bokeh.pydata.org">https://bokeh.pydata.org</a>).</p><p>In addition, Python has also emerged as one <a id="id58" class="indexterm"/>of the top languages for developers as shown in this IEEE Spectrum 2017 survey (<a class="ulink" href="https://spectrum.ieee.org/computing/software/the-2017-top-programming-languages">https://spectrum.ieee.org/computing/software/the-2017-top-programming-languages</a>):</p><div><img src="img/B09699_02_02.jpg" alt="Why choose Python?" width="1000" height="589"/><div><p>Usage statistics by programming languages</p></div></div><p>This trend is also confirmed <a id="id59" class="indexterm"/>on GitHub where Python is now number three in the total number of repositories, just behind Java and JavaScript:</p><div><img src="img/B09699_02_03.jpg" alt="Why choose Python?" width="1000" height="448"/><div><p>GitHub repositories statistics by programming language</p></div></div><p>The preceding chart shows some interesting statistics, demonstrating how active the Python developer community is. Python - related repositories that are active on GitHub are the third biggest in size, with similarly healthy total code pushes and opened issues per repository.</p><p>Python has also become ubiquitous on the web, powering numerous high-profile websites with web <a id="id60" class="indexterm"/>development frameworks, such as Django (<a class="ulink" href="https://www.djangoproject.com">https://www.djangoproject.com</a>), Tornado (<a class="ulink" href="http://www.tornadoweb.org">http://www.tornadoweb.org</a>) and TurboGears (<a class="ulink" href="http://turbogears.org">http://turbogears.org</a>). More recently, there are signs that Python is also making its way into the field of cloud services with all major Cloud providers including it in <a id="id61" class="indexterm"/>some capacity in their offerings.</p><p>Python obviously has a bright future in the field of data science, especially when used in conjunction with powerful tools such as Jupyter Notebooks, which have become very popular in the data scientist community. The value proposition of Notebooks is that they are very easy to create and <a id="id62" class="indexterm"/>perfect for quickly running experiments. In addition, Notebooks support multiple high-fidelity serialization formats that can capture instructions, code, and results, which can then very easily be shared with other data scientists on the team or as open source for everyone to use. For example, we're seeing an explosion of Jupyter Notebooks being shared on GitHub, numbering in excess of 2.5 million and counting.</p><p>The following screenshot shows the result of a GitHub search for any file with the extension <code class="literal">.ipynb,</code> which is the most popular format for serialized Jupyter Notebooks (JSON format):</p><div><img src="img/B09699_02_04.jpg" alt="Why choose Python?" width="1000" height="542"/><div><p>Search results for Jupyter Notebooks on GitHub</p></div></div><p>This is great, but Jupyter Notebooks are too often thought of as data scientist tools only. We'll see in the coming chapters that they can be much more and that they can also help all types of teams solve data problems. For example, they can help business analysts quickly load and visualize a dataset, enable developers to work with data scientists directly within a Notebook <a id="id63" class="indexterm"/>to leverage their analytics and build powerful dashboards, or allow DevOps to effortlessly deploy these dashboards into scalable, enterprise-ready microservices that can run as standalone web applications or embeddable components. It is based on this vision of bringing the tools of data science to non-data scientists that the PixieDust open source project was created.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec26"/>Introducing PixieDust</h1></div></div></div><div><div><h3 class="title"><a id="tip02"/>Tip</h3><p>
<strong>Fun fact</strong>
</p><p>I am often asked how I came up with the name PixieDust, for which I answer that I simply wanted to make Notebook simple, as in magical, for non-data scientists.</p></div></div><p>PixieDust (<a class="ulink" href="https://github.com/ibm-watson-data-lab/pixiedust">https://github.com/ibm-watson-data-lab/pixiedust</a>) is an open-source project composed primarily of three components designed to address the three goals stated at the beginning of this chapter:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A helper Python library <a id="id64" class="indexterm"/>for Jupyter Notebooks that provides simple APIs to load data from various sources into popular frameworks, such as pandas and Apache Spark DataFrame, and then to visualize and explore the dataset interactively.</li><li class="listitem" style="list-style-type: disc">A simple Python-based programming model that enables developers to "productize" the analytics directly into the Notebook by creating powerful dashboards called PixieApps. As we'll see in the next chapters, PixieApps are different from traditional <strong>BI</strong> (short for, <strong>Business Intelligence</strong>) dashboards because developers <a id="id65" class="indexterm"/>can directly use HTML and CSS to create an arbitrary complex layout. In addition, they can embed in their business logic access to any variable, class, or function created in the Notebook.</li><li class="listitem" style="list-style-type: disc">A secure microservice web server called PixieGateway that can run PixieApps as standalone web applications or as components that can be embedded into any website. PixieApps can easily be deployed from the Jupyter Notebook using a graphical wizard and without requiring any code changes. In addition, PixieGateway supports the sharing of any charts created by PixieDust as embeddable web pages, allowing data scientists to easily communicate results outside of the Notebook.</li></ul></div><p>It is important to note that the PixieDust <code class="literal">display()</code> API primarily supports two popular data processing frameworks:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>pandas</strong> (<a class="ulink" href="https://pandas.pydata.org">https://pandas.pydata.org</a>): By far the most popular Python data analysis package, pandas provides two <a id="id66" class="indexterm"/>main data structures: DataFrame for manipulating two-dimensional table-like datasets, and Series for one-dimensional column-like datasets.<div><div><h3 class="title"><a id="note08"/>Note</h3><p>Currently, only pandas DataFrames are supported by PixieDust <code class="literal">display()</code>.</p></div></div></li><li class="listitem" style="list-style-type: disc"><strong>Apache Spark DataFrame</strong> (<a class="ulink" href="https://spark.apache.org/docs/latest/sql-programming-guide.html">https://spark.apache.org/docs/latest/sql-programming-guide.html</a>): This is a high-level data <a id="id67" class="indexterm"/>structure for manipulating distributed datasets across a Spark Cluster. Spark DataFrames are built on top of the lower-level <strong>RDD</strong> (short for, <strong>Resilient Distributed Dataset</strong>) with the added <a id="id68" class="indexterm"/>functionality that it supports SQL queries.</li></ul></div><p>Another less commonly used format supported by PixieDust <code class="literal">display()</code> is an array of JSON objects. In this case, PixieDust will use the values to build the rows and keys are used as columns, for example, as follows:</p><div><pre class="programlisting">my_data = [
{"name": "Joe", "age": 24},
{"name": "Harry", "age": 35},
{"name": "Liz", "age": 18},
...
]
</pre></div><p>In addition, PixieDust is highly extensible both at the data handling and rendering level. For example, you can add new data types to be rendered by the visualization framework or if you want to leverage a plotting library you particularly like, you can easily add it to the list of renderers supported by PixieDust (see the next chapters for more details).</p><p>You will also find that PixieDust contains a few extra utilities related to Apache Spark, such as the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>PackageManager</strong>: This lets you install <a id="id69" class="indexterm"/>Spark packages inside a Python Notebook.</li><li class="listitem" style="list-style-type: disc"><strong>Scala Bridge</strong>: This lets you use Scala <a id="id70" class="indexterm"/>directly in a Python Notebook using the <code class="literal">%%scala</code> magic. Variables are automatically transferred from Python to Scala and vice versa.</li><li class="listitem" style="list-style-type: disc"><strong>Spark Job Progress Monitor</strong>: Track the status of any Spark job by showing a progress bar <a id="id71" class="indexterm"/>directly in the cell output.</li></ul></div><p>Before we dive into each of the three PixieDust components, it would be a good idea to get access to a Jupyter Notebook, either by signing up to a hosted solution on the cloud (for example, Watson Studio at <a class="ulink" href="https://datascience.ibm.com">https://datascience.ibm.com</a>) or installing a development version on your local machine.</p><div><div><h3 class="title"><a id="note09"/>Note</h3><p>You can install the Notebook server locally by following the instructions here: <a class="ulink" href="http://jupyter.readthedocs.io/en/latest/install.html">http://jupyter.readthedocs.io/en/latest/install.html</a>.</p></div></div><p>To start the Notebook server locally, simply run the following command from a Terminal:</p><div><pre class="programlisting"><strong>jupyter notebook --notebook-dir=&lt;&lt;directory path where notebooks are stored&gt;&gt;</strong>
</pre></div><p>The Notebook home page will automatically open in a browser. There are many configuration options to control how the Notebook server is launched. These options can be added to the command line or persisted in the Notebook configuration file. If you want to experiment with all the possible configuration options, you can generate a configuration file using the <code class="literal">--generate-config</code> option as follows:</p><div><pre class="programlisting">
<strong>jupyter notebook --generate-config</strong>
</pre></div><p>This will generate the following Python file, <code class="literal">&lt;home_directory&gt;/.jupyter/jupyter_notebook_config.py</code>, which contains a set of auto-documented options that have been disabled. For example, if you don't want to have the browser automatically opened when the Jupyter Notebook starts, locate the line that contains the <code class="literal">sc.NotebookApp.open_browser</code> variable, uncomment it, and set it to <code class="literal">False</code>:</p><div><pre class="programlisting">## Whether to open in a browser after starting. The specific browser used is
#  platform dependent and determined by the python standard library 'web browser'
#  module, unless it is <a id="id72" class="indexterm"/>overridden using the --browser (NotebookApp.browser)
#  configuration option.
<strong>c.NotebookApp.open_browser = False</strong>
</pre></div><p>After making that change, simply save the <code class="literal">jupyter_notebook_config.py</code> file and restart the Notebook server.</p><p>The next step is to install the PixieDust library using the <code class="literal">pip</code> tool:</p><div><ol class="orderedlist arabic"><li class="listitem">From the Notebook itself, enter the following command in a cell:<div><pre class="programlisting">
<strong>!pip install pixiedust</strong>
</pre></div><div><div><h3 class="title"><a id="note10"/>Note</h3><p>
<strong>Note</strong>: The exclamation point syntax is specific to Jupyter Notebook and denotes that the rest of the command will be executed as a system command. For example, you could use <code class="literal">!ls</code> to list all the files and directories that are under the current working directory.</p></div></div></li><li class="listitem">Run the cell using the <strong>Cell</strong> | <strong>Run Cells</strong> menu or the <strong>Run</strong> icon on the toolbar. You can also use the following keyboard shortcuts to run a cell:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>Ctrl</em> + <em>Enter</em>: Run and keep the current cell selected</li><li class="listitem" style="list-style-type: disc"><em>Shift</em> + <em>Enter</em>: Run and select the next cell</li><li class="listitem" style="list-style-type: disc"><em>Alt</em> + <em>Enter</em>: Run and create new empty cell just below</li></ul></div></li><li class="listitem">Restart the kernel <a id="id73" class="indexterm"/>to make sure the <code class="literal">pixiedust</code> library is correctly loaded into the kernel.</li></ol></div><p>The following screenshot shows the results after installing <code class="literal">pixiedust</code> for the first time:</p><div><img src="img/B09699_02_05.jpg" alt="Introducing PixieDust" width="1000" height="398"/><div><p>Installing the PixieDust library on a Jupyter Notebook</p></div></div><div><div><h3 class="title"><a id="tip03"/>Tip</h3><p>I strongly recommend using Anaconda (<a class="ulink" href="https://anaconda.org">https://anaconda.org</a>), which provides excellent Python <a id="id74" class="indexterm"/>package management capabilities. If, like me, you like to experiment with different versions of Python and libraries dependencies, I suggest you use Anaconda virtual environments.</p><p>They are lightweight Python sandboxes that are very easy to create and activate (see <a class="ulink" href="https://conda.io/docs/user-guide/tasks/manage-environments.html">https://conda.io/docs/user-guide/tasks/manage-environments.html</a>):</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Create a new environment: <code class="literal">conda create --name env_name</code></li><li class="listitem" style="list-style-type: disc">List all environments: <code class="literal">conda env list</code></li><li class="listitem" style="list-style-type: disc">Activate an environment: <code class="literal">source activate env_name</code></li></ul></div><p>I also recommend that, optionally, you get familiar with the source code, which is available at <a class="ulink" href="https://github.com/ibm-watson-data-lab/pixiedust">https://github.com/ibm-watson-data-lab/pixiedust</a> and <a class="ulink" href="https://github.com/ibm-watson-data-lab/pixiegateway">https://github.com/ibm-watson-data-lab/pixiegateway</a>.</p></div></div><p>We are now ready to <a id="id75" class="indexterm"/>explore the PixieDust APIs starting with <code class="literal">sampleData()</code> in the next section.</p></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec27"/>SampleData – a simple API for loading data</h1></div></div></div><p>Loading data into a Notebook <a id="id76" class="indexterm"/>is one of the most <a id="id77" class="indexterm"/>repetitive tasks a data scientist can do, yet depending on the framework or data source being used, writing the code can be difficult and time-consuming.</p><p>Let's take a concrete example of trying to load a CSV file from an open data site (say <a class="ulink" href="https://data.cityofnewyork.us">https://data.cityofnewyork.us</a>) into both a <a id="id78" class="indexterm"/>pandas and Apache Spark DataFrame.</p><div><div><h3 class="title"><a id="note11"/>Note</h3><p>
<strong>Note</strong>: Going forward, all the code is assumed to run in a Jupyter Notebook.</p></div></div><p>For pandas, the code is pretty straightforward as it provides an API to directly load from URL:</p><div><pre class="programlisting">import pandas
data_url = "https://data.cityofnewyork.us/api/views/e98g-f8hy/rows.csv?accessType=DOWNLOAD"
building_df = pandas.read_csv(data_url)
building_df</pre></div><p>The last statement, calling <code class="literal">building_df,</code> will print its contents in the output cell. This is possible without a print because Jupyter is interpreting the last statement of a cell calling a variable as a directive to print it:</p><div><img src="img/B09699_02_06.jpg" alt="SampleData – a simple API for loading data" width="1000" height="348"/><div><p>The default output of a pandas DataFrame</p></div></div><p>However, for Apache Spark, we need to first download the data into a file then use the Spark CSV connector to load it into a DataFrame:</p><div><pre class="programlisting">#Spark CSV Loading
from pyspark.sql import SparkSession
try:
    from urllib import urlretrieve
except ImportError:
    #urlretrieve package has been refactored in Python 3
    from urllib.request import urlretrieve

data_url = "https://data.cityofnewyork.us/api/views/e98g-f8hy/rows.csv?accessType=DOWNLOAD"
urlretrieve (data_url, "building.csv")

spark = SparkSession.builder.getOrCreate()
building_df = spark.read\
  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\
  .option('header', True)\
  .load("building.csv")
building_df</pre></div><p>The output is slightly different since <code class="literal">building_df</code> is now a Spark DataFrame:</p><div><img src="img/B09699_02_07.jpg" alt="SampleData – a simple API for loading data" width="1000" height="65"/><div><p>Default output of a Spark DataFrame</p></div></div><p>Even though this code is not that big, it has to be repeated every time and, most likely, will require spending the time to do a Google search to remember the correct syntax. The data may also be in a different format, for example, JSON, which will require calling different APIs both for pandas <a id="id79" class="indexterm"/>and Spark. The data may also not <a id="id80" class="indexterm"/>be well-formed and can contain a bad line in a CSV file or have a wrong JSON syntax. All these issues are unfortunately not rare and contribute to the 80/20 rule of data science, which states that data scientists spends on average 80% of their time acquiring, cleaning, and loading data and only 20% doing the actual analysis.</p><p>PixieDust provides a simple <code class="literal">sampleData</code> API to help improve the situation. When called with no parameters, it displays a list of pre-curated datasets ready for analysis:</p><div><pre class="programlisting">import pixiedust
pixiedust.sampleData()</pre></div><p>The results are shown as follows:</p><div><img src="img/B09699_02_08.jpg" alt="SampleData – a simple API for loading data" width="1000" height="354"/><div><p>PixieDust built-in datasets</p></div></div><p>The list of prebuilt curated datasets can be customized to fit the organization, which is a good step toward our <em>data</em> pillar, as described in the previous chapter.</p><p>The user can then simply call the <code class="literal">sampleData</code> API again with the ID of the prebuilt dataset and get a Spark DataFrame if the Spark framework in the Jupyter Kernel is available or fall back to a pandas DataFrame if not.</p><p>In the following example, we call <code class="literal">sampleData()</code> on a Notebook connected with Spark. We also call <code class="literal">enableSparkJobProgressMonitor()</code> to display real-time information about the Spark jobs involved in the operation.</p><div><div><h3 class="title"><a id="note12"/>Note</h3><p>
<strong>Note</strong>: Spark jobs are processes that run on a particular node in the Spark cluster with a specific subset of the data. In the case of loading a large amount data from a data source, each Spark job is given a specific subset to work on (the actual size depends on the number of nodes in the cluster and the size of the overall data), running in parallel with the other jobs.</p></div></div><p>In a separate cell, we run the following code to enable the Spark Job Progress Monitor:</p><div><pre class="programlisting">pixiedust.enableSparkJobProgressMonitor()</pre></div><p>The results are as follows:</p><div><pre class="programlisting">Successfully enabled Spark Job Progress Monitor</pre></div><p>We then invoke <code class="literal">sampleData</code> to load the <code class="literal">cars</code> dataset:</p><div><pre class="programlisting">cars = pixiedust.sampleData(1)</pre></div><p>The results are shown as follows:</p><div><img src="img/B09699_02_09.jpg" alt="SampleData – a simple API for loading data" width="1000" height="268"/><div><p>Loading a built-in dataset with PixieDust sampleData API</p></div></div><p>The user can also pass an arbitrary URL that points to a downloadable file; PixieDust currently supports JSON and CSV files. In this case, PixieDust will automatically download the file, cache <a id="id81" class="indexterm"/>it in a temporary area, detect the format, and load it into a Spark or pandas DataFrame depending on whether Spark is <a id="id82" class="indexterm"/>available in the Notebook. Note that the user can also force loading into pandas even if Spark is available using the <code class="literal">forcePandas</code> keyword argument:</p><div><pre class="programlisting">import pixiedust
data_url = "https://data.cityofnewyork.us/api/views/e98g-f8hy/rows.csv?accessType=DOWNLOAD"
building_dataframe = pixiedust.sampleData(data_url, forcePandas=True)</pre></div><p>The results are as follows:</p><div><pre class="programlisting">Downloading 'https://data.cityofnewyork.us/api/views/e98g-f8hy/rows.csv?accessType=DOWNLOAD' from https://data.cityofnewyork.us/api/views/e98g-f8hy/rows.csv?accessType=DOWNLOAD
Downloaded 13672351 bytes
Creating pandas DataFrame for 'https://data.cityofnewyork.us/api/views/e98g-f8hy/rows.csv?accessType=DOWNLOAD'. Please wait...
Loading file using 'pandas'
Successfully created pandas DataFrame for 'https://data.cityofnewyork.us/api/views/e98g-f8hy/rows.csv?accessType=DOWNLOAD'</pre></div><p>The <code class="literal">sampleData()</code> API is also smart enough to recognize URLs that point to compressed files of the ZIP and GZ types. In this case, it will automatically unpack the raw binary data and load the file included in the archive. For ZIP files, it looks at the first file in the archive and, for GZ <a id="id83" class="indexterm"/>files, it simply decompresses the content as GZ files are <a id="id84" class="indexterm"/>not archives and do not contain multiple files. The <code class="literal">sampleData()</code> API will then load the DataFrame from the decompressed file.</p><p>For example, we can directly load borough information from a ZIP file provided by the London open data website and display the results as a pie chart using the <code class="literal">display()</code> API, as follows:</p><div><pre class="programlisting">import pixiedust
london_info = pixiedust.sampleData("https://files.datapress.com/london/dataset/london-borough-profiles/2015-09-24T15:50:01/London-borough-profiles.zip")</pre></div><p>The results are as follows (assuming that your Notebook is connected to Spark, otherwise a pandas DataFrame will be loaded):</p><div><pre class="programlisting">Downloading 'https://files.datapress.com/london/dataset/london-borough-profiles/2015-09-24T15:50:01/London-borough-profiles.zip' from https://files.datapress.com/london/dataset/london-borough-profiles/2015-09-24T15:50:01/London-borough-profiles.zip
Extracting first item in zip file...
File extracted: london-borough-profiles.csv
Downloaded 948147 bytes
Creating pySpark DataFrame for 'https://files.datapress.com/london/dataset/london-borough-profiles/2015-09-24T15:50:01/London-borough-profiles.zip'. Please wait...
Loading file using 'com.databricks.spark.csv'
Successfully created pySpark DataFrame for 'https://files.datapress.com/london/dataset/london-borough-profiles/2015-09-24T15:50:01/London-borough-profiles.zip'</pre></div><p>We can then call <code class="literal">display()</code> on the <code class="literal">london_info</code> DataFrame, as shown here:</p><div><pre class="programlisting">display(london_info)</pre></div><p>We select <strong>Pie Chart</strong> in the Chart menu and in the <strong>Options</strong> dialog, we drag and drop the <code class="literal">Area name</code> column in the <strong>Keys</strong> area and the <code class="literal">Crime rates per thousand population 2014/15</code> in the <strong>Values</strong> area, as shown in the following screenshot:</p><div><img src="img/B09699_02_10.jpg" alt="SampleData – a simple API for loading data" width="451" height="484"/><div><p>Chart options for visualizing the london_info DataFrame</p></div></div><p>After clicking on the <strong>OK</strong> button in the <strong>Options</strong> dialog, we get the following results:</p><div><img src="img/B09699_02_11.jpg" alt="SampleData – a simple API for loading data" width="1000" height="603"/><div><p>Pie chart created from a URL pointing at a compressed file</p></div></div><p>Many times, you have found a great dataset, but the file contains errors or the data that's important to you is in the <a id="id85" class="indexterm"/>wrong format or <a id="id86" class="indexterm"/>buried in some unstructured text that needs to be extracted into its own column. This process is also known as <strong>data wrangling</strong> and can be very time-consuming. In the next section, we will <a id="id87" class="indexterm"/>look at an extension to PixieDust called <code class="literal">pixiedust_rosie</code> that provides a <code class="literal">wrangle_data</code> method, which helps with this process.</p></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec28"/>Wrangling data with pixiedust_rosie</h1></div></div></div><p>Working in a <a id="id88" class="indexterm"/>controlled experiment is, most of the time, not the same <a id="id89" class="indexterm"/>as working in the real world. By this I mean that, during development, we usually pick (or I should say manufacture) a sample dataset that is designed to behave; it has the right format, it complies with the schema specification, no data is missing, and so on. The goal is to focus on verifying the hypotheses and build the algorithms, and not so much on data cleansing, which can be very painful and time-consuming. However, there is an undeniable benefit to get data that is as close to the real thing as early as possible in the development process. To help with this task, I worked with two IBM colleagues, Jamie Jennings and Terry Antony, who volunteered to build <a id="id90" class="indexterm"/>an extension to PixieDust called <code class="literal">pixiedust_rosie</code>. </p><p>This Python package implements a simple <code class="literal">wrangle_data()</code> method <a id="id91" class="indexterm"/>to automate the cleansing of raw data. The <code class="literal">pixiedust_rosie</code> package currently supports CSV and JSON, but more formats will be added in the future. The underlying data processing engine uses the <strong>Rosie Pattern Language</strong> (<strong>RPL</strong>) open <a id="id92" class="indexterm"/>source component, which is a regular expressions engine designed to be simpler to use for developers, more performant, and scalable to big data. You can find more information about Rosie here: <a class="ulink" href="http://rosie-lang.org">http://rosie-lang.org</a>.</p><p>To get started, you need to install the <code class="literal">pixiedust_rosie</code> package using the following command:</p><div><pre class="programlisting">
<strong>!pip install pixiedust_rosie</strong>
</pre></div><p>The <code class="literal">pixiedust_rosie</code> package has a dependency on <code class="literal">pixiedust</code> and <code class="literal">rosie,</code> which will be automatically downloaded if not already installed on the system.</p><p>The <code class="literal">wrangle_data()</code> method is very similar to the <code class="literal">sampleData()</code> API. When called with no parameters, it will show you the list of pre-curated datasets, as shown here:</p><div><pre class="programlisting">import pixiedust_rosie
pixiedust_rosie.wrangle_data()</pre></div><p>This produces the following results:</p><div><img src="img/B09699_02_12.jpg" alt="Wrangling data with pixiedust_rosie" width="1000" height="474"/><div><p>List of pre-curated datasets available for wrangle_data()</p></div></div><p>You can also invoke it with the ID of a pre-curated dataset or a URL link, for example, as follows:</p><div><pre class="programlisting">url = "https://github.com/ibm-watson-data-lab/pixiedust_rosie/raw/master/sample-data/Healthcare_Cost_and_Utilization_Project__HCUP__-_National_Inpatient_Sample.csv"
pixiedust_rosie.wrangle_data(url)</pre></div><p>In the preceding code, we invoke <code class="literal">wrangle_data()</code> on a CSV file referenced by the <code class="literal">url</code> variable. The function starts by downloading the file in the local filesystem and performs an automated data <a id="id93" class="indexterm"/>classification on a subset of the data, to infer the <a id="id94" class="indexterm"/>data schema. A schema editor PixieApp is then launched, which provides a set of wizard screens to let the user configure the schema. For example, the user will be able to drop and rename columns and, more importantly, destructure existing columns into new columns by providing Rosie patterns.</p><p>The workflow is illustrated in the following diagram:</p><div><img src="img/B09699_02_13.jpg" alt="Wrangling data with pixiedust_rosie" width="1000" height="463"/><div><p>wrangle_data() workflow</p></div></div><p>The first screen of the <code class="literal">wrangle_data()</code> wizard shows the schema that has been inferred by the Rosie data classifier as shown in the following screenshot:</p><div><img src="img/B09699_02_14.jpg" alt="Wrangling data with pixiedust_rosie" width="1000" height="823"/><div><p>The wrangle_data() schema editor</p></div></div><p>The preceding schema widget shows the column names, <code class="literal">Rosie Type</code> (advanced type representation specific to Rosie) and <code class="literal">Column Type</code> (map to the supported pandas types). Each row also contains three action buttons:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Delete column</strong>: This removes the columns from the schema. This column will not appear in the final pandas DataFrame.</li><li class="listitem" style="list-style-type: disc"><strong>Rename column</strong>: This changes the name of the column.</li><li class="listitem" style="list-style-type: disc"><strong>Transform column</strong>: This transforms a column by destructuring it into new columns.</li></ul></div><p>At any time, the user is able to preview the data (shown in the preceding SampleData widget) to validate that the schema configuration is behaving as intended.</p><p>When the user clicks on the transform column button, a new screen is shown that lets the user specify patterns for building new columns. In some cases, the data classifier will be able to automatically <a id="id95" class="indexterm"/>detect the patterns, in which case, a button will be <a id="id96" class="indexterm"/>added to ask the user whether the suggestions should be applied.</p><p>The following screenshot shows the <strong>Transform Selected Column</strong> screen with automated suggestions:</p><div><img src="img/B09699_02_15.jpg" alt="Wrangling data with pixiedust_rosie" width="1000" height="849"/><div><p>Transform column screen</p></div></div><p>This screen shows four widgets with the following information:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Rosie Pattern input is where you can enter a custom Rosie Pattern that represents the data for this column. You then use the <strong>Extract Variables</strong> button to tell the schema editor which part of the pattern should be extracted into a new column (more on that is explained soon).</li><li class="listitem" style="list-style-type: disc">There's a help widget <a id="id97" class="indexterm"/>that provides a link to the RPL documentation.</li><li class="listitem" style="list-style-type: disc">There's a preview <a id="id98" class="indexterm"/>of the data for the current column.</li><li class="listitem" style="list-style-type: disc">There's a preview of the data with the Rosie Pattern applied.</li></ul></div><p>When the user clicks on the <strong>Extract Variables</strong> button, the widget is updated as follow:</p><div><img src="img/B09699_02_16.jpg" alt="Wrangling data with pixiedust_rosie" width="846" height="450"/><div><p>Extracting Rosie variables into columns</p></div></div><p>At this point, the user has the option to edit the definition and then click on the <strong>Create Columns</strong> button to add the new columns to the schema. The <strong>Sample of New Column(s)</strong> widget is then updated to show a preview of what the data would look like. An error is shown in this widget if the pattern definition contains bad syntax:</p><div><img src="img/B09699_02_17.jpg" alt="Wrangling data with pixiedust_rosie" width="846" height="814"/><div><p>Preview of new columns after applying pattern definitions</p></div></div><p>When the user clicks on the <strong>Commit Columns</strong> button, the main schema editor screen is displayed again with the new columns added, as shown in the following screenshot:</p><div><img src="img/B09699_02_18.jpg" alt="Wrangling data with pixiedust_rosie" width="1000" height="933"/><div><p>Schema editor with new columns</p></div></div><p>The final step is to click on the <strong>Finish</strong> button to apply the schema definition to the raw file and create a <a id="id99" class="indexterm"/>pandas DataFrame that will be available as a variable <a id="id100" class="indexterm"/>in the Notebook. At this point, the user is presented with a dialog box that contains a default variable name that can be edited, as shown in the following screenshot:</p><div><img src="img/B09699_02_19.jpg" alt="Wrangling data with pixiedust_rosie" width="756" height="260"/><div><p>Edit the variable name for the Result Pandas DataFrame</p></div></div><p>After clicking on the <strong>Finish</strong> button, <code class="literal">pixiedust_rosie</code> goes over the entire dataset, applying the schema definition. When done, it creates a new cell just below the current one with a <a id="id101" class="indexterm"/>generated code that invokes the <code class="literal">display()</code> API on the <a id="id102" class="indexterm"/>newly generated pandas DataFrame, as shown here:</p><div><pre class="programlisting">#Code generated by pixiedust_rosie
display(wrangled_df)</pre></div><p>Running the preceding cell will let you explore and visualize the new dataset.</p><p>The <code class="literal">wrangle_data()</code> capability we've explored in this section is a first step toward helping data scientists spend less time cleaning the data and more time analyzing it. In the next section, we will discuss how to help data scientists with data exploration and visualization.</p></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec29"/>Display – a simple interactive API for data visualization</h1></div></div></div><p>Data visualization is another <a id="id103" class="indexterm"/>very important data science task that is indispensable for exploring and <a id="id104" class="indexterm"/>forming <a id="id105" class="indexterm"/>hypotheses. Fortunately, the Python <a id="id106" class="indexterm"/>ecosystem has <a id="id107" class="indexterm"/>a lot of powerful libraries <a id="id108" class="indexterm"/>dedicated to data visualization, such as these popular examples:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Matplotlib: <a class="ulink" href="http://matplotlib.org">http://matplotlib.org</a></li><li class="listitem" style="list-style-type: disc">Seaborn: <a class="ulink" href="https://seaborn.pydata.org">https://seaborn.pydata.org</a></li><li class="listitem" style="list-style-type: disc">Bokeh: <a class="ulink" href="http://bokeh.pydata.org">http://bokeh.pydata.org</a></li><li class="listitem" style="list-style-type: disc">Brunel: <a class="ulink" href="https://brunelvis.org">https://brunelvis.org</a></li></ul></div><p>However, similar to data loading and cleaning, using these libraries in a Notebook can be difficult and time-consuming. Each of these libraries come with their own programming model and APIs are not always easy to learn and use, especially if you are not an experienced developer. Another issue is that these libraries do not have a high-level interface to commonly used data processing frameworks such as pandas (except maybe Matplotlib) or Apache Spark and, as a result, a lot of data preparation is needed before plotting the data.</p><p>To help with this problem, PixieDust provides a simple <code class="literal">display()</code> API that enables Jupyter Notebook users to plot data using an interactive graphical interface and without any required coding. This API <a id="id109" class="indexterm"/>doesn't actually create charts but does all the heavy lifting of preparing the data before delegating to a renderer by calling its APIs according to the user selection.</p><p>The <code class="literal">display()</code> API supports multiple data structures (pandas, Spark, and JSON) as well as multiple renderers (Matplotlib, Seaborn, Bokeh, and Brunel).</p><p>As an illustration, let's use the built-in car performance dataset and start visualizing the data by calling the <code class="literal">display()</code> API:</p><div><pre class="programlisting">import pixiedust
cars = pixiedust.sampleData(1, forcePandas=True) #car performance data
display(cars)</pre></div><p>The first time the command is called on the cell, a tabular view is displayed and, as the user navigates through the menus, selected options are stored in the cell metadata as JSON so they can be used again the next time the cell is running. The output layout for all the visualizations follows the same pattern:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There's an extensible top-level menu for switching between charts.</li><li class="listitem" style="list-style-type: disc">There's a download menu for downloading the file in the local machine.</li><li class="listitem" style="list-style-type: disc">There's a filter toggle button that lets users refine their exploration by filtering the data. We'll discuss the filter capability in the <em>Filtering</em> section.</li><li class="listitem" style="list-style-type: disc">There's a Expand/Collapse Pixiedust Output button for collapsing/expanding the output content.</li><li class="listitem" style="list-style-type: disc">There's an <strong>Options</strong> button that invokes a dialog box with configurations specific to the current visualization.</li><li class="listitem" style="list-style-type: disc">There's a <strong>Share</strong> button that lets you publish the visualization on the web.<div><div><h3 class="title"><a id="note13"/>Note</h3><p>
<strong>Note</strong>: This button can only be used if you have deployed a PixieGateway, which we'll discuss in detail in <a class="link" href="ch04.xhtml" title="Chapter 4. Publish your Data Analysis to the Web - the PixieApp Tool">Chapter 4</a>, <em>Publish your Data Analysis to the Web - the PixieApp Tool</em>.</p></div></div></li><li class="listitem" style="list-style-type: disc">There's a contextual set of options on the right-hand side of the visualization.</li><li class="listitem" style="list-style-type: disc">There's the main visualization area.</li></ul></div><div><img src="img/B09699_02_20.jpg" alt="Display – a simple interactive API for data visualization" width="1000" height="594"/><div><p>Visualization output layout for the table renderer</p></div></div><p>To start creating a chart, first <a id="id110" class="indexterm"/>select the appropriate type in the menu. Out of the box, PixieDust supports six types of charts: <strong>Bar Chart</strong>, <strong>Line Chart</strong>, <strong>Scatter Plot</strong>, <strong>Pie Chart</strong>, <strong>Map</strong>, and <strong>Histogram</strong>. As we'll see in <a class="link" href="ch05.xhtml" title="Chapter 5. Python and PixieDust Best Practices and Advanced Concepts">Chapter 5</a>, <em>Python and PixieDust Best Practices and Advanced Concepts</em>, PixieDust also provides APIs to let you customize these menus by adding new ones or adding options to existing ones:</p><div><img src="img/B09699_02_21.jpg" alt="Display – a simple interactive API for data visualization" width="261" height="322"/><div><p>PixieDust Charts menu</p></div></div><p>The first time a chart menu is called, an options dialog will be displayed to configure a set of basic configuration options, such as what to use for the <em>X</em> and <em>Y</em> axes, the type of aggregation, and many more. To <a id="id111" class="indexterm"/>save you time, the dialog will be prepopulated with the data schema that PixieDust automatically introspected from the DataFrame.</p><p>In the following example, we will create a bar chart showing the average mileage consumption by horsepower:</p><div><img src="img/B09699_02_22.jpg" alt="Display – a simple interactive API for data visualization" width="1000" height="869"/><div><p>Bar chart dialog options</p></div></div><p>Clicking <strong>OK</strong> will display the interactive interface in the cell output area:</p><div><img src="img/B09699_02_23.jpg" alt="Display – a simple interactive API for data visualization" width="1000" height="609"/><div><p>Bar chart visualization</p></div></div><p>The canvas shows the chart in the center area and some contextual options on the side relevant to the type of chart selected. For example, we can select the field <strong>origin</strong> in the <strong>Cluster By</strong> combobox to show a breakdown by country of origin:</p><div><img src="img/B09699_02_24.jpg" alt="Display – a simple interactive API for data visualization" width="1000" height="717"/><div><p>Clustered bar chart visualization</p></div></div><p>As mentioned before, PixieDust <code class="literal">display()</code> doesn't actually create the chart, rather it prepares the data based on the selected options and does the heavy lifting of calling the APIs of a renderer engine, with the correct parameters. The goal behind this design is for each chart type to support <a id="id112" class="indexterm"/>multiple renderers without any extra coding, providing as much freedom of exploration to the user as possible.</p><p>Out of the box, PixieDust supports the following renderers provided that the corresponding libraries are installed. For those that are not installed, a warning will be generated in the PixieDust log and the corresponding renderer will not be displayed in the menu. We'll cover in detail the PixieDust log in <a class="link" href="ch05.xhtml" title="Chapter 5. Python and PixieDust Best Practices and Advanced Concepts">Chapter 5</a>, <em>Python and PixieDust Best Practices and Advanced Concepts</em>.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Matplotlib (<a class="ulink" href="https://matplotlib.org">https://matplotlib.org</a>)</li><li class="listitem" style="list-style-type: disc">Seaborn (<a class="ulink" href="https://seaborn.pydata.org">https://seaborn.pydata.org</a>)<div><div><h3 class="title"><a id="note14"/>Note</h3><p>This library needs to be installed using: <code class="literal">!pip install seaborn.</code>
</p></div></div></li><li class="listitem" style="list-style-type: disc">Bokeh (<a class="ulink" href="https://bokeh.pydata.org">https://bokeh.pydata.org</a>)<div><div><h3 class="title"><a id="note15"/>Note</h3><p>This library needs to be installed using: <code class="literal">!pip install bokeh.</code>
</p></div></div></li><li class="listitem" style="list-style-type: disc">Brunel (<a class="ulink" href="https://brunelvis.org">https://brunelvis.org</a>)<div><div><h3 class="title"><a id="note16"/>Note</h3><p>This library needs to be installed using: <code class="literal">!pip install brunel.</code>
</p></div></div></li><li class="listitem" style="list-style-type: disc">Google Map (<a class="ulink" href="https://developers.google.com/maps">https://developers.google.com/maps</a>)</li><li class="listitem" style="list-style-type: disc">Mapbox (<a class="ulink" href="https://www.mapbox.com">https://www.mapbox.com</a>)<div><div><h3 class="title"><a id="note17"/>Note</h3><p>
<strong>Note</strong>: Google Map and Mapbox require an API key that you can obtain on their respective sites.</p></div></div></li></ul></div><p>You can switch <a id="id113" class="indexterm"/>between renderers using the <strong>Renderer</strong> combobox. For example, if we want more interactivity to explore the chart (such as zooming <a id="id114" class="indexterm"/>and panning), we can use the Bokeh renderer instead of Matplotlib, which gives us only a static image:</p><div><img src="img/B09699_02_25.jpg" alt="Display – a simple interactive API for data visualization" width="1000" height="518"/><div><p>Cluster bar chart using the Bokeh renderer</p></div></div><p>Another chart type worth mentioning is Map, which is interesting when your data contains geospatial information, such as <a id="id115" class="indexterm"/>longitude, latitude, or country/state information. PixieDust supports multiple types of geo-mapping rendering engines including the popular Mapbox engine.</p><div><div><h3 class="title"><a id="note18"/>Note</h3><p>Before using the Mapbox renderer, it is recommended to get an API key from the Mapbox site at this location: (<a class="ulink" href="https://www.mapbox.com/help/how-access-tokens-work">https://www.mapbox.com/help/how-access-tokens-work</a>). However, if you don't <a id="id116" class="indexterm"/>have one, a default key will be provided by PixieDust.</p></div></div><p>To create a Map chart, let's use the <em>Million-dollar home sales in NE Mass</em> dataset, as follows:</p><div><pre class="programlisting">import pixiedust
homes = pixiedust.sampleData(6, forcePandas=True) #Million dollar home sales in NE Mass
display(homes)</pre></div><p>First, select <strong>Map</strong> in the chart drop-down button, then in the options dialog, select <code class="literal">LONGITUDE</code> and <code class="literal">LATITUDE</code> as the keys and enter the Mapbox access token in the provided input. You can add multiples fields in the <strong>Values</strong> area, and they will be displayed as tooltips on the map:</p><div><img src="img/B09699_02_26.jpg" alt="Display – a simple interactive API for data visualization" width="480" height="506"/><div><p>Options dialog for Mapbox charts</p></div></div><p>When clicking the <strong>OK</strong> button, you'll <a id="id117" class="indexterm"/>get an interactive map that you can customize using the style (simple, choropleth, or density map), color, and basemap (light, satellite, dark, and outdoors) options:</p><div><img src="img/B09699_02_27.jpg" alt="Display – a simple interactive API for data visualization" width="1000" height="543"/><div><p>Interactive Mapbox visualization</p></div></div><p>Each chart type has its own set of contextual options, which are self-explanatory, and I encourage you at this point to play with each and every one of them. If you find issues or have enhancement ideas, you can always create a new issue on GitHub at <a class="ulink" href="https://github.com/ibm-watson-data-lab/pixiedust/issues">https://github.com/ibm-watson-data-lab/pixiedust/issues</a> or, better yet, submit a pull request with your code changes (there's more information on how to do that here: <a class="ulink" href="https://help.github.com/articles/creating-a-pull-request">https://help.github.com/articles/creating-a-pull-request</a>).</p><p>To avoid reconfiguring the chart every time the cell runs, PixieDust stores the chart options as a JSON object <a id="id118" class="indexterm"/>in the cell metadata, which is eventually saved in the Notebook. You can manually inspect this data by selecting the <strong>View</strong> | <strong>Cell Toolbar</strong> | <strong>Edit Metadata</strong> menu, as shown in the following screenshot:</p><div><img src="img/B09699_02_28.jpg" alt="Display – a simple interactive API for data visualization" width="427" height="245"/><div><p>Show Edit Metadata button</p></div></div><p>An <strong>Edit Metadata</strong> button will be shown at the top of the cell, which, when clicked on, displays the PixieDust configuration:</p><div><img src="img/B09699_02_29.jpg" alt="Display – a simple interactive API for data visualization" width="1000" height="740"/><div><p>Edit Cell Metadata dialog</p></div></div><p>This JSON configuration will be important when we discuss PixieApps in the next section.</p></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec30"/>Filtering</h1></div></div></div><p>To better explore data, PixieDust <a id="id119" class="indexterm"/>also provides a built-in, simple graphical interface that lets you quickly filter the data being visualized. You can quickly invoke the filter by clicking on the filter toggle button in the top-level menu. To keep things simple, the filter only supports building predicates based on one column only, which is sufficient in most cases to validate simple hypotheses (based on feedback, this feature may be enhanced in the future to support multiple predicates). The filter UI will automatically let you select the column to filter on and, based on its type, will show different options:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Numerical type</strong>: The user can select a mathematical comparator and enter a value for the operand. For <a id="id120" class="indexterm"/>convenience, the UI will also show statistical values related to the chosen column, which can be used when picking the operand value:<div><img src="img/B09699_02_30.jpg" alt="Filtering" width="1000" height="325"/><div><p>Filter on the mpg numerical column of the cars data set</p></div></div></li><li class="listitem" style="list-style-type: disc"><strong>String type</strong>: The user can enter an expression to match the column value, which can be either a regular <a id="id121" class="indexterm"/>expression or a plain string. For convenience, the UI also shows basic help on how to build a regular expression:</li></ul></div><div><img src="img/B09699_02_31.jpg" alt="Filtering" width="1000" height="384"/><div><p>Filter on the name String type of the cars dataset</p></div></div><p>When clicking on the <strong>Apply</strong> button, the current visualization is updated to reflect the filter configuration. It is important to note <a id="id122" class="indexterm"/>that the filter applies to the whole cell and not only to the current visualization. Therefore, it will continue to apply when switching between chart types. The filter configuration is also saved in the cell metadata, so it will be preserved when saving the Notebook and rerunning the cell.</p><p>For example, the following screenshot visualizes the <code class="literal">cars</code> dataset as a bar chart showing only the rows with <code class="literal">mpg</code> greater than <code class="literal">23,</code> which, according to the statistics box, is the mean for the dataset, and clustered by years. In the options dialog, we select the <code class="literal">mpg</code> column as the key and <code class="literal">origin</code> as the value:</p><div><img src="img/B09699_02_32.jpg" alt="Filtering" width="1000" height="761"/><div><p>Filtered bar chart for the cars dataset</p></div></div><p>To summarize, in this section, we've discussed how PixieDust can help with three difficult and time-consuming data <a id="id123" class="indexterm"/>science tasks: data loading, data wrangling, and data visualization. Next, we are going to see how PixieDust can help increase collaboration between data scientists and developers.</p></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec31"/>Bridging the gap between developers and data scientists with PixieApps</h1></div></div></div><p>Solving hard data problems <a id="id124" class="indexterm"/>is only part of the mission given to data science teams. They also need to make sure that data science results get properly operationalized to deliver business value to the organization. Operationalizing data analytics is very much use case - dependent. It could mean, for example, creating a dashboard that synthesizes insights for decision makers or integrating a machine learning model, such as a recommendation engine, into a web application.</p><p>In most cases, this is where data science meets software engineering (or as some would say, <em>where the rubber meets the road</em>). Sustained collaboration between the teams—instead of a one-time handoff—is key to a successful completion of the task. More often than not, they also have to grapple with different languages and platforms, leading to significant code rewrites by the software engineering team.</p><p>We experienced it firsthand in our <em>Sentiment analysis of Twitter hashtags</em> project when we needed to build a real-time dashboard to visualize the results. The data analytics was written in Python <a id="id125" class="indexterm"/>using pandas, Apache Spark, and a few plotting libraries such as Matplotlib and Bokeh, while the dashboard <a id="id126" class="indexterm"/>was written in Node.js (<a class="ulink" href="https://nodejs.org">https://nodejs.org</a>) and D3 (<a class="ulink" href="https://d3js.org">https://d3js.org</a>).</p><p>We also needed to build a data interface between the analytics and the dashboard and, since we needed the system <a id="id127" class="indexterm"/>to be real-time, we chose to use Apache Kafka to stream events formatted with the analytics results.</p><p>The following diagram generalizes an approach <a id="id128" class="indexterm"/>that I call the <strong>hand-off pattern</strong> where the data science team builds the analytics and deploys the results in a data interface layer. The results are then consumed by the application. The data layer is usually handled by the data engineer, which is one of the roles we discussed in <a class="link" href="ch01.xhtml" title="Chapter 1. Programming and Data Science – A New Toolset">Chapter 1</a>, <em>Programming and Data Science – A New Toolset</em>:</p><div><img src="img/B09699_02_33.jpg" alt="Bridging the gap between developers and data scientists with PixieApps" width="562" height="348"/><div><p>Hand-off between data science and engineering</p></div></div><p>The problem with this hand-off pattern is that it is not conducive to rapid iteration. Any changes in the data layer need to be synchronized with the software engineering team to avoid breaking the application. The idea behind PixieApps is to build the application while staying as close as possible to the data science environment, which is, in our case, the Jupyter Notebook. With this approach, the analytics are directly called from the PixieApp, which runs embedded in the Jupyter Notebook, hence making it easy for data scientists and developers to collaborate and iterate to make rapid improvements.</p><p>PixieApp defines a simple programming model for building single-page applications with direct access to the IPython Notebook Kernel (which is the Python backend process running the Notebook code). In essence, a PixieApp is a Python class that encapsulates both the presentation and <a id="id129" class="indexterm"/>business logic. The presentation is composed of a set of special methods called routes that return an arbitrary HTML fragment. Each PixieApp has a default route that returns the HTML fragment for the starting page. Developers can use custom HTML attributes to invoke other routes and dynamically update all or part of the page. A route may, for example, invoke a machine learning algorithm created from within the Notebook or generate a chart using the PixieDust display framework.</p><p>The following diagram shows the high-level architecture of how PixieApps interact with the Jupyter Notebook client frontend and the IPython Kernel:</p><div><img src="img/B09699_02_34.jpg" alt="Bridging the gap between developers and data scientists with PixieApps" width="1000" height="364"/><div><p>PixieApp interaction with the Jupyter Kernel</p></div></div><p>As a preview of what a PixieApp looks like, here's a <em>hello world</em> sample application that has one button showing a bar chart for the cars DataFrame we created in the previous section:</p><div><pre class="programlisting">#import the pixieapp decorators
from pixiedust.display.app import *

#Load the cars dataframe into the Notebook
cars = pixiedust.sampleData(1)

@PixieApp   #decorator for making the class a PixieApp
class HelloWorldApp():
    #decorator for making a method a
    #route (no arguments means default route)
    @route()
    def main_screen(self):
        return """
        &lt;button type="submit" pd_options="show_chart=true" pd_target="chart"&gt;Show Chart&lt;/button&gt;
        &lt;!--Placeholder div to display the chart--&gt;
        &lt;div id="chart"&gt;&lt;/div&gt;
        """
    
    @route(show_chart="true")
    def chart(self):
        #Return a div bound to the cars dataframe
        #using the pd_entity attribute
        #pd_entity can refer a class variable or
        #a global variable scoped to the notebook
        return """
        &lt;div pd_render_onload pd_entity="cars"&gt;
            &lt;pd_options&gt;
                {
                  "title": "Average Mileage by Horsepower",
                  "aggregation": "AVG",
                  "clusterby": "origin",
                  "handlerId": "barChart",
                  "valueFields": "mpg",
                  "rendererId": "bokeh",
                  "keyFields": "horsepower"
                }
            &lt;/pd_options&gt;
        &lt;/div&gt;
        """
#Instantiate the application and run it
app = HelloWorldApp()
app.run()</pre></div><p>When the preceding code runs in a Notebook cell, we get the following results:</p><div><img src="img/B09699_02_35.jpg" alt="Bridging the gap between developers and data scientists with PixieApps" width="605" height="345"/><div><p>Hello World PixieApp</p></div></div><p>You probably have a lot of <a id="id130" class="indexterm"/>questions about the preceding code, but don't worry. In the next chapters, we'll cover all the PixieApp technical details, including how to use them in end-to-end pipelines.</p></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec32"/>Architecture for operationalizing data science analytics</h1></div></div></div><p>In the previous section, we saw how PixieApps combined with the PixieDust display framework offer an easy way to build powerful dashboards that connect directly with your data analytics, allowing for rapid iterations between the algorithms and the user interface. This is great for rapid prototyping, but Notebooks are not suitable to be used in a production environment where the target persona is the line of business user. One obvious solution would be to rewrite the PixieApp using a <a id="id131" class="indexterm"/>traditional three tiers web application architecture, for example, as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">React (<a class="ulink" href="https://reactjs.org">https://reactjs.org</a>) for the presentation layer</li><li class="listitem" style="list-style-type: disc">Node.js for the web layer</li><li class="listitem" style="list-style-type: disc">A data access library targeted at the web analytics layer for machine learning scoring or running any other analytic jobs</li></ul></div><p>However, this would provide only a marginal improvement over the existing process, which would consist only, in this case, of the ability to do iterative implementation with the PixieApp.</p><p>A much better solution would be to directly deploy and run PixieApps as web applications, including the analytics in the surrounding Notebook and, while we're at it, without any code change.</p><p>Using this model, Jupyter Notebooks would become the central tool for a simplified development life cycle, as shown in the following diagram:</p><div><img src="img/B09699_02_36.jpg" alt="Architecture for operationalizing data science analytics" width="529" height="238"/><div><p>Data science pipeline development life cycle</p></div></div><div><ol class="orderedlist arabic"><li class="listitem">Data scientists use a Python Notebook to load, enrich, and analyze data and create analytics (machine learning models, statistics, and so on)</li><li class="listitem">From the same Notebook, developers <a id="id132" class="indexterm"/>create a PixieApp to operationalize these analytics</li><li class="listitem">Once ready, developers publish the PixieApp as a web application, where it can be easily consumed interactively by line-of-business users without the need to access Notebooks</li></ol></div><p>PixieDust provides an implementation of this solution with the PixieGateway component. PixieGateway is a web application server responsible for loading and running PixieApps. It is built on top of the Jupyter Kernel Gateway (<a class="ulink" href="https://github.com/jupyter/kernel_gateway">https://github.com/jupyter/kernel_gateway</a>), which itself is built on top of the Tornado web framework, and therefore follows an architecture as shown in the following diagram:</p><div><img src="img/B09699_02_37.jpg" alt="Architecture for operationalizing data science analytics" width="1000" height="577"/><div><p>PixieGateway architecture diagram</p></div></div><div><ol class="orderedlist arabic"><li class="listitem">The PixieApp is published into the PixieGateway server directly from the Notebook and a URL is generated. Behind the scene, PixieGateway allocates a Jupyter Kernel to run the PixieApp. Based on configuration, the PixieApp could share the kernel <a id="id133" class="indexterm"/>instance with other apps or have a dedicated kernel based on needs. The PixieGateway middleware can scale horizontally by managing the lifecycle of multiple kernels instances, which themselves can either be local to the server or remote on a cluster.<div><div><h3 class="title"><a id="note19"/>Note</h3><p>
<strong>Note</strong>: Remote kernels must be Jupyter Kernel Gateways.</p></div></div><p>Using the publishing wizard, the user can optionally define security for the application. Multiple options are available including Basic Authentication, OAuth 2.0, and Bearer Token.</p></li><li class="listitem">The line of business users accesses the app from their browser using the URL from step 1.</li><li class="listitem">PixieGateway provides a comprehensive admin console for managing the server including configuring the applications, configuring and monitoring kernels, access to the logs for troubleshooting, and so on.</li><li class="listitem">The PixieGateway manages <a id="id134" class="indexterm"/>sessions for each active user and dispatches requests to the appropriate kernels for execution using the IPython messaging protocol (<a class="ulink" href="http://jupyter-client.readthedocs.io/en/latest/messaging.html">http://jupyter-client.readthedocs.io/en/latest/messaging.html</a>) over WebSocket or ZeroMQ depending on whether the Kernel is local or remote.</li></ol></div><p>When productizing your analytics, this solution provides a major improvement over the classic three-tier web application architecture because it collapses the web and the data tier into one <strong>web analytics tiers,</strong> as shown in the following diagram:</p><div><img src="img/B09699_02_38.jpg" alt="Architecture for operationalizing data science analytics" width="510" height="307"/><div><p>Comparison between classic three tiers and PixieGateway web architecture</p></div></div><p>In the classic three-tier architecture, developers have to maintain multiple REST endpoints that invoke the analytics in the data tier and massage the data to comply with the presentation tier requirements <a id="id135" class="indexterm"/>for correctly displaying the data. As a result, a lot of engineering has to be added to these endpoints, increasing the cost of development and code maintenance. In contrast, in the PixieGateway two-tier architecture, developers do not have to worry about creating endpoints because the server is responsible for dispatching the requests to the appropriate kernel using built-in generic endpoints. Explained another way, the PixieApp Python methods automatically become endpoints for the presentation tier without any code change. This model is conducive to rapid iterations since any change in the Python code is directly reflected in the application after republishing.</p><p>PixieApps are great to rapidly build single-page applications and dashboards. However, you may also want to generate simpler one-page reports and share them with your users. To that end, PixieGateway <a id="id136" class="indexterm"/>also lets you share charts generated by the <code class="literal">display()</code> API using the <strong>Share</strong> button, resulting in a URL linking to a web page containing the chart. In turn, a user can embed the chart into a website or a blog post by copying and pasting the code generated for the page.</p><div><div><h3 class="title"><a id="note20"/>Note</h3><p>
<strong>Note</strong>: We'll cover PixieGateway in details in <a class="link" href="ch04.xhtml" title="Chapter 4. Publish your Data Analysis to the Web - the PixieApp Tool">Chapter 4</a>, <em>Publish your Data Analysis to the Web - the PixieApp Tool</em>, including how to install a new instance both locally and on the cloud.</p></div></div><p>To demonstrate this capability, let's use the cars DataFrame created earlier:</p><div><img src="img/B09699_02_39.jpg" alt="Architecture for operationalizing data science analytics" width="1000" height="626"/><div><p>Share Chart dialog</p></div></div><p>If sharing is successful, then the next page will show the generated URL and the code snippet to embed into a web application or blog post:</p><div><img src="img/B09699_02_40.jpg" alt="Architecture for operationalizing data science analytics" width="1000" height="568"/><div><p>Confirmation of a shared chart</p></div></div><p>Clicking on the link will take you to the page:</p><div><img src="img/B09699_02_41.jpg" alt="Architecture for operationalizing data science analytics" width="1000" height="566"/><div><p>Display chart as a web page</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec33"/>Summary</h1></div></div></div><p>In this chapter, we discussed the reasons why our data science tooling strategy was centered around Python and Jupyter Notebook. We also introduced the PixieDust capabilities that improve user productivity with features such as the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data loading and cleaning</li><li class="listitem" style="list-style-type: disc">Data visualization and exploration without any coding</li><li class="listitem" style="list-style-type: disc">A simple programming model based on HTML and CSS, called PixieApp, for building tools and dashboards that interact directly with the Notebook</li><li class="listitem" style="list-style-type: disc">A point and click mechanism to publish charts and PixieApp directly to the web</li></ul></div><p>In the next chapter, we'll do a deep dive on the PixieApp programming model, discussing every aspect of the APIs with numerous code samples.</p></div></div>



  </body></html>