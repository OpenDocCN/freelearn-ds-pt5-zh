<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">PySpark and SparkR</h1>
                
            
            
                
<p class="mce-root">In this chapter, we will discuss two other popular APIs: PySpark and SparkR for writing Spark code in Python and R programming languages respectively. The first part of this chapter will cover some technical aspects while working with Spark using PySpark. Then we will move to SparkR and see how to use it with ease. The following topics will be discussed throughout this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Introduction to PySpark</li>
<li class="mce-root1">Installation and getting started with PySpark</li>
<li class="mce-root1">Interacting with DataFrame APIs</li>
<li class="mce-root1">UDFs with PySpark</li>
<li class="mce-root1">Data analytics using PySpark</li>
<li class="mce-root1">Introduction to SparkR</li>
<li class="mce-root1">Why SparkR?</li>
<li class="mce-root1">Installation and getting started with SparkR</li>
<li class="mce-root1">Data processing and manipulation</li>
<li class="mce-root1">Working with RDD and DataFrame using SparkR</li>
<li class="mce-root1">Data visualization using SparkR</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Introduction to PySpark</h1>
                
            
            
                
<p class="mce-root">Python is one of the most popular and general purpose programming languages with a number of exciting features for data processing and machine learning tasks. To use Spark from Python, PySpark was initially developed as a lightweight frontend of Python to Apache Spark and using Spark's distributed computation engine. In this chapter, we will discuss a few technical aspects of using Spark from Python IDE such as PyCharm.</p>
<p class="mce-root">Many data scientists use Python because it has a rich variety of numerical libraries with a statistical, machine learning, or optimization focus. However, processing large-scale datasets in Python is usually tedious as the runtime is single-threaded. As a result, data that fits in the main memory can only be processed. Considering this limitation and for getting the full flavor of Spark in Python, PySpark was initially developed as a lightweight frontend of Python to Apache Spark and using Spark's distributed computation engine. This way, Spark provides APIs in non-JVM languages like Python.</p>
<p class="mce-root">The purpose of this PySpark section is to provide basic distributed algorithms using PySpark. Note that PySpark is an interactive shell for basic testing and debugging and is not supposed to be used for a production environment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Installation and configuration</h1>
                
            
            
                
<p class="mce-root">There are many ways of installing and configuring PySpark on Python IDEs such as PyCharm, Spider, and so on. Alternatively, you can use PySpark if you have already installed Spark and configured the <kbd class="calibre11">SPARK_HOME</kbd>. Thirdly, you can also use PySpark from the Python shell. Below we will see how to configure PySpark for running standalone jobs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">By setting SPARK_HOME</h1>
                
            
            
                
<p class="mce-root">At first, download and place the Spark distribution at your preferred place, say <kbd class="calibre11">/home/asif/Spark</kbd>. Now let's set the <kbd class="calibre11">SPARK_HOME</kbd> as follows:</p>
<pre class="calibre19">
<strong class="calibre1">echo "export SPARK_HOME=/home/asif/Spark" &gt;&gt; ~/.bashrc</strong>
</pre>
<p class="mce-root">Now let's set <kbd class="calibre11">PYTHONPATH</kbd> as follows:</p>
<pre class="calibre19">
<strong class="calibre1">echo "export PYTHONPATH=$SPARK_HOME/python/" &gt;&gt; ~/.bashrc</strong><br class="title-page-name"/><strong class="calibre1">echo "export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.1-src.zip" &gt;&gt; ~/.bashrc</strong>
</pre>
<p class="mce-root">Now we need to add the following two paths to the environmental path:</p>
<pre class="calibre19">
<strong class="calibre1">echo "export PATH=$PATH:$SPARK_HOME" &gt;&gt; ~/.bashrc</strong><br class="title-page-name"/><strong class="calibre1">echo "export PATH=$PATH:$PYTHONPATH" &gt;&gt; ~/.bashrc</strong>
</pre>
<p class="mce-root">Finally, let's refresh the current terminal so that the newly modified <kbd class="calibre11">PATH</kbd> variable is used:</p>
<pre class="calibre19">
<strong class="calibre1">source ~/.bashrc</strong>
</pre>
<p class="mce-root">PySpark depends on the <kbd class="calibre11">py4j</kbd> Python package. It helps the Python interpreter to dynamically access the Spark object from the JVM. This package can be installed on Ubuntu as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$ sudo pip install py4j</strong>
</pre>
<p class="mce-root">Alternatively, the default <kbd class="calibre11">py4j</kbd>, which is already included in Spark (<kbd class="calibre11">$SPARK_HOME/python/lib</kbd>), can be used too.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Using Python shell</h1>
                
            
            
                
<p class="mce-root">Like Scala interactive shell, an interactive shell is also available for Python. You can execute Python code from Spark root folder as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$ cd $SPARK_HOME</strong><br class="title-page-name"/><strong class="calibre1">$ ./bin/pyspark</strong>
</pre>
<p class="mce-root">If the command went fine, you should observer the following screen on Terminal (Ubuntu):</p>
<div><img class="image-border277" src="img/00375.jpeg"/></div>
<div><strong class="calibre1">Figure 1</strong>: Getting started with PySpark shell</div>
<p class="mce-root">Now you can enjoy Spark using the Python interactive shell. This shell might be sufficient for experimentations and developments. However, for production level, you should use a standalone application.</p>
<p class="mce-root">PySpark should be available in the system path by now. After writing the Python code, one can simply run the code using the Python command, then it runs in local Spark instance with default configurations:</p>
<pre class="calibre19">
<strong class="calibre1">$ python &lt;python_file.py&gt;</strong>
</pre>
<p class="mce-root">Note that the current distribution of Spark is only Python 2.7+ compatible. Hence, we will have been strict on this.</p>
<p class="mce-root">Furthermore, it is better to use the <kbd class="calibre11">spark-submit</kbd> script if you want to pass the configuration values at runtime. The command is pretty similar to the Scala one:</p>
<pre class="calibre19">
<strong class="calibre1">$ cd $SPARK_HOME</strong><br class="title-page-name"/><strong class="calibre1">$ ./bin/spark-submit  --master local[*] &lt;python_file.py&gt;</strong>
</pre>
<p class="mce-root">The configuration values can be passed at runtime, or alternatively, they can be changed in the <kbd class="calibre11">conf/spark-defaults.conf</kbd> file. After configuring the Spark config file, the changes also get reflected while running PySpark applications using a simple Python command.</p>
<p class="mce-root">However, unfortunately, at the time of this writing, there's no pip install advantage for using PySpark. But it is expected to be available in the Spark 2.2.0 release (for more, refer to <a href="https://issues.apache.org/jira/browse/SPARK-1267" class="calibre10">https://issues.apache.org/jira/browse/SPARK-1267</a>). The reason why there is no pip install for PySpark can be found in the JIRA ticket at <a href="https://issues.apache.org/jira/browse/SPARK-1267" class="calibre10">https://issues.apache.org/jira/browse/SPARK-1267</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">By setting PySpark on Python IDEs</h1>
                
            
            
                
<p class="mce-root">We can also configure and run PySpark from Python IDEs such as PyCharm. In this section, we will show how to do it. If you're a student, you can get the free licensed copy of PyCharm once you register using your university/college/institute email address at <a href="https://www.jetbrains.com/student/" class="calibre10">https://www.jetbrains.com/student/</a>. Moreover, there's also a community (that is, free) edition of PyCharm, so you don't need to be a student in order to use it.</p>
<p class="mce-root">Recently PySpark has been published with Spark 2.2.0 PyPI (see <a href="https://pypi.python.org/pypi/pyspark" target="_blank" class="calibre10">https://pypi.python.org/pypi/pyspark</a>/. This has been a long time coming (previous releases included pip installable artifacts that for a variety of reasons couldn't be published to PyPI). So if you (or your friends) want to be able to work with PySpark locally on your laptop you've got an easier path getting started, just execute the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ sudo pip install pyspark # for python 2.7 <br class="title-page-name"/>$ sudo pip3 install pyspark # for python 3.3+</strong>
</pre>
<p class="mce-root">However, if you are using Windos 7, 8 or 10, you should install pyspark manually. For exmple using PyCharm, you can do it as follows:</p>
<div><img class="image-border278" src="img/00281.jpeg"/></div>
<div><strong class="calibre1">Figure 2:</strong> Installing PySpark on Pycharm IDE on Windows 10</div>
<p class="mce-root">At first, you should create a Python script with Project interpreter as Python 2.7+. Then you can import pyspark along with other required models as follows:</p>
<pre class="calibre19">
import os<br class="title-page-name"/>import sys<br class="title-page-name"/>import pyspark
</pre>
<p class="mce-root">Now that if you're a Windows user, Python also needs to have the Hadoop runtime; you should put the <kbd class="calibre11">winutils.exe</kbd> file in the <kbd class="calibre11">SPARK_HOME/bin</kbd> folder. Then create a environmental variable as follows:</p>
<p class="mce-root">Select your python file | Run | Edit configuration | Create an environmental variable whose key is <kbd class="calibre11">HADOOP_HOME</kbd> and the value is the <kbd class="calibre11">PYTHON_PATH</kbd> for example for my case it's <kbd class="calibre11">C:\Users\admin-karim\Downloads\spark-2.1.0-bin-hadoop2.7</kbd>. Finally, press OK then you're done:</p>
<div><img class="image-border279" src="img/00110.jpeg"/></div>
<div><strong class="calibre1">Figure 3:</strong> Setting Hadoop runtime env on Pycharm IDE on Windows 10</div>
<p class="mce-root">That's all you need. Now if you start writing Spark code, you should at first place the imports in the <kbd class="calibre11">try</kbd> block as follows (just for example):</p>
<pre class="calibre19">
try: <br class="title-page-name"/>    from pyspark.ml.featureimport PCA<br class="title-page-name"/>    from pyspark.ml.linalgimport Vectors<br class="title-page-name"/>    from pyspark.sqlimport SparkSession<br class="title-page-name"/>    print ("Successfully imported Spark Modules")
</pre>
<p class="mce-root">And the <kbd class="calibre11">catch</kbd> block can be placed as follows:</p>
<pre class="calibre19">
ExceptImportErroras e: <br class="title-page-name"/>    print("Can not import Spark Modules", e)<br class="title-page-name"/>    sys.exit(1)
</pre>
<p class="mce-root">Refer to the following figure that shows importing and placing Spark packages in the PySpark shell:</p>
<div><img class="image-border280" src="img/00256.jpeg"/></div>
<div><strong class="calibre1">Figure 4</strong>: Importing and placing Spark packages in PySpark shell</div>
<p class="mce-root">If these blocks execute successfully, you should observe the following message on the console:</p>
<div><img class="image-border281" src="img/00005.jpeg"/></div>
<div><strong class="calibre1">Figure 5</strong>: PySpark package has been imported successfully</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Getting started with PySpark</h1>
                
            
            
                
<p class="mce-root">Before going deeper, at first, we need to see how to create the Spark session. It can be done as follows:</p>
<pre class="calibre19">
spark = SparkSession\<br class="title-page-name"/>         .builder\<br class="title-page-name"/>         .appName("PCAExample")\<br class="title-page-name"/>         .getOrCreate()
</pre>
<p class="mce-root">Now under this code block, you should place your codes, for example:</p>
<pre class="calibre19">
data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),<br class="title-page-name"/>         (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),<br class="title-page-name"/>         (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]<br class="title-page-name"/> df = spark.createDataFrame(data, ["features"])<br class="title-page-name"/><br class="title-page-name"/> pca = PCA(k=3, inputCol="features", outputCol="pcaFeatures")<br class="title-page-name"/> model = pca.fit(df)<br class="title-page-name"/><br class="title-page-name"/> result = model.transform(df).select("pcaFeatures")<br class="title-page-name"/> result.show(truncate=False)
</pre>
<p class="mce-root">The preceding code demonstrates how to compute principal components on a RowMatrix and use them to project the vectors into a low-dimensional space. For a clearer picture, refer to the following code that shows how to use the PCA algorithm on PySpark:</p>
<pre class="calibre19">
<strong class="calibre1">import </strong>os<br class="title-page-name"/><strong class="calibre1">import </strong>sys<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">try</strong>:<br class="title-page-name"/><strong class="calibre1">from </strong>pyspark.sql <strong class="calibre1">import </strong>SparkSession<br class="title-page-name"/><strong class="calibre1">from </strong>pyspark.ml.feature <strong class="calibre1">import </strong>PCA<br class="title-page-name"/><strong class="calibre1">from </strong>pyspark.ml.linalg <strong class="calibre1">import </strong>Vectors<br class="title-page-name"/><strong class="calibre1">print </strong>(<strong class="calibre1">"Successfully imported Spark Modules"</strong>)<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">except </strong>ImportError<strong class="calibre1">as</strong>e:<br class="title-page-name"/><strong class="calibre1">print </strong>(<strong class="calibre1">"Can not import Spark Modules"</strong>, e)<br class="title-page-name"/> sys.exit(1)<br class="title-page-name"/><br class="title-page-name"/>spark = SparkSession\<br class="title-page-name"/>   .builder\<br class="title-page-name"/>   .appName(<strong class="calibre1">"PCAExample"</strong>)\<br class="title-page-name"/>   .getOrCreate()<br class="title-page-name"/><br class="title-page-name"/>data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),<br class="title-page-name"/>    (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),<br class="title-page-name"/>    (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]<br class="title-page-name"/>df = spark.createDataFrame(data, [<strong class="calibre1">"features"</strong>])<br class="title-page-name"/><br class="title-page-name"/>pca = PCA(k=3, inputCol=<strong class="calibre1">"features"</strong>, outputCol=<strong class="calibre1">"pcaFeatures"</strong>)<br class="title-page-name"/>model = pca.fit(df)<br class="title-page-name"/><br class="title-page-name"/>result = model.transform(df).select(<strong class="calibre1">"pcaFeatures"</strong>)<br class="title-page-name"/>result.show(truncate=<strong class="calibre1">False</strong>)<br class="title-page-name"/><br class="title-page-name"/>spark.stop()
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="image-border282" src="img/00188.jpeg"/></div>
<div><strong class="calibre1">Figure 6</strong>: PCA result after successful execution of the Python script</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Working with DataFrames and RDDs</h1>
                
            
            
                
<p class="mce-root">SparkDataFrame is a distributed collection of rows under named columns. Less technically, it can be considered as a table in a relational database with column headers. Furthermore, PySpark DataFrame is similar to Python pandas. However, it also shares some mutual characteristics with RDD:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Immutable</strong>: Just like an RDD, once a DataFrame is created, it can't be changed. We can transform a DataFrame to an RDD and vice versa after applying transformations.</li>
<li class="mce-root1"><strong class="calibre1">Lazy Evaluations:</strong> Its nature is a lazy evaluation. In other words, a task is not executed until an action is performed.</li>
<li class="mce-root1"><strong class="calibre1">Distributed:</strong> Both the RDD and DataFrame are distributed in nature.</li>
</ul>
<p class="mce-root">Just like Java/Scala's DataFrames, PySpark DataFrames are designed for processing a large collection of structured data; you can even handle petabytes of data. The tabular structure helps us understand the schema of a DataFrame, which also helps optimize execution plans on SQL queries. Additionally, it has a wide range of data formats and sources.</p>
<p class="mce-root">You can create RDDs, datasets, and DataFrames in a number of ways using PySpark. In the following subsections, we will show some examples of doing that.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Reading a dataset in Libsvm format</h1>
                
            
            
                
<p class="mce-root">Let's see how to read data in LIBSVM format using the read API and the <kbd class="calibre11">load()</kbd> method by specifying the format of the data (that is, <kbd class="calibre11">libsvm</kbd>) as follows:</p>
<pre class="calibre19">
# Creating DataFrame from libsvm dataset<br class="title-page-name"/> myDF = spark.read.format("libsvm").load("C:/Exp//mnist.bz2")
</pre>
<p class="mce-root">The preceding MNIST dataset can be downloaded from <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2" class="calibre10">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/mnist.bz2</a>. This will essentially return a DataFrame and the content can be seen by calling the <kbd class="calibre11">show()</kbd> method as follows:</p>
<pre class="calibre19">
myDF.show() 
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="image-border283" src="img/00068.gif"/></div>
<div><strong class="calibre1">Figure 7</strong>: A snap of the handwritten dataset in LIBSVM format</div>
<p class="mce-root">You can also specify other options such as how many features of the raw dataset you want to give to your DataFrame as follows:</p>
<pre class="calibre19">
myDF= spark.read.format(<strong class="calibre1">"libsvm"</strong>)<br class="title-page-name"/>           .option("numFeatures", "780")<br class="title-page-name"/>           .load(<strong class="calibre1">"data/Letterdata_libsvm.data"</strong>)
</pre>
<p class="mce-root">Now if you want to create an RDD from the same dataset, you can use the MLUtils API from <kbd class="calibre11">pyspark.mllib.util</kbd> as follows:</p>
<pre class="calibre19">
<em class="calibre8">Creating RDD from the libsvm data file<br class="title-page-name"/></em>myRDD = MLUtils.loadLibSVMFile(spark.sparkContext, <strong class="calibre1">"data/Letterdata_libsvm.data"</strong>)
</pre>
<p class="mce-root">Now you can save the RDD in your preferred location as follows:</p>
<pre class="calibre19">
myRDD.saveAsTextFile("data/myRDD")
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Reading a CSV file</h1>
                
            
            
                
<p class="mce-root">Let's start with loading, parsing, and viewing simple flight data. At first, download the NYC flights dataset as a CSV from <a href="https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv" class="calibre10">https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv</a>. Now let's load and parse the dataset using <kbd class="calibre11">read.csv()</kbd> API of PySpark:</p>
<pre class="calibre19">
# Creating DataFrame from data file in CSV format<em class="calibre8"><br class="title-page-name"/></em>df = spark.read.format(<strong class="calibre1">"com.databricks.spark.csv"</strong>)<br class="title-page-name"/>          .option(<strong class="calibre1">"header"</strong>, <strong class="calibre1">"true"</strong>)<br class="title-page-name"/>          .load(<strong class="calibre1">"data/nycflights13.csv"</strong>)
</pre>
<p class="mce-root">This is pretty similar to reading the libsvm format. Now you can see the resulting DataFrame's structure as follows:</p>
<pre class="calibre19">
df.printSchema() 
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="image-border284" src="img/00338.gif"/></div>
<div><strong class="calibre1">Figure 8</strong>: Schema of the NYC flight dataset</div>
<p class="mce-root">Now let's see a snap of the dataset using the <kbd class="calibre11">show()</kbd> method as follows:</p>
<pre class="calibre19">
df.show() 
</pre>
<p class="mce-root">Now let's view the sample of the data as follows:</p>
<div><img class="image-border285" src="img/00370.gif"/></div>
<div><strong class="calibre1">Figure 9</strong>: Sample of the NYC flight dataset</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Reading and manipulating raw text files</h1>
                
            
            
                
<p class="mce-root">You can read a raw text data file using the <kbd class="calibre11">textFile()</kbd> method. Suppose you have the logs of some purchase:</p>
<pre class="calibre19">
number\tproduct_name\ttransaction_id\twebsite\tprice\tdate0\tjeans\t30160906182001\tebay.com\t100\t12-02-20161\tcamera\t70151231120504\tamazon.com\t450\t09-08-20172\tlaptop\t90151231120504\tebay.ie\t1500\t07--5-20163\tbook\t80151231120506\tpackt.com\t45\t03-12-20164\tdrone\t8876531120508\talibaba.com\t120\t01-05-2017
</pre>
<p class="mce-root">Now reading and creating RDD is pretty straightforward using the <kbd class="calibre11">textFile()</kbd> method as follows:</p>
<pre class="calibre19">
myRDD = spark.sparkContext.textFile(<strong class="calibre1">"sample_raw_file.txt"</strong>)<br class="title-page-name"/>$cd myRDD<br class="title-page-name"/>$ cat part-00000  <br class="title-page-name"/>number\tproduct_name\ttransaction_id\twebsite\tprice\tdate  0\tjeans\t30160906182001\tebay.com\t100\t12-02-20161\tcamera\t70151231120504\tamazon.com\t450\t09-08-2017
</pre>
<p class="mce-root">As you can see, the structure is not that readable. So we can think of giving a better structure by converting the texts as DataFrame. At first, we need to collect the header information as follows:</p>
<pre class="calibre19">
header = myRDD.first() 
</pre>
<p class="mce-root">Now filter out the header and make sure the rest looks correct as follows:<em class="calibre8"><br class="title-page-name"/></em></p>
<pre class="calibre19">
textRDD = myRDD.filter(lambda line: line != header)<br class="title-page-name"/>newRDD = textRDD.map(lambda k: k.split("\\t"))
</pre>
<p class="mce-root">We still have the RDD but with a bit better structure of the data. However, converting it into DataFrame will provide a better view of the transactional data.</p>
<p class="mce-root">The following code creates a DataFrame by specifying the <kbd class="calibre11">header.split</kbd> is providing the names of the columns:</p>
<pre class="calibre19">
 textDF = newRDD.toDF(header.split("\\t"))<br class="title-page-name"/> textDF.show()
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="image-border286" src="img/00208.gif"/></div>
<div><strong class="calibre1">Figure 10</strong>: Sample of the transactional data</div>
<p class="mce-root">Now you could save this DataFrame as a view and make a SQL query. Let's do a query with this DataFrame now:</p>
<pre class="calibre19">
textDF.createOrReplaceTempView(<strong class="calibre1">"transactions"</strong>)<br class="title-page-name"/>spark.sql(<strong class="calibre1">"SELECT </strong><em class="calibre8">*</em><strong class="calibre1"> FROM transactions"</strong>).show()<br class="title-page-name"/>spark.sql(<strong class="calibre1">"SELECT product_name, price FROM transactions WHERE price &gt;=500 "</strong>).show()<br class="title-page-name"/>spark.sql(<strong class="calibre1">"SELECT product_name, price FROM transactions ORDER BY price DESC"</strong>).show()
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone9" src="img/00051.gif"/></div>
<div><strong class="calibre1">Figure 11</strong>: Query result on the transactional data using Spark SQL</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Writing UDF on PySpark</h1>
                
            
            
                
<p class="mce-root">Like Scala and Java, you can also work with <strong class="calibre1">User Defined Functions</strong> (aka. <strong class="calibre1">UDF</strong>) on PySpark. Let's see an example in the following. Suppose we want to see the grade distribution based on the score for some students who have taken courses at a university.</p>
<p class="mce-root">We can store them in two separate arrays as follows:</p>
<pre class="calibre19">
# Let's generate somerandom lists<br class="title-page-name"/> students = ['Jason', 'John', 'Geroge', 'David']<br class="title-page-name"/> courses = ['Math', 'Science', 'Geography', 'History', 'IT', 'Statistics']
</pre>
<p class="mce-root">Now let's declare an empty array for storing the data about courses and students so that later on both can be appended to this array as follows:</p>
<pre class="calibre19">
rawData = []<br class="title-page-name"/>for (student, course) in itertools.product(students, courses):<br class="title-page-name"/>    rawData.append((student, course, random.randint(0, 200)))
</pre>
<p class="mce-root">Note that for the preceding code to work, please import the following at the beginning of the file:</p>
<pre class="calibre19">
<strong class="calibre1">import </strong>itertools<br class="title-page-name"/><strong class="calibre1">import </strong>random
</pre>
<p class="mce-root">Now let's create a DataFrame from these two objects toward converting corresponding grades against each one's score. For this, we need to define an explicit schema. Let's suppose that in your planned DataFrame, there would be three columns named <kbd class="calibre11">Student</kbd>, <kbd class="calibre11">Course</kbd>, and <kbd class="calibre11">Score</kbd>.</p>
<p class="mce-root">At first, let's import necessary modules:</p>
<pre class="calibre19">
<strong class="calibre1">from </strong>pyspark.sql.types<br class="title-page-name"/><strong class="calibre1">import </strong>StructType, StructField, IntegerType, StringType
</pre>
<p class="mce-root">Now the schema can be defined as follows:</p>
<pre class="calibre19">
schema = StructType([StructField("Student", StringType(), nullable=False),<br class="title-page-name"/>                     StructField("Course", StringType(), nullable=False),<br class="title-page-name"/>                     StructField("Score", IntegerType(), nullable=False)])
</pre>
<p class="mce-root">Now let's create an RDD from the Raw Data as follows:</p>
<pre class="calibre19">
courseRDD = spark.sparkContext.parallelize(rawData)
</pre>
<p class="mce-root">Now let's convert the RDD into the DataFrame as follows:</p>
<pre class="calibre19">
courseDF = spark.createDataFrame(courseRDD, schema) <br class="title-page-name"/>coursedDF.show() 
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone10" src="img/00311.gif"/></div>
<div><strong class="calibre1">Figure 12</strong>: Sample of the randomly generated score for students in subjects</div>
<p class="mce-root">Well, now we have three columns. However, we need to convert the score into grades. Say you have the following grading schema:</p>
<ul class="calibre9">
<li class="mce-root1"><em class="calibre8">90~100=&gt; A</em></li>
<li class="mce-root1"><em class="calibre8">80~89 =&gt; B</em></li>
<li class="mce-root1"><em class="calibre8">60~79 =&gt; C</em></li>
<li class="mce-root1"><em class="calibre8">0~59 =&gt; D</em></li>
</ul>
<p class="mce-root">For this, we can create our own UDF such that this will convert the numeric score to grade. It can be done in several ways. Following is an example of doing so:</p>
<pre class="calibre19">
# Define udf<em class="calibre8"><br class="title-page-name"/></em><strong class="calibre1">def </strong>scoreToCategory(grade):<br class="title-page-name"/><strong class="calibre1">     if </strong>grade &gt;= 90:<br class="title-page-name"/><strong class="calibre1">         return </strong><strong class="calibre1">'A'<br class="title-page-name"/></strong><strong class="calibre1">     elif </strong>grade &gt;= 80:<br class="title-page-name"/><strong class="calibre1">         return </strong><strong class="calibre1">'B'<br class="title-page-name"/></strong><strong class="calibre1">     elif </strong>grade &gt;= 60:<br class="title-page-name"/><strong class="calibre1">         return </strong><strong class="calibre1">'C'<br class="title-page-name"/></strong><strong class="calibre1">     else</strong>:<br class="title-page-name"/><strong class="calibre1">         return </strong><strong class="calibre1">'D'</strong>
</pre>
<p class="mce-root">Now we can have our own UDF as follows:</p>
<pre class="calibre19">
<strong class="calibre1">from </strong>pyspark.sql.functions<br class="title-page-name"/><strong class="calibre1">import </strong>udf<br class="title-page-name"/>udfScoreToCategory = udf(scoreToCategory, StringType())
</pre>
<p class="mce-root">The second argument in the <kbd class="calibre11">udf()</kbd> method is the return type of the method (that is, <kbd class="calibre11">scoreToCategory</kbd>). Now you can call this UDF to convert the score into grade in a pretty straightforward way. Let's see an example of it:</p>
<pre class="calibre19">
courseDF.withColumn(<strong class="calibre1">"Grade"</strong>, udfScoreToCategory(<strong class="calibre1">"Score"</strong>)).show(100)
</pre>
<p class="mce-root">The preceding line will take score as input for all entries and convert the score to a grade. Additionally, a new DataFrame with a column named <kbd class="calibre11">Grade</kbd> will be added.</p>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone11" src="img/00354.gif"/></div>
<div><strong class="calibre1">Figure 13</strong>: Assigned grades</div>
<p class="mce-root">Now we can use the UDF with the SQL statement as well. However, for that, we need to register this UDF as follows:</p>
<pre class="calibre19">
spark.udf.register("udfScoreToCategory", scoreToCategory, StringType()) 
</pre>
<p class="mce-root">The preceding line will register the UDF as a temporary function in the database by default. Now we need to create a team view to allow executing SQL queries:</p>
<pre class="calibre19">
courseDF.createOrReplaceTempView("score")
</pre>
<p class="mce-root">Now let's execute an SQL query on the view <kbd class="calibre11">score</kbd> as follows:</p>
<pre class="calibre19">
spark.sql("SELECT Student, Score, udfScoreToCategory(Score) as Grade FROM score").show() 
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone12" src="img/00153.gif"/></div>
<div><strong class="calibre1">Figure 14</strong>: Query on the students score and corresponding grades</div>
<p class="mce-root">The complete source code for this example is as follows:</p>
<pre class="calibre19">
import os<br class="title-page-name"/>import sys<br class="title-page-name"/>import itertools<br class="title-page-name"/>import random<br class="title-page-name"/><br class="title-page-name"/>from pyspark.sql import SparkSession<br class="title-page-name"/>from pyspark.sql.types import StructType, StructField, IntegerType, StringType<br class="title-page-name"/>from pyspark.sql.functions import udf<br class="title-page-name"/><br class="title-page-name"/>spark = SparkSession \<br class="title-page-name"/>        .builder \<br class="title-page-name"/>        .appName("PCAExample") \<br class="title-page-name"/>        .getOrCreate()<br class="title-page-name"/><br class="title-page-name"/># Generate Random RDD<br class="title-page-name"/>students = ['Jason', 'John', 'Geroge', 'David']<br class="title-page-name"/>courses = ['Math', 'Science', 'Geography', 'History', 'IT', 'Statistics']<br class="title-page-name"/>rawData = []<br class="title-page-name"/>for (student, course) in itertools.product(students, courses):<br class="title-page-name"/>    rawData.append((student, course, random.randint(0, 200)))<br class="title-page-name"/><br class="title-page-name"/># Create Schema Object<br class="title-page-name"/>schema = StructType([<br class="title-page-name"/>    StructField("Student", StringType(), nullable=False),<br class="title-page-name"/>    StructField("Course", StringType(), nullable=False),<br class="title-page-name"/>    StructField("Score", IntegerType(), nullable=False)<br class="title-page-name"/>])<br class="title-page-name"/><br class="title-page-name"/>courseRDD = spark.sparkContext.parallelize(rawData)<br class="title-page-name"/>courseDF = spark.createDataFrame(courseRDD, schema)<br class="title-page-name"/>courseDF.show()<br class="title-page-name"/><br class="title-page-name"/># Define udf<br class="title-page-name"/>def scoreToCategory(grade):<br class="title-page-name"/>    if grade &gt;= 90:<br class="title-page-name"/>        return 'A'<br class="title-page-name"/>    elif grade &gt;= 80:<br class="title-page-name"/>        return 'B'<br class="title-page-name"/>    elif grade &gt;= 60:<br class="title-page-name"/>        return 'C'<br class="title-page-name"/>    else:<br class="title-page-name"/>        return 'D'<br class="title-page-name"/><br class="title-page-name"/>udfScoreToCategory = udf(scoreToCategory, StringType())<br class="title-page-name"/>courseDF.withColumn("Grade", udfScoreToCategory("Score")).show(100)<br class="title-page-name"/><br class="title-page-name"/>spark.udf.register("udfScoreToCategory", scoreToCategory, StringType())<br class="title-page-name"/>courseDF.createOrReplaceTempView("score")<br class="title-page-name"/>spark.sql("SELECT Student, Score, udfScoreToCategory(Score) as Grade FROM score").show()<br class="title-page-name"/><br class="title-page-name"/>spark.stop()<strong class="calibre1"> </strong>
</pre>
<p>A more detailed discussion on using UDF can be found at <a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html" class="calibre21">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-udfs.html.</a></p>
<p class="mce-root">Now let's do some analytics tasks on PySpark. In the next section, we will show an example using the k-means algorithm for a clustering task using PySpark.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Let's do some analytics with k-means clustering</h1>
                
            
            
                
<p class="mce-root">Anomalous data refers to data that is unusual from normal distributions. Thus, detecting anomalies is an important task for network security, anomalous packets or requests can be flagged as errors or potential attacks.</p>
<p class="mce-root">In this example, we will use the KDD-99 dataset (can be downloaded here: <a href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html" class="calibre10">http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html</a> ). A number of columns will be filtered out based on certain criteria of the data points. This will help us understand the example. Secondly, for the unsupervised task; we will have to remove the labeled data. Let's load and parse the dataset as simple texts. Then let's see how many rows there are in the dataset:</p>
<pre class="calibre19">
INPUT = <strong class="calibre1">"C:/Users/rezkar/Downloads/kddcup.data"<br class="title-page-name"/></strong>spark = SparkSession\<br class="title-page-name"/>         .builder\<br class="title-page-name"/>         .appName(<strong class="calibre1">"PCAExample"</strong>)\<br class="title-page-name"/>         .getOrCreate()<br class="title-page-name"/><br class="title-page-name"/> kddcup_data = spark.sparkContext.textFile(INPUT)
</pre>
<p class="mce-root">This essentially returns an RDD. Let's see how many rows in the dataset are using the <kbd class="calibre11">count()</kbd> method as follows:</p>
<pre class="calibre19">
count = kddcup_data.count()<br class="title-page-name"/>print(count)&gt;&gt;4898431
</pre>
<p class="mce-root">So, the dataset is pretty big with lots of features. Since we have parsed the dataset as simple texts, we should not expect to see the better structure of the dataset. Thus, let's work toward converting the RDD into DataFrame as follows:</p>
<pre class="calibre19">
kdd = kddcup_data.map(<strong class="calibre1">lambda </strong>l: l.split(<strong class="calibre1">","</strong>))<br class="title-page-name"/><strong class="calibre1">from </strong>pyspark.sql <strong class="calibre1">import </strong>SQLContext<br class="title-page-name"/>sqlContext = SQLContext(spark)<br class="title-page-name"/>df = sqlContext.createDataFrame(kdd)
</pre>
<p class="mce-root">Then let's see some selected columns in the DataFrame as follows:</p>
<pre class="calibre19">
df.select(<strong class="calibre1">"_1"</strong>, <strong class="calibre1">"_2"</strong>, <strong class="calibre1">"_3"</strong>, <strong class="calibre1">"_4"</strong>, <strong class="calibre1">"_42"</strong>).show(5)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone13" src="img/00080.gif"/></div>
<div><strong class="calibre1">Figure 15</strong>: Sample of the KKD cup 99 dataset</div>
<p class="mce-root">Thus, this dataset is already labeled. This means that the types of malicious cyber behavior have been assigned to a row where the label is the last column (that is, <kbd class="calibre11">_42</kbd>). The first five rows off the DataFrame are labeled normal. This means that these data points are normal. Now this is the time that we need to determine the counts of the labels for the entire dataset for each type of labels:</p>
<pre class="calibre19">
#Identifying the labels for unsupervised task<em class="calibre8"><br class="title-page-name"/></em>labels = kddcup_data.map(<strong class="calibre1">lambda </strong>line: line.strip().split(<strong class="calibre1">","</strong>)[-1])<br class="title-page-name"/><strong class="calibre1">from </strong>time <strong class="calibre1">import </strong>time<br class="title-page-name"/>start_label_count = time()<br class="title-page-name"/>label_counts = labels.countByValue()<br class="title-page-name"/>label_count_time = time()-start_label_count<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">from </strong>collections <strong class="calibre1">import </strong>OrderedDict<br class="title-page-name"/>sorted_labels = OrderedDict(sorted(label_counts.items(), key=<strong class="calibre1">lambda </strong>t: t[1], reverse=<strong class="calibre1">True</strong>))<br class="title-page-name"/><strong class="calibre1">for </strong>label, count <strong class="calibre1">in </strong>sorted_labels.items():<br class="title-page-name"/><strong class="calibre1">       print </strong>label, count
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone14" src="img/00134.gif"/></div>
<div><strong class="calibre1">Figure 16</strong>: Available labels (attack types) in the KDD cup dataset</div>
<p class="mce-root">We can see that there are 23 distinct labels (behavior for data objects). The most data points belong to Smurf. This is an abnormal behavior also known as DoS packet floods. The Neptune is the second highest abnormal behavior. The <em class="calibre8">normal</em> events are the third most occurring types of events in the dataset. However, in a real network dataset, you will not see any such labels.</p>
<p class="mce-root">Also, the normal traffic will be much higher than any anomalous traffic. As a result, identifying the anomalous attack or anomaly from the large-scale unlabeled data would be tedious. For simplicity, let's ignore the last column (that is, labels) and think that this dataset is unlabeled too. In that case, the only way to conceptualize the anomaly detection is using unsupervised learning algorithms such as k-means for clustering.</p>
<p class="mce-root">Now let's work toward clustering the data points for this. One important thing about K-means is that it only accepts numeric values for modeling. However, our dataset also contains some categorical features. Now we can assign the categorical features binary values of 1 or 0 based on whether they are <em class="calibre8">TCP</em> or not. This can be done as follows:</p>
<pre class="calibre19">
<strong class="calibre1">from </strong>numpy <strong class="calibre1">import </strong>array<br class="title-page-name"/>def parse_interaction(line):<br class="title-page-name"/>     line_split = line.split(",")<br class="title-page-name"/>     clean_line_split = [line_split[0]]+line_split[4:-1]<br class="title-page-name"/>     return (line_split[-1], array([float(x) for x in clean_line_split]))<br class="title-page-name"/><br class="title-page-name"/> parsed_data = kddcup_data.map(parse_interaction)<br class="title-page-name"/> pd_values = parsed_data.values().cache()
</pre>
<p class="mce-root">Thus, our dataset is almost ready. Now we can prepare our training and test set to training the k-means model with ease:</p>
<pre class="calibre19">
 kdd_train = pd_values.sample(False, .75, 12345)<br class="title-page-name"/> kdd_test = pd_values.sample(False, .25, 12345)<br class="title-page-name"/> print("Training set feature count: " + str(kdd_train.count()))<br class="title-page-name"/> print("Test set feature count: " + str(kdd_test.count()))
</pre>
<p class="mce-root">The output is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">Training set feature count: 3674823<br class="title-page-name"/></strong><strong class="calibre1">Test set feature count: 1225499</strong>
</pre>
<p class="mce-root">However, some standardization is also required since we converted some categorical features to numeric features. Standardization can improve the convergence rate during the optimization process and can also prevent features with very large variances exerting an influence during model training.</p>
<p class="mce-root">Now we will use StandardScaler, which is a feature transformer. It helps us standardize features by scaling them to unit variance. It then sets the mean to zero using column summary statistics in the training set samples:</p>
<pre class="calibre19">
standardizer = StandardScaler(True, True) 
</pre>
<p class="mce-root">Now let's compute the summary statistics by fitting the preceding transformer as follows:</p>
<pre class="calibre19">
standardizer_model = standardizer.fit(kdd_train) 
</pre>
<p class="mce-root">Now the problem is the data that we have for training the k-means does not have a normal distribution. Thus, we need to normalize each feature in the training set to have the unit standard deviation. To make this happen, we need to further transform the preceding standardizer model as follows:</p>
<pre class="calibre19">
data_for_cluster = standardizer_model.transform(kdd_train) 
</pre>
<p class="mce-root">Well done! Now the training set is finally ready to train the k-means model. As we discussed in the clustering chapter, the trickiest thing in the clustering algorithm is finding the optimal number of clusters by setting the value of K so that the data objects get clustered automatically.</p>
<p class="mce-root">One Naive approach considered a brute force is setting <kbd class="calibre11">K=2</kbd> and observing the results and trying until you get an optimal one. However, a much better approach is the Elbow approach, where we can keep increasing the value of <kbd class="calibre11">K</kbd> and compute the <strong class="calibre1">Within Set Sum of Squared Errors</strong> (<strong class="calibre1">WSSSE</strong>) as the clustering cost. In short, we will be looking for the optimal <kbd class="calibre11">K</kbd> values that also minimize the WSSSE. Whenever a sharp decrease is observed, we will get to know the optimal value for <kbd class="calibre11">K</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">import numpy<br class="title-page-name"/>our_k = numpy.arange(10, 31, 10)<br class="title-page-name"/>metrics = []<br class="title-page-name"/>def </strong>computeError(point):<br class="title-page-name"/> center = clusters.centers[clusters.predict(point)]<br class="title-page-name"/> denseCenter = DenseVector(numpy.ndarray.tolist(center))<br class="title-page-name"/><strong class="calibre1">return </strong>sqrt(sum([x**2 <strong class="calibre1">for </strong>x <strong class="calibre1">in </strong>(DenseVector(point.toArray()) - denseCenter)]))<br class="title-page-name"/><strong class="calibre1">for </strong>k <strong class="calibre1">in </strong>our_k:<br class="title-page-name"/>      clusters = KMeans.train(data_for_cluster, k, maxIterations=4, initializationMode=<strong class="calibre1">"random"</strong>)<br class="title-page-name"/>      WSSSE = data_for_cluster.map(<strong class="calibre1">lambda </strong>point: computeError(point)).reduce(<strong class="calibre1">lambda </strong>x, y: x + y)<br class="title-page-name"/>      results = (k, WSSSE)<br class="title-page-name"/> metrics.append(results)<br class="title-page-name"/><strong class="calibre1">print</strong>(metrics)
</pre>
<p class="mce-root">The output is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">[(10, 3364364.5203123973), (20, 3047748.5040717563), (30, 2503185.5418753517)]</strong>
</pre>
<p class="mce-root">In this case, 30 is the best value for k. Let's check the cluster assignments for each data point when we have 30 clusters. The next test would be to run for <kbd class="calibre11">k</kbd> values of 30, 35, and 40. Three values of k are not the most you would test in a single run, but only used for this example:</p>
<pre class="calibre19">
modelk30 = KMeans.train(data_for_cluster, 30, maxIterations=4, initializationMode="random")<br class="title-page-name"/> cluster_membership = data_for_cluster.map(lambda x: modelk30.predict(x))<br class="title-page-name"/> cluster_idx = cluster_membership.zipWithIndex()<br class="title-page-name"/> cluster_idx.take(20)<br class="title-page-name"/> print("Final centers: " + str(modelk30.clusterCenters))
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone15" src="img/00100.jpeg"/></div>
<div><strong class="calibre1">Figure 17</strong>: Final cluster centers for each attack type (abridged)</div>
<p class="mce-root">Now let's compute and print the total cost for the overall clustering as follows:</p>
<pre class="calibre19">
<strong class="calibre1">print</strong>(<strong class="calibre1">"Total Cost: " </strong>+ str(modelk30.computeCost(data_for_cluster)))
</pre>
<p class="mce-root">The output is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">Total Cost: 68313502.459</strong>
</pre>
<p class="mce-root">Finally, the WSSSE of our k-means model can be computed and printed as follows:</p>
<pre class="calibre19">
WSSSE = data_for_cluster.map(<strong class="calibre1">lambda </strong>point: computeError<br class="title-page-name"/>(point)).reduce(lambda x, y: x + y)<br class="title-page-name"/> print("WSSSE: " + str(WSSSE))
</pre>
<p class="mce-root">The output is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">WSSSE: 2503185.54188</strong>
</pre>
<p class="mce-root">Your results might be slightly different. This is due to the random placement of the centroids when we first begin the clustering algorithm. Performing this many times allows you to see how points in your data change their value of k or stay the same. The full source code for this solution is given in the following:</p>
<pre class="calibre19">
import os<br class="title-page-name"/>import sys<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from collections import OrderedDict<br class="title-page-name"/><br class="title-page-name"/>try:<br class="title-page-name"/>    from collections import OrderedDict<br class="title-page-name"/>    from numpy import array<br class="title-page-name"/>    from math import sqrt<br class="title-page-name"/>    import numpy<br class="title-page-name"/>    import urllib<br class="title-page-name"/>    import pyspark<br class="title-page-name"/>    from pyspark.sql import SparkSession<br class="title-page-name"/>    from pyspark.mllib.feature import StandardScaler<br class="title-page-name"/>    from pyspark.mllib.clustering import KMeans, KMeansModel<br class="title-page-name"/>    from pyspark.mllib.linalg import DenseVector<br class="title-page-name"/>    from pyspark.mllib.linalg import SparseVector<br class="title-page-name"/>    from collections import OrderedDict<br class="title-page-name"/>    from time import time<br class="title-page-name"/>    from pyspark.sql.types import *<br class="title-page-name"/>    from pyspark.sql import DataFrame<br class="title-page-name"/>    from pyspark.sql import SQLContext<br class="title-page-name"/>    from pyspark.sql import Row<br class="title-page-name"/>    print("Successfully imported Spark Modules")<br class="title-page-name"/><br class="title-page-name"/>except ImportError as e:<br class="title-page-name"/>    print ("Can not import Spark Modules", e)<br class="title-page-name"/>    sys.exit(1)<br class="title-page-name"/><br class="title-page-name"/>spark = SparkSession\<br class="title-page-name"/>        .builder\<br class="title-page-name"/>        .appName("PCAExample")\<br class="title-page-name"/>        .getOrCreate()<br class="title-page-name"/><br class="title-page-name"/>INPUT = "C:/Exp/kddcup.data.corrected"<br class="title-page-name"/>kddcup_data = spark.sparkContext.textFile(INPUT)<br class="title-page-name"/>count = kddcup_data.count()<br class="title-page-name"/>print(count)<br class="title-page-name"/>kddcup_data.take(5)<br class="title-page-name"/>kdd = kddcup_data.map(lambda l: l.split(","))<br class="title-page-name"/>sqlContext = SQLContext(spark)<br class="title-page-name"/>df = sqlContext.createDataFrame(kdd)<br class="title-page-name"/>df.select("_1", "_2", "_3", "_4", "_42").show(5)<br class="title-page-name"/><br class="title-page-name"/>#Identifying the leabels for unsupervised task<br class="title-page-name"/>labels = kddcup_data.map(lambda line: line.strip().split(",")[-1])<br class="title-page-name"/>start_label_count = time()<br class="title-page-name"/>label_counts = labels.countByValue()<br class="title-page-name"/>label_count_time = time()-start_label_count<br class="title-page-name"/><br class="title-page-name"/>sorted_labels = OrderedDict(sorted(label_counts.items(), key=lambda t: t[1], reverse=True))<br class="title-page-name"/>for label, count in sorted_labels.items():<br class="title-page-name"/>    print(label, count)<br class="title-page-name"/><br class="title-page-name"/>def parse_interaction(line):<br class="title-page-name"/>    line_split = line.split(",")<br class="title-page-name"/>    clean_line_split = [line_split[0]]+line_split[4:-1]<br class="title-page-name"/>    return (line_split[-1], array([float(x) for x in clean_line_split]))<br class="title-page-name"/><br class="title-page-name"/>parsed_data = kddcup_data.map(parse_interaction)<br class="title-page-name"/>pd_values = parsed_data.values().cache()<br class="title-page-name"/><br class="title-page-name"/>kdd_train = pd_values.sample(False, .75, 12345)<br class="title-page-name"/>kdd_test = pd_values.sample(False, .25, 12345)<br class="title-page-name"/>print("Training set feature count: " + str(kdd_train.count()))<br class="title-page-name"/>print("Test set feature count: " + str(kdd_test.count()))<br class="title-page-name"/><br class="title-page-name"/>standardizer = StandardScaler(True, True)<br class="title-page-name"/>standardizer_model = standardizer.fit(kdd_train)<br class="title-page-name"/>data_for_cluster = standardizer_model.transform(kdd_train)<br class="title-page-name"/><br class="title-page-name"/>initializationMode="random"<br class="title-page-name"/><br class="title-page-name"/>our_k = numpy.arange(10, 31, 10)<br class="title-page-name"/>metrics = []<br class="title-page-name"/><br class="title-page-name"/>def computeError(point):<br class="title-page-name"/>    center = clusters.centers[clusters.predict(point)]<br class="title-page-name"/>    denseCenter = DenseVector(numpy.ndarray.tolist(center))<br class="title-page-name"/>    return sqrt(sum([x**2 for x in (DenseVector(point.toArray()) - denseCenter)]))<br class="title-page-name"/><br class="title-page-name"/>for k in our_k:<br class="title-page-name"/>     clusters = KMeans.train(data_for_cluster, k, maxIterations=4, initializationMode="random")<br class="title-page-name"/>     WSSSE = data_for_cluster.map(lambda point: computeError(point)).reduce(lambda x, y: x + y)<br class="title-page-name"/>     results = (k, WSSSE)<br class="title-page-name"/>     metrics.append(results)<br class="title-page-name"/>print(metrics)<br class="title-page-name"/><br class="title-page-name"/>modelk30 = KMeans.train(data_for_cluster, 30, maxIterations=4, initializationMode="random")<br class="title-page-name"/>cluster_membership = data_for_cluster.map(lambda x: modelk30.predict(x))<br class="title-page-name"/>cluster_idx = cluster_membership.zipWithIndex()<br class="title-page-name"/>cluster_idx.take(20)<br class="title-page-name"/>print("Final centers: " + str(modelk30.clusterCenters))<br class="title-page-name"/>print("Total Cost: " + str(modelk30.computeCost(data_for_cluster)))<br class="title-page-name"/>WSSSE = data_for_cluster.map(lambda point: computeError(point)).reduce(lambda x, y: x + y)<br class="title-page-name"/>print("WSSSE" + str(WSSSE))<strong class="calibre1"> </strong>
</pre>
<p>A more comprehensive discussion on this topic can be found at <a href="https://github.com/jadianes/kdd-cup-99-spark" class="calibre21">https://github.com/jadianes/kdd-cup-99-spark</a>. Also, interested readers can refer to the main and latest documentation on PySpark APIs at <a href="http://spark.apache.org/docs/latest/api/python/" class="calibre21">http://spark.apache.org/docs/latest/api/python/</a>.</p>
<p class="mce-root">Well, now it's time to move to SparkR, another Spark API to work with population statistical programming language called R.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Introduction to SparkR</h1>
                
            
            
                
<p class="mce-root">R is one of the most popular statistical programming languages with a number of exciting features that support statistical computing, data processing, and machine learning tasks. However, processing large-scale datasets in R is usually tedious as the runtime is single-threaded. As a result, only datasets that fit in someone's machine memory can be processed. Considering this limitation and for getting the full flavor of Spark in R, SparkR was initially developed at the AMPLab as a lightweight frontend of R to Apache Spark and using Spark's distributed computation engine.</p>
<p class="mce-root">This way it enables the R programmer to use Spark from RStudio for large-scale data analysis from the R shell. In Spark 2.1.0, SparkR provides a distributed data frame implementation that supports operations such as selection, filtering, and aggregation. This is somewhat similar to R data frames like <kbd class="calibre11">dplyr</kbd> but can be scaled up for large-scale datasets.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Why SparkR?</h1>
                
            
            
                
<p class="mce-root">You can write Spark codes using SparkR too that supports distributed machine learning using MLlib. In summary, SparkR inherits many benefits from being tightly integrated with Spark including the following:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Supports various data sources API</strong>: SparkR can be used to read in data from a variety of sources including Hive tables, JSON files, RDBMS, and Parquet files.</li>
<li class="mce-root1"><strong class="calibre1">DataFrame optimizations</strong>: SparkR DataFrames also inherit all of the optimizations made to the computation engine in terms of code generation, memory management, and so on. From the following graph, it can be observed that the optimization engine of Spark enables SparkR competent with Scala and Python:</li>
</ul>
<div><img class="alignnone16" src="img/00108.jpeg"/></div>
<div><strong class="calibre1">Figure 18:</strong> SparkR DataFrame versus Scala/Python DataFrame</div>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Scalability:</strong> Operations executed on SparkR DataFrames get automatically distributed across all the cores and machines available on the Spark cluster. Thus, SparkR DataFrames can be used on terabytes of data and run on clusters with thousands of machines.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Installing and getting started</h1>
                
            
            
                
<p class="mce-root">The best way of using SparkR is from RStudio. Your R program can be connected to a Spark cluster from RStudio using R shell, Rescript, or other R IDEs.</p>
<p class="mce-root"><strong class="calibre1">Option 1.</strong> Set <kbd class="calibre11">SPARK_HOME</kbd> in the environment (you can check <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html" class="calibre10">https://stat.ethz.ch/R-manual/R-devel/library/base/html/Sys.getenv.html</a>), load the SparkR package, and call <kbd class="calibre11">sparkR.session</kbd> as follows. It will check for the Spark installation, and, if not found, it will be downloaded and cached automatically:</p>
<pre class="calibre19">
if (nchar(Sys.getenv("SPARK_HOME")) &lt; 1) { <br class="title-page-name"/>Sys.setenv(SPARK_HOME = "/home/spark") <br class="title-page-name"/>} <br class="title-page-name"/>library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))) 
</pre>
<p class="mce-root"><strong class="calibre1">Option 2.</strong> You can also manually configure SparkR on RStudio. For doing so, create an R script and execute the following lines of R code on RStudio:</p>
<pre class="calibre19">
SPARK_HOME = "spark-2.1.0-bin-hadoop2.7/R/lib" <br class="title-page-name"/>HADOOP_HOME= "spark-2.1.0-bin-hadoop2.7/bin" <br class="title-page-name"/>Sys.setenv(SPARK_MEM = "2g") <br class="title-page-name"/>Sys.setenv(SPARK_HOME = "spark-2.1.0-bin-hadoop2.7") <br class="title-page-name"/>.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths())) 
</pre>
<p class="mce-root">Now load the SparkR library as follows:</p>
<pre class="calibre19">
library(SparkR, lib.loc = SPARK_HOME)
</pre>
<p class="mce-root">Now, like Scala/Java/PySpark, the entry point to your SparkR program is the SparkR session that can be created by calling <kbd class="calibre11">sparkR.session</kbd> as follows:</p>
<pre class="calibre19">
sparkR.session(appName = "Hello, Spark!", master = "local[*]")
</pre>
<p class="mce-root">Furthermore, if you want, you could also specify certain Spark driver properties. Normally, these application properties and runtime environment cannot be set programmatically, as the driver JVM process would have been started; in this case, SparkR takes care of this for you. To set them, pass them as you would pass other configuration properties in the <kbd class="calibre11">sparkConfig</kbd> argument to <kbd class="calibre11">sparkR.session()</kbd> as follows:</p>
<pre class="calibre19">
sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g")) 
</pre>
<p class="mce-root">In addition, the following Spark driver properties can be set in <kbd class="calibre11">sparkConfig</kbd> with <kbd class="calibre11">sparkR.session</kbd> from RStudio:</p>
<div><img class="alignnone17" src="img/00146.gif"/></div>
<div><strong class="calibre1">Figure 19</strong>: Spark driver properties can be set in <kbd class="calibre33">sparkConfig</kbd> with <kbd class="calibre33">sparkR.session</kbd> from RStudio</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Getting started</h1>
                
            
            
                
<p class="mce-root">Let's start with loading, parsing, and viewing simple flight data. At first, download the NY flights dataset as a CSV from <a href="https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv" class="calibre10">https://s3-us-west-2.amazonaws.com/sparkr-data/nycflights13.csv</a>. Now let's load and parse the dataset using <kbd class="calibre11">read.csv()</kbd> API of R:</p>
<pre class="calibre19">
#Creating R data frame<br class="title-page-name"/>dataPath&lt;- "C:/Exp/nycflights13.csv"<br class="title-page-name"/>df&lt;- read.csv(file = dataPath, header = T, sep =",")
</pre>
<p class="mce-root">Now let's view the structure of the dataset using <kbd class="calibre11">View()</kbd> method of R as follows:</p>
<pre class="calibre19">
<kbd class="calibre33">View(df)</kbd>
</pre>
<div><img class="alignnone18" src="img/00217.jpeg"/></div>
<div><strong class="calibre1">Figure 20</strong>: A snap of the NYC flight dataset</div>
<p class="mce-root">Now let's create the Spark DataFrame from the R DataFrame as follows:</p>
<pre class="calibre19">
##Converting Spark DataFrame <br class="title-page-name"/> flightDF&lt;- as.DataFrame(df)
</pre>
<p class="mce-root">Let's see the structure by exploring the schema of the DataFrame:</p>
<pre class="calibre19">
printSchema(flightDF)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone19" src="img/00197.gif"/></div>
<div><strong class="calibre1">Figure 21</strong>: The schema of the NYC flight dataset</div>
<p class="mce-root">Now let's see the first 10 rows of the DataFrame:</p>
<pre class="calibre19">
showDF(flightDF, numRows = 10)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone20" src="img/00222.gif"/></div>
<div><strong class="calibre1">Figure 22</strong>: The first 10 rows of the NYC flight dataset</div>
<p class="mce-root">So, you can see the same structure. However, this is not scalable since we loaded the CSV file using standard R API. To make it faster and scalable, like in Scala, we can use external data source APIs.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Using external data source APIs</h1>
                
            
            
                
<p class="mce-root">As mentioned earlier, we can create DataFrame using external data source APIs as well. For the following example, we used <kbd class="calibre11">com.databricks.spark.csv</kbd> API as follows:</p>
<pre class="calibre19">
flightDF&lt;- read.df(dataPath,  <br class="title-page-name"/>header='true',  <br class="title-page-name"/>source = "com.databricks.spark.csv",  <br class="title-page-name"/>inferSchema='true') 
</pre>
<p class="mce-root">Let's see the structure by exploring the schema of the DataFrame:</p>
<pre class="calibre19">
printSchema(flightDF)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone19" src="img/00132.gif"/></div>
<div><strong class="calibre1">Figure 23</strong>: The same schema of the NYC flight dataset using external data source API</div>
<p class="mce-root">Now let's see the first 10 rows of the DataFrame:</p>
<pre class="calibre19">
showDF(flightDF, numRows = 10)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone21" src="img/00003.jpeg"/></div>
<div><strong class="calibre1">Figure 24</strong>: Same sample data from NYC flight dataset using external data source API</div>
<p class="mce-root">So, you can see the same structure. Well done! Now it's time to explore something more, such as data manipulation using SparkR.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Data manipulation</h1>
                
            
            
                
<p class="mce-root">Show the column names in the SparkDataFrame as follows:</p>
<pre class="calibre19">
columns(flightDF)<br class="title-page-name"/>[1] "year" "month" "day" "dep_time" "dep_delay" "arr_time" "arr_delay" "carrier" "tailnum" "flight" "origin" "dest" <br class="title-page-name"/>[13] "air_time" "distance" "hour" "minute" 
</pre>
<p class="mce-root">Show the number of rows in the SparkDataFrame as follows:</p>
<pre class="calibre19">
count(flightDF)<br class="title-page-name"/>[1] 336776
</pre>
<p class="mce-root">Filter flights data whose destination is only Miami and show the first six entries as follows:</p>
<pre class="calibre19">
 showDF(flightDF[flightDF$dest == "MIA", ], numRows = 10)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone22" src="img/00285.jpeg"/></div>
<div><strong class="calibre1">Figure 25</strong>: Flights with destination Miami only</div>
<p class="mce-root">Select specific columns. For example, let's select all the flights that are going to Iowa that are delayed. Also, include the origin airport names:</p>
<pre class="calibre19">
delay_destination_DF&lt;- select(flightDF, "flight", "dep_delay", "origin", "dest") <br class="title-page-name"/> delay_IAH_DF&lt;- filter(delay_destination_DF, delay_destination_DF$dest == "IAH") showDF(delay_IAH_DF, numRows = 10)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone23" src="img/00269.gif"/></div>
<div><strong class="calibre1">Figure 26</strong>: All the flights that are going to Iowa that are delayed</div>
<p class="mce-root">We can even use it to chain data frame operations. To show an example, at first, group the flights by date and then find the average daily delay. Then, finally, write the result into a SparkDataFrame as follows:</p>
<pre class="calibre19">
install.packages(c("magrittr")) <br class="title-page-name"/>library(magrittr) <br class="title-page-name"/>groupBy(flightDF, flightDF$day) %&gt;% summarize(avg(flightDF$dep_delay), avg(flightDF$arr_delay)) -&gt;dailyDelayDF 
</pre>
<p class="mce-root">Now print the computed DataFrame:</p>
<pre class="calibre19">
head(dailyDelayDF)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone24" src="img/00300.gif"/></div>
<div><strong class="calibre1">Figure 27</strong>: Group the flights by date and then find the average daily delay</div>
<p class="mce-root">Let's see another example that aggregates average arrival delay for the entire destination airport:</p>
<pre class="calibre19">
avg_arr_delay&lt;- collect(select(flightDF, avg(flightDF$arr_delay))) <br class="title-page-name"/> head(avg_arr_delay)<br class="title-page-name"/>avg(arr_delay)<br class="title-page-name"/> 1 6.895377
</pre>
<p class="mce-root">Even more complex aggregation can be performed. For example, the following code aggregates the average, maximum, and minimum delay per each destination airport. It also shows the number of flights that land in those airports:</p>
<pre class="calibre19">
flight_avg_arrival_delay_by_destination&lt;- collect(agg( <br class="title-page-name"/> groupBy(flightDF, "dest"), <br class="title-page-name"/> NUM_FLIGHTS=n(flightDF$dest), <br class="title-page-name"/> AVG_DELAY = avg(flightDF$arr_delay), <br class="title-page-name"/> MAX_DELAY=max(flightDF$arr_delay), <br class="title-page-name"/> MIN_DELAY=min(flightDF$arr_delay) <br class="title-page-name"/> ))<br class="title-page-name"/>head(flight_avg_arrival_delay_by_destination)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone25" src="img/00290.gif"/></div>
<div><strong class="calibre1">Figure 28</strong>: Maximum and minimum delay per each destination airport</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Querying SparkR DataFrame</h1>
                
            
            
                
<p class="mce-root">Similar to Scala, we can perform a SQL query on the DataFrame once it is saved as <kbd class="calibre11">TempView</kbd> using the <kbd class="calibre11">createOrReplaceTempView()</kbd> method. Let's see an example of that. At first, let's save the fight DataFrame (that is, <kbd class="calibre11">flightDF</kbd>) as follows:</p>
<pre class="calibre19">
# First, register the flights SparkDataFrame as a table<br class="title-page-name"/>createOrReplaceTempView(flightDF, "flight")
</pre>
<p class="mce-root">Now let's select destination and destinations of all the flights with their associated carrier information as follows:</p>
<pre class="calibre19">
destDF&lt;- sql("SELECT dest, origin, carrier FROM flight") <br class="title-page-name"/> showDF(destDF, numRows=10)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone26" src="img/00293.jpeg"/></div>
<div><strong class="calibre1">Figure 29</strong>: All the flights with their associated carrier information</div>
<p class="mce-root">Now let's make the SQL a bit more complex, such as finding the destination's airport of all the flights that are at least 120 minutes delayed as follows:</p>
<pre class="calibre19">
selected_flight_SQL&lt;- sql("SELECT dest, origin, arr_delay FROM flight WHERE arr_delay&gt;= 120")<br class="title-page-name"/>showDF(selected_flight_SQL, numRows = 10)
</pre>
<p class="mce-root">The preceding code segment queries and shows the name of the airports of all the flights that are delayed by at least 2 hours:</p>
<div><img class="alignnone27" src="img/00302.jpeg"/></div>
<div><strong class="calibre1">Figure 30</strong>: Destination airports of all the flights that are delayed by at least 2 hours</div>
<p class="mce-root">Now let's do a more complex query. Let's find the origins of all the flights to Iowa that are delayed by at least 2 hours. Finally, sort them by arrival delay and limit the count up to 20 as follows:</p>
<pre class="calibre19">
selected_flight_SQL_complex&lt;- sql("SELECT origin, dest, arr_delay FROM flight WHERE dest='IAH' AND arr_delay&gt;= 120 ORDER BY arr_delay DESC LIMIT 20")<br class="title-page-name"/>showDF(selected_flight_SQL_complex, numRows=20)
</pre>
<p class="mce-root">The preceding code segment queries and shows the name of the airports of all the flights that are delayed by at least 2 hours to Iowa:</p>
<div><img class="alignnone28" src="img/00308.jpeg"/></div>
<div><strong class="calibre1">Figure 31</strong>: Origins of all the flights that are delayed by at least 2 hours where the destination is Iowa</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Visualizing your data on RStudio</h1>
                
            
            
                
<p class="mce-root">In the previous section, we have seen how to load, parse, manipulate, and query the DataFrame. Now it would be great if we could show the data for better visibility. For example, what could be done for the airline carriers? I mean, is it possible to find the most frequent carriers from the plot? Let's give <kbd class="calibre11">ggplot2</kbd> a try. At first, load the library for the same:</p>
<pre class="calibre19">
library(ggplot2) 
</pre>
<p class="mce-root">Now we already have the SparkDataFrame. What if we directly try to use our SparkSQL DataFrame class in <kbd class="calibre11">ggplot2</kbd>?</p>
<pre class="calibre19">
my_plot&lt;- ggplot(data=flightDF, aes(x=factor(carrier)))<br class="title-page-name"/>&gt;&gt;<br class="title-page-name"/><strong class="calibre1">ERROR</strong>: ggplot2 doesn't know how to deal with data of class SparkDataFrame.
</pre>
<p class="mce-root">Obviously, it doesn't work that way because the <kbd class="calibre11">ggplot2</kbd> function doesn't know how to deal with those types of distributed data frames (the Spark ones). Instead, we need to collect the data locally and convert it back to a traditional R data frame as follows:</p>
<pre class="calibre19">
flight_local_df&lt;- collect(select(flightDF,"carrier"))
</pre>
<p class="mce-root">Now let's have a look at what we got using the <kbd class="calibre11">str()</kbd> method as follows:</p>
<pre class="calibre19">
str(flight_local_df)
</pre>
<p class="mce-root">The output is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">'data.frame':  336776 obs. of 1 variable: $ carrier: chr "UA" "UA" "AA" "B6" ...</strong>
</pre>
<p class="mce-root">This is good because when we collect results from a SparkSQL DataFrame, we get a regular R <kbd class="calibre11">data.frame</kbd>. It is also very convenient since we can manipulate it as needed. And now we are ready to create the <kbd class="calibre11">ggplot2</kbd> object as follows:</p>
<pre class="calibre19">
my_plot&lt;- ggplot(data=flight_local_df, aes(x=factor(carrier)))
</pre>
<p class="mce-root">Finally, let's give the plot a proper representation as a bar diagram as follows:</p>
<pre class="calibre19">
my_plot + geom_bar() + xlab("Carrier")
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone29" src="img/00314.jpeg"/></div>
<div><strong class="calibre1">Figure 32</strong>: Most frequent carriers are UA, B6, EV, and DL</div>
<p class="mce-root">From the graph, it is clear that the most frequent carriers are UA, B6, EV, and DL. This gets clearer from the following line of code in R:</p>
<pre class="calibre19">
carrierDF = sql("SELECT carrier, COUNT(*) as cnt FROM flight GROUP BY carrier ORDER BY cnt DESC")<br class="title-page-name"/>showDF(carrierDF)
</pre>
<p class="mce-root">The output is as follows:</p>
<div><img class="alignnone30" src="img/00317.gif"/></div>
<div><strong class="calibre1">Figure 33:</strong> Most most frequent carriers are UA, B6, EV, and DL</div>
<p class="mce-root">The full source code of the preceding analysis is given in the following to understand the flow of the code:</p>
<pre class="calibre19">
#Configure SparkR<br class="title-page-name"/>SPARK_HOME = "C:/Users/rezkar/Downloads/spark-2.1.0-bin-hadoop2.7/R/lib"<br class="title-page-name"/>HADOOP_HOME= "C:/Users/rezkar/Downloads/spark-2.1.0-bin-hadoop2.7/bin"<br class="title-page-name"/>Sys.setenv(SPARK_MEM = "2g")<br class="title-page-name"/>Sys.setenv(SPARK_HOME = "C:/Users/rezkar/Downloads/spark-2.1.0-bin-hadoop2.7")<br class="title-page-name"/>.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))<br class="title-page-name"/><br class="title-page-name"/>#Load SparkR<br class="title-page-name"/>library(SparkR, lib.loc = SPARK_HOME)<br class="title-page-name"/><br class="title-page-name"/># Initialize SparkSession<br class="title-page-name"/>sparkR.session(appName = "Example", master = "local[*]", sparkConfig = list(spark.driver.memory = "8g"))<br class="title-page-name"/># Point the data file path:<br class="title-page-name"/>dataPath&lt;- "C:/Exp/nycflights13.csv"<br class="title-page-name"/><br class="title-page-name"/>#Creating DataFrame using external data source API<br class="title-page-name"/>flightDF&lt;- read.df(dataPath,<br class="title-page-name"/>header='true',<br class="title-page-name"/>source = "com.databricks.spark.csv",<br class="title-page-name"/>inferSchema='true')<br class="title-page-name"/>printSchema(flightDF)<br class="title-page-name"/>showDF(flightDF, numRows = 10)<br class="title-page-name"/># Using SQL to select columns of data<br class="title-page-name"/><br class="title-page-name"/># First, register the flights SparkDataFrame as a table<br class="title-page-name"/>createOrReplaceTempView(flightDF, "flight")<br class="title-page-name"/>destDF&lt;- sql("SELECT dest, origin, carrier FROM flight")<br class="title-page-name"/>showDF(destDF, numRows=10)<br class="title-page-name"/><br class="title-page-name"/>#And then we can use SparkR sql function using condition as follows:<br class="title-page-name"/>selected_flight_SQL&lt;- sql("SELECT dest, origin, arr_delay FROM flight WHERE arr_delay&gt;= 120")<br class="title-page-name"/>showDF(selected_flight_SQL, numRows = 10)<br class="title-page-name"/><br class="title-page-name"/>#Bit complex query: Let's find the origins of all the flights that are at least 2 hours delayed where the destiantionn is Iowa. Finally, sort them by arrival delay and limit the count upto 20 and the destinations<br class="title-page-name"/>selected_flight_SQL_complex&lt;- sql("SELECT origin, dest, arr_delay FROM flight WHERE dest='IAH' AND arr_delay&gt;= 120 ORDER BY arr_delay DESC LIMIT 20")<br class="title-page-name"/>showDF(selected_flight_SQL_complex)<br class="title-page-name"/><br class="title-page-name"/># Stop the SparkSession now<br class="title-page-name"/>sparkR.session.stop()
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, we showed some examples of how to write your Spark code in Python and R. These are the most popular programming languages in the data scientist community.</p>
<p class="mce-root">We covered the motivation of using PySpark and SparkR for big data analytics with almost similar ease with Java and Scala. We discussed how to install these APIs on their popular IDEs such as PyCharm for PySpark and RStudio for SparkR. We also showed how to work with DataFrames and RDDs from these IDEs. Furthermore, we discussed how to execute Spark SQL queries from PySpark and SparkR. Then we also discussed how to perform some analytics with visualization of the dataset. Finally, we saw how to use UDFs with PySpark with examples.</p>
<p class="mce-root">Thus, we have discussed several aspects for two Spark's APIs; PySpark and SparkR. There are much more to explore. Interested readers should refer to their websites for more information:</p>
<ul class="calibre9">
<li class="mce-root1">PySpark: <a href="http://spark.apache.org/docs/latest/api/python/" class="calibre10">http://spark.apache.org/docs/latest/api/python/</a></li>
<li class="mce-root1">SparkR: <a href="https://spark.apache.org/docs/latest/sparkr.html" class="calibre10">https://spark.apache.org/docs/latest/sparkr.html﻿</a></li>
</ul>


            

            
        
    </body></html>