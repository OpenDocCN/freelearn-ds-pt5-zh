["```py\n# Load libraries\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\ndef get_data():\n    \"\"\"\n    Return boston dataset\n    as x - predictor and\n    y - response variable\n    \"\"\"\n    data = load_boston()\n    x    = data['data']\n    y    = data['target']\n    return x,y    \n```", "```py\ndef build_model(x,y):\n    \"\"\"\n    Build a linear regression model\n    \"\"\"\n    model = LinearRegression(normalize=True,fit_intercept=True)\n    model.fit(x,y)\n    return model    \n\ndef view_model(model):\n    \"\"\"\n    Look at model coeffiecients\n    \"\"\"\n    print \"\\n Model coeffiecients\"\n    print \"======================\\n\"\n    for i,coef in enumerate(model.coef_):\n        print \"\\tCoefficient %d  %0.3f\"%(i+1,coef)\n\n    print \"\\n\\tIntercept %0.3f\"%(model.intercept_)\n\ndef model_worth(true_y,predicted_y):\n    \"\"\"\n    Evaluate the model\n    \"\"\"\n    print \"\\tMean squared error = %0.2f\"%(mean_squared_error(true_y,predicted_y))\n```", "```py\ndef plot_residual(y,predicted_y):\n    \"\"\"\n    Plot residuals\n    \"\"\"\n    plt.cla()\n    plt.xlabel(\"Predicted Y\")\n    plt.ylabel(\"Residual\")\n    plt.title(\"Residual Plot\")\n    plt.figure(1)\n    diff = y - predicted_y\n    plt.plot(predicted_y,diff,'go')\n    plt.show()\n```", "```py\nif __name__ == \"__main__\":\n\n    x,y = get_data()\n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    # Build the model\n    model = build_model(x_train,y_train)\n    predicted_y = model.predict(x_train)\n\n    # Plot the residual\n    plot_residual(y_train,predicted_y)\n    # View model coeffiecients    \n    view_model(model)\n\n    print \"\\n Model Performance in Training set\\n\"\n    model_worth(y_train,predicted_y)  \n\n    # Apply the model on dev set\n    predicted_y = model.predict(x_dev)\n    print \"\\n Model Performance in Dev set\\n\"\n    model_worth(y_dev,predicted_y)  \n\n    #Prepare some polynomial features\n    poly_features = PolynomialFeatures(2)\n    poly_features.fit(x_train)\n    x_train_poly = poly_features.transform(x_train)\n    x_dev_poly   = poly_features.transform(x_dev)\n\n    # Build model with polynomial features\n    model_poly = build_model(x_train_poly,y_train)\n    predicted_y = model_poly.predict(x_train_poly)\n    print \"\\n Model Performance in Training set (Polynomial features)\\n\"\n    model_worth(y_train,predicted_y)  \n\n    # Apply the model on dev set\n    predicted_y = model_poly.predict(x_dev_poly)\n    print \"\\n Model Performance in Dev set  (Polynomial features)\\n\"\n    model_worth(y_dev,predicted_y)  \n\n    # Apply the model on Test set\n    x_test_poly = poly_features.transform(x_test)\n    predicted_y = model_poly.predict(x_test_poly)\n\n    print \"\\n Model Performance in Test set  (Polynomial features)\\n\"\n    model_worth(y_test,predicted_y)  \n\n    predicted_y = model.predict(x_test)\n    print \"\\n Model Performance in Test set  (Regular features)\\n\"\n    model_worth(y_test,predicted_y)  \n```", "```py\ndef get_data():\n    \"\"\"\n    Return boston dataset\n    as x - predictor and\n    y - response variable\n    \"\"\"\n    data = load_boston()\n    x    = data['data']\n    y    = data['target']\n    return x,y    \n```", "```py\nx_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n```", "```py\nx_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n```", "```py\n    model = LinearRegression(normalize=True,fit_intercept=True)\n```", "```py\n    # Plot the residual\n    plot_residual(y_train,predicted_y)\n```", "```py\n    #Prepare some polynomial features\n    poly_features = PolynomialFeatures(2)\n    poly_features.fit(x_train)\n    x_train_poly = poly_features.transform(x_train)\n    x_dev_poly   = poly_features.transform(x_dev)\n```", "```py\n    # Build model with polynomial features\n    model_poly = build_model(x_train_poly,y_train)\n    predicted_y = model_poly.predict(x_train_poly)\n    print \"\\n Model Performance in Training set (Polynomial           features)\\n\"\n    model_worth(y_train,predicted_y)  \n\n    # Apply the model on dev set\n    predicted_y = model_poly.predict(x_dev_poly)\n    print \"\\n Model Performance in Dev set  (Polynomial features)\\n\"\n    model_worth(y_dev,predicted_y)  \n```", "```py\n    # Apply the model on Test set\n    x_test_poly = poly_features.transform(x_test)\n    predicted_y = model_poly.predict(x_test_poly)\n\n    print \"\\n Model Performance in Test set  (Polynomial features)\\n\"\n    model_worth(y_test,predicted_y)  \n\npredicted_y = model.predict(x_test)\n    print \"\\n Model Performance in Test set  (Regular features)\\n\"\n    model_worth(y_test,predicted_y)  \n```", "```py\npoly_features = PolynomialFeatures(interaction_only=True)\n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom itertools import combinations\nfrom sklearn.feature_selection import RFE\n\ndef get_data():\n    \"\"\"\n    Return boston dataset\n    as x - predictor and\n    y - response variable\n    \"\"\"\n    data = load_boston()\n    x    = data['data']\n    y    = data['target']\n    return x,y    \n\ndef build_model(x,y,no_features):\n    \"\"\"\n    Build a linear regression model\n    \"\"\"\n    model = LinearRegression(normalize=True,fit_intercept=True)\n    rfe_model = RFE(estimator=model,n_features_to_select=no_features)\n    rfe_model.fit(x,y)\n    return rfe_model    \n\ndef view_model(model):\n    \"\"\"\n    Look at model coeffiecients\n    \"\"\"\n    print \"\\n Model coeffiecients\"\n    print \"======================\\n\"\n    for i,coef in enumerate(model.coef_):\n        print \"\\tCoefficient %d  %0.3f\"%(i+1,coef)\n\n    print \"\\n\\tIntercept %0.3f\"%(model.intercept_)\n\ndef model_worth(true_y,predicted_y):\n    \"\"\"\n    Evaluate the model\n    \"\"\"\n    print \"\\tMean squared error = %0.2f\"%(mean_squared_error(true_y,predicted_y))\n    return mean_squared_error(true_y,predicted_y)\n\ndef plot_residual(y,predicted_y):\n    \"\"\"\n    Plot residuals\n    \"\"\"\n    plt.cla()\n    plt.xlabel(\"Predicted Y\")\n    plt.ylabel(\"Residual\")\n    plt.title(\"Residual Plot\")\n    plt.figure(1)\n    diff = y - predicted_y\n    plt.plot(predicted_y,diff,'go')\n    plt.show()\n\ndef subset_selection(x,y):\n    \"\"\"\n    subset selection method\n    \"\"\"\n    # declare variables to track\n    # the model and attributes which produces\n    # lowest mean square error\n    choosen_subset = None\n    low_mse = 1e100\n    choosen_model = None\n    # k value ranges from 1 to the number of \n    # attributes in x\n    for k in range(1,x.shape[1]+1):\n        print \"k= %d \"%(k)\n        # evaluate all attribute combinations\n        # of size k+1\n        subsets = combinations(range(0,x.shape[1]),k+1)\n        for subset in subsets:\n            x_subset = x[:,subset]\n            model = build_model(x_subset,y)\n            predicted_y = model.predict(x_subset)\n            current_mse = mean_squared_error(y,predicted_y)\n            if current_mse < low_mse:\n                low_mse = current_mse\n                choosen_subset = subset\n                choosen_model = model\n\n    return choosen_model, choosen_subset,low_mse    \n\nif __name__ == \"__main__\":\n\n    x,y = get_data()\n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    #Prepare some polynomial features\n    poly_features = PolynomialFeatures(interaction_only=True)\n    poly_features.fit(x_train)\n    x_train_poly = poly_features.transform(x_train)\n    x_dev_poly   = poly_features.transform(x_dev)\n\n    #choosen_model,choosen_subset,low_mse = subset_selection(x_train_poly,y_train)    \n    choosen_model = build_model(x_train_poly,y_train,20)\n    #print choosen_subse\n    predicted_y = choosen_model.predict(x_train_poly)\n    print \"\\n Model Performance in Training set (Polynomial features)\\n\"\n    mse  = model_worth(y_train,predicted_y)  \n\n    # Apply the model on dev set\n    predicted_y = choosen_model.predict(x_dev_poly)\n    print \"\\n Model Performance in Dev set  (Polynomial features)\\n\"\n    model_worth(y_dev,predicted_y)  \n\n    # Apply the model on Test set\n    x_test_poly = poly_features.transform(x_test)\n    predicted_y = choosen_model.predict(x_test_poly)\n\n    print \"\\n Model Performance in Test set  (Polynomial features)\\n\"\n    model_worth(y_test,predicted_y)  \n```", "```py\ndef build_model(x,y,no_features):\n    \"\"\"\n    Build a linear regression model\n    \"\"\"\n    model = LinearRegression(normalize=True,fit_intercept=True)\n    rfe_model = RFE(estimator=model,n_features_to_select=no_features)\n    rfe_model.fit(x,y)\n    return rfe_model    \n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef get_data():\n    \"\"\"\n    Return boston dataset\n    as x - predictor and\n    y - response variable\n    \"\"\"\n    data = load_boston()\n    x    = data['data']\n    y    = data['target']\n    x    = x - np.mean(x,axis=0)\n\n    return x,y    \n```", "```py\ndef build_model(x,y):\n    \"\"\"\n    Build a Ridge regression model\n    \"\"\"\n    model = Ridge(normalize=True,alpha=0.015)\n    model.fit(x,y)\n    # Track the scores- Mean squared residual for plot\n    return model    \n\ndef view_model(model):\n    \"\"\"\n    Look at model coeffiecients\n    \"\"\"\n    print \"\\n Model coeffiecients\"\n    print \"======================\\n\"\n    for i,coef in enumerate(model.coef_):\n        print \"\\tCoefficient %d  %0.3f\"%(i+1,coef)\n\n    print \"\\n\\tIntercept %0.3f\"%(model.intercept_)\n\ndef model_worth(true_y,predicted_y):\n    \"\"\"\n    Evaluate the model\n    \"\"\"\n    print \"\\tMean squared error = %0.2f\"%(mean_squared_error(true_y,predicted_y))\n    return mean_squared_error(true_y,predicted_y)\n```", "```py\nif __name__ == \"__main__\":\n\n    x,y = get_data()\n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    #Prepare some polynomial features\n    poly_features = PolynomialFeatures(interaction_only=True)\n    poly_features.fit(x_train)\n    x_train_poly = poly_features.transform(x_train)\n    x_dev_poly   = poly_features.transform(x_dev)\n    x_test_poly = poly_features.transform(x_test)\n\n    #choosen_model,choosen_subset,low_mse = subset_selection(x_train_poly,y_train)    \n    choosen_model = build_model(x_train_poly,y_train)\n\n    predicted_y = choosen_model.predict(x_train_poly)\n    print \"\\n Model Performance in Training set (Polynomial features)\\n\"\n    mse  = model_worth(y_train,predicted_y)  \n    view_model(choosen_model)\n\n    # Apply the model on dev set\n    predicted_y = choosen_model.predict(x_dev_poly)\n    print \"\\n Model Performance in Dev set  (Polynomial features)\\n\"\n    model_worth(y_dev,predicted_y)  \n\n    # Apply the model on Test set\n    predicted_y = choosen_model.predict(x_test_poly)\n\n    print \"\\n Model Performance in Test set  (Polynomial features)\\n\"\n    model_worth(y_test,predicted_y)  \n```", "```py\npoly_features = PolynomialFeatures(interaction_only=True)\npoly_features.fit(x_train)\n```", "```py\nx_train_poly = poly_features.transform(x_train)\nx_dev_poly = poly_features.transform(x_dev)\nx_test_poly = poly_features.transform(x_test)\n```", "```py\nmodel = Ridge(normalize=True,alpha=0.015)\nmodel.fit(x,y)\n```", "```py\npredicted_y = choosen_model.predict(x_train_poly)\nprint \"\\n Model Performance in Training set (Polynomial features)\\n\"\nmse = model_worth(y_train,predicted_y) \n```", "```py\nview_model(choosen_model)\n```", "```py\npredicted_y = choosen_model.predict(x_dev_poly)\nprint \"\\n Model Performance in Dev set (Polynomial features)\\n\"\nmodel_worth(y_dev,predicted_y) \n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef get_data():\n    \"\"\"\n    Return boston dataset\n    as x - predictor and\n    y - response variable\n    \"\"\"\n    data = load_boston()\n    x    = data['data']\n    y    = data['target']\n    x    = x - np.mean(x,axis=0)\n\n    return x,y    \n```", "```py\nlin_model,ridg_model = build_model(x,y)\n```", "```py\n# Add some noise to the dataset\nnoise = np.random.normal(0,1,(x.shape))\nx = x + noise\n```", "```py\nalpha_range = np.linspace(10,100.2,300)\n```", "```py\nfor alpha in alpha_range:\n    model = Ridge(normalize=True,alpha=alpha)\n    model.fit(x,y)\n    coeffs.append(model.coef_)\n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import Lasso, LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\n\ndef get_data():\n    \"\"\"\n    Return boston dataset\n    as x - predictor and\n    y - response variable\n    \"\"\"\n    data = load_boston()\n    x    = data['data']\n    y    = data['target']\n    return x,y    \n```", "```py\ndef build_models(x,y):\n    \"\"\"\n    Build a Lasso regression model\n    \"\"\"\n    # Alpha values uniformly\n    # spaced between 0.01 and 0.02\n    alpha_range = np.linspace(0,0.5,200)\n    model = Lasso(normalize=True)\n    coeffiecients = []\n    # Fit a model for each alpha value\n    for alpha in alpha_range:\n        model.set_params(alpha=alpha)\n        model.fit(x,y)\n        # Track the coeffiecients for plot\n        coeffiecients.append(model.coef_)\n    # Plot coeffients weight decay vs alpha value\n    # Plot model RMSE vs alpha value\n    coeff_path(alpha_range,coeffiecients)\n    # View coeffiecient value\n    #view_model(model)\n\ndef view_model(model):\n    \"\"\"\n    Look at model coeffiecients\n    \"\"\"\n    print \"\\n Model coeffiecients\"\n    print \"======================\\n\"\n    for i,coef in enumerate(model.coef_):\n        print \"\\tCoefficient %d  %0.3f\"%(i+1,coef)\n\n    print \"\\n\\tIntercept %0.3f\"%(model.intercept_)\n\ndef model_worth(true_y,predicted_y):\n    \"\"\"\n    Evaluate the model\n    \"\"\"\n    print \"\\t Mean squared error = %0.2f\\n\"%(mean_squared_error(true_y,predicted_y))\n```", "```py\ndef coeff_path(alpha_range,coeffiecients):\n    \"\"\"\n    Plot residuals\n    \"\"\"\n    plt.close('all')\n    plt.cla()\n\n    plt.figure(1)\n    plt.xlabel(\"Alpha Values\")\n    plt.ylabel(\"Coeffiecient Weight\")\n    plt.title(\"Coeffiecient weights for different alpha values\")\n    plt.plot(alpha_range,coeffiecients)\n    plt.axis('tight')\n\n    plt.show()\n\ndef get_coeff(x,y,alpha):\n    model = Lasso(normalize=True,alpha=alpha)\n    model.fit(x,y)\n    coefs = model.coef_\n    indices = [i for i,coef in enumerate(coefs) if abs(coef) > 0.0]\n    return indices\n```", "```py\nif __name__ == \"__main__\":\n\n    x,y = get_data()\n    # Build multiple models for different alpha values\n    # and plot them    \n    build_models(x,y)\n    print \"\\nPredicting using all the variables\"\n    full_model = LinearRegression(normalize=True)\n    full_model.fit(x,y)\n    predicted_y = full_model.predict(x)\n    model_worth(y,predicted_y)    \n\n    print \"\\nModels at different alpha values\\n\"\n    alpa_values = [0.22,0.08,0.01]\n    for alpha in alpa_values:\n\n        indices = get_coeff(x,y,alpha)   \n        print \"\\t alpah =%0.2f Number of variables selected = %d \"%(alpha,len(indices))\n        print \"\\t attributes include \", indices\n        x_new = x[:,indices]\n        model = LinearRegression(normalize=True)\n        model.fit(x_new,y)\n        predicted_y = model.predict(x_new)\n        model_worth(y,predicted_y)\n```", "```py\nalpha_range = np.linspace(0,0.5,200)\nmodel = Lasso(normalize=True)\ncoeffiecients = []\n# Fit a model for each alpha value\nfor alpha in alpha_range:\nmodel.set_params(alpha=alpha)\nmodel.fit(x,y)\n# Track the coeffiecients for plot\ncoeffiecients.append(model.coef_)\n```", "```py\nplt.close('all')\nplt.cla()\n\nplt.figure(1)\nplt.xlabel(\"Alpha Values\")\nplt.ylabel(\"Coeffiecient Weight\")\nplt.title(\"Coeffiecient weights for different alpha values\")\nplt.plot(alpha_range,coeffiecients)\nplt.axis('tight')\nplt.show()\n```", "```py\nprint \"\\nPredicting using all the variables\"\nfull_model = LinearRegression(normalize=True)\nfull_model.fit(x,y)\npredicted_y = full_model.predict(x)\nmodel_worth(y,predicted_y) \n```", "```py\nprint \"\\nModels at different alpha values\\n\"\nalpa_values = [0.22,0.08,0.01]\nfor alpha in alpa_values:\nindices = get_coeff(x,y,alpha) \n```", "```py\nmodel = Lasso(normalize=True,alpha=alpha)\nmodel.fit(x,y)\ncoefs = model.coef_\n\nindices = [i for i,coef in enumerate(coefs) if abs(coef) > 0.0]\n```", "```py\nprint \"\\t alpah =%0.2f Number of variables selected = %d \"%(alpha,len(indices))\nprint \"\\t attributes include \", indices\nx_new = x[:,indices]\nmodel = LinearRegression(normalize=True)\nmodel.fit(x_new,y)\npredicted_y = model.predict(x_new)\nmodel_worth(y,predicted_y)\n```", "```py\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import KFold,StratifiedKFold\n\ndef get_data():\n    data = load_iris()\n    x = data['data']\n    y = data['target']\n    return x,y\n\ndef class_distribution(y):\n        class_dist = {}\n        total = 0\n        for entry in y:\n            try:\n                class_dist[entry]+=1\n            except KeyError:\n                class_dist[entry]=1\n            total+=1\n\n        for k,v in class_dist.items():\n            print \"\\tclass %d percentage =%0.2f\"%(k,v/(1.0*total))\n\nif __name__ == \"__main__\":\n    x,y = get_data()\n    # K Fold\n    # 3 folds\n    kfolds = KFold(n=y.shape[0],n_folds=3)\n    fold_count  =1\n    print\n    for train,test in kfolds:\n        print \"Fold %d x train shape\"%(fold_count),x[train].shape,\\\n        \" x test shape\",x[test].shape\n        fold_count==1\n    print\n    #Stratified KFold\n    skfolds = StratifiedKFold(y,n_folds=3)\n    fold_count  =1\n    for train,test in skfolds:\n        print \"\\nFold %d x train shape\"%(fold_count),x[train].shape,\\\n        \" x test shape\",x[test].shape\n        y_train = y[train]\n        y_test  = y[test]\n        print \"Train Class Distribution\"\n        class_distribution(y_train)\n        print \"Test Class Distribution\"\n        class_distribution(y_test)\n\n        fold_count+=1\n\n    print\n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import KFold,train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\n\ndef get_data():\n    \"\"\"\n    Return boston dataset\n    as x - predictor and\n    y - response variable\n    \"\"\"\n    data = load_boston()\n    x    = data['data']\n    y    = data['target']\n    return x,y    \n```", "```py\ndef build_model(x,y):\n    \"\"\"\n    Build a Ridge regression model\n    \"\"\"\n    kfold = KFold(y.shape[0],5)\n    model = Ridge(normalize=True)\n\n    alpha_range = np.linspace(0.0015,0.0017,30)\n    grid_param = {\"alpha\":alpha_range}\n    grid = GridSearchCV(estimator=model,param_grid=grid_param,cv=kfold,scoring='mean_squared_error')\n    grid.fit(x,y)\n    display_param_results(grid.grid_scores_)\n    print grid.best_params_\n    # Track the scores- Mean squared residual for plot\n    return grid.best_estimator_\n\ndef view_model(model):\n    \"\"\"\n    Look at model coeffiecients\n    \"\"\"\n    #print \"\\n Estimated Alpha = %0.3f\"%model.alpha_\n    print \"\\n Model coeffiecients\"\n    print \"======================\\n\"\n    for i,coef in enumerate(model.coef_):\n        print \"\\tCoefficient %d  %0.3f\"%(i+1,coef)\n\n    print \"\\n\\tIntercept %0.3f\"%(model.intercept_)\n\ndef model_worth(true_y,predicted_y):\n    \"\"\"\n    Evaluate the model\n    \"\"\"\n    print \"\\tMean squared error = %0.2f\"%(mean_squared_error(true_y,predicted_y))\n    return mean_squared_error(true_y,predicted_y)\n\ndef display_param_results(param_results):\n    fold = 1\n    for param_result in param_results:\n        print \"Fold %d Mean squared error %0.2f\"%(fold,abs(param_result[1])),param_result[0]\n        fold+=1\n```", "```py\nif __name__ == \"__main__\":\n\n    x,y = get_data()\n\n    # Divide the data into Train and test    \n    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state=9)\n\n    #Prepare some polynomial features\n    poly_features = PolynomialFeatures(interaction_only=True)\n    poly_features.fit(x_train)\n    x_train_poly = poly_features.transform(x_train)\n    x_test_poly  = poly_features.transform(x_test)\n\n    choosen_model = build_model(x_train_poly,y_train)\n    predicted_y = choosen_model.predict(x_train_poly)\n    model_worth(y_train,predicted_y)\n\n    view_model(choosen_model)\n\n    predicted_y = choosen_model.predict(x_test_poly)\n    model_worth(y_test,predicted_y)\n```", "```py\nkfolds = KFold(n=y.shape[0],n_folds=3)\n```", "```py\nfold_count =1\nprint\nfor train,test in kfolds:\nprint \"Fold %d x train shape\"%(fold_count),x[train].shape,\\\n\" x test shape\",x[test].shape\nfold_count==1\n```", "```py\nskfolds = StratifiedKFold(y,n_folds=3)\n```", "```py\nfold_count =1\nfor train,test in skfolds:\nprint \"\\nFold %d x train shape\"%(fold_count),x[train].shape,\\\n\" x test shape\",x[test].shape\ny_train = y[train]\ny_test = y[test]\nprint \"Train Class Distribution\"\nclass_distribution(y_train)\nprint \"Test Class Distribution\"\nclass_distribution(y_test)\n\nfold_count+=1\n```", "```py\npoly_features = PolynomialFeatures(interaction_only=True)\npoly_features.fit(x_train)\n```", "```py\nx_train_poly = poly_features.transform(x_train)\nx_test_poly = poly_features.transform(x_test)\n```", "```py\nkfold = KFold(y.shape[0],5)\n```", "```py\nmodel = Ridge(normalize=True)\n```", "```py\ngrid = GridSearchCV(estimator=model,param_grid=grid_param,cv=kfold,scoring='mean_squared_error')\n```", "```py\nalpha_range = np.linspace(0.0015,0.0017,30)\n```", "```py\ngrid_param = {\"alpha\":alpha_range}\n```", "```py\nprint grid.best_params_\nreturn grid.best_estimator_\n```", "```py\nalpha_range = np.linspace(0.01,1.0,30)\n```", "```py\nalpha_range = np.linspace(0.001,0.1,30)\n```", "```py\nchoosen_model = build_model(x_train_poly,y_train)\npredicted_y = choosen_model.predict(x_train_poly)\nmodel_worth(y_train,predicted_y)\n```", "```py\ngrid = GridSearchCV(estimator=model,param_grid=grid_param,cv=None,scoring='mean_squared_error')\n```"]