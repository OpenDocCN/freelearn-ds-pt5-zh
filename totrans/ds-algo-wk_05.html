<html><head></head><body><div><h1 class="header-title">Clustering into K Clusters</h1>
                
            
            
                
<p class="mce-root">Clustering is a technique for dividing data into clusters, with the same features in the same cluster.</p>
<p class="mce-root">In this chapter, we will cover the following topics:</p>
<ul class="calibre14">
<li class="calibre15">How to use the <em class="calibre5">k</em>-means clustering algorithm, using an example involving household incomes</li>
<li class="calibre15">How to classify features by clustering them first with the features, along with the known classes, using an example of gender classification </li>
<li class="calibre15">How to implement the <em class="calibre5">k</em>-means clustering algorithm in Python in the <em class="calibre5">Implementation of k-means clustering algorithm</em> section</li>
<li class="calibre15">An example of house ownership and how to choose an appropriate number of clusters for your analysis</li>
<li class="calibre15">How to use the example of house ownership to scale a given set of numerical data appropriately to improve the accuracy of classification by using a clustering algorithm</li>
<li class="calibre15">An understanding of how different numbers of clusters alter the meaning of the dividing line between those clusters, using an example of document clustering</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Household incomes – clustering into k clusters</h1>
                
            
            
                
<p class="mce-root">For example, let's look at households whose annual earnings in USD are $40,000, $55,000, $70,000, $100,000, $115,000, $130,000 and $135,000. Then, if we were to combine those households into two clusters, taking their earnings as a measure of similarity, the first cluster would have those households earning 40 k, 55 k, and 70 k, while the second cluster would include those households earning 100 k, 115 k, 130 k, and 135 k.</p>
<p class="mce-root">This is because 40k and 135k are furthest away from each other so, because we want to have two clusters, these have to be in different clusters. 55 K is closer to 40 k than to 135 k, so 40 k and 55 k will be in the same cluster. Similarly, 130 k and 135 k will be in the same cluster. 70 K is closer to 40 k and 55 k than to 130 K and 135 k, so 70 k should be in the cluster with 40 k and 55 k. 115 K is closer to 130 k and 135 k than to the first cluster containing 40 k, 55 k, and 70 k, so it will be in the second cluster. Finally, 100 k is closer to the second cluster, containing 115 k, 130 k, and 135 k, so it will be located there. Therefore, the first cluster will include 40 k, 55 k, and 70  households. The second cluster will include the 100 k, 115 k, 130 k, and 135 k households.</p>
<p class="mce-root">Clustering the features of groups with similar properties and assigning a cluster to a feature is a form of classification. It is up to a data scientist to interpret the result of the clustering and what classification it induces. Here, the cluster containing the households with annual incomes of USD 40 k, 55 k, and 70 k represents a class of households with a low income. The second cluster, including households with an annual income of USD 100 k, 115 k, 130 k, and 135 k, represents a class of households with a high income.</p>
<p class="mce-root">We clustered the households into the two clusters in an informal way based on intuition and common sense. There are clustering algorithms that cluster data according to precise rules. These algorithms include a fuzzy c-means clustering algorithm, a hierarchical clustering algorithm, a Gaussian (EM) clustering algorithm, a quality threshold clustering algorithm, and a <em class="calibre18">k</em>-means clustering algorithm, which is the focus of this chapter.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">K-means clustering algorithm</h1>
                
            
            
                
<p class="mce-root">The <em class="calibre18">k</em>-means clustering algorithm classifies given points into <em class="calibre18">k</em> groups in such a way that the distance between members of the same group is minimized.</p>
<p class="mce-root">The <em class="calibre18">k</em>-means clustering algorithm determines the initial <em class="calibre18">k</em>-centroids (points in a cluster center)—one for each cluster. Then, each feature is classified into the cluster whose centroid is closest to that feature. After classifying all the features, we have formed an initial <em class="calibre18">k</em> clusters.</p>
<p class="mce-root">For each cluster, we recomputed the centroid to be the average of the points in that cluster. After we have moved the centroids, we recompute the classes again. The features in the classes may change. In this case, we have to recompute the centroids again. If the centroids no longer move, then the <em class="calibre18">k</em>-means clustering algorithm terminates.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Picking the initial k-centroids</h1>
                
            
            
                
<p class="mce-root">We could pick the initial <em class="calibre18">k</em>-centroids to be any of the <em class="calibre18">k</em> features in the data to be classified. But, ideally, we would like to pick points that belong to different clusters from the very outset. Therefore, we may want to aim to maximize their mutual distance in a certain way. Simplifying the process, we could pick the first centroid to be any point from the features. The second could be the one that is furthest from the first. The third could be the one that is furthest from both the first and second, and so on.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Computing a centroid of a given cluster</h1>
                
            
            
                
<p class="mce-root">A centroid of a cluster is just an average of the points in that cluster. If a cluster contains one-dimensional points with the coordinates <em class="calibre18">x<sub class="calibre29">1</sub>, x<sub class="calibre29">2</sub>, …, x<sub class="calibre29">n</sub></em>, then the centroid of that cluster would be  <img class="fm-editor-equation129" src="img/a1e22e13-4d22-4526-a133-86bca036067f.png" width="1990" height="410"/>. If a cluster contains two-dimensional points with the coordinates <em class="calibre18">(x<sub class="calibre29">1</sub>,y<sub class="calibre29">1</sub>),(x<sub class="calibre29">2</sub>,y<sub class="calibre29">2</sub>),…,(x<sub class="calibre29">n</sub>,y<sub class="calibre29">n</sub>)</em>, then the <em class="calibre18">x</em>-coordinate of the centroid of the cluster would have the value <em class="calibre18">(1/n)*(x<sub class="calibre29">1</sub>+x<sub class="calibre29">2</sub>+...+x<sub class="calibre29">n</sub>)</em>, and the <em class="calibre18">y</em>-coordinate would have the value <img class="fm-editor-equation130" src="img/afbc1a34-7864-4496-a187-513dc5e5d8f1.png" width="1920" height="260"/>.</p>
<p class="mce-root">This computation generalizes easily to higher dimensions. If the value of the higher-dimensional features for the <em class="calibre18">x</em>-coordinate are <em class="calibre18">x<sub class="calibre29">1</sub>, x<sub class="calibre29">2</sub>, …, x<sub class="calibre29">n</sub></em>, then the value of the <em class="calibre18">x</em>-coordinate for the centroid is <img class="fm-editor-equation131" src="img/afbc1a34-7864-4496-a187-513dc5e5d8f1.png" width="1920" height="260"/>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Using the k-means clustering algorithm on the household income example</h1>
                
            
            
                
<p class="mce-root">We will apply the <em class="calibre18">k</em>-clustering algorithm to the household income example. In the beginning, we have households with incomes of $40,000, $55,000, $70,000, $100,000, $115,000, $130,000 and $135,000.</p>
<p class="mce-root">The first centroid to be picked up can be any feature, for example, $70,000. The second centroid should be the feature that is furthest from the first one, that is 135 k, since 135 k minus 70 k is 65 k, which is the greatest difference between any other feature and 70 k. Thus, 70 k is the centroid of the first cluster, while 135 k is the centroid of the second cluster.</p>
<p class="mce-root">Now, by taking the difference, 40 k, 55 k, 70 k, and 100 k are closer to 70 k than to 135 k, so they will be in the first cluster, while 115 k, 130 k, and 135 k are closer to 135 k than to 70 k, so they will be in the second cluster.</p>
<p class="mce-root">After we have classified the features according to the initial centroids, we recompute the centroids. The centroid of the first cluster is as follows:</p>
<p class="mce-root"><img class="fm-editor-equation132" src="img/ee300cca-5ab2-44aa-ab19-2c686e619d9d.png" width="7480" height="700"/></p>
<p class="mce-root">The centroid of the second cluster is as follows:</p>
<p class="mce-root"><img class="fm-editor-equation133" src="img/179e82f3-1388-4a6c-b804-02c8bd469ee9.png" width="7480" height="490"/></p>
<p class="mce-root">Using the new centroids, we reclassify the features as follows:</p>
<ul class="calibre14">
<li class="calibre15">The first cluster containing the centroid 66.25 k will contain the features 40 k, 55 k, and 70 k</li>
<li class="calibre15">The second cluster containing the centroid 126.66 k will contain the features 100 k, 115 k, 130 k, and 135 k</li>
</ul>
<p class="mce-root">We notice that the 100 k feature moved from the first cluster to the second, since it is now closer to the centroid of the second cluster (<em class="calibre18">distance |100 k-126.66 k|=26.66 k</em>) than to the centroid of the first cluster (<em class="calibre18">distance |100 k-66.25 k|=33.75 k</em>). Since the features in the clusters have changed, we have to recompute the centroids again.</p>
<p class="mce-root">The centroid of the first cluster is as follows:</p>
<p class="mce-root"><img class="fm-editor-equation134" src="img/e5779d6d-ccc3-4ba7-ada3-509f3df61c4c.png" width="7480" height="700"/></p>
<p class="mce-root">The centroid of the second cluster is as follows:</p>
<p class="mce-root"><img class="fm-editor-equation134" src="img/28b0ff92-f3a2-4778-82cd-d6d8e878de27.png" width="7480" height="700"/></p>
<p class="mce-root">Using these centroids, we reclassify the features into the clusters. The first centroid, 55 k, will contain the features 40 k, 55 k, and 70 k. The second centroid, 120 k, will contain the features 100 k, 115 k, 130 k, and 135 k. Thus, when the centroids were updated, the clusters did not change. Hence, their centroids will remain the same.</p>
<p class="mce-root">Therefore, the algorithm terminates with the two clusters: the first cluster having the features 40 k, 55 k, and 70 k, and the second cluster having the features 100 k, 115 k, 130 k, and 135 k.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Gender classification – clustering to classify</h1>
                
            
            
                
<p class="mce-root">The following data is taken from the gender classification example, <em class="calibre18">Problem 6</em>, <a href="" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 2</a>, <em class="calibre18">Naive Bayes</em>:</p>
<table border="1" class="msotablegrid">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Height in cm</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Weight in kg</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Hair length</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Gender</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">180</p>
</td>
<td class="calibre25">
<p class="calibre26">75</p>
</td>
<td class="calibre25">
<p class="calibre26">Short</p>
</td>
<td class="calibre25">
<p class="calibre26">Male</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">174</p>
</td>
<td class="calibre25">
<p class="calibre26">71</p>
</td>
<td class="calibre25">
<p class="calibre26">Short</p>
</td>
<td class="calibre25">
<p class="calibre26">Male</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">184</p>
</td>
<td class="calibre25">
<p class="calibre26">83</p>
</td>
<td class="calibre25">
<p class="calibre26">Short</p>
</td>
<td class="calibre25">
<p class="calibre26">Male</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">168</p>
</td>
<td class="calibre25">
<p class="calibre26">63</p>
</td>
<td class="calibre25">
<p class="calibre26">Short</p>
</td>
<td class="calibre25">
<p class="calibre26">Male</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">178</p>
</td>
<td class="calibre25">
<p class="calibre26">70</p>
</td>
<td class="calibre25">
<p class="calibre26">Long</p>
</td>
<td class="calibre25">
<p class="calibre26">Male</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">170</p>
</td>
<td class="calibre25">
<p class="calibre26">59</p>
</td>
<td class="calibre25">
<p class="calibre26">Long</p>
</td>
<td class="calibre25">
<p class="calibre26">Female</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">164</p>
</td>
<td class="calibre25">
<p class="calibre26">53</p>
</td>
<td class="calibre25">
<p class="calibre26">Short</p>
</td>
<td class="calibre25">
<p class="calibre26">Female</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">155</p>
</td>
<td class="calibre25">
<p class="calibre26">46</p>
</td>
<td class="calibre25">
<p class="calibre26">Long</p>
</td>
<td class="calibre25">
<p class="calibre26">Female</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">162</p>
</td>
<td class="calibre25">
<p class="calibre26">52</p>
</td>
<td class="calibre25">
<p class="calibre26">Long</p>
</td>
<td class="calibre25">
<p class="calibre26">Female</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">166</p>
</td>
<td class="calibre25">
<p class="calibre26">55</p>
</td>
<td class="calibre25">
<p class="calibre26">Long</p>
</td>
<td class="calibre25">
<p class="calibre26">Female</p>
</td>
</tr>
<tr class="calibre33">
<td class="calibre25">
<p class="calibre26">172</p>
</td>
<td class="calibre25">
<p class="calibre26">60</p>
</td>
<td class="calibre25">
<p class="calibre26">Long</p>
</td>
<td class="calibre25">
<p class="calibre26">?</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">To simplify matters, we will remove the column entitled <strong class="calibre8">Hair length</strong>. We will also remove the column entitled <strong class="calibre8">Gender</strong>, since we would like to cluster the people in the table based on their height and weight. We would like to establish whether the eleventh person in the table is more likely to be a man or a woman using clustering:</p>
<table border="1" class="msotablegrid">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Height in cm</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Weight in kg</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">180</p>
</td>
<td class="calibre25">
<p class="calibre26">75</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">174</p>
</td>
<td class="calibre25">
<p class="calibre26">71</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">184</p>
</td>
<td class="calibre25">
<p class="calibre26">83</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">168</p>
</td>
<td class="calibre25">
<p class="calibre26">63</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">178</p>
</td>
<td class="calibre25">
<p class="calibre26">70</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">170</p>
</td>
<td class="calibre25">
<p class="calibre26">59</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">164</p>
</td>
<td class="calibre25">
<p class="calibre26">53</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">155</p>
</td>
<td class="calibre25">
<p class="calibre26">46</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">162</p>
</td>
<td class="calibre25">
<p class="calibre26">52</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">166</p>
</td>
<td class="calibre25">
<p class="calibre26">55</p>
</td>
</tr>
<tr class="calibre33">
<td class="calibre25">
<p class="calibre26">172</p>
</td>
<td class="calibre25">
<p class="calibre26">60</p>
</td>
</tr>
</tbody>
</table>


            

            
        
    </div>



  
<div><h1 class="header-title">Analysis</h1>
                
            
            
                
<p class="mce-root">We may apply scaling to the initial data, but to simplify matters, we will use the unscaled data in the algorithm. We will cluster the data we have into two clusters, since there are two possibilities for gender—male or female. Then, we will aim to classify a person with a height of 172 cm and a weight of 60 kg as being more likely be a man if, and only if, there are more men in that cluster. The clustering algorithm is a very efficient technique. Thus, classifying in this way is very fast, especially if there are a lot of features to classify.</p>
<p class="mce-root">So, let's apply the <em class="calibre18">k</em>-means clustering algorithm to the data we have. First, we pick the initial centroids. Let the first centroid, for example, be a person with a height of 180 cm and a weight of 75 kg, denoted in a vector as <em class="calibre18">(180,75)</em>. The point that is furthest away from <em class="calibre18">(180,75)</em> is <em class="calibre18">(155,46)</em>. So that will be the second centroid.</p>
<p class="mce-root">The points that are closer to the first centroid <em class="calibre18">(180,75)</em> by taking Euclidean distance are <em class="calibre18">(180,75)</em>, <em class="calibre18">(174,71)</em>, <em class="calibre18">(184,83)</em>, <em class="calibre18">(168,63), (178,70)</em>, <em class="calibre18">(170,59)</em>, and <em class="calibre18">(172,60)</em>. So these points will be in the first cluster. The points that are closer to the second centroid <em class="calibre18">(155,46)</em> are <em class="calibre18">(155,46)</em>, <em class="calibre18">(164,53)</em>, <em class="calibre18">(162,52)</em>, and <em class="calibre18">(166,55)</em>. So these points will be in the second cluster. The current situation involving these two clusters is displayed in the following diagram:</p>
<div><img class="image-border2" src="img/2ca5c8e6-7dd7-4d76-83ac-ea0752aebe07.png" width="800" height="600"/></div>
<p>Figure 5.1: Clustering of people according to their height and weight</p>
<p class="mce-root">Let's recompute the centroids of the clusters. The blue cluster with the features (<em class="calibre18">180,75)</em>, <em class="calibre18">(174,71)</em>, <em class="calibre18">(184,83)</em>, <em class="calibre18">(168,63)</em>, <em class="calibre18">(178,70), (170,59)</em>, and <em class="calibre18">(172,60)</em> will have the centroid <em class="calibre18">((180+174+184+168+178+170+172)/7,(75+71+83+63+70+59+60)/7)~(175.14,68.71)</em>.</p>
<p class="mce-root">The red cluster with the features <em class="calibre18">(155,46)</em>, <em class="calibre18">(164,53)</em>, <em class="calibre18">(162,52)</em>, and <em class="calibre18">(166,55)</em> will have the centroid <em class="calibre18">((155+164+162+166)/4,(46+53+52+55)/4)=(161.75, 51.5)</em>.</p>
<p class="mce-root">Reclassifying the points using the new centroid, the classes of the points do not change. The blue cluster will have the points <em class="calibre18">(180,75)</em>, <em class="calibre18">(174,71)</em>, <em class="calibre18">(184,83)</em>, <em class="calibre18">(168,63)</em>, <em class="calibre18">(178,70)</em>, <em class="calibre18">(170,59)</em>, and <em class="calibre18">(172,60)</em>. The red cluster will have the points <em class="calibre18">(155,46)</em>, <em class="calibre18">(164,53)</em>, <em class="calibre18">(162,52)</em>, and <em class="calibre18">(166,55)</em>. Therefore, the clustering algorithm terminates with clusters, as depicted in the following diagram:</p>
<div><img class="image-border3" src="img/736d91cc-3378-4953-9286-8034a22357e1.png" width="800" height="600"/></div>
<p>Figure 5.2: Clustering of people according to their height and weight</p>
<p class="mce-root">Now we would like to classify the instance <em class="calibre18">(172,60)</em> as to whether that person is a male or a female. The instance <em class="calibre18">(172,60)</em> is in the blue cluster, so it is similar to the features in the blue cluster. Are the remaining features in the blue cluster more likely male or female? Five out of six features are male, while only one is female. Since the majority of the features are male in the blue cluster and the person <em class="calibre18">(172,60)</em> is in the blue cluster as well, we classify a person with a height of 172 cm and a weight of 60 kg as male.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Implementation of the k-means clustering algorithm</h1>
                
            
            
                
<p class="mce-root">We will now implement the <em class="calibre18">k</em>-means clustering algorithm. It takes a CSV file as input with one data item per line. A data item is converted into a point. The algorithms classify these points into the specified number of clusters. In the end, the clusters are visualized on a graph using the <kbd class="calibre17">matplotlib</kbd> library:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/5/k-means_clustering.py</strong><br class="calibre2"/>import math<br class="calibre2"/>import imp<br class="calibre2"/>import sys<br class="calibre2"/>import matplotlib.pyplot as plt<br class="calibre2"/>import matplotlib<br class="calibre2"/>import sys<br class="calibre2"/>sys.path.append('../common')<br class="calibre2"/>import common # noqa<br class="calibre2"/>matplotlib.style.use('ggplot')<br class="calibre2"/><br class="calibre2"/># Returns k initial centroids for the given points.<br class="calibre2"/><strong class="calibre3">def choose_init_centroids(points, k):</strong><br class="calibre2"/>    centroids = []<br class="calibre2"/>    centroids.append(points[0])<br class="calibre2"/>    while len(centroids) &lt; k:<br class="calibre2"/>        # Find the centroid that with the greatest possible distance<br class="calibre2"/>        # to the closest already chosen centroid.<br class="calibre2"/>        candidate = points[0]<br class="calibre2"/>        candidate_dist = min_dist(points[0], centroids)<br class="calibre2"/>        for point in points:<br class="calibre2"/>            dist = min_dist(point, centroids)<br class="calibre2"/>            if dist &gt; candidate_dist:<br class="calibre2"/>                candidate = point<br class="calibre2"/>                candidate_dist = dist<br class="calibre2"/>        centroids.append(candidate)<br class="calibre2"/>    return centroids<br class="calibre2"/><br class="calibre2"/># Returns the distance of a point from the closest point in points.<br class="calibre2"/><strong class="calibre3">def min_dist(point, points):</strong><br class="calibre2"/>    min_dist = euclidean_dist(point, points[0])<br class="calibre2"/>    for point2 in points:<br class="calibre2"/>        dist = euclidean_dist(point, point2)<br class="calibre2"/>        if dist &lt; min_dist:<br class="calibre2"/>            min_dist = dist<br class="calibre2"/>    return min_dist<br class="calibre2"/><br class="calibre2"/># Returns an Euclidean distance of two 2-dimensional points.<br class="calibre2"/><strong class="calibre3">def euclidean_dist((x1, y1), (x2, y2)):</strong><br class="calibre2"/>    return math.sqrt((x1 - x2) * (x1 - x2) + (y1 - y2) * (y1 - y2))<br class="calibre2"/><br class="calibre2"/># PointGroup is a tuple that contains in the first coordinate a 2d point<br class="calibre2"/># and in the second coordinate a group which a point is classified to.<br class="calibre2"/><strong class="calibre3">def choose_centroids(point_groups, k):</strong><br class="calibre2"/>    centroid_xs = [0] * k<br class="calibre2"/>    centroid_ys = [0] * k<br class="calibre2"/>    group_counts = [0] * k<br class="calibre2"/>    for ((x, y), group) in point_groups:<br class="calibre2"/>        centroid_xs[group] += x<br class="calibre2"/>        centroid_ys[group] += y<br class="calibre2"/>        group_counts[group] += 1<br class="calibre2"/>    centroids = []<br class="calibre2"/>    for group in range(0, k):<br class="calibre2"/>        centroids.append((<br class="calibre2"/>            float(centroid_xs[group]) / group_counts[group],<br class="calibre2"/>            float(centroid_ys[group]) / group_counts[group]))<br class="calibre2"/>    return centroids<br class="calibre2"/><br class="calibre2"/># Returns the number of the centroid which is closest to the point.<br class="calibre2"/># This number of the centroid is the number of the group where<br class="calibre2"/># the point belongs to.<br class="calibre2"/><strong class="calibre3">def closest_group(point, centroids):</strong><br class="calibre2"/>    selected_group = 0<br class="calibre2"/>    selected_dist = euclidean_dist(point, centroids[0])<br class="calibre2"/>    for i in range(1, len(centroids)):<br class="calibre2"/>        dist = euclidean_dist(point, centroids[i])<br class="calibre2"/>        if dist &lt; selected_dist:<br class="calibre2"/>            selected_group = i<br class="calibre2"/>            selected_dist = dist<br class="calibre2"/>    return selected_group<br class="calibre2"/><br class="calibre2"/># Reassigns the groups to the points according to which centroid<br class="calibre2"/># a point is closest to.<br class="calibre2"/><strong class="calibre3">def assign_groups(point_groups, centroids):</strong><br class="calibre2"/>    new_point_groups = []<br class="calibre2"/>    for (point, group) in point_groups:<br class="calibre2"/>        new_point_groups.append(<br class="calibre2"/>            (point, closest_group(point, centroids)))<br class="calibre2"/>    return new_point_groups<br class="calibre2"/><br class="calibre2"/># Returns a list of pointgroups given a list of points.<br class="calibre2"/><strong class="calibre3">def points_to_point_groups(points):</strong><br class="calibre2"/>    point_groups = []<br class="calibre2"/>    for point in points:<br class="calibre2"/>        point_groups.append((point, 0))<br class="calibre2"/>    return point_groups<br class="calibre2"/><br class="calibre2"/># Clusters points into the k groups adding every stage<br class="calibre2"/># of the algorithm to the history which is returned.<br class="calibre2"/><strong class="calibre3">def cluster_with_history(points, k):</strong><br class="calibre2"/>    history = []<br class="calibre2"/>    centroids = choose_init_centroids(points, k)<br class="calibre2"/>    point_groups = points_to_point_groups(points)<br class="calibre2"/>    while True:<br class="calibre2"/>        point_groups = assign_groups(point_groups, centroids)<br class="calibre2"/>        history.append((point_groups, centroids))<br class="calibre2"/>        new_centroids = choose_centroids(point_groups, k)<br class="calibre2"/>        done = True<br class="calibre2"/>        for i in range(0, len(centroids)):<br class="calibre2"/>            if centroids[i] != new_centroids[i]:<br class="calibre2"/>                done = False<br class="calibre2"/>                break<br class="calibre2"/>        if done:<br class="calibre2"/>            return history<br class="calibre2"/>        centroids = new_centroids<br class="calibre2"/><br class="calibre2"/><strong class="calibre3"># Program start</strong><br class="calibre2"/>csv_file = sys.argv[1]<br class="calibre2"/>k = int(sys.argv[2])<br class="calibre2"/>everything = False<br class="calibre2"/># The third argument sys.argv[3] represents the number of the step of the<br class="calibre2"/># algorithm starting from 0 to be shown or "last" for displaying the last<br class="calibre2"/># step and the number of the steps.<br class="calibre2"/>if sys.argv[3] == "last":<br class="calibre2"/>    everything = True<br class="calibre2"/>else:<br class="calibre2"/>    step = int(sys.argv[3])<br class="calibre2"/><br class="calibre2"/>data = common.csv_file_to_list(csv_file)<br class="calibre2"/>points = data_to_points(data)  # Represent every data item by a point.<br class="calibre2"/>history = cluster_with_history(points, k)<br class="calibre2"/>if everything:<br class="calibre2"/>    print "The total number of steps:", len(history)<br class="calibre2"/>    print "The history of the algorithm:"<br class="calibre2"/>    (point_groups, centroids) = history[len(history) - 1]<br class="calibre2"/>    # Print all the history.<br class="calibre2"/>    print_cluster_history(history)<br class="calibre2"/>    # But display the situation graphically at the last step only.<br class="calibre2"/>    draw(point_groups, centroids)<br class="calibre2"/>else:<br class="calibre2"/>    (point_groups, centroids) = history[step]<br class="calibre2"/>    print "Data for the step number", step, ":"<br class="calibre2"/>    print point_groups, centroids<br class="calibre2"/>    draw(point_groups, centroids)</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Input data from gender classification</h1>
                
            
            
                
<p class="mce-root">We save data from the gender classification example into the CSV file as follows:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/5/persons_by_height_and_weight.csv
</strong>180,75
174,71
184,83
168,63
178,70
170,59
164,53
155,46
162,52
166,55
172,60</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Program output for gender classification data</h1>
                
            
            
                
<p class="mce-root">We run the program, implementing the <em class="calibre18">k</em>-means clustering algorithm on the data from the gender classification example. The numerical argument <kbd class="calibre17">2</kbd> means that we would like to cluster the data into two clusters, as can be seen in the following block of code:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py persons_by_height_weight.csv 2 last
</strong>The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((180.0, 75.0), 0), ((174.0, 71.0), 0), ((184.0, 83.0), 0), ((168.0, 63.0), 0), ((178.0, 70.0), 0), ((170.0, 59.0), 0), ((164.0, 53.0), 1), ((155.0, 46.0), 1), ((162.0, 52.0), 1), ((166.0, 55.0), 1), ((172.0, 60.0), 0)]
centroids = [(180.0, 75.0), (155.0, 46.0)]
Step number 1: point_groups = [((180.0, 75.0), 0), ((174.0, 71.0), 0), ((184.0, 83.0), 0), ((168.0, 63.0), 0), ((178.0, 70.0), 0), ((170.0, 59.0), 0), ((164.0, 53.0), 1), ((155.0, 46.0), 1), ((162.0, 52.0), 1), ((166.0, 55.0), 1), ((172.0, 60.0), 0)]
centroids = [(175.14285714285714, 68.71428571428571), (161.75, 51.5)]</pre>
<p class="mce-root">The program also outputs a graph, visible in <em class="calibre18">Figure 5.2.</em> The <kbd class="calibre17">last</kbd> parameter means that we would like the program to perform clustering until the final step. If we would like to display only the first step (step 0), we could change the last to <kbd class="calibre17">0</kbd> in order to run, as can be seen in the following code:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py persons_by_height_weight.csv 2 0</strong></pre>
<p class="mce-root">Upon execution of the program, we would get a graph of the clusters and their centroids in the initial step, as in <em class="calibre18">Figure 5.1</em>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">House ownership – choosing the number of clusters</h1>
                
            
            
                
<p class="mce-root">Let's take the example from the first chapter regarding house ownership:</p>
<table border="1" class="msotablegrid">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25"><strong class="calibre3">Age</strong></td>
<td class="calibre25"><strong class="calibre3">Annual income in USD</strong></td>
<td class="calibre25"><strong class="calibre3">House ownership status</strong></td>
</tr>
<tr class="calibre27">
<td class="calibre25">23</td>
<td class="calibre25">50,000</td>
<td class="calibre25">Non-owner</td>
</tr>
<tr class="calibre24">
<td class="calibre25">37</td>
<td class="calibre25">34,000</td>
<td class="calibre25">Non-owner</td>
</tr>
<tr class="calibre27">
<td class="calibre25">48</td>
<td class="calibre25">40,000</td>
<td class="calibre25">Owner</td>
</tr>
<tr class="calibre24">
<td class="calibre25">52</td>
<td class="calibre25">30,000</td>
<td class="calibre25">Non-owner</td>
</tr>
<tr class="calibre27">
<td class="calibre25">28</td>
<td class="calibre25">95,000</td>
<td class="calibre25">Owner</td>
</tr>
<tr class="calibre24">
<td class="calibre25">25</td>
<td class="calibre25">78,000</td>
<td class="calibre25">Non-owner</td>
</tr>
<tr class="calibre27">
<td class="calibre25">35</td>
<td class="calibre25">13,0000</td>
<td class="calibre25">Owner</td>
</tr>
<tr class="calibre24">
<td class="calibre25">32</td>
<td class="calibre25">10,5000</td>
<td class="calibre25">Owner</td>
</tr>
<tr class="calibre27">
<td class="calibre25">20</td>
<td class="calibre25">10,0000</td>
<td class="calibre25">Non-owner</td>
</tr>
<tr class="calibre24">
<td class="calibre25">40</td>
<td class="calibre25">60,000</td>
<td class="calibre25">Owner</td>
</tr>
<tr class="calibre33">
<td class="calibre25">50</td>
<td class="calibre25">80,000</td>
<td class="calibre25">Peter</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">We would like to predict whether Peter is a house owner using clustering.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Analysis</h1>
                
            
            
                
<p class="mce-root">Just as in the first chapter, we will have to scale the data, since the <em class="calibre18">income</em> axis is significantly greater and thus would diminish the impact of the <em class="calibre18">age</em> axis, which actually has a good predictive power in this kind of problem. This is because it is expected that older people have had more time to settle down, save money, and buy a house, as compared to younger people.</p>
<p class="mce-root">We apply the same rescaling from <a href="e0824a1e-65dc-4fee-a0e5-56170fdb36b9.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 1</a>, <em class="calibre18">Classification Using K Nearest Neighbors</em>, and obtain the following table:</p>
<table border="1" class="msotablegrid">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25"><strong class="calibre3">Age</strong></td>
<td class="calibre25"><strong class="calibre3">Scaled age</strong></td>
<td class="calibre25"><strong class="calibre3">Annual income in USD</strong></td>
<td class="calibre25"><strong class="calibre3">Scaled annual income</strong></td>
<td class="calibre25"><strong class="calibre3">House ownership status</strong></td>
</tr>
<tr class="calibre27">
<td class="calibre25">23</td>
<td class="calibre25">0.09375</td>
<td class="calibre25">50000</td>
<td class="calibre25">0.2</td>
<td class="calibre25">non-owner</td>
</tr>
<tr class="calibre24">
<td class="calibre25">37</td>
<td class="calibre25">0.53125</td>
<td class="calibre25">34000</td>
<td class="calibre25">0.04</td>
<td class="calibre25">non-owner</td>
</tr>
<tr class="calibre27">
<td class="calibre25">48</td>
<td class="calibre25">0.875</td>
<td class="calibre25">40000</td>
<td class="calibre25">0.1</td>
<td class="calibre25">owner</td>
</tr>
<tr class="calibre24">
<td class="calibre25">52</td>
<td class="calibre25">1</td>
<td class="calibre25">30000</td>
<td class="calibre25">0</td>
<td class="calibre25">non-owner</td>
</tr>
<tr class="calibre27">
<td class="calibre25">28</td>
<td class="calibre25">0.25</td>
<td class="calibre25">95000</td>
<td class="calibre25">0.65</td>
<td class="calibre25">owner</td>
</tr>
<tr class="calibre24">
<td class="calibre25">25</td>
<td class="calibre25">0.15625</td>
<td class="calibre25">78000</td>
<td class="calibre25">0.48</td>
<td class="calibre25">non-owner</td>
</tr>
<tr class="calibre27">
<td class="calibre25">35</td>
<td class="calibre25">0.46875</td>
<td class="calibre25">130000</td>
<td class="calibre25">1</td>
<td class="calibre25">owner</td>
</tr>
<tr class="calibre24">
<td class="calibre25">32</td>
<td class="calibre25">0.375</td>
<td class="calibre25">105000</td>
<td class="calibre25">0.75</td>
<td class="calibre25">owner</td>
</tr>
<tr class="calibre27">
<td class="calibre25">20</td>
<td class="calibre25">0</td>
<td class="calibre25">100000</td>
<td class="calibre25">0.7</td>
<td class="calibre25">non-owner</td>
</tr>
<tr class="calibre24">
<td class="calibre25">40</td>
<td class="calibre25">0.625</td>
<td class="calibre25">60000</td>
<td class="calibre25">0.3</td>
<td class="calibre25">owner</td>
</tr>
<tr class="calibre33">
<td class="calibre25">50</td>
<td class="calibre25">0.9375</td>
<td class="calibre25">80000</td>
<td class="calibre25">0.5</td>
<td class="calibre25">?</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">Given the table, we produce the input file for the algorithm and execute it, clustering the features into two clusters.<br class="calibre10"/>
<strong class="calibre8">Input</strong>:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/5/house_ownership2.csv
</strong>0.09375,0.2
0.53125,0.04
0.875,0.1
1,0
0.25,0.65
0.15625,0.48
0.46875,1
0.375,0.75
0,0.7
0.625,0.3
0.9375,0.5</pre>
<p class="mce-root"><strong class="calibre8">Output for two clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py house_ownership2.csv 2 last
</strong>The total number of steps: 3
The history of the algorithm:
Step number 0: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 0), ((0.15625, 0.48), 0), ((0.46875, 1.0), 0), ((0.375, 0.75), 0), ((0.0, 0.7), 0), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.09375, 0.2), (1.0, 0.0)]
Step number 1: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 1), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 0), ((0.15625, 0.48), 0), ((0.46875, 1.0), 0), ((0.375, 0.75), 0), ((0.0, 0.7), 0), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.26785714285714285, 0.5457142857142857), (0.859375, 0.225)]
Step number 2: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 1), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 0), ((0.15625, 0.48), 0), ((0.46875, 1.0), 0), ((0.375, 0.75), 0), ((0.0, 0.7), 0), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.22395833333333334, 0.63), (0.79375, 0.188)]</pre>
<div><img class="image-border4" src="img/d27bf970-4129-4f73-a650-d9109a755968.png" width="800" height="600"/></div>
<p class="mce-root">The blue cluster contains scaled features – <em class="calibre18">(0.09375,0.2)</em>, <em class="calibre18">(0.25,0.65)</em>, <em class="calibre18">(0.15625,0.48)</em>, <em class="calibre18">(0.46875,1)</em>, <em class="calibre18">(0.375,0.75)</em>, and <em class="calibre18">(0,0.7),</em> and unscaled ones – <em class="calibre18">(23,50000)</em>, <em class="calibre18">(28,95000)</em>, <em class="calibre18">(25,78000)</em>, <em class="calibre18">(35,130000)</em>, <em class="calibre18">(32,105000)</em>, and <em class="calibre18">(20,100000)</em>. The red cluster contains scaled features –<em class="calibre18">(0.53125,0.04)</em>, <em class="calibre18">(0.875,0.1)</em>, <em class="calibre18">(1,0)</em>, <em class="calibre18">(0.625,0.3)</em>, and <em class="calibre18">(0.9375,0.5),</em> and unscaled ones <em class="calibre18">(37,34000)</em>, <em class="calibre18">(48,40000)</em>, <em class="calibre18">(52,30000)</em>, <em class="calibre18">(40,60000)</em>, and <em class="calibre18">(50,80000)</em>.</p>
<p class="mce-root">So, Peter belongs to the red cluster. What is the proportion of house owners in the red cluster, not counting Peter? 2/4, or 1/2, of the people in the red cluster are house owners. Thus, the red cluster, to which Peter belongs, does not seem to have a high predictive power in determining whether Peter would be a house owner or not. We may try to cluster the data into more clusters, in the hope that we would gain a purer cluster that could be more reliable for the prediction of house ownership for Peter. Therefore, let's try to cluster the data into three clusters.</p>
<p class="mce-root"><strong class="calibre8">Output for three clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py house_ownership2.csv 3 last
</strong>The total number of steps: 3
The history of the algorithm:
Step number 0: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 2), ((0.15625, 0.48), 0), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 0), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.09375, 0.2), (1.0, 0.0), (0.46875, 1.0)]
Step number 1: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 1), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 2), ((0.15625, 0.48), 0), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 2), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.1953125, 0.355), (0.859375, 0.225), (0.3645833333333333, 0.7999999999999999)]
Step number 2: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 1), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 2), ((0.15625, 0.48), 0), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 2), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.125, 0.33999999999999997), (0.79375, 0.188), (0.2734375, 0.7749999999999999)]</pre>
<div><img class="image-border5" src="img/4e4dbc70-3394-483c-9e88-fcfc3cbcd199.png" width="800" height="600"/></div>
<p class="mce-root">The red cluster has stayed the same. So let's cluster the data into four clusters.</p>
<p class="mce-root"><strong class="calibre8">Output for four clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py house_ownership2.csv 4 last
</strong>The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 3), ((0.15625, 0.48), 3), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 3), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.09375, 0.2), (1.0, 0.0), (0.46875, 1.0), (0.0, 0.7)]
Step number 1: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 3), ((0.15625, 0.48), 3), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 3), ((0.625, 0.3), 1), ((0.9375, 0.5), 1)]
centroids = [(0.3125, 0.12000000000000001), (0.859375, 0.225), (0.421875, 0.875), (0.13541666666666666, 0.61)]</pre>
<div><img class="image-border6" src="img/39fd3799-c004-48c0-bfa9-e87f0befb50b.png" width="800" height="600"/></div>
<p class="mce-root">Now, the red cluster, where Peter belongs, has changed. What is the proportion of house owners in the red cluster now? If we do not count Peter, 2/3 of people in the red cluster own a house. When we clustered into two or three clusters, the proportion was only ½, which did not tell us whether Peter is a house owner or not. Now, there is a majority of house owners in the red cluster, not counting Peter, so we have a stronger belief that Peter is also a house owner. However, 2/3 is still a relatively low confidence level for classifying Peter as a house owner. Let's cluster the data into five clusters to see what happens.</p>
<p class="mce-root"><strong class="calibre8">Output for five clusters:</strong></p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py house_ownership2.csv 5 last
</strong>The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 3), ((0.15625, 0.48), 3), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 3), ((0.625, 0.3), 4), ((0.9375, 0.5), 4)]
centroids = [(0.09375, 0.2), (1.0, 0.0), (0.46875, 1.0), (0.0, 0.7), (0.9375, 0.5)]
Step number 1: point_groups = [((0.09375, 0.2), 0), ((0.53125, 0.04), 0), ((0.875, 0.1), 1), ((1.0, 0.0), 1), ((0.25, 0.65), 3), ((0.15625, 0.48), 3), ((0.46875, 1.0), 2), ((0.375, 0.75), 2), ((0.0, 0.7), 3), ((0.625, 0.3), 4), ((0.9375, 0.5), 4)]
centroids = [(0.3125, 0.12000000000000001), (0.9375, 0.05), (0.421875, 0.875), (0.13541666666666666, 0.61), (0.78125, 0.4)]</pre>
<div><img class="image-border3" src="img/51db4dd3-3ecb-4aa4-a035-8dfeb21018fb.png" width="800" height="600"/></div>
<p class="mce-root">Now, the red cluster contains only Peter and a non-owner. This clustering suggests that Peter is more likely a non-owner as well. However, according to the previous cluster, Peter is more likely be an owner of a house. Therefore, it may not be so clear whether Peter owns a house or not. Collecting more data would improve our analysis and should be carried out before making a definite classification of this problem.</p>
<p class="mce-root">From our analysis, we noticed that a different number of clusters can result in a different result for a classification, since the nature of members in an individual cluster can change. After collecting more data, we should perform cross-validation to determine the number of clusters that classifies the data with the highest accuracy.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Document clustering – understanding the number of k clusters in a semantic context</h1>
                
            
            
                
<p class="mce-root">We are given the following information about the frequency counts for the words <em class="calibre18">money</em> and <em class="calibre18">god(s)</em> in the following 17 books from the Project Gutenberg library:</p>
<table border="1" class="msotablegrid">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre51">
<p class="calibre26"><strong class="calibre8">Book number</strong></p>
</td>
<td class="calibre52">
<p class="calibre26"><strong class="calibre8">Book name</strong></p>
</td>
<td class="calibre40">
<p class="calibre26"><strong class="calibre8">Money as a %</strong></p>
</td>
<td class="calibre40">
<p class="calibre26"><strong class="calibre8">God(s) as a %</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre51">
<p class="calibre26">1</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The Vedanta-Sutras, with the commentary by</em></p>
<p class="calibre26"><em class="calibre18">Ramanuja</em>, by Trans. George Thibaut</p>
</td>
<td class="calibre40">
<p class="calibre26">0</p>
</td>
<td class="calibre40">
<p class="calibre26">0.07</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre51">
<p class="calibre26">2</p>
</td>
<td class="calibre52">
<p class="calibre26"> </p>
<p class="calibre26"><em class="calibre18">The Mahabharata of Krishna-Dwaipayana Vyasa</em></p>
<p class="calibre26"><em class="calibre18">-Adi Parva</em>, by Kisari Mohan Ganguli</p>
<p class="calibre26"> </p>
</td>
<td class="calibre40">
<p class="calibre26">0</p>
</td>
<td class="calibre40">
<p class="calibre26">0.17</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre51">
<p class="calibre26">3</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The Mahabharata of Krishna-Dwaipayana</em></p>
<p class="calibre26"><em class="calibre18">Vyasa, Pt. 2</em>, by Krishna-Dwaipayana Vyasa</p>
</td>
<td class="calibre40">
<p class="calibre26">0.01</p>
</td>
<td class="calibre40">
<p class="calibre26">0.10</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre51">
<p class="calibre26">4</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The Mahabharata of Krishna-Dwaipayana Vyasa Bk.</em></p>
<p class="calibre26"><em class="calibre18">3 Pt. 1</em>, by Krishna-Dwaipayana Vyasa</p>
</td>
<td class="calibre40">
<p class="calibre26">0</p>
</td>
<td class="calibre40">
<p class="calibre26">0.32</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre51">
<p class="calibre26">5</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The Mahabharata of Krishna-Dwaipayana Vyasa</em></p>
<p class="calibre26"><em class="calibre18">Bk. 4</em>, by Kisari Mohan Ganguli</p>
</td>
<td class="calibre40">
<p class="calibre26">0</p>
</td>
<td class="calibre40">
<p class="calibre26">0.06</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre51">
<p class="calibre26">6</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The Mahabharata of Krishna-Dwaipayana Vyasa</em></p>
<p class="calibre26"><em class="calibre18">Bk. 3 Pt. 2</em>, translated by Kisari Mohan Ganguli</p>
</td>
<td class="calibre40">
<p class="calibre26">0</p>
</td>
<td class="calibre40">
<p class="calibre26">0.27</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre51">
<p class="calibre26">7</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The Vedanta-Sutras, with commentary</em> by</p>
<p class="calibre26">Sankaracarya</p>
</td>
<td class="calibre40">
<p class="calibre26">0</p>
</td>
<td class="calibre40">
<p class="calibre26">0.06</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre51">
<p class="calibre26">8</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The King James Bible</em></p>
</td>
<td class="calibre40">
<p class="calibre26">0.02</p>
</td>
<td class="calibre40">
<p class="calibre26">0.59</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre51">
<p class="calibre26">9</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">Paradise Regained</em>, by John Milton</p>
</td>
<td class="calibre40">
<p class="calibre26">0.02</p>
</td>
<td class="calibre40">
<p class="calibre26">0.45</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre51">
<p class="calibre26">10</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">Imitation of Christ</em>, by Thomas A Kempis</p>
</td>
<td class="calibre40">
<p class="calibre26">0.01</p>
</td>
<td class="calibre40">
<p class="calibre26">0.69</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre51">
<p class="calibre26">11</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The Koran</em>, as translated by Rodwell</p>
</td>
<td class="calibre40">
<p class="calibre26">0.01</p>
</td>
<td class="calibre40">
<p class="calibre26">1.72</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre51">
<p class="calibre26">12</p>
</td>
<td class="calibre52">
<p class="calibre26">The Adventures of Tom Sawyer, complete, by</p>
<p class="calibre26">Mark Twain (Samuel Clemens)</p>
</td>
<td class="calibre40">
<p class="calibre26">0.05</p>
</td>
<td class="calibre40">
<p class="calibre26">0.01</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre51">
<p class="calibre26">13</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The Adventures of Huckleberry Finn</em>, complete,</p>
<p class="calibre26">by Mark Twain (Samuel Clemens)</p>
</td>
<td class="calibre40">
<p class="calibre26">0.08</p>
</td>
<td class="calibre40">
<p class="calibre26">0</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre51">
<p class="calibre26">14</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">Great Expectations</em>, by Charles Dickens</p>
</td>
<td class="calibre40">
<p class="calibre26">0.04</p>
</td>
<td class="calibre40">
<p class="calibre26">0.01</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre51">
<p class="calibre26">15</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The Picture of Dorian Gray</em>, by Oscar Wilde</p>
</td>
<td class="calibre40">
<p class="calibre26">0.03</p>
</td>
<td class="calibre40">
<p class="calibre26">0.03</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre51">
<p class="calibre26">16</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">The Adventures of Sherlock Holmes</em>, by Arthur Conan Doyle</p>
</td>
<td class="calibre40">
<p class="calibre26">0.04</p>
</td>
<td class="calibre40">
<p class="calibre26">0.03</p>
</td>
</tr>
<tr class="calibre33">
<td class="calibre51">
<p class="calibre26">17</p>
</td>
<td class="calibre52">
<p class="calibre26"><em class="calibre18">Metamorphosi</em><em class="calibre18">s</em>, by Franz Kafka</p>
<p class="calibre26">Translated by David Wyllie</p>
</td>
<td class="calibre40">
<p class="calibre26">0.06</p>
</td>
<td class="calibre40">
<p class="calibre26">0.03</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">We would like to cluster this dataset, based on the chosen frequency counts of the words, into groups according to their semantic context.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Analysis</h1>
                
            
            
                
<p class="mce-root">First, we will perform rescaling, since the highest frequency count of the word <em class="calibre18">money</em> is 0.08 percent, whereas the highest frequency count of the word "god(s)" is 1.72%. So, we will divide the frequency count of money by 0.08, and the frequency count of god(s) by 1.72, as follows:</p>
<table border="1" class="msotablegrid">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25"><strong class="calibre3">Book number</strong></td>
<td class="calibre25"><strong class="calibre3">Money scaled</strong></td>
<td class="calibre25"><strong class="calibre3">God(s) scaled</strong></td>
</tr>
<tr class="calibre27">
<td class="calibre25">1</td>
<td class="calibre25">0</td>
<td class="calibre25">0.0406976744</td>
</tr>
<tr class="calibre24">
<td class="calibre25">2</td>
<td class="calibre25">0</td>
<td class="calibre25">0.0988372093</td>
</tr>
<tr class="calibre27">
<td class="calibre25">3</td>
<td class="calibre25">0.125</td>
<td class="calibre25">0.0581395349</td>
</tr>
<tr class="calibre24">
<td class="calibre25">4</td>
<td class="calibre25">0</td>
<td class="calibre25">0.1860465116</td>
</tr>
<tr class="calibre27">
<td class="calibre25">5</td>
<td class="calibre25">0</td>
<td class="calibre25">0.0348837209</td>
</tr>
<tr class="calibre24">
<td class="calibre25">6</td>
<td class="calibre25">0</td>
<td class="calibre25">0.1569767442</td>
</tr>
<tr class="calibre27">
<td class="calibre25">7</td>
<td class="calibre25">0</td>
<td class="calibre25">0.0348837209</td>
</tr>
<tr class="calibre24">
<td class="calibre25">8</td>
<td class="calibre25">0.25</td>
<td class="calibre25">0.3430232558</td>
</tr>
<tr class="calibre27">
<td class="calibre25">9</td>
<td class="calibre25">0.25</td>
<td class="calibre25">0.261627907</td>
</tr>
<tr class="calibre24">
<td class="calibre25">10</td>
<td class="calibre25">0.125</td>
<td class="calibre25">0.4011627907</td>
</tr>
<tr class="calibre27">
<td class="calibre25">11</td>
<td class="calibre25">0.125</td>
<td class="calibre25">1</td>
</tr>
<tr class="calibre24">
<td class="calibre25">12</td>
<td class="calibre25">0.625</td>
<td class="calibre25">0.0058139535</td>
</tr>
<tr class="calibre27">
<td class="calibre25">13</td>
<td class="calibre25">1</td>
<td class="calibre25">0</td>
</tr>
<tr class="calibre24">
<td class="calibre25">14</td>
<td class="calibre25">0.5</td>
<td class="calibre25">0.0058139535</td>
</tr>
<tr class="calibre27">
<td class="calibre25">15</td>
<td class="calibre25">0.375</td>
<td class="calibre25">0.0174418605</td>
</tr>
<tr class="calibre24">
<td class="calibre25">16</td>
<td class="calibre25">0.5</td>
<td class="calibre25">0.0174418605</td>
</tr>
<tr class="calibre33">
<td class="calibre25">17</td>
<td class="calibre25">0.75</td>
<td class="calibre25">0.0174418605</td>
</tr>
</tbody>
</table>
<p class="mce-root">Now that we have rescaled the data, let's apply the <em class="calibre18">k</em>-means clustering algorithm by trying to divide the data into a different amount of clusters.</p>
<p class="mce-root"><strong class="calibre8">Input</strong>:</p>
<pre class="calibre22"><strong class="calibre3">source_code/5/document_clustering/word_frequencies_money_god_scaled.csv
</strong>0,0.0406976744
0,0.0988372093
0.125,0.0581395349
0,0.1860465116
0,0.0348837209
0,0.1569767442
0,0.0348837209
0.25,0.3430232558
0.25,0.261627907
0.125,0.4011627907
0.125,1
0.625,0.0058139535
1,0
0.5,0.0058139535
0.375,0.0174418605
0.5,0.0174418605
0.75,0.0174418605</pre>
<p class="mce-root"><strong class="calibre8">Output for two clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py document_clustering/word_frequencies_money_god_scaled.csv 2 last
</strong>The total number of steps: 3
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 0), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 0), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.0, 0.0406976744), (1.0, 0.0)]
Step number 1: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 0), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 1), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.10416666666666667, 0.21947674418333332), (0.675, 0.0093023256)]
Step number 2: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 0), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 1), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.07954545454545454, 0.2378435517909091), (0.625, 0.01065891475)]</pre>
<div><img class="image-border7" src="img/42911ca1-e009-4853-a961-b7eda68a1a91.png" width="800" height="600"/></div>
<p class="mce-root">We can observe that clustering into two clusters divides books into religious ones, which are the ones in the blue cluster, and non-religious ones, which are the ones in the red cluster. Let's try to cluster the books into three clusters to observe how the algorithm will divide up the data.</p>
<p class="mce-root"><strong class="calibre8">Output for three clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py document_clustering/word_frequencies_money_god_scaled.csv 3 last
</strong>The total number of steps: 3
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 2), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 0), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.0, 0.0406976744), (1.0, 0.0), (0.125, 1.0)]
Step number 1: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 2), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 1), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.10227272727272728, 0.14852008456363636), (0.675, 0.0093023256), (0.125, 1.0)]
Step number 2: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 0), ((0.25, 0.261627907), 0), ((0.125, 0.4011627907), 0), ((0.125, 1.0), 2), ((0.625, 0.0058139535), 1), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 1), ((0.375, 0.0174418605), 1), ((0.5, 0.0174418605), 1), ((0.75, 0.0174418605), 1)]
centroids = [(0.075, 0.16162790697), (0.625, 0.01065891475), (0.125, 1.0)]</pre>
<div><img class="image-border8" src="img/baabeb91-1761-4629-b5d2-02c942e15b85.png" width="800" height="600"/></div>
<p class="mce-root">This time, the algorithm separated The Koran, from the religious books, into a green cluster. This is because, in fact, the word <em class="calibre18">god</em> is the fifth most commonly occurring word in The Koran. The clustering here happens to divide the books according to the writing style they were written in. Clustering into four clusters separates one book, which has a relatively high frequency of the word <em class="calibre18">money</em>, from the red cluster of non-religious books into a separate cluster. Now let's look at clustering into five clusters.</p>
<p class="mce-root"><strong class="calibre8">Output for five clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py word_frequencies_money_god_scaled.csv 5 last
</strong>The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 4), ((0.25, 0.261627907), 4), ((0.125, 0.4011627907), 4), ((0.125, 1.0), 2), ((0.625, 0.0058139535), 3), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 3), ((0.375, 0.0174418605), 3), ((0.5, 0.0174418605), 3), ((0.75, 0.0174418605), 3)]
centroids = [(0.0, 0.0406976744), (1.0, 0.0), (0.125, 1.0), (0.5, 0.0174418605), (0.25, 0.3430232558)]
Step number 1: point_groups = [((0.0, 0.0406976744), 0), ((0.0, 0.0988372093), 0), ((0.125, 0.0581395349), 0), ((0.0, 0.1860465116), 0), ((0.0, 0.0348837209), 0), ((0.0, 0.1569767442), 0), ((0.0, 0.0348837209), 0), ((0.25, 0.3430232558), 4), ((0.25, 0.261627907), 4), ((0.125, 0.4011627907), 4), ((0.125, 1.0), 2), ((0.625, 0.0058139535), 3), ((1.0, 0.0), 1), ((0.5, 0.0058139535), 3), ((0.375, 0.0174418605), 3), ((0.5, 0.0174418605), 3), ((0.75, 0.0174418605), 3)]
centroids = [(0.017857142857142856, 0.08720930231428571), (1.0, 0.0), (0.125, 1.0), (0.55, 0.0127906977), (0.20833333333333334, 0.3352713178333333)]</pre>
<div><img class="image-border9" src="img/ba0f2f79-1c06-45b9-97fa-73b1a7e1ff2b.png" width="800" height="600"/></div>
<p class="mce-root"/>
<p class="mce-root">This clustering further divides the blue cluster of the remaining religious books into a blue cluster of Hindu books and a gray cluster of Christian books.</p>
<p class="mce-root">We can use clustering in this way to group items with similar properties and then find similar items quickly based on the given example. The granularity of the clustering under the parameter <em class="calibre18">k</em> determines how similar we can expect the items in a group to be. The higher the parameter, the more similar the items in the cluster are going to be, albeit in a smaller number.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, we learned how the clustering of data is very efficient and can be used to facilitate the faster classification of new features by classifying a feature as belonging to the class that is represented in the cluster of that feature. An appropriate number of clusters can be determined through cross-validation, by choosing the one that results in the most accurate classification.</p>
<p class="mce-root">Clustering orders data according to its similarity. The more clusters there are, the greater the similarity between the features in a cluster, but the fewer features in a cluster there are.</p>
<p class="mce-root">We also learned that the <em class="calibre18">k</em>-means algorithm is a clustering algorithm that tries to cluster features in such a way that the mutual distance of the features in a cluster is minimized. To do this, the algorithm computes the centroid of each cluster and a feature belongs to the cluster whose centroid is closest to it. The algorithm finishes the computation of the clusters as soon as they or their centroids no longer change.</p>
<p class="mce-root">In the next chapter, we will analyze the relationship between dependent variables using mathematical regression. Unlike with the classification and clustering algorithms, regression analysis will be used to estimate the most probable value of a variable, such as weight, distance, or temperature.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Problems</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre8">Problem 1</strong>: Compute the centroid of the following clusters:</p>
<p class="calibre36">a) 2, 3, 4</p>
<p class="calibre36">b) USD 100, USD 400, USD 1,000</p>
<p class="calibre36">c) (10,20), (40, 60), (0, 40)</p>
<p class="calibre36">d) (USD 200, 40 km), (USD 300, 60 km), (USD 500, 100 km), (USD 250, 200 km)</p>
<p class="calibre36">e) (1,2,4), (0,0,3), (10,20,5), (4,8,2), (5,0,1)</p>
<p class="mce-root"><strong class="calibre8">Problem 2</strong>: Cluster the following datasets into two, three, and four clusters using the <em class="calibre18">k</em>-means clustering algorithm:</p>
<p class="calibre36">a) 0, 2, 5, 4, 8, 10, 12, 11</p>
<p class="calibre36">b) (2,2), (2,5), (10,4), (3,5), (7,3), (5,9), (2,8), (4,10), (7,4), (4,4), (5,8), (9,3)</p>
<p class="mce-root"><strong class="calibre8">Problem 3</strong>: We are given the ages of the couples and the number of children they have:</p>
<table border="1" class="msotablegrid">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<div><strong class="calibre3">Couple number</strong></div>
</td>
<td class="calibre25">
<div><strong class="calibre3">Wife's age</strong></div>
</td>
<td class="calibre25">
<div><strong class="calibre3">Husband's age</strong></div>
</td>
<td class="calibre25">
<div><strong class="calibre3">Number of children</strong></div>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p>1</div>
</td>
<td class="calibre25">
<div>48</p>
</td>
<td class="calibre25">
<p>49</p>
</td>
<td class="calibre25">
<p>5</div>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<div>2</p>
</td>
<td class="calibre25">
<p>40</p>
</td>
<td class="calibre25">
<p>43</p>
</td>
<td class="calibre25">
<p>2</div>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<div>3</p>
</td>
<td class="calibre25">
<p>24</p>
</td>
<td class="calibre25">
<p>28</p>
</td>
<td class="calibre25">
<p>1</div>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<div>4</p>
</td>
<td class="calibre25">
<p>49</p>
</td>
<td class="calibre25">
<p>42</p>
</td>
<td class="calibre25">
<p>3</div>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<div>5</p>
</td>
<td class="calibre25">
<p>32</p>
</td>
<td class="calibre25">
<p>34</p>
</td>
<td class="calibre25">
<p>0</div>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<div>6</p>
</td>
<td class="calibre25">
<p>24</p>
</td>
<td class="calibre25">
<p>27</p>
</td>
<td class="calibre25">
<p>0</div>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<div>7</p>
</td>
<td class="calibre25">
<p>29</p>
</td>
<td class="calibre25">
<p>32</p>
</td>
<td class="calibre25">
<p>2</div>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<div>8</p>
</td>
<td class="calibre25">
<p>35</p>
</td>
<td class="calibre25">
<p>35</p>
</td>
<td class="calibre25">
<p>2</div>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<div>9</p>
</td>
<td class="calibre25">
<p>33</p>
</td>
<td class="calibre25">
<p>36</p>
</td>
<td class="calibre25">
<p>1</div>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<div>10</p>
</td>
<td class="calibre25">
<p>42</p>
</td>
<td class="calibre25">
<p>47</p>
</td>
<td class="calibre25">
<p>3</div>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<div>11</p>
</td>
<td class="calibre25">
<p>22</p>
</td>
<td class="calibre25">
<p>27</p>
</td>
<td class="calibre25">
<p>2</div>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<div>12</p>
</td>
<td class="calibre25">
<p>41</p>
</td>
<td class="calibre25">
<p>45</p>
</td>
<td class="calibre25">
<p>4</div>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<div>13</p>
</td>
<td class="calibre25">
<p>39</p>
</td>
<td class="calibre25">
<p>43</p>
</td>
<td class="calibre25">
<p>4</div>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<div>14</p>
</td>
<td class="calibre25">
<p>36</p>
</td>
<td class="calibre25">
<p>38</p>
</td>
<td class="calibre25">
<p>2</div>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<div>15</p>
</td>
<td class="calibre25">
<p>30</p>
</td>
<td class="calibre25">
<p>32</p>
</td>
<td class="calibre25">
<p>1</div>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<div>16</p>
</td>
<td class="calibre25">
<p>36</p>
</td>
<td class="calibre25">
<p>38</p>
</td>
<td class="calibre25">
<p>0</div>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<div>17</p>
</td>
<td class="calibre25">
<p>36</p>
</td>
<td class="calibre25">
<p>39</p>
</td>
<td class="calibre25">
<p>3</div>
</td>
</tr>
<tr class="calibre28">
<td class="calibre25">
<div>18</p>
</td>
<td class="calibre25">
<p>37</p>
</td>
<td class="calibre25">
<p>38</p>
</td>
<td class="calibre25">
<p>?</div>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">We would like to guess, using clustering, how many children a couple has where the age of the husband is 37 and the age of the wife is 38.</p>


            

            
        
    </p>



  
<div><h1 class="header-title">Analysis</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre8">Problem 1</strong>:  a) <strong class="calibre8"><img class="fm-editor-equation135" src="img/dbe2db56-78ad-4fb8-874e-3b405828a540.png" width="1820" height="220"/></strong><br class="calibre10"/>
b) <img class="fm-editor-equation136" src="img/05c3cee4-8cb6-42a3-a46d-1762b351aedd.png" width="3030" height="220"/><br class="calibre10"/>c)  <img class="fm-editor-equation137" src="img/e5697e9f-d754-41e6-b0a4-1a9f1fb72978.png" width="5170" height="220"/></p>
<p class="mce-root"><br class="calibre10"/>d)<strong class="calibre8"><img class="fm-editor-equation138" src="img/cc01cdbe-b8d3-4609-ace6-a4054f085f4f.png" width="7480" height="750"/></strong></p>
<p class="mce-root"><br class="calibre10"/>
e) <img class="fm-editor-equation139" src="img/135753ed-7971-4a68-998f-e25309588af5.png" width="6450" height="220"/>  </p>
<p class="mce-root"><strong class="calibre8">Problem 2</strong>: a) We add a second coordinate and set it to <kbd class="calibre17">0</kbd> for all the features. This way, the distance between the features does not change and we can use the clustering algorithm we implemented earlier in this chapter.</p>
<p class="mce-root"><strong class="calibre8">Input</strong>:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/5/problem5_2.csv
</strong>0,0
2,0
5,0
4,0
8,0
10,0
12,0
11,0</pre>
<p class="mce-root"><strong class="calibre8">For two clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py problem5_2.csv 2 last
</strong>The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 0), ((4.0, 0.0), 0), ((8.0, 0.0), 1), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(0.0, 0.0), (12.0, 0.0)]
Step number 1: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 0), ((4.0, 0.0), 0), ((8.0, 0.0), 1), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(2.75, 0.0), (10.25, 0.0)]</pre>
<p class="mce-root"><strong class="calibre8">For three clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py problem5_2.csv 3 last
</strong>The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 2), ((4.0, 0.0), 2), ((8.0, 0.0), 2), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(0.0, 0.0), (12.0, 0.0), (5.0, 0.0)]
Step number 1: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 2), ((4.0, 0.0), 2), ((8.0, 0.0), 2), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(1.0, 0.0), (11.0, 0.0), (5.666666666666667, 0.0)]</pre>
<p class="mce-root"><strong class="calibre8">For four clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py problem5_2.csv 4 last
</strong>The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 2), ((4.0, 0.0), 2), ((8.0, 0.0), 3), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(0.0, 0.0), (12.0, 0.0), (5.0, 0.0), (8.0, 0.0)]
Step number 1: point_groups = [((0.0, 0.0), 0), ((2.0, 0.0), 0), ((5.0, 0.0), 2), ((4.0, 0.0), 2), ((8.0, 0.0), 3), ((10.0, 0.0), 1), ((12.0, 0.0), 1), ((11.0, 0.0), 1)]
centroids = [(1.0, 0.0), (11.0, 0.0), (4.5, 0.0), (8.0, 0.0)]</pre>
<p class="mce-root">b) We use the implemented algorithm again.</p>
<p class="mce-root"><strong class="calibre8">Input</strong>:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/5/problem5_2b.csv
</strong>2,2
2,5
10,4
3,5
7,3
5,9
2,8
4,10
7,4
4,4
5,8
9,3</pre>
<p class="mce-root"><strong class="calibre8">Output for two clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py problem5_2b.csv 2 last
</strong>The total number of steps: 3
The history of the algorithm:
Step number 0: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 0), ((10.0, 4.0), 1), ((3.0, 5.0), 0), ((7.0, 3.0), 1), ((5.0, 9.0), 1), ((2.0, 8.0), 0), ((4.0, 10.0), 0), ((7.0, 4.0), 1), ((4.0, 4.0), 0), ((5.0, 8.0), 1), ((9.0, 3.0), 1)]
centroids = [(2.0, 2.0), (10.0, 4.0)]
Step number 1: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 0), ((10.0, 4.0), 1), ((3.0, 5.0), 0), ((7.0, 3.0), 1), ((5.0, 9.0), 0), ((2.0, 8.0), 0), ((4.0, 10.0), 0), ((7.0, 4.0), 1), ((4.0, 4.0), 0), ((5.0, 8.0), 0), ((9.0, 3.0), 1)]
centroids = [(2.8333333333333335, 5.666666666666667), (7.166666666666667, 5.166666666666667)]
Step number 2: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 0), ((10.0, 4.0), 1), ((3.0, 5.0), 0), ((7.0, 3.0), 1), ((5.0, 9.0), 0), ((2.0, 8.0), 0), ((4.0, 10.0), 0), ((7.0, 4.0), 1), ((4.0, 4.0), 0), ((5.0, 8.0), 0), ((9.0, 3.0), 1)]
centroids = [(3.375, 6.375), (8.25, 3.5)]</pre>
<p class="mce-root"><strong class="calibre8">Output for three clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py problem5_2b.csv 3 last
</strong>The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 0), ((10.0, 4.0), 1), ((3.0, 5.0), 0), ((7.0, 3.0), 1), ((5.0, 9.0), 2), ((2.0, 8.0), 2), ((4.0, 10.0), 2), ((7.0, 4.0), 1), ((4.0, 4.0), 0), ((5.0, 8.0), 2), ((9.0, 3.0), 1)]
centroids = [(2.0, 2.0), (10.0, 4.0), (4.0, 10.0)]
Step number 1: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 0), ((10.0, 4.0), 1), ((3.0, 5.0), 0), ((7.0, 3.0), 1), ((5.0, 9.0), 2), ((2.0, 8.0), 2), ((4.0, 10.0), 2), ((7.0, 4.0), 1), ((4.0, 4.0), 0), ((5.0, 8.0), 2), ((9.0, 3.0), 1)]
centroids = [(2.75, 4.0), (8.25, 3.5), (4.0, 8.75)]</pre>
<p class="mce-root"><strong class="calibre8">Output for four clusters</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python k-means_clustering.py problem5_2b.csv 4 last
</strong>The total number of steps: 2
The history of the algorithm:
Step number 0: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 3), ((10.0, 4.0), 1), ((3.0, 5.0), 3), ((7.0, 3.0), 1), ((5.0, 9.0), 2), ((2.0, 8.0), 2), ((4.0, 10.0), 2), ((7.0, 4.0), 1), ((4.0, 4.0), 3), ((5.0, 8.0), 2), ((9.0, 3.0), 1)]
centroids = [(2.0, 2.0), (10.0, 4.0), (4.0, 10.0), (3.0, 5.0)]
Step number 1: point_groups = [((2.0, 2.0), 0), ((2.0, 5.0), 3), ((10.0, 4.0), 1), ((3.0, 5.0), 3), ((7.0, 3.0), 1), ((5.0, 9.0), 2), ((2.0, 8.0), 2), ((4.0, 10.0), 2), ((7.0, 4.0), 1), ((4.0, 4.0), 3), ((5.0, 8.0), 2), ((9.0, 3.0), 1)]
centroids = [(2.0, 2.0), (8.25, 3.5), (4.0, 8.75), (3.0, 4.666666666666667)]</pre>
<p class="mce-root"><strong class="calibre8">Problem 3</strong>: We are given the ages of 17 couples and the number of children they have, and would like to find out how many children the 18<sup class="calibre30">th</sup> couple has. We will use the first 14 couples as training data, and the next 3 couples for cross-validation to determine the number of clusters <em class="calibre18">k</em> that we will use to find out how many children the 18<sup class="calibre30">th</sup> couple is expected to have.</p>
<p class="cdpalignleft1">After clustering, we will say that a couple is likely to have roughly the number of children that is the average in that cluster. Using cross-validation, we will choose the number of clusters that will minimize the difference between the actual and predicted number of children. We will capture this difference for all the items in the cluster cumulatively as the square root of the squares of the differences between the number of children of each couple. This will minimize the variance of the random variable in relation to the predicted number of children for the 18<sup class="calibre30">th</sup> couple.</p>
<p class="mce-root">We will perform clustering into two, three, four, and five clusters.</p>
<p class="mce-root"><strong class="calibre8">Input</strong>:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/5/couples_children.csv
</strong>48,49
40,43
24,28
49,42
32,34
24,27
29,32
35,35
33,36
42,47
22,27
41,45
39,43
36,38
30,32
36,38
36,39
37,38</pre>
<p class="mce-root"><strong class="calibre8">Output for two clusters</strong>:</p>
<p class="mce-root">A couple listed for a cluster is of the form <kbd class="calibre17">(couple_number,(wife_age,husband_age))</kbd>:</p>
<pre class="calibre22">Cluster 0: [(1, (48.0, 49.0)), (2, (40.0, 43.0)), (4, (49.0, 42.0)), (10, (42.0, 47.0)), (12, (41.0, 45.0)), (13, (39.0, 43.0)), (14, (36.0, 38.0)), (16, (36.0, 38.0)), (17, (36.0, 39.0)), (18, (37.0, 38.0))]
Cluster 1: [(3, (24.0, 28.0)), (5, (32.0, 34.0)), (6, (24.0, 27.0)), (7, (29.0, 32.0)), (8, (35.0, 35.0)), (9, (33.0, 36.0)), (11, (22.0, 27.0)), (15, (30.0, 32.0))]</pre>
<p class="mce-root">We would like to determine the expected number of children for the 15<sup class="calibre30">th</sup> couple <em class="calibre18">(30,32)</em>, that is, where the wife is 30 years old and the husband is 32 years old. <em class="calibre18">(30,32)</em> is in cluster 1. The couples in cluster 1 are as follows: <em class="calibre18">(24.0, 28.0)</em>, <em class="calibre18">(32.0, 34.0)</em>, <em class="calibre18">(24.0, 27.0)</em>, <em class="calibre18">(29.0, 32.0)</em>, <em class="calibre18">(35.0, 35.0)</em>, <em class="calibre18">(33.0, 36.0)</em>, <em class="calibre18">(22.0, 27.0)</em>, and <em class="calibre18">(30.0, 32.0)</em>. Of these, and the first 14 couples used for data purposes, the remaining couples are: <em class="calibre18">(24.0, 28.0)</em>, <em class="calibre18">(32.0, 34.0)</em>, <em class="calibre18">(24.0, 27.0)</em>, <em class="calibre18">(29.0, 32.0)</em>, <em class="calibre18">(35.0, 35.0)</em>, <em class="calibre18">(33.0, 36.0)</em>, and <em class="calibre18">(22.0, 27.0)</em>. The average number of children for these couples is <em class="calibre18">est15=8/7~1.14</em>. This is the estimated number of children for the 15<sup class="calibre30">th</sup> couple, based on the data from the first 14 couples.</p>
<p class="mce-root">The estimated number of children for the 16<sup class="calibre30">th</sup> couple is <em class="calibre18">est16=23/7~3.29</em>. The estimated number of children for the 17<sup class="calibre30">th</sup> couple is also <em class="calibre18">est17=23/7~3.29 </em>since both the 16<sup class="calibre30">th</sup> and 17<sup class="calibre30">th</sup> couples belong to the same cluster.</p>
<p class="mce-root">Now we will calculate the <em class="calibre18">E2</em> error (two for two clusters) between the estimated number of children (for example, denoted <em class="calibre18">est15</em> for the 15<sup class="calibre30">th</sup> couple) and the actual number of children (for example, denoted <em class="calibre18">act15</em> for the 15<sup class="calibre30">th</sup> couple) as follows:</p>
<p class="mce-root"><img class="fm-editor-equation140" src="img/002cecdc-82e4-40f7-9781-a478866c51c4.png" width="5870" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation141" src="img/b2ca6e56-f0de-42e9-9227-cb25a7fd8498.png" width="4790" height="220"/></p>
<p class="mce-root">Now that we have calculated the <em class="calibre18">E2</em> error, we will calculate the errors in terms of the estimation with the other clusters. We will choose the number of clusters containing the fewest errors to estimate the number of children for the 18<sup class="calibre30">th</sup> couple.</p>
<p class="mce-root"><strong class="calibre8">Output for three clusters</strong>:</p>
<pre class="calibre22">Cluster 0: [(1, (48.0, 49.0)), (2, (40.0, 43.0)), (4, (49.0, 42.0)), (10, (42.0, 47.0)), (12, (41.0, 45.0)), (13, (39.0, 43.0))]
Cluster 1: [(3, (24.0, 28.0)), (6, (24.0, 27.0)), (7, (29.0, 32.0)), (11, (22.0, 27.0)), (15, (30.0, 32.0))]
Cluster 2: [(5, (32.0, 34.0)), (8, (35.0, 35.0)), (9, (33.0, 36.0)), (14, (36.0, 38.0)), (16, (36.0, 38.0)), (17, (36.0, 39.0)), (18, (37.0, 38.0))]</pre>
<p class="mce-root">Now, the 15<sup class="calibre30">th</sup> couple is in cluster 1, the 16<sup class="calibre30">th</sup> couple is in cluster 2, and the 17<sup class="calibre30">th</sup> couple is in cluster 2. So the estimated number of children for each couple is <em class="calibre18">5/4=1.25</em>.</p>
<p class="mce-root">The <em class="calibre18">E3</em> error of the estimation is as follows:</p>
<p class="mce-root"><img class="fm-editor-equation142" src="img/72664168-009d-4b4f-a5e4-833789796640.png" width="4620" height="240"/></p>
<p class="mce-root"><strong class="calibre8">Output for four clusters</strong>:</p>
<pre class="calibre22">Cluster 0: [(1, (48.0, 49.0)), (4, (49.0, 42.0)), (10, (42.0, 47.0)), (12, (41.0, 45.0))]
Cluster 1: [(3, (24.0, 28.0)), (6, (24.0, 27.0)), (11, (22.0, 27.0))]
Cluster 2: [(2, (40.0, 43.0)), (13, (39.0, 43.0)), (14, (36.0, 38.0)), (16, (36.0, 38.0)), (17, (36.0, 39.0)), (18, (37.0, 38.0))]
Cluster 3: [(5, (32.0, 34.0)), (7, (29.0, 32.0)), (8, (35.0, 35.0)), (9, (33.0, 36.0)), (15, (30.0, 32.0))]</pre>
<p class="mce-root">The 15<sup class="calibre30">th</sup> couple is in cluster 3, the 16<sup class="calibre30">th</sup> is in cluster 2, and the 17<sup class="calibre30">th</sup> is in cluster 2. So, the estimated number of children for the 15<sup class="calibre30">th</sup> couple is <em class="calibre18">5/4=1.25</em>. The estimated number of children for the 16<sup class="calibre30">th</sup> and 17<sup class="calibre30">th</sup> couples is 8/3=2.67.</p>
<p class="mce-root">The <em class="calibre18">E4</em> error of the estimation is as follows:</p>
<p class="cdpaligncenter"><img class="fm-editor-equation143" src="img/f995b78a-8035-4b00-91cc-f4593b3795c8.png" width="4280" height="410"/></p>
<p class="mce-root"><strong class="calibre8">Output for five clusters</strong>:</p>
<pre class="calibre22">Cluster 0: [(1, (48.0, 49.0)), (4, (49.0, 42.0))]
Cluster 1: [(3, (24.0, 28.0)), (6, (24.0, 27.0)), (11, (22.0, 27.0))]
Cluster 2: [(8, (35.0, 35.0)), (9, (33.0, 36.0)), (14, (36.0, 38.0)), (16, (36.0, 38.0)), (17, (36.0, 39.0)), (18, (37.0, 38.0))]
Cluster 3: [(5, (32.0, 34.0)), (7, (29.0, 32.0)), (15, (30.0, 32.0))]
Cluster 4: [(2, (40.0, 43.0)), (10, (42.0, 47.0)), (12, (41.0, 45.0)), (13, (39.0, 43.0))]</pre>
<p class="mce-root">The 15<sup class="calibre30">th</sup> couple is in cluster 3, the 16<sup class="calibre30">th</sup> is in cluster 2, and the 17<sup class="calibre30">th</sup> is in cluster 2. So, the estimated number of children for the 15<sup class="calibre30">th</sup> couple is 1. The estimated number of children for the 16<sup class="calibre30">th</sup> and 17<sup class="calibre30">th</sup> couples is 5/3=1.67.</p>
<p class="mce-root">The <em class="calibre18">E5</em> error of the estimation is as follows:</p>
<p class="cdpaligncenter"><img class="fm-editor-equation144" src="img/9781e3d2-892b-419f-8121-85ece972c9e4.png" width="3110" height="410"/></p>
<p class="mce-root"><strong class="calibre8">Using cross-validation to determine the outcome</strong></p>
<p class="mce-root">We used 14 couples as training data for the estimation and three other couples for cross-validation to find the best parameter of <em class="calibre18">k</em> clusters among the values 2, 3, 4, and 5. We could try to cluster into more clusters, but since we have such a relatively small amount of data, it should be sufficient to cluster into five clusters at most. Let's summarize the errors arising from the estimation:</p>
<table border="1" class="msotablegrid">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<div><strong class="calibre3">Number of clusters</strong></div>
</td>
<td class="calibre25">
<div><strong class="calibre3">Error rate</strong></div>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p>2</div>
</td>
<td class="calibre25">
<div>3.3</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p>3</div>
</td>
<td class="calibre25">
<div>2.17</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p>4</div>
</td>
<td class="calibre25">
<div>2.7</p>
</td>
</tr>
<tr class="calibre28">
<td class="calibre25">
<p>5</div>
</td>
<td class="calibre25">
<div>2.13</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">The error rate is lowest for <strong class="calibre8">3</strong> and <strong class="calibre8">5</strong> clusters. The fact that the error rate goes up for <strong class="calibre8">4</strong> clusters and then down again for <strong class="calibre8">5</strong> clusters could indicate that we don't have enough data to make a good estimate. A natural expectation would be that there are not local maxims of errors for values of <em class="calibre18">k</em> greater than 2. Moreover, the difference between the error rate for clustering with <strong class="calibre8">3</strong> and <strong class="calibre8">5</strong> clusters is very small, and one cluster out of <strong class="calibre8">5</strong> is smaller than one cluster out of <strong class="calibre8">3</strong>. For this reason, we choose 3 clusters over 5 to estimate the number of children for the 18<sup class="calibre30">th</sup> couple.</p>
<p class="mce-root">When clustering into three clusters, the 18<sup class="calibre30">th</sup> couple is in cluster <strong class="calibre8">2</strong>. Therefore, the estimated number of children for the 18<sup class="calibre30">th</sup> couple is 1.25.</p>


            

            
        
    </div>



  </body></html>