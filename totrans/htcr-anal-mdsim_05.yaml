- en: Computing Foundations – Introduction to Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will provide an introduction to Python for analytics. It is meant
    mainly for novice programmers or developers who are not familiar with Python.
    By the end of the chapter, you will have a basic familiarity with the features
    of the Python base language, which is integral for healthcare analytics and machine
    learning. You will also understand how to get started using `pandas` and `scikit-learn`,
    two important Python libraries for analytics.
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to follow along using the Jupyter Notebook, we encourage you
    to refer to the directions in [Chapter 1](b15b2b73-d2bb-410f-ab55-5f0f1e91730e.xhtml),
    *Introduction to Healthcare Analytics*, to start a new Jupyter session. The notebook
    for this chapter is also available online at the book's official code repository.
  prefs: []
  type: TYPE_NORMAL
- en: Variables and types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Basic variable types in Python consist of strings and numeric types. Let's look
    at both of these types in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Strings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Python, a **string** is a variable type that stores text characters such
    as letters, numbers, special characters, and punctuation. In Python, we use single
    or double quotation marks to indicate that the variable is a string rather than
    a number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Strings cannot be used for mathematical operations on numbers. But they can
    be used for other useful operations, as we see in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The result of the preceding code is to print string `'12'`, not `'3'`. Instead
    of adding the two numbers, the `+` operator performs concatenation (appending
    the second string to the end of the first string) in Python when operating on
    two strings.
  prefs: []
  type: TYPE_NORMAL
- en: Other operators that act on strings include the `*` operator (for repeating
    strings ![](img/14655201-6690-4b4c-ad90-85b3c37d6601.png) number of times, for
    example, `string_1 * 3`) and the `<` and `>` operators (to compare the ASCII values
    of the strings).
  prefs: []
  type: TYPE_NORMAL
- en: To convert data from a numeric type to a string, we can use the `str()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because strings are sequences (of characters), we can index them and slice
    them (like we can do with other data containers, as you will see later). A slice
    is a contiguous section of a string. To index/slice them, we use integers enclosed
    in square brackets to indicate the character''s position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To slice strings, we include the beginning and end positions, separated by
    a colon, in the square brackets. Note that the end position will include all the
    characters *up to but not including* the end position, as we see in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Earlier, we mentioned the `str()` method. There are dozens of other methods
    for strings. A full list of them is available in the online Python documentation
    at [www.python.org](http://www.python.org). Methods include those for case conversion,
    finding specific substrings, and stripping whitespace. We'll discuss one more
    method here–the `split()` method. The `split()` method acts on a string and takes
    a `separator` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is a list of strings; each item in the list is a component of the
    original string, split by `separator`. This is very useful for parsing strings
    that are delimited by punctuation characters such as `,` or `;`. We will discuss
    lists in the next section. Here is an example of the `split()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Numeric types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The two numeric types in Python that are most useful for analytics are **integers**
    and **floating-point** numbers. To convert to these types, you can use the `int()`
    and `float()` functions, respectively. The most common operations on numbers are
    supported with the usual operators: `+`, `-`, `*`, `/`, `<`, and `>`. Modules
    containing special methods for numeric types that are particularly useful for
    analytics include `math` and `random`. More information on numeric types is available
    in the online Python documentation (see the link in the previous section).'
  prefs: []
  type: TYPE_NORMAL
- en: Note that with some Python versions, dividing two integers using the / operator
    performs **floor division** (with the numbers after the decimal place omitted);
    for example, `10/4` would equal `2`, not `2.5`. This is a stealthy yet egregious
    error that can throw off numerical calculations. However, with the version of
    Python we are using in this book, we don't need to worry about this error.
  prefs: []
  type: TYPE_NORMAL
- en: The **Boolean type** is a special integer type that can be used to represent
    the `True` and `False` values. To convert an integer to a Boolean type, you can
    use the `bool()` function. A zero gets converted to `False`; any other integer
    would get converted to `True`. Boolean variables behave like 1 (True) and 0 (False),
    except that they return `True` and `False`, respectively, when converted to strings.
  prefs: []
  type: TYPE_NORMAL
- en: Data structures and containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we talked about variable types that store single values.
    Now we will move on to data structures that can hold multiple values. These data
    structures are lists, tuples, dictionaries, and sets. Lists and tuples are commonly
    referred to as sequences in Python. In this book, we will use the terms data structures
    and data containers interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: Lists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lists are a widely used data structure that can hold multiple values. Let''s
    look at some features of lists:'
  prefs: []
  type: TYPE_NORMAL
- en: To make a list, we use square brackets, `[]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: `my_list = [1, 2, 3]`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Lists can hold any combination of numeric types, strings, Boolean types, tuples,
    dictionaries, or even other lists.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: `my_diverse_list = [51, ''Health'', True, [1, 2, 3]]`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Lists, like strings, are sequences and support indexing and slicing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, in the preceding example, `my_diverse_list[0]` would equal `51`.
    `my_diverse_list[0:2]` would equal `[51, 'Health']`. To access the `3` of the
    nested list, we can use `my_diverse_list[3][2]`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Lists are **mutable** (unlike strings and tuples), meaning that we can change
    individual components using indices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if we entered the `my_diverse_list[2] = False` command, our new
    `my_diverse_list` would be equal to `[51, 'Health', False, [1, 2, 3]]` .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notable advantages of lists for analytics include their vast array of helper
    methods, such as `append()`, `extend()`, and `join()`, and their interchangeability
    with the `pandas` and `numpy` data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Tuples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tuples are similar to lists. To make a tuple, we use parentheses, `()`. Example:
    `my_tuple = (1, 2, 3)`. The main difference between tuples and lists is that tuples
    are **immutable**, so we cannot change any components of a tuple. If we tried
    `my_tuple[0] = 4`, an error would be thrown. Because their values are immutable,
    tuples are useful for setting constant variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Dictionaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **dictionary** is a common data structure in Python. It is used to store
    unidirectional mappings from keys to values. For example, if we wanted to create
    a dictionary that stored a list of patient names and their corresponding room
    numbers, we could use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s talk about the preceding code snippet in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The names in the `rooms` dictionary are referred to as **keys**. The keys in
    a dictionary must be unique. To access them, we can use the `keys()` function, `rooms.keys()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The room numbers in the `rooms` dictionary are referred to as **values**. To
    access all of the values, we can use the `values()` function, `rooms.values()`.
    To access an individual value, we just supply the name of its key in square brackets.
    For example, `rooms['Smith']` will return `'141-A'`. For this reason, we say that
    a dictionary maps keys to their values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To access a nested list of tuples that contains each key along with its corresponding
    value, we can use the `items()` function, `rooms.items()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionaries don't have to just be strings; in fact, the values can be any data
    type/structure. The keys can be particular variables such as integers or strings.
    While the values are mutable, the keys are immutable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionaries have no intrinsic order, so indexing and slicing by number is not
    supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although in Python the set doesn''t receive as much attention as its popular
    cousin the list, sets play an important role in analytics, so we include them
    here. To make a set, we use the built-in `set()` function. There are three things
    you need to know about sets:'
  prefs: []
  type: TYPE_NORMAL
- en: They are immutable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are unordered
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The elements of a set are unique
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, sets in Python are very similar to their mathematical counterparts
    if you are familiar with the basic set theory. The set methods also duplicate
    typical set operations and include `union()`, `intersection()`, `add()`, and `remove()`.
    These functions come in handy when wanting to perform typical set-like operations
    on data structures, such as lists or tuples, following conversions to sets.
  prefs: []
  type: TYPE_NORMAL
- en: Programming in Python – an illustrative example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we discussed variable types and data containers. There
    are many more aspects of Python programming, such as control flow with if/else
    statements, loops, and comprehensions; functions; and classes and object-oriented
    programming. Commonly, Python programs are packaged into **modules**, which are
    self-standing scripts that can be run from the command line to perform computing
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s introduce some of these concepts in Python with a "module" of our own
    (you can use the Jupyter Notebook for this):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is a Python module that prints the height and **body mass
    indices** (**BMIs**) for two mock patients. Let''s take a more detailed look at
    each element of this code:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line of the code block is an **import statement**. This allows us
    to import functions and classes that have been written in other modules that are
    either distributed with Python, written as open source software, or written by
    ourselves. A **module** can simply be thought of as a file that contains Python
    functions, constants, and/or classes. It has a `.py` extension. To import a module
    in its entirety, we can simply use the `import` word followed by the module name,
    for example, `import math`. Notice we used the `from` keyword as well because
    we only want to import a specific function, the `pow()` function. This also saves
    us from the inconvenience of having to type `math.pow()` every time we want to
    raise something to a power.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next two lines contain **constants** that we will use to perform unit conversions.
    Constants are usually indicated by capital letters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we define a `Patient` class that has a **constructor** and two **methods**.
    The constructor takes three arguments–the name, height, and weight–and sets three
    attributes of the specific `Patient` instance to equal those three values. It
    also converts the weight from pounds to kilograms and the height from inches to
    meters, and stores those values in two extra attributes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two methods are coded as **functions**, using the `def` keyword. `calculate_bmi()`
    returns the BMI of the patient, while `get_height()` simply returns the height
    in meters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we have a mini `if` statement. All that this `if` statement is saying
    is to run the subsequent code only if it is the main module invoked at the command
    line. Other `if` statements may have multiple `elif` clauses and can also include
    a final `else` clause.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we create a list of two patients, John Smith and Patty Johnson, with their
    heights and weights as listed in the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following line uses a list **comprehension** to create a list of heights
    of the two patients. Comprehensions are very popular in Python programming and
    can also be performed with dictionaries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, our `print` statement prints the four numbers as the output (the two
    heights and the two BMI values).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further references for the base Python programming language are given at the
    end of this chapter. You can also check out the online documentation at [www.python.org](http://www.python.org).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Almost all of the features we''ve discussed so far are features of *base* Python;
    that is, no external packages or libraries were required. The truth of the matter
    is that the majority of the code we write in this book will pertain to one of
    several *external* Python packages commonly used for analytics. The **pandas**
    library ([http://pandas.pydata.org](http://pandas.pydata.org)) is an integral
    part of the later programming chapters. The functions of pandas for machine learning
    are threefold:'
  prefs: []
  type: TYPE_NORMAL
- en: Import data from flat files into your Python session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrangle, manipulate, format, and cleanse data using the pandas DataFrame and
    its library of functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Export data from your Python session to flat files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's review each of these functions in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Flat files are popular methods of storing healthcare-related data (along with
    HL7 formats, which are not covered in this book). A **flat file** is a text file
    representation of data. Using flat files, data can be represented as rows and
    columns, similar to databases, except that punctuation or whitespace are used
    as column delimiters, while carriage returns are used as row delimiters. We will
    see an example flat file in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*.
  prefs: []
  type: TYPE_NORMAL
- en: pandas allows us to import data into a tabular Python data structure, called
    a **DataFrame**, from a variety of other Python structures and flat files, including
    Python dictionaries, pickle objects, **comma-separated values** (**csv**) files,
    **fixed-width format** (**fwf**) files, Microsoft Excel files, JSON files, HTML
    files, and even SQL database tables.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is in Python, there are additional functions that you can use
    to explore and transform the data. Need to perform a mathematical function on
    a column, such as finding its sum? Need to perform SQL-like operations, such as
    JOINs or adding columns (see [Chapter 3](46c83498-cb6e-45b4-ac39-6875a8d32400.xhtml),
    *Machine Learning Foundations*)? Need to filter rows by a condition? All of the
    functionality is there in pandas' API. We will make good use of some of pandas'
    functionality in [Chapter 6](023c1d7e-f3f0-42e6-a2be-64bd5ba4ab80.xhtml), *Measuring
    Healthcare Quality* and in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when we are done exploring, cleansing, and wrangling our data, if we
    can choose to export it out as most of the formats listed. Or we can convert it
    to a NumPy array and train machine learning models, as we will do later in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: What is a pandas DataFrame?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **pandas DataFrame** can be thought of as a two-dimensional, matrix-like data
    structure that consists of rows and columns. A pandas DataFrame is analogous to
    a dataframe in R or a table in SQL. Advantages over traditional matrices and other
    Python data structures include the ability to have columns of different types
    in the same DataFrame, a wide array of predefined functions for easy data manipulation,
    and one-line interfaces that allow quick conversion to other file formats including
    databases, flat file formats, and NumPy arrays (for integration with scikit-learn's
    machine learning functionality). Therefore, `pandas` is indeed the glue that holds
    together many machine learning pipelines, from data importation to algorithm application.
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of pandas include slower performance and lack of built-in parallel
    processing for pandas functionality. Therefore, if you are working with millions
    or billions of data points, **Apache Spark** ([https://spark.apache.org/](https://spark.apache.org/))
    may be a better option, since it has parallel processing built into its language.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we demonstrate how to load data into Python via dictionaries,
    flat files, and databases.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data into pandas from Python data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step in working with `pandas` DataFrames is to create one using the
    `pandas` constructor function, `DataFrame()`. The constructor takes many Python
    data structures as input. It also takes as input NumPy arrays and pandas **Series**,
    another type of one-dimensional `pandas` data structure that is similar to a list.
    Here we demonstrate how to convert a dictionary of lists into a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Importing data into pandas from a flat file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because healthcare data can often be in flat file format, such as `.csv`  or
    `.fwf`, it is important to know of the `read_csv()` and `read_fwf()` functions
    that import data into `pandas` from these two formats, respectively. Both of the
    functions take as mandatory arguments the full path of the flat file, along with
    over a dozen additional optional arguments that specify options including the
    data types of the columns, the header rows, the columns to include in the DataFrame,
    and so on (a full listing of the function arguments is available online). It is
    often easiest to import all the columns as string types and convert the columns
    to other data types later on. In the following example, a DataFrame called `data`
    is created by using the `read_csv()` function to read in a flat `.csv` file that
    contains one header row (`row #0`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Because fixed-width files have no explicit character separator, the `read_fwf()`
    function needs an additional argument, `widths`, which is a list of integers specifying
    the column widths for each column. The length of `widths` should match the number
    of columns in the file. As an alternative, the `colspecs` argument takes in a
    list of tuples specifying the starting points and endpoints of each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Importing data into pandas from a database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `pandas` library also has functions to support the import of tables directly
    from SQL databases. Functions that can accomplish this include `read_sql_query()`
    and `read_sql_table()`. Before using these functions, the connection to the database
    must be established so that it can be passed to the function. In the following
    example, a table from a SQLite database is read into a DataFrame using the `read_sql_query()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If you wish to connect to a standard database, such as a MySQL database, the
    code would be similar, except for the connection statement, which would use the
    corresponding function for a MySQL database.
  prefs: []
  type: TYPE_NORMAL
- en: Common operations on DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll go over DataFrame operations useful for performing analytics.
    For descriptions of additional operations, please refer to the official pandas
    documentation at [https://pandas.pydata.org/](https://pandas.pydata.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Adding columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding columns is a common operation in analytics, whether adding new columns
    from scratch or transforming existing columns. Let's go over both types of operations
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Adding blank or user-initialized columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To add a new column of a DataFrame, you can follow the name of the DataFrame
    with the name of the new column (enclosed in single quotes and square brackets)
    and set it equal to whatever value you like. To add a column of empty strings
    or integers, you can set the column equal to `""` or `numpy.nan`, respectively
    (the latter requires importing `numpy` beforehand). To add a column of zeros,
    set the column equal to `0`. The following examples illustrate these points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Adding new columns by transforming existing columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In some cases, you might wish to add a new column that is a function of existing
    columns. In the following example, the new column, titled `example_new_column_3`,
    is added as a sum of the existing columns, `old_column_1` and `old_column_2`.
    The `axis=1` argument indicates that you wish to take the horizontal sum across
    the columns instead of the vertical sum of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following second example accomplishes a similar task using the pandas `apply()`
    function. `apply()` is a special function because it allows you to apply any function
    to columns in a DataFrame (including your own custom functions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Dropping columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To drop columns, you can use pandas'' `drop()` function. It takes a single
    column as well as a list of columns, and in this example, additional optional
    arguments indicate which is the axis along which to drop and whether or not to
    drop columns in place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Applying functions to multiple columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To apply a function over multiple columns in a DataFrame, the list of columns
    can be iterated over using a `for` loop. In the following example, a predefined
    list of columns is converted from the string type to the numeric type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Combining DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DataFrames can also be combined with each other, as long as they have the same
    number of entries along the combining axis. In this example, two DataFrames are
    concatenated vertically (for example, they contain the same number of columns,
    and their rows are stacked upon each other). DataFrames can also be concatenated
    horizontally (if they contain the same number of rows) by specifying the `axis`
    parameter. Note that the column names and row names should correspond to each
    other across the DataFrames; if they do not, new columns will be formed and NaN
    values will be inserted for any missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a new DataFrame name, `df2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we perform the concatenation. We set the optional `ignore_index` argument
    equal to `True` to avoid duplicate row indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Converting DataFrame columns to lists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To extract the contents of a column into a list, you can use the `tolist()`
    function. After being converted to a list, the data can then be iterated over
    using `for` loops and comprehensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Getting and setting DataFrame values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `pandas` library offers two main methods for selectively getting and setting
    values in DataFrames: `loc` and `iloc`. The `loc` method is mainly for **label-based
    indexing** (for example, identifying rows/columns using their indices/column names,
    respectively), while the `iloc` method is primarily for **integer-based indexing**
    (for example, identifying rows/columns using their integer positions in the DataFrame).
    The specific labels/indices of the rows and columns you wish to access are provided
    following the name of the DataFrame using square brackets, with row labels/indices
    preceding column labels/indices and separated from them by a comma. Let''s look
    at some examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting/setting values using label-based indexing with loc
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `.loc` attribute of a DataFrame is used to select values using the labels
    of the entries. It can be used to retrieve single scalar values from a DataFrame
    (using singular string labels for both row and column), or multiple values from
    a DataFrame (using lists of row/column labels). Single and multiple indexing can
    also be used in combination to get multiple values from a single row or column.
    The following lines of code illustrate the retrieval of a single scalar value
    from the `df` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The output will be `7.0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Single/multiple values can also be set using the `.loc` attribute and an equals
    sign:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Getting/setting values using integer-based labeling with iloc
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `.iloc` attribute works very similarly to the `.loc` attribute, except
    that it uses the integer positions of the rows and columns being accessed, not
    their labels. In the following example, the value in the 101st row (not the 100th
    row, since indexing starts at 0) and the 100th column is transferred to `scalar_value`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The output is `7.0`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, similarly to `.loc`, lists containing multiple values can be passed
    to the `.iloc` attribute to change multiple entries of a DataFrame at once.
  prefs: []
  type: TYPE_NORMAL
- en: Getting/setting multiple contiguous values using slicing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, the multiple values that we wish to get or set are coincidentally
    in neighboring (contiguous) columns. When this is the case, we can use **slicing**
    within the square brackets to select multiple values. With slicing, we specify
    the starting point and endpoint of the data that we wish to access. We can use
    slicing with both `.loc` and `.iloc`, although slicing using integers and `.iloc`
    is more common. The following lines of code illustrate slicing to retrieve part
    of a DataFrame (we can also assign elements using an equals sign). Note that slicing
    can also be used to access values in lists and tuples (as covered previously in
    the current chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Fast getting/setting of scalar values using at and iat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we are certain that we only wish to get/set single values in a DataFrame,
    we can use the `.at` and `.iat` attributes, along with singular labels/integers,
    respectively. Just remember, the `i` in `.iloc` and `.iat` stands for "integer":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The output is `11`.
  prefs: []
  type: TYPE_NORMAL
- en: Other operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two other common operations are filtering rows using a Boolean condition and
    sorting rows. Here we will review each of these operations.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering rows using Boolean indexing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've discussed using labels, integers, and slicing to select values
    in DataFrames. Sometimes, it is convenient to select certain rows that meet a
    certain condition in one of their statements. For example, if we wanted to restrict
    an analysis on people whose age is greater than or equal to 50 years.
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas DataFrames support **Boolean indexing**, that is, indexing using a vector
    of Boolean values to indicate which values we wish to include, provided that the
    length of the Boolean vector is equal to the number of rows in the DataFrame.
    Because a conditional statement involving a DataFrame column yields exactly that,
    we can index DataFrames using such conditional statements. In the following example,
    the `df` DataFrame is filtered to include only rows in which the value of the
    `age` column is equal to or exceeds `50`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Conditional statements can be chained together using logical operators such
    as `|` or `&`.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting rows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you wish to sort a DataFrame by the value of one of its columns, that can
    be done using the `sort_values()` function; simply specify the column name as
    the first parameter. `ascending` is an optional parameter that lets you specify
    the sorting direction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: SQL-like operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For people who are used to working with heterogeneously typed tables in SQL,
    switching to similar analyses in Python may seem like a daunting task. Fortunately,
    there are a number of `pandas` functions that can be combined to yield results
    similar to those yielded by common SQL queries, using operations such as grouping
    and joining. There is even a subsection in the `pandas` documentation ([https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html](https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html))
    that describes how to perform SQL-like operations with `pandas` DataFrames. We
    provide two such examples in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Getting aggregate row COUNTs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, you may wish to get a count or tally of the occurrences of particular
    values in a column. For example, you might have a healthcare dataset and you want
    to know how many times particular payment methods were used during patient visits.
    In SQL, you could write a query that uses a `GROUP BY` clause in conjunction with
    an aggregate function (in this case, `COUNT(*)`) to get a tally of the payment
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In `pandas`, the same result is accomplished by chaining together the `groupby()`
    and `size()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Joining DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 4](e1b89921-e75b-4b16-a567-8970a173db53.xhtml), *Computing Foundations
    – Databases*, we discussed merging data from two database tables using the `JOIN`
    operation. To use a JOIN operation, you need to specify the names of the two tables,
    along with the type of JOIN (left, right, outer, or inner) and the columns on
    which to join:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In pandas, you can accomplish table joins using the `merge()` or `join()` functions.
    By default, the `join()` function joins data on the index of the tables; however,
    other columns can be used by specifying the `on` parameter. If column names are
    overlapping in the two tables being joined, you will need to specify a `rsuffix`
    or `lsuffix` argument that renames the columns so they no longer have identical
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (note the `NaN` values in Row 3, a row that was not
    present in `df`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Introduction to scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Entire books have been written on **scikit-learn** ([http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)).
    The scikit-learn library has numerous submodules. Only a few of these submodules
    will be used in this book (in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*). These include the `sklearn.linear_model`
    and `sklearn.ensemble` submodules, for example. Here we will give an overview
    of some of the more commonly used submodules. For convenience, we have grouped
    the relevant modules into various segments of the data science pipeline discussed
    in [Chapter 1](b15b2b73-d2bb-410f-ab55-5f0f1e91730e.xhtml), *Introduction to Healthcare
    Analytics***.**
  prefs: []
  type: TYPE_NORMAL
- en: Sample data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn includes several sample datasets in the `sklearn.datasets` submodule.
    At least two of these datasets, `sklearn.datasets.load_breast_cancer` and `sklearn.datasets.load_diabetes`,
    are healthcare-related. These datasets have been already preprocessed and are
    small in size, spanning only dozens of features and hundreds of patients. The
    data we will use in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml), *Making
    Predictive Models in Healthcare* is much bigger and resembles the data you are
    likely to receive from modern healthcare organizations. These sample sklearn datasets,
    however, are useful for experimenting with scikit-learn functions.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data preprocessing functionality is present in the `sklearn.preprocessing` submodule,
    among others. Some of the relevant functions of this module are discussed in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding of categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost every dataset has some categorical data contained in it. **Categorical
    data** is discrete data in which the value can take on a finite number of possible
    values (usually encoded as a "string"). Because Python's scikit-learn can handle
    only numeric data, before performing machine learning with scikit-learn, we must
    find alternative ways of encoding categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: With **one-hot encoding**, also known as a **1-of-K encoding scheme**, a single
    categorical variable having *k* possible values is converted into *k* different
    binary variables, each one is positive if and only if the column's value for that
    observation equaled the value it represents. In [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*, we provide a detailed example of what
    one-hot encoding is and use a pandas function called `get_dummies()` to perform
    one-hot encoding on a real clinical dataset. scikit-learn also has a class used
    to perform one-hot encoding, however, it is the `OneHotEncoder` class in the `sklearn.preprocessing`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: For instructions on how `OneHotEncoder` is used, you can visit the scikit-learn
    documentation: [http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features](http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features).
  prefs: []
  type: TYPE_NORMAL
- en: Scaling and centering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For some machine learning algorithms, it is preferable to transform not only
    the categorical variables (using one-hot encoding, discussed previously) but also
    the continuous variables. Recall from [Chapter 1](b15b2b73-d2bb-410f-ab55-5f0f1e91730e.xhtml),
    *Introduction to Healthcare Analytics* that a continuous variable is numerical
    and can take on any rational value (although in many cases they are restricted
    to integers). A particularly common practice is to **standardize** each continuous
    variable so that the *mean of the variable is zero and the standard deviation
    is one*. For example, take the `AGE` variable: it typically ranges from 0 to about
    100, with a mean of perhaps 40\. Let''s pretend that for a particular population,
    the mean of our `AGE` variable is 40 with a standard deviation of 20\. If we were
    to center and rescale our `AGE` variable, a person whose age was 40 would be represented
    as zero in the transformed variable. A person who was 20 years old would be represented
    as -1, a person who was 60 years old would be represented as 1, a person who was
    80 years old would be represented as 2, and a person who was 50 years old would
    be 0.5\. This transformation prevents variables with larger ranges from being
    overrepresented in the machine learning algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn has many built-in classes and functions for centering and scaling
    data, including `sklearn.preprocessing.StandardScaler()`, `sklearn.preprocessing.MinMaxScaler()`,
    and `sklearn.preprocessing.RobustScaler()`. These various tools are specialized
    for centering and scaling different types of continuous data, such as normally
    distributed variables, or variables that have many outliers.
  prefs: []
  type: TYPE_NORMAL
- en: For instructions on how the scaling classes are used, you can check out the
    scikit-learn documentation: [http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling](http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling).
  prefs: []
  type: TYPE_NORMAL
- en: Binarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Binarization** is yet another type of transformation in which continuous
    variables are transformed into binary variables. For example, if we had a continuous
    variable named `AGE,` we could binarize the variable around 50 years by thresholding
    ages 50 and above to have a value of one, and ages with values below 50 to have
    a value of zero. Binarizing is good to save time and memory when you have many
    variables; however, in practice, the raw continuous values usually perform better
    since they are more informative.'
  prefs: []
  type: TYPE_NORMAL
- en: While binarization can also be performed in pandas using the code demonstrated
    earlier, scikit-learn comes with a `Binarizer` class that can also be used to
    binarize features. For instructions on using the `Binarizer` class, you can visit
    [http://scikit-learn.org/stable/modules/preprocessing.html#binarization](http://scikit-learn.org/stable/modules/preprocessing.html#binarization).
  prefs: []
  type: TYPE_NORMAL
- en: Imputation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](b15b2b73-d2bb-410f-ab55-5f0f1e91730e.xhtml), *Introduction to
    Healthcare Analytics*, we mentioned the importance of handling missing data. **Imputation**
    is one strategy for dealing with missing values in which missing values are filled
    in with estimates that are derived based on the data that is present. In healthcare,
    two common types of imputation are **zero imputation**, in which missing data
    is taken to be zero (for example, if a particular diagnosis has a value of `NULL`,
    most likely that is because it is not present in the patient chart) and **mean
    imputation**, in which the missing data is taken to be the mean of the distribution
    of the present data (for example, if a patient has a missing age, we can impute
    it as 40). We demonstrated various imputation methods in [Chapter 4](e1b89921-e75b-4b16-a567-8970a173db53.xhtml),
    *Computing Foundations – Databases*, and we will write our own custom functions
    for performing imputation in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn comes with an `Imputer` class for performing different types of
    imputation. You can see details on how it is used at [http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values](http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values).
  prefs: []
  type: TYPE_NORMAL
- en: Feature-selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, there is often a misconception that the more data you have,
    the better off you are. This is usually true with observations (for example, the
    number of rows in the dataset). However, with features, more isn't always better.
    In some cases, the performance may be paradoxically better with fewer features,
    because multiple features with high correlation are biasing the predictions, or
    because there are more features present than the number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, the performance may be the same or infinitesimally worse with,
    say, half the features, but the smaller number of features may be desirable for
    a number of reasons, including time considerations, memory availability, or ease
    of explanation and interpretation to other non-technical stakeholders. In any
    case, it is almost always a good idea to perform some feature selection on the
    data. Even if you don't wish to remove any features, performing feature selection
    and ranking the feature importance can give you great insight into your model
    and understanding its predictive behavior and performance.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of classes and functions in the `sklearn.feature_selection`
    module that are built for feature selection, and different sets of classes correspond
    to different methods of performing feature selection. For example, univariate
    feature selection involves measuring the statistical dependency between each predictor
    variable and the target variable, and this can be done using the `SelectKBest`
    or `SelectPercentile` classes, among others. The `VarianceThreshold` class removes
    features that have a low variance across observations, for example, those features
    that are almost always zero. And the `SelectFromModel` class prunes features that
    don't meet a certain strength requirement (in terms of either coefficient or feature
    importance) after the model has been fit.
  prefs: []
  type: TYPE_NORMAL
- en: For a full list of the feature selection classes in scikit-learn, you can visit
    [http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection).
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning algorithms provide a mathematical framework for making predictions
    for new observations. scikit-learn supports dozens of different ML algorithms
    that have different strengths and weaknesses. We will discuss some of these algorithms
    and their corresponding scikit-learn API functionality briefly here. We will use
    some of these algorithms in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml),
    *Making Predictive Models in Healthcare*.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 3](46c83498-cb6e-45b4-ac39-6875a8d32400.xhtml),
    *Machine Learning Foundations*, a **linear model** can be thought of casually
    as a weighted combination of features (for example, a weighted sum) to predict
    a target value. The features are determined by the observations; the weights of
    each feature are determined by the model. Linear regression predicts a continuous
    variable, while logistic regression can be thought of as an extended form of linear
    regression in which the predicted target undergoes a **logit transformation**
    to be converted to a variable that has a range between zero and one. Such a transformation
    is useful for performing binary classification tasks such as when there are two
    possible outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, these two algorithms are represented by the `sklearn.linear_model.LogisticRegression`
    and `sklearn.linear_model.LinearRegression` classes. We will demonstrate logistic
    regression in [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml), *Making
    Predictive Models in Healthcare*.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ensemble methods** involve making predictions using combinations of different
    ML models. For example, a **random forest** is a collection of decision tree classifiers
    that have been decorrelated from each other by choosing and using specific feature
    sets for each tree. Additionally, **AdaBoost** is an algorithm that fits many
    weak learners on the data to make effective predictions. These algorithms are
    supported by the `sklearn.ensemble` module.'
  prefs: []
  type: TYPE_NORMAL
- en: Additional machine learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some other popular machine learning algorithms include the Naive Bayes algorithm,
    k-nearest neighbors, neural networks, decision trees, and support vector machines.
    These are supported in scikit-learn by the `sklearn.naive_bayes`, `sklearn.neighbors`,
    `sklearn.neural_network`, `sklearn.tree`, and `sklearn.svm` modules, respectively.
    In [Chapter 7](d029d858-9c6e-4bf0-b793-87cdc4395e86.xhtml), *Making Predictive
    Models in Healthcare*, we will make neural network models on a clinical dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Performance assessment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lastly, once we make our model using our desired algorithm, it is important
    to measure its performance. The `sklearn.metrics` module is useful for this. As
    discussed in [Chapter 3](46c83498-cb6e-45b4-ac39-6875a8d32400.xhtml), *Machine
    Learning Foundations*, the confusion matrix is particularly important for classification
    tasks, and it is supported by the `sklearn.metrics.confusion_matrix()` function.
    Determining the receiver operating characteristic (ROC) curve and calculating
    the **area under the curve** (**AUC**) can be accomplished using the `sklearn.metrics.roc_curve()`
    and `sklearn.metrics.roc_auc_score()` functions, respectively. Precision-recall
    curves are an alternative to the ROC curve that are important for imbalanced datasets,
    and they are supported by the `sklearn.metrics.precision_recall_curve()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Additional analytics libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here we mention three important packages that are frequently used for analytics:
    NumPy, SciPy, and matplotlib.'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy and SciPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**NumPy** ([www.numpy.org](http://www.numpy.org/)) is Python''s matrix library.
    Using `numpy.array()` and similar constructs, large matrices can be created and
    various mathematical operations (including matrix addition and multiplication)
    can be performed on them. NumPy also has many functions for manipulating the shapes
    of matrices. Another feature of NumPy is the presence of familiar mathematical
    functions such as `sin()`, `cos()`, and `exp()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**SciPy** ([www.scipy.org](http://www.scipy.org)) is a toolbox that contains
    many advanced mathematical modules. Its machine-learning-related subpackages include
    `cluster`, `stats`, `sparse`, and `optimize`. SciPy is an important package that
    enables scientific computing in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: matplotlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**matplotlib** ([https://matplotlib.org](https://matplotlib.org)) is a popular
    Python 2-D plotting library. According to its website, one "can generate plots,
    histograms, power spectra, bar charts, error charts, scatterplots, and so on,
    with just a few lines of code." Its plotting library comes with a myriad of options
    and features to enable a high degree of customization.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we took a whirlwind tour of the base Python language, along
    with two Python libraries that are important for performing analytics: pandas,
    and scikit-learn. We have now completed the foundational chapters of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](023c1d7e-f3f0-42e6-a2be-64bd5ba4ab80.xhtml), *Measuring Healthcare
    Quality*, we will dive into some real-world healthcare provider performance data
    and analyze it using pandas.
  prefs: []
  type: TYPE_NORMAL
