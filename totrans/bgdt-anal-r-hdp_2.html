<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Writing Hadoop MapReduce Programs</h1></div></div></div><p>In the previous chapter, we learned how to set up the R and Hadoop development environment. Since we are interested in performing Big Data analytics, we need to learn Hadoop to perform operations with Hadoop MapReduce. In this chapter, we will discuss what MapReduce is, why it is necessary, how MapReduce programs can be developed through Apache Hadoop, and more.</p><p>In this chapter, we will cover:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the basics of MapReduce</li><li class="listitem" style="list-style-type: disc">Introducing Hadoop MapReduce</li><li class="listitem" style="list-style-type: disc">Understanding the Hadoop MapReduce fundamentals</li><li class="listitem" style="list-style-type: disc">Writing a Hadoop MapReduce example</li><li class="listitem" style="list-style-type: disc">Understanding several possible MapReduce definitions to solve business problems</li><li class="listitem" style="list-style-type: disc">Learning different ways to write  Hadoop MapReduce in R</li></ul></div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec23"/>Understanding the basics of MapReduce</h1></div></div></div><p>Understanding <a id="id145" class="indexterm"/>the basics of MapReduce could well be a long-term solution if one doesn't have a cluster or uses <strong>Message Passing Interface</strong> (<strong>MPI</strong>)<a id="id146" class="indexterm"/>. However, a more realistic use case is when the <a id="id147" class="indexterm"/>data doesn't fit on one disk but fits on a <strong>Distributed File System</strong> (<strong>DFS</strong>), or already lives on Hadoop-related software.</p><p>Moreover, MapReduce is a programming model that works in a distributed fashion, but it is not the only one that does. It might be illuminating to describe other programming models, for <a id="id148" class="indexterm"/>example, MPI and <strong>Bulk Synchronous Parallel</strong> (<strong>BSP</strong>). To process Big Data with tools such as R and several machine learning techniques requires a high-configuration machine, but that's not the permanent solution. So, distributed processing is the key to handling this data. This distributed computation can be implemented with the MapReduce programming model.</p><p>MapReduce is the one that answers the Big Data question. Logically, to process data we need parallel processing, which means processing over large computation; it can either be obtained by clustering the computers or increasing the configuration of the machine. Using the computer cluster is an ideal way to process data with a large size.</p><p>Before we talk<a id="id149" class="indexterm"/> more about MapReduce by parallel processing, we will discuss Google MapReduce research and a white paper written by <em>Jeffrey Dean</em> and <em>Sanjay Ghemawat</em> in 2004. They introduced MapReduce as simplified data processing software on large clusters. MapReduce implementation runs on large clusters with commodity hardware. This data processing platform is easier for programmers to perform various operations. The system takes care of input data, distributes data across the computer network, processes it in parallel, and finally combines its output into a single file to be aggregated later. This is very helpful in terms of cost and is also a time-saving system for processing large datasets over the cluster. Also, it will efficiently use computer resources to perform analytics over huge data. Google has been granted a patent on MapReduce.</p><p>For MapReduce, programmers need to just design/migrate applications into two phases: Map and Reduce. They simply have to design Map functions for processing a key-value pair to generate a set of intermediate key-value pairs, and Reduce functions to merge all the intermediate keys. Both the Map and Reduce functions maintain MapReduce workflow. The Reduce function will start executing the code after completion or once the Map output is available to it.</p><p>Their execution sequence can be seen as follows:</p><div><img src="img/3282OS_02_00.jpg" alt="Understanding the basics of MapReduce"/></div><p>MapReduce assumes that the Maps are independent and will execute them in parallel. The key aspect of the MapReduce algorithm is that if every Map and Reduce is independent of all other ongoing Maps and Reduces in the network, the operation will run in parallel on different keys and lists of data.</p><p>A distributed filesystem spreads multiple copies of data across different machines. This offers reliability as well as fault tolerance. If a machine with one copy of the file crashes, the same data <a id="id150" class="indexterm"/>will be provided from another replicated data source.</p><p>The master node of the MapReduce daemon will take care of all the responsibilities of the MapReduce jobs, such as the execution of jobs, the scheduling of Mappers, Reducers, Combiners, and Partitioners, the monitoring of successes as well as failures of individual job tasks, and finally, the completion of the batch job.</p><p>Apache Hadoop processes the distributed data in a parallel manner by running Hadoop MapReduce jobs on servers near the data stored on Hadoop's distributed filesystem.</p><p>Companies using MapReduce include:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Amazon</strong>: This is an online e-commerce and cloud web service provider for Big Data analytics</li><li class="listitem" style="list-style-type: disc"><strong>eBay</strong>: This is an e-commerce portal for finding articles by its description</li><li class="listitem" style="list-style-type: disc"><strong>Google</strong>: This is a web search engine for finding relevant pages relating to a particular topic</li><li class="listitem" style="list-style-type: disc"><strong>LinkedIn</strong>: This is a professional networking site for Big Data storage and generating personalized recommendations</li><li class="listitem" style="list-style-type: disc"><strong>Trovit</strong>: This is a vertical search engine for finding jobs that match a given description</li><li class="listitem" style="list-style-type: disc"><strong>Twitter</strong>: This is a social networking site for finding messages</li></ul></div><p>Apart from these, there are many other brands that are using Hadoop for Big Data analytics.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec24"/>Introducing Hadoop MapReduce</h1></div></div></div><p>Basically, the MapReduce <a id="id151" class="indexterm"/>model can be implemented in several languages, but apart from that, Hadoop MapReduce is a popular Java framework for easily written applications. It processes vast amounts of data (multiterabyte datasets) in parallel on large clusters (thousands of nodes) of commodity hardware in a reliable and fault-tolerant manner. This MapReduce paradigm is divided into two phases, Map and Reduce, that mainly deal with key-value pairs of data. The Map and Reduce<a id="id152" class="indexterm"/> tasks run sequentially in a cluster, and the output of the Map phase becomes the input of the Reduce phase.</p><p>All data input elements in MapReduce cannot be updated. If the input <code class="literal">(key, value)</code> pairs for mapping tasks are changed, it will not be reflected in the input files. The Mapper output will be piped to the appropriate Reducer grouped with the key attribute as input. This sequential data process will be carried away in a parallel manner with the help of Hadoop MapReduce algorithms as well as Hadoop clusters.</p><p>MapReduce programs transform the input dataset present in the list format into output data that will also be in the list format. This logical list translation process is mostly repeated twice in the Map and Reduce phases. We can also handle these repetitions by fixing the number of Mappers and Reducers. In the next section, MapReduce concepts are described based on the old MapReduce API.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec18"/>Listing Hadoop MapReduce entities</h2></div></div></div><p>The following <a id="id153" class="indexterm"/>are the components of Hadoop that <a id="id154" class="indexterm"/>are responsible for performing analytics over Big Data:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Client</strong>: This<a id="id155" class="indexterm"/> initializes the job</li><li class="listitem" style="list-style-type: disc"><strong>JobTracker</strong>: This<a id="id156" class="indexterm"/> monitors the job</li><li class="listitem" style="list-style-type: disc"><strong>TaskTracker</strong>: This <a id="id157" class="indexterm"/>executes the job</li><li class="listitem" style="list-style-type: disc"><strong>HDFS</strong>: This <a id="id158" class="indexterm"/>stores the input and output data</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec19"/>Understanding the Hadoop MapReduce scenario</h2></div></div></div><p>The four main stages of Hadoop MapReduce data processing are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The loading of data into HDFS</li><li class="listitem" style="list-style-type: disc">The execution of the Map phase</li><li class="listitem" style="list-style-type: disc">Shuffling and sorting</li><li class="listitem" style="list-style-type: disc">The execution of the Reduce phase</li></ul></div><div><div><div><div><h3 class="title"><a id="ch02lvl3sec08"/>Loading data into HDFS</h3></div></div></div><p>The input <a id="id159" class="indexterm"/>dataset needs to be uploaded to <a id="id160" class="indexterm"/>the Hadoop directory so it can be used by MapReduce<a id="id161" class="indexterm"/> nodes. Then, <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>) will divide the input dataset into data splits and store them to DataNodes in a cluster by taking care of the replication factor for fault tolerance. All the data splits will be processed by TaskTracker for the Map and Reduce tasks in a parallel manner.</p><p>Also, there are some alternative ways to get the dataset in HDFS with Hadoop components:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Sqoop</strong>: This is an open source tool designed for efficiently transferring bulk data between <a id="id162" class="indexterm"/>Apache Hadoop and structured, relational databases. Suppose your application has already been configured with the MySQL database and you want to use the same data for performing data analytics, Sqoop is recommended for importing datasets to HDFS. Also, after the completion of the data analytics process, the output can be exported to the MySQL database.</li><li class="listitem" style="list-style-type: disc"><strong>Flume</strong>: This <a id="id163" class="indexterm"/>is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data to HDFS. Flume is able to read data from most sources, such as logfiles, sys logs, and the standard output of the Unix process.</li></ul></div><p>Using the preceding data collection and moving the framework can make this data transfer process very easy for the MapReduce application for data analytics.</p></div><div><div><div><div><h3 class="title"><a id="ch02lvl3sec09"/>Executing the Map phase</h3></div></div></div><p>Executing the<a id="id164" class="indexterm"/> client application starts the <a id="id165" class="indexterm"/>Hadoop MapReduce processes. The Map phase then copies the job resources (unjarred class files) and stores it to HDFS, and requests JobTracker to execute the job. The JobTracker initializes the job, retrieves the input, splits the information, and creates a Map task for each job.</p><p>The JobTracker will call TaskTracker to run the Map task over the assigned input data subset. The Map task reads this input split data as input <code class="literal">(key, value)</code> pairs provided to the Mapper method, which then produces intermediate <code class="literal">(key, value)</code> pairs. There will be at least one output for each input <code class="literal">(key, value)</code> pair.</p><div><img src="img/3282OS_02_01.jpg" alt="Executing the Map phase"/><div><p>Mapping individual elements of an input list</p></div></div><p>The list of (key, value) pairs is generated such that the key attribute will be repeated many times. So, its key attribute will be re-used in the Reducer for aggregating values in MapReduce. As<a id="id166" class="indexterm"/> far as format is concerned, Mapper<a id="id167" class="indexterm"/> output format values and Reducer input values must be the same.</p><p>After the completion of this Map operation, the TaskTracker will keep the result in its buffer storage and local disk space (if the output data size is more than the threshold).</p><p>For example, suppose we have a <code class="literal">Map</code> function that converts the input text into lowercase. This will convert the list of input strings into a list of lowercase strings.</p><div><div><h3 class="title"><a id="tip11"/>Tip</h3><p><strong>Keys and values</strong>: In MapReduce, every value has its identifier that is considered as key. The key-value pairs received by the Mapper are dependent on the input datatype as specified in the job configuration file.</p></div></div></div><div><div><div><div><h3 class="title"><a id="ch02lvl3sec10"/>Shuffling and sorting</h3></div></div></div><p>To optimize the MapReduce program, this intermediate phase is very important.</p><p>As soon as the<a id="id168" class="indexterm"/> Mapper output from the Map phase is<a id="id169" class="indexterm"/> available, this intermediate phase will be called automatically. After the completion of the Map phase, all the emitted intermediate (key, value) pairs will be partitioned by a Partitioner at the Mapper side, only if the Partitioner is present. The output of the Partitioner will be sorted out based on the key attribute at the Mapper side. Output from sorting the operation is stored on buffer memory available at the Mapper node, TaskTracker.</p><p>The Combiner is often the Reducer itself. So by <a id="id170" class="indexterm"/>compression, it's not <strong>Gzip</strong> or some similar <a id="id171" class="indexterm"/>compression but the Reducer on the node that the map is <a id="id172" class="indexterm"/>outputting the data on. The data returned by the Combiner is then shuffled and sent to the reduced nodes. To speed up data transmission of the Mapper output to the Reducer slot at TaskTracker, you need to compress that <a id="id173" class="indexterm"/>output with the <code class="literal">Combiner</code> function. By default, the Mapper output will be stored to buffer memory, and if the output size is larger than threshold, it will be stored to a local disk. This output data will be available through <a id="id174" class="indexterm"/>
<strong>Hypertext Transfer Protocol</strong> (<strong>HTTP</strong>).</p></div><div><div><div><div><h3 class="title"><a id="ch02lvl3sec11"/>Reducing phase execution</h3></div></div></div><p>As soon as the Mapper output is available, TaskTracker in the Reducer node will retrieve the available <a id="id175" class="indexterm"/>partitioned Map's output data, and <a id="id176" class="indexterm"/>they will be grouped together and merged into one large file, which will then be assigned to a process with a <code class="literal">Reducer</code> method. Finally, this will be sorted out before data is provided to the <code class="literal">Reducer</code> method.</p><p>The <code class="literal">Reducer</code> method<a id="id177" class="indexterm"/> receives a list of input values from an input <code class="literal">(key, list (value))</code> and aggregates them based on custom logic, and produces the output <code class="literal">(key, value)</code> pairs.</p><div><img src="img/3282OS_02_02.jpg" alt="Reducing phase execution"/><div><p>Reducing input values to an aggregate value as output</p></div></div><p>The <a id="id178" class="indexterm"/>output of the <code class="literal">Reducer</code> method of the Reduce phase will directly be written into HDFS as per <a id="id179" class="indexterm"/>the format specified<a id="id180" class="indexterm"/> by the MapReduce job configuration class.</p></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec20"/>Understanding the limitations of MapReduce</h2></div></div></div><p>Let's see<a id="id181" class="indexterm"/> some of Hadoop MapReduce's limitations:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The MapReduce framework is notoriously difficult to leverage for transformational logic that is not as simple, for example, real-time streaming, graph processing, and message passing.</li><li class="listitem" style="list-style-type: disc">Data querying is inefficient over distributed, unindexed data than in a database created with indexed data. However, if the index over the data is generated, it needs to be maintained when the data is removed or added.</li><li class="listitem" style="list-style-type: disc">We can't parallelize the Reduce task to the Map task to reduce the overall processing time because Reduce tasks do not start until the output of the Map tasks is available to it. (The Reducer's input is fully dependent on the Mapper's output.) Also, we can't control the sequence of the execution of the Map and Reduce task. But sometimes, based on application logic, we can definitely configure a slow start for the Reduce tasks at the instance when the data collection starts as soon as the Map tasks complete.</li><li class="listitem" style="list-style-type: disc">Long-running Reduce tasks can't be completed because of their poor resource utilization either if the Reduce task is taking too much time to complete and fails or if there are no other Reduce slots available for rescheduling it (this can be solved with YARN).</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec21"/>Understanding Hadoop's ability to solve problems</h2></div></div></div><p>Since this <a id="id182" class="indexterm"/>book is geared towards analysts, it might be relevant to provide analytical examples; for instance, if the reader has a problem similar to the one described previously, Hadoop might be of use. Hadoop is not a universal solution to all Big Data issues; it's just a good technique to use when large data needs to be divided into small chunks and distributed across servers that need to be processed in a parallel fashion. This saves time and the cost of performing analytics over a huge dataset.</p><p>If we are able to design the Map and Reduce phase for the problem, it will be possible to solve it with <a id="id183" class="indexterm"/>MapReduce. Generally, Hadoop provides computation power to process data that does not fit into machine memory. (R users mostly found an error message while processing large data and see the following message: cannot allocate vector of size 2.5 GB.)</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec22"/>Understanding the different Java concepts used in Hadoop programming</h2></div></div></div><p>There are some <a id="id184" class="indexterm"/>classic Java concepts that make Hadoop more interactive. They are <a id="id185" class="indexterm"/>as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Remote procedure calls</strong>: This is an interprocess communication that allows a computer <a id="id186" class="indexterm"/>program to cause a subroutine or procedure to execute in another address space (commonly on another computer on shared network) without the programmer explicitly coding the details for this remote interaction. That is, the programmer writes essentially the same code whether the subroutine is local to the executing program or remote.</li><li class="listitem" style="list-style-type: disc"><strong>Serialization/Deserialization</strong>: With serialization, a <strong>Java Virtual Machine</strong> (<strong>JVM</strong>) can <a id="id187" class="indexterm"/>write out the state of the object to some stream so that<a id="id188" class="indexterm"/> we can basically read all the members and write <a id="id189" class="indexterm"/>out their state to a stream, disk, and so on. The default mechanism is in a binary format so it's more compact than the textual format. Through this, machines can send data across the network. Deserialization is vice versa and is used for receiving data objects over the network.</li><li class="listitem" style="list-style-type: disc"><strong>Java generics</strong>: This <a id="id190" class="indexterm"/>allows a type or method to operate on objects of various types while providing compile-time type safety, making Java a fully static typed language.</li><li class="listitem" style="list-style-type: disc"><strong>Java collection</strong>: This<a id="id191" class="indexterm"/> framework is a set of classes and interfaces for handling various types of data collection with single Java objects.</li><li class="listitem" style="list-style-type: disc"><strong>Java concurrency</strong>: This <a id="id192" class="indexterm"/>has been designed to support concurrent programming, and all execution takes place in the context of threads. It is mainly used for implementing computational processes as a set of threads within a single operating system process.</li><li class="listitem" style="list-style-type: disc"><strong>Plain Old Java Objects</strong> (<strong>POJO</strong>): These are actually ordinary JavaBeans. POJO<a id="id193" class="indexterm"/> is temporarily used for setting up as <a id="id194" class="indexterm"/>well <a id="id195" class="indexterm"/>as retrieving the value of data objects.</li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec25"/>Understanding the Hadoop MapReduce fundamentals</h1></div></div></div><p>To understand<a id="id196" class="indexterm"/> Hadoop MapReduce fundamentals properly, we will:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understand MapReduce objects</li><li class="listitem" style="list-style-type: disc">Learn how to decide the number of Maps in MapReduce</li><li class="listitem" style="list-style-type: disc">Learn how to decide the number of Reduces in MapReduce</li><li class="listitem" style="list-style-type: disc">Understand MapReduce dataflow</li><li class="listitem" style="list-style-type: disc">Take a closer look at Hadoop MapReduce terminologies</li></ul></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec23"/>Understanding MapReduce objects</h2></div></div></div><p>As we <a id="id197" class="indexterm"/>know, MapReduce operations in <a id="id198" class="indexterm"/>Hadoop are carried out mainly by three objects: Mapper, Reducer, and Driver.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Mapper</strong>: This is <a id="id199" class="indexterm"/>designed for the Map phase of MapReduce, which starts MapReduce operations by carrying input files and splitting them into several pieces. For each piece, it will emit a key-value data pair as the output value.</li><li class="listitem" style="list-style-type: disc"><strong>Reducer</strong>: This <a id="id200" class="indexterm"/>is designed for the Reduce phase of a MapReduce job; it accepts key-based grouped data from the Mapper output, reduces it by aggregation logic, and emits the <code class="literal">(key, value)</code> pair for the group of values.</li><li class="listitem" style="list-style-type: disc"><strong>Driver</strong>: This <a id="id201" class="indexterm"/>is the main file that drives the MapReduce process. It starts the execution of MapReduce tasks after getting a request from the client application with parameters. The Driver file is responsible for building the configuration of a job and submitting it to the Hadoop cluster. The <a id="id202" class="indexterm"/>Driver code will contain the <code class="literal">main()</code> method that accepts arguments from the command line. The input and output directory of the Hadoop MapReduce job will be accepted by this program. Driver is the main file for defining job configuration details, such as the job name, job input format, job output format, and the Mapper, Combiner, Partitioner, and Reducer classes. MapReduce is initialized by calling this <code class="literal">main()</code> function of the Driver class.</li></ul></div><p>Not every <a id="id203" class="indexterm"/>problem can be solved <a id="id204" class="indexterm"/>with a single Map and single Reduce program, but fewer can't be solved with a single Map and single Reduce task. Sometimes, it is also necessary to design the MapReduce job with multiple Map and Reduce tasks. We can design this type of job when we need to perform data operations, such as data extraction, data cleaning, and data merging, together in a single job. Many problems can be solved by writing multiple Mapper and Reducer tasks for a single job. The MapReduce steps that will be called sequentially in the case of multiple Map and Reduce tasks are Map1 followed by Reduce1, Map2 followed by Reduce2, and so on.</p><p>When we need to write a MapReduce job with multiple Map and Reduce tasks, we have to write multiple MapReduce application drivers to run them sequentially.</p><p>At the time of the MapReduce job submission, we can provide a number of Map tasks, and a number of Reducers will be created based on the output from the Mapper input and Hadoop cluster capacity. Also, note that setting the number of Mappers and Reducers is not mandatory.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec24"/>Deciding the number of Maps in MapReduce</h2></div></div></div><p>The <a id="id205" class="indexterm"/>number of Maps is <a id="id206" class="indexterm"/>usually defined by the size of the input data and size of the data split block that is calculated by the size of the HDFS file / data split. Therefore, if we have an HDFS datafile of 5 TB and a block size of 128 MB, there will be 40,960 maps present in the file. But sometimes, the number of Mappers created will be more than this count because of speculative execution. This is true when the input is a file, though it entirely depends on the <code class="literal">InputFormat</code> class.</p><p>In Hadoop MapReduce processing, there will be a delay in the result of the job when the assigned Mapper or Reducer is taking a long time to finish. If you want to avoid this, speculative execution in Hadoop can run multiple copies of the same Map or Reduce task on different nodes, and the result from the first completed nodes can be used. From the Hadoop API with the <code class="literal">setNumMapTasks(int)</code> method, we can get an idea of the number of Mappers.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec25"/>Deciding the number of Reducers in MapReduce</h2></div></div></div><p>A <a id="id207" class="indexterm"/>numbers of Reducers are<a id="id208" class="indexterm"/> created based on the Mapper's input. However, if you hardcode the number of Reducers in MapReduce, it won't matter how many nodes are present in a cluster. It will be executed as specified in the configuration.</p><p>Additionally, we can set the number of Reducers at runtime along with the MapReduce command at the command prompt <code class="literal">-D mapred.reduce.tasks</code>, with the number you want. Programmatically, it can be set via <code class="literal">conf.setNumReduceTasks(int)</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec26"/>Understanding MapReduce dataflow</h2></div></div></div><p>Now that we <a id="id209" class="indexterm"/>have seen the components that make a basic MapReduce job possible, we will distinguish how everything <a id="id210" class="indexterm"/>works together at a higher level. From the following diagram, we will understand MapReduce dataflow with multiple nodes in a Hadoop cluster:</p><div><img src="img/3282OS_02_03.jpg" alt="Understanding MapReduce dataflow"/><div><p>MapReduce dataflow</p></div></div><p>The two APIs available for Hadoop MapReduce are: New (Hadoop 1.x and 2.x) and Old Hadoop (0.20). YARN is the next generation of Hadoop MapReduce and the new Apache Hadoop<a id="id211" class="indexterm"/> subproject that has been<a id="id212" class="indexterm"/> released for Hadoop resource management.</p><p>Hadoop data processing includes several tasks that help achieve the final output from an input dataset. These tasks are as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Preloading data in HDFS.</li><li class="listitem">Running MapReduce by calling Driver.</li><li class="listitem">Reading of input data by the Mappers, which results in the splitting of the data execution of the Mapper custom logic and the generation of intermediate key-value pairs</li><li class="listitem">Executing Combiner and the shuffle phase to optimize the overall Hadoop MapReduce process.</li><li class="listitem">Sorting and providing of intermediate key-value pairs to the Reduce phase. The Reduce phase is then executed. Reducers take these partitioned key-value pairs and aggregate them based on Reducer logic.</li><li class="listitem">The final output data is stored at HDFS.</li></ol></div><p>Here, Map and Reduce tasks can be defined for several data operations as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data extraction</li><li class="listitem" style="list-style-type: disc">Data loading</li><li class="listitem" style="list-style-type: disc">Data segmentation</li><li class="listitem" style="list-style-type: disc">Data cleaning</li><li class="listitem" style="list-style-type: disc">Data transformation</li><li class="listitem" style="list-style-type: disc">Data integration</li></ul></div><p>We will explore MapReduce tasks in more detail in the next part of this chapter.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec27"/>Taking a closer look at Hadoop MapReduce terminologies</h2></div></div></div><p>In<a id="id213" class="indexterm"/> this section, we <a id="id214" class="indexterm"/>will see further details on Hadoop MapReduce dataflow with several MapReduce terminologies and their Java class details. In the MapReduce dataflow figure in the previous section, multiple nodes are connected <a id="id215" class="indexterm"/>across the network for performing distributed processing with a Hadoop setup. The ensuing attributes of the Map <a id="id216" class="indexterm"/>and Reduce phases play an important role for getting the final output.</p><p>The attributes<a id="id217" class="indexterm"/> of the Map phase are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">InputFiles</code> <a id="id218" class="indexterm"/>term refers to input, raw datasets that have been created/extracted to be analyzed for business analytics, which have been stored in HDFS. These input files are very large, and they are available in several types.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">InputFormat</code><a id="id219" class="indexterm"/> is a Java class to process the input files by obtaining the text of each line of offset and the contents. It defines how to split and read input data files. We can set the several input types, such as <code class="literal">TextInputFormat</code>, <code class="literal">KeyValueInputFormat</code>, and <code class="literal">SequenceFileInputFormat</code>, of the input format that are relevant to the Map and Reduce phase.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">InputSplits</code> <a id="id220" class="indexterm"/>class is used for setting the size of the data split.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">RecordReader</code><a id="id221" class="indexterm"/> is a Java class that comes with several methods to retrieve key and values by iterating them among the data splits. Also, it includes other methods to get the status on the current progress.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">Mapper</code> <a id="id222" class="indexterm"/>instance is created for the Map phase. The <code class="literal">Mapper</code> class takes input <code class="literal">(key, value)</code> pairs (generated by RecordReader) and produces an intermediate <code class="literal">(key, value)</code> pair by performing user-defined code in a <code class="literal">Map()</code> method. The <code class="literal">Map()</code> method mainly takes two input parameters: key and value; the remaining ones are <code class="literal">OutputCollector</code> and <code class="literal">Reporter. OutputCollector</code>. They will provide intermediate the key-value pair to reduce the phase of the job. Reporter provides the status of the current job to JobTracker periodically. The JobTracker will aggregate them for later retrieval when the job ends.</li></ul></div><p>The attributes <a id="id223" class="indexterm"/>of the Reduce phase are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">After completing the Map phase, the generated intermediate <code class="literal">(key, value)</code> pairs are partitioned based on a key attribute similarity consideration in the <code class="literal">hash</code> function. So, each Map task may emit <code class="literal">(key, value)</code> pairs to partition; all values for the same key are always reduced together without it caring about which Mapper is its origin. This partitioning and shuffling will be done automatically by the MapReduce job after the completion of the Map phase. There is no need to call them separately. Also, we can explicitly override their logic code as per the requirements of the MapReduce job.</li><li class="listitem" style="list-style-type: disc">After completing partitioning and shuffling and before initializing the Reduce task, the intermediate <code class="literal">(key, value)</code> pairs are sorted based on a key attribute <a id="id224" class="indexterm"/>value by the Hadoop MapReduce job.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">Reduce</code> instance is created for the Reduce phase. It is a section of user-provided code that performs the Reduce task. A <code class="literal">Reduce()</code> method of the <code class="literal">Reducer</code> class<a id="id225" class="indexterm"/> mainly takes two parameters along with <code class="literal">OutputCollector</code> and <code class="literal">Reporter</code>, which is the same as the <code class="literal">Map()</code> function<a id="id226" class="indexterm"/>. They are the <code class="literal">OutputCollector</code> and <code class="literal">Reporter</code> objects. <code class="literal">OutputCollector</code> in both Map and Reduce has the same functionality, but in the Reduce phase, <code class="literal">OutputCollector</code> provides output to either the next Map phase (in case of multiple map and Reduce job combinations) or reports it as the final output of the jobs based on the requirement. Apart from that, <code class="literal">Reporter</code> periodically reports to JobTracker about the current status of the running task.</li><li class="listitem" style="list-style-type: disc">Finally, in <code class="literal">OutputFormat</code> the generated output (key, value) pairs are provided to the <code class="literal">OutputCollector</code> parameter and then written to <code class="literal">OutputFiles</code>, which is governed by <code class="literal">OutputFormat</code>. It controls the setting of the <code class="literal">OutputFiles</code> format as defined in the MapReduce Driver. The format will be chosen from either <code class="literal">TextOutputFormat</code>, <code class="literal">SequenceFileOutputFileFormat</code>, or <code class="literal">NullOutputFormat</code>.</li><li class="listitem" style="list-style-type: disc">The factory <code class="literal">RecordWriter</code> used by <code class="literal">OutputFormat</code> to write the output data in the appropriate format.</li><li class="listitem" style="list-style-type: disc">The output files are the output data written to HDFS by <code class="literal">RecordWriter</code> after the completion of the MapReduce job.</li></ul></div><p>To run this MapReduce job efficiently, we need to have some knowledge of Hadoop shell commands to perform administrative tasks. Refer to the following table:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Shell commands</p>
</th><th style="text-align: left" valign="bottom">
<p>Usage and code sample</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>cat</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To copy source paths to <code class="literal">stdout</code>:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -cat URI [URI …]</strong>
</pre></div></td></tr><tr><td style="text-align: left" valign="top"><div><pre class="programlisting">
<strong>chmod</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To change the permissions of files:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI …]</strong>
</pre></div></td></tr><tr><td style="text-align: left" valign="top"><div><pre class="programlisting">
<strong>copyFromLocal</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To copy a file from local storage to HDFS:</p>
<div><pre class="programlisting">
<strong>Hadoop fs –copyFromLocal&lt;localsrc&gt; URI</strong>
</pre></div></td></tr><tr><td style="text-align: left" valign="top"><div><pre class="programlisting">
<strong>copyToLocal</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To copy a file from HDFS to local storage:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -copyToLocal [-ignorecrc] [-crc] URI &lt;localdst&gt;</strong>
</pre></div></td></tr><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>cp</strong>
</pre></div></td><td style="text-align: left" valign="top">
<p>To copy a file from the source to the destination in HDFS:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -cp URI [URI …] &lt;dest&gt;</strong>
</pre></div></td></tr><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>du</strong>
</pre></div><p>
</p></td><td style="text-align: left" valign="top">
<p>To display the aggregate length of a file:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -du URI [URI …]</strong>
</pre></div>
</td></tr><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>dus</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To display the summary of file length:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -dus&lt;args&gt;</strong>
</pre></div>
</td></tr><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>get</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To copy files to a local filesystem:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -get [-ignorecrc] [-crc] &lt;src&gt;&lt;localdst&gt;</strong>
</pre></div>
</td></tr><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>ls</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To list all files in the current directory in HDFS:</p>
<div><pre class="programlisting">
<strong>Hadoop fs –ls&lt;args&gt;</strong>
</pre></div>
</td></tr><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>mkdir</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To create a directory in HDFS:</p>
<div><pre class="programlisting">
<strong>Hadoop fs –mkdir&lt;paths&gt;</strong>
</pre></div>
</td></tr><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>lv</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To move files from the source to the destination:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -mv URI [URI …] &lt;dest&gt;</strong>
</pre></div>
</td></tr><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>rmr</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To remove files from the current directory:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -rmr URI [URI …]</strong>
</pre></div>
</td></tr><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>setrep</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To change the replication factor of a file:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -setrep [-R] &lt;path&gt;</strong>
</pre></div>
</td></tr><tr><td style="text-align: left" valign="top">
<div><pre class="programlisting">
<strong>tail</strong>
</pre></div>
</td><td style="text-align: left" valign="top">
<p>To display the last kilobyte of a file to <code class="literal">stdout</code>:</p>
<div><pre class="programlisting">
<strong>Hadoop fs -tail [-f] URI</strong>
</pre></div>
</td></tr></tbody></table></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec26"/>Writing a Hadoop MapReduce example</h1></div></div></div><p>Now we will move forward with MapReduce by learning a very common and easy example of <a id="id227" class="indexterm"/>word count. The goal of this example is to calculate how many times each word occurs in the provided documents. These documents can be considered as input to MapReduce's file.</p><p>In this example, we already have a set of text files—we want to identify the frequency of all the unique words existing in the files. We will get this by designing the Hadoop MapReduce phase.</p><p>In this section, we will see more on Hadoop MapReduce programming using Hadoop MapReduce's old API. Here we assume that the reader has already set up the Hadoop environment as described in <a class="link" href="ch01.html" title="Chapter 1. Getting Ready to Use R and Hadoop">Chapter 1</a>, <em>Getting Ready to Use R and Hadoop</em>. Also, keep in mind that we are not going to use R to count words; only Hadoop will be used here.</p><p>Basically, Hadoop MapReduce has three main objects: Mapper, Reducer, and Driver. They can be developed with three Java classes; they are the <code class="literal">Map</code> class, <code class="literal">Reduce</code> class, and <code class="literal">Driver</code> class, where the <code class="literal">Map</code> class denotes the Map phase, the <code class="literal">Reducer</code> class denotes the Reduce phase, and the <code class="literal">Driver</code> class denotes the class with the <code class="literal">main()</code> method to initialize the Hadoop MapReduce program.</p><p>In the previous section of Hadoop MapReduce fundamentals, we already discussed what Mapper, Reducer, and Driver are. Now, we will learn how to define them and program for them in Java. In upcoming chapters, we will be learning to do more with a combination of R and Hadoop.</p><div><div><h3 class="title"><a id="tip12"/>Tip</h3><p>There are many languages and frameworks that are used for building MapReduce, but each of them has different strengths. There are multiple factors that by modification can provide high latency over MapReduce. Refer to the article <em>10 MapReduce Tips</em><a id="id228" class="indexterm"/> by Cloudera at <a class="ulink" href="http://blog.cloudera.com/blog/2009/05/10-mapreduce-tips/">http://blog.cloudera.com/blog/2009/05/10-mapreduce-tips/</a>.</p><p>To make MapReduce development easier, use <a id="id229" class="indexterm"/>
<strong>Eclipse</strong> configured with <a id="id230" class="indexterm"/>
<strong>Maven</strong>, which supports the old MapReduce API.</p></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec28"/>Understanding the steps to run a MapReduce job</h2></div></div></div><p>Let's see <a id="id231" class="indexterm"/>the steps to run a MapReduce job with Hadoop:</p><div><ol class="orderedlist arabic"><li class="listitem">In the initial steps of preparing Java classes, we need you to develop a Hadoop MapReduce program as per the definition of our business problem. In this <a id="id232" class="indexterm"/>example, we have considered a word count problem. So, we have developed three Java classes for the MapReduce program; they are <code class="literal">Map.java</code>, <code class="literal">Reduce.java</code>, and <code class="literal">WordCount.java</code>, used for calculating the frequency of the word in the provided text files.<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Map.java</code>: This is the Map class for the word count Mapper.<div><pre class="programlisting">// Defining package of the class
package com.PACKT.chapter1;

// Importing java libraries 
import java.io.*;
importjava.util.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;

// Defining the Map class
public class Map extends MapReduceBase implements
         Mapper&lt;LongWritable, 
                Text, 
                Text, 
                IntWritable&gt;{

//Defining the map method – for processing the data with // problem specific logic
public void map(LongWritable key,
                Text value,
                OutputCollector&lt;Text,
                IntWritable&gt; output,
                Reporter reporter) 
                throws IOException {

// For breaking the string to tokens and convert them to lowercase
StringTokenizer st = new StringTokenizer(value.toString().toLowerCase());

// For every string tokens
while(st.hasMoreTokens()) {

// Emitting the (key,value) pair with value 1.
output.collect(new Text(st.nextToken()), 
               new IntWritable(1));
        }

    }

}</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">Reduce.java</code>: This<a id="id233" class="indexterm"/> is the Reduce class for the word count Reducer.<div><pre class="programlisting">// Defining package of the class
package com.PACKT.chapter1;

// Importing java libraries
import java.io.*;
importjava.util.*;
import org.apache.hadoop.io.*;
importorg.apache.hadoop.mapred.*;

// Defining the Reduce class 
public class Reduce extends MapReduceBase implements
          Reducer&lt;Text,
                  IntWritable,
                  Text,
                  IntWritable&gt; {

// Defining the reduce method for aggregating the //generated output of Map phase
public void reduce(Text key,
                   Iterator&lt;IntWritable&gt; values,
                   OutputCollector&lt;Text,IntWritable&gt;
                   output, 
                   Reporter reporter) throws IOException {

// Setting initial counter value as 0
int count = 0;

// For every element with similar key attribute, increment its counter value by adding 1.
while(values.hasNext()) {
count += values.next().get();
        }

// Emitting the (key,value) pair
output.collect(key, new IntWritable(count));
    }
}</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">WordCount.java</code>: This is the task of Driver in the Hadoop MapReduce <a id="id234" class="indexterm"/>Driver main file.<div><pre class="programlisting">//Defining package of the class
package com.PACKT.chapter1;

// Importing java libraries
import java.io.*;
importorg.apache.hadoop.fs.*;
import org.apache.hadoop.io.*;
importorg.apache.hadoop.mapred.*;
importorg.apache.hadoop.util.*;
importorg.apache.hadoop.conf.*;

//Defining wordcount class for job configuration 
  // information
public class WordCount extends Configured implements Tool{

publicint run(String[] args) throws IOException{
JobConfconf = new JobConf(WordCount.class);
conf.setJobName("wordcount");

//For defining the output key format
conf.setOutputKeyClass(Text.class);

//For defining the output value format
conf.setOutputValueClass(IntWritable.class);

// For defining the Mapper class implementation
conf.setMapperClass(Map.class);

// For defining the Reducer class implementation
conf.setReducerClass(Reduce.class);

// For defining the type of input format 
conf.setInputFormat(TextInputFormat.class);

// For defining the type of output format
conf.setOutputFormat(TextOutputFormat.class);

// For defining the command line argument sequence for // input dataset path
FileInputFormat.setInputPaths(conf, new Path(args[0]));

// For defining the command line argument sequence for // output dataset path
FileOutputFormat.setOutputPath(conf, new Path(args[1]));


// For submitting the configuration object
JobClient.runJob(conf);

return 0;
    }

// Defining the main() method to start the execution of // the MapReduce program
public static void main(String[] args) throws Exception {
  intexitCode = ToolRunner.run(new WordCount(), args);
  System.exit(exitCode); } }</pre></div></li></ul></div></li><li class="listitem">Compile <a id="id235" class="indexterm"/>the Java classes.<div><pre class="programlisting">
<strong>// create a folder for storing the compiled classes</strong>
<strong>hduser@ubuntu:~/Desktop/PacktPub$ mkdir classes</strong>

<strong>// compile the java class files with classpath</strong>
<strong>hduser@ubuntu:~/Desktop/PacktPub$ javac -classpath /usr/local/hadoop/hadoop-core-1.1.0.jar:/usr/local/hadoop/lib/commons-cli-1.2.jar -d classes *.java</strong>
</pre></div></li><li class="listitem">Create a <code class="literal">.jar</code> file<a id="id236" class="indexterm"/> from the compiled classes.<div><pre class="programlisting">
<strong>hduser@ubuntu:~/Desktop/PacktPub$ cd classes/</strong>

<strong>// create jar of developed java classes</strong>
<strong>hduser@ubuntu:~/Desktop/PacktPub/classes$ jar -cvf wordcount.jar com</strong>
</pre></div></li><li class="listitem">Start the Hadoop daemons.<div><pre class="programlisting">
<strong>// Go to Hadoop home Directory</strong>
<strong>hduser@ubuntu:~$ cd $HADOOP_HOME</strong>

<strong>// Start Hadoop Cluster</strong>
<strong>hduser@ubuntu:/usr/local/hadoop$ bin/start-all.sh</strong>
</pre></div></li><li class="listitem">Check all the running daemons.<div><pre class="programlisting">
<strong>// Ensure all daemons are running properly </strong>
<strong>hduser@ubuntu:/usr/local/hadoop$ jps</strong>
</pre></div></li><li class="listitem">Create the HDFS directory <code class="literal">/wordcount/input/</code>.<div><pre class="programlisting">
<strong>// Create Hadoop directory for storing the input dataset</strong>
<strong>hduser@ubuntu:/usr/local/hadoop$ bin/Hadoop fs -mkdir /wordcount/input</strong>
</pre></div></li><li class="listitem">Extract <a id="id237" class="indexterm"/>the input dataset to be used in the word count example. As we need to have text files to be processed by the word count example, we will use the text files provided with the Hadoop distribution (<code class="literal">CHANGES.txt</code>, <code class="literal">LICENSE.txt</code>, <code class="literal">NOTICE.txt</code>, and <code class="literal">README.txt</code>) by copying them to the Hadoop directory. We can have other text datasets from the Internet input in this MapReduce algorithm instead of using readymade text files. We can also extract data from the Internet to process them, but here we are using readymade input files.</li><li class="listitem">Copy all the text files to HDFS.<div><pre class="programlisting">
<strong>// To copying the text files from machine's local</strong>
<strong>  // directory in to Hadoop directory</strong>

<strong>hduser@ubuntu:/usr/local/hadoop$ bin/hadoopfs -copyFromLocal $HADOOP_HOME/*.txt /wordcount/input/</strong>
</pre></div></li><li class="listitem">Run the Hadoop MapReduce job with the following command:<div><pre class="programlisting">
<strong>// Command for running the Hadoop job by specifying jar, main class, input directory and output directory.</strong>

<strong>hduser@ubuntu:/usr/local/hadoop$ bin/hadoop jar wordcount.jar com.PACKT.chapter1.WordCount /wordcount/input/ /wordcount/output/</strong>
</pre></div></li><li class="listitem">This is how the final output will look.<div><pre class="programlisting">
<strong>// To read the generated output from HDFS directory</strong>

<strong>hduser@ubuntu:/usr/local/hadoop$ bin/hadoopfs -cat /wordcount/output/part-00000</strong>
</pre></div><div><div><h3 class="title"><a id="tip13"/>Tip</h3><p>During <a id="id238" class="indexterm"/>the MapReduce phase, you need to monitor the job as well as the nodes. Use the following to monitor MapReduce jobs in web browsers:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">localhost:50070: NameNode Web interface (for HDFS)</li><li class="listitem" style="list-style-type: disc"><code class="literal">localhost:50030</code>: JobTracker Web interface (for MapReduce layer)</li><li class="listitem" style="list-style-type: disc"><code class="literal">localhost:50060</code>: TaskTracker Web interface (for MapReduce layer)</li></ul></div></div></div></li></ol></div><div><div><div><div><h3 class="title"><a id="ch02lvl3sec12"/>Learning to monitor and debug a Hadoop MapReduce job</h3></div></div></div><p>In this section, we <a id="id239" class="indexterm"/>will learn how to monitor as well as<a id="id240" class="indexterm"/> debug a Hadoop MapReduce job without any commands.</p><p>This is one of the easiest ways to use the Hadoop MapReduce administration UI. We can access this via a browser by entering the URL <code class="literal">http://localhost:50030</code> (web UI for the JobTracker daemon). This will show the logged information of the Hadoop MapReduce jobs, which looks like following screenshot:</p><div><img src="img/3282OS_02_04.jpg" alt="Learning to monitor and debug a Hadoop MapReduce job"/><div><p>Map/Reduce administration</p></div></div><p>Here we can check the information and status of running jobs, the status of the Map and Reduce tasks <a id="id241" class="indexterm"/>of a job, and the past completed jobs<a id="id242" class="indexterm"/> as well as failed jobs with failed Map and Reduce tasks. Additionally, we can debug a MapReduce job by clicking on the hyperlink of the failed Map or Reduce task of the failed job. This will produce an error message printed on standard output while the job is running.</p></div><div><div><div><div><h3 class="title"><a id="ch02lvl3sec13"/>Exploring HDFS data</h3></div></div></div><p>In this section, we <a id="id243" class="indexterm"/>will see how to explore HDFS directories without <a id="id244" class="indexterm"/>running any <strong>Bash</strong> command. The web UI of the NameNode daemon provides such a facility. We just need to locate it at <code class="literal">http://localhost:50070</code>.</p><div><img src="img/3282OS_02_05.jpg" alt="Exploring HDFS data"/><div><p>NameNode administration</p></div></div><p>This UI enables us to get a cluster summary (memory status), NameNode logs, as well as information on<a id="id245" class="indexterm"/> live and dead nodes in the cluster. Also, this allows us to explore the Hadoop directory that we have created for storing input and output data for Hadoop MapReduce jobs.</p></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec29"/>Understanding several possible MapReduce definitions to solve business problems</h2></div></div></div><p>Until now we have learned what MapReduce is and how to code it. Now, we will see some common MapReduce problem definitions that are used for business analytics. Any reader who <a id="id246" class="indexterm"/>knows MapReduce <a id="id247" class="indexterm"/>with Hadoop will easily be able to code and solve these problem definitions by modifying the MapReduce example for word count. The major changes will be in data parsing and in the logic behind operating the data. The major effort will be required in data collection, data cleaning, and data storage.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Server web log processing</strong>: Through this MapReduce definition, we can perform web log analysis. Logs of the web server provide information about web requests, such as requested page's URL, date, time, and protocol. From this, we can identify the peak load hours of our website from the web server log and scale <a id="id248" class="indexterm"/>our web <a id="id249" class="indexterm"/>server configuration <a id="id250" class="indexterm"/>based on the traffic on the site. So, the identification of no traffic at night will help us save money by scaling down the server. Also, there are a number of business cases that can be solved by this web log server analysis.</li><li class="listitem" style="list-style-type: disc"><strong>Web analytics with website statistics</strong>: Website statistics can provide more detailed <a id="id251" class="indexterm"/>information about the visitor's metadata, such as the source, campaign, visitor type, visitor location, search keyword, requested page URL, browser, and total time spent on pages. Google analytics is one of the popular, free service providers for websites. By analyzing all this information, we can understand visitors' behavior on a website. By descriptive analytics, we can identify the importance of web pages or other web attributes based on visitors' addiction towards them. For an e-commerce website, we can identify popular products based on the total number of visits, page views, and time spent by a visitor on a page. Also, predictive analytics can be implemented on web data to predict the business.</li><li class="listitem" style="list-style-type: disc"><strong>Search engine</strong>: Suppose we have a large set of documents and want to search the <a id="id252" class="indexterm"/>document for a specific keyword, inverted indices with Hadoop MapReduce will help us find keywords so we can build a search engine for Big Data.</li><li class="listitem" style="list-style-type: disc"><strong>Stock market analysis</strong>: Let's say that we have collected stock market data (Big Data) for a <a id="id253" class="indexterm"/>long period of time and now want to identify the pattern and predict it for the next time period. This requires training of all historical datasets. Then we can compute the frequency of the stock market changes for the said time period using several machine-learning libraries with Hadoop MapReduce.</li></ul></div><p>Also, there are too many possible MapReduce applications that can be applied to improve business cost.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec27"/>Learning the different ways to write Hadoop MapReduce in R</h1></div></div></div><p>We know that Hadoop Big Data processing with MapReduce is a big deal for statisticians, web analysts, and product managers who used to use the R tool for analyses because <a id="id254" class="indexterm"/>supplementary programming knowledge <a id="id255" class="indexterm"/>of MapReduce is required to migrate the analyses into MapReduce with Hadoop. Also, we know R is a tool that is consistently increasing in popularity; there are many packages/libraries that are being developed for integrating with R. So to develop a MapReduce algorithm or program that runs with the log of R and computation power of Hadoop, we require the middleware for R and Hadoop. RHadoop, RHIPE, and Hadoop streaming are the middleware that help develop and execute Hadoop MapReduce within R. In this last section, we will talk about RHadoop, RHIPE, and introducing Hadoop streaming, and from the later chapters we will purely develop MapReduce with these packages.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec30"/>Learning RHadoop</h2></div></div></div><p>RHadoop is a<a id="id256" class="indexterm"/> great open source software framework of R for performing data analytics with the Hadoop platform via R functions. RHadoop has been developed by <a id="id257" class="indexterm"/>
<strong>Revolution Analytics</strong>, which is the leading commercial provider of software and services based on the open source R project for statistical computing. The RHadoop project has three different R packages: <code class="literal">rhdfs</code>, <code class="literal">rmr</code>, and <code class="literal">rhbase</code>. All these packages are implemented and tested on the Cloudera Hadoop distributions CDH3, CDH4, and R 2.15.0. Also, these are tested with the R version 4.3, 5.0, and 6.0 distributions of Revolution Analytics.</p><p>These three different R packages have been designed on Hadoop's two main features HDFS and MapReduce:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">rhdfs</code>: This is an <a id="id258" class="indexterm"/>R package for providing all Hadoop HDFS access to R. All distributed files can be managed with R functions.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rmr</code>: This is an <a id="id259" class="indexterm"/>R package for providing Hadoop MapReduce interfaces to R. With the help of this package, the Mapper and Reducer can easily be developed.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhbase</code>: This<a id="id260" class="indexterm"/> is an R package for handling data at HBase distributed database through R.</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec31"/>Learning RHIPE</h2></div></div></div><p><strong>R and Hadoop Integrated Programming Environment</strong> (<strong>RHIPE</strong>) is a free and open source <a id="id261" class="indexterm"/>project. RHIPE is widely used for performing Big Data analysis with <strong>D&amp;R</strong> analysis<a id="id262" class="indexterm"/>. D&amp;R analysis is used to divide huge data, process it in parallel on a distributed network to produce intermediate output, and finally recombine all this intermediate output into a set. RHIPE is designed to carry out D&amp;R analysis on complex Big Data in R on the Hadoop platform. RHIPE was developed by <em>Saptarshi Joy Guha</em> (Data Analyst at Mozilla Corporation) and her team as part of her PhD thesis in the Purdue Statistics Department.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec32"/>Learning Hadoop streaming</h2></div></div></div><p>Hadoop streaming is a utility that comes with the Hadoop distribution. This utility allows<a id="id263" class="indexterm"/> you to create and run MapReduce jobs with any executable or script as the Mapper and/or Reducer. This is supported by R, Python, Ruby, Bash, Perl, and so on. We will use the R language with a bash script.</p><p>Also, there is one R package named <code class="literal">HadoopStreaming</code> that has been developed for performing data analysis on Hadoop clusters with the help of R scripts, which is an interface to Hadoop streaming with R. Additionally, it also allows the running of MapReduce tasks without Hadoop. This package was developed by <em>David Rosenberg</em>, Chief Scientist at SenseNetworks. He has expertise in machine learning and statistical modeling.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec28"/>Summary</h1></div></div></div><p>In this chapter, we have seen what Hadoop MapReduce is, and how to develop it as well as run it. In the next chapter, we will learn how to install RHIPE and RHadoop, and develop MapReduce and its available functional libraries with examples.</p></div></body></html>