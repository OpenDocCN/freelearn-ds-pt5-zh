["```py\ndate, source, pageTitle, pagePath\n```", "```py\n# Loading the RGoogleAnalytics library\nrequire(\"RGoogleAnalyics\")\n\n# Step 1\\. Authorize your account and paste the access_token\nquery <- QueryBuilder()\naccess_token <- query$authorize()\n\n# Step 2\\. Create a new Google Analytics API object\nga <- RGoogleAnalytics()\n\n# To retrieve profiles from Google Analytics\nga.profiles <- ga$GetProfileData(access_token)\n\n# List the GA profiles \nga.profiles\n\n# Step 3\\. Setting up the input parameters\nprofile <- ga.profiles$id[3] \nstartdate <- \"2010-01-08\"\nenddate <- \"2013-08-23\"\ndimension <- \"ga:date,ga:source,ga:pageTitle,ga:pagePath\"\nmetric <- \"ga:visits\"\nsort <- \"ga:visits\"\nmaxresults <- 100099\n\n# Step 4\\. Build the query string, use the profile by setting its index value\nquery$Init(start.date = startdate,\n           end.date = enddate,\n           dimensions = dimension,\n           metrics = metric,\n\n           max.results = maxresults,\n           table.id = paste(\"ga:\",profile,sep=\"\",collapse=\",\"),\n           access_token=access_token)\n\n# Step 5\\. Make a request to get the data from the API\nga.data <- ga$GetReportData(query)\n\n# Look at the returned data\nhead(ga.data)\nwrite.csv(ga.data,\"webpages.csv\", row.names=FALSE)\n```", "```py\n    pagePath <- as.character(data$pagePath)\n    pagePath <- strsplit(pagePath,\"\\\\?\")\n    pagePath <- do.call(\"rbind\", pagePath)\n    pagePath <- pagePath [,1]\n    ```", "```py\n    data  <- data.frame(source=data$source, pagePath=d,visits =)\n    write.csv(data, \"webpages_mapreduce.csv\" , row.names=FALSE)\n    ```", "```py\n    # setting up the Hadoop variables need by RHadoop\n    Sys.setenv(HADOOP_HOME=\"/usr/local/hadoop/\")\n    Sys.setenv(HADOOP_CMD=\"/usr/local/hadoop/bin/hadoop\")\n\n    # Loading the RHadoop libraries rmr2 and rhdfs\n    library(rmr2)\n    library(rhdfs)\n\n    # To initializing hdfs\n    hdfs.init()\n    ```", "```py\n    # First uploading the data to R console,\n    webpages <- read.csv(\"/home/vigs/Downloads/webpages_mapreduce.csv\")\n\n    # saving R file object to HDFS,\n    webpages.hdfs <- to.dfs(webpages) \n    ```", "```py\n    mapper1 <- function(k,v) {\n\n     # To storing pagePath column data in to key object\n     key <- v[2]\n\n     # To store visits column data into val object\n     Val <- v[3]\n\n     # emitting key and value for each row\n     keyval(key, val)\n    }\n    totalvisits <- sum(webpages$visits)\n    ```", "```py\n    reducer1 <- function(k,v) {\n\n      # Calculating percentage visits for the specific URL\n      per <- (sum(v)/totalvisits)*100\n      # Identify the category of URL\n      if (per <33 )\n     {\n    val <- \"low\"\n    }\n     if (per >33 && per < 67)\n     {\n     val <- \"medium\"\n     }\n     if (per > 67)\n     {\n     val <- \"high\"\n     }\n\n     # emitting key and values\n     keyval(k, val)\n    }\n    ```", "```py\n    #Mapper:\n    mapper2 <- function(k, v) {\n\n    # Reversing key and values and emitting them \n     keyval(v,k)\n    }\n    ```", "```py\n    key <- NA\n    val <- NULL\n    # Reducer:\n    reducer2  <-  function(k, v) {\n\n    # for checking whether key-values are already assigned or not.\n     if(is.na(key)) {\n     key <- k\n     val <- v\n      } else {\n       if(key==k) {\n     val <- c(val,v)\n      } else{\n       key <- k\n       val <- v\n      }\n     }\n    # emitting key and list of values \n    keyval(key,list(val))\n    }\n    ```", "```py\nfrom.dfs(output)\n\n```", "```py\nfrom.dfs(output)\n\n```", "```py\nstock_BP <- read.csv(\"http://ichart.finance.yahoo.com/table.csv?s=BP\")\n\n```", "```py\nwget http://ichart.finance.yahoo.com/table.csv?s=BP\n#exporting to csv file\n\nwrite.csv(stock_BP,\"table.csv\", row.names=FALSE)\n\n```", "```py\n# creating /stock directory in hdfs\nbin/hadoop dfs -mkdir /stock\n\n# uploading table.csv to hdfs in /stock directory\nbin/hadoop dfs -put /home/Vignesh/downloads/table.csv /stock/ \n\n```", "```py\n    #! /usr/bin/env/Rscript\n    # To disable the warnings\n    options(warn=-1)\n    # To take input the data from streaming\n    input <- file(\"stdin\", \"r\")\n\n    # To reading the each lines of documents till the end\n    while(length(currentLine <-readLines(input, n=1, warn=FALSE)) > 0)\n    {\n\n    # To split the line by \",\" seperator\n    fields <- unlist(strsplit(currentLine, \",\"))\n\n    # Capturing open column value\n     open <- as.double(fields[2])\n\n    # Capturing close columns value\n     close <- as.double(fields[5])\n\n    # Calculating the difference of close and open attribute\n     change <- (close-open)\n\n    # emitting change as key and value as 1\n    write(paste(change, 1, sep=\"\\t\"), stdout())\n    }\n\n    close(input)\n\n    ```", "```py\n    #! /usr/bin/env Rscript\n    stock.key <- NA\n    stock.val <- 0.0\n\n    conn <- file(\"stdin\", open=\"r\")\n    while (length(next.line <- readLines(conn, n=1)) > 0) {\n     split.line <- strsplit(next.line, \"\\t\")\n     key <- split.line[[1]][1]\n     val <- as.numeric(split.line[[1]][2])\n     if (is.na(current.key)) {\n     current.key <- key\n     current.val <- val\n     }\n     else {\n     if (current.key == key) {\n    current.val <- current.val + val\n    }\n    else {\n    write(paste(current.key, current.val, sep=\"\\t\"), stdout())\n    current.key <- key\n    current.val<- val\n    }\n    }\n    }\n    write(paste(current.key, current.val, sep=\"\\t\"), stdout())\n    close(conn)\n\n    ```", "```py\n# For locating at Hadoop Directory\nsystem(\"cd $HADOOP_HOME\")\n\n# For listing all HDFS first level directorysystem(\"bin/hadoop dfs -ls /\")\n\n# For running Hadoop MapReduce with streaming parameters\nsystem(paste(\"bin/hadoop jar \n/usr/local/hadoop/contrib/streaming/hadoop-streaming-1.0.3.jar \",\n\n\" -input /stock/table.csv\",\n\" -output /stock/outputs\",\n\" -file /usr/local/hadoop/stock/stock_mapper.R\",\n\" -mapper /usr/local/hadoop/stock/stock_mapper.R\",\n\" -file /usr/local/hadoop/stock/stock_reducer.R\",\n\" -reducer /usr/local/hadoop/stock/stock_reducer.R\"))\n\n# For storing the output of list command \ndir <- system(\"bin/hadoop dfs -ls /stock/outputs\", intern=TRUE)\ndir\n\n# For storing the output from part-oooo (output file)\nout <- system(\"bin/hadoop dfs -cat /stock/outputs/part-00000\", intern=TRUE)\n\n# displaying Hadoop MapReduce output data out\n\n```", "```py\nbin/hadoop jar /usr/local/hadoop/contrib/streaming/hadoop-streaming-1.0.3.jar \\\n\n -input /stock/table.csv \\\n -output /stock/outputs\\\n -file /usr/local/hadoop/stock/stock_mapper.R \\\n -mapper /usr/local/hadoop/stock/stock_mapper.R \\\n -file /usr/local/hadoop/stock/stock_reducer.R \\\n -reducer /usr/local/hadoop/stock/stock_reducer.R \n\n```", "```py\n    bin/hadoop dfs -cat /stock/outputs/part-00000\n\n    ```", "```py\n    change    frequency\n\n    ```", "```py\n# Loading ggplot2 library\nlibrary(ggplot2);\n\n# we have stored above terminal output to stock_output.txt file\n\n#loading it to R workspace\nmyStockData <- read.delim(\"stock_output.txt\", header=F, sep=\"\", dec=\".\");\n\n# plotting the data with ggplot2 geom_smooth function\nggplot(myStockData, aes(x=V1, y=V2)) + geom_smooth() + geom_point();\n```", "```py\n# Loading Train.csv dataset which includes the Sales as well as machine identifier data attributes.\n\ntransactions <- read.table(file=\"~/Downloads/Train.csv\",\nheader=TRUE,\nsep=\",\",\nquote=\"\\\"\",\nrow.names=1,\nfill=TRUE,\ncolClasses=c(MachineID=\"factor\",\n ModelID=\"factor\",\ndatasource=\"factor\",\nYearMade=\"character\",\nSalesID=\"character\",\nauctioneerID=\"factor\",\nUsageBand=\"factor\",\nsaledate=\"custom.date.2\",\nTire_Size=\"tire.size\",\nUndercarriage_Pad_Width=\"undercarriage\",\nStick_Length=\"stick.length\"),\nna.strings=na.values)\n\n# Loading Machine_Appendix.csv for machine configuration information\n\nmachines <- read.table(file=\"~/Downloads/Machine_Appendix.csv\",\nheader=TRUE,\nsep=\",\",\nquote=\"\\\"\",\nfill=TRUE,\ncolClasses=c(MachineID=\"character\",\nModelID=\"factor\",\nfiManufacturerID=\"factor\"),\nna.strings=na.values)\n\n# Updating the values to numeric \n# updating sale data number\ntransactions$saledatenumeric <- as.numeric(transactions$saledate)\ntransactions$ageAtSale <- as.numeric(transactions$saledate - as.Date(transactions$YearMade, format=\"%Y\"))\n\ntransactions$saleYear <- as.numeric(format(transactions$saledate, \"%Y\"))\n\n# updating the month of sale from transaction\ntransactions$saleMonth <- as.factor(format(transactions$saledate, \"%B\"))\n\n# updating the date of sale from transaction\ntransactions$saleDay <- as.factor(format(transactions$saledate, \"%d\"))\n\n# updating the day of week of sale from transaction\ntransactions$saleWeekday <- as.factor(format(transactions$saledate, \"%A\"))\n\n# updating the year of sale from transaction\ntransactions$YearMade <- as.integer(transactions$YearMade)\n\n# deriving the model price from transaction\ntransactions$MedianModelPrice <- unsplit(lapply(split(transactions$SalePrice, \ntransactions$ModelID), median), transactions$ModelID)\n\n# deriving the model count from transaction\ntransactions$ModelCount <- unsplit(lapply(split(transactions$SalePrice, transactions$ModelID), length), transactions$ModelID)\n\n# Merging the transaction and machine data in to dataframe \ntraining.data <- merge(x=transactions, y=machines, by=\"MachineID\")\n\n# write denormalized data out\nwrite.table(x=training.data,\nfile=\"~/temp/training.csv\",\nsep=\",\",\nquote=TRUE,\nrow.names=FALSE,\neol=\"\\n\",\ncol.names=FALSE)\n# Create poisson directory at HDFS\nbin/hadoop dfs -mkdir /poisson\n\n# Uploading file training.csv at HDFS\nbin/hadoop dfs -put ~/temp/training.csv /poisson/\n```", "```py\n    T = 0.1  # param 1: K / N-average fraction of input data in each model 10%\n\n    M = 50   # param 2: number of models\n    ```", "```py\n    def map(k, v):\n    // for each input data point\n        for i in 1:M  \n        // for each model\n            m = Poisson(T)  \n        // num times curr point should appear in this sample\n            if m > 0\n                for j in 1:m\n       // emit current input point proper num of times\n                    emit (i, v)\n    ```", "```py\n    def reduce(k, v):\n        fit model or calculate statistic with the sample in v\n    ```", "```py\n#10% of input data to each sample on avg\nfrac.per.model <- 0.1  \nnum.models <- 50\n```", "```py\n    poisson.subsample <- function(k, input) {\n      # this function is used to generate a sample from the current block of data\n      generate.sample <- function(i) {\n        # generate N Poisson variables\n        draws <- rpois(n=nrow(input), lambda=frac.per.model)\n        # compute the index vector for the corresponding rows,\n        # weighted by the number of Poisson draws\n        indices <- rep((1:nrow(input)), draws)\n        # emit the rows; RHadoop takes care of replicating the key appropriately\n        # and rbinding the data frames from different mappers together for the\n        # reducer\n        keyval(i, input[indices, ])\n      }\n\n      # here is where we generate the actual sampled data\n      c.keyval(lapply(1:num.models, generate.sample))\n    }\n    ```", "```py\n    # REDUCE function\n    fit.trees <- function(k, v) {\n      # rmr rbinds the emitted values, so v is a dataframe\n      # note that do.trace=T is used to produce output to stderr to keep the reduce task from timing out\n      rf <- randomForest(formula=model.formula,\n                            data=v,\n                            na.action=na.roughfix,\n                            ntree=10,\n                            do.trace=FALSE)\n\n     # rf is a list so wrap it in another list to ensure that only\n     # one object gets emitted. this is because keyval is vectorized\n      keyval(k, list(forest=rf))\n    }\n    ```", "```py\n    model.formula <- SalePrice ~ datasource + auctioneerID + YearMade + saledatenumeric + ProductSize + ProductGroupDesc.x + Enclosure + Hydraulics + ageAtSale + saleYear + saleMonth + saleDay + saleWeekday + MedianModelPrice + ModelCount + MfgYear\n\n    ```", "```py\n    mapreduce(input=\"/poisson/training.csv\",\n     input.format=bulldozer.input.format,\n     map=poisson.subsample,\n     reduce=fit.trees,\n     output=\"/poisson/output\")\n\n    ```", "```py\n    mraw.forests <- values(from.dfs(\"/poisson/output\"))\n    forest <- do.call(combine, raw.forests)\n\n    ```"]