["```py\n# Run application as standalone mode on 8 cores\nSPARK_HOME/bin/spark-submit \\\n --class org.apache.spark.examples.KMeansDemo \\\n --master local[8] \\\n KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \\\n Saratoga_NY_Homes.txt\n\n```", "```py\nspark.eventLog.enabled=true \nspark.eventLog.dir=file:///home/username/log\"\n\n```", "```py\n# Run application as standalone mode on 8 cores\nSPARK_HOME/bin/spark-submit \\\n --conf \"spark.eventLog.enabled=true\" \\\n --conf \"spark.eventLog.dir=file:///tmp/test\" \\\n --class org.apache.spark.examples.KMeansDemo \\\n --master local[8] \\\n KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \\\n Saratoga_NY_Homes.txt\n\n```", "```py\n yarn logs -applicationId <application ID> [OPTIONS]\n\n```", "```py\n yarn logs -applicationId application_561453090098_0005 \n yarn logs -applicationId application_561453090070_0005 userid\n\n```", "```py\nlog4j.logger.spark.storage=INFO, RollingAppender\nlog4j.additivity.spark.storage=false\nlog4j.logger.spark.scheduler=INFO, RollingAppender\nlog4j.additivity.spark.scheduler=false\nlog4j.logger.spark.CacheTracker=INFO, RollingAppender\nlog4j.additivity.spark.CacheTracker=false\nlog4j.logger.spark.CacheTrackerActor=INFO, RollingAppender\nlog4j.additivity.spark.CacheTrackerActor=false\nlog4j.logger.spark.MapOutputTrackerActor=INFO, RollingAppender\nlog4j.additivity.spark.MapOutputTrackerActor=false\nlog4j.logger.spark.MapOutputTracker=INFO, RollingAppender\nlog4j.additivty.spark.MapOutputTracker=false\n\n```", "```py\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.log4j.LogManager\nimport org.apache.log4j.Level\nimport org.apache.log4j.Logger\n\nobject MyLog {\n def main(args: Array[String]):Unit= {\n   // Stting logger level as WARN\n   val log = LogManager.getRootLogger\n   log.setLevel(Level.WARN)\n\n   // Creating Spark Context\n   val conf = new SparkConf().setAppName(\"My App\").setMaster(\"local[*]\")\n   val sc = new SparkContext(conf)\n\n   //Started the computation and printing the logging information\n   log.warn(\"Started\")                        \n   val data = sc.parallelize(1 to 100000)\n   log.warn(\"Finished\")\n }\n}\n\n```", "```py\nobject MyLog {\n  def main(args: Array[String]):Unit= {\n    // Stting logger level as WARN\n    val log = LogManager.getRootLogger\n    log.setLevel(Level.WARN)\n    // Creating Spark Context\n    val conf = new SparkConf().setAppName(\"My App\").setMaster(\"local[*]\")\n    val sc = new SparkContext(conf)\n    //Started the computation and printing the logging information\n    log.warn(\"Started\")\n    val i = 0\n    val data = sc.parallelize(i to 100000)\n    data.foreach(i => log.info(\"My number\"+ i))\n    log.warn(\"Finished\")\n  }\n}\n\n```", "```py\nclass MyMapper(n: Int) extends Serializable{\n  @transient lazy val log = org.apache.log4j.LogManager.getLogger\n                                (\"myLogger\")\n  def MyMapperDosomething(rdd: RDD[Int]): RDD[String] =\n   rdd.map{ i =>\n    log.warn(\"mapping: \" + i)\n    (i + n).toString\n  }\n}\n\n```", "```py\npackage com.example.Personal\nimport org.apache.log4j.{Level, LogManager, PropertyConfigurator}\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD\n\nclass MyMapper(n: Int) extends Serializable{\n  @transient lazy val log = org.apache.log4j.LogManager.getLogger\n                                (\"myLogger\")\n  def MyMapperDosomething(rdd: RDD[Int]): RDD[String] =\n   rdd.map{ i =>\n    log.warn(\"Serialization of: \" + i)\n    (i + n).toString\n  }\n}\n\nobject MyMapper{\n  def apply(n: Int): MyMapper = new MyMapper(n)\n}\n\nobject MyLog {\n  def main(args: Array[String]) {\n    val log = LogManager.getRootLogger\n    log.setLevel(Level.WARN)\n    val conf = new SparkConf().setAppName(\"My App\").setMaster(\"local[*]\")\n    val sc = new SparkContext(conf)\n    log.warn(\"Started\")\n    val data = sc.parallelize(1 to 100000)\n    val mapper = MyMapper(1)\n    val other = mapper.MyMapperDosomething(data)\n    other.collect()\n    log.warn(\"Finished\")\n  }\n}\n\n```", "```py\n17/04/29 15:33:43 WARN root: Started \n.\n.\n17/04/29 15:31:51 WARN myLogger: mapping: 1 \n17/04/29 15:31:51 WARN myLogger: mapping: 49992\n17/04/29 15:31:51 WARN myLogger: mapping: 49999\n17/04/29 15:31:51 WARN myLogger: mapping: 50000 \n.\n. \n17/04/29 15:31:51 WARN root: Finished\n\n```", "```py\nsetAppName() // App name \nsetMaster() // Master URL \nsetSparkHome() // Set the location where Spark is installed on worker nodes. \nsetExecutorEnv() // Set single or multiple environment variables to be used when launching executors. \nsetJars() // Set JAR files to distribute to the cluster. \nsetAll() // Set multiple parameters together.\n\n```", "```py\nval conf = new SparkConf() \n             .setMaster(\"local[2]\") \n             .setAppName(\"SampleApp\") \nval sc = new SparkContext(conf)\n\n```", "```py\nval sc = new SparkContext(new SparkConf())\n\n```", "```py\nSPARK_HOME/bin/spark-submit \n --name \"SmapleApp\" \\\n --class org.apache.spark.examples.KMeansDemo \\\n --master mesos://207.184.161.138:7077 \\ # Use your IP address\n --conf spark.eventLog.enabled=false \n --conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails\" \\ \n --deploy-mode cluster \\\n --supervise \\\n --executor-memory 20G \\\n myApp.jar\n\n```", "```py\nspark.master  spark://5.6.7.8:7077 \nspark.executor.memor y   4g \nspark.eventLog.enabled true \nspark.serializer org.apache.spark.serializer.KryoSerializer\n\n```", "```py\nlog4j.logger.org=OFF\n\n```", "```py\nval rdd1 = sc.textFile(“hdfs://data/data.csv”)\n                       .map(someMethod)\n                       .filter(filterMethod)   \n\n```", "```py\nval rdd2 = sc.hadoopFile(“hdfs://data/data2.csv”)\n                      .groupByKey()\n                      .map(secondMapMethod)\n\n```", "```py\nval rdd3 = rdd1.join(rdd2).map(thirdMapMethod)\n\n```", "```py\nrdd3.collect()\n\n```", "```py\n--executor-memory 20G\n\n```", "```py\nsc.wholeTextFiles(\"/mnt/temp\") // note the location of the data files is /mnt/temp/\n\n```", "```py\n02/05/17 12:44:45 ERROR AppClient$ClientActor: All masters are unresponsive! Giving up. \n02/05/17 12:45:31 ERROR SparkDeploySchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up. \n02/05/17 12:45:35 ERROR TaskSchedulerImpl: Exiting due to error from cluster scheduler: Spark cluster looks down\n\n```", "```py\n $ bin/spark-shell --master spark://master-ip:7077\n\n```", "```py\nval conf = new SparkConf()\n               .setMaster(“local[*]”)\n               .setAppName(“MyApp”)\nconf.registerKryoClasses(Array(classOf[MyOwnClass1], classOf[MyOwnClass2]))\nval sc = new SparkContext(conf)\n\n```", "```py\nclass MyMapper(n: Int) { // without any serialization\n  @transient lazy val log = org.apache.log4j.LogManager.getLogger(\"myLogger\")\n  def MyMapperDosomething(rdd: RDD[Int]): RDD[String] = rdd.map { i =>\n    log.warn(\"mapping: \" + i)\n    (i + n).toString\n  }\n}\n\n```", "```py\nconf.registerKryoClasses(Array(classOf[MyMapper])) // register the class with Kyro\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") // set Kayro serialization\n\n```", "```py\npackage com.chapter14.Serilazition\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD\nclass MyMapper(n: Int) { // without any serilization\n  @transient lazy val log = org.apache.log4j.LogManager.getLogger\n                                (\"myLogger\")\n  def MyMapperDosomething(rdd: RDD[Int]): RDD[String] = rdd.map { i =>\n    log.warn(\"mapping: \" + i)\n    (i + n).toString\n  }\n}\n//Companion object\nobject MyMapper {\n  def apply(n: Int): MyMapper = new MyMapper(n)\n}\n//Main object\nobject KyroRegistrationDemo {\n  def main(args: Array[String]) {\n    val log = LogManager.getRootLogger\n    log.setLevel(Level.WARN)\n    val conf = new SparkConf()\n      .setAppName(\"My App\")\n      .setMaster(\"local[*]\")\n    conf.registerKryoClasses(Array(classOf[MyMapper2]))\n     // register the class with Kyro\n    conf.set(\"spark.serializer\", \"org.apache.spark.serializer\n             .KryoSerializer\") // set Kayro serilazation\n    val sc = new SparkContext(conf)\n    log.warn(\"Started\")\n    val data = sc.parallelize(1 to 100000)\n    val mapper = MyMapper(1)\n    val other = mapper.MyMapperDosomething(data)\n    other.collect()\n    log.warn(\"Finished\")\n  }\n}\n\n```", "```py\n17/04/29 15:33:43 WARN root: Started \n.\n.\n17/04/29 15:31:51 WARN myLogger: mapping: 1 \n17/04/29 15:31:51 WARN myLogger: mapping: 49992\n17/04/29 15:31:51 WARN myLogger: mapping: 49999\n17/04/29 15:31:51 WARN myLogger: mapping: 50000 \n.\n.                                                                                \n17/04/29 15:31:51 WARN root: Finished\n\n```", "```py\n--conf “spark.executor.extraJavaOptions = -verbose:gc -XX:-PrintGCDetails -XX:+PrintGCTimeStamps\"\n\n```", "```py\nval m = 5\nval bv = sc.broadcast(m)\n\n```", "```py\nbv.value()\n\n```"]