<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch08" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 8.   Analyzing Unstructured Data  </h1></div></div></div><p class="calibre11">In this Big Data era, the proliferation of unstructured data is overwhelming. Numerous methods such as data mining, <strong class="calibre19">Natural Language Processing</strong> (<strong class="calibre19">NLP</strong>), information retrieval, and so on, exist for analyzing unstructured data. Due to the rapid growth of unstructured data in all kinds of businesses, scalable solutions have become the need of the hour. Apache Spark is equipped with out of the box algorithms for text analytics, and it also supports custom development of algorithms that are not available by default.</p><p class="calibre11">In the previous chapter we have shown how SparkR, an R API to Spark for R programmers can harness the power of Spark, without learning a new language .  In this chapter, we are going to step into a whole new dimension and explore algorithms and techniques to extract information out of unstructured data by leveraging Spark.</p><p class="calibre11">As a prerequisite for this chapter, a basic understanding of programming in Python or Scala and an overall understanding of text analytics and machine learning are nice to have. However, we have covered some theoretical basics with the right set of practical examples to make those more comprehendible and easy to implement. The topics covered in this chapter are:</p><div><ul class="itemizedlist"><li class="listitem">Sources of unstructured data</li><li class="listitem">Processing unstructured data<div><ul class="itemizedlist1"><li class="listitem">Count vectorizer</li><li class="listitem">TF-IDF</li><li class="listitem">Stop-word removal</li><li class="listitem">Normalization/scaling</li><li class="listitem">Word2Vec</li><li class="listitem">n-gram modeling</li></ul></div><p class="calibre31">
</p></li></ul></div><div><ul class="itemizedlist"><li class="listitem">Text classification<div><ul class="itemizedlist1"><li class="listitem">Naive Bayes classifier</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Text clustering<div><ul class="itemizedlist1"><li class="listitem">K-means</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Dimensionality reduction<div><ul class="itemizedlist1"><li class="listitem">Singular value decomposition</li><li class="listitem">Principal component analysis</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Summary</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec61" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sources of unstructured data</h1></div></div></div><p class="calibre11">Data analytics has come very far since the spreadsheets and the BI tools in the eighties and nineties. Tremendous improvements in computing power, sophisticated algorithms, and an open source culture fueled unprecedented growth in data analytics, as well as in other fields. These advances in technologies paved the way for new opportunities and new challenges. Businesses started looking at generating insights from hitherto impossible to handle data sources such as internal memos, emails, customer satisfaction surveys, and the like. Data analytics now encompass this unstructured, usually text based data along with traditional rows and columns of data. Between the highly structured data stored in RDBMS table and completely unstructured plain text, we have semi-structured data sources in NoSQL data stores, XML or JSON documents, and graph or network data sources. As per current estimates, unstructured data forms about 80 percent of enterprise data and is growing rapidly. Satellite images, atmospheric data, social networks, blogs and other web pages, patient records and physicians' notes, companies' internal communications, and so on - all these combined are just a subset of unstructured data sources.</p><p class="calibre11">We have already been seeing successful data products that leverage unstructured data along with structured data. Some of the companies leverage the power of social networks to provide actionable insights to their customers. New fields such as <strong class="calibre19">Sentiment Analysis</strong> and <strong class="calibre19">Multimedia Analytics</strong> are emerging to draw insights from unstructured data. However, analyzing unstructured data is still a daunting feat. For example, contemporary text analytics tools and techniques cannot identify sarcasm. However, the potential benefits undoubtedly outweigh the limitations.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec62" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Processing unstructured data</h1></div></div></div><p class="calibre11">Unstructured data does not lend itself to most of the programming tasks. It has to be processed in various different ways as applicable, to be able to serve as an input to any machine learning algorithm or for visual analysis. Broadly, the unstructured data analysis can be viewed as a series of steps as shown in the following diagram:</p><p class="calibre11">
</p><div><img src="img/image_08_001.jpg" alt="Processing unstructured data" class="calibre139"/></div><p class="calibre11">
</p><p class="calibre11">Data pre-processing is the most vital step in any unstructured data analysis. Fortunately, there have been several proven techniques accumulated over time that come in handy. Spark offers most of these techniques out of the box through the <code class="literal">ml.features</code> package. Most of the techniques aim to convert text data to concise numerical vectors that can be easily consumed by machine learning algorithms. Developers should understand the specific requirements of their organizations to arrive at the best pre-processing workflow. Remember that better, relevant data is the key to generate better insights.</p><p class="calibre11">Let us explore a couple of examples that process raw text and convert them into data frames. First example takes some text as input and extracts all date-like strings whereas the second example extracts tags from twitter text. First example is just a warm-up, using a simple, regex (regular expression) tokenizer feature transformer without using any spark-specific libraries. It also draws your attention to the possibility of misinterpretation. For example, a product code of the form 1-11-1111 may be interpreted as a date. The second example illustrates a non-trivial, multi-step extraction process that resulted in just the required tags.<strong class="calibre19"> User defined functions</strong> (<strong class="calibre19">udf</strong>) and ML pipelines come in handy in developing such multi-step extraction processes. Remaining part of this section describes some more handy tools supplied out of box in apache Spark.</p><p class="calibre11">
<strong class="calibre19">Example-1: </strong>
<strong class="calibre19">Extract date like strings from text</strong>
</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.feature.RegexTokenizer
import org.apache.spark.ml.feature.RegexTokenizer
scala&gt; val date_pattern: String = "\\d{1,4}[/ -]\\d{1,4}[/ -]\\d{1,4}"
date_pattern: String = \d{1,4}[/ -]\d{1,4}[/ -]\d{1,4}
scala&gt; val textDF  = spark.createDataFrame(Seq(
    (1, "Hello 1996-12-12 this 1-21-1111 is a 18-9-96 text "),
    (2, "string with dates in different 01/02/89 formats"))).
    toDF("LineNo","Text")
textDF: org.apache.spark.sql.DataFrame = [LineNo: int, Text: string]
scala&gt; val date_regex = new RegexTokenizer().
        setInputCol("Text").setOutputCol("dateStr").
        setPattern(date_pattern).setGaps(false)
date_regex: org.apache.spark.ml.feature.RegexTokenizer = regexTok_acdbca6d1c4c
scala&gt; date_regex.transform(textDF).select("dateStr").show(false)
+--------------------------------+
|dateStr                         |
+--------------------------------+
|[1996-12-12, 1-21-1111, 18-9-96]|
|[01/02/89]                      |
+--------------------------------+</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><pre class="programlisting">// Example-1: Extract date like strings from text
&gt;&gt;&gt; from pyspark.ml.feature import RegexTokenizer
&gt;&gt;&gt; date_pattern = "\\d{1,4}[/ -]\\d{1,4}[/ -]\\d{1,4}"
&gt;&gt;&gt; textDF  = spark.createDataFrame([
        [1, "Hello 1996-12-12 this 1-21-1111 is a 18-9-96 text "],
        [2, "string with dates in different 01/02/89 formats"]]).toDF(
        "LineNo","Text")
&gt;&gt;&gt; date_regex = RegexTokenizer(inputCol="Text",outputCol="dateStr",
            gaps=False, pattern=date_pattern)
&gt;&gt;&gt; date_regex.transform(textDF).select("dateStr").show(5,False)
+--------------------------------+
|dateStr                         |
+--------------------------------+
|[1996-12-12, 1-21-1111, 18-9-96]|
|[01/02/89]                      |
+--------------------------------+</pre><p class="calibre11">The preceding example defined a regular expression pattern to recognize date strings. The regex pattern and the sample text DataFrame are passed to the <code class="literal">RegexTokenizer</code> to extract matching, date like strings. The <code class="literal">gaps=False</code> option picks matching strings and a value of <code class="literal">False</code> would use the given pattern as a separator. Note that <code class="literal">1-21-1111</code>, which is obviously not a date, is also selected. </p><p class="calibre11">Next example extracts tags from twitter text and identifies most popular tags. You can use the same approach to collect hash (<code class="literal">#</code>) tags too.</p><p class="calibre11">This example uses a built in function <code class="literal">explode</code>, which converts a single row with an array of values into multiple rows, one value per array element.</p><p class="calibre11">
<strong class="calibre19">Example-2: Extract tags from twitter "text" </strong>
</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">//Step1: Load text containing @ from source file
scala&gt; val path = "&lt;Your path&gt;/tweets.json"
path: String = &lt;Your path&gt;/tweets.json
scala&gt; val raw_df = spark.read.text(path).filter($"value".contains("@"))
raw_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: string]
//Step2: Split the text to words and filter out non-tag words
scala&gt; val df1 = raw_df.select(explode(split('value, " ")).as("word")).
        filter($"word".startsWith("@"))
df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [word: string]
//Step3: compute tag-wise counts and report top 5
scala&gt; df1.groupBy($"word").agg(count($"word")).
        orderBy($"count(word)".desc).show(5)
+------------+-----------+
+                                                     
|        word|count(word)|
+------------+-----------+
|@ApacheSpark|         15|
|    @SSKapci|          9|
|@databricks:|          4|
|     @hadoop|          4|
| @ApacheApex|          4|
+------------+-----------+</pre><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">
&gt;&gt; from pyspark.sql.functions import explode, split
//Step1: Load text containing @ from source file
&gt;&gt;&gt; path ="&lt;Your path&gt;/tweets.json"
&gt;&gt;&gt; raw_df1 = spark.read.text(path)
&gt;&gt;&gt; raw_df = raw_df1.where("value like '%@%'")
&gt;&gt;&gt; 
//Step2: Split the text to words and filter out non-tag words
&gt;&gt;&gt; df = raw_df.select(explode(split("value"," ")))
&gt;&gt;&gt; df1 = df.where("col like '@%'").toDF("word")
&gt;&gt;&gt; 
//Step3: compute tag-wise counts and report top 5
&gt;&gt;&gt; df1.groupBy("word").count().sort(
     "count",ascending=False).show(5)
+------------+-----+
+                                                        
|        word|count|
+------------+-----+
|@ApacheSpark|   15|
|    @SSKapci|    9|
|@databricks:|    4|
| @ApacheApex|    4|
|     @hadoop|    4|
+------------+-----+
</pre><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec94" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Count vectorizer</h2></div></div></div><p class="calibre11">Count vectorizer extracts vocabulary (tokens) from documents and generates a <code class="literal">CountVectorizerModel</code> model when a dictionary is not available priori. As the name indicates, a text document is converted into a vector of tokens and counts. The model produces a sparse representation of the documents over the vocabulary.</p><p class="calibre11">You can fine tune the behavior to limit the vocabulary size, minimum token count, and much more as applicable in your business case.</p><p class="calibre11">//Example 3: Count Vectorizer example</p><p class="calibre11">
<strong class="calibre19">Scala</strong>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}
import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}
scala&gt; import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.DataFrame
scala&gt; import org.apache.spark.ml.linalg.Vector
import org.apache.spark.ml.linalg.Vector
scala&gt; val df: DataFrame = spark.createDataFrame(Seq(
  (0, Array("ant", "bat", "cat", "dog", "eel")),
  (1, Array("dog","bat", "ant", "bat", "cat"))
)).toDF("id", "words")
df: org.apache.spark.sql.DataFrame = [id: int, words: array&lt;string&gt;]
scala&gt;
<strong class="calibre19">// Fit a CountVectorizerModel from the corpus
</strong>
<strong class="calibre19">// Minimum occurrences (DF) is 2 and pick 10 top words(vocabsize) only
</strong>scala&gt; val cvModel: CountVectorizerModel = new CountVectorizer().
        setInputCol("words").setOutputCol("features").
        setMinDF(2).setVocabSize(10).fit(df)
cvModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_7e79157ba561
<strong class="calibre19">// Check vocabulary. Words are arranged as per frequency
</strong>
<strong class="calibre19">// eel is dropped because it is below minDF = 2
</strong>scala&gt; cvModel.vocabulary
res6: Array[String] = Array(bat, dog, cat, ant)
//Apply the model on document
scala&gt; val cvDF: DataFrame = cvModel.transform(df)
cvDF: org.apache.spark.sql.DataFrame = [id: int, words: array&lt;string&gt; ... 1 more field]
<strong class="calibre19">//Check the word count
</strong>scala&gt; cvDF.select("features").collect().foreach(row =&gt;
println(row(0).asInstanceOf[Vector].toDense))

[1.0,1.0,1.0,1.0]
[2.0,1.0,1.0,1.0]</pre><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">&gt;&gt;&gt; from pyspark.ml.feature import CountVectorizer,CountVectorizerModel
&gt;&gt;&gt; from pyspark.ml.linalg import Vector
&gt;&gt;&gt; 
// Define source DataFrame
&gt;&gt;&gt; df = spark.createDataFrame([
    [0, ["ant", "bat", "cat", "dog", "eel"]],
    [1, ["dog","bat", "ant", "bat", "cat"]]
  ]).toDF("id", "words")
&gt;&gt;&gt; 
// Fit a CountVectorizerModel from the corpus
// Minimum occorrences (DF) is 2 and pick 10 top words(vocabsize) only
&gt;&gt;&gt; cvModel = CountVectorizer(inputCol="words", outputCol="features",
        minDF = 2, vocabSize = 10).fit(df)
&gt;&gt;&gt; 
// Check vocabulary. Words are arranged as per frequency
// eel is dropped because it is below minDF = 2
&gt;&gt;&gt; cvModel.vocabulary
[u'bat', u'ant', u'cat', u'dog']
//Apply the model on document
&gt;&gt;&gt; cvDF = cvModel.transform(df)
//Check the word count
&gt;&gt;&gt; cvDF.show(2,False)
+---+-------------------------+-------------------------------+
|id |words                    |features                       |
+---+-------------------------+-------------------------------+
|0  |[ant, bat, cat, dog, eel]|(4,[0,1,2,3],[1.0,1.0,1.0,1.0])|
|1  |[dog, bat, ant, bat, cat]|(4,[0,1,2,3],[2.0,1.0,1.0,1.0])|
+---+-------------------------+-------------------------------+</pre><p class="calibre11">
<strong class="calibre19">Input</strong>:</p><pre class="programlisting"> |id | text                  
 +---+-------------------------+-------------------------------+
 |0  | "ant", "bat", "cat", "dog", "eel"     
 |1  | "dog","bat", "ant", "bat", "cat"</pre><p class="calibre11">
<strong class="calibre19">Output</strong>:</p><pre class="programlisting">id| text                               | Vector 
--|------------------------------------|-------------------- 
0 | "ant", "bat", "cat", "dog", "eel" |[1.0,1.0,1.0,1.0] 
1 | "dog","bat", "ant", "bat", "cat"   |[2.0,1.0,1.0,1.0]
 
</pre><p class="calibre11">The preceding example demonstrates how <code class="literal">CountVectorizer</code> works as an estimator to extract the vocabulary and generate a <code class="literal">CountVectorizerModel</code>. Note that the features vector order corresponds to vocabulary and not the input sequence. Let's also look at how the same can be achieved by building a dictionary a-priori. However, keep in mind that they have their own use cases.</p><p class="calibre11">Example 4: define CountVectorizerModel with a-priori vocabulary</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">// Example 4: define CountVectorizerModel with a-priori vocabulary
scala&gt; val cvm: CountVectorizerModel = new CountVectorizerModel(
        Array("ant", "bat", "cat")).
        setInputCol("words").setOutputCol("features")
cvm: org.apache.spark.ml.feature.CountVectorizerModel = cntVecModel_ecbb8e1778d5

//<strong class="calibre19">Apply on the same data. Feature order corresponds to a-priory vocabulary order
</strong>scala&gt; cvm.transform(df).select("features").collect().foreach(row =&gt;
        println(row(0).asInstanceOf[Vector].toDense))
[1.0,1.0,1.0]
[1.0,2.0,1.0]</pre><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><p class="calibre11">Not available as of Spark 2.0.0</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec95" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>TF-IDF</h2></div></div></div><p class="calibre11">The <strong class="calibre19">Term Frequency-Inverse Document Frequency</strong> (<strong class="calibre19">TF-IDF</strong>) is perhaps one of the most popular measures in text analytics. This metric indicates the importance of a given term in a given document within a set of documents. This consists two measurements, <strong class="calibre19">Term Frequency</strong> (<strong class="calibre19">TF</strong>) and <strong class="calibre19">Inverse Document Frequency</strong> (<strong class="calibre19">IDF</strong>). Let us discuss them one by one and then see their combined effect.</p><p class="calibre11">TF is a measure of the relative importance of a term in a document, which is usually the frequency of that term divided by the number of terms in that document. Consider a text document containing 100 words wherein the word <em class="calibre22">apple</em> appears eight times. The TF for <em class="calibre22">apple</em> would be <em class="calibre22">TF = (8 / 100) = 0.08</em>. So, the more frequently a term occurs in a document, the larger is its TF coefficient.</p><p class="calibre11">IDF is a measure of the importance of a particular term in the entire collection of documents, that is, how infrequently the word occurs across all the documents. The importance of a term is inversely proportional to its frequency. Spark provides two separate methods to perform these tasks. Assume we have 6 million documents and the word <em class="calibre22">apple</em> appears in 6000 of these. Then, IDF is calculated as <em class="calibre22">IDF = Log(6,000,000 / 6,000) = 3</em>. If you observe this carefully, the lower the denominator, the higher is the IDF value. This means that the fewer the number of documents containing a particular word, the higher would be its importance.</p><p class="calibre11">Thus, the TF-IDF score would be <em class="calibre22">TF * IDF = 0.08 * 3 = 0.24</em>. Note that it would penalize the words that are more frequent across documents and less important, such as <em class="calibre22">the</em>, <em class="calibre22">this</em>, <em class="calibre22">a</em>, and so on, and give more weight to the ones that are important.</p><p class="calibre11">In Spark, TF is implemented as HashingTF. It takes a sequence of terms (often the output of a tokenizer) and produces a fixed length features vector. It performs feature hashing to convert the terms into fixed length indices. IDF then takes that features vector (the output of HashingTF) as input and scales it based on the term frequency in the set of documents. The previous chapter has an example of this transformation.</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec96" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Stop-word removal</h2></div></div></div><p class="calibre11">Common words such as <em class="calibre22">is</em>, <em class="calibre22">was</em>, and <em class="calibre22">the</em> are called stop-words. They do not usually add value to analysis and should be dropped during the data preparation step. Spark provides <code class="literal">StopWordsRemover</code> transformer, which does just that. It takes a sequence of tokens as a series of string inputs, such as the output of a tokenizer, and removes all the stop words. Spark has a stop-words list by default that you may override by providing your own stop-words list as a parameter. You may optionally turn on <code class="literal">caseSensitive</code> match which is off by default.</p><p class="calibre11">Example 5: Stopword Remover</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.feature.StopWordsRemover
import org.apache.spark.ml.feature.StopWordsRemover
scala&gt; import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.DataFrame
scala&gt; import org.apache.spark.ml.linalg.Vector
import org.apache.spark.ml.linalg.Vector
scala&gt; val rawdataDF = spark.createDataFrame(Seq(
        (0, Array("I", "ate", "the", "cake")),
        (1, Array("John ", "had", "a", " tennis", "racquet")))).
        toDF("id","raw_text")
rawdataDF: org.apache.spark.sql.DataFrame = [id: int, raw_text: array&lt;string&gt;]
scala&gt; val remover = new StopWordsRemover().setInputCol("raw_text").
                setOutputCol("processed_text")
remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_55edbac88edb
scala&gt; remover.transform(rawdataDF).show(truncate=false)
+---+---------------------------------+-------------------------+
|id |raw_text                         |processed_text           |
+---+---------------------------------+-------------------------+
|0  |[I, ate, the, cake]              |[ate, cake]              |
|1  |[John , had, a,  tennis, racquet]|[John ,  tennis, racquet]|
+---+---------------------------------+-------------------------+</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><pre class="programlisting">&gt;&gt;&gt; from pyspark.ml.feature import StopWordsRemover
&gt;&gt;&gt; RawData = sqlContext.createDataFrame([
    (0, ["I", "ate", "the", "cake"]),
    (1, ["John ", "had", "a", " tennis", "racquet"])
    ], ["id", "raw_text"])
&gt;&gt;&gt; 
&gt;&gt;&gt; remover = StopWordsRemover(inputCol="raw_text",
        outputCol="processed_text")
&gt;&gt;&gt; remover.transform(RawData).show(truncate=False)
+---+---------------------------------+-------------------------+
|id |raw_text                         |processed_text           |
+---+---------------------------------+-------------------------+
|0  |[I, ate, the, cake]              |[ate, cake]              |
|1  |[John , had, a,  tennis, racquet]|[John ,  tennis, racquet]|
+---+---------------------------------+-------------------------+</pre><p class="calibre11">Assume that we have the following DataFrame with columns <code class="literal">id</code> and <code class="literal">raw_text</code>:</p><pre class="programlisting"> id | raw_text 
----|---------- 
 0  | [I, ate, the, cake] 
 1  | [John, had, a, tennis, racquet] 
</pre><p class="calibre11">After applying <code class="literal">StopWordsRemover </code>with <code class="literal">raw_text</code> as the input column and <code class="literal">processed_text</code> as the output column for the preceding example, we should get the following output:</p><pre class="programlisting"> 
 id | raw_text                       | processed_text 
----|--------------------------------|-------------------- 
 0  | [I, ate, the, cake]            |  [ate, cake] 
 1  |[John, had, a, tennis, racquet] |[John, tennis, racquet] 
 
</pre></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec97" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Normalization/scaling</h2></div></div></div><p class="calibre11">Normalization is a common and preliminary step in data preparation. Most of the machine learning algorithms work better when all features are on the same scale. For example, if there are two features where the value of one is about 100 times greater than the other, bringing them to the same scale reflects meaningful relative activity between the two variables. Any non-numeric values, such as high, medium, and low, should ideally be converted to appropriate numerical quantification as a best practice. However, you need to be careful in doing so as it may require domain expertise. For example, if you assign 3, 2, and 1 for high, medium, and low respectively, then it should be checked that these three units are equidistant from each other.</p><p class="calibre11">The common methods of feature normalization are <em class="calibre22">scaling</em>, <em class="calibre22">mean subtraction</em>, and <em class="calibre22">feature standardization</em>, just to name a few. In scaling, each numerical feature vector is rescaled such that its value range is between <em class="calibre22">-1</em> to <em class="calibre22">+1</em> or <em class="calibre22">0</em> to <em class="calibre22">1</em> or something similar. In mean subtraction, you compute mean of a numerical feature vector and subtract that mean from each of the values. We are interested in the relative deflection from the mean, while the absolute value could be immaterial. Feature standardization refers to setting the data to zero mean and unit (1) variance.</p><p class="calibre11">Spark provides a <code class="literal">Normalizer</code> feature transformer to normalize each vector to have unit norm; <code class="literal">StandardScaler</code> to have unit norm and zero mean; and <code class="literal">MinMaxScaler</code> to rescale each feature to a specific range of values. By default, min and max are 0 and 1 but you may set the value parameters yourself as per the data requirement.</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec98" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Word2Vec</h2></div></div></div><p class="calibre11">The Word2Vec is a type of PCA (you will find out more about this shortly) that takes a sequence of words and produces a map (of string, vector). The string is the word and the vector is a unique fixed size vector. The resulting word vector representation is useful in many machine learning and NLP applications, such as named entity recognition and tagging. Let us look at an example.</p><p class="calibre11">
<strong class="calibre19">Example 6: Word2Vec</strong>
</p><p class="calibre11">
<strong class="calibre19">Scala</strong>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.feature.Word2Vec
import org.apache.spark.ml.feature.Word2Vec

<strong class="calibre19">//Step1: Load text file and split to words
</strong>scala&gt; val path = "&lt;Your path&gt;/RobertFrost.txt"
path: String = &lt;Your path&gt;/RobertFrost.txt
scala&gt; val raw_text = spark.read.text(path).select(
        split('value, " ") as "words")
raw_text: org.apache.spark.sql.DataFrame = [words: array&lt;string&gt;]

<strong class="calibre19">//Step2: Prepare features vector of size 4
</strong>scala&gt; val resultDF = new Word2Vec().setInputCol("words").
        setOutputCol("features").setVectorSize(4).
        setMinCount(2).fit(raw_text).transform(raw_text)
resultDF: org.apache.spark.sql.DataFrame = [words: array&lt;string&gt;, features: vector]

<strong class="calibre19">//Examine results
</strong>scala&gt; resultDF.show(5)
+--------------------+--------------------+
|               words|            features|
+--------------------+--------------------+
|[Whose, woods, th...|[-0.0209098898340...|
|[His, house, is, ...|[-0.0013444167044...|
|[He, will, not, s...|[-0.0058525378408...|
|[To, watch, his, ...|[-0.0189630933296...|
|[My, little, hors...|[-0.0084691265597...|
+--------------------+--------------------+</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><pre class="programlisting">&gt;&gt;&gt; from pyspark.ml.feature import Word2Vec
&gt;&gt;&gt; from pyspark.sql.functions import explode, split
&gt;&gt;&gt;

<strong class="calibre19">//Step1: Load text file and split to words
</strong>&gt;&gt;&gt; path = "&lt;Your path&gt;/RobertFrost.txt"
&gt;&gt;&gt; raw_text = spark.read.text(path).select(
        split("value"," ")).toDF("words")

<strong class="calibre19">//Step2: Prepare features vector of size 4
</strong>&gt;&gt;&gt; resultDF = Word2Vec(inputCol="words",outputCol="features",
                 vectorSize=4, minCount=2).fit(
                 raw_text).transform(raw_text)

<strong class="calibre19">//Examine results
</strong>scala&gt; resultDF.show(5)
+--------------------+--------------------+
|               words|            features|
+--------------------+--------------------+
|[Whose, woods, th...|[-0.0209098898340...|
|[His, house, is, ...|[-0.0013444167044...|
|[He, will, not, s...|[-0.0058525378408...|
|[To, watch, his, ...|[-0.0189630933296...|
|[My, little, hors...|[-0.0084691265597...|
+--------------------+--------------------+</pre></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec99" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>n-gram modelling</h2></div></div></div><p class="calibre11">An n-gram is a contiguous sequence of <em class="calibre22">n</em> items from a given sequence of text or speech. An n-gram of size <em class="calibre22">1</em> is referred to as a <em class="calibre22">unigram</em>, size <em class="calibre22">2</em> is a <em class="calibre22">bigram</em>, and size <em class="calibre22">3</em> is a <em class="calibre22">trigram</em>. Alternatively, they can be referred to by the value of <em class="calibre22">n</em>, for example, four-gram, five-gram, and so on. Let us take a look at an example to understand the possible outcomes of this model:</p><pre class="programlisting">  
 input |1-gram sequence  | 2-gram sequence | 3-gram sequence 
-------|-----------------|-----------------|--------------- 
 apple | a,p,p,l,e       |  ap,pp,pl,le    |  app,ppl,ple 
</pre><p class="calibre11">This is an example of words to n-gram letters. The same is the case for sentence (or tokenized words) to n-gram words. For example, the 2-gram equivalent of the sentence <em class="calibre22">Kids love to eat chocolates</em> is:</p><p class="calibre11">'Kids love', 'love to', 'to eat', 'eat chocolates'.</p><p class="calibre11">There are various applications of n-gram modelling in text mining and NLP. One of the examples is predicting the probability of each word occurring given a prior context (conditional probability).</p><p class="calibre11">In Spark, <code class="literal">NGram</code> is a feature transformer that converts the input array (for example, the output of a Tokenizer) of strings into an array of n-grams. Null values in the input array are ignored by default. It returns an array of n-grams where each n-gram is represented by a space-separated string of words.</p><p class="calibre11">
<strong class="calibre19">Example 7: NGram</strong>
</p><p class="calibre11">
<strong class="calibre19">Scala</strong>
</p><pre class="programlisting">scala&gt; import org.apache.spark.ml.feature.NGram
import org.apache.spark.ml.feature.NGram
scala&gt; val wordDF = spark.createDataFrame(Seq(
        (0, Array("Hi", "I", "am", "a", "Scientist")),
        (1, Array("I", "am", "just", "learning", "Spark")),
        (2, Array("Coding", "in", "Scala", "is", "easy"))
        )).toDF("label", "words")

<strong class="calibre19">//Create an ngram model with 3 words length (default is 2)
</strong>scala&gt; val ngramModel = new NGram().setInputCol(
                "words").setOutputCol("ngrams").setN(3)
ngramModel: org.apache.spark.ml.feature.NGram = ngram_dc50209cf693

<strong class="calibre19">//Apply on input data frame
</strong>scala&gt; ngramModel.transform(wordDF).select("ngrams").show(false)
+--------------------------------------------------+
|ngrams                                            |
+--------------------------------------------------+
|[Hi I am, I am a, am a Scientist]                 |
|[I am just, am just learning, just learning Spark]|
|[Coding in Scala, in Scala is, Scala is easy]     |
+--------------------------------------------------+

<strong class="calibre19">//Apply the model on another dataframe, Word2Vec raw_text
</strong>scala&gt;ngramModel.transform(raw_text).select("ngrams").take(1).foreach(println)
[WrappedArray(Whose woods these, woods these are, these are I, are I think, I think I, think I know.)]</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><pre class="programlisting">&gt;&gt;&gt; from pyspark.ml.feature import NGram
&gt;&gt;&gt; wordDF = spark.createDataFrame([
         [0, ["Hi", "I", "am", "a", "Scientist"]],
         [1, ["I", "am", "just", "learning", "Spark"]],
         [2, ["Coding", "in", "Scala", "is", "easy"]]
         ]).toDF("label", "words")

<strong class="calibre19">//Create an ngram model with 3 words length (default is 2)
</strong>&gt;&gt;&gt; ngramModel = NGram(inputCol="words", outputCol= "ngrams",n=3)
&gt;&gt;&gt; 

<strong class="calibre19">//Apply on input data frame
</strong>&gt;&gt;&gt; ngramModel.transform(wordDF).select("ngrams").show(4,False)
+--------------------------------------------------+
|ngrams                                            |
+--------------------------------------------------+
|[Hi I am, I am a, am a Scientist]                 |
|[I am just, am just learning, just learning Spark]|
|[Coding in Scala, in Scala is, Scala is easy]     |
+--------------------------------------------------+

<strong class="calibre19">//Apply the model on another dataframe from Word2Vec example 
</strong>&gt;&gt;&gt; ngramModel.transform(resultDF).select("ngrams").take(1)
[Row(ngrams=[u'Whose woods these', u'woods these are', u'these are I', u'are I think', u'I think I', u'think I know.'])]</pre></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec63" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Text classification</h1></div></div></div><p class="calibre11">Text classification is about assigning a topic, subject category, genre, or something similar to the text blob. For example, spam filters assign spam or not spam to an email.</p><p class="calibre11">Apache Spark supports various classifiers through MLlib and ML packages. The SVM classifier and Naive Bayes classifier are popular classifiers, and the former was already covered in the previous chapter. Let's take a look at the latter now.</p><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec100" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Naive Bayes classifier</h2></div></div></div><p class="calibre11">The <strong class="calibre19">Naive Bayes</strong> (<strong class="calibre19">NB</strong>) classifier is a multiclass probabilistic classifier and is one of the best classification algorithms. It assumes strong independence between every pair of features. It computes the conditional probability distribution of each feature and a given label, and then applies Bayes' theorem to compute the conditional probability of a label given an observation. In terms of document classification, an observation is a document to be classified into some class. Despite its strong assumptions on data, it is quite popular. It works with small amount of training data - whether real or discrete. It works very efficiently because it takes a single pass through the training data; one constraint is that the feature vectors must be non-negative. By default, ML package supports multinomial NB. However, you may set the parameter <code class="literal">modelType</code> to <code class="literal">Bernoulli</code> if bernoulli NB is required.</p><p class="calibre11">The <strong class="calibre19">laplace smoothing</strong> technique may be applied by specifying the smoothing parameters and is extremely useful in situations where you want to assign a small non-zero probability to a rare word or new word so that the posterior probabilities do not suddenly drop to zero.</p><p class="calibre11">Spark also provides some other hyper parameters such as <code class="literal">thresholds</code> also to gain fine grain control. Here is an example that categorizes twitter text. This example contains some hand-coded rules that assign a category to the train data. A particular category is assigned if any of the corresponding words are found in the text. For example, the category is "survey" if text contains "survey" or "poll". The model is trained based on this train data and evaluated on a different text sample collected at a different time:</p><p class="calibre11">
<strong class="calibre19">Example 8: Naive Bayes</strong>
</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">
<strong class="calibre19">// Step 1: Define a udf to assign a category
</strong>// One or more similar words are treated as one category (eg survey, poll)
// If input list contains any of the words in a category list, it is assigned to that category
// "General" is assigned if none of the categories matched
scala&gt; import scala.collection.mutable.WrappedArray
import scala.collection.mutable.WrappedArray
scala&gt; val findCategory = udf ((words: WrappedArray[String]) =&gt;
    { var idx = 0; var category : String = ""
    val categories : List[Array[String]] =  List(
     Array("Python"), Array("Hadoop","hadoop"),
     Array("survey","poll"),
      Array("event","training", "Meetup", "summit",
          "talk", "talks", "Setting","sessions", "workshop"),
     Array("resource","Guide","newsletter", "Blog"))
    while(idx &lt; categories.length &amp;&amp; category.isEmpty ) {
        if (!words.intersect(categories(idx)).isEmpty) {
         category = categories(idx)(0) }  //First word in the category list
     idx += 1 }
    if (category.isEmpty) {
    category = "General"  }
    category
  })
findCategory: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(&lt;function1&gt;,StringType,Some(List(ArrayType(StringType,true))))

<strong class="calibre19">//UDF to convert category to a numerical label
</strong>scala&gt; val idxCategory = udf ((category: String) =&gt;
        {val catgMap = Map({"General"-&gt;1},{"event"-&gt;2},{"Hadoop"-&gt;3},
                             {"Python"-&gt;4},{"resource"-&gt;5})
         catgMap(category)})
idxCategory: org.apache.spark.sql.expressions.UserDefinedFunction =
UserDefinedFunction(&lt;function1&gt;,IntegerType,Some(List(StringType)))
scala&gt; val labels = Array("General","event","Hadoop","Python","resource")
<strong class="calibre19">

//Step 2: Prepare train data
</strong>
<strong class="calibre19">//Step 2a: Extract "text" data and split to words
</strong>scala&gt; val path = "&lt;Your path&gt;/tweets_train.txt"
path: String = &lt;Your path&gt;../work/tweets_train.txt
scala&gt; val pattern = ""text":"
pattern: String = "text":
scala&gt; val raw_text = spark.read.text(path).filter($"value".contains(pattern)).
               select(split('value, " ") as "words")
raw_text: org.apache.spark.sql.DataFrame = [words: array&lt;string&gt;]
scala&gt;

<strong class="calibre19">//Step 2b: Assign a category to each line
</strong>scala&gt; val train_cat_df = raw_text.withColumn("category",
       
findCategory(raw_text("words"))).withColumn("label",idxCategory($"category"))
train_cat_df: org.apache.spark.sql.DataFrame = [words: array&lt;string&gt;, category:
string ... 1 more field]

<strong class="calibre19">//Step 2c: Examine categories
</strong>scala&gt; train_cat_df.groupBy($"category").agg(count("category")).show()
+--------+---------------+                                                     
|category|count(category)|
+--------+---------------+
| General|            146|
|resource|              1|
|  Python|              2|
|   event|             10|
|  Hadoop|              6|
+--------+---------------+ 

<strong class="calibre19">//Step 3: Build pipeline
</strong>scala&gt; import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.Pipeline
scala&gt; import org.apache.spark.ml.feature.{StopWordsRemover, CountVectorizer,
                  IndexToString}
import org.apache.spark.ml.feature.{StopWordsRemover, CountVectorizer,
StringIndexer, IndexToString}
scala&gt; import org.apache.spark.ml.classification.NaiveBayes
import org.apache.spark.ml.classification.NaiveBayes
scala&gt;

<strong class="calibre19">//Step 3a: Define pipeline stages
</strong>
<strong class="calibre19">//Stop words should be removed first
</strong>scala&gt; val stopw = new StopWordsRemover().setInputCol("words").
                setOutputCol("processed_words")
stopw: org.apache.spark.ml.feature.StopWordsRemover = stopWords_2fb707daa92e
<strong class="calibre19">//Terms to term frequency converter
</strong>scala&gt; val cv = new CountVectorizer().setInputCol("processed_words").
             setOutputCol("features")
cv: org.apache.spark.ml.feature.CountVectorizer = cntVec_def4911aa0bf
<strong class="calibre19">//Define model
</strong>scala&gt; val model = new NaiveBayes().
                setFeaturesCol("features").
                setLabelCol("label")
model: org.apache.spark.ml.classification.NaiveBayes = nb_f2b6c423f12c
<strong class="calibre19">//Numerical prediction label to category converter
</strong>scala&gt; val lc = new IndexToString().setInputCol("prediction").
              setOutputCol("predictedCategory").
              setLabels(labels)
lc: org.apache.spark.ml.feature.IndexToString = idxToStr_3d71be25382c
<strong class="calibre19">

//Step 3b: Build pipeline with desired stages
</strong>scala&gt; val p = new Pipeline().setStages(Array(stopw,cv,model,lc))
p: org.apache.spark.ml.Pipeline = pipeline_956942e70b3f
<strong class="calibre19">

//Step 4: Process train data and get predictions
</strong>
<strong class="calibre19">//Step 4a: Execute pipeline with train data
</strong>scala&gt; val resultsDF = p.fit(train_cat_df).transform(train_cat_df)
resultsDF: org.apache.spark.sql.DataFrame = [words: array&lt;string&gt;, category:
string ... 7 more fields]

<strong class="calibre19">//Step 4b: Examine results
</strong>scala&gt; resultsDF.select("category","predictedCategory").show(3)
+--------+-----------------+
|category|predictedCategory|
+--------+-----------------+
|   event|            event|
|   event|            event|
| General|          General|
+--------+-----------------+
<strong class="calibre19">

//Step 4c: Look for prediction mismatches
</strong>scala&gt; resultsDF.filter("category != predictedCategory").select(
         "category","predictedCategory").show(3)
+--------+-----------------+
|category|predictedCategory|
+--------+-----------------+
| General|            event|
| General|           Hadoop|
|resource|           Hadoop|
+--------+-----------------+
<strong class="calibre19">
//Step 5: Evaluate model using test data
</strong>
<strong class="calibre19">//Step5a: Prepare test data
</strong>scala&gt; val path = "&lt;Your path&gt; /tweets.json"
path: String = &lt;Your path&gt;/tweets.json
scala&gt; val raw_test_df =
spark.read.text(path).filter($"value".contains(pattern)).
               select(split('value, " ") as "words"

raw_test_df: org.apache.spark.sql.DataFrame = [words: array&lt;string&gt;]
scala&gt; val test_cat_df = raw_test_df.withColumn("category",
       
findCategory(raw_test_df("words")))withColumn("label",idxCategory($"category"))
test_cat_df: org.apache.spark.sql.DataFrame = [words: array&lt;string&gt;, category:
string ... 1 more field]
scala&gt; test_cat_df.groupBy($"category").agg(count("category")).show()
+--------+---------------+                                                     
|category|count(category)|
+--------+---------------+
| General|              6|
|   event|             11|
+--------+---------------+
<strong class="calibre19">
//Step 5b: Run predictions on test data
</strong>scala&gt; val testResultsDF = p.fit(test_cat_df).transform(test_cat_df)
testResultsDF: org.apache.spark.sql.DataFrame = [words: array&lt;string&gt;,
category: string ... 7 more fields]
//Step 5c:: Examine results
scala&gt; testResultsDF.select("category","predictedCategory").show(3)
+--------+-----------------+
|category|predictedCategory|
+--------+-----------------+
| General|            event|
|   event|          General|
|   event|          General|
+--------+-----------------+

<strong class="calibre19">//Step 5d: Look for prediction mismatches
</strong>scala&gt; testResultsDF.filter("category != predictedCategory").select(
         "category","predictedCategory").show()
+--------+-----------------+
|category|predictedCategory|
+--------+-----------------+
|   event|          General|
|   event|          General|
+--------+-----------------+</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><pre class="programlisting">
<strong class="calibre19">// Step 1: Initialization
</strong>
<strong class="calibre19">//Step1a: Define a udfs to assign a category
</strong>// One or more similar words are treated as one category (eg survey, poll)
// If input list contains any of the words in a category list, it is assigned to that category
// "General" is assigned if none of the categories matched
&gt;&gt;&gt; def findCategory(words):
        idx = 0; category  = ""
        categories = [["Python"], ["Hadoop","hadoop"],
          ["survey","poll"],["event","training", "Meetup", "summit",
          "talk", "talks", "Setting","sessions", "workshop"],
          ["resource","Guide","newsletter", "Blog"]]
        while(not category and idx &lt; len(categories)):
          if len(set(words).intersection(categories[idx])) &gt; 0:
             category = categories[idx][0] #First word in the category list
          else:
             idx+=1
        if not category:   #No match found
          category = "General"
        return category
&gt;&gt;&gt; 
<strong class="calibre19">//Step 1b: Define udf to convert string category to a numerical label
</strong>&gt;&gt;&gt; def idxCategory(category):
       catgDict = {"General" :1, "event" :2, "Hadoop" :2,
             "Python": 4, "resource" : 5}
       return catgDict[category]
&gt;&gt;&gt; 
<strong class="calibre19">//Step 1c: Register UDFs
</strong>&gt;&gt;&gt; from pyspark.sql.functions import udf
&gt;&gt;&gt; from pyspark.sql.types import StringType, IntegerType
&gt;&gt;&gt; findCategoryUDF = udf(findCategory, StringType())
&gt;&gt;&gt; idxCategoryUDF = udf(idxCategory, IntegerType())

<strong class="calibre19">//Step 1d: List categories
</strong>&gt;&gt;&gt; categories =["General","event","Hadoop","Python","resource"]
<strong class="calibre19">//Step 2: Prepare train data
</strong>
<strong class="calibre19">//Step 2a: Extract "text" data and split to words
</strong>&gt;&gt;&gt; from pyspark.sql.functions import split
&gt;&gt;&gt; path = "../work/tweets_train.txt"
&gt;&gt;&gt; raw_df1 = spark.read.text(path)
&gt;&gt;&gt; raw_df = raw_df1.where("value like '%"text":%'").select(
             split("value", " ")).toDF("words")

<strong class="calibre19">//Step 2b: Assign a category to each line
</strong>&gt;&gt;&gt; train_cat_df = raw_df.withColumn("category",\
        findCategoryUDF("words")).withColumn(
        "label",idxCategoryUDF("category"))

<strong class="calibre19">//Step 2c: Examine categories
</strong>scala&gt; train_cat_df.groupBy("category").count().show()
+--------+---------------+                                                     
|category|count(category)|
+--------+---------------+
| General|            146|
|resource|              1|
|  Python|              2|
|   event|             10|
|  Hadoop|              6|
+--------+---------------+

<strong class="calibre19">//Step 3: Build pipeline
</strong>&gt;&gt;&gt; from pyspark.ml import Pipeline
&gt;&gt;&gt; from pyspark.ml.feature import StopWordsRemover, CountVectorizer,
IndexToString
&gt;&gt;&gt; from pyspark.ml.classification import NaiveBayes
&gt;&gt;&gt;

<strong class="calibre19">//Step 3a: Define pipeline stages
</strong>
<strong class="calibre19">//Stop words should be removed first
</strong>&gt;&gt;&gt; stopw = StopWordsRemover(inputCol = "words",
                  outputCol = "processed_words")
<strong class="calibre19">//Terms to term frequency converter
</strong>&gt;&gt;&gt; cv = CountVectorizer(inputCol = "processed_words",
             outputCol = "features")
<strong class="calibre19">//Define model
</strong>&gt;&gt;&gt; model = NaiveBayes(featuresCol="features",
                   labelCol = "label")
<strong class="calibre19">//Numerical prediction label to category converter
</strong>&gt;&gt;&gt; lc = IndexToString(inputCol = "prediction",
           outputCol = "predictedCategory",
           labels = categories)
&gt;&gt;&gt; 

<strong class="calibre19">//Step 3b: Build pipeline with desired stages
</strong>&gt;&gt;&gt; p = Pipeline(stages = [stopw,cv,model,lc])
&gt;&gt;&gt; 
<strong class="calibre19">

//Step 4: Process train data and get predictions
</strong>
<strong class="calibre19">//Step 4a: Execute pipeline with train data
</strong>&gt;&gt;&gt; resultsDF = p.fit(train_cat_df).transform(train_cat_df)

<strong class="calibre19">//Step 4b: Examine results
</strong>&gt;&gt;&gt; resultsDF.select("category","predictedCategory").show(3)
+--------+-----------------+
|category|predictedCategory|
+--------+-----------------+
|   event|            event|
|   event|            event|
| General|          General|
+--------+-----------------+
<strong class="calibre19">
//Step 4c: Look for prediction mismatches
</strong>&gt;&gt;&gt; resultsDF.filter("category != predictedCategory").select(
         "category","predictedCategory").show(3)
+--------+-----------------+
|category|predictedCategory|
+--------+-----------------+
|  Python|           Hadoop|
|  Python|           Hadoop|
|  Hadoop|            event|
+--------+-----------------+
<strong class="calibre19">
//Step 5: Evaluate model using test data
</strong>
<strong class="calibre19">//Step5a: Prepare test data
</strong>&gt;&gt;&gt; path = "&lt;Your path&gt;/tweets.json"&gt;&gt;&gt; raw_df1 = spark.read.text(path)
&gt;&gt;&gt; raw_test_df = raw_df1.where("va
ue like '%"text":%'").select(
               split("value", " ")).toDF("words")
&gt;&gt;&gt; test_cat_df = raw_test_df.withColumn("category",
        findCategoryUDF("words")).withColumn(
        "label",idxCategoryUDF("category"))
&gt;&gt;&gt; test_cat_df.groupBy("category").count().show()
+--------+---------------+                                                     
|category|count(category)|
+--------+---------------+
| General|              6|
|   event|             11|
+--------+---------------+
<strong class="calibre19">
//Step 5b: Run predictions on test data
</strong>&gt;&gt;&gt; testResultsDF = p.fit(test_cat_df).transform(test_cat_df)
<strong class="calibre19">//Step 5c:: Examine results
</strong>&gt;&gt;&gt; testResultsDF.select("category","predictedCategory").show(3)
+--------+-----------------+
|category|predictedCategory|
+--------+-----------------+
| General|          General|
|   event|            event|
|   event|            event|
+--------+-----------------+
<strong class="calibre19">//Step 5d: Look for prediction mismatches
</strong>&gt;&gt;&gt; testResultsDF.filter("category != predictedCategory").select(
         "category","predictedCategory").show()
+--------+-----------------+
|category|predictedCategory|
+--------+-----------------+
|   event|          General|
|   event|          General|
+--------+-----------------+</pre><p class="calibre11">Once this is done, a model can be trained with the output of this step, which can classify a text blob or file.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec64" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Text clustering</h1></div></div></div><p class="calibre11">Clustering is an unsupervised learning technique. Intuitively, clustering groups objects into disjoint sets. We do not know how many groups exist in the data, or what might be the commonality within these groups (clusters).</p><p class="calibre11">Text clustering has several applications. For example, an organizational entity may want to organize its internal documents into similar clusters based on some similarity measure. The notion of similarity or distance is central to the clustering process. Common measures used are TF-IDF and cosine similarity. Cosine similarity, or the cosine distance, is the cos product of the word frequency vectors of two documents. Spark provides a variety of clustering algorithms that can be effectively used in text analytics.</p><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec101" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>K-means</h2></div></div></div><p class="calibre11">Perhaps K-means is the most intuitive of all the clustering algorithms. The idea is to segregate data points as <em class="calibre22">K</em> different clusters based on some similarity measure, say cosine distance or Euclidean distance. This algorithm that starts with <em class="calibre22">K</em> random single point clusters, and each of the remaining data points are assigned to nearest cluster. Then cluster centers are recomputed and the algorithm loops through the data points once again. This process continues iteratively until there are no re-assignments or when pre-defined iteration count is reached.</p><p class="calibre11">How to fix the number of clusters (<em class="calibre22">K</em>) is not obvious. Identifying the initial cluster centers is also not obvious. Sometimes the business requirement may dictate the number of clusters; for example, partition all existing documents into 10 different sections. But in most of the real world scenarios, we need to find <em class="calibre22">K</em> through trial and error. One way is to progressively increase the <em class="calibre22">K</em> value and compute the cluster quality, such as cluster variance. The quality ceases to improve significantly beyond a certain value of <em class="calibre22">K,</em> which could be your ideal <em class="calibre22">K</em>. There are various other techniques, such as the elbow method, <strong class="calibre19">Akaike information criterion</strong> (<strong class="calibre19">AIC</strong>), and <strong class="calibre19">Bayesian information criterion</strong> (<strong class="calibre19">BIC</strong>).</p><p class="calibre11">Likewise, start with different starting points until the cluster quality is satisfactory. Then you may wish to validate your result using techniques such as Silhouette Score. However, these activities are computationally intensive.</p><p class="calibre11">Spark provides K-means from MLlib as well as ml packages. You may specify maximum iterations or convergence tolerance to fine tune algorithm performance.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec65" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Dimensionality reduction</h1></div></div></div><p class="calibre11">Imagine a large matrix with many rows and columns. In many matrix applications, this large matrix can be represented by some narrow matrices with small number of rows and columns that still represents the original matrix. Then processing this smaller matrix may yield similar results as that of the original matrix. This can be computationally efficient.</p><p class="calibre11">Dimensionality reduction is about finding that small matrix. MLLib supports two algorithms, SVD and PCA for dimensionality reduction on RowMatrix class. Both of these  algorithms allow us to specify the number of dimensions we are interested in retaining. Let us look at example first and then delve into the underlying theory .</p><p class="calibre11">
<strong class="calibre19">Example 9: Dimensionality reduction</strong>
</p><p class="calibre11">
<strong class="calibre19">Scala:</strong>
</p><pre class="programlisting">scala&gt; import scala.util.Random
import scala.util.Random
scala&gt; import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.linalg.{Vector, Vectors}
scala&gt; import org.apache.spark.mllib.linalg.distributed.RowMatrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

<strong class="calibre19">//Create a RowMatrix of 6 rows and 5 columns
</strong>scala&gt; var vlist: Array[Vector] = Array()
vlist: Array[org.apache.spark.mllib.linalg.Vector] = Array()
scala&gt; for (i &lt;- 1 to 6) vlist = vlist :+ Vectors.dense(
       Array.fill(5)(Random.nextInt*1.0))
scala&gt; val rows_RDD = sc.parallelize(vlist)
rows_RDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] =
ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:29
scala&gt; val row_matrix = new RowMatrix(rows_RDD)
row_matrix: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@348a6639
<strong class="calibre19">
//SVD example for top 3 singular values
</strong>scala&gt; val SVD_result = row_matrix.computeSVD(3)
SVD_result:
org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mlli
.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix] =
SingularValueDecomposition(null,
[4.933482776606544E9,3.290744495921952E9,2.971558550447048E9],
-0.678871347405378    0.054158900880961904  -0.23905281217240534
0.2278187940802       -0.6393277579229861   0.078663353163388
0.48824560481341733   0.3139021297613471    -0.7800061948839081
-0.4970903877201546   2.366428606359744E-4  -0.3665502780139027
0.041829015676406664  0.6998515759330556    0.4403374382132576    )

scala&gt; SVD_result.s   //Show the singular values (strengths)
res1: org.apache.spark.mllib.linalg.Vector =
[4.933482776606544E9,3.290744495921952E9,2.971558550447048E9]

<strong class="calibre19">//PCA example to compute top 2 principal components
</strong>scala&gt; val PCA_result = row_matrix.computePrincipalComponents(2)
PCA_result: org.apache.spark.mllib.linalg.Matrix =
-0.663822435334425    0.24038790854106118
0.3119085619707716    -0.30195355896094916
0.47440026368044447   0.8539858509513869
-0.48429601343640094  0.32543904517535094
-0.0495437635382354   -0.12583837216152594</pre><p class="calibre11">
<strong class="calibre19">Python:</strong>
</p><p class="calibre11">Not available in Python as of Spark 2.0.0</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec66" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Singular Value Decomposition</h1></div></div></div><p class="calibre11">The <strong class="calibre19">Singular Value Decomposition</strong> (<strong class="calibre19">SVD</strong>) is one of the centerpieces of linear algebra and is widely used for many real-world modeling requirements. It provides a convenient way of breaking a matrix into simpler, smaller matrices. This leads to a low-dimensional representation of a high-dimensional matrix. It helps us eliminate less important parts of the matrix to produce an approximate representation. This technique is useful in dimensionality reduction and data compression.</p><p class="calibre11">Let <em class="calibre22">M</em> be a matrix of size m-rows and n-columns. The rank of a matrix is the number of rows that are linearly independent. A row is considered independent if it has at least one non-zero element and it is not a linear combination of one or more rows. The same rank will be obtained if we considered columns instead of rows - as in linear algebra.</p><p class="calibre11">If the elements of one row are the sum of two rows, then that row is not independent. Then as a result of SVD, we find three matrices, <em class="calibre22">U</em>, <em class="calibre22">∑</em>, and <em class="calibre22">V</em> that satisfy the following equation:</p><p class="calibre11">
<em class="calibre22">M = U∑VT</em>
</p><p class="calibre11">These three matrices have the following properties:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">U</strong>: This is a column-orthonormal matrix with m rows and r columns. An orthonormal matrix implies that each of the columns is a unit vector and the pairwise dot product between any two columns is 0.</li><li class="listitem"><strong class="calibre19">V</strong>: This is a column-orthonormal matrix with <em class="calibre22">n</em> rows and <em class="calibre22">r</em> columns.</li><li class="listitem"><strong class="calibre19">∑</strong>: This is an <em class="calibre22">r </em>x <em class="calibre22">r</em> diagonal matrix with non-negative real numbers as principal diagonal values in descending order. In a diagonal matrix, all elements except the ones on the principal diagonal are zero.</li></ul></div><p class="calibre11">The principal diagonal values in the <em class="calibre22">∑</em> matrix are called singular values. They are considered as the underlying <em class="calibre22">concepts</em> or <em class="calibre22">components</em> that connect the rows and columns of the matrix. Their magnitude represents the strength of the corresponding components. For example, imagine that the matrix in the previous example contains ratings of five books by six readers. SVD allows us to split them into three matrices: <em class="calibre22">∑</em> containing the singular values representing the <em class="calibre22">strength</em> of underlying topics; <em class="calibre22">U</em> connecting people to concepts; and <em class="calibre22">V</em> connecting concepts to books.</p><p class="calibre11">In a large matrix, we can replace the lower magnitude singular values to zero and thereby reduce the corresponding rows in the remaining two matrices. Note that if we re-compute the matrix product on the right hand side and compare the value with the original matrix on the left hand side, they will be almost similar. We can use this technique to retain the desired number of dimensions.</p><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec102" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Principal Component Analysis</h2></div></div></div><p class="calibre11">
<strong class="calibre19">Principal Component Analysis</strong> (<strong class="calibre19">PCA</strong>) is a technique that takes n-dimensional data points and project onto a smaller (fewer dimensions) subspace with minimum loss of information. A set of data points in a high dimensional space find the directions along which these tuples line up best. In other words, we need to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible. The idea is to treat the set of tuples as a matrix <em class="calibre22">M</em> and find the eigenvectors for MMT.</p><p class="calibre11">If <em class="calibre22">A</em> is a square matrix, <em class="calibre22">e</em> is a column matrix with the same number of rows as <em class="calibre22">A</em>, and <em class="calibre22">λ</em> is a constant such that <em class="calibre22">Me = λe</em>, then <em class="calibre22">e</em> is called the eigenvector of <em class="calibre22">M</em> and <em class="calibre22">λ</em> is called the eigenvalue of <em class="calibre22">M</em>. In terms of n-dimensional plane, the eigenvector is the direction and the eigenvalue is a measure of variance along that direction. We can drop the dimensions with a low eigenvalue, thereby finding a smaller subspace without loss of information.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec67" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we examined the sources of unstructured data and the motivation behind analyzing the unstructured data. We explained various techniques that are required in pre-processing unstructured data and how Spark provides most of these tools out of the box. We also covered some of the algorithms supported by Spark that can be used in text analytics.</p><p class="calibre11">In the next chapter, we will go through different types of visualization techniques that are insightful in different stages of data analytics lifecycle.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec68" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References:</h1></div></div></div><p class="calibre11">The following are the references:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf">http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf">https://www.cs.nyu.edu/web/Research/Theses/borthwick_andrew.pdf</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://web.stanford.edu/class/cs124/lec/naivebayes.pdf">https://web.stanford.edu/class/cs124/lec/naivebayes.pdf</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html">http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.mmds.org/">http://www.mmds.org/</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://sebastianraschka.com/Articles/2014_pca_step_by_step.html">http://sebastianraschka.com/Articles/2014_pca_step_by_step.html</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://arxiv.org/pdf/1404.1100.pdf">http://arxiv.org/pdf/1404.1100.pdf</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html">http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html</a></li></ul></div><p class="calibre11">Count Vectorizer:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html">https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/ml/feature/CountVectorizer.html</a></li></ul></div><p class="calibre11">n-gram modeling:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://en.wikipedia.org/wiki/N-gram">https://en.wikipedia.org/wiki/N-gram</a></li></ul></div></div></div>



  </body></html>