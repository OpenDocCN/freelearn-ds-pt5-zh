["```py\n# Load mlbench and create a regression model of glucose (outcome/dependent variable) with pressure, triceps and insulin as the independent variables.\n\n> library(\"mlbench\") \n>lm_model<- lm(glucose ~ pressure + triceps + insulin, data=PimaIndiansDiabetes[1:100,]) \n> plot(lm_model) \n```", "```py\n# The LHS (left-hand side) leads to the RHS (right-hand side) in the relationships shown below.\n\n# For instance, {Milk, Bread} --> {Butter} indicates that someone purchasing milk and bread is also likely to purchase butter.\n\n{Milk, Bread} --> {Butter}\n{Butter, Egg} --> {Baking Tray}\n{Baking Tray, Butter} --> {Sugar}\n...\n```", "```py\n{Milk} à {Bread}\n```", "```py\n{Bread} à {Butter}\n```", "```py\ninstall.packages(\"rpart\") \ninstall.packages(\"rpart.plot\") \n\nlibrary(rpart) \nlibrary(rpart.plot) \n\nrpart_model<- rpart (diabetes ~ glucose + insulin + mass + age, data = PimaIndiansDiabetes) \n\n>rpart_model \nn= 768  \n\nnode), split, n, loss, yval, (yprob) \n      * denotes terminal node \n\n  1) root 768 268 neg (0.6510417 0.3489583)   \n    2) glucose< 127.5 485  94neg (0.8061856 0.1938144) * \n    3) glucose>=127.5 283 109 pos (0.3851590 0.6148410)   \n      6) mass< 29.95 76  24neg (0.6842105 0.3157895)   \n       12) glucose< 145.5 41   6 neg (0.8536585 0.1463415) * \n       13) glucose>=145.5 35  17pos (0.4857143 0.5142857)   \n         26) insulin< 14.5 21   8 neg (0.6190476 0.3809524) * \n         27) insulin>=14.5 14   4 pos (0.2857143 0.7142857) * \n      7) mass>=29.95 207  57pos (0.2753623 0.7246377)   \n       14) glucose< 157.5 115  45pos (0.3913043 0.6086957)   \n         28) age< 30.5 50  23neg (0.5400000 0.4600000)   \n           56) insulin>=199 14   3 neg (0.7857143 0.2142857) * \n           57) insulin< 199 36  16pos (0.4444444 0.5555556)   \n            114) age>=27.5 10   3 neg (0.7000000 0.3000000) * \n            115) age< 27.5 26   9 pos (0.3461538 0.6538462) * \n         29) age>=30.5 65  18pos (0.2769231 0.7230769) * \n       15) glucose>=157.5 92  12pos (0.1304348 0.8695652) * \n\n>rpart.plot(rpart_model, extra=102, nn=TRUE)\n\n# The plot shown below illustrates the decision tree that the model, rpart_model represents.\n```", "```py\n> sum(PimaIndiansDiabetes$diabetes==\"neg\") \n[1] 500 \n```", "```py\ninstall.packages(\"FFTrees\") \nlibrary(caret) \nlibrary(mlbench) \nlibrary(FFTrees) \nset.seed(123) \n\ndata(\"PimaIndiansDiabetes\") \ndiab<- PimaIndiansDiabetes \ndiab$diabetes<- 1 * (diab$diabetes==\"pos\") \n\ntrain_ind<- createDataPartition(diab$diabetes,p=0.8,list=FALSE,times=1) \n\ntraining_diab<- diab[train_ind,] \ntest_diab<- diab[-train_ind,] \n\ndiabetes.fft<- FFTrees(diabetes ~.,data = training_diab,data.test = test_diab) \nplot(diabetes.fft)\n\n# The plot below illustrates the decision tree representing diabetes.fft using the FFTrees package.\n```", "```py\n> rf_model1 <- randomForest(diabetes ~ ., data=PimaIndiansDiabetes) > rf_model1 Call: randomForest(formula = diabetes ~ ., data = PimaIndiansDiabetes) \nType of random forest: classification Number of trees: 500 No. of variables tried at each split: 2 OOB estimate of error rate: 23.44% Confusion matrix: negposclass.error neg430 70 0.1400000 pos 110 158 0.4104478\n```", "```py\n> library(caret) \n> library(doMC) \n\n# THE NEXT STEP IS VERY CRITICAL - YOU DO 'NOT' NEED TO USE MULTICORE \n# NOTE THAT THIS WILL USE ALL THE CORES ON THE MACHINE THAT YOU ARE \n# USING TO RUN THE EXERCISE \n\n# REMOVE THE # MARK FROM THE FRONT OF registerDoMC BEFORE RUNNING \n# THE COMMAND \n\n># registerDoMC(cores = 8) # CHANGE NUMBER OF CORES TO MATCH THE NUMBER OF CORES ON YOUR MACHINE  \n\n>rf_model<- train(diabetes ~ ., data=PimaIndiansDiabetes, method=\"rf\") \n>rf_model \nRandom Forest  \n\n768 samples \n  8 predictor \n  2 classes: 'neg', 'pos'  \n\nNo pre-processing \nResampling: Bootstrapped (25 reps)  \nSummary of sample sizes: 768, 768, 768, 768, 768, 768, ...  \nResampling results across tuning parameters: \n\nmtry  Accuracy   Kappa     \n  2     0.7555341  0.4451835 \n  5     0.7556464  0.4523084 \n  8     0.7500721  0.4404318 \n\nAccuracy was used to select the optimal model using  the largest value. \nThe final value used for the model was mtry = 5\\. \n\n>getTrainPerf(rf_model) \n\nTrainAccuracyTrainKappa method \n1     0.7583831  0.4524728rf \n```", "```py\n>getTree(rf_model1,1,labelVar = TRUE) \n    left daughter right daughter split var split point status prediction \n1               2              3      mass     27.8500      1       <NA> \n2               4              5       age     28.5000      1       <NA> \n3               6              7   glucose    155.0000      1       <NA> \n4               8              9       age     27.5000      1       <NA> \n5              10             11      mass      9.6500      1       <NA> \n6              12             13  pregnant      7.5000      1       <NA> \n7              14             15   insulin     80.0000      1       <NA> \n8               0              0      <NA>      0.0000     -1        neg \n9              16             17  pressure     68.0000      1       <NA> \n10              0              0      <NA>      0.0000     -1        pos \n11             18             19   insulin    131.0000      1       <NA> \n12             20             21   insulin     87.5000      1       <NA> \n\n [...]\n```", "```py\n# Creating an XGBoost model in R\n\nlibrary(caret)\nlibrary(xgboost) \n\nset.seed(123) \ntrain_ind<- sample(nrow(PimaIndiansDiabetes),as.integer(nrow(PimaIndiansDiabetes)*.80)) \n\ntraining_diab<- PimaIndiansDiabetes[train_ind,] \ntest_diab<- PimaIndiansDiabetes[-train_ind,] \n\ndiab_train<- sparse.model.matrix(~.-1, data=training_diab[,-ncol(training_diab)]) \ndiab_train_dmatrix<- xgb.DMatrix(data = diab_train, label=training_diab$diabetes==\"pos\") \n\ndiab_test<- sparse.model.matrix(~.-1, data=test_diab[,-ncol(test_diab)]) \ndiab_test_dmatrix<- xgb.DMatrix(data = diab_test, label=test_diab$diabetes==\"pos\") \n\nparam_diab<- list(objective = \"binary:logistic\", \neval_metric = \"error\", \n              booster = \"gbtree\", \nmax_depth = 5, \n              eta = 0.1) \n\nxgb_model<- xgb.train(data = diab_train_dmatrix, \nparam_diab, nrounds = 1000, \nwatchlist = list(train = diab_train_dmatrix, test = diab_test_dmatrix), \nprint_every_n = 10) \n\npredicted <- predict(xgb_model, diab_test_dmatrix) \npredicted <- predicted > 0.5 \n\nactual <- test_diab$diabetes == \"pos\" \nconfusionMatrix(actual,predicted) \n\n# RESULT \n\nConfusion Matrix and Statistics \n\n          Reference \nPrediction FALSE TRUE \n     FALSE    80   17 \n     TRUE     21   36 \n\nAccuracy : 0.7532           \n                 95% CI : (0.6774, 0.8191) \n    No Information Rate : 0.6558           \n    P-Value [Acc> NIR] : 0.005956         \n\nKappa : 0.463            \nMcnemar's Test P-Value : 0.626496         \n\nSensitivity : 0.7921           \nSpecificity : 0.6792           \nPosPredValue : 0.8247           \nNegPredValue : 0.6316           \nPrevalence : 0.6558           \n         Detection Rate : 0.5195           \n   Detection Prevalence : 0.6299           \n      Balanced Accuracy : 0.7357           \n\n       'Positive' Class : FALSE       \n```", "```py\nlibrary(mlbench) \nlibrary(caret) \nlibrary(e1071) \nset.seed(123) \n\ndata(\"PimaIndiansDiabetes\") \ndiab<- PimaIndiansDiabetes \n\ntrain_ind<- createDataPartition(diab$diabetes,p=0.8,list=FALSE,times=1) \n\ntraining_diab<- diab[train_ind,] \ntest_diab<- diab[-train_ind,] \n\nsvm_model<- svm(diabetes ~ ., data=training_diab) \nplot(svm_model,training_diab, glucose ~ mass) \n\n# The plot below illustrates the areas that are classified 'positive' and 'negative'\n```", "```py\n# Creating and evaluating the Confusion Matrix for the SVM model\n\nsvm_predicted<- predict(svm_model,test_diab[,-ncol(test_diab)]) \nconfusionMatrix(svm_predicted,test_diab$diabetes) \n\nConfusion Matrix and Statistics \n\n          Reference \nPrediction negpos \nneg  93  26 \npos7  27 \n\nAccuracy : 0.7843           \n                 95% CI : (0.7106, 0.8466) \n    No Information Rate : 0.6536           \n    P-Value [Acc> NIR] : 0.0003018        \n\nKappa : 0.4799           \nMcnemar's Test P-Value : 0.0017280        \n\nSensitivity : 0.9300           \nSpecificity : 0.5094           \nPosPredValue : 0.7815           \nNegPredValue : 0.7941           \nPrevalence : 0.6536           \n         Detection Rate : 0.6078           \n   Detection Prevalence : 0.7778           \n      Balanced Accuracy : 0.7197           \n\n       'Positive' Class :neg \n```", "```py\nlibrary(data.table) \nlibrary(ggplot2) \nlibrary() \n\nhistoryData<- fread(\"~/Desktop/history.csv\") \nggplot(historyData,aes(american_history,asian_history)) + geom_point() + geom_jitter() \n\nhistoryCluster<- kmeans(historyData,2) # Create 2 clusters \nhistoryData[,cluster:=as.factor(historyCluster$cluster)] \nggplot(historyData, aes(american_history,asian_history,color=cluster)) + geom_point() + geom_jitter()\n\n# The image below shows the output of the ggplot command. Note that the effect of geom_jitter can be seen in the image below (the points are nudged so that overlapping points can be easily visible)\n```", "```py\nlibrary(mlbench) \nlibrary(caret) \nset.seed(123) \n\ndata(\"PimaIndiansDiabetes\") \ndiab<- PimaIndiansDiabetes \n\ntrain_ind<- createDataPartition(diab$diabetes,p=0.8,list=FALSE,times=1) \n\ntraining_diab<- diab[train_ind,] \ntest_diab<- diab[-train_ind,] \n\nnnet_grid<- expand.grid(.decay = c(0.5,0.1), .size = c(3,5,7)) \n\nnnet_model<- train(diabetes ~ ., data = training_diab, method = \"nnet\", metric = \"Accuracy\", maxit = 500, tuneGrid = nnet_grid) \n\n# Generating predictions using the neural network model\nnnet_predicted <- predict(nnet_model, test_diab)\n\n> plot (nnet_model)\n\n```", "```py\n# Confusion Matrix for the Neural Network model\n\nconfusionMatrix(nnet_predicted,test_diab$diabetes)\n\nConfusion Matrix and Statistics \n\n          Reference \nPrediction negpos \nneg  86  22 \npos  14  31 \n\nAccuracy : 0.7647           \n                 95% CI : (0.6894, 0.8294) \n    No Information Rate : 0.6536           \n    P-Value [Acc> NIR] : 0.001988         \n\nKappa : 0.4613           \nMcnemar's Test P-Value : 0.243345         \n\nSensitivity : 0.8600           \nSpecificity : 0.5849           \nPosPredValue : 0.7963           \nNegPredValue : 0.6889           \nPrevalence : 0.6536           \n         Detection Rate : 0.5621           \n   Detection Prevalence : 0.7059           \n      Balanced Accuracy : 0.7225           \n\n       'Positive' Class :neg \n```", "```py\ntime wget -O cms2016_2.csv 'https://openpaymentsdata.cms.gov/resource/vq63-hu5i.csv?$query=select Physician_First_Name as firstName,Physician_Last_Name as lastName,Recipient_City as city,Recipient_State as state,Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name as company,Total_Amount_of_Payment_USDollars as payment,Nature_of_Payment_or_Transfer_of_Value as paymentNature,Product_Category_or_Therapeutic_Area_1 as category,Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_1 as product where covered_recipient_type like \"Covered Recipient Physician\" and Recipient_State like \"NY\" limit 1200000' \n```", "```py\nlibrary(data.table) \nlibrary(arules) \n\ncms<- fread(\"~/cms2016_2.csv\") # CHANGE THIS TO YOUR LOCATION OF THE DATA \n\ncols <- c(\"category\",\"city\",\"company\",\"firstName\",\"lastName\",\"paymentNature\",\"product\") \n\ncms[ ,(cols) := lapply(.SD, toupper), .SDcols = cols] \n\ncms[,payment:=as.numeric(payment)] \n\nquantile_values<- quantile(cms$payment,seq(0,1,.25)) \ninterval_values<- findInterval(cms$payment,quantile_values,rightmost.closed=TRUE) \n\ncms[,quantileVal:=factor(interval_values, labels=c(\"0-25\",\"25-50\",\"50-75\",\"75-100\"))] \n\nrules_cols<- c(\"category\",\"city\",\"company\",\"paymentNature\",\"product\",\"quantileVal\") \n\ncms[ ,(rules_cols) := lapply(.SD, factor), .SDcols = rules_cols] \n\ncms_factor<- cms[,.(category,city,company,paymentNature,product,quantileVal)] \n\nrhsVal<- paste0(\"quantileVal\",\"=\",c(\"0-25\",\"25-50\",\"50-75\",\"75-100\")) \n\ncms_rules<- apriori(cms_factor,parameter=list(supp=0.001,conf=0.25,target=\"rules\",minlen=3)) \n\ncms_rules_dt<- data.table(as(cms_rules,\"data.frame\")) \ncms_rules_dt[, c(\"LHS\", \"RHS\") := tstrsplit(rules, \"=>\", fixed=TRUE)] \nnum_cols<- c(\"support\",\"confidence\",\"lift\") \ncms_rules_dt[,(num_cols) := lapply(.SD, function(x){round(x,2)}), .SDcols = num_cols] \n\nsaveRDS(cms_rules_dt,\"cms_rules_dt.rds\") \nsaveRDS(cms_factor,\"cms_factor_dt.rds\") \n```", "```py\n# Packt: Big Data Analytics \n# Chapter 8 Tutorial \n\nlibrary(shiny) \nlibrary(shinydashboard) \nlibrary(data.table) \nlibrary(DT) \nlibrary(shinyjs) \n\ncms_factor_dt<- readRDS(\"~/r/rulespackt/cms_factor_dt.rds\") \ncms_rules_dt<- readRDS(\"~/r/rulespackt/cms_rules_dt.rds\") \n\n# Define UI for application that draws a histogram \nui<- dashboardPage (skin=\"green\",    \ndashboardHeader(title = \"Apriori Algorithm\"), \ndashboardSidebar( \nuseShinyjs(), \nsidebarMenu( \nuiOutput(\"company\"), \nuiOutput(\"searchlhs\"), \nuiOutput(\"searchrhs\"), \nuiOutput(\"support2\"), \nuiOutput(\"confidence\"), \nuiOutput(\"lift\"), \ndownloadButton('downloadMatchingRules', \"Download Rules\") \n\n         ) \n),dashboardBody( \ntags$head( \ntags$link(rel = \"stylesheet\", type = \"text/css\", href = \"packt2.css\"), \ntags$link(rel = \"stylesheet\", type = \"text/css\", href = \"//fonts.googleapis.com/css?family=Fanwood+Text\"), \ntags$link(rel = \"stylesheet\", type = \"text/css\", href = \"//fonts.googleapis.com/css?family=Varela\"), \ntags$link(rel = \"stylesheet\", type = \"text/css\", href = \"fonts.css\"), \n\ntags$style(type=\"text/css\", \"select { max-width: 200px; }\"), \ntags$style(type=\"text/css\", \"textarea { max-width: 185px; }\"), \ntags$style(type=\"text/css\", \".jslider { max-width: 200px; }\"), \ntags$style(type='text/css', \".well { max-width: 250px; padding: 10px; font-size: 8px}\"), \ntags$style(type='text/css', \".span4 { max-width: 250px; }\") \n\n         ), \nfluidRow( \ndataTableOutput(\"result\") \n) \n       ), \n       title = \"Aprior Algorithm\" \n) \n\n# Define server logic required to draw a histogram \nserver <- function(input, output, session) { \n\n  PLACEHOLDERLIST2 <- list( \n    placeholder = 'Select All', \nonInitialize = I('function() { this.setValue(\"\"); }') \n  ) \n\noutput$company<- renderUI({ \ndatasetList<- c(\"Select All\",as.character(unique(sort(cms_factor_dt$company)))) \nselectizeInput(\"company\", \"Select Company\" ,  \ndatasetList, multiple = FALSE,options = PLACEHOLDERLIST2,selected=\"Select All\") \n  }) \n\noutput$searchlhs<- renderUI({ \ntextInput(\"searchlhs\", \"Search LHS\", placeholder = \"Search\") \n  }) \n\noutput$searchrhs<- renderUI({ \ntextInput(\"searchrhs\", \"Search RHS\", placeholder = \"Search\") \n  }) \n\n  output$support2 <- renderUI({ \nsliderInput(\"support2\", label = 'Support',min=0,max=0.04,value=0.01,step=0.005) \n  }) \n\noutput$confidence<- renderUI({ \nsliderInput(\"confidence\", label = 'Confidence',min=0,max=1,value=0.5) \n  }) \n\noutput$lift<- renderUI({ \nsliderInput(\"lift\", label = 'Lift',min=0,max=10,value=0.8) \n  }) \n\ndataInput<- reactive({ \n    print(input$support2) \n    print(input$company) \n    print(identical(input$company,\"\")) \n\n    temp <- cms_rules_dt[support > input$support2 & confidence >input$confidence& lift >input$lift] \n\n    if(!identical(input$searchlhs,\"\")){ \nsearchTerm<- paste0(\"*\",input$searchlhs,\"*\") \n      temp <- temp[LHS %like% searchTerm] \n    } \n\n    if(!identical(input$searchrhs,\"\")){ \nsearchTerm<- paste0(\"*\",input$searchrhs,\"*\") \n      temp <- temp[RHS %like% searchTerm] \n    } \n\nif(!identical(input$company,\"Select All\")){ \n      # print(\"HERE\") \n      temp <- temp[grepl(input$company,rules)] \n    } \n    temp[,.(LHS,RHS,support,confidence,lift)] \n  }) \n\noutput$downloadMatchingRules<- downloadHandler( \n    filename = \"Rules.csv\", \n    content = function(file) { \n      write.csv(dataInput(), file, row.names=FALSE) \n    } \n  ) \n\noutput$result<- renderDataTable({ \n    z = dataInput() \n    if (nrow(z) == 0) { \n      z <- data.table(\"LHS\" = '', \"RHS\"='', \"Support\"='', \"Confidence\"='', \"Lift\" = '') \n    } \nsetnames(z, c(\"LHS\", \"RHS\", \"Support\", \"Confidence\", \"Lift\")) \ndatatable(z,options = list(scrollX = TRUE)) \n  }) \n\n}  shinyApp(ui = ui, server = server)\n```"]