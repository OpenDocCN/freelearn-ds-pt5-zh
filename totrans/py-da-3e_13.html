<html><head></head><body>
        

                            
                    Supervised Learning - Classification Techniques
                
            
            
                
<p class="mce-root"> Most real-world machine learning problems use supervised learning. In supervised learning, the model will learn from a labeled training dataset. A label is a target variable that we want to predict. It is an extra piece of information that helps in making decisions or predictions, for example, which loan application is safe or risky, whether a patient suffers from a disease or not, house prices, and credit eligibility scores. These labels act as a supervisor or teacher for the learning process. Supervised learning algorithms can be of two types: classification or regression. A classification problem has a categorical target variable, such as a loan application status as safe or risky, whether a patient suffers from a "disease" or "not disease," or whether a customer is "potential" or "not potential."</p>
<p>This chapter focuses on supervised machine learning, and specifically covers classification techniques. This chapter will mostly be using scikit-learn. It will delve into basic techniques of classification, such as naive Bayes, <strong>Support Vector Machines</strong> (<strong>SVMs</strong>), <strong>K-Nearest Neighbor</strong> (<strong>KNN</strong>), and decision trees. Also, it focuses on train-test split strategies and model evaluation methods and parameters.</p>
<p>The topics of this chapter are listed as follows:</p>
<ul>
<li>Classification </li>
<li>Naive Bayes classification</li>
<li>Decision tree classification</li>
<li>KNN classification</li>
<li>SVM classification </li>
<li>Splitting training and testing sets</li>
<li>Evaluating the classification model performance</li>
<li>ROC curve and AUC</li>
</ul>
<h1 id="uuid-c6dd8290-6cb6-4a92-b7f6-99c4bde160e6">Technical requirements</h1>
<p>This chapter has the following technical requirements:</p>
<ul>
<li>You can find the code and the datasets at the following GitHub link: <a href="https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter10">https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter10</a>.</li>
<li>All the code blocks are available in the <kbd>ch10.ipynb</kbd> file.  </li>
<li>This chapter uses only one CSV file (<kbd>diabetes.csv</kbd>) for practice purposes.</li>
<li>In this chapter, we will use the <kbd>pandas</kbd> and <kbd>scikit-learn</kbd> Python libraries.</li>
</ul>
<h1 id="uuid-0ee3cc76-c69a-4303-a971-d44fab309d47">Classification</h1>
<p>As a healthcare data analyst, your job is to identify patients or sufferers that have a higher chance of a particular disease, for example, diabetes or cancer. These predictions will help you to treat patients before the disease occurs. Similarly, a sales and marketing manager wants to predict potential customers who have more of a chance of buying a product. This is the process of categorizing customers into two or more categories known as classification. The classification model predicts the categorical class label, such as whether the customer is potential or not. In the classification process, the model is trained on available data, makes predictions, and evaluates the model performance. Developed models are called classifiers. This means it has three stages: training, prediction, and evaluation. The trained model is evaluated using parameters such as accuracy, precision, recall, F1-score, and <strong>Area Under Curve</strong> (<strong>AUC</strong>). Classification has a variety of applications in various domains, such as banking, finance, citizen services, healthcare, text analysis, image identification, and object detection.</p>
<p>As an analyst, you have to first define the problem that you want to solve using classification and then identify the potential features that predict the labels accurately. Features are the columns or attributes that are responsible for prediction. In diabetes prediction problems, health analysts will collect patient information, such as age, exercise routine, junk food-eating habits, alcohol consumption, and smoking habit characteristics or features. These features will be used to predict whether the patient will suffer from diabetes. You can see in the following diagram how data can be classified into two classes using a line:</p>
<div><img src="img/1640f8cc-3ce7-47e7-99b5-f52cd572fd96.png" style=""/></div>
<p>Machine learning and data mining processes have various steps: data collection, data preprocessing, train-test split, model generation, and evaluation. We have seen data analysis models such as KDD, SEMMA, and CRISP-DM. In classification, we only focus on the train-test split, model generation, and evaluation. </p>
<p>The classification model has three stages: train-test split, model generation, and model evaluation. In the train-test split stage, data is divided into two parts: training and testing sets. In training, the training set is used to generate the model, and testing is used in the model evaluation stage to assess the model's performance using evaluation metrics such as accuracy, error, precision, and recall. You can see the classification process in the following diagram:</p>
<div><img src="img/e0deb2e7-0dd2-4bf4-a2c1-162c675ad21f.png" style=""/></div>
<p>In the preceding diagram, steps for the classification process are presented. Now that we understand the classification process, it's time to learn the classification techniques. In the next section, we will focus on the naive Bayes classification algorithm. </p>
<h1 id="uuid-4abe1dde-379d-4344-bfd7-cee5017ad458">Naive Bayes classification</h1>
<p>Naive Bayes is a classification method based on the Bayes theorem. Bayes' theorem is named after its inventor, the statistician Thomas Bayes. It is a fast, accurate, robust, easy-to-understand, and interpretable technique. It can also work faster on large datasets. Naive Bayes is effectively deployed in text mining applications such as document classification, predicting sentiments of customer reviews, and spam filtering.</p>
<p>The naive Bayes classifier is called naive because it assumes class conditional independence. Class conditional independence means each feature column is independent of the remaining other features. For example, in the case of determining whether a person has diabetes or not, it depends upon their eating habits, their exercise routine, the nature of their profession, and their lifestyle. Even if features are correlated or depend on each other, naive Bayes will still assume they are independent. Let's understand the Bayes theorem formula:</p>
<div><img src="img/eab7c66a-04d9-4a17-8d6c-80cf6afd72b5.png" style=""/></div>
<p>Here, <em>y</em> is the target and <em>X</em> is the set of features. <em>p(y)</em> and <em>p(X)</em> are the prior probabilities regardless of evidence. This means the probability of events before evidence is seen. <em>p(y|X)</em> is the posterior probability of event <em>X</em> after evidence is seen. It is the probability of <em>y</em> given evidence <em>X</em>. <em>p(X|y)</em> is the posterior probability of event <em>y</em> after evidence is seen. It is the probability of <em>X</em> given evidence <em>y</em>. Let's take an example of the preceding equation:</p>
<div><img src="img/d1307850-e64d-4428-b376-8fdad1ece1da.png" style=""/></div>
<p>Here, we are finding the probability of a patient who will suffer from diabetes based on their smoking frequency using Bayes' theorem.</p>
<p>Let's see the working of the naive Bayes classification algorithm. Assume that dataset <em>D</em> has <em>X</em> features and label <em>y</em>. Features can be n-dimensional, <em>X</em>=<em>X</em>1, <em>X</em>2, <em>X</em>3... <em>Xn</em>. Label <em>y</em> may have <em>m</em> classes, <em>C</em>1, <em>C</em>2, <em>C</em>3...<em>Cm</em>. It will work as follows:</p>
<ol>
<li>Calculate the prior probabilities,<img src="img/8fe3120f-0421-4f46-9e43-dbd9d4abfc96.png" style="width:2.17em;height:1.33em;"/> and <img src="img/36a92b16-c79e-467c-9b08-19c72ff86086.png" style="width:1.92em;height:1.25em;"/>, for the given class labels.</li>
<li>Calculate the posterior probabilities, <img src="img/8480cb8f-da61-418f-ac55-39bb30687dc2.png" style="width:2.67em;height:1.17em;"/>and <img src="img/51fa1b1c-b657-4692-93d1-bf6d84a2ca3a.png" style="width:2.83em;height:1.33em;"/>, with each attribute for each class:</li>
</ol>
<div><img src="img/1b29185a-39c7-4cb6-b2dd-ed851326b4ad.png" style=""/></div>
<ol start="3">
<li>Multiply the same class posterior probability,<img src="img/706c0f20-4bb2-46ba-a418-b7b8b050295c.png" style="width:3.42em;height:1.42em;"/>:</li>
</ol>
<div><img src="img/d8bd2e11-eeb9-4840-b8ec-b914b3c7f6b2.png" style=""/></div>
<ol start="4">
<li>If the attribute is categorical then there should be several records of class <img src="img/5a8777da-f995-4a0b-8b00-46bc99b9d197.png" style="width:1.17em;height:1.50em;"/> in with <img src="img/98048ac3-e469-462f-8485-d38627764f62.png" style="width:1.50em;height:1.75em;"/>value, divided by <img src="img/c87ea333-1e06-47fd-bdc0-abfe136958fc.png" style="width:1.33em;height:1.83em;"/>records in the dataset.</li>
<li>If the attribute is continuous, then it is calculated using Gaussian distribution:</li>
</ol>
<div><img src="img/92d65d23-c06e-42f8-910a-19e5c8471c8e.png" style=""/></div>
<ol start="6">
<li>Multiply the prior probability, <em>p</em>(<em>y</em>), by the posterior probability from <em>step 3</em>:</li>
</ol>
<div><img src="img/052d16b7-760b-4974-bcd1-9df8053f9b06.png" style=""/></div>
<ol start="7">
<li class="CDPAlignLeft CDPAlign">Find the class with the maximum probability for the given input feature set. This class will be our final prediction.</li>
</ol>
<p>Now, let's create a model using naive Bayes classification in Python:</p>
<ol>
<li>Load the Pima Indians Diabetes dataset (<a href="https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter09/diabetes.csv">https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter09/diabetes.csv</a>) using the following lines of code:</li>
</ol>
<pre style="padding-left: 60px"># Import libraries
import pandas as pd<br/># read the dataset
diabetes = pd.read_csv("diabetes.csv")
<br/># Show top 5-records
diabetes.head()</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<div><img src="img/947371ae-5c7c-4092-9ca9-d99c27df1772.png" style=""/></div>
<p style="padding-left: 60px">We have thus imported <kbd>pandas</kbd> and read the dataset. In the preceding example, we are reading the Pima Indians Diabetes dataset.</p>
<ol start="2">
<li>We will now split the dataset into two parts, as follows:</li>
</ol>
<pre style="padding-left: 60px"># split dataset in two parts: feature set and target label
feature_set = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree'] features = diabetes[feature_set]<br/>target = diabetes.label
<br/># partition data into training and testing set
from sklearn.model_selection import train_test_split
<br/>feature_train,feature_test, target_train, target_test = \<br/>train_test_split(features, target, test_size=0.3, random_state=1)</pre>
<p style="padding-left: 60px">After loading the dataset, we divide the dataset into a dependent or label column (<kbd>target</kbd>) and independent or feature columns (<kbd>feature_set</kbd>). After this, the dataset will be broken up into train and test sets. Now, both the dependent and independent columns are broken up into train and test sets (<kbd>feature_train</kbd>, <kbd>feature_test</kbd>, <kbd>target_train</kbd>, and <kbd>target_test</kbd>) using <kbd>train_test_split()</kbd>. <kbd>train_test_split()</kbd> takes dependent and independent DataFrames, <kbd>test_size</kbd> and <kbd>random_state</kbd>. Here, <kbd>test_size</kbd> will decide the ratio of the train-test split (that is, <kbd>test_size 0.3</kbd> means 30% is the testing set and the remaining 70% of data will be the training set), and <kbd>random_state</kbd> is used as a seed value for reproducing the same data split each time. If <kbd>random_state</kbd> is <kbd>None</kbd>, then it will randomly split the records each time, which will give different performance measures.</p>
<ol start="3">
<li>We will now build the naive Bayes classification model:</li>
</ol>
<pre style="padding-left: 60px"># Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB<br/># Create a Gaussian Classifier
model = GaussianNB()
<br/># Train the model using the training sets
model.fit(feature_train,target_train)
<br/># Forecast the target variable for given test dataset
predictions = model.predict(feature_test)</pre>
<p style="padding-left: 60px">Here, we have created a naive Bayes model. First, we will import the <kbd>GaussianNB</kbd> class and create its object or model. This model will fit on the training dataset (<kbd>feature_train</kbd>, <kbd>target_train</kbd>). After training, the model is ready to make predictions using the <kbd>predict()</kbd> method.</p>
<ol start="4">
<li>Finally, we will evaluate the model's performance:</li>
</ol>
<pre style="padding-left: 60px"># Import metrics module for performance evaluation
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score<br/># Calculate model accuracy
print("Accuracy:",accuracy_score(target_test, predictions))
<br/># Calculate model precision
print("Precision:",precision_score(target_test, predictions))
<br/># Calculate model recall
print("Recall:",recall_score(target_test, predictions))
<br/># Calculate model f1 score
print("F1-Score:",f1_score(target_test, predictions))</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px"><strong>Accuracy: 0.7748917748917749
Precision: 0.7391304347826086
Recall: 0.6</strong><br/><strong>F1-Score: 0.6623376623376623</strong></pre>
<p>scikit-learn's <kbd>metrics</kbd> class offers various methods for performance evaluation, for example, accuracy, precision, recall, and F1-score metrics. These methods will take actual target labels (<kbd>target_test</kbd>) and predicted labels (<kbd>predictions</kbd>). We will understand these metrics in detail in the <em>Evaluating the classification model performance</em> section.</p>
<p>Naive Bayes is a simple, fast, accurate, and easy-to-understand method for prediction. It has a lower computation cost and can work with large datasets. Naive Bayes can also be employed in multi-class classification problems. The naive Bayes classifier performs better compared to logistic regression when data has a class independence assumption.</p>
<p>Naive Bayes suffers from the <strong>zero frequency problem</strong>. Zero frequency means that if any category in the feature is missing, then it will have a zero frequency count. This problem is solved by Laplacian correction. Laplacian correction (or Laplace transformation) is a kind of smoothing technique that will add one record for each class so that the frequency count for the missing class will become 1, thus probabilities of Bayes' theorem will not be affected. Another issue with naive Bayes is its assumption of class conditional independence, as it is practically impossible for all the predictors to be fully independent. In this section, we have learned about naive Bayes classification. Now it's time to learn about the decision tree classification algorithm.</p>
<h1 id="uuid-2be93800-95ef-45af-8e05-0775a925391c">Decision tree classification</h1>
<p>A decision tree is one of the most well-known classification techniques. It can be employed for both types of supervised learning problems (classification and regression problems). It is a flowchart-like tree structure and mimics human-level thinking, which makes it easier to understand and interpret. It also makes you see the logic behind the prediction unlike black-box algorithms such as SVMs and neural networks.</p>
<p>The decision tree has three basic components: the internal node, the branch, and leaf nodes. Here, each terminal node represents a feature, the link represents the decision rule or split rule, and the leaf provides the result of the prediction. The first starting or master node in the tree is the root node. It partitions the data based on features or attributes values. Here, we divide the data and again divide the remaining data recursively until all the items refer to the same class or there are no more columns left. Decision trees can be employed in both types of problems: classification and regression. There are lots of decision tree algorithms available, for example, CART, ID3, C4.5, and CHAID. But here, we are mainly focusing on CART and ID3 because in scikit-learn, these are the two that are available. Let's see the decision tree classifier generation process in the following figure:</p>
<div><img src="img/58f77656-6f19-4456-a98b-9595cf104458.png" style=""/></div>
<p><strong>CART</strong> stands for <strong>Classification and Regression Tree</strong>. CART utilizes the Gini index for selecting the best column. The Gini index is the difference between the sum of the squared probabilities of each class from 1. The feature or column with the minimum Gini index value is selected as the splitting or partition feature. The value of the Gini index lies in the range of 0 and 1. If the Gini index value is 0, it indicates that all items belong to one class, and if the Gini index value is exactly 1, it indicates that all the elements are randomly distributed. A 0.5 value of the Gini index indicates the equal distribution of items into some classes:</p>
<div><img class="fm-editor-equation" src="img/92f2aebd-a624-4a83-b54d-b065530bfbff.png" style="width:12.17em;height:4.33em;"/></div>
<p><strong>ID3</strong> stands for <strong>Iterative Dichotomiser 3</strong>. It uses information gain or entropy as an attribute selection measure. Entropy was invented by Shannon, and it measures the amount of impurity or randomness in a dataset. Information gain measures the variations between entropy before partition and mean entropy after the partition of the dataset for a specific column. The feature or attribute with the largest value of information gain will be selected as a splitting feature or attribute. If entropy is 0, it indicates that there exists only a single class, and if entropy is 1, it indicates that items are equally distributed:</p>
<div><img class="fm-editor-equation" src="img/4797209b-540d-4f34-91e9-daf873cbfb9c.png" style="width:21.83em;height:4.33em;"/></div>
<p>The decision tree is very intuitive and easy to understand, interpret, and explain to stakeholders. There is no need to normalize features and distribution-free algorithms. Decision trees are also used to predict missing values. They have the capability to capture non-linear patterns. Decision trees can overfit and are sensitive to noisy data. Decision trees are biased with imbalanced data, which is why before applying decision trees, we should balance out the dataset. Decision trees are more expensive in terms of time and complexity.</p>
<p>Let's work on a decision tree using scikit-learn and perform a prediction dataset. After this, we will be ready for the model building:</p>
<ol>
<li>First, you need to import <kbd>pandas</kbd> and load the Pimas dataset using the <kbd>read_csv()</kbd> method that we already saw in the last section. </li>
</ol>
<ol start="2">
<li>After this, we need to divide the dataset into training and testing datasets similar to what we performed in the preceding section. </li>
</ol>
<ol start="3">
<li>Now, we will build the decision tree classification model:</li>
</ol>
<pre style="padding-left: 60px"># Import Decision Tree model
from sklearn.tree import DecisionTreeClassifier
<br/># Create a Decision Tree classifier object
clf = DecisionTreeClassifier()
<br/># Train the model using training dataset
clf = clf.fit(feature_train,target_train)
<br/># Predict the response for test dataset
predictions = clf.predict(feature_test)</pre>
<p style="padding-left: 60px">Here, we have created a decision tree model. First, we will import the <kbd>DecisionTreeClassifier</kbd> class and create its object or model. This model will fit on the training dataset (<kbd>feature_train</kbd>, <kbd>target_train</kbd>). After training, the model is ready to make predictions using the <kbd>predict()</kbd> method.</p>
<ol start="4">
<li>We will now evaluate the model's performance:</li>
</ol>
<pre style="padding-left: 60px"># Import metrics module for performance evaluation<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.metrics import precision_score<br/>from sklearn.metrics import recall_score<br/>from sklearn.metrics import f1_score
<br/># Calculate model accuracy
print("Accuracy:",accuracy_score(target_test, predictions))
<br/># Calculate model precision
print("Precision:",precision_score(target_test, predictions))
<br/># Calculate model recall
print("Recall:",recall_score(target_test, predictions))
<br/># Calculate model f1 score
print("F1-Score:",f1_score(target_test, predictions))</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px"><strong>Accuracy: 0.7229437229437229
Precision: 0.6438356164383562
Recall: 0.5529411764705883
F1-Score: 0.5949367088607594</strong></pre>
<p>In the preceding example, model performance is assessed using accuracy, precision, recall, and F1-score.</p>
<p>After getting a full understanding of decision trees, let's move on to the KNN classification.</p>
<h1 id="uuid-b04d8864-3962-4fed-a00d-d380ab4cbb12">KNN classification</h1>
<p>KNN is a simple, easy-to-comprehend, and easy-to-implement classification algorithm. It can also be used for regression problems. KNN can be employed in lots of use cases, such as item recommendations and classification problems. Specifically, it can suggest movies on Netflix, articles on Medium, candidates on naukari.com, products on eBay, and videos on YouTube. In classification, it can be used to classify instances such as, for example, banking institutes that can classify the loan of risky candidates, or political scientists can classify potential voters.</p>
<p>KNN has three basic properties, which are non-parametric, lazy learner, and instance-based learning. Non-parametric means the algorithm is distribution-free and there is no need for parameters such as mean and standard deviation. Lazy learner means KNN does not train the model; that is, the model is trained in the testing phase. This makes for faster training but slower testing. It is also more time- and memory-consuming. Instance-based learning means the predicted outcome is based on the similarity with its nearest neighbors. It does not create any abstract equations or rules for prediction; instead, it stores all the data and queries each record.</p>
<p>The KNN classification algorithm finds the <em>k</em> most similar instances from the training dataset and the majority decides the predicted label of the given input features. The following steps will be performed by the KNN classifier to make predictions:</p>
<ol>
<li>Compute the distance for an input observation with all the observations in the training dataset.</li>
<li>Find the <em>K</em> top closest neighbors by sorting the distance with all the instances in ascending order.</li>
<li>Perform voting on the <em>K</em> top closest neighbors and predict the label with the majority of votes.</li>
</ol>
<p style="padding-left: 60px">This is better represented using the following diagram:</p>
<div><img src="img/90a682db-5fe7-44c1-88cb-918b3096f997.png" style=""/></div>
<p>Let's work on a KNN classifier using scikit-learn and perform a prediction on a dataset:</p>
<ol>
<li>Load the Pima Indians Diabetes dataset. </li>
</ol>
<p style="padding-left: 60px">First, you need to import <kbd>pandas</kbd> and load the dataset using the <kbd>read_csv()</kbd> method that we have already seen in the <em>Naive Bayes classification</em> session. </p>
<ol start="2">
<li>Split the dataset.</li>
</ol>
<p style="padding-left: 60px">After this, we need to break down the dataset into two sets – a training and a testing set – as we did in the <em>Naive Bayes classification</em> section. </p>
<ol start="3">
<li>Build the KNN classification model. </li>
</ol>
<p style="padding-left: 60px">Now, we are ready for the model building:</p>
<pre style="padding-left: 60px"># Import KNN model<br/>from sklearn.neighbors import KNeighborsClassifier <br/><br/># Create a KNN classifier object <br/>model = KNeighborsClassifier(n_neighbors=3) <br/><br/># Train the model using the training dataset <br/>model.fit(feature_train,target_train) <br/><br/># Predict the target variable for test dataset <br/>predictions = model.predict(feature_test)</pre>
<p style="padding-left: 60px">In the preceding code block, we imported the <kbd>KNeighborsClassifier</kbd> class and created its object or model. Here, we have taken 3 neighbors as an input parameter to the model. If we do not specify the number of neighbors as an input parameter, then the model will choose 5 neighbors by default. This model will fit on the training dataset (<kbd>feature_train</kbd>, <kbd>target_train</kbd>). After training, the model is ready to make predictions using the <kbd>predict()</kbd> method. </p>
<ol start="4">
<li>Evaluate the model's performance:</li>
</ol>
<pre style="padding-left: 60px"># Import metrics module for performance evaluation<br/>from sklearn.metrics import accuracy_score <br/>from sklearn.metrics import precision_score <br/>from sklearn.metrics import recall_score <br/>from sklearn.metrics import f1_score <br/><br/># Calculate model accuracy <br/>print("Accuracy:",accuracy_score(target_test, predictions)) <br/><br/># Calculate model precision <br/>print("Precision:",precision_score(target_test, predictions)) <br/><br/># Calculate model recall <br/>print("Recall:",recall_score(target_test, predictions)) <br/><br/># Calculate model f1 score <br/>print("F1-Score:",f1_score(target_test, predictions))</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px">Accuracy: 0.7532467532467533
Precision: 0.7058823529411765
Recall: 0.5647058823529412
F1-Score: 0.6274509803921569</pre>
<p>In the preceding example, model performance is assessed using accuracy, precision, recall, and F1-score.</p>
<p>After understanding the KNN classification algorithm, it's time to learn about the SVM classification algorithm.</p>
<h1 id="uuid-ae3a9621-4136-4b8a-bd80-2cb6bf4b196f">SVM classification</h1>
<p>SVMs are the most preferred and favorite machine learning algorithms by many data scientists due to their accuracy with less computation power. They are employed for both regression and classification problems. They also offer a kernel trick to model non-linear relationships. SVM has a variety of use cases, such as intrusion detection, text classification, face detection, and handwriting recognition.</p>
<p>SVM is a discriminative model that generates optimal hyperplanes with a large margin in n-dimensional space to separate data points. The basic idea is to discover the <strong>Maximum Marginal Hyperplane</strong> (<strong>MMH</strong>) that perfectly separates data into given classes. The maximum margin means the maximum distance between data points of both classes.</p>
<h2 id="uuid-0b07f1c0-3502-4513-a411-64355d410f13">Terminology</h2>
<p>We will now explore some of the terminology that goes into SVM classification:</p>
<ul>
<li><strong>Hyperplane</strong>: Hyperplane is a decision boundary used to distinguish between two classes. Hyperplane dimensionality is decided by the number of features. It is also known as a decision plane.</li>
<li><strong>Support</strong> <strong>vectors</strong>: Support vectors are the closest points to the hyperplane and help in the orientation of the hyperplane by maximizing the margin.</li>
<li><strong>Margin</strong>: Margin is the maximum gap between the closest points. The larger the margin, the better the classification is considered. The margin can be calculated by the perpendicular distance from the support vector line.</li>
</ul>
<p>The core objective of an SVM is to choose the hyperplane with the largest possible boundary between support vectors. The SVM finds the MMH in the following two stages:</p>
<ol>
<li>Create hyperplanes that separate the data points in the best possible manner.</li>
<li>Select the hyperplane with maximum margin hyperplane:</li>
</ol>
<div><img src="img/18fb04d2-0af9-45d5-a9b4-5e492df75aa5.png" style=""/></div>
<p>The SVM algorithm is a faster and more accurate classifier compared to naive Bayes. It performs better with a larger margin of separation. SVM is not favorable for large datasets. Its performance also depends upon the type of kernel used. It performs poorly with overlapping classes.</p>
<p>Let's work on support vector classifiers using <kbd>scikit-learn</kbd> and perform a prediction dataset. After this, we will divide the dataset into two sets of training and testing sets as we did in the <em>Naive Bayes classification</em> section. After this, we are ready with the model building:</p>
<ol>
<li>Load the Pima Indians Diabetes dataset. </li>
</ol>
<p style="padding-left: 60px">First, you need to import <kbd>pandas</kbd> and load the dataset using the <kbd>read_csv()</kbd> method that we already saw in the <em>Naive Bayes classification</em> session. </p>
<ol start="2">
<li>Split the dataset.</li>
</ol>
<p style="padding-left: 60px">After this, we need to break the dataset up into two sets – a training and testing set – as we did in the <em>naive Bayes classification</em> section. </p>
<ol start="3">
<li>Build the SVM classification model. </li>
</ol>
<p style="padding-left: 60px">Now, we are ready with the model building:</p>
<pre style="padding-left: 60px"># Import SVM model<br/>from sklearn import svm<br/><br/># Create a SVM classifier object<br/>clf = svm.SVC(kernel='linear')<br/><br/># Train the model using the training sets<br/>clf.fit(feature_train,target_train)<br/><br/># Predict the target variable for test dataset<br/>predictions = clf.predict(feature_test)</pre>
<p style="padding-left: 60px">In the preceding code block, we will import the <kbd>svm</kbd> module and create its <kbd>svm.SVC()</kbd> object or model. Here, we have passed the <kbd>linear</kbd> kernel. You can also pass another kernel, such as <strong><kbd>poly</kbd></strong>,<strong> <kbd>rbf</kbd></strong>, or <kbd>sigmoid</kbd>. If we don't specify the kernel, then it will select <kbd>rbf</kbd> by default as the kernel. The linear kernel will create a linear hyperplane to separate diabetic and non-diabetic patients. This model will fit on the training dataset (<kbd>feature_train</kbd>, <kbd>target_train</kbd>). After training, the model is ready to make predictions using the <kbd>predict()</kbd> method. </p>
<ol start="4">
<li>Evaluate the model's performance:</li>
</ol>
<pre style="padding-left: 60px"># Import metrics module for performance evaluation<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.metrics import precision_score<br/>from sklearn.metrics import recall_score<br/>from sklearn.metrics import f1_score<br/><br/># Calculate model accuracy<br/>print("Accuracy:",accuracy_score(target_test, predictions))<br/><br/># Calculate model precision<br/>print("Precision:",precision_score(target_test, predictions))<br/><br/># Calculate model recall<br/>print("Recall:",recall_score(target_test, predictions))<br/><br/># Calculate model f1 score<br/>print("F1-Score:",f1_score(target_test, predictions))</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px">Accuracy: 0.7835497835497836
Precision: 0.7868852459016393
Recall: 0.5647058823529412
F1-Score: 0.6575342465753424</pre>
<p>In the preceding example, model performance will be assessed using metrics such as accuracy, precision, recall, and F1-score. After understanding all these classifiers, it's time to see the training and testing set splitting strategies. </p>
<h1 id="uuid-5a577fa8-7785-4c23-b2a7-b67c9ce3fed3">Splitting training and testing sets</h1>
<p>Data scientists need to assess the performance of a model, overcome overfitting, and tune the hyperparameters. All these tasks require some hidden data records that were not used in the model development phase. Before model development, the data needs to be divided into some parts, such as train, test, and validation sets. The training dataset is used to build the model. The test dataset is used to assess the performance of a model that was trained on the train set. The validation set is used to find the hyperparameters. Let's look at the following strategies for the train-test split in the upcoming subsections:</p>
<ul>
<li>Holdout method</li>
<li>K-fold cross-validation</li>
<li>Bootstrap method</li>
</ul>
<h2 id="uuid-9210cc1a-b259-4955-96c1-6641d0bd5c5e">Holdout</h2>
<p>In this method, the dataset is divided randomly into two parts: a training and testing set. Generally, this ratio is 2:1, which means 2/3 for training and 1/3 for testing. We can also split it into different ratios, such as 6:4, 7:3, and 8:2:</p>
<pre># partition data into training and testing set<br/>from sklearn.model_selection import train_test_split<br/><br/># split train and test set<br/>feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=1)</pre>
<p>In the preceding example, <kbd>test_size=0.3</kbd> represents 30% for the testing set and 70% for the training set. <kbd>train_test_split()</kbd> splits the dataset into 7:3.</p>
<h2 id="uuid-49be5372-275a-49a8-9a76-31a4410fe5ec">K-fold cross-validation</h2>
<p>In this approach, the data is split into <em>k</em> partitions of approximately equal size. It will train <em>k</em> models and evaluate them using each partition. In each iteration, one partition will hold for testing, and the remaining <em>k</em> partitions are collectively used for training purposes. Classification accuracy will be the average of all accuracies. It also ensures that the model is not overfitting:</p>
<div><img src="img/0203d2e5-87ec-4e32-981f-1a9c10575719.png" style=""/></div>
<p>In stratified cross-validation, <em>k</em> partitions are divided with approximately the same class distribution. This means it preserves the percentages of each class in each partition.</p>
<h2 id="uuid-262ddecf-0ca0-4b4d-8a74-bba158b02943">Bootstrap method</h2>
<p>Bootstrap is a resampling technique. It performs a sampling iteratively from the dataset with replacement. Sampling with replacement will make random selections. It requires the size of the sample and the number of iterations. In each iteration, it uniformly selects the records. Each record has equal chances of being selected again. The samples that are not selected are known as "out-of-bag" samples. Let's understand bootstrap using the following diagram:</p>
<div><img src="img/26693f60-98b4-4383-b4bf-49da50df23f1.png" style=""/></div>
<p>In the preceding diagram, we can see that each element has an equal chance of selection in each bootstrap sample. Let's jump to another important topic of classification, which is classification model evaluation. The next topic helps us to assess the performance of the classification model.</p>
<h1 id="uuid-2987efca-3db3-4c0a-bae0-c3d3c7ce1b53">Evaluating the classification model performance</h1>
<p>Up to now, we have learned how to create classification models. Creating a machine learning classification model is not enough; as a business or data analyst, you also want to assess its performance so that you can deploy it in live projects.</p>
<p>scikit-learn offers various metrics, such as a confusion matrix, accuracy, precision, recall, and F1-score, to evaluate the performance of a model.</p>
<h2 id="uuid-0524f975-2679-4d01-a79b-ba8f59939650">Confusion matrix</h2>
<p>A confusion matrix is an approach that gives a brief statement of prediction results on a binary and multi-class classification problem. Let's assume we have to find out whether a person has diabetes or not. The concept behind the confusion matrix is to find the number of right and mistaken forecasts, which are further summarized and separated into each class. It clarifies all the confusion related to the performance of our classification model. This 2x2 matrix not only shows the error being made by our classifier but also represents what sort of mistakes are being made. A confusion matrix is used to make a complete analysis of statistical data faster and also make the results more readable and understandable through clear data visualization. It contains two rows and columns, as shown in the following list. Let's understand the basic terminologies of the confusion matrix:</p>
<ul>
<li><strong>True Positive</strong> (<strong>TP</strong>): This represents cases that are forecasted as <kbd>Yes</kbd> and in reality, the cases are <kbd>Yes</kbd>; for example, we have forecasted them as fraudulent cases and in reality, they are fraudulent.</li>
<li><strong>True Negative</strong> (<strong>TN</strong>): This represents cases that are forecasted as <kbd>No</kbd> and in reality, the cases are <kbd>No</kbd>; for example, we have forecasted them as non-fraudulent cases and in reality, they are non-fraudulent.</li>
</ul>
<ul>
<li><strong>False Positive</strong> (<strong>FP</strong>): This represents cases that are forecasted as <kbd>Yes</kbd> and in reality, the cases are <kbd>No</kbd>; for example, we have forecasted them as fraudulent cases and in reality, they are not fraudulent. This type of incident class represents a Type I error.</li>
<li><strong>False Negative</strong> (<strong>FN</strong>): This represents cases that are forecasted as <kbd>No</kbd> and in reality, the cases are <kbd>No</kbd>; for example, we have forecasted them as non-fraudulent cases and in reality, they are fraudulent. This type of incident class represents a Type II error.</li>
</ul>
<p>Let's take an example of a fraud detection problem:</p>
<div><img src="img/db102069-a0c6-4c33-ab1a-c02bfef8f809.png" style=""/></div>
<p>In the preceding example, we have taken two classes of fraud: Yes and No. Yes indicates fraudulent activity and No indicates non-fraudulent activity. The total number of predicted records is 825, which means 825 transactions were tested. In all these 825 cases, the model or classifier forecasted 550 times Yes and 275 times No. In reality, actual fraudulent cases are 525 and non-fraudulent cases are 300.</p>
<p>Let's create a confusion matrix in Python using scikit-learn:</p>
<pre># Import libraries<br/>import pandas as pd<br/><br/># read the dataset<br/>diabetes = pd.read_csv("diabetes.csv")<br/><br/># split dataset in two parts: feature set and target label<br/>feature_set = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']<br/>features = diabetes[feature_set]<br/><br/>target = diabetes.label<br/><br/># partition data into training and testing set<br/>from sklearn.model_selection import train_test_split<br/>feature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=1)<br/><br/># import logistic regression scikit-learn model<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score # for performance evaluation<br/><br/># instantiate the model<br/>logreg = LogisticRegression(solver='lbfgs')<br/><br/># fit the model with data<br/>logreg.fit(feature_train,target_train)<br/><br/># Forecast the target variable for given test dataset<br/>predictions = logreg.predict(feature_test)<br/><br/># Get prediction probability <br/>predictions_prob = logreg.predict_proba(feature_test)[::,1]<br/><br/># Import the confusion matrix<br/>from sklearn.metrics import plot_confusion_matrix<br/><br/># Plot Confusion matrix<br/>plot_confusion_matrix(logreg , feature_test, target_test, values_format='d')</pre>
<p class="mce-root">This results in the following output:<br/></p>
<p class="CDPAlignCenter CDPAlign"><img src="img/2d68507c-9909-4826-9122-cc0af782847c.png" style="width:25.67em;height:21.92em;"/></p>
<p>In the preceding example, we have loaded the data and divided the data into two parts: training and testing sets. After this, we performed model training using logistic regression as we did in the previous chapter. Here, to plot the confusion matrix, we have used the <kbd>plot_confusion_matrix()</kbd> method with the model object, testing feature set, testing label set, and <kbd>values_format</kbd> parameters.</p>
<h2 id="uuid-587824c3-7328-4507-a98a-1179cda25959">Accuracy</h2>
<p>Now, we will find the accuracy of the model calculated from the confusion matrix. It tells us how accurate our predictive model is:</p>
<div><img class="fm-editor-equation" src="img/7e728bf1-6dd0-48b2-b96b-f24ff72cb7fb.png" style="width:14.67em;height:3.25em;"/></div>
<p class="CenterAlign"><img class="fm-editor-equation" src="img/8c6821d9-986d-4396-bf62-0b9a4a4ba929.png" style="width:11.92em;height:3.00em;"/> </p>
<h2 id="uuid-ba98bec0-0471-4044-96b5-1706dbadd86f">Precision</h2>
<p>When the model predicted <kbd>Yes</kbd>, how often was it correct? This is the percentage of positive cases out of the total predicted cases in the dataset. In simple terms, we can understand precision as "Up to what level our model is right when it says it's right":</p>
<div><img class="fm-editor-equation" src="img/03e9f997-f47f-44cd-86d2-7f40f66a4e11.png" style="width:23.08em;height:2.58em;"/></div>
<p class="CenterAlign"><img class="fm-editor-equation" src="img/e43eede2-9848-4347-83c5-a83c8acb6087.png" style="width:10.33em;height:2.92em;"/> </p>
<h2 id="uuid-d806be1a-7346-4fbc-a4b2-af27c8e94ceb">Recall</h2>
<p>When it is actually <kbd>Yes</kbd>, how often did the model predict <kbd>Yes</kbd>? This is also known as sensitivity. This is the percentage of positive cases out of all the total actual cases present in the dataset:</p>
<div><img class="fm-editor-equation" src="img/a9bdced7-b936-4517-88b1-f0bf3592e0c0.png" style="width:20.58em;height:2.67em;"/></div>
<div><img class="fm-editor-equation" src="img/ad712a56-df69-4cae-bcf3-6a1044ab7692.png" style="width:9.83em;height:2.75em;"/></div>
<h2 id="uuid-c5fbe310-be73-4b41-b7da-f57f9fca6f0e">F-measure</h2>
<p>F-measure is considered as one of the better ways to assess the model. In lots of areas of data science, competition model performance is assessed using F-measure. It is a harmonic mean of precision and recall. The higher the value of the F1-score, the better the model is considered. F1-score provides equal weightage to precision and recall, which means it indicates a balance between both:</p>
<div><img class="fm-editor-equation" src="img/f3139cdd-3edf-4e89-8059-487ef59d4b49.png" style="width:20.50em;height:2.92em;"/></div>
<div><img class="fm-editor-equation" src="img/6caa7b33-dbe8-4d9d-b552-2054cb9cd1e4.png" style="width:12.08em;height:2.67em;"/></div>
<p>One drawback of F-measure is that it assigns equal weightage to precision and recall but in some examples, one needs to be higher than the other, which is the reason why the F1-score may not be an exact metric.</p>
<p>In the preceding sections, we have seen classification algorithms such as naive Bayes, decision trees, KNN, and SVMs. We have assessed the model performance using scikit-learn's <kbd>accuracy_score()</kbd> for model accuracy, <kbd>precision_score()</kbd> for model precision, <kbd>recall_score()</kbd> for model recall, and <kbd>f1_score()</kbd> for model F1-score.</p>
<p>We can also print the classification report to dig down into the details to understand the classification model. Let's create the confusion report:</p>
<pre># import classification report<br/>from sklearn.metrics import classification_report<br/><br/># Create classification report<br/>print(classification_report(target_test, predictions, target_names=['Yes(1)','No(0)']))<br/><br/></pre>
<p class="mce-root">This results in the following output:</p>
<div><img src="img/dc2c28e2-df40-4598-9a31-ff995bca8173.png" style=""/></div>
<p>In the preceding code, we have printed the confusion matrix report using the <kbd>confusion_report()</kbd> method with test set labels, prediction set or predicted labels, and target value list parameters.</p>
<h1 id="uuid-cb8391de-a645-41da-8ead-f82561a78118">ROC curve and AUC</h1>
<p>AUC-ROC curve is a tool to measure and assess the performance of classification models. <strong>ROC</strong> (<strong>Receiver Operating Characteristics</strong>) is a pictorial visualization of model performance. It plots a two-dimensional probability plot between the FP rate (or 1-specificity) and the TP rate (or sensitivity). We can also represent the area covered by a model with a single number using AUC:</p>
<div><img src="img/2eadc2fe-ce50-4b8f-b875-d0ffe2e3eef1.png" style=""/></div>
<p>Let's create the ROC curve using the scikit-learn module:</p>
<pre># import plot_roc_curve<br/>from sklearn.metrics import plot_roc_curve<br/><br/>plot_roc_curve(logreg , feature_test, target_test)</pre>
<p class="mce-root">This results in the following output:</p>
<div><img src="img/bf293445-39ad-44e0-b00a-236e8908669f.png" style=""/></div>
<p>In the preceding example, We have drawn the ROC plot <kbd>plot_roc_curve()</kbd> method with model object, testing feature set, and testing label set parameters.</p>
<p>In the ROC curve, the AUC is a measure of divisibility. It tells us about the model's class distinction capability. The higher the AUC value, the better the model is at distinguishing between "fraud" and "not fraud." For an ideal classifier, the AUC is equal to 1:</p>
<div><img src="img/6dcb60fc-8600-4651-8ef5-ae66d3c78f63.png" style=""/></div>
<p>Let's compute an AUC score as follows:</p>
<pre># import ROC AUC score<br/>from sklearn.metrics import roc_auc_score<br/><br/># Compute the area under ROC curve<br/>auc = roc_auc_score(target_test, predictions_prob)<br/><br/># Print auc value<br/>print("Area Under Curve:",auc)</pre>
<p>This results in the following output:</p>
<pre>Area Under Curve: 0.8628525382755843</pre>
<p>scikit-learn's <kbd>metrics</kbd> class offers an AUC performance evaluation measure. <kbd>roc_auc_score()</kbd> methods will take actual labels (<kbd>y_test</kbd>) and predicted probability (<kbd>y_pred_prob</kbd>).</p>
<h1 id="uuid-4bb4e6d2-cb1c-4d15-b2a3-850b1bc4a7c2">Summary</h1>
<p>In this chapter, we discovered classification, its techniques, the train-test split strategy, and performance evaluation measures. This will benefit you in gaining an important skill for predictive data analysis. You have seen how to develop linear and non-linear classifiers for predictive analytics using scikit-learn. In the earlier topics of the chapter, you got an understanding of the basics of classification and machine learning algorithms, such as naive Bayes classification, decision tree classification, KNN, and SVMs. In later sections, you saw data splitting approaches and model performance evaluation measures such as accuracy score, precision score, recall score, F1-score, ROC curve, and AUC score.</p>
<p>The next chapter, <a href="cb4ebee8-1420-48f2-ad5d-6e49f241e9e2.xhtml">Chapter 11</a>, <em>Unsupervised Learning – PCA and Clustering</em>, will concentrate on the important topics of unsupervised machine learning techniques and dimensionality reduction techniques in Python. The chapter starts with dimension reduction and principal component analysis. In the later sections of the chapter, the focus will be on clustering methods such as k-means, hierarchical, DBSCAN, and spectral clustering.</p>


            

            
        
    </body></html>