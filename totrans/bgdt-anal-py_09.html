<html><head></head><body>
		<div><h1 id="_idParaDest-203"><em class="italics"><a id="_idTextAnchor230"/>Appendix</em></h1>
		</div>
		<div><div></div>
		</div>
		<div><h2>About</h2>
			<p>This section is included to assist the students in performing the activities in the book. It includes detailed steps that are to be performed by the students to achieve the objectives of the activities.</p>
		</div>
		<div><h2 id="_idParaDest-204"><a id="_idTextAnchor231"/>Chapter 01: The Python Data Science Stack</h2>
			<h3 id="_idParaDest-205"><a id="_idTextAnchor232"/>Activity 1: IPython and Jupyter</h3>
			<ol>
				<li>Open the <code>python_script_student.py</code> file in a text editor, copy the contents to a notebook in IPython, and execute the operations.</li>
				<li>Copy and paste the code from the Python script into a Jupyter notebook:<pre>import numpy as np
def square_plus(x, c):
    return np.power(x, 2) + c</pre></li>
				<li>Now, update the values of the <code>x</code> and <code>c</code> variables. Then, change the definition of the function:<pre>x = 10
c = 100
result = square_plus(x, c)
print(result)</pre><p>The output is as follows:</p><pre>200</pre></li>
			</ol>
			<h3 id="_idParaDest-206"><a id="_idTextAnchor233"/>Activity 2: Working with Data Problems</h3>
			<ol>
				<li value="1">Import pandas and NumPy library:<pre>import pandas as pd
import numpy as np</pre></li>
				<li>Read the RadNet dataset from the U.S. Environmental Protection Agency, available from the Socrata project:<pre>url = "https://opendata.socrata.com/api/views/cf4r-dfwe/rows.csv?accessType=DOWNLOAD"
df = pd.read_csv(url)</pre></li>
				<li>Create a list with numeric columns for radionuclides in the RadNet dataset:<pre>columns = df.columns
id_cols = ['State', 'Location', "Date Posted", 'Date Collected', 'Sample Type', 'Unit']
columns = list(set(columns) - set(id_cols))
columns</pre></li>
				<li>Use the <code>apply</code> method on one column, with a <code>lambda</code> function that compares the <code>Non-detect</code> string:<pre>df['Cs-134'] = df['Cs-134'].apply(lambda x: np.nan if x == "Non-detect" else x)
df.head()</pre><p>The output is as follows:</p><div><img alt="Figure 1.19: DataFrame after applying the lambda function" src="img/C12913_01_19.jpg"/></div><h6>Figure 1.19: DataFrame after applying the lambda function</h6></li>
				<li>Replace the text values with <code>NaN</code> in one column with <code>np.nan</code>:<pre>df.loc[:, columns] = df.loc[:, columns].applymap(lambda x: np.nan if x == 'Non-detect' else x)
df.loc[:, columns] = df.loc[:, columns].applymap(lambda x: np.nan if x == 'ND' else x)</pre></li>
				<li>Use the same lambda comparison and use the <code>applymap</code> method on several columns at the same time, using the list created in the first step:<pre>df.loc[:, ['State', 'Location', 'Sample Type', 'Unit']] = df.loc[:, ['State', 'Location', 'Sample Type', 'Unit']].applymap(lambda x: x.strip())</pre></li>
				<li>Create a list of the remaining columns that are not numeric:<pre>df.dtypes</pre><p>The output is as follows:</p><div><img alt="Figure 1.20: List of columns and their type" src="img/C12913_01_20.jpg"/></div><h6>Figure 1.20: List of columns and their type</h6></li>
				<li>Convert the DataFrame objects into floats using the <code>to_numeric</code> function:<pre>df['Date Posted'] = pd.to_datetime(df['Date Posted'])
df['Date Collected'] = pd.to_datetime(df['Date Collected'])
for col in columns:
    df[col] = pd.to_numeric(df[col])
df.dtypes</pre><p>The output is as follows:</p><div><img alt="Figure 1.21: List of columns and their type" src="img/C12913_01_21.jpg"/></div><h6>Figure 1.21: List of columns and their type</h6></li>
				<li>Using the selection and filtering methods, verify that the names of the string columns don't have any spaces:<pre>df['Date Posted'] = pd.to_datetime(df['Date Posted'])
df['Date Collected'] = pd.to_datetime(df['Date Collected'])
for col in columns:
    df[col] = pd.to_numeric(df[col])
df.dtypes</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img alt="Figure 1.22: DataFrame after applying the selection and filtering method" src="img/C12913_01_22.jpg"/>
				</div>
			</div>
			<h6>Figure 1.22: DataFrame after applying the selection and filtering method</h6>
			<h3 id="_idParaDest-207"><a id="_idTextAnchor234"/>Activity 3: Plotting Data with Pandas</h3>
			<ol>
				<li value="1">Use the RadNet DataFrame that we have been working with.</li>
				<li>Fix all the data type problems, as we saw before.</li>
				<li>Create a plot with a filter per <code>Location</code>, selecting the city of <code>San Bernardino</code>, and one radionuclide, with the <em class="italics">x</em>-axis set to the <code>date</code> and the <em class="italics">y</em>-axis with radionuclide <code>I-131</code>:<pre>df.loc[df.Location == 'San Bernardino'].plot(x='Date Collected', y='I-131')</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_01_23.jpg"/></div><h6>Figure 1.23: Plot of Date collected vs I-131</h6></li>
				<li>Create a scatter plot with the concentration of two related radionuclides, <code>I-131</code> and <code>I-132</code>:<pre>fig, ax = plt.subplots()
ax.scatter(x=df['I-131'], y=df['I-132'])
_ = ax.set(
    xlabel='I-131',
    ylabel='I-132',
    title='Comparison between concentrations of I-131 and I-132'
)</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img alt="Figure 1.24: Plot of concentration of I-131 and I-132" src="img/C12913_01_24.jpg"/>
				</div>
			</div>
			<h6>Figure 1.24: Plot of concentration of I-131 and I-132</h6>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor235"/>Chapter 02: Statistical Visualizations Using Matplotlib and Seaborn</h2>
			<h3 id="_idParaDest-209"><a id="_idTextAnchor236"/>Activity 4: Line Graphs with the Object-Oriented API and Pandas DataFrames</h3>
			<ol>
				<li value="1">Import the required libraries in the Jupyter notebook and read the dataset from the Auto-MPG dataset repository:<pre>%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt import numpy as np
import pandas as pd
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
df = pd.read_csv(url)</pre></li>
				<li>Provide the column names to simplify the dataset, as illustrated here:<pre>column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']</pre></li>
				<li>Now read the new dataset with column names and display it:<pre>df = pd.read_csv(url, names= column_names, delim_whitespace=True)
df.head()</pre><p>The plot is as follows:</p><div><img alt="Figure 2.29: The auto-mpg DataFrame" src="img/C12913_02_13.jpg"/></div><h6>Figure 2.29: The auto-mpg DataFrame</h6></li>
				<li>Convert the <code>horsepower</code> and <code>year</code> data types to float and integer using the following command:<pre>df.loc[df.horsepower == '?', 'horsepower'] = np.nan
df['horsepower'] = pd.to_numeric(df['horsepower'])
df['full_date'] = pd.to_datetime(df.year, format='%y')
df['year'] = df['full_date'].dt.year</pre></li>
				<li>Let's display the data types:<pre>df.dtypes</pre><p>The output is as follows:</p><div><img alt="Figure 2.30: The data types" src="img/C12913_02_30.jpg"/></div><h6>Figure 2.30: The data types</h6></li>
				<li>Now plot the average <code>horsepower</code> per <code>year</code> using the following command:<pre>df.groupby('year')['horsepower'].mean().plot()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img alt="" src="img/C12913_02_31.jpg"/>
				</div>
			</div>
			<h6>Figure 2.31: Line plot</h6>
			<h3 id="_idParaDest-210"><a id="_idTextAnchor237"/>Activity 5: Understanding Relationships of Variables Using Scatter Plots</h3>
			<ol>
				<li value="1">Import the required libraries into the Jupyter notebook and read the dataset from the Auto-MPG dataset repository:<pre>%matplotlib inline
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
df = pd.read_csv(url)</pre></li>
				<li>Provide the column names to simplify the dataset as illustrated here:<pre>column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']</pre></li>
				<li>Now read the new dataset with column names and display it:<pre>df = pd.read_csv(url, names= column_names, delim_whitespace=True)
df.head()</pre><p>The plot is as follows:</p><div><img alt="Figure 2.32: Auto-mpg DataFrame" src="img/C12913_02_13.jpg"/></div><h6>Figure 2.32: Auto-mpg DataFrame</h6></li>
				<li>Now plot the scatter plot using the <code>scatter</code> method:<pre>fig, ax = plt.subplots()
ax.scatter(x = df['horsepower'], y=df['weight'])</pre><p>The output will be as follows:</p></li>
			</ol>
			<div><div><img alt="Figure 2.33: Scatter plot using the scatter method" src="img/Image54488.jpg"/>
				</div>
			</div>
			<h6>Figure 2.33: Scatter plot using the scatter method</h6>
			<h3 id="_idParaDest-211"><a id="_idTextAnchor238"/>Activity 6: Exporting a Graph to a File on Disk</h3>
			<ol>
				<li value="1">Import the required libraries in the Jupyter notebook and read the dataset from the Auto-MPG dataset repository:<pre>%matplotlib inline
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
df = pd.read_csv(url)</pre></li>
				<li>Provide the column names to simplify the dataset, as illustrated here:<pre>column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']</pre></li>
				<li>Now read the new dataset with column names and display it:<pre>df = pd.read_csv(url, names= column_names, delim_whitespace=True)</pre></li>
				<li>Create a bar plot using the following command:<pre>fig, ax = plt.subplots()
df.weight.plot(kind='hist', ax=ax)</pre><p>The output is as follows:</p><div><img alt="Figure 2.34: Bar plot" src="img/C12913_02_34.jpg"/></div><h6>Figure 2.34: Bar plot</h6></li>
				<li>Export it to a PNG file using the <code>savefig</code> function:<pre>fig.savefig('weight_hist.png')</pre></li>
			</ol>
			<h3 id="_idParaDest-212"><a id="_idTextAnchor239"/>Activity 7: Complete Plot Design</h3>
			<ol>
				<li value="1">Import the required libraries in the Jupyter notebook and read the dataset from the Auto-MPG dataset repository:<pre>%matplotlib inline
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
df = pd.read_csv(url)</pre></li>
				<li>Provide the column names to simplify the dataset, as illustrated here:<pre>column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']</pre></li>
				<li>Now read the new dataset with column names and display it:<pre>df = pd.read_csv(url, names= column_names, delim_whitespace=True)</pre></li>
				<li>Perform GroupBy on <code>year</code> and <code>cylinders</code>, and unset the option to use them as indexes:<pre>df_g = df.groupby(['year', 'cylinders'], as_index=False)</pre></li>
				<li>Calculate the average miles per gallon over the grouping and set <code>year</code> as index:<pre>df_g = df_g.mpg.mean()</pre></li>
				<li>Set year as the DataFrame index:<pre>df_g = df_g.set_index(df_g.year)</pre></li>
				<li>Create the figure and axes using the object-oriented API:<pre>import matplotlib.pyplot as plt
fig, axes = plt.subplots()</pre></li>
				<li>Group the <code>df_g</code> dataset by <code>cylinders</code> and plot the miles per gallon variable using the axes created with size (<code>10</code>,<code>8</code>):<pre>df = df.convert_objects(convert_numeric=True)
df_g = df.groupby(['year', 'cylinders'], as_index=False).horsepower.mean()
df_g = df_g.set_index(df_g.year)</pre></li>
				<li>Set the <strong class="bold">title</strong>, <strong class="bold">x</strong> label, and <strong class="bold">y</strong> label on the axes:<pre>fig, axes = plt.subplots()
df_g.groupby('cylinders').horsepower.plot(axes=axes, figsize=(12,10))
_ = axes.set(
    title="Average car power per year",
    xlabel="Year",
    ylabel="Power (horsepower)"
    
)</pre><p>The output is as follows:</p><div><img alt="" src="img/Image54506.jpg"/></div><h6>Figure 2.35: Line plot for average car power per year (without legends)</h6></li>
				<li>Include legends, as follows:<pre>axes.legend(title='Cylinders', fancybox=True)</pre><div><img alt="" src="img/Image54515.jpg"/></div><h6>Figure 2.36: Line plot for average car power per year (with legends)</h6></li>
				<li>Save the figure to disk as a PNG file:<pre>fig.savefig('mpg_cylinder_year.png')</pre></li>
			</ol>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor240"/>Chapter 03: Working with Big Data Frameworks</h2>
			<h3 id="_idParaDest-214"><a id="_idTextAnchor241"/>Activity 8: Parsing Text</h3>
			<ol>
				<li value="1">Read the text files into the Spark object using the <code>text</code> method:<pre>rdd_df = spark.read.text("/localdata/myfile.txt").rdd</pre><p>To parse the file that we are reading, we will use lambda functions and Spark operations such as <code>map</code>, <code>flatMap</code>, and <code>reduceByKey</code>. <code>flatmap</code> applies a function to all elements of an RDD, flattens the results, and returns the transformed RDD. <code>reduceByKey</code> merges the values based on the given key, combining the values. With these functions, we can count the number of lines and words in the text.</p></li>
				<li>Extract the <code>lines</code> from the text using the following command:<pre>lines = rdd_df.map(lambda line: line[0])</pre></li>
				<li>This splits each line in the file as an entry in the list. To check the result, you can use the <code>collect</code> method, which gathers all data back to the driver process:<pre>lines.collect()</pre></li>
				<li>Now, let's count the number of lines, using the <code>count</code> method:<pre>lines.count()</pre><h4>Note</h4><p class="callout">Be careful when using the <code>collect</code> method! If the DataFrame or RDD being collected is larger than the memory of the local driver, Spark will throw an error.</p></li>
				<li>Now, let's first split each line into words, breaking it by the space around it, and combining all elements, removing words in uppercase:<pre>splits = lines.flatMap(lambda x: x.split(' '))
lower_splits = splits.map(lambda x: x.lower())</pre></li>
				<li>Let's also remove the <em class="italics">stop words</em>. We could use a more consistent stop words list from <strong class="bold">NLTK</strong>, but for now, we will row our own:<pre>stop_words = ['of', 'a', 'and', 'to']</pre></li>
				<li>Use the following command to remove the stop words from our token list:<pre>tokens = lower_splits.filter(lambda x: x and x not in stop_words)</pre><p>We can now process our token list and count the unique words. The idea is to generate a list of tuples, where the first element is the <code>token</code> and the second element is the <code>count</code> of that particular token.</p></li>
				<li>First, let's <code>map</code> our token to a list:<pre>token_list = tokens.map(lambda x: [x, 1])</pre></li>
				<li>Use the <code>reduceByKey</code> operation, which will apply the operation to each of the lists:<pre>count = token_list.reduceByKey(add).sortBy(lambda x: x[1], ascending=False)
count.collect()</pre></li>
			</ol>
			<p>Remember, collect all data back to the driver node! Always check whether there is enough memory by using tools such as <code>top</code> and <code>htop</code>.</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor242"/>Chapter 04: Diving Deeper with Spark</h2>
			<h3 id="_idParaDest-216"><a id="_idTextAnchor243"/>Activity 9: Getting Started with Spark DataFrames</h3>
			<p>If you are using Google Collab to run the Jupyter notebook, add these lines to ensure you have set the environment:</p>
			<pre>!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null
!wget -q http://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz
!tar xf spark-2.4.0-bin-hadoop2.7.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.2-bin-hadoop2.7"</pre>
			<p>Install findspark if not installed using the following command:</p>
			<pre>pip install -q findspark</pre>
			<ol>
				<li value="1">To create a sample DataFrame by manually specifying the schema, importing findspark module to connect Jupyter with Spark:<pre>import findspark
findspark.init()
import pyspark
import os</pre></li>
				<li>Create the <code>SparkContext</code> and <code>SQLContext</code> using the following command:<pre>sc = pyspark.SparkContext()
from pyspark.sql import SQLContext
sqlc = SQLContext(sc)
from pyspark.sql import *
na_schema = Row("Name","Subject","Marks")
row1 = na_schema("Ankit", "Science",95)
row2 = na_schema("Ankit", "Maths", 86)
row3 = na_schema("Preity", "Maths", 92)
na_list = [row1, row2, row3]
df_na = sqlc.createDataFrame(na_list)
type(df_na)</pre><p>The output is as follows:</p><pre>pyspark.sql.dataframe.DataFrame</pre></li>
				<li>Check the DataFrame using the following command:<pre>df_na.show()</pre><p>The output is as follows:</p><div><img alt="Figure 4.29: Sample DataFrame" src="img/C12913_04_29.jpg"/></div><h6>Figure 4.29: Sample DataFrame</h6></li>
				<li>Create a sample DataFrame from an existing RDD. First creating RDD as illustrated here:<pre>data = [("Ankit","Science",95),("Preity","Maths",86),("Ankit","Maths",86)]
data_rdd = sc.parallelize(data)
type(data_rdd)</pre><p>The output is as follows:</p><pre>pyspark.rdd.RDD</pre></li>
				<li>Converting RDD to DataFrame using the following command:<pre>data_df = sqlc.createDataFrame(data_rdd)
data_df.show()</pre><p>The output is as follows:</p><div><img alt="Figure 4.30: RDD to DataFrame" src="img/Image54539.jpg"/></div><h6>Figure 4.30: RDD to DataFrame</h6></li>
				<li>Create a sample DataFrame by reading the data from a CSV file:<pre>df = sqlc.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('mtcars.csv')
type(df)</pre><p>The output is as follows:</p><pre>pyspark.sql.dataframe.DataFrame</pre></li>
				<li>Print first seven rows of the DataFrame:<pre>df.show(7)</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_31.jpg"/></div><h6>Figure 4.31: First seven rows of the DataFrame</h6></li>
				<li>Print the schema of the DataFrame:<pre>df.printSchema()</pre></li>
				<li>The output is as follows:<div><img alt="" src="img/C12913_04_32.jpg"/></div><h6>Figure 4.32: Schema of the DataFrame</h6></li>
				<li>Print the number of columns and rows in DataFrame:<pre>print('number of rows:'+ str(df.count()))
print('number of columns:'+ str(len(df.columns)))</pre><p>The output is as follows:</p><pre>number of rows:32
number of columns:11</pre></li>
				<li>Print the summary statistics of DataFrame and any two individual columns:<pre>df.describe().show()</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_33.jpg"/></div><h6>Figure 4.33: Summary statistics of DataFrame</h6><p>Print the summary of any two columns:</p><pre>df.describe(['mpg','cyl']).show()</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_34.jpg"/></div><h6>Figure 4.34: Summary statistics of mpg and cyl columns</h6></li>
				<li>Write first seen rows of the sample DataFrame in a CSV file:<pre>df_p = df.toPandas()
df_p.head(7).to_csv("mtcars_head.csv")</pre></li>
			</ol>
			<h3 id="_idParaDest-217"><a id="_idTextAnchor244"/>Activity 10: Data Manipulation with Spark DataFrames</h3>
			<ol>
				<li value="1">Install the packages as illustrated here:<pre>!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null
!wget -q http://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz
!tar xf spark-2.4.0-bin-hadoop2.7.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.0-bin-hadoop2.7"</pre></li>
				<li>Then, import the <code>findspark</code> module to connect the Jupyter with Spark use the following command:<pre>import findspark
findspark.init()
import pyspark
import os</pre></li>
				<li>Now, create the <code>SparkContext</code> and <code>SQLContext</code> as illustrated here:<pre>sc = pyspark.SparkContext()
from pyspark.sql import SQLContext
sqlc = SQLContext(sc)</pre></li>
				<li>Create a DataFrame in Spark as illustrated here:<pre>df = sqlc.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('mtcars.csv')
df.show(4)</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_35.jpg"/></div><h6>Figure 4.35: DataFrame in Spark</h6></li>
				<li>Rename any five columns of DataFrame using the following command:<pre>data = df
new_names = ['mpg_new', 'cyl_new', 'disp_new', 'hp_new', 'drat_new']
for i,z in zip(data.columns[0:5],new_names):
    data = data.withColumnRenamed(str(i),str(z))
    
data.columns</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_36.jpg"/></div><h6>Figure 4.36: Columns of DataFrame</h6></li>
				<li>Select any two numeric and one categorical column from the DataFrame:<pre>data = df.select(['cyl','mpg','hp'])
data.show(5)</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_37.jpg"/></div><h6>Figure 4.37: Two numeric and one categorical column from the DataFrame</h6></li>
				<li>Count the number of distinct categories in the categorical variable:<pre>data.select('cyl').distinct().count() #3</pre></li>
				<li>Create two new columns in DataFrame by summing up and multiplying together the two numerical columns:<pre>data = data.withColumn('colsum',(df['mpg'] + df['hp']))
data = data.withColumn('colproduct',(df['mpg'] * df['hp']))
data.show(5)</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_38.jpg"/></div><h6>Figure 4.38: New columns in DataFrame</h6></li>
				<li>Drop both the original numerical columns:<pre>data = data.drop('mpg','hp')
data.show(5)</pre><div><img alt="" src="img/C12913_04_39.jpg"/></div><h6>Figure 4.39: New columns in DataFrame after dropping</h6></li>
				<li>Sort the data by categorical column:<pre>data = data.orderBy(data.cyl)
data.show(5)</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_40.jpg"/></div><h6>Figure 4.40: Sort data by categorical columns</h6></li>
				<li>Calculate the <code>mean</code> of the summation column for each distinct category in the <code>categorical</code> variable:<pre>data.groupby('cyl').agg({'colsum':'mean'}).show()</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_41.jpg"/></div><h6>Figure 4.41: Mean of the summation column</h6></li>
				<li>Filter the rows with values greater than the <code>mean</code> of all the <code>mean</code> values calculated in the previous step:<pre>data.count()#15
cyl_avg = data.groupby('cyl').agg({'colsum':'mean'})
avg = cyl_avg.agg({'avg(colsum)':'mean'}).toPandas().iloc[0,0]
data = data.filter(data.colsum &gt; avg)
data.count()
data.show(5)</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_42.jpg"/></div><h6>Figure 4.42: Mean of all the mean values calculated of the summation column</h6></li>
				<li>De-duplicate the resultant DataFrame to make sure it has all unique records:<pre>data = data.dropDuplicates()
data.count()</pre><p>The output is <code>15</code>.</p></li>
			</ol>
			<h3 id="_idParaDest-218"><a id="_idTextAnchor245"/>Activity 11: Graphs in Spark</h3>
			<ol>
				<li value="1">Import the required Python libraries in the Jupyter Notebook:<pre>import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline</pre></li>
				<li>Read and show the data from the CSV file using the following command:<pre>df = pd.read_csv('mtcars.csv')
df.head()</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_43.jpg"/></div><h6>Figure 4.43: Auto-mpg DataFrame</h6></li>
				<li>Visualize the discrete frequency distribution of any continuous numeric variable from your dataset using a histogram:<pre>plt.hist(df['mpg'], bins=20)
plt.ylabel('Frequency')
plt.xlabel('Values')
plt.title('Frequency distribution of mpg')
plt.show()</pre><p>The output is as follows:</p><div><img alt="Figure 4.44: Discrete frequency distribution histogram" src="img/C12913_04_44.jpg"/></div><h6>Figure 4.44: Discrete frequency distribution histogram</h6></li>
				<li>Visualize the percentage share of the categories in the dataset using a pie chart:<pre>## Calculate count of records for each gear
data = pd.DataFrame([[3,4,5],df['gear'].value_counts().tolist()]).T
data.columns = ['gear','gear_counts']
## Visualising percentage contribution of each gear in data using pie chart
plt.pie(data.gear_counts, labels=data.gear, startangle=90, autopct='%.1f%%')
plt.title('Percentage contribution of each gear')
plt.show()</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_04_45.jpg"/></div><h6>Figure 4.45: Percentage share of the categories using pie chart</h6></li>
				<li>Plot the distribution of a continuous variable across the categories of a categorical variable using a boxplot:<pre>sns.boxplot(x = 'gear', y = 'disp', data = df)
plt.show()</pre><p>The output is as follows:</p><div><img alt="Figure 4.46: Distribution of a continuous using boxplot" src="img/C12913_04_46.jpg"/></div><h6>Figure 4.46: Distribution of a continuous using boxplot</h6></li>
				<li>Visualize the values of a continuous numeric variable using a line chart:<pre>data = df[['hp']]
data.plot(linestyle='-')
plt.title('Line Chart for hp')
plt.ylabel('Values')
plt.xlabel('Row number')
plt.show()</pre><p>The output is as follows:</p><div><img alt="Figure 4.47: Continuous numeric variable using a line chart" src="img/Image54696.jpg"/></div><h6>Figure 4.47: Continuous numeric variable using a line chart</h6></li>
				<li>Plot the values of multiple continuous numeric variables on the same line chart:<pre>data = df[['hp','disp', 'mpg']]
data.plot(linestyle='-')
plt.title('Line Chart for hp, disp &amp; mpg')
plt.ylabel('Values')
plt.xlabel('Row number')
plt.show()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img alt="Figure 4.48: Multiple continuous numeric variables" src="img/C12913_04_48.jpg"/>
				</div>
			</div>
			<h6>Figure 4.48: Multiple continuous numeric variables</h6>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor246"/>Chapter 05: Missing Value Handling and Correlation Analysis in Spark</h2>
			<h3 id="_idParaDest-220"><a id="_idTextAnchor247"/>Activity 12: Missing Value Handling and Correlation Analysis with PySpark DataFrames</h3>
			<ol>
				<li value="1">Import the required libraries and modules in the Jupyter notebook, as illustrated here:<pre>import findspark
findspark.init()
import pyspark
import random</pre></li>
				<li>Set up the <code>SparkContext</code> with the help of the following command in the Jupyter notebook:<pre>sc = pyspark.SparkContext(appName = "chapter5")</pre></li>
				<li>Similarly, set up the <code>SQLContext</code> in the notebook:<pre>from pyspark.sql import SQLContext
sqlc = SQLContext(sc)</pre></li>
				<li>Now, read the CSV data into a Spark object using the following command:<pre>df = sqlc.read.format('com.databricks.spark.csv').options(header = 'true', inferschema = 'true').load('iris.csv')
df.show(5)</pre><p>The output is as follows:</p><div><img alt="Figure 5.14: Iris DataFrame, reading the CSV data into a Spark object" src="img/C12913_05_14.jpg"/></div><h6>Figure 5.14: Iris DataFrame, reading the CSV data into a Spark object</h6></li>
				<li>Fill in the missing values in the <code>Sepallength</code> column with the column's mean.</li>
				<li>First, calculate the mean of the <code>Sepallength</code> column using the following command:<pre>from pyspark.sql.functions import mean
avg_sl = df.select(mean('Sepallength')).toPandas()['avg(Sepallength)']</pre></li>
				<li>Now, impute the missing values in the <code>Sepallength</code> column with the column's mean, as illustrated here:<pre>y = df
y = y.na.fill(float(avg_sl),['Sepallength'])
y.describe().show(1)</pre><p>The output is as follows:</p><div><img alt="Figure 5.15: Iris DataFrame" src="img/C12913_05_15.jpg"/></div><h6>Figure 5.15: Iris DataFrame</h6></li>
				<li>Compute the correlation matrix for the dataset. Make sure to import the required modules, as shown here:<pre>from pyspark.mllib.stat import Statistics
import pandas as pd</pre></li>
				<li>Now, fill the missing values in the DataFrame before computing the correlation:<pre>z = y.fillna(1)</pre></li>
				<li>Next, remove the <code>String</code> columns from the PySpark DataFrame, as illustrated here:<pre>a = z.drop('Species') 
features = a.rdd.map(lambda row: row[0:])</pre></li>
				<li>Now, compute the correlation matrix in Spark:<pre>correlation_matrix = Statistics.corr(features, method="pearson")</pre></li>
				<li>Next, convert the correlation matrix into a pandas DataFrame using the following command:<pre>correlation_df = pd.DataFrame(correlation_matrix)
correlation_df.index, correlation_df.columns = a.columns, a.columns
correlation_df</pre><p>The output is as follows:</p><div><img alt="Figure 5.16: Convert the correlation matrix into a pandas DataFrame" src="img/Image54735.jpg"/></div><h6>Figure 5.16: Convert the correlation matrix into a pandas DataFrame</h6></li>
				<li>Plot the variable pairs showing strong positive correlation and fit a linear line on them.</li>
				<li>First, load the data from the Spark DataFrame into a pandas DataFrame:<pre>import pandas as pd
dat = y.toPandas()
type(dat)</pre><p>The output is as follows:</p><pre>pandas.core.frame.DataFrame</pre></li>
				<li>Next, load the required modules and plotting data using the following commands:<pre>import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
sns.lmplot(x = "Sepallength", y = "Petallength", data = dat)
plt.show()</pre><p>The output is as follows:</p><div><img alt="Figure 5.17: Seaborn plot for x = “Sepallength”, y = “Petallength”" src="img/Image54744.jpg"/></div><h6>Figure 5.17: Seaborn plot for x = "Sepallength", y = "Petallength"</h6></li>
				<li>Plot the graph so that <code>x</code> equals <code>Sepallength</code>, and <code>y</code> equals <code>Petalwidth</code>:<pre>import seaborn as sns
sns.lmplot(x = "Sepallength", y = "Petalwidth", data = dat)
plt.show()</pre><p>The output is as follows:</p><div><img alt="Figure 5.18: Seaborn plot for x = “Sepallength”, y = “Petalwidth”" src="img/Image54754.jpg"/></div><h6>Figure 5.18: Seaborn plot for x = "Sepallength", y = "Petalwidth"</h6></li>
				<li>Plot the graph so that <code>x</code> equals <code>Petalwidth</code> and <code>y</code> equals <code>Petalwidth</code>:<pre>sns.lmplot(x = "Petallength", y = "Petalwidth", data = dat)
plt.show()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img alt="Figure 5.19: Seaborn plot for x = “Petallength”, y = “Petalwidth”" src="img/C12913_05_19.jpg"/>
				</div>
			</div>
			<h6>Figure 5.19: Seaborn plot for x = "Petallength", y = "Petalwidth"</h6>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor248"/>Chapter 6: Business Process Definition and Exploratory Data Analysis</h2>
			<h3 id="_idParaDest-222"><a id="_idTextAnchor249"/>Activity 13: Carry Out Mapping to Gaussian Distribution of Numeric Features from the Given Data</h3>
			<ol>
				<li value="1">Download the <code>bank.csv</code>. Now, use the following commands to read the data from it:<pre>import numpy as np
import pandas as pd
import seaborn as sns
import time
import re
import os
import matplotlib.pyplot as plt
sns.set(style="ticks")
# import libraries required for preprocessing
import sklearn as sk
from scipy import stats
from sklearn import preprocessing
# set the working directory to the following
os.chdir("/Users/svk/Desktop/packt_exercises")
# read the downloaded input data (marketing data)
df = pd.read_csv('bank.csv', sep=';')</pre></li>
				<li>Identify the numeric data from the DataFrame. The data can be categorized according to its type, such as categorical, numeric (float, integer), date, and so on. We identify numeric data here because we can only carry out normalization on numeric data:<pre>numeric_df = df._get_numeric_data()
numeric_df.head()</pre><p>The output is as follows:</p><div><img alt="Figure 6.12: DataFrame" src="img/C12913_06_12.jpg"/></div><h6>Figure 6.12: DataFrame</h6></li>
				<li>Carry out a normality test and identify the features that have a non-normal distribution:<pre>numeric_df_array = np.array(numeric_df) # converting to numpy arrays for more efficient computation
loop_c = -1
col_for_normalization = list()
for column in numeric_df_array.T:
    loop_c+=1
    x = column
    k2, p = stats.normaltest(x) 
    alpha = 0.001
    print("p = {:g}".format(p))
        
    # rules for printing the normality output
    if p &lt; alpha:
        test_result = "non_normal_distr"
        col_for_normalization.append((loop_c)) # applicable if yeo-johnson is used
        
        #if min(x) &gt; 0: # applicable if box-cox is used
            #col_for_normalization.append((loop_c)) # applicable if box-cox is used
        print("The null hypothesis can be rejected: non-normal distribution")
        
    else:
        test_result = "normal_distr"
        print("The null hypothesis cannot be rejected: normal distribution")</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_06_13.jpg"/></div><h6>Figure 6.13: Normality test and identify the features</h6><h4>Note</h4><p class="callout">The normality test conducted here is based on D'Agostino and Pearson's test (<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html</a>), which combines skew and kurtosis to identify how close the distribution of the features is to a Gaussian distribution. In this test, if the p-value is less than the set alpha value, then the null hypothesis is rejected, and the feature does not have a normal distribution. Here, we look into each column using a loop function and identify the distribution of each feature.</p></li>
				<li>Plot the probability density of the features to visually analyze their distribution:<pre>columns_to_normalize = numeric_df[numeric_df.columns[col_for_normalization]]
names_col = list(columns_to_normalize)
# density plots of the features to check the normality
columns_to_normalize.plot.kde(bw_method=3)</pre><p>The density plot of the features to check the normality is as follows:</p><div><img alt="Figure 6.14: Plot of features" src="img/C12913_06_14.jpg"/></div><h6>Figure 6.14: Plot of features</h6><h4>Note</h4><p class="callout">Multiple variables' density plots are shown in the previous graph. The distribution of the features in the graph can be seen with a high positive kurtosis, which is not a normal distribution.</p></li>
				<li>Prepare the power transformation model and carry out transformations on the identified features to convert them to normal distribution based on the <code>box-cox</code> or <code>yeo-johnson</code> method:<pre>pt = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True, copy=True)
normalized_columns = pt.fit_transform(columns_to_normalize)
normalized_columns = pd.DataFrame(normalized_columns, columns=names_col)</pre><p>In the previous commands, we prepare the power transformation model and apply it to the data of selected features.</p></li>
				<li>Plot the probability density of the features again after the transformations to visually analyze the distribution of the features:<pre>normalized_columns.plot.kde(bw_method=3)</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img alt="Figure 6.15: Plot of features" src="img/C12913_06_15.jpg"/>
				</div>
			</div>
			<h6>Figure 6.15: Plot of features</h6>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor250"/>Chapter 07: Reproducibility in Big Data Analysis</h2>
			<h3 id="_idParaDest-224"><a id="_idTextAnchor251"/>Activity 14: Test normality of data attributes (columns) and carry out Gaussian normalization of non-normally distributed attributes</h3>
			<ol>
				<li value="1">Import the required libraries and packages in the Jupyter notebook:<pre>import numpy as np
import pandas as pd
import seaborn as sns
import time
import re
import os
import matplotlib.pyplot as plt
sns.set(style="ticks")</pre></li>
				<li>Now, import the libraries required for preprocessing:<pre>import sklearn as sk
from scipy import stats
from sklearn import preprocessing</pre></li>
				<li>Set the working directory using the following command:<pre>os.chdir("/Users/svk/Desktop/packt_exercises")</pre></li>
				<li>Now, import the dataset into the Spark object:<pre>df = pd.read_csv('bank.csv', sep=';')</pre></li>
				<li>Identify the target variable in the data:<pre>DV = 'y'
df[DV]= df[DV].astype('category')
df[DV] = df[DV].cat.codes</pre></li>
				<li>Generate training and testing data using the following command:<pre>msk = np.random.rand(len(df)) &lt; 0.8
train = df[msk]
test = df[~msk]</pre></li>
				<li>Create the <code>Y</code> and <code>X</code> data, as illustrated here:<pre># selecting the target variable (dependent variable) as y
y_train = train[DV]</pre></li>
				<li>Drop the <code>DV</code> or <code>y</code> using the <code>drop</code> command:<pre>train = train.drop(columns=[DV])
train.head()</pre><p>The output is as follows:</p><div><img alt="Figure 7.22: Bank dataset" src="img/C12913_07_22.jpg"/></div><h6>Figure 7.22: Bank dataset</h6></li>
				<li>Segment the data numerically and categorically and perform distribution transformation on the numeric data:<pre>numeric_df = train._get_numeric_data()</pre><p>Perform data preprocessing on the data.</p></li>
				<li>Now, create a <code>loop</code> to identify the columns with a non-normal distribution using the following command (converting to NumPy arrays for more efficient computation):<pre>numeric_df_array = np.array(numeric_df)
loop_c = -1
col_for_normalization = list()
for column in numeric_df_array.T:
    loop_c+=1
    x = column
    k2, p = stats.normaltest(x) 
    alpha = 0.001
    print("p = {:g}".format(p))
        
    # rules for printing the normality output
    if p &lt; alpha:
        test_result = "non_normal_distr"
        col_for_normalization.append((loop_c)) # applicable if yeo-johnson is used
        
        #if min(x) &gt; 0: # applicable if box-cox is used
            #col_for_normalization.append((loop_c)) # applicable if box-cox is used
        print("The null hypothesis can be rejected: non-normal distribution")
        
    else:
        test_result = "normal_distr"
        print("The null hypothesis cannot be rejected: normal distribution")</pre><p>The output is as follows:</p><div><img alt="Figure 7.23: Identifying the columns with a non-linear distribution" src="img/C12913_07_23.jpg"/></div><h6>Figure 7.23: Identifying the columns with a non-linear distribution</h6></li>
				<li>Create a <code>PowerTransformer</code> based transformation (<code>box-cox</code>):<pre>pt = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True, copy=True)</pre><h4>Note</h4><p class="callout"><code>box-cox</code> can handle only positive values.</p></li>
				<li>Apply the power transformation model on the data. Select the columns to normalize:<pre>columns_to_normalize = numeric_df[numeric_df.columns[col_for_normalization]]
names_col = list(columns_to_normalize)</pre></li>
				<li>Create a density plot to check the normality:<pre>columns_to_normalize.plot.kde(bw_method=3)</pre><p>The output is as follows:</p><div><img alt="Figure 7.24: Density plot to check the normality" src="img/Image54824.jpg"/></div><h6>Figure 7.24: Density plot to check the normality</h6></li>
				<li>Now, transform the columns to a normal distribution using the following command:<pre>normalized_columns = pt.fit_transform(columns_to_normalize)
normalized_columns = pd.DataFrame(normalized_columns, columns=names_col)</pre></li>
				<li>Again, create a density plot to check the normality:<pre>normalized_columns.plot.kde(bw_method=3)</pre><p>The output is as follows:</p><div><img alt="Figure 7.25: Another density plot to check the normality" src="img/Image54838.jpg"/></div><h6>Figure 7.25: Another density plot to check the normality</h6></li>
				<li>Use a <code>loop</code> to identify the columns with non-normal distribution on the transformed data:<pre>numeric_df_array = np.array(normalized_columns) 
loop_c = -1
for column in numeric_df_array.T:
    loop_c+=1
    x = column
    k2, p = stats.normaltest(x) 
    alpha = 0.001
    print("p = {:g}".format(p))
        
    # rules for printing the normality output
    if p &lt; alpha:
        test_result = "non_normal_distr"
        print("The null hypothesis can be rejected: non-normal distribution")
        
    else:
        test_result = "normal_distr"
        print("The null hypothesis cannot be rejected: normal distribution")</pre><p>The output is as follows:</p><div><img alt="Figure 7.26: Power transformation model to data" src="img/Image54847.jpg"/></div><h6>Figure 7.26: Power transformation model to data</h6></li>
				<li>Bind the normalized and non-normalized columns. Select the columns not to normalize:<pre>columns_to_notnormalize = numeric_df
columns_to_notnormalize.drop(columns_to_notnormalize.columns[col_for_normalization], axis=1, inplace=True)</pre></li>
				<li>Use the following command to bind both the non-normalized and normalized columns:<pre>numeric_df_normalized = pd.concat([columns_to_notnormalize.reset_index(drop=True), normalized_columns], axis=1)
numeric_df_normalized</pre></li>
			</ol>
			<div><div><img alt="Figure 7.27: Non-normalized and normalized columns" src="img/C12913_07_27.jpg"/>
				</div>
			</div>
			<h6>Figure 7.27: Non-normalized and normalized columns</h6>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor252"/>Chapter 08: Creating a Full Analysis Report</h2>
			<h3 id="_idParaDest-226"><a id="_idTextAnchor253"/>Activity 15: Generating Visualization Using Plotly</h3>
			<ol>
				<li value="1">Import all the required libraries and packages into the Jupyter notebook. Make sure to read the data from <code>bank.csv</code> into the Spark DataFrame.</li>
				<li>Import the libraries for Plotly, as illustrated here:<pre>import plotly.graph_objs as go
from plotly.plotly import iplot
import plotly as py</pre></li>
				<li>Now, for visualization in Plotly, we need to initiate an offline session. Use the following command (requires version &gt;= 1.9.0):<pre>from plotly import __version__
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
print(__version__)</pre></li>
				<li>Now Plotly is initiated offline. Use the following command to start a Plotly notebook:<pre>import plotly.plotly as py
import plotly.graph_objs as go
init_notebook_mode(connected=True)</pre><p>After starting the Plotly notebook, we can use Plotly to generate many types of graphs, such as a bar graph, a boxplot, or a scatter plot, and convert the entire output into a user interface or an app that is supported by Python's Flask framework.</p></li>
				<li>Now, plot each graph using Plotly:<p>Bar graph:</p><pre>df = pd.read_csv('bank.csv', sep=';')
data = [go.Bar(x=df.y,
            y=df.balance)]
py.iplot(data)</pre><p>The bar graph is as follows:</p><div><img alt="" src="img/C12913_08_18.jpg"/></div></li>
			</ol>
			<h6>Figure 8.18: Bar graph</h6>
			<p>Scatter plot:</p>
			<pre>py.iplot([go.Histogram2dContour(x=df.balance, y=df.age, contours=dict(coloring='heatmap')),
       go.Scatter(x=df.balance, y=df.age, mode='markers', marker=dict(color='red', size=8, opacity=0.3))], show_link=False)</pre>
			<p>The scatter plot is as follows:</p>
			<div><div><img alt="Figure 8.19: Scatter plot" src="img/C12913_08_19.jpg"/>
				</div>
			</div>
			<h6>Figure 8.19: Scatter plot</h6>
			<p>Boxplot:</p>
			<pre>plot1 = go.Box(
    y=df.age,
    name = 'age of the customers',
    marker = dict(
        color = 'rgb(12, 12, 140)',
    )
)
py.iplot([plot1])</pre>
			<p>The boxplot is as follows:</p>
			<div><div><img alt="Figure 8.20: Boxplot" src="img/C12913_08_20.jpg"/>
				</div>
			</div>
			<h6>Figure 8.20: Boxplot</h6>
		</div>
	</body></html>