- en: '*Chapter 7*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reproducibility in Big Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the concept of reproducibility with Jupyter notebooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform data gathering in a reproducible way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement suitable code practices and standards to keep analysis reproducible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid the duplication of work with IPython scripts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will discover how reproducibility plays a vital role in
    big data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to define a business problem from a
    data science perspective through a very structured approach, which included how
    to identify and understand business requirements, an approach to solutioning it,
    and how to build data pipelines and carry out analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at the reproducibility of computational work and
    research practices, which is a major challenge faced today across the industry,
    as well as by academics—especially in data science work, in which most of the
    data, complete datasets, and associated workflow cannot be accessed completely.
  prefs: []
  type: TYPE_NORMAL
- en: Today, most research and technical papers conclude with the approach used on
    the sample data, a brief mention of the methodology used, and a theoretical approach
    to a solution. Most of these works lack detailed calculations and step-by-step
    approaches. This is a very limited amount of knowledge for anyone reading it to
    be able to reproduce the same work that was carried out. This is the basic objective
    of reproducible coding, where ease of reproducing the code is key.
  prefs: []
  type: TYPE_NORMAL
- en: There have been advancements in notebooks in general, which can include text
    elements for commenting in detail; this improves the reproduction process. This
    is where Jupyter as a notebook is gaining traction within the data science and
    research communities.
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter was developed with the intention of being open source software with
    open standards and services for interactive computing across dozens of programming
    languages, including Python, Spark, and R.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility with Jupyter Notebooks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by learning what it is meant by **computational reproducibility**.
    Research, solutions, prototypes, and even a simple algorithm that is developed
    is said to be reproducible if access is provided to the original source code that
    was used to develop the solution, and the data that was used to build any related
    software should be able to produce the same results. However, today, the scientific
    community is experiencing some challenges in reproducing work developed previously
    by peers. This is mainly due to the lack of documentation and difficulty in understanding
    process workflows.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of a lack of documentation can be seen at every level, right from
    understanding the approach to the code level. Jupyter is one of the best tools
    for improvising this process, for better reproducibility, and for the reuse of
    developed code. This includes not just understanding what each line or snippet
    of code does, but also understanding and visualizing data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Jon Claerbout, who is considered the father of reproducible computational research,
    in the early 1990s required his students to develop research work and create results
    that could be regenerated in a single click. He believed that work that was completed
    took effort and tim, and should be left as it was so that further work on it could
    be done by reusing the earlier work without any difficulties. On a macro level,
    an economy's growth is strongly determined by the amount of innovation. The reproducibility
    of earlier work or research contributes to overall innovation growth.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see how we can maintain effective computational reproducibility using
    the Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following pointers are broad ways to achieve reproducibility using a Jupyter
    notebook in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a detailed introduction to the business problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document the approach and workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the data pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use source code version control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modularize the process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, let's explore and discuss the previously mentioned
    topics in a brief manner.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the Business Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key advantages of using the Jupyter notebook is that it includes
    textual content along with the code to create a workflow.
  prefs: []
  type: TYPE_NORMAL
- en: We must start with a good introduction to the business problem we have identified,
    and a summarization of it has to be documented in the Jupyter notebook to provide
    the gist of the problem. It has to include a problem statement, with the identified
    business problem from a data science perspective, concluding why we need to carry
    out this analysis, or what the objective of the process is.
  prefs: []
  type: TYPE_NORMAL
- en: Documenting the Approach and Workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In data science, there can be a lot of back and forth in computational work,
    such as, for instance, the explorations that are carried out, the kind of algorithms
    used, and the parameter changes to tune.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once changes to the approach have been finalized, those changes need to be
    documented to avoid work being repeated. Documenting the approach and workflows
    helps to set up a process. Adding comments to code while developing is a must
    and this has to be a continuous practice rather than waiting until the end or
    the results to add comments. By the end of the process, you may have forgotten
    the details, and this could result in miscalculating the effort that goes into
    it. The advantages of maintaining the Jupyter notebook with good documentation
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Tracking the development effort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-explanatory code with comments for each process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A better understanding of the code workflow and the results of each step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding back-and-forth work by making previous code snippets for specific tasks
    easy to find
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding duplicating work by understanding the repeated use of code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ease of knowledge transfer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining the Data Pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data for identifying and quantifying the problem can be generated from multiple
    data sources, databases, legacy systems, real-time data sources, and so on. The
    data scientist involved in this closely works with the data management teams of
    the client to extract and gather the required data and ingest it into the analytical
    tools for further analysis, and creates a strong data pipeline to acquire this
    data.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to document the data sources in detail (covered in the previous
    chapter) to maintain a data dictionary that explains the variables that are considered,
    why they are considered, what kind of data we have (structured or unstructured),
    and the type of data that we have; that is, whether we have a time-series, multivariate,
    or data that needs to be preprocessed and generated from raw sources such as image,
    text, speech, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Explain the Dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dependencies are the packages and libraries that are available in a tool. For
    instance, you may use OpenCV ([https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html)),
    a library in Python for image-related modeling, or you may use an API such as
    TensorFlow for deep-learning modeling. Here''s another example: if you use Matplotlib
    ( [https://matplotlib.org/](https://matplotlib.org/)) for visualization in Python,
    Matplotlib can be a part of dependencies. On other hand, dependencies can include
    the hardware and software specifications that are required for an analysis. You
    can manage your dependencies explicitly from the beginning by employing a tool
    such as a Conda environment to list all relevant dependencies (covered in previous
    chapters on dependencies for pandas, NumPy, and so on), including their package/library
    versions.'
  prefs: []
  type: TYPE_NORMAL
- en: Using Source Code Version Control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Version control is an important aspect when it comes to any kind of computational
    activity that involves code. When code is being developed, bugs or errors will
    arise. If previous versions of the code are available, we will then be able to
    pinpoint when the bug was identified, when it was resolved, and the amount of
    effort that went into it. This is possible through version control. At times,
    you may need to revert to older versions, because of scalability, performance,
    or for some other reasons. Using source code version control tools, you can always
    easily access previous versions of code.
  prefs: []
  type: TYPE_NORMAL
- en: Modularizing the Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Avoiding duplicate code is an effective practice for managing repetitive tasks,
    for maintaining code, and for debugging. To carry this out efficiently, you must
    modularize the process.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand this in detail. Say you carry out a set of data manipulation
    processes, where you develop the code to complete a task. Now, suppose you need
    to use the same code in a later section of the code; you need to do add, copy,
    or run the same steps again, which is a repetitive task. The input data and variable
    names can change. To handle this, you can write the earlier steps as a function
    for a dataset or on a variable and save all such functions as a separate module.
    You can call it a functions file (for example, `functions.py`, a Python file).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at this in more detail, particularly with
    respect to gathering and building an efficient data pipeline in a reproducible
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering Data in a Reproducible Way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the problem is defined, the first step in an analysis task is gathering
    data. Data can be extracted from multiple sources: databases, legacy systems,
    real-time data, external data, and so on. Data sources and the way data can be
    ingested into the model needs to be documented.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand how to use markdown and code block functionalities in the Jupyter
    notebook. Text can be added to Jupyter notebooks using markdown cells. These texts
    can be changed to bold or italic, like in any text editor. To change the cell
    type to markdown, you can use the **Cell** menu. We will look at the ways you
    can use various functionalities in markdown and code cells in Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: Functionalities in Markdown and Code Cells
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Markdown in Jupyter**: To select the markdown option in **Jupyter**, click
    on **Widgets** and **Markdown** from the drop-down menu:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12913_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: The markdown option in the Jupyter notebook'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`<h1>` and `<h2>` tags:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.2: Heading levels in the Jupyter notebook](img/C12913_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Heading levels in the Jupyter notebook'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Text in Jupyter**: To add text just the way it is, we do not add any tags
    to it:![Figure 7.3: Using normal text in the Jupyter notebook](img/Image36337.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 7.3: Using normal text in the Jupyter notebook'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`**`) to the start and end of the text, for example, **Bold**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.4: Using bold text in the Jupyter notebook](img/C12913_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Using bold text in the Jupyter notebook'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`*`) to the start and end of the text:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.5: Using italicized text in the Jupyter notebook](img/C12913_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Using italicized text in the Jupyter notebook'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Code in Jupyter**: To make text appear as code, select the **Code** option
    from the dropdown:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.6: Code in the Jupyter notebook](img/C12913_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Code in the Jupyter notebook'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Explaining the Business Problem in the Markdown
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Provide a brief introduction to the business problem to understand the objective
    of the project. The business problem definition is a summarization of the problem
    statement and includes the way in which the problem can be resolved using a data
    science algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7: Snippet of the problem definition](img/C12913_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Snippet of the problem definition'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Providing a Detailed Introduction to the Data Source
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The data source needs to be documented properly to understand the data license
    for reproducibility and for further work. A sample of how the data source can
    be added is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8: Data Source in Jupyter notebook](img/C12913_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Data Source in Jupyter notebook'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Explain the Data Attributes in the Markdown
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A data dictionary needs to be maintained to understand the data on an attribute
    level. This can include defining the attribute with what type of data it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9: Detailed attributes in markdown](img/C12913_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Detailed attributes in markdown'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To understand the data on an attribute level, we can use functions such as
    `info` and `describe`; however, `pandas_profiling` is a library that provides
    a lot of descriptive information in one function, from which we can extract the
    following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10: Profiling report](img/C12913_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Profiling report'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'On the DataFrame level, that is, for the overall data, which includes all the
    columns and rows that are considered:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total missing (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total size in memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average record size in memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the attribute level, which is for a specific column, the specifications
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Distinct count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unique (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing (n)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infinite (%)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infinite (n)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histogram for distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extreme values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.11: Data profiling report on the attribute level](img/C12913_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Data profiling report on the attribute level'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 45: Performing Data Reproducibility'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The aim of this exercise is to learn how to develop code for high reproducibility
    in terms of data understanding. We will be using the UCI Bank and Marketing dataset
    taken from this link: [https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson07/Dataset/bank/bank.csv](https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson07/Dataset/bank/bank.csv).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to achieve data reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add headings and mention the business problem in the notebook using markup:![Figure
    7.12: Introduction and business problem](img/C12913_07_12.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 7.12: Introduction and business problem'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Import the required libraries into the Jupyter notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now set the working directory, as illustrated in the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import and read the input dataset as `df` using pandas'' `read_csv` function
    from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now view the first five rows of the dataset using the `head` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.13: Data in the CSV file](img/C12913_07_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.13: Data in the CSV file'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Add the **Data Dictionary** and **Data Understanding** sections in the Jupyter
    notebook:![Figure 7.14: Data Dictionary](img/C12913_07_14.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 7.14: Data Dictionary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The **Data Understanding** section is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.15: Data Understanding](img/C12913_07_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.15: Data Understanding'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To understand the data specifications, use pandas profiling to generate the
    descriptive information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.16: Summary of the specifications with respect to data](img/C12913_07_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.16: Summary of the specifications with respect to data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Instructor Note:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This exercise identifies how a Jupyter notebook is created and includes how
    to develop a reproducible Jupyter notebook for a bank marketing problem. This
    must include a good introduction to the business problem, data, data types, data
    sources, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Code Practices and Standards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Writing code with a set of practices and standards is important for code reproducibility,
    as is explaining the workflow of the process descriptively in a step-wise manner.
  prefs: []
  type: TYPE_NORMAL
- en: This is universally applicable across any coding tool that you may use, not
    just with Jupyter. Some coding practices and standards should be followed strictly
    and a few of these will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Environment Documentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For installation purposes, you should maintain a snippet of code to install
    the necessary packages and libraries. The following practices help with code reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: Include the versions used for libraries/packages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the original version of packages/libraries used and call the packages
    internally for installation in a new setup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective implementation by running it in a script that automatically installs
    dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing Readable Code with Comments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Code commenting is an important aspect. Apart from the markdown option available
    on Jupyter, we must include comments for each code snippet. At times, we make
    changes in the code in a way that may not be used immediately but will be required
    for later steps. For instance, we can create an object that may not be used immediately
    for the next step but for later steps. This can confuse a new user in terms of
    understanding the flow. Commenting such specifics is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use a specific method, we must provide a reason for using that particular
    method. For example, let''s say, for the transformation of data for normal distribution,
    you can use `box-cox` or `yeo-johnson`. If there are negative values, you may
    prefer `yeo-johnson`, as it can handle negative values. It needs to be commented
    as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17: Comments with reasons](img/C12913_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.17: Comments with reasons'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We should also follow a good practice for naming the objects that we create.
    For example, you can name raw data `raw_data`, and can do the same for model data,
    preprocessed data, analysis data, and so on. The same goes when creating objects
    such as models and methods, for example, we can call power transformations `pt`.
  prefs: []
  type: TYPE_NORMAL
- en: Effective Segmentation of Workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When developing code, there are steps that you design to achieve end results.
    Each step can be part of a process. For instance, reading data, understanding
    data, carrying out various transformations, or building a model. Each of these
    steps, needs to be clearly separated for multiple reasons; firstly, code readability
    for how each stage is carried out and, secondly, how the result is generated at
    each stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here, we are looking at two sets of activities. One where the
    loop is generated to identify the columns that need to be normalized, and the
    second generating the columns that do not need to be normalized using the previous
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18: Effective segmentation of workflows](img/C12913_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.18: Effective segmentation of workflows'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Workflow Documentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When products and solutions are developed, they are mostly developed, monitored,
    deployed, and tested in a sandbox environment. To ensure a smooth deployment process
    in a new environment, we must provide sufficient support documents for technical,
    as well as non-technical, users. Workflow documentation includes requirements
    and design documents, product documentation, methodology documentation, installation
    guides, software user manuals, hardware and software requirements, troubleshooting
    management, and test documents. These are mostly required for a product or a solution
    development. We cannot just hand over a bunch of code to a client/user and ask
    them to get it running. Workflow documentation helps during the deployment and
    integration stages in a client/user environment, which is highly important for
    code reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, data science project documentation can be divided into two
    segments:'
  prefs: []
  type: TYPE_NORMAL
- en: Product documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methodology documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Product documentation provides information on how each functionality is used
    in the UI/UX and the application of it. Product documentation can be further segmented
    into:'
  prefs: []
  type: TYPE_NORMAL
- en: Installation guides
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software design and user manual
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methodology documentation provides information on the algorithms that are used,
    the methods, the solution approach, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 46: Missing Value Preprocessing with High Reproducibility'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The aim of this exercise is to learn how to develop code for high reproducibility
    in terms of missing value treatment preprocessing. We will be using the UCI Bank
    and Marketing dataset taken from this link: [https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson07/Dataset/bank/bank.csv](https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson07/Dataset/bank/bank.csv).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to find the missing value preprocessing reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries and packages in the Jupyter notebook, as illustrated
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the working directory of your choice as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the dataset from `back.csv` to the Spark object using the `read_csv`
    function, as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, view the first five rows of the dataset using the head function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.19: Bank dataset](img/C12913_07_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.19: Bank dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As the dataset has no missing values, we have to introduce some into the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, set the loop parameters, as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `for` loop for generating missing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following command to identify the missing values in the data by looking
    into each column''s missing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12913_07_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.20: Find the missing values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Define the range of **Interquartile Ranges** (**IQRs**) and apply them to the
    dataset to identify the outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.21: Identifying outliers](img/C12913_07_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.21: Identifying outliers'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Avoiding Repetition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We all know that the duplication or repetition of code is not a good practice.
    It becomes difficult to handle bugs, and the length of code increases. Different
    versions of the same code can lead to difficulty after a point, in terms of understanding
    which version is correct. For debugging, a change in one position needs to be
    reflected across the code. To avoid bad practices and write and maintain high-level
    code, let's learn about some best practices in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Using Functions and Loops for Optimizing Code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A function confines a task which requires a set of steps that from a single
    of multiple inputs to single or multiple outputs and loops are used for repetitive
    tasks on the same block of code for a different set of sample or subsetted data.
    Functions can be written for a single variable, multiple variables, a DataFrame,
    or a multiple set of parameter inputs.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's say you need to carry out some kind of transformation for
    only numeric variables in a DataFrame or matrix. A function can be written for
    a single variable and it can be applied to all numeric columns, or it can be written
    for a DataFrame, where the function identifies the set of numeric variables and
    applies them to generate the output. Once a function is written, it can be applied
    any future similar application in the proceeding code. This reduces duplicate
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the challenges that need to be considered when writing a
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Internal parameter changes**: There can be changes in the input parameter
    from one task to another. This is a common challenge. To handle this, you can
    mention the dynamic variables or objects in the function inputs when defining
    the inputs for a function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variations in the calculation process for a future task**: Write a function
    with internal functions that will not require many changes if any variations need
    to be captured. This way, rewriting the function for a new kind of task will be
    easy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoiding loops in functions**: If a process needs to be carried out across
    many subsets of the data by row, functions can be directly applied in each loop.
    This way, your function will be not be constrained by repetitive blocks of code
    on the same data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handing changes in data type changes**: The return object in a function can
    be different for different tasks. Depending on the task, the return object can
    be converted to other data classes or data types as required. However, input data
    classes or data types can change from task to task. To handle this, you need to
    clearly provide comments to understand the inputs for a function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Writing optimized functions**: Arrays are efficient when it comes to repetitive
    tasks such as loops or functions. In Python, using NumPy arrays generates very
    efficient data processing for most arithmetic operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing Libraries/Packages for Code/Algorithm Reuse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Packages or libraries encapsulate a collection of modules. They are highly dependable
    for code reproducibility and the modules that are generated. There are thousands
    of packages/libraries that are generated daily by developers and researchers around
    the globe. You can follow the package developing instructions from the Python
    project packaging guide for developing a new package ([https://packaging.python.org/tutorials/packaging-projects/](https://packaging.python.org/tutorials/packaging-projects/)).
    This tutorial will provide you with information on how to upload and distribute
    your package publicly as well as for internal use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 14: Carry normalisation of data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The aim of this activity is to apply various preprocessing techniques that were
    learned in previous exercises and to develop a model using preprocessed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries and read the data from the `bank.csv` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the dataset and read the CSV file into the Spark object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the normality of the data—the following step is to identify the normality
    of data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Segment the data numerically and categorically and perform distribution transformation
    on the numeric data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `for` loop that loops through each column to carry out a normality
    test to detect the normal distribution of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a power transformer. A power transformer will transform the data from
    a non-normal distribution to a normal distribution. The model developed will be
    used to transform the previously identified columns, which are non-normal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the created power transformer model to the non-normal data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To develop a model, first split the data into training and testing for cross-validation,
    train the model, then predict the model in test data for cross-validation. Finally,
    generate a confusion matrix for cross-validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution of this activity can be found on page 240.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to maintain code reproducibility from a
    data science perspective through structured standards and practices to avoid duplicate
    work using the Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: We started by gaining an understanding of what reproducibility is and how it
    impacts research and data science work. We looked into areas where we can improve
    code reproducibility, particularly looking at how we can maintain effective coding
    standards in terms of data reproducibility. Following that, we looked at important
    coding standards and practices to avoid duplicate work using the effective management
    of code through the segmentation of workflows, by developing functions for all
    key tasks, and how we can generalize coding to create libraries and packages from
    a reusability standpoint.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to use all the functionalities we have
    learned about so far to generate a full analysis report. We will also learn how
    to use various PySpark functionalities for SQL operations and how to develop various
    visualization plots.
  prefs: []
  type: TYPE_NORMAL
