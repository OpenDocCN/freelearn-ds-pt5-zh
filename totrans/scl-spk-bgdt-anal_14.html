<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Time to Put Some Order - Cluster Your Data with Spark MLlib</h1>
                
            
            
                
<div><em class="calibre8">"If you take a galaxy and try to make it bigger, it becomes a cluster of galaxies, not a galaxy. If you try to make it smaller than that, it seems to blow itself apart"</em></div>
<p class="cdpalignright">- Jeremiah P. Ostriker</p>
<p class="mce-root">In this chapter, we will delve deeper into machine learning and find out how we can take advantage of it to cluster records belonging to a certain group or class for a dataset of unsupervised observations. In a nutshell, the following topics will be covered in this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Unsupervised learning</li>
<li class="mce-root1">Clustering techniques</li>
<li class="mce-root1">Hierarchical clustering (HC)</li>
<li class="mce-root1">Centroid-based clustering (CC)</li>
<li class="mce-root1">Distribution-based clustering (DC)</li>
<li class="mce-root1">Determining number of clusters</li>
<li class="mce-root1">A comparative analysis between clustering algorithms</li>
<li class="mce-root1">Submitting jobs on computing clusters</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Unsupervised learning</h1>
                
            
            
                
<p class="mce-root">In this section, we will provide a brief introduction to unsupervised machine learning technique with appropriate examples. Let's start the discussion with a practical example. Suppose you have a large collection of not-pirated-totally-legal mp3s in a crowded and massive folder on your hard drive. Now, what if you can build a predictive model that helps automatically group together similar songs and organize them into your favorite categories such as country, rap, rock, and so on. This act of assigning an item to a group such that a mp3 to is added to the respective playlist in an unsupervised way. In the previous chapters, we assumed you're given a training dataset of correctly labeled data. Unfortunately, we don't always have that extravagance when we collect data in the real-world. For example, suppose we would like to divide up a large amount of music into interesting playlists. How could we possibly group together songs if we don't have direct access to their metadata? One possible approach could be a mixture of various machine learning techniques, but clustering is often at the heart of the solution.</p>
<p class="mce-root">In short, iIn unsupervised machine learning problem, correct classes of the training dataset are not available or unknown. Thus, classes have to be deduced from the structured or unstructured datasets as shown in <em class="calibre8">Figure 1</em>. This essentially implies that the goal of this type of algorithm is to preprocess the data in some structured ways. In other words, the main objective of the unsupervised learning algorithms is to explore the unknown/hidden patterns in the input data that are unlabeled<em class="calibre8">.</em> Unsupervised learning, however, also comprehends other techniques to explain the key features of the data in an exploratory way toward finding the hidden patterns. To overcome this challenge, clustering techniques are used widely to group unlabeled data points based on certain similarity measures in an unsupervised way.</p>
<div><p class="calibre20">For an in-depth theoretical knowledge of how unsupervised algorithms work, please refer to the following three books: <em class="calibre25">Bousquet</em>, <em class="calibre25">O.; von Luxburg</em>, <em class="calibre25">U.; Raetsch</em>, <em class="calibre25">G., eds</em> (2004). <em class="calibre25">Advanced Lectures on Machine Learning</em>. <em class="calibre25">Springer-Verlag</em>. ISBN 978-3540231226. Or <em class="calibre25">Duda</em>, <em class="calibre25">Richard O.</em>; <em class="calibre25">Hart</em>, <em class="calibre25">Peter E.</em>; <em class="calibre25">Stork</em>, <em class="calibre25">David G</em>. (2001). <em class="calibre25">Unsupervised Learning and Clustering</em>. <em class="calibre25">Pattern classification</em> (2nd Ed.). <em class="calibre25">Wiley</em>. ISBN 0-471-05669-3 and <em class="calibre25">Jordan</em>, <em class="calibre25">Michael I.</em>; <em class="calibre25">Bishop</em>, <em class="calibre25">Christopher M</em>. (2004) <em class="calibre25">Neural Networks</em>. In <em class="calibre25">Allen B. Tucker</em> <em class="calibre25">Computer Science Handbook, Second Edition</em> (Section VII: Intelligent Systems). <em class="calibre25">Boca Raton</em>, FL: Chapman and Hall/CRC Press LLC. ISBN 1-58488-360-X.</p>
</div>
<div><img class="image-border187" src="img/00263.jpeg"/></div>
<div><strong class="calibre1">Figure 1:</strong> Unsupervised learning with Spark</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Unsupervised learning example</h1>
                
            
            
                
<p class="mce-root">In clustering tasks, an algorithm groups related features into categories by analyzing similarities between input examples where similar features are clustered and marked using circles around. Clustering uses include but are not limited to the following: search result grouping such as grouping customers, anomaly detection for suspicious pattern finding, text categorization for finding useful pattern in tests, social network analysis for finding coherent groups, data center computing clusters for finding a way to put related computers together, astronomic data analysis for galaxy formation, and real estate data analysis to identify neighborhoods based on similar features. We will show a Spark MLlib-based solution for the last use cases.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Clustering techniques</h1>
                
            
            
                
<p class="mce-root">In this section, we will discuss clustering techniques along with challenges and suitable examples. A brief overview of hierarchical clustering, centroid-based clustering, and distribution-based clustering will be provided too.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Unsupervised learning and the clustering</h1>
                
            
            
                
<p class="mce-root">Clustering analysis is about dividing data samples or data points and putting them into corresponding homogeneous classes or clusters. Thus a trivial definition of clustering can be thought as the process of organizing objects into groups whose members are similar in some way.<br class="title-page-name"/>
A <em class="calibre8">cluster</em> is, therefore, a collection of objects that are <em class="calibre8">similar</em> between them and are <em class="calibre8">dissimilar</em> to the objects belonging to other clusters. As shown in <em class="calibre8">Figure 2</em>, if a collection of objects is given, clustering algorithms put those objects into a group based on similarity. A clustering algorithm such as K-means has then located the centroid of the group of data points. However, to make the clustering accurate and effective, the algorithm evaluates the distance between each point from the centroid of the cluster. Eventually, the goal of clustering is to determine the intrinsic grouping in a set of unlabeled data.</p>
<div><img class="image-border188" src="img/00059.jpeg"/></div>
<div><strong class="calibre1">Figure 2:</strong> Clustering raw data</div>
<p class="mce-root">Spark supports many clustering algorithms such as <strong class="calibre1">K-means</strong>, <strong class="calibre1">Gaussian mixture</strong>, <strong class="calibre1">power iteration clustering</strong> (<strong class="calibre1">PIC</strong>), l<strong class="calibre1">atent dirichlet allocation</strong> (<strong class="calibre1">LDA</strong>), <strong class="calibre1">bisecting K-means</strong>, and <strong class="calibre1">Streaming K-means</strong>. LDA is used for document classification and clustering commonly used in text mining. PIC is used for clustering vertices of a graph consisting of pairwise similarities as edge properties. However, to keep the objective of this chapter clearer and focused, we will confine our discussion to the K-means, bisecting K-means, and Gaussian mixture algorithms.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Hierarchical clustering</h1>
                
            
            
                
<p class="mce-root">The hierarchical clustering technique is based on the fundamental idea of objects or features that are more related to those nearby than others far away. Bisecting K-means is an example of such hierarchical clustering algorithm that connects data objects to form clusters based on their corresponding distance.</p>
<p class="mce-root">In the hierarchical clustering technique, a cluster can be described trivially by the maximum distance needed to connect parts of the cluster. As a result, different clusters will be formed at different distances. Graphically, these clusters can be represented using a dendrogram. Interestingly, the common name hierarchical clustering evolves from the concept of the dendrogram.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Centroid-based clustering</h1>
                
            
            
                
<p class="mce-root">In centroid-based clustering technique, clusters are represented by a central vector. However, the vector itself may not necessarily be a member of the data points. In this type of learning, a number of the probable clusters must be provided prior to training the model. K-means is a very famous example of this learning type, where, if you set the number of clusters to a fixed integer to say K, the K-means algorithm provides a formal definition as an optimization problem, which is a separate problem to be resolved to find the K cluster centers and assign the data objects the nearest cluster center. In short, this is an optimization problem where the objective is to minimize the squared distances from the clusters.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Distribution-based clustestering</h1>
                
            
            
                
<p class="mce-root">Distribution-based clustering algorithms are based on statistical distribution models that provide more convenient ways to cluster related data objects to the same distribution. Although the theoretical foundations of these algorithms are very robust, they mostly suffer from overfitting. However, this limitation can be overcome by putting constraints on the model complexity.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Centroid-based clustering (CC)</h1>
                
            
            
                
<p class="mce-root">In this section, we discuss the centroid-based clustering technique and its computational challenges. An example of using K-means with Spark MLlib will be shown for a better understanding of the centroid-based clustering.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Challenges in CC algorithm</h1>
                
            
            
                
<p class="mce-root">As discussed previously, in a centroid-based clustering algorithm like K-means, setting the optimal value of the number of clusters K is an optimization problem. This problem can be described as NP-hard (that is non-deterministic polynomial-time hard) featuring high algorithmic complexities, and thus the common approach is trying to achieve only an approximate solution. Consequently, solving these optimization problems imposes an extra burden and consequently nontrivial drawbacks. Furthermore, the K-means algorithm expects that each cluster has approximately similar size. In other words, data points in each cluster have to be uniform to get better clustering performance.</p>
<p class="mce-root">Another major drawback of this algorithm is that this algorithm tries to optimize the cluster centers but not cluster borders, and this often tends to inappropriately cut the borders in between the clusters. However, sometimes, we can have the advantage of visual inspection, which is often not available for data on hyperplanes or multidimensional data. Nonetheless, a complete section on how to find the optimal value of K will be discussed later in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">How does K-means algorithm work?</h1>
                
            
            
                
<p class="mce-root">Suppose we have <em class="calibre8">n</em> data points <em class="calibre8">x<sub class="calibre43">i</sub></em>, <em class="calibre8">i=1...n</em> that need to be partitioned into <em class="calibre8">k</em> clusters. Now that the target here is to assign a cluster to each data point. K-means then aims to find the positions <em class="calibre8">μ<sub class="calibre43">i</sub>,i=1...k</em> of the clusters that minimize the distance from the data points to the cluster. Mathematically, the K-means algorithm tries to achieve the goal by solving the following equation, that is, an optimization problem:</p>
<div><img class="alignnone7" src="img/00320.jpeg"/></div>
<p class="mce-root">In the preceding equation, <em class="calibre8">c<sub class="calibre43">i</sub></em> is the set of data points assigned to cluster <em class="calibre8">i</em>, and <em class="calibre8">d(x,μ<sub class="calibre43">i</sub>) =||x−μ<sub class="calibre43">i</sub>||<sup class="calibre26">2</sup><sub class="calibre43">2</sub></em> is the Euclidean distance to be calculated (we will explain why we should use this distance measurement shortly). Therefore, we can understand that the overall clustering operation using K-means is not a trivial one but an NP-hard optimization problem. This also means that the K-means algorithm not only tries to find the global minima but also often gets stuck in different solutions.</p>
<p class="mce-root"> </p>
<p class="mce-root">Now, let's see how we could formulate the algorithm before we can feed the data to the K-means model. At first, we need to decide the number of tentative clusters, <em class="calibre8">k</em> priory. Then, typically, you need to follow these steps:</p>
<div><img class="alignnone8" src="img/00367.jpeg"/></div>
<p class="mce-root">Here <em class="calibre8">|c|</em> is the number of elements in <em class="calibre8">c</em>.</p>
<p class="mce-root">Clustering using the K-means algorithm begins by initializing all the coordinates to centroids. With every pass of the algorithm, each point is assigned to its nearest centroid based on some distance metric, usually <em class="calibre8">Euclidean distance</em>.</p>
<div><strong class="calibre27">Distance calculation:</strong> Note that there are other ways to calculate the distance too, for example:<br class="calibre23"/>
<em class="calibre25">Chebyshev distance</em> can be used to measure the distance by considering only the most notable dimensions.<br class="calibre23"/>
The <em class="calibre25">Hamming distance</em> algorithm can identify the difference between two strings. On the other hand, to make the distance metric scale-undeviating, <em class="calibre25">Mahalanobis distance</em> can be used to normalize the covariance matrix. The <em class="calibre25">Manhattan distance</em> is used to measure the distance by considering only axis-aligned directions. The <em class="calibre25">Minkowski distance</em> algorithm is used to make the Euclidean distance, Manhattan distance, and Chebyshev distance. The <em class="calibre25">Haversine distance</em> is used to measure the great-circle distances between two points on a sphere from the location, that is, longitudes and latitudes.</div>
<p class="mce-root">Considering these distance-measuring algorithms, it is clear that the Euclidean distance algorithm would be the most appropriate to solve our purpose of distance calculation in the K-means algorithm. The centroids are then updated to be the centers of all the points assigned to it in that iteration. This repeats until there is a minimal change in the centers. In short, the K-means algorithm is an iterative algorithm and works in two steps:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Cluster assignment step</strong>: K-means goes through each of the m data points in the dataset which is assigned to a cluster that is represented by the closest of the k centroids. For each point, the distances to each centroid is then calculated and simply pick the least distant one.</li>
<li class="mce-root1">
<p class="calibre32"><strong class="calibre1">Update step</strong>: For each cluster, a new centroid is calculated as the mean of all points in the cluster. From the previous step, we have a set of points which are assigned to a cluster. Now, for each such set, we calculate a mean that we declare a new centroid of the cluster.</p>
</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">An example of clustering using K-means of Spark MLlib</h1>
                
            
            
                
<p class="mce-root">To further demonstrate the clustering example, we will use the <em class="calibre8">Saratoga NY Homes</em> dataset downloaded from <a href="http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html" class="calibre10">http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html</a> as an unsupervised learning technique using Spark MLlib. The dataset contains several features of houses located in the suburb of the New York City. For example, price, lot size, waterfront, age, land value, new construct, central air, fuel type, heat type, sewer type, living area, pct.college, bedrooms, fireplaces, bathrooms, and the number of rooms. However, only a few features have been shown in the following table:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Price</strong></td>
<td class="calibre7"><strong class="calibre1">Lot Size</strong></td>
<td class="calibre7"><strong class="calibre1">Water Front</strong></td>
<td class="calibre7"><strong class="calibre1">Age</strong></td>
<td class="calibre7"><strong class="calibre1">Land Value</strong></td>
<td class="calibre7"><strong class="calibre1">Rooms</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7">132,500</td>
<td class="calibre7">0.09</td>
<td class="calibre7">0</td>
<td class="calibre7">42</td>
<td class="calibre7">5,000</td>
<td class="calibre7">5</td>
</tr>
<tr class="calibre6">
<td class="calibre7">181,115</td>
<td class="calibre7">0.92</td>
<td class="calibre7">0</td>
<td class="calibre7">0</td>
<td class="calibre7">22,300</td>
<td class="calibre7">6</td>
</tr>
<tr class="calibre6">
<td class="calibre7">109,000</td>
<td class="calibre7">0.19</td>
<td class="calibre7">0</td>
<td class="calibre7">133</td>
<td class="calibre7">7,300</td>
<td class="calibre7">8</td>
</tr>
<tr class="calibre6">
<td class="calibre7">155,000</td>
<td class="calibre7">0.41</td>
<td class="calibre7">0</td>
<td class="calibre7">13</td>
<td class="calibre7">18,700</td>
<td class="calibre7">5</td>
</tr>
<tr class="calibre6">
<td class="calibre7">86,060</td>
<td class="calibre7">0.11</td>
<td class="calibre7">0</td>
<td class="calibre7">0</td>
<td class="calibre7">15,000</td>
<td class="calibre7">3</td>
</tr>
<tr class="calibre6">
<td class="calibre7">120,000</td>
<td class="calibre7">0.68</td>
<td class="calibre7">0</td>
<td class="calibre7">31</td>
<td class="calibre7">14,000</td>
<td class="calibre7">8</td>
</tr>
<tr class="calibre6">
<td class="calibre7">153,000</td>
<td class="calibre7">0.4</td>
<td class="calibre7">0</td>
<td class="calibre7">33</td>
<td class="calibre7">23,300</td>
<td class="calibre7">8</td>
</tr>
<tr class="calibre6">
<td class="calibre7">170,000</td>
<td class="calibre7">1.21</td>
<td class="calibre7">0</td>
<td class="calibre7">23</td>
<td class="calibre7">146,000</td>
<td class="calibre7">9</td>
</tr>
<tr class="calibre6">
<td class="calibre7">90,000</td>
<td class="calibre7">0.83</td>
<td class="calibre7">0</td>
<td class="calibre7">36</td>
<td class="calibre7">222,000</td>
<td class="calibre7">8</td>
</tr>
<tr class="calibre6">
<td class="calibre7">122,900</td>
<td class="calibre7">1.94</td>
<td class="calibre7">0</td>
<td class="calibre7">4</td>
<td class="calibre7">212,000</td>
<td class="calibre7">6</td>
</tr>
<tr class="calibre6">
<td class="calibre7">325,000</td>
<td class="calibre7">2.29</td>
<td class="calibre7">0</td>
<td class="calibre7">123</td>
<td class="calibre7">126,000</td>
<td class="calibre7">12</td>
</tr>
</tbody>
</table>
<div><strong class="calibre1">Table 1:</strong> Sample data from the Saratoga NY Homes dataset</div>
<p class="mce-root">The target of this clustering technique here is to show an exploratory analysis based on the features of each house in the city for finding possible neighborhoods for the house located in the same area. Before performing feature extraction, we need to load and parse the Saratoga NY Homes dataset. This step also includes loading packages and related dependencies, reading the dataset as RDD, model training, prediction, collecting the local parsed data, and clustering comparing.</p>
<p class="mce-root"><strong class="calibre1">Step 1</strong>. Import-related packages:</p>
<pre class="calibre19">
package com.chapter13.Clustering<br class="title-page-name"/>import org.apache.spark.{SparkConf, SparkContext}<br class="title-page-name"/>import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}<br class="title-page-name"/>import org.apache.spark.mllib.linalg.Vectors<br class="title-page-name"/>import org.apache.spark._<br class="title-page-name"/>import org.apache.spark.rdd.RDD<br class="title-page-name"/>import org.apache.spark.sql.functions._<br class="title-page-name"/>import org.apache.spark.sql.types._<br class="title-page-name"/>import org.apache.spark.sql._<br class="title-page-name"/>import org.apache.spark.sql.SQLContext
</pre>
<p class="mce-root"><strong class="calibre1">Step 2. Create a Spark session - the entry point</strong> - Here we at first set the Spark configuration by setting the application name and master URL. For simplicity, it's standalone with all the cores on your machine:</p>
<pre class="calibre19">
val spark = SparkSession<br class="title-page-name"/>                 .builder<br class="title-page-name"/>                 .master("local[*]")<br class="title-page-name"/>                 .config("spark.sql.warehouse.dir", "E:/Exp/")<br class="title-page-name"/>                 .appName("KMeans")<br class="title-page-name"/>                 .getOrCreate()
</pre>
<p class="mce-root"><strong class="calibre1">Step 3. Load and parse the dataset</strong> - Read, parse, and create RDDs from the dataset as follows:</p>
<pre class="calibre19">
//Start parsing the dataset<br class="title-page-name"/>val start = System.currentTimeMillis()<br class="title-page-name"/>val dataPath = "data/Saratoga NY Homes.txt"<br class="title-page-name"/>//val dataPath = args(0)<br class="title-page-name"/>val landDF = parseRDD(spark.sparkContext.textFile(dataPath))<br class="title-page-name"/>                                 .map(parseLand).toDF().cache()<br class="title-page-name"/>landDF.show()
</pre>
<p class="calibre46">Note that, to make the preceding code work, you should import the following package:</p>
<pre class="calibre19">
import spark.sqlContext.implicits._
</pre>
<p class="calibre46">You will get the following output:</p>
<div><img class="image-border189" src="img/00339.jpeg"/></div>
<div><strong class="calibre1">Figure 3:</strong> A snapshot of the Saratoga NY Homes dataset</div>
<p class="calibre46">The following is the <kbd class="calibre11">parseLand</kbd> method that is used to create a <kbd class="calibre11">Land</kbd> class from an array of <kbd class="calibre11">Double</kbd> as follows:</p>
<pre class="calibre19">
// function to create a  Land class from an Array of Double<br class="title-page-name"/>def parseLand(line: Array[Double]): Land = {<br class="title-page-name"/>  Land(line(0), line(1), line(2), line(3), line(4), line(5),<br class="title-page-name"/>   line(6), line(7), line(8), line(9), line(10),<br class="title-page-name"/>   line(11), line(12), line(13), line(14), line(15)<br class="title-page-name"/>  )<br class="title-page-name"/>}
</pre>
<p class="calibre46">And the <kbd class="calibre11">Land</kbd> class that reads all the features as a double is as follows:</p>
<pre class="calibre19">
case class Land(<br class="title-page-name"/>  Price: Double, LotSize: Double, Waterfront: Double, Age: Double,<br class="title-page-name"/>  LandValue: Double, NewConstruct: Double, CentralAir: Double, <br class="title-page-name"/>  FuelType: Double, HeatType: Double, SewerType: Double, <br class="title-page-name"/>  LivingArea: Double, PctCollege: Double, Bedrooms: Double,<br class="title-page-name"/>  Fireplaces: Double, Bathrooms: Double, rooms: Double<br class="title-page-name"/>)
</pre>
<p class="calibre46">As you already know, to train the K-means model, we need to ensure all the data points and features to be numeric. Therefore, we further need to convert all the data points to double as follows:</p>
<pre class="calibre19">
// method to transform an RDD of Strings into an RDD of Double<br class="title-page-name"/>def parseRDD(rdd: RDD[String]): RDD[Array[Double]] = {<br class="title-page-name"/>  rdd.map(_.split(",")).map(_.map(_.toDouble))<br class="title-page-name"/>}
</pre>
<p class="mce-root"><strong class="calibre1">Step 4. Preparing the training set</strong> - At first, we need to convert the data frame (that is, <kbd class="calibre11">landDF</kbd>) to an RDD of doubles and cache the data to create a new data frame to link the cluster numbers as follows:</p>
<pre class="calibre19">
val rowsRDD = landDF.rdd.map(r =&gt; (<br class="title-page-name"/>  r.getDouble(0), r.getDouble(1), r.getDouble(2),<br class="title-page-name"/>  r.getDouble(3), r.getDouble(4), r.getDouble(5),<br class="title-page-name"/>  r.getDouble(6), r.getDouble(7), r.getDouble(8),<br class="title-page-name"/>  r.getDouble(9), r.getDouble(10), r.getDouble(11),<br class="title-page-name"/>  r.getDouble(12), r.getDouble(13), r.getDouble(14),<br class="title-page-name"/>  r.getDouble(15))<br class="title-page-name"/>)<br class="title-page-name"/>rowsRDD.cache()
</pre>
<p class="calibre46">Now that we need to convert the preceding RDD of doubles into an RDD of dense vectors as follows:</p>
<pre class="calibre19">
// Get the prediction from the model with the ID so we can<br class="title-page-name"/>   link them back to other information<br class="title-page-name"/>val predictions = rowsRDD.map{r =&gt; (<br class="title-page-name"/>  r._1, model.predict(Vectors.dense(<br class="title-page-name"/>    r._2, r._3, r._4, r._5, r._6, r._7, r._8, r._9,<br class="title-page-name"/>    r._10, r._11, r._12, r._13, r._14, r._15, r._16<br class="title-page-name"/>  )<br class="title-page-name"/>))}
</pre>
<p class="mce-root"><strong class="calibre1">Step 5. Train the K-means model</strong> - Train the model by specifying 10 clusters, 20 iterations, and 10 runs as follows:</p>
<pre class="calibre19">
val numClusters = 5<br class="title-page-name"/>val numIterations = 20<br class="title-page-name"/>val run = 10<br class="title-page-name"/>val model = KMeans.train(numericHome, numClusters,numIterations, run,<br class="title-page-name"/>                         KMeans.K_MEANS_PARALLEL)
</pre>
<p>The Spark-based implementation of K-means starts working by initializing a set of cluster centers using the K-means algorithm by <em class="calibre25">Bahmani et al.</em>, <em class="calibre25">Scalable K-Means++</em>, VLDB 2012. This is a variant of K-means++ that tries to find dissimilar cluster centers by starting with a random center and then doing passes where more centers are chosen with a probability proportional to their squared distance to the current cluster set. It results in a provable approximation to an optimal clustering. The original paper can be found at <a href="http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf" class="calibre21">http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf</a>.</p>
<p class="mce-root"> </p>
<p class="mce-root"><strong class="calibre1">Step 6. Evaluate the model error rate</strong> - The standard K-means algorithm aims at minimizing the sum of squares of the distance between the points of each set, that is, the squared Euclidean distance, which is the WSSSE's objective. The K-means algorithm aims at minimizing the sum of squares of the distance between the points of each set (that is, the cluster center). However, if you really wanted to minimize the sum of squares of the distance between the points of each set, you would end up with a model where each cluster is its own cluster center; in that case, that measure would be 0.</p>
<p class="mce-root">Therefore, once you have trained your model by specifying the parameters, you can evaluate the result by using <strong class="calibre1">Within Set Sum of Squared Errors</strong> (<strong class="calibre1">WSSE</strong>). Technically, it is something like the sum of the distances of each observation in each K cluster that can be computed as follows:</p>
<pre class="calibre19">
// Evaluate clustering by computing Within Set Sum of Squared Errors<br class="title-page-name"/>val WCSSS = model.computeCost(landRDD)<br class="title-page-name"/>println("Within-Cluster Sum of Squares = " + WCSSS)
</pre>
<p class="mce-root">The preceding model training set produces the value of WCSSS:</p>
<pre class="calibre19">
<strong class="calibre1">Within-Cluster Sum of Squares = 1.455560123603583E12 </strong>
</pre>
<p class="mce-root"><strong class="calibre1">Step 7. Compute and print the cluster centers</strong> - At first, we get the prediction from the model with the ID so that we can link them back to other information related to each house. Note that we will use an RDD of rows that we prepared in step 4<em class="calibre8">:</em></p>
<pre class="calibre19">
// Get the prediction from the model with the ID so we can link them<br class="title-page-name"/>   back to other information<br class="title-page-name"/>val predictions = rowsRDD.map{r =&gt; (<br class="title-page-name"/>  r._1, model.predict(Vectors.dense(<br class="title-page-name"/>    r._2, r._3, r._4, r._5, r._6, r._7, r._8, r._9, r._10,<br class="title-page-name"/>    r._11, r._12, r._13, r._14, r._15, r._16<br class="title-page-name"/>  )<br class="title-page-name"/>))}
</pre>
<p class="mce-root">However, it should be provided when a prediction is requested about the price. This should be done as follows:</p>
<pre class="calibre19">
val predictions = rowsRDD.map{r =&gt; (<br class="title-page-name"/>  r._1, model.predict(Vectors.dense(<br class="title-page-name"/>    r._1, r._2, r._3, r._4, r._5, r._6, r._7, r._8, r._9, r._10,<br class="title-page-name"/>    r._11, r._12, r._13, r._14, r._15, r._16<br class="title-page-name"/>  )<br class="title-page-name"/>))}
</pre>
<p class="mce-root">For better visibility and an exploratory analysis, convert the RDD to a DataFrame as follows:</p>
<pre class="calibre19">
import spark.sqlContext.implicits._<strong class="calibre1"><br class="title-page-name"/></strong>val predCluster = predictions.toDF("Price", "CLUSTER")<br class="title-page-name"/>predCluster.show()
</pre>
<p class="mce-root">This should produce the output shown in the following figure:</p>
<div><img class="image-border190" src="img/00044.gif"/></div>
<div><strong class="calibre1">Figure 4:</strong> A snapshot of the clusters predicted</div>
<p class="mce-root">Since there's no distinguishable ID in the dataset, we represented the <kbd class="calibre11">Price</kbd> field to make the linking. From the preceding figure, you can understand where does a house having a certain price falls, that is, in which cluster. Now for better visibility, let's join the prediction DataFrame with the original DataFrame to know the individual cluster number for each house:</p>
<pre class="calibre19">
val newDF = landDF.join(predCluster, "Price") <br class="title-page-name"/>newDF.show()
</pre>
<p class="mce-root">You should observe the output in the following figure:</p>
<div><img class="image-border31" src="img/00138.jpeg"/></div>
<div><strong class="calibre1">Figure 5:</strong> A snapshot of the clusters predicted across each house</div>
<p class="mce-root">To make the analysis, we dumped the output in RStudio and generated the clusters shown in <em class="calibre8">Figure 6</em>. The R script can be found on my GitHub repositories at <a href="https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics" class="calibre10">https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics</a>. Alternatively, you can write your own script and do the visualization accordingly.</p>
<div><img class="image-border191" src="img/00142.jpeg"/></div>
<div><strong class="calibre1">Figure 6:</strong> Clusters of the neighborhoods</div>
<p class="mce-root">Now, for more extensive analysis and visibility, we can observe related statistics for each cluster. For example, below I printed thestatistics related to cluster 3 and 4 in <em class="calibre8">Figure 8</em> and <em class="calibre8">Figure 9</em>, respectively:</p>
<pre class="calibre19">
newDF.filter("CLUSTER = 0").show() <br class="title-page-name"/>newDF.filter("CLUSTER = 1").show()<br class="title-page-name"/>newDF.filter("CLUSTER = 2").show()<br class="title-page-name"/>newDF.filter("CLUSTER = 3").show()<br class="title-page-name"/>newDF.filter("CLUSTER = 4").show()
</pre>
<p class="mce-root">Now get the descriptive statistics for each cluster as follows:</p>
<pre class="calibre19">
newDF.filter("CLUSTER = 0").describe().show()<br class="title-page-name"/>newDF.filter("CLUSTER = 1").describe().show()<br class="title-page-name"/>newDF.filter("CLUSTER = 2").describe().show()<br class="title-page-name"/>newDF.filter("CLUSTER = 3").describe().show() <br class="title-page-name"/>newDF.filter("CLUSTER = 4").describe().show()
</pre>
<p class="mce-root">At first, let's observe the related statistics of cluster 3 in the following figure:</p>
<div><img class="image-border192" src="img/00353.jpeg"/></div>
<div><strong class="calibre1">Figure 7:</strong> Statistics on cluster 3</div>
<p class="mce-root">Now let's observe the related statistics of cluster 4 in the following figure:</p>
<div><img class="image-border193" src="img/00377.jpeg"/></div>
<div><strong class="calibre1">Figure 8:</strong> Statistics on cluster 4</div>
<p class="mce-root">Note that, since the original screenshot was too large to fit in this page, the original images were modified and the column containing other variables of the houses were removed.</p>
<p class="mce-root">Due to the random nature of this algorithm, you might receive different results for each successful iteration. However, you can lock the random nature of this algorithm by setting the seed as follows:</p>
<pre class="calibre19">
val numClusters = 5 <br class="title-page-name"/>val numIterations = 20 <br class="title-page-name"/>val seed = 12345 <br class="title-page-name"/>val model = KMeans.train(landRDD, numClusters, numIterations, seed)
</pre>
<p class="mce-root"><strong class="calibre1">Step 8. Stop the Spark session</strong> - Finally, stop the Spark session using the stop method as follows:</p>
<pre class="calibre19">
spark.stop()
</pre>
<p class="mce-root">In the preceding example, we dealt with a very small set of features; common-sense and visual inspection would also lead us to the same conclusions. From the above example using the K-means algorithm, we can understand that there are some limitations for this algorithm. For example, it's really difficult to predict the K-value, and with a global cluster it does not work well. Moreover, different initial partitions can result in different final clusters, and, finally, it does not work well with clusters of different sizes and densities.</p>
<p>To overcome these limitations, we have some more robust algorithms in this book like MCMC (Markov Chain Monte Carlo; see also at <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" class="calibre21">https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo</a>) presented in the book: <em class="calibre25">Tribble</em>, <em class="calibre25">Seth D.</em>, <strong class="calibre27">Markov chain Monte Carlo</strong> algorithms using completely uniformly distributed driving sequences, Diss. Stanford University, 2007.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Hierarchical clustering (HC)</h1>
                
            
            
                
<p class="mce-root">In this section, we discuss the hierarchical clustering technique and its computational challenges. An example of using the bisecting K-means algorithm of hierarchical clustering with Spark MLlib will be shown too for a better understanding of hierarchical clustering.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">An overview of HC algorithm and challenges</h1>
                
            
            
                
<p class="mce-root">A hierarchical clustering technique is computationally different from the centroid-based clustering in the way the distances are computed. This is one of the most popular and widely used clustering analysis technique that looks to build a hierarchy of clusters. Since a cluster usually consists of multiple objects, there will be other candidates to compute the distance too. Therefore, with the exception of the usual choice of distance functions, you also need to decide on the linkage criterion to be used. In short, there are two types of strategies in hierarchical clustering:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Bottom-up approach</strong>: In this approach, each observation starts within its own cluster. After that, the pairs of clusters are merged together and one moves up the hierarchy.</li>
<li class="mce-root1"><strong class="calibre1">Top-down approach</strong>: In this approach, all observations start in one cluster, splits are performed recursively, and one moves down the hierarchy.</li>
</ul>
<p class="mce-root">These bottom-up or top-down approaches are based on the s<strong class="calibre1">ingle-linkage clustering</strong> (<strong class="calibre1">SLINK</strong>) technique, which considers the minimum object distances, the <strong class="calibre1">complete linkage clustering</strong> (<strong class="calibre1">CLINK</strong>), which considers the maximum of object distances, and the u<strong class="calibre1">nweighted pair group method with arithmetic mean</strong> (<strong class="calibre1">UPGMA</strong>). The latter is also known as <strong class="calibre1">average-linkage clustering</strong>. Technically, these methods will not produce unique partitions out of the dataset (that is, different clusters).</p>
<p>A comparative analysis on these three approaches can be found at <a href="https://nlp.stanford.edu/IR-book/completelink.html" class="calibre21">https://nlp.stanford.edu/IR-book/completelink.html.</a></p>
<p class="mce-root">However, the user still needs to choose appropriate clusters from the hierarchy for better cluster prediction and assignment. Although algorithms of this class like bisecting K-means are computationally faster than the K-means algorithm, there are three disadvantages to this type of algorithm:</p>
<ul class="calibre9">
<li class="mce-root1">First, these methods are not very robust toward outliers or datasets containing noise or missing values. This disadvantage either imposes additional clusters or even causes other clusters to merge. This problem is commonly referred to as the chaining phenomenon, especially for single-linkage clustering.</li>
<li class="mce-root1">Second, from the algorithmic analysis, the complexity is for agglomerative clustering and for divisive clustering, which makes them too slow for large data sets.</li>
<li class="mce-root1">Third, SLINK and CLINK were previously used widely in data mining tasks as theoretical foundations of cluster analysis, but nowadays they are considered obsolete.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Bisecting K-means with Spark MLlib</h1>
                
            
            
                
<p class="mce-root">Bisecting K-means can often be much faster than regular K-means, but it will generally produce a different clustering. A bisecting K-means algorithm is based on the paper, <em class="calibre8">A comparison of document clustering</em> techniques by <em class="calibre8">Steinbach</em>, <em class="calibre8">Karypis</em>, and <em class="calibre8">Kumar</em>, with modification to fit with Spark MLlib.</p>
<p class="mce-root">Bisecting K-means is a kind of divisive algorithm that starts from a single cluster that contains all the data points. Iteratively, it then finds all the divisible clusters on the bottom level and bisects each of them using K-means until there are K leaf clusters in total or no leaf clusters divisible. After that, clusters on the same level are grouped together to increase the parallelism. In other words, bisecting K-means is computationally faster than the regular K-means algorithm. Note that if bisecting all the divisible clusters on the bottom level results in more than K leaf clusters, larger clusters will always get higher priority.</p>
<p class="mce-root">Note that if the bisecting of all the divisible clusters on the bottom level results in more than K leaf clusters, larger clusters will always get higher priority. The following parameters are used in the Spark MLlib implementation:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">K</strong>: This is the desired number of leaf clusters. However, the actual number could be smaller if there are no divisible leaf clusters left during the computation. The default value is 4.</li>
<li class="mce-root1"><strong class="calibre1">MaxIterations</strong>: This is the max number of K-means iterations to split the clusters. The default value is 20.</li>
<li class="mce-root1"><strong class="calibre1">MinDivisibleClusterSize</strong>: This is the minimum number of points. The default value is set as 1.</li>
<li class="mce-root1"><strong class="calibre1">Seed</strong>: This is a random seed that disallows random clustering and tries to provide almost similar result in each iteration. However, it is recommended to use a long seed value like 12345 and so on.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Bisecting K-means clustering of the neighborhood using Spark MLlib</h1>
                
            
            
                
<p class="mce-root">In the previous section, we saw how to cluster similar houses together to determine the neighborhood. The bisecting K-means is also similar to regular K-means except that the model training that takes different training parameters as follows:</p>
<pre class="calibre19">
// Cluster the data into two classes using KMeans <br class="title-page-name"/>val bkm = new BisectingKMeans() <br class="title-page-name"/>                 .setK(5) // Number of clusters of the similar houses<br class="title-page-name"/>                 .setMaxIterations(20)// Number of max iteration<br class="title-page-name"/>                 .setSeed(12345) // Setting seed to disallow randomness <br class="title-page-name"/>val model = bkm.run(landRDD)
</pre>
<p class="mce-root">You should refer to the previous example and just reuse the previous steps to get the trained data. Now let's evaluate clustering by computing WSSSE as follows:</p>
<pre class="calibre19">
val WCSSS = model.computeCost(landRDD)<br class="title-page-name"/>println("Within-Cluster Sum of Squares = " + WCSSS) // Less is better    
</pre>
<p class="mce-root">You should observe the following output: <kbd class="calibre11">Within-Cluster Sum of Squares = 2.096980212594632E11</kbd>. Now for more analysis, please refer to step 5 in the previous section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Distribution-based clustering (DC)</h1>
                
            
            
                
<p class="mce-root">In this section, we will discuss the distribution-based clustering technique and its computational challenges. An example of using <strong class="calibre1">Gaussian mixture models</strong> (<strong class="calibre1">GMMs</strong>) with Spark MLlib will be shown for a better understanding of distribution-based clustering.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Challenges in DC algorithm</h1>
                
            
            
                
<p class="mce-root">A distribution-based clustering algorithm like GMM is an expectation-maximization algorithm. To avoid the overfitting problem, GMM usually models the dataset with a fixed number of Gaussian distributions. The distributions are initialized randomly, and the related parameters are iteratively optimized too to fit the model better to the training dataset. This is the most robust feature of GMM and helps the model to be converged toward the local optimum. However, multiple runs of this algorithm may produce different results.</p>
<p class="mce-root">In other words, unlike the bisecting K-means algorithm and soft clustering, GMM is optimized for hard clustering, and in order to obtain of that type, objects are often assigned to the Gaussian distribution. Another advantageous feature of GMM is that it produces complex models of clusters by capturing all the required correlations and dependence between data points and attributes.</p>
<p class="mce-root">On the down-side, GMM has some assumptions about the format and shape of the data, and this puts an extra burden on us (that is, users). More specifically, if the following two criteria do not meet, performance decreases drastically:</p>
<ul class="calibre9">
<li class="mce-root1">Non-Gaussian dataset: The GMM algorithm assumes that the dataset has an underlying Gaussian, which is generative distribution. However, many practical datasets do not satisfy this assumption that is subject to provide low clustering performance.</li>
<li class="mce-root1">If the clusters do not have even sizes, there is a high chance that small clusters will be dominated by larger ones.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">How does a Gaussian mixture model work?</h1>
                
            
            
                
<p class="mce-root">Using GMM is a popular technique of soft clustering. GMM tries to model all the data points as a finite mixture of Gaussian distributions; the probability that each point belongs to each cluster is computed along with the cluster related statistics and represents an amalgamate distribution: where all the points are derived from one of <em class="calibre8">K</em> Gaussian subdistributions having own probability. In short, the functionality of GMM can be described in a three-steps pseudocode:</p>
<ol class="calibre14">
<li value="1" class="mce-root1"><strong class="calibre1">Objective function:</strong> Compute and maximize the log-likelihood using expectation–maximization (EM) as a framework</li>
<li value="2" class="mce-root1"><strong class="calibre1">EM algorithm:</strong>
<ul class="calibre38">
<li class="mce-root1"><strong class="calibre1">E step:</strong> Compute the posterior probability of membership -i.e. nearer data points</li>
<li class="mce-root1"><strong class="calibre1">M step:</strong> Optimize the parameters.</li>
</ul>
</li>
<li value="3" class="mce-root1"><strong class="calibre1">Assignment:</strong> Perform soft assignment during step E.</li>
</ol>
<p class="mce-root">Technically, when a statistical model is given, parameters of that model (that is, when applied to a data set) are estimated using the <strong class="calibre1">maximum-likelihood estimation</strong> (<strong class="calibre1">MLE</strong>). On the other hand, <strong class="calibre1">EM</strong> algorithm is an iterative process of finding maximum likelihood.</p>
<p>Since the GMM is an unsupervised algorithm, GMM model depends on the inferred variables. Then EM iteration rotates toward performing the expectation (E) and maximization (M) step.</p>
<p class="mce-root">The Spark MLlib implementation uses the expectation-maximization algorithm to induce the maximum-likelihood model from a given a set of data points. The current implementation uses the following parameters:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">K</strong> is the number of desired clusters to cluster your data points</li>
<li class="mce-root1"><strong class="calibre1">ConvergenceTol</strong> is the maximum change in log-likelihood at which we consider convergence achieved.</li>
<li class="mce-root1"><strong class="calibre1">MaxIterations</strong> is the maximum number of iterations to perform without reaching the convergence point.</li>
<li class="mce-root1"><strong class="calibre1">InitialModel</strong> is an optional starting point from which to start the EM algorithm. If this parameter is omitted, a random starting point will be constructed from the data.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">An example of clustering using GMM with Spark MLlib</h1>
                
            
            
                
<p class="mce-root">In the previous sections, we saw how to cluster the similar houses together to determine the neighborhood. Using GMM, it is also possible to cluster the houses toward finding the neighborhood except the model training that takes different training parameters as follows:</p>
<pre class="calibre19">
val K = 5 <br class="title-page-name"/>val maxIteration = 20 <br class="title-page-name"/>val model = new GaussianMixture()<br class="title-page-name"/>                .setK(K)// Number of desired clusters<br class="title-page-name"/>                .setMaxIterations(maxIteration)//Maximum iterations<br class="title-page-name"/>                .setConvergenceTol(0.05) // Convergence tolerance. <br class="title-page-name"/>                .setSeed(12345) // setting seed to disallow randomness<br class="title-page-name"/>                .run(landRDD) // fit the model using the training set
</pre>
<p class="mce-root">You should refer to the previous example and just reuse the previous steps of getting the trained data. Now to evaluate the model's performance, GMM does not provide any performance metrics like WCSS as a cost function. However, GMM provides some performance metrics like mu, sigma, and weight. These parameters signify the maximum likelihood among different clusters (five clusters in our case). This can be demonstrated as follows:</p>
<pre class="calibre19">
// output parameters of max-likelihood model<br class="title-page-name"/>for (i &lt;- 0 until model.K) {<br class="title-page-name"/>  println("Cluster " + i)<br class="title-page-name"/>  println("Weight=%f\nMU=%s\nSigma=\n%s\n" format(model.weights(i),   <br class="title-page-name"/>           model.gaussians(i).mu, model.gaussians(i).sigma))<br class="title-page-name"/>}
</pre>
<p class="mce-root">You should observe the following output:</p>
<div><img class="image-border194" src="img/00154.jpeg"/></div>
<div><strong class="calibre1">Figure 9:</strong> Cluster 1<strong class="calibre1"><br class="title-page-name"/></strong></div>
<div><img class="image-border195" src="img/00017.jpeg"/></div>
<div><strong class="calibre1">Figure 10:</strong> Cluster 2<strong class="calibre1"><br class="title-page-name"/></strong></div>
<div><img class="image-border196" src="img/00240.jpeg"/></div>
<div><strong class="calibre1">Figure 11:</strong> Cluster 3<strong class="calibre1"><br class="title-page-name"/></strong></div>
<div><img class="image-border197" src="img/00139.jpeg"/></div>
<div><strong class="calibre1">Figure 12:</strong> Cluster 4<strong class="calibre1"><br class="title-page-name"/></strong></div>
<div><img class="image-border198" src="img/00002.jpeg"/></div>
<div><strong class="calibre1">Figure 13:</strong> Cluster 5<strong class="calibre1"><br class="title-page-name"/></strong></div>
<p class="mce-root">The weight of clusters 1 to 4 signifies that these clusters are homogeneous and significantly different compared with cluster 5.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Determining number of clusters</h1>
                
            
            
                
<p class="mce-root">The beauty of clustering algorithms like K-means algorithm is that it does the clustering on the data with an unlimited number of features. It is a great tool to use when you have a raw data and would like to know the patterns in that data. However, deciding the number of clusters prior to doing the experiment might not be successful but may sometimes lead to an overfitting or underfitting problem. On the other hand, one common thing to all three algorithms (that is, K-means, bisecting K-means, and Gaussian mixture) is that the number of clusters must be determined in advance and supplied to the algorithm as a parameter. Hence, informally, determining the number of clusters is a separate optimization problem to be solved.</p>
<p class="mce-root">In this section, we will use a heuristic approach based on the Elbow method. We start from K = 2 clusters, and then we ran the K-means algorithm for the same data set by increasing K and observing the value of cost function <strong class="calibre1">Within-Cluster Sum of Squares</strong> (<strong class="calibre1">WCSS</strong>). At some point, a big drop in cost function can be observed, but then the improvement became marginal with the increasing value of K. As suggested in cluster analysis literature, we can pick the K after the last big drop of WCSS as an optimal one.</p>
<p class="mce-root">By analysing below parameters, you can find out the performance of K-means:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Betweenness:</strong> This is the between sum of squares also called as <em class="calibre8">intracluster similarity.</em></li>
<li class="mce-root1"><strong class="calibre1">Withiness:</strong> This is the within sum of square also called <em class="calibre8">intercluster similarity.</em></li>
<li class="mce-root1"><strong class="calibre1">Totwithinss:</strong> This is the sum of all the withiness of all the clusters also called <em class="calibre8">total intracluster similarity.</em></li>
</ul>
<p class="mce-root">It is to be noted that a robust and accurate clustering model will have a lower value of withiness and a higher value of betweenness. However, these values depend on the number of clusters, that is, K that is chosen before building the model.</p>
<p class="mce-root">Now let us discuss how to take advantage of the Elbow method to determine the number of clusters. As shown in the following, we calculated the cost function WCSS as a function of a number of clusters for the K-means algorithm applied to home data based on all the features. It can be observed that a big drop occurs when K = 5. Therefore, we chose the number of clusters as 5, as shown in <em class="calibre8">Figure 10</em>. Basically, this is the one after the last big drop.</p>
<div><img class="image-border199" src="img/00303.jpeg"/></div>
<div><strong class="calibre1">Figure 14:</strong> Number of clusters as a function of WCSS</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">A comparative analysis between clustering algorithms</h1>
                
            
            
                
<p class="mce-root">Gaussian mixture is used mainly for expectation minimization, which is an example of an optimization algorithm. Bisecting K-means, which is faster than regular K-means, also produces slightly different clustering results. Below we try to compare these three algorithms. We will show a performance comparison in terms of model building time and the computional cost for each algorithm. As shown in the following code, we can compute the cost in terms of WCSS. The following lines of code can be used to compute the WCSS for the K-means and <strong class="calibre1">b</strong>isecting algorithms:</p>
<pre class="calibre19">
val WCSSS = model.computeCost(landRDD) // land RDD is the training set <br class="title-page-name"/>println("Within-Cluster Sum of Squares = " + WCSSS) // Less is better 
</pre>
<p class="mce-root">For the dataset we used throughout this chapter, we got the following values of WCSS:</p>
<pre class="calibre19">
<strong class="calibre1">Within-Cluster Sum of Squares of Bisecting K-means = 2.096980212594632E11 </strong><br class="title-page-name"/><strong class="calibre1">Within-Cluster Sum of Squares of K-means = 1.455560123603583E12</strong>
</pre>
<p class="mce-root">This means that K-means shows slightly better performance in terms of the compute cost. Unfortunately, we don't have any metrics like WCSS for the GMM algorithm. Now let's observe the model building time for these three algorithms. We can start the system clock before starting model training and stop it immediately after the training has been finished as follows (for K-means):</p>
<pre class="calibre19">
val start = System.currentTimeMillis() <br class="title-page-name"/>val numClusters = 5 <br class="title-page-name"/>val numIterations = 20  <br class="title-page-name"/>val seed = 12345 <br class="title-page-name"/>val runs = 50 <br class="title-page-name"/>val model = KMeans.train(landRDD, numClusters, numIterations, seed) <br class="title-page-name"/>val end = System.currentTimeMillis()<br class="title-page-name"/>println("Model building and prediction time: "+ {end - start} + "ms")
</pre>
<p class="mce-root">For the training set we used throughout this chapter, we got the following values of model building time:</p>
<pre class="calibre19">
<strong class="calibre1">Model building and prediction time for Bisecting K-means: 2680ms </strong><br class="title-page-name"/><strong class="calibre1">Model building and prediction time for Gaussian Mixture: 2193ms </strong><br class="title-page-name"/><strong class="calibre1">Model building and prediction time for K-means: 3741ms</strong>
</pre>
<p class="mce-root">In different research articles, it has been found that the bisecting K-means algorithm has been shown to result in better cluster assignment for data points. Moreover, compared to K-means, bisecting K-means, alos converges well towards global minima. K-means on the other hand, gets stuck in local minima. In other words, using bisecting K-means algorithm, we can avoid the local minima that K-means can suffer from.</p>
<p class="mce-root">Note that you might observe different values of the preceding parameters depending upon your machine's hardware configuration and the random nature of the dataset.</p>
<p>More details analysis is up to the readers from the theoretical views. Interested readers should also refer to Spark MLlib-based clustering techniques at <a href="https://spark.apache.org/docs/latest/mllib-clustering.html" class="calibre21">https://spark.apache.org/docs/latest/mllib-clustering.html</a> to get more insights.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Submitting Spark job for cluster analysis</h1>
                
            
            
                
<p class="mce-root">The examples shown in this chapter can be made scalable for the even larger dataset to serve different purposes. You can package all three clustering algorithms with all the required dependencies and submit them as a Spark job in the cluster. Now use the following lines of code to submit your Spark job of K-means clustering, for example (use similar syntax for other classes), for the Saratoga NY Homes dataset:</p>
<pre class="calibre19">
<strong class="calibre1"># Run application as standalone mode on 8 cores</strong> <br class="title-page-name"/>SPARK_HOME/bin/spark-submit \   <br class="title-page-name"/>--class org.apache.spark.examples.KMeansDemo \   <br class="title-page-name"/>--master local[8] \   <br class="title-page-name"/>KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \   <br class="title-page-name"/>Saratoga_NY_Homes.txt<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Run on a YARN cluster</strong> <br class="title-page-name"/>export HADOOP_CONF_DIR=XXX <br class="title-page-name"/>SPARK_HOME/bin/spark-submit \   <br class="title-page-name"/>--class org.apache.spark.examples.KMeansDemo \   <br class="title-page-name"/>--master yarn \   <br class="title-page-name"/>--deploy-mode cluster \  # can be client for client mode   <br class="title-page-name"/>--executor-memory 20G \   <br class="title-page-name"/>--num-executors 50 \   <br class="title-page-name"/>KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \   <br class="title-page-name"/>Saratoga_NY_Homes.txt<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1"># Run on a Mesos cluster in cluster deploy mode with supervising</strong> <br class="title-page-name"/>SPARK_HOME/bin/spark-submit \  <br class="title-page-name"/>--class org.apache.spark.examples.KMeansDemo \  <br class="title-page-name"/>--master mesos://207.184.161.138:7077 \ # Use your IP aadress   <br class="title-page-name"/>--deploy-mode cluster \   <br class="title-page-name"/>--supervise \   <br class="title-page-name"/>--executor-memory 20G \   <br class="title-page-name"/>--total-executor-cores 100 \   <br class="title-page-name"/>KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar \   <br class="title-page-name"/>Saratoga_NY_Homes.txt
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, we delved even deeper into machine learning and found out how we can take advantage of machine learning to cluster records belonging to a dataset of unsupervised observations. Consequently, you learnt the practical know-how needed to quickly and powerfully apply supervised and unsupervised techniques on available data to new problems through some widely used examples based on the understandings from the previous chapters. The examples we are talking about will be demonstrated from the Spark perspective. For any of the K-means, bisecting K-means, and Gaussian mixture algorithms, it is not guaranteed that the algorithm will produce the same clusters if run multiple times. For example, we observed that running the K-means algorithm multiple times with the same parameters generated slightly different results at each run.</p>
<p class="mce-root">For a performance comparison between K-means and Gaussian mixture, see <em class="calibre8">Jung. et. al and cluster analysis</em> lecture notes. In addition to K-means, bisecting K-means, and Gaussian mixture, MLlib provides implementations of three other clustering algorithms, namely, PIC, LDA, and streaming K-means. One thing is also worth mentioning is that to fine tune clustering analysis, often we need to remove unwanted data objects called outlier or anomaly. But using distance based clustering it's really difficult to identify such data pints. Therefore, other distance metrics other than Euclidean can be used. Nevertheless, these links would be a good resource to start with:</p>
<ol class="calibre14">
<li value="1" class="mce-root1"><a href="https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html" class="calibre10">https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html</a></li>
<li value="2" class="mce-root1"><a href="https://github.com/keiraqz/anomaly-detection" class="calibre10">https://github.com/keiraqz/anomaly-detection</a></li>
<li value="3" class="mce-root1"><a href="https://mapr.com/ebooks/spark/08-unsupervised-anomaly-detection-apache-spark.html" class="calibre10">http://www.dcc.fc.up.pt/~ltorgo/Papers/ODCM.pdf</a></li>
</ol>
<p class="mce-root">In the next chapter, we will dig even deeper into tuning Spark applications for better performance. We will see some best practice to optimize the performance of Spark applications.</p>


            

            
        
    </body></html>