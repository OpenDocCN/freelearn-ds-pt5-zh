<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Collecting and Cleaning Data from the Web</h1></div></div></div><p>One of the most common and useful kitchen tools is a strainer, also called a sieve, a colander, or chinois, the purpose of which is to separate solids from liquids during cooking. In this chapter, we will be building strainers for the data we find on the Web. We will learn how to create several types of programs that can help us find and keep the data we want, while discarding the parts we do not want.</p><p>In this chapter, we will:</p><div><ul class="itemizedlist"><li class="listitem">Understand two options to envision the structure of an HTML page, either (a) as a collection of lines that we can look for patterns in, or (b) as a tree structure containing nodes for which we can identify and collect values.</li><li class="listitem">Try out three methods to parse web pages, one that uses the line-by-line approach (regular expressions-based HTML parsing), and two that use the tree structure approach (Python's BeautifulSoup library and the Chrome browser tool called Scraper).</li><li class="listitem">Implement all three of these techniques on some real-world data; we will practice scraping out the date and time from messages posted to a web forum.</li></ul></div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec27"/>Understanding the HTML page structure</h1></div></div></div><p>A web <a id="id320" class="indexterm"/>page is just a text file that contains some special <a id="id321" class="indexterm"/>markup <strong>elements</strong> (sometimes called HTML <strong>tags</strong>) intended to<a id="id322" class="indexterm"/> indicate to a web browser how the page should look when displayed to the user, for example, if we want a particular word to be displayed in a way that indicates emphasis, we can surround it with <code class="literal">&lt;em&gt;</code> tags like this:</p><p>It is <code class="literal">&lt;em&gt;very important&lt;/em&gt;</code> that you follow these instructions.</p><p>All web pages have these same features; they are made up of text and the text may include tags. There are two main mental models we can employ to extract data from web pages. Both models have their useful aspects. In this section, we will describe the two structural <a id="id323" class="indexterm"/>models, and then in the next section, we will use three different tools for extracting data.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec45"/>The line-by-line delimiter model</h2></div></div></div><p>In the <a id="id324" class="indexterm"/>simplest way of thinking about web pages, we<a id="id325" class="indexterm"/> concentrate on the fact that there are many dozens of HTML elements/tags that are used to organize and display pages on the Web. If we want to extract interesting data from web pages in this simple model, we can use the page text and the<a id="id326" class="indexterm"/> embedded HTML elements themselves as <strong>delimiters</strong>. For instance, in case of the preceding example, we may decide we want to collect everything inside the <code class="literal">&lt;em&gt;</code> tags, or maybe we want to collect everything before an <code class="literal">&lt;em&gt;</code> tag or after an <code class="literal">&lt;/em&gt;</code> tag.</p><p>In this model, we conceive the web page as a collection of largely unstructured text, and the HTML tags (or other features of the text, such as recurring words) help to provide structure which we can use to delimit the parts that we want. Once we have delimiters, we have the ability to strain out the interesting data from the junk.</p><p>For example, here is an excerpt from a real-world HTML page, a chat log from the Django IRC channel. Let's consider how we can use its HTML elements as delimiters to extract the interesting data:</p><div><pre class="programlisting">&lt;div id="content"&gt;
&lt;h2&gt;Sep 13, 2014&lt;/h2&gt;

&lt;a href="/2014/sep/14/"&gt;← next day&lt;/a&gt; Sep 13, 2014  &lt;a href="/2014/sep/12/"&gt;previous day →&lt;/a&gt;

&lt;ul id="ll"&gt;
&lt;li class="le" rel="petisnnake"&gt;&lt;a href="#1574618" name="1574618"&gt;#&lt;/a&gt; &lt;span style="color:#b78a0f;8" class="username" rel="petisnnake"&gt;&amp;lt;petisnnake&amp;gt;&lt;/span&gt; i didnt know that &lt;/li&gt;
...
&lt;/ul&gt;
...
&lt;/div&gt;</pre></div><p>Given this example text, we could use the <code class="literal">&lt;h2&gt;&lt;/h2&gt;</code> tags as delimiters to extract the date of this particular chat log. We could use the <code class="literal">&lt;li&gt;&lt;/li&gt;</code> tags as delimiters for a line of text, and within that line, we can see that <code class="literal">rel=""</code> can be used to extract the username of the chatter. Finally, it appears that all the text extending from the end of <code class="literal">&lt;/span&gt;</code> to the beginning of <code class="literal">&lt;/li&gt;</code> is the actual line message sent to the chat channel by the user.</p><div><h3 class="title"><a id="note11"/>Note</h3><p>These chat <a id="id327" class="indexterm"/>logs are all available online at the Django IRC log website, <a class="ulink" href="http://django-irc-logs.com">http://django-irc-logs.com</a>. This website also provides a keyword search interface to the logs. The ellipses (<code class="literal">…</code>) in the preceding code represent text that has been removed for brevity in this example.</p></div><p>From this<a id="id328" class="indexterm"/> messy text, we are able to use the delimiter<a id="id329" class="indexterm"/> concept to extract three clean pieces of data (the date of log, user, and line message).</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec46"/>The tree structure model</h2></div></div></div><p>Another way <a id="id330" class="indexterm"/>to imagine the textual web page is as <a id="id331" class="indexterm"/>a tree structure made up of HTML elements/tags, each of which is related to the other tags on the page. Each tag is shown as a <strong>node</strong>, and a<a id="id332" class="indexterm"/> tree is made up of all the different nodes in a particular page. A <a id="id333" class="indexterm"/>tag that shows up within another tag in the HTML is considered a <strong>child</strong>, and<a id="id334" class="indexterm"/> the enclosing tag is the <strong>parent</strong>. In the previous example of IRC chat, the HTML excerpt can be shown in a tree diagram that looks like this:</p><div><img src="img/image00273.jpeg" alt="The tree structure model"/></div><p style="clear:both; height: 1em;"> </p><p>If we are able to envision our HTML text as a tree structure, we can use a programming language to build a tree for us. This allows us to pull out our desired text values based on their element name or position in the element list. For example:</p><div><ul class="itemizedlist"><li class="listitem">We <a id="id335" class="indexterm"/>may want the value of a tag<a id="id336" class="indexterm"/> by name (give me the text from the <code class="literal">&lt;h2&gt;</code> node)</li><li class="listitem">We may want all the nodes of a particular type (give me all the <code class="literal">&lt;li&gt;</code> nodes which are inside <code class="literal">&lt;ul&gt;</code> which are inside <code class="literal">&lt;div&gt;</code>)</li><li class="listitem">We may want all the attributes of a given element (give me a list of <code class="literal">rel</code> attributes from the <code class="literal">&lt;li&gt;</code> elements)</li></ul></div><p>In the rest of the chapter, we will put both of these mental models—the line-by-line and the tree structure—into practice with some examples. We will walk through three different methods to extract and clean the data out of HTML pages.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec28"/>Method one – Python and regular expressions</h1></div></div></div><p>In this section, we <a id="id337" class="indexterm"/>will use a simple method to extract the data we want from an HTML page. This method is based on the concept of identifying delimiters in the page and using pattern matching via regular expressions to pull out the data we want.</p><p>You may remember that we experimented a little bit with regular expressions (regex) in <a class="link" title="Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors" href="part0024.xhtml#aid-MSDG2">Chapter 3</a>, <em>Workhorses of Clean Data – Spreadsheets and Text Editors</em>, when we were learning about text editors. In this chapter, some of the concepts will be similar, except we will <a id="id338" class="indexterm"/>write a Python program to find matching text and extract it instead of using a text editor for replacements like we did in that chapter.</p><p>One final note before we start the example, although this regex method is fairly easy to understand, it does have some limitations which could be significant, depending on your particular project. We will describe the limitations of this method in detail at the end of the section.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec47"/>Step one – find and save a Web file for experimenting</h2></div></div></div><p>For this example, we are going to<a id="id339" class="indexterm"/> grab<a id="id340" class="indexterm"/> one of the IRC chat logs previously mentioned, from the Django project. These are publicly available files with a fairly regular structure, so they make a nice target for this project. Go <a id="id341" class="indexterm"/>to the Django IRC log archive at <a class="ulink" href="http://django-irc-logs.com/">http://django-irc-logs.com/</a> and find a date that looks appealing to you. Navigate to the target page for one of the dates and save it to your working directory. You should have a single <code class="literal">.html</code> file when you are done.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec48"/>Step two – look into the file and decide what is worth extracting</h2></div></div></div><p>Since we learned in <a class="link" title="Chapter 2. Fundamentals – Formats, Types, and Encodings" href="part0020.xhtml#aid-J2B82">Chapter 2</a>, <em>Fundamentals – Formats, Types, and Encodings</em>, that <code class="literal">.html</code> files are just text, and <a class="link" title="Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors" href="part0024.xhtml#aid-MSDG2">Chapter 3</a>, <em>Workhorses of Clean Data – Spreadsheets and Text Editors</em>, made us very comfortable with viewing text files in a text editor, this step should be easy. Just open the HTML file in a text editor and look at it. What looks ripe for extracting?</p><p>When I <a id="id342" class="indexterm"/>look in my file, I see several things I want to extract. Right away I see that for each chat comment, there is a line number, a username, and the comment itself. Let's plan on extracting these three things from each chat line.</p><p>The following figure shows the HTML file open in my text editor. I have turned on soft wrapping since some of the lines are quite long (in TextWrangler this option is located in the menu under <strong>View</strong> | <strong>Text Display</strong> | <strong>Soft Wrap Text</strong>). Around line <strong>29</strong> we see the beginning of a list of chat lines, each of which has the three items we are interested in:</p><div><img src="img/image00274.jpeg" alt="Step two – look into the file and decide what is worth extracting"/></div><p style="clear:both; height: 1em;"> </p><p>Our job is therefore to find the features of each line that look the same so we can predictably pull out the same three items from each chat line. Looking at the text, here are some possible rules we can follow to extract each data item accurately and with minimal tweaking:</p><div><ul class="itemizedlist"><li class="listitem">It appears that all three items we want are found within the <code class="literal">&lt;li&gt;</code> tags, which are themselves found inside the <code class="literal">&lt;ul id="ll"&gt;</code> tag. Each <code class="literal">&lt;li&gt;</code> represents one chat message.</li><li class="listitem">Within that message, the line number is located in two places: it follows the string <code class="literal">&lt;a href="#</code> and it is found within the quotation marks following the <code class="literal">name</code> attribute. In the example text shown, the first line number is <code class="literal">1574618</code>.</li><li class="listitem">The <code class="literal">username</code> attribute is found in three places, the first of which is as the value of the <code class="literal">rel</code> attribute of the <code class="literal">li class="le"</code>. Within the <code class="literal">span</code> tag, the <code class="literal">username</code> attribute is found again as the value of the <code class="literal">rel</code> attribute, and it is also found between the <code class="literal">&amp;lt</code>; and <code class="literal">&amp;gt</code>; symbols. In the example text, the first <code class="literal">username</code> is <code class="literal">petisnnake</code>.</li><li class="listitem">The line message is found following the <code class="literal">&lt;/span&gt;</code> tag and before the <code class="literal">&lt;/li&gt;</code> tag. In the example shown, the first line message is <code class="literal">i didnt know that</code>.</li></ul></div><p>Now<a id="id343" class="indexterm"/> that we have the rules about where to find the data items, we can write our program.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec49"/>Step three – write a Python program to pull out the interesting pieces and save them to a CSV file</h2></div></div></div><p>Here is<a id="id344" class="indexterm"/> a short bit of code to open a given IRC log file in the format shown previously, parse out the three pieces we are interested in, and print them to a new CSV file:</p><div><pre class="programlisting">import re
import io

row = []

infile  = io.open('django13-sept-2014.html', 'r', encoding='utf8')
outfile = io.open('django13-sept-2014.csv', 'a+', encoding='utf8')
for line in infile:
    pattern = re.compile(ur'&lt;li class=\"le\" rel=\"(.+?)\"&gt;&lt;a href=\"#(.+?)\" name=\"(.+?)&lt;\/span&gt; (.+?)&lt;/li&gt;', re.UNICODE)
    if pattern.search(line):
        username = pattern.search(line).group(1)
        linenum = pattern.search(line).group(2)
        message = pattern.search(line).group(4)
        row.append(linenum)
        row.append(username)
        row.append(message)
        outfile.write(', '.join(row))
        outfile.write(u'\n')
        row = []
infile.close()</pre></div><p>The trickiest bit of this code is the <code class="literal">pattern</code> line. This line builds the pattern match against which each line of the file will be compared.</p><div><h3 class="title"><a id="tip17"/>Tip</h3><p>Be vigilant. Any time the website developers change the HTML in the page, we run the risk that our carefully constructed regular expression pattern will no longer work. In fact, in the months spent writing this book, the HTML for this page changed at least once!</p></div><p>Each matching target group looks like this:<code class="literal">.+?</code>. There are five of them. Three of these are the items we are interested in (<code class="literal">username</code>, <code class="literal">linenum</code>, and <code class="literal">message</code>), while the other two groups are just junk that we can discard. We will also discard the rest of the web page contents, since that did not match our pattern at all. Our program is like a sieve with exactly<a id="id345" class="indexterm"/> three functional holes in it. The good stuff will flow through the holes, leaving the junk behind.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec50"/>Step four – view the file and make sure it is clean</h2></div></div></div><p>When we<a id="id346" class="indexterm"/> open the new CSV file in a text editor, we can see that the first few lines now look like this:</p><div><pre class="programlisting">1574618, petisnnake, i didnt know that 
1574619, dshap, FunkyBob: ahh, hmm, i wonder if there's a way to do it in my QuerySet subclass so i'm not creating a new manager subclass *only* for get_queryset to do the intiial filtering 
1574620, petisnnake, haven used Django since 1.5</pre></div><p>That looks like a solid result. One thing you may notice is that there is no encapsulation character surrounding the text in the third column. This could prove to be a problem since we have used commas as a delimiter. What if we have commas in our third column? If this worries you, you can either add quotation marks around the third column, or you can use tabs to delimit the columns. To do this, change the first <code class="literal">outfile.write()</code> line to have <code class="literal">\t</code> (tab) as the join character instead of comma. You can also add some whitespace trimming<a id="id347" class="indexterm"/> to the message via <code class="literal">ltrim()</code> to remove any stray characters.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec51"/>The limitations of parsing HTML using regular expressions</h2></div></div></div><p>This regular expressions method seems<a id="id348" class="indexterm"/> pretty straightforward at first, but it has some limitations. First, for new data cleaners, regular expressions can be kind of a pain in the neck to design and perfect. You should definitely plan on spending a lot of time debugging and writing yourself copious documentation. To assist in the generation of regular expressions, I would definitely recommend using a regular expression tester, such as <a class="ulink" href="http://Pythex.org">http://Pythex.org</a>, or just <a id="id349" class="indexterm"/>use your favorite search engine to find one. Make sure you specify that you want a Python regex tester if that is the language you are using.</p><p>Next, you should know in advance that regular expressions are completely dependent on the structure of the web page staying the same in the future. So, if you plan to collect data from a website on a schedule, the regular expressions you write today may not work tomorrow. They will only work if the layout of the page does not change. A single space added between two tags will cause the entire regex to fail and will be extremely difficult to troubleshoot. Keep in mind too that most of the time you have little or no control over website changes, since it is usually not your own website that you are collecting data from!</p><p>Finally, there are many, many cases where it is next-to-impossible to accurately write a regular expression to match a given HTML construct. Regex is powerful but not perfect or infallible. For an amusing take on this issue, I refer you to the famous Stack Overflow answer<a id="id350" class="indexterm"/> that has been upvoted over 4000 times: <a class="ulink" href="http://stackoverflow.com/questions/1732348/">http://stackoverflow.com/questions/1732348/</a>. In this answer, the author humorously expresses the <a id="id351" class="indexterm"/>frustration of so many programmers who try over and over to explain why regex is not a perfect solution to parsing irregular and ever-changing HTML found in web pages.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec29"/>Method two – Python and BeautifulSoup</h1></div></div></div><p>Since<a id="id352" class="indexterm"/> regular expressions have some limitations, we will definitely need more<a id="id353" class="indexterm"/> tools in our data cleaning toolkit. Here, we describe how to extract data from HTML pages using a parse tree-based Python library called <strong>BeautifulSoup</strong>.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec52"/>Step one – find and save a file for experimenting</h2></div></div></div><p>For this<a id="id354" class="indexterm"/> step, we will use the same file as we<a id="id355" class="indexterm"/> did for Method 1: the file from the Django IRC channel. We will search for the same three items. Doing this will make it easy to compare the two methods to each other.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec53"/>Step two – install BeautifulSoup</h2></div></div></div><p>BeautifulSoup is<a id="id356" class="indexterm"/> currently in version 4. This version will work for both Python 2.7 and Python 3.</p><div><h3 class="title"><a id="note12"/>Note</h3><p>If you are using the Enthought Canopy Python environment, simply run <code class="literal">pip install beautifulsoup4</code> in the Canopy Terminal.</p></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec54"/>Step three – write a Python program to extract the data</h2></div></div></div><p>The<a id="id357" class="indexterm"/> three items we are interested in are found within the set of <code class="literal">li</code> tags, specifically those with <code class="literal">class="le"</code>. There are not any other <code class="literal">li</code> tags in this particular file, but let's be specific just in case. Here are the items we want and where to find them in the parse tree:</p><div><ul class="itemizedlist"><li class="listitem">We can extract the username from the <code class="literal">li</code> tag underneath the <code class="literal">rel</code> attribute.</li><li class="listitem">We can get the <code class="literal">linenum</code> value from the <code class="literal">name</code> attribute in the <code class="literal">a</code> tag. The <code class="literal">a</code> tag is also the first item in the contents of the <code class="literal">li</code> tag.<div><h3 class="title"><a id="note13"/>Note</h3><p>Remember that arrays are zero-based so we need to ask for item 0.</p></div><p>In BeautifulSoup, the<a id="id358" class="indexterm"/> <strong>contents</strong> of a tag are the items <a id="id359" class="indexterm"/>underneath that tag in the parse tree. Some other packages will call them <strong>child</strong> items.</p></li><li class="listitem">We can extract the message <a id="id360" class="indexterm"/>as the fourth content item in the <code class="literal">li</code> tag (referenced as array item [3]). We also notice that there is a leading space at the front of every message, so we want to strip that off before saving the data.</li></ul></div><p>Here is the Python code that corresponds to the outline of what we want from the parse tree:</p><div><pre class="programlisting">from bs4 import BeautifulSoup
import io

infile  = io.open('django13-sept-2014.html', 'r', encoding='utf8')
outfile = io.open('django13-sept-2014.csv', 'a+', encoding='utf8')
soup = BeautifulSoup(infile)

row = []
allLines = soup.findAll("li","le")
for line in allLines:
    username = line['rel']
    linenum = line.contents[0]['name']
    message = line.contents[3].lstrip()
    row.append(linenum)
    row.append(username)
    row.append(message)
    outfile.write(', '.join(row))
    outfile.write(u'\n')
    row = []
infile.close()</pre></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec55"/>Step four – view the file and make sure it is clean</h2></div></div></div><p>When <a id="id361" class="indexterm"/>we open the new CSV file in a text editor, we can see that the first few lines now look identical to the ones from Method 1:</p><div><pre class="programlisting">1574618, petisnnake, i didnt know that 
1574619, dshap, FunkyBob: ahh, hmm, i wonder if there's a way to do it in my QuerySet subclass so i'm not creating a new manager subclass *only* for get_queryset to do the intiial filtering 
1574620, petisnnake, haven used Django since 1.5</pre></div><p>Just like with the regular expressions method, if you are worried about commas embedded within the last column, you can encapsulate the column text in quotes or just use tabs to delimit the columns instead.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Method three – Chrome Scraper</h1></div></div></div><p>If you really <a id="id362" class="indexterm"/>do not want to write a program to parse out data, there are several browser-based tools that use a tree structure to allow you to identify and extract the data you are interested in. I think the easiest to use with minimum work is a Chrome <a id="id363" class="indexterm"/>extension<a id="id364" class="indexterm"/> called <strong>Scraper</strong>, created by a <a id="id365" class="indexterm"/>developer called <strong>mnmldave</strong> (real name: <strong>Dave Heaton</strong>).</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec56"/>Step one – install the Scraper Chrome extension</h2></div></div></div><p>Download<a id="id366" class="indexterm"/> the Chrome browser if you do not already have that running. Make sure that you get the correct Scraper extension; there are several extensions that have very similar names. I recommend using the developer's own<a id="id367" class="indexterm"/> GitHub site for this product, available at <a class="ulink" href="http://mnmldave.github.io/scraper/">http://mnmldave.github.io/scraper/</a>. This way you will be able to have the correct scraper tool, rather than trying to search using the Chrome store. From the <a class="ulink" href="http://mmldave.github.io/scraper">http://mmldave.github.io/scraper</a> site, click the link to install the extension from the Google Store, and restart your browser.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec57"/>Step two – collect data from the website</h2></div></div></div><p>Point<a id="id368" class="indexterm"/> your browser to the same web URL we have been using to get the data for the other two web data extraction experiments, one of the Django IRC logs. I have been using the September 13, 2014 log for the examples and screenshots here, so I will go to <a class="ulink" href="http://django-irc-logs.com/2014/sep/13/">http://django-irc-logs.com/2014/sep/13/</a>.</p><p>In my browser, at the time of writing, this page looks like this:</p><div><img src="img/image00275.jpeg" alt="Step two – collect data from the website"/></div><p style="clear:both; height: 1em;"> </p><p>We have three items from this IRC log that we are interested in:</p><div><ul class="itemizedlist"><li class="listitem">The line number (we know from our previous two experiments that this is part of the link underneath the <strong>#</strong> sign)</li><li class="listitem">The username (located between the <strong>&lt;</strong> and <strong>&gt;</strong> symbols)</li><li class="listitem">The actual line message</li></ul></div><p>Scraper allows us to highlight each of these three items in turn and export the values to a Google Spreadsheet, where we can then reassemble them into a single sheet and export as CSV (or do whatever else we want with them). Here is how to do it:</p><div><ol class="orderedlist arabic"><li class="listitem">Use your mouse to highlight the item you want to scrape.</li><li class="listitem">Right-click and choose <strong>Scrape similar…</strong> from the menu. In the following example, I have selected the username <strong>petisnnake</strong> as the one that I want Scraper to use:<div><img src="img/image00276.jpeg" alt="Step two – collect data from the website"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">After<a id="id369" class="indexterm"/> selecting <strong>Scrape similar</strong>, the tool will show you a new window with a collection of all the similar items from the page. The following screenshot shows the entire list of usernames that Scraper found:<div><img src="img/image00277.jpeg" alt="Step two – collect data from the website"/></div><p style="clear:both; height: 1em;"> </p><p>Scraper finds all the similar items based on one sample username.</p></li><li class="listitem">At the <a id="id370" class="indexterm"/>bottom of the window, there is a button labeled <strong>Export to Google Docs…</strong>. Note that depending on your settings, you may have to click to agree to allow access to Google Docs from within Scraper.</li></ol><div></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec58"/>Step three – final cleaning on the data columns</h2></div></div></div><p>Once we<a id="id371" class="indexterm"/> have all the data elements extracted from the page and housed in separate Google Docs, we will need to combine them into one file and do some final cleaning. Here is an example of what the line numbers look like once they have been extracted, but before we have cleaned them:</p><div><img src="img/image00278.jpeg" alt="Step three – final cleaning on the data columns"/></div><p style="clear:both; height: 1em;"> </p><p>We are<a id="id372" class="indexterm"/> not interested in column <strong>A</strong> at all, nor are we interested in the leading <strong>#</strong> symbol. The username and line message data is similar—we want most of it, but we would like to remove some symbols and combine everything into a single Google Spreadsheet.</p><p>Using our find-and-replace techniques from <a class="link" title="Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors" href="part0024.xhtml#aid-MSDG2">Chapter 3</a>, <em>Workhorses of Clean Data – Spreadsheets and Text Editors</em> (namely removing the <strong>#</strong>, <strong>&lt;</strong>, and <strong>&gt;</strong> symbols and pasting the rows into a single sheet), we end up with a single clean dataset that looks like this:</p><div><img src="img/image00279.jpeg" alt="Step three – final cleaning on the data columns"/></div><p style="clear:both; height: 1em;"> </p><p>Scraper is a nice tool for extracting small amounts of data from web pages. It has a handy Google Spreadsheets interface, and can be a quick solution if you do not feel like writing a program to do this work. In the next section, we will tackle a larger project. It may be complicated enough that we will have to implement a few of the concepts from this chapter into a single solution.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec31"/>Example project – Extracting data from e-mail and web forums</h1></div></div></div><p>The<a id="id373" class="indexterm"/> Django IRC logs project was pretty <a id="id374" class="indexterm"/>simple. It was designed to show you the differences between three solid techniques that are commonly used to extract clean data from within HTML pages. The data we extracted included the line number, the username, and the IRC chat message, all of which were easy to find and required almost no additional cleaning. In this new example project, we will consider a case that is conceptually similar, but that will require us to extend the idea of data extraction beyond HTML to two other types of semi-structured text found on the Web: e-mail messages hosted on the Web and web-based discussion forums.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec59"/>The background of the project</h2></div></div></div><p>I was<a id="id375" class="indexterm"/> recently working on a research study about how social media can be used to provide software technical support. Specifically, I was trying to discover whether certain types of software development organizations that make APIs and frameworks should move their technical support for developers to Stack Overflow or whether they should continue to use older media, such as e-mail and web forums. To complete this study, I compared (among other things) how long it took developers to get an answer to their API question via Stack Overflow versus how long it took on older social media such as web forums and e-mail groups.</p><p>In this project, we will work on a small piece of this question. We will download two types of raw data representing the older social media: HTML files from a web forum and e-mail messages from Google Groups. We will write Python code to extract the dates and time of the messages sent to these two support forums. We will then figure out which messages were sent in reply to the others and calculate some basic summary statistics about how long it took each message to get a reply.</p><div><h3 class="title"><a id="tip18"/>Tip</h3><p>If you are wondering why we aren't extracting data for the Stack Overflow portion of the question in this example project, just wait until <a class="link" title="Chapter 9. Stack Overflow Project" href="part0059.xhtml#aid-1O8H61">Chapter 9</a>, <em>Stack Overflow Project</em>. That entire chapter is devoted to creating and cleaning a Stack Overflow database.</p></div><p>This project will be divided into two parts. In Part one, we will extract data from the e-mail archive from a project hosted on Google Groups, and in Part two, we will extract our data from the HTML files of a different project.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec60"/>Part one – cleaning data from Google Groups e-mail</h2></div></div></div><p>Many <a id="id376" class="indexterm"/>software companies have traditionally used e-mail mailing lists or hybrid e-mail-web forums to provide technical support for their products. Google Groups is a popular choice for this service. Users can either send e-mails to the group, or they can read and search the messages in a web browser. However, some companies have moved away from providing technical support to developers via Google Groups (including Google products themselves), and are instead using Stack Overflow. The database product Google BigQuery is one such group that now uses Stack Overflow.</p><div><div><div><div><h3 class="title"><a id="ch05lvl3sec35"/>Step one – collect the Google Groups messages</h3></div></div></div><p>To study <a id="id377" class="indexterm"/>the response times for <a id="id378" class="indexterm"/>questions on the BigQuery Google Group, I first<a id="id379" class="indexterm"/> created a list of the URLs for all the postings in that group. You can find my complete list of URLs on my GitHub site: <a class="ulink" href="https://github.com/megansquire/stackpaper2015/blob/master/BigQueryGGurls.txt">https://github.com/megansquire/stackpaper2015/blob/master/BigQueryGGurls.txt</a>.</p><p>Once we have a list of target URLs, we can write a Python program to download all the e-mails residing in those URLs, and save them to disk. In the following program, my list of URLs has been saved as the file called <code class="literal">GGurls.txt</code>. The <code class="literal">time</code> library is included, so we can take a short <code class="literal">sleep()</code> method in between requests to the Google Groups server:</p><div><pre class="programlisting">import urllib2
import time

with open('GGurls.txt', 'r') as f:
    urls = []
    for url in f:
        urls.append(url.strip())

currentFileNum = 1
for url in urls:
    print("Downloading: {0} Number: {1}".format(url, currentFileNum))
    time.sleep(2)
    htmlFile = urllib2.urlopen(url)
    urlFile = open("msg%d.txt" %currentFileNum,'wb')
    urlFile.write(htmlFile.read())
    urlFile.close()
    currentFileNum = currentFileNum +1</pre></div><p>This program results in 667 files being written to disk.</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec36"/>Step two – extract data from the Google Groups messages</h3></div></div></div><p>We<a id="id380" class="indexterm"/> now have 667 e-mail messages in separate files. Our task is to write a program to read these in one at a time and use one of the techniques from this chapter to extract the pieces of information we need. If we peek inside one of the e-mail messages, we see lots of <strong>headers</strong>, which <a id="id381" class="indexterm"/>store information about the e-mail, or its <strong>metadata</strong>. We <a id="id382" class="indexterm"/>can quickly see the three headers that identify the metadata elements that we need:</p><div><pre class="programlisting">In-Reply-To: &lt;ab71b72a-ef9b-4484-b0cc-a72ecb2a3b85@r9g2000yqd.googlegroups.com&gt;
Date: Mon, 30 Apr 2012 10:33:18 -0700
Message-ID: &lt;CA+qSDkQ4JB+Cn7HNjmtLOqqkbJnyBu=Z1Ocs5-dTe5cN9UEPyA@mail.gmail.com&gt;</pre></div><p>All messages have <code class="literal">Message-ID</code> and <code class="literal">Date</code>, but the <code class="literal">In-Reply-To</code> header will only appear in a message that is a reply to another message. An <code class="literal">In-Reply-To</code> value must be the <code class="literal">Message-ID</code> value of another message.</p><p>The following code shows a regular expression-based solution to extract the <code class="literal">Date</code>, <code class="literal">Message-ID</code>, and <code class="literal">In-Reply-To</code> (if available) values and to create some lists of original and reply messages. Then, the code attempts to calculate the time differences between a message and its replies:</p><div><pre class="programlisting">import os
import re
import email.utils
import time
import datetime
import numpy

originals = {}
replies = {}
timelist = []
        
for filename in os.listdir(os.getcwd()):
    if filename.endswith(".txt"):
        f=open(filename, 'r')
        i=''
        m=''
        d=''
        for line in f:
            irt = re.search('(In\-Reply\-To: &lt;)(.+?)@', line)    
            mid = re.search('(Message\-ID: &lt;)(.+?)@', line)
            dt = re.search('(Date: )(.+?)\r', line)
            if irt: 
                i= irt.group(2) 
            if mid:
                m= mid.group(2)
            if dt:
                d= dt.group(2)
        f.close()
        if i and d:
            replies[i] = d
        if m and d:
            originals[m] = d
               
for (messageid, origdate) in originals.items():
    try:
        if replies[messageid]:
            replydate = replies[messageid]                
            try:
                parseddate = email.utils.parsedate(origdate)
                parsedreply = email.utils.parsedate(replydate)
            except:
                pass
            try:
                # this still creates some malformed (error) times
                timeddate = time.mktime(parseddate)
                timedreply = time.mktime(parsedreply)
            except:
                pass
            try:
                dtdate = datetime.datetime.fromtimestamp(timeddate)
                dtreply = datetime.datetime.fromtimestamp(timedreply)
            except:
                pass
            try:
                difference = dtreply - dtdate
                totalseconds = difference.total_seconds()
                timeinhours =  (difference.days*86400+difference.seconds)/3600
                # this is a hack to take care of negative times
                # I should probably handle this with timezones but alas
                if timeinhours &gt; 1:
                    #print timeinhours
                    timelist.append(timeinhours)
            except:
                pass
    except:
        pass
        
print numpy.mean(timelist)
print numpy.std(timelist)
print numpy.median(timelist)</pre></div><p>In this code, the initial <code class="literal">for</code> loop zips through each message and extracts the three pieces of data we are interested in. (This program does not store these to a separate file or to disk, but <a id="id383" class="indexterm"/>you could add this functionality if you wish to.) This portion of the code also creates two important lists:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">originals[]</code> is<a id="id384" class="indexterm"/> a list of original messages. We make the assumption that these are primarily questions being asked of the list members.</li><li class="listitem"><code class="literal">replies[]</code> is <a id="id385" class="indexterm"/>a list of reply messages. We assume that these are primarily answers to questions asked in another message.</li></ul></div><p>The second <code class="literal">for</code> loop processes each message in the list of original messages, doing the following, if there is a reply to the original message, try to figure out how long that reply took to be sent. We then keep a list of reply times.</p><div><div><div><div><h4 class="title"><a id="ch05lvl4sec24"/>Extraction code</h4></div></div></div><p>For this<a id="id386" class="indexterm"/> chapter, we are mostly interested in the cleaning and extraction portion of the code, so let's look closely at those lines. Here, we process each line of the e-mail file looking for three e-mail headers: <code class="literal">In-Reply-To</code>, <code class="literal">Message-ID</code>, and <code class="literal">Date</code>. We use regex searching and grouping, just like we did in Method 1 earlier in this chapter, to delimit the headers and easily extract the values that follow:</p><div><pre class="programlisting">for line in f:
    irt = re.search('(In\-Reply\-To: &lt;)(.+?)@', line) 
    mid = re.search('(Message\-ID: &lt;)(.+?)@', line)
    dt = re.search('(Date: )(.+?)\r', line)
    if irt: 
        i = irt.group(2) 
    if mid:
        m = mid.group(2)
    if dt:
        d = dt.group(2)</pre></div><p>Why did we decide to <a id="id387" class="indexterm"/>use regex here instead of a tree-based parser? There are two main reasons:</p><div><ol class="orderedlist arabic"><li class="listitem">Because the e-mails we downloaded are not HTML, they cannot easily be described as a tree of nodes with parents and children. Therefore, a parse tree-based solution such as BeautifulSoup is not the best choice.</li><li class="listitem">Because e-mail headers are structured and very predictable (especially the three headers<a id="id388" class="indexterm"/> we are looking for here), a regex solution is acceptable.</li></ol><div></div></div><div><div><div><div><h4 class="title"><a id="ch05lvl4sec25"/>Program output</h4></div></div></div><p>The output of this program is to print three numbers that estimate the mean, standard deviation, and<a id="id389" class="indexterm"/> median time in hours for replies to messages on this Google Group. When I run this code, my results are as follows:</p><div><pre class="programlisting">178.911877395
876.102630872
18.0</pre></div><p>This means that the median response time to a message posted to the BigQuery Google Group was about 18 hours. Now let's consider how to extract similar data from a different source: web forums. Do you think responses to questions in a web forum will be faster, slower, or about the same as a Google Group?</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec61"/>Part two – cleaning data from web forums</h2></div></div></div><p>The <a id="id390" class="indexterm"/>web forums we will study for this project<a id="id391" class="indexterm"/> are from a company called <strong>DocuSign</strong>. They also moved their developer support to Stack Overflow, but they have an archive of their older web-based developer forum still online. I poked around on their website until I found out how to download some of the messages from those old forums. The process shown here is more involved than the Google Groups example, but you will learn a lot about how to collect data automatically.</p><div><div><div><div><h3 class="title"><a id="ch05lvl3sec37"/>Step one – collect some RSS that points us to HTML files</h3></div></div></div><p>The <a id="id392" class="indexterm"/>DocuSign developer forum has thousands of messages on it. We would like to have a list of the URLs for all those messages or discussion threads so that we can write some code to download them all automatically, and extract the reply times efficiently.</p><p>To do this, first <a id="id393" class="indexterm"/>we will need a list of all the URLs for the discussions. I found that the archive of DocuSign's old Dev-Zone developer site is located at <a class="ulink" href="https://community.docusign.com/t5/DevZone-Archives-Read-Only/ct-p/dev_zone">https://community.docusign.com/t5/DevZone-Archives-Read-Only/ct-p/dev_zone</a>.</p><p>The site looks like this in the browser:</p><div><img src="img/image00280.jpeg" alt="Step one – collect some RSS that points us to HTML files"/></div><p style="clear:both; height: 1em;"> </p><p>We <a id="id394" class="indexterm"/>definitely do not want to click through each one of those forums and then click into each message and save it manually. That would take forever, and it would be extremely boring. Is there a better way?</p><p>The DocuSign website's <strong>Help</strong> pages indicate that it is possible to download a <strong>Really Simple Syndication</strong> (<strong>RSS</strong>) file showing the newest threads and messages in each forum. We can <a id="id395" class="indexterm"/>use the RSS files to automatically collect the URLs for many of the discussions on the site. The RSS files we are interested in are the ones relating to the developer support forums only ( not the announcements <a id="id396" class="indexterm"/>or sales forums). These RSS files are available from the following URLs:</p><div><ul class="itemizedlist"><li class="listitem"><a class="ulink" href="https://community.docusign.com/docusign/rss/board?board.id=upcoming_releases">https://community.docusign.com/docusign/rss/board?board.id=upcoming_releases</a></li><li class="listitem"><a class="ulink" href="https://community.docusign.com/docusign/rss/board?board.id=DocuSign_Developer_Connection">https://community.docusign.com/docusign/rss/board?board.id=DocuSign_Developer_Connection</a></li><li class="listitem"><a class="ulink" href="https://community.docusign.com/docusign/rss/board?board.id=Electronic_Signature_API">https://community.docusign.com/docusign/rss/board?board.id=Electronic_Signature_API</a></li><li class="listitem"><a class="ulink" href="https://community.docusign.com/docusign/rss/board?board.id=Java">https://community.docusign.com/docusign/rss/board?board.id=Java</a></li><li class="listitem"><a class="ulink" href="https://community.docusign.com/docusign/rss/board?board.id=php_api">https://community.docusign.com/docusign/rss/board?board.id=php_api</a></li><li class="listitem"><a class="ulink" href="https://community.docusign.com/docusign/rss/board?board.id=dev_other">https://community.docusign.com/docusign/rss/board?board.id=dev_other</a></li><li class="listitem"><a class="ulink" href="https://community.docusign.com/docusign/rss/board?board.id=Ask_A_Development_Question_Board">https://community.docusign.com/docusign/rss/board?board.id=Ask_A_Development_Question_Board</a></li></ul></div><p>Visit each URL in that list in your web browser (or just one, if you are pressed for time). The file is RSS, which will look like semi-structured text with tags, similar to HTML. Save the<a id="id397" class="indexterm"/> RSS as a file on your local system and give each one a <code class="literal">.rss</code> file extension. At the end of this process, you should have at most seven RSS files, one for each preceding URL shown.</p><p>Inside each of these RSS files is metadata describing all the discussion threads on the forum, including the one piece of data that we really want at this stage: the URL for each particular discussion thread. Open one of the RSS files in a text editor and you will be able to spot an example of a URL we are interested in. It looks like this, and inside the file, you will see that there is one of these for each discussion thread:</p><div><pre class="programlisting">&lt;guid&gt;http://community.docusign.com/t5/Misc-Dev-Archive-READ-ONLY/Re-Custom-CheckBox-Tabs-not-marked-when-setting-value-to-quot-X/m-p/28884#M1674&lt;/guid&gt;</pre></div><p>Now, we can write a program to loop through each RSS file, look for these URLs, visit them, and then extract the reply times we are interested in. The next section breaks this down into a series of smaller steps, and then shows a program that does the entire job.</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec38"/>Step two – Extract URLs from RSS; collect and parse HTML</h3></div></div></div><p>In this <a id="id398" class="indexterm"/>step, we will write a program that <a id="id399" class="indexterm"/>will do the following:</p><div><ol class="orderedlist arabic"><li class="listitem">Open<a id="id400" class="indexterm"/> each RSS file that we saved in Step 1.</li><li class="listitem">Every time we see a <code class="literal">&lt;guid&gt;</code> and <code class="literal">&lt;/guid&gt;</code> tag pair, extract the URL inside and add it to a list.</li><li class="listitem">For each URL in the list, download whatever HTML file is at that location.</li><li class="listitem">Read that HTML file and extract the original post time and the reply time from each message.</li><li class="listitem">Calculate how long it took to send a reply with mean, median, and standard deviation, like we did in Part 1.</li></ol><div></div><p>Here is some Python code to handle all these steps. We will go over the extraction parts in detail at the end of the code listing:</p><div><pre class="programlisting">import os
import re
import urllib2
import datetime
import numpy

alllinks = []
timelist = []
for filename in os.listdir(os.getcwd()):
    if filename.endswith('.rss'):
        f = open(filename, 'r')
        linktext = ''
        linkurl = ''
        for line in f:
            # find the URLs for discussion threads
            linktext = re.search('(&lt;guid&gt;)(.+?)(&lt;\/guid&gt;)', line)    
            
            if linktext:
                linkurl= linktext.group(2)
                alllinks.append(linkurl)
        f.close()

mainmessage = ''
reply = ''
maindateobj = datetime.datetime.today()
replydateobj = datetime.datetime.today()
for item in alllinks:
    print "==="
    print "working on thread\n" + item
    response = urllib2.urlopen(item)
    html = response.read() 
    # this is the regex needed to match the timestamp 
    tuples = re.findall('lia-message-posted-on\"&gt;\s+&lt;span class=\"local-date\"&gt;\\xe2\\x80\\x8e(.*?)&lt;\/span&gt;\s+&lt;span class=\"local-time\"&gt;([\w:\sAM|PM]+)&lt;\/span&gt;', html)	
    mainmessage = tuples[0]
    if len(tuples) &gt; 1:
        reply = tuples[1]
    if mainmessage:
        print "main: "
        maindateasstr = mainmessage[0] + " " + mainmessage[1]
        print maindateasstr
        maindateobj = datetime.datetime.strptime(maindateasstr, '%m-%d-%Y %I:%M %p')
    if reply:
        print "reply: "
        replydateasstr = reply[0] + " " + reply[1]
        print replydateasstr
        replydateobj = datetime.datetime.strptime(replydateasstr, '%m-%d-%Y %I:%M %p')

        # only calculate difference if there was a reply 
        difference = replydateobj - maindateobj
        totalseconds = difference.total_seconds()
        timeinhours =  (difference.days*86400+difference.seconds)/3600
        if timeinhours &gt; 1:
            print timeinhours
            timelist.append(timeinhours)
            
print "when all is said and done, in hours:"
print numpy.mean(timelist)
print numpy.std(timelist)
print numpy.median(timelist)</pre></div><div><div><div><div><h4 class="title"><a id="ch05lvl4sec26"/>Program status</h4></div></div></div><p>As the <a id="id401" class="indexterm"/>program works, it prints out status messages so we know what it is working on. The status messages look like this, and there is one of these for each URL that is found in the RSS feed(s):</p><div><pre class="programlisting">===
working on thread
http://community.docusign.com/t5/Misc-Dev-Archive-READ-ONLY/Can-you-disable-the-Echosign-notification-in-Adobe-Reader/m-p/21473#M1156
main: 
06-21-2013 08:09 AM
reply: 
06-24-2013 10:34 AM
74</pre></div><p>In this display, 74 represents the rounded number of hours between the posted time of the first message in the thread and the first reply in the thread (about three days, plus two hours).</p></div><div><div><div><div><h4 class="title"><a id="ch05lvl4sec27"/>Program output</h4></div></div></div><p>At its <a id="id402" class="indexterm"/>conclusion, this program prints out the mean, standard deviation, and median reply times in hours, just like the Part 1 program did for Google Groups:</p><div><pre class="programlisting">when all is said and done, in hours:
695.009009009
2506.66701108
20.0</pre></div><p>It looks like reply time in the DocuSign forum is a tiny bit slower than Google Groups. It is reporting 20 hours compared to Google Groups, which took 18 hours, but at least both numbers are in the same approximate range. Your results might change, since new messages <a id="id403" class="indexterm"/>are getting added all the time.</p></div><div><div><div><div><h4 class="title"><a id="ch05lvl4sec28"/>Extraction code</h4></div></div></div><p>Since we are mostly interested in data extraction, let's look closely at the part of the code <a id="id404" class="indexterm"/>where that happens. Here is the most relevant line of code:</p><div><pre class="programlisting">tuples = re.findall('lia-message-posted-on\"&gt;\s+&lt;span class=\"local-date\"&gt;\\xe2\\x80\\x8e(.*?)&lt;\/span&gt;\s+&lt;span class=\"local-time\"&gt;([\w:\sAM|PM]+)&lt;\/span&gt;', html)</pre></div><p>Just like with some of our previous examples, this code also relies on regular expressions to do its work. However, this regex is pretty messy. Maybe we should have written this with BeautifulSoup? Let's take a look at the original HTML that we are trying to match so that we can understand more about what this code is trying to do and whether we should have done this a different way. What follows is a screenshot of how the page looks in the browser. The times we are interested in have been annotated on the screenshot:</p><div><img src="img/image00281.jpeg" alt="Extraction code"/></div><p style="clear:both; height: 1em;"> </p><p>What does the underlying HTML look like though? That is the part that our program will need to be able to parse. It turns out that the date of the original message is printed in several places on the HTML page, but the date and time combination is only printed once for the <a id="id405" class="indexterm"/>original and once for the reply. Here is the HTML showing how these look (the HTML has been condensed and newlines removed for easier viewing):</p><div><pre class="programlisting">&lt;p class="lia-message-dates lia-message-post-date lia-component-post-date-last-edited" class="lia-message-dates lia-message-post-date"&gt;
&lt;span class="DateTime lia-message-posted-on lia-component-common-widget-date" class="DateTime lia-message-posted-on"&gt;
&lt;span class="local-date"&gt;‎06-18-2013&lt;/span&gt;
&lt;span class="local-time"&gt;08:21 AM&lt;/span&gt;

&lt;p class="lia-message-dates lia-message-post-date lia-component-post-date-last-edited" class="lia-message-dates lia-message-post-date"&gt;
&lt;span class="DateTime lia-message-posted-on lia-component-common-widget-date" class="DateTime lia-message-posted-on"&gt;
&lt;span class="local-date"&gt;‎06-25-2013&lt;/span&gt;
&lt;span class="local-time"&gt;12:11 AM&lt;/span&gt;</pre></div><p>This turns out to be a pretty straightforward problem for regex to solve, since we can write a single regular expression and find all the instances of it for both types of messages. In the code, we state that the first instance we find becomes the original message, and the next one becomes the reply, as follows:</p><div><pre class="programlisting">mainmessage = tuples[0]
if len(tuples) &gt; 1:
    reply = tuples[1]</pre></div><p>We could have used a parse tree-based solution such as BeautifulSoup, but we would have to contend with the fact that the <code class="literal">span</code> class values are identical for both sets of dates, and even the parent element (the <code class="literal">&lt;p&gt;</code> tag) turns out to have the same class. So, this parse tree is substantially more complex than the one shown in Method 2 earlier in the chapter.</p><p>If you really wanted to try to use BeautifulSoup for this extraction, my recommendation would be first to look at the structure of the page using your browser's Developer Tools, for example, in the Chrome browser, you can select the element you are interested in—the date and time in this case—and right-click it, and then choose <strong>Inspect Element</strong>. This will open a Developer Tools panel showing where this piece of data is found in the overall document tree. Little arrows to the left of each HTML element indicate if there are child nodes. You can then decide how to proceed through locating your target element in the parse tree programmatically, and you could make a plan to differentiate it from the other nodes. Since this task is well beyond the scope of this introductory book, I will leave <a id="id406" class="indexterm"/>that as an exercise for the reader.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Summary</h1></div></div></div><p>In this chapter, we discovered a few tried-and-true techniques for separating interesting data from unwanted data. When we make broth in our chef's kitchen, we use a strainer to catch the bones and vegetable husks that we do not want, while allowing the delicious liquid that we <em>do</em> want to flow through the holes in the sieve into our container. The same idea applies when we are extracting data from web pages in our data science kitchen. We want to devise a cleaning plan that allows us to extract what we want, while leaving the rest of the HTML behind.</p><p>Along the way, we reviewed the two main mental models used in extracting data from HTML, namely a line-by-line delimiter approach and the parse tree/nodes model. We then looked into three solid, proven methods to parse HTML pages to extract the data we want: regular expressions, BeautifulSoup, and a Chrome-based point-and-click Scraper tool. Finally, we put together a project that collected and extracted useful data from real-world e-mail and HTML pages.</p><p>Text data such as e-mail and HTML turned out to not be very difficult to clean, but what about binary files? In the next chapter, we will explore how to extract clean data from a much more difficult target: PDF files.</p></div></body></html>