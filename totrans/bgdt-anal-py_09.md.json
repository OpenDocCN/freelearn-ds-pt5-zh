["```py\n    import numpy as np\n    def square_plus(x, c):\n        return np.power(x, 2) + c\n    ```", "```py\n    x = 10\n    c = 100\n    result = square_plus(x, c)\n    print(result)\n    ```", "```py\n    200\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    ```", "```py\n    url = \"https://opendata.socrata.com/api/views/cf4r-dfwe/rows.csv?accessType=DOWNLOAD\"\n    df = pd.read_csv(url)\n    ```", "```py\n    columns = df.columns\n    id_cols = ['State', 'Location', \"Date Posted\", 'Date Collected', 'Sample Type', 'Unit']\n    columns = list(set(columns) - set(id_cols))\n    columns\n    ```", "```py\n    df['Cs-134'] = df['Cs-134'].apply(lambda x: np.nan if x == \"Non-detect\" else x)\n    df.head()\n    ```", "```py\n    df.loc[:, columns] = df.loc[:, columns].applymap(lambda x: np.nan if x == 'Non-detect' else x)\n    df.loc[:, columns] = df.loc[:, columns].applymap(lambda x: np.nan if x == 'ND' else x)\n    ```", "```py\n    df.loc[:, ['State', 'Location', 'Sample Type', 'Unit']] = df.loc[:, ['State', 'Location', 'Sample Type', 'Unit']].applymap(lambda x: x.strip())\n    ```", "```py\n    df.dtypes\n    ```", "```py\n    df['Date Posted'] = pd.to_datetime(df['Date Posted'])\n    df['Date Collected'] = pd.to_datetime(df['Date Collected'])\n    for col in columns:\n        df[col] = pd.to_numeric(df[col])\n    df.dtypes\n    ```", "```py\n    df['Date Posted'] = pd.to_datetime(df['Date Posted'])\n    df['Date Collected'] = pd.to_datetime(df['Date Collected'])\n    for col in columns:\n        df[col] = pd.to_numeric(df[col])\n    df.dtypes\n    ```", "```py\n    df.loc[df.Location == 'San Bernardino'].plot(x='Date Collected', y='I-131')\n    ```", "```py\n    fig, ax = plt.subplots()\n    ax.scatter(x=df['I-131'], y=df['I-132'])\n    _ = ax.set(\n        xlabel='I-131',\n        ylabel='I-132',\n        title='Comparison between concentrations of I-131 and I-132'\n    )\n    ```", "```py\n    %matplotlib inline\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt import numpy as np\n    import pandas as pd\n    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n    df = pd.read_csv(url)\n    ```", "```py\n    column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']\n    ```", "```py\n    df = pd.read_csv(url, names= column_names, delim_whitespace=True)\n    df.head()\n    ```", "```py\n    df.loc[df.horsepower == '?', 'horsepower'] = np.nan\n    df['horsepower'] = pd.to_numeric(df['horsepower'])\n    df['full_date'] = pd.to_datetime(df.year, format='%y')\n    df['year'] = df['full_date'].dt.year\n    ```", "```py\n    df.dtypes\n    ```", "```py\n    df.groupby('year')['horsepower'].mean().plot()\n    ```", "```py\n    %matplotlib inline\n    import pandas as pd\n    import numpy as np\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n    df = pd.read_csv(url)\n    ```", "```py\n    column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']\n    ```", "```py\n    df = pd.read_csv(url, names= column_names, delim_whitespace=True)\n    df.head()\n    ```", "```py\n    fig, ax = plt.subplots()\n    ax.scatter(x = df['horsepower'], y=df['weight'])\n    ```", "```py\n    %matplotlib inline\n    import pandas as pd\n    import numpy as np\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n    df = pd.read_csv(url)\n    ```", "```py\n    column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']\n    ```", "```py\n    df = pd.read_csv(url, names= column_names, delim_whitespace=True)\n    ```", "```py\n    fig, ax = plt.subplots()\n    df.weight.plot(kind='hist', ax=ax)\n    ```", "```py\n    fig.savefig('weight_hist.png')\n    ```", "```py\n    %matplotlib inline\n    import pandas as pd\n    import numpy as np\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n    df = pd.read_csv(url)\n    ```", "```py\n    column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']\n    ```", "```py\n    df = pd.read_csv(url, names= column_names, delim_whitespace=True)\n    ```", "```py\n    df_g = df.groupby(['year', 'cylinders'], as_index=False)\n    ```", "```py\n    df_g = df_g.mpg.mean()\n    ```", "```py\n    df_g = df_g.set_index(df_g.year)\n    ```", "```py\n    import matplotlib.pyplot as plt\n    fig, axes = plt.subplots()\n    ```", "```py\n    df = df.convert_objects(convert_numeric=True)\n    df_g = df.groupby(['year', 'cylinders'], as_index=False).horsepower.mean()\n    df_g = df_g.set_index(df_g.year)\n    ```", "```py\n    fig, axes = plt.subplots()\n    df_g.groupby('cylinders').horsepower.plot(axes=axes, figsize=(12,10))\n    _ = axes.set(\n        title=\"Average car power per year\",\n        xlabel=\"Year\",\n        ylabel=\"Power (horsepower)\"\n\n    )\n    ```", "```py\n    axes.legend(title='Cylinders', fancybox=True)\n    ```", "```py\n    fig.savefig('mpg_cylinder_year.png')\n    ```", "```py\n    rdd_df = spark.read.text(\"/localdata/myfile.txt\").rdd\n    ```", "```py\n    lines = rdd_df.map(lambda line: line[0])\n    ```", "```py\n    lines.collect()\n    ```", "```py\n    lines.count()\n    ```", "```py\n    splits = lines.flatMap(lambda x: x.split(' '))\n    lower_splits = splits.map(lambda x: x.lower())\n    ```", "```py\n    stop_words = ['of', 'a', 'and', 'to']\n    ```", "```py\n    tokens = lower_splits.filter(lambda x: x and x not in stop_words)\n    ```", "```py\n    token_list = tokens.map(lambda x: [x, 1])\n    ```", "```py\n    count = token_list.reduceByKey(add).sortBy(lambda x: x[1], ascending=False)\n    count.collect()\n    ```", "```py\n!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n!wget -q http://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n!pip install -q findspark\nimport os\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\nos.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.2-bin-hadoop2.7\"\n```", "```py\npip install -q findspark\n```", "```py\n    import findspark\n    findspark.init()\n    import pyspark\n    import os\n    ```", "```py\n    sc = pyspark.SparkContext()\n    from pyspark.sql import SQLContext\n    sqlc = SQLContext(sc)\n    from pyspark.sql import *\n    na_schema = Row(\"Name\",\"Subject\",\"Marks\")\n    row1 = na_schema(\"Ankit\", \"Science\",95)\n    row2 = na_schema(\"Ankit\", \"Maths\", 86)\n    row3 = na_schema(\"Preity\", \"Maths\", 92)\n    na_list = [row1, row2, row3]\n    df_na = sqlc.createDataFrame(na_list)\n    type(df_na)\n    ```", "```py\n    pyspark.sql.dataframe.DataFrame\n    ```", "```py\n    df_na.show()\n    ```", "```py\n    data = [(\"Ankit\",\"Science\",95),(\"Preity\",\"Maths\",86),(\"Ankit\",\"Maths\",86)]\n    data_rdd = sc.parallelize(data)\n    type(data_rdd)\n    ```", "```py\n    pyspark.rdd.RDD\n    ```", "```py\n    data_df = sqlc.createDataFrame(data_rdd)\n    data_df.show()\n    ```", "```py\n    df = sqlc.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('mtcars.csv')\n    type(df)\n    ```", "```py\n    pyspark.sql.dataframe.DataFrame\n    ```", "```py\n    df.show(7)\n    ```", "```py\n    df.printSchema()\n    ```", "```py\n    print('number of rows:'+ str(df.count()))\n    print('number of columns:'+ str(len(df.columns)))\n    ```", "```py\n    number of rows:32\n    number of columns:11\n    ```", "```py\n    df.describe().show()\n    ```", "```py\n    df.describe(['mpg','cyl']).show()\n    ```", "```py\n    df_p = df.toPandas()\n    df_p.head(7).to_csv(\"mtcars_head.csv\")\n    ```", "```py\n    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n    !wget -q http://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n    !tar xf spark-2.4.0-bin-hadoop2.7.tgz\n    !pip install -q findspark\n    import os\n    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n    os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n    ```", "```py\n    import findspark\n    findspark.init()\n    import pyspark\n    import os\n    ```", "```py\n    sc = pyspark.SparkContext()\n    from pyspark.sql import SQLContext\n    sqlc = SQLContext(sc)\n    ```", "```py\n    df = sqlc.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('mtcars.csv')\n    df.show(4)\n    ```", "```py\n    data = df\n    new_names = ['mpg_new', 'cyl_new', 'disp_new', 'hp_new', 'drat_new']\n    for i,z in zip(data.columns[0:5],new_names):\n        data = data.withColumnRenamed(str(i),str(z))\n\n    data.columns\n    ```", "```py\n    data = df.select(['cyl','mpg','hp'])\n    data.show(5)\n    ```", "```py\n    data.select('cyl').distinct().count() #3\n    ```", "```py\n    data = data.withColumn('colsum',(df['mpg'] + df['hp']))\n    data = data.withColumn('colproduct',(df['mpg'] * df['hp']))\n    data.show(5)\n    ```", "```py\n    data = data.drop('mpg','hp')\n    data.show(5)\n    ```", "```py\n    data = data.orderBy(data.cyl)\n    data.show(5)\n    ```", "```py\n    data.groupby('cyl').agg({'colsum':'mean'}).show()\n    ```", "```py\n    data.count()#15\n    cyl_avg = data.groupby('cyl').agg({'colsum':'mean'})\n    avg = cyl_avg.agg({'avg(colsum)':'mean'}).toPandas().iloc[0,0]\n    data = data.filter(data.colsum > avg)\n    data.count()\n    data.show(5)\n    ```", "```py\n    data = data.dropDuplicates()\n    data.count()\n    ```", "```py\n    import pandas as pd\n    import os\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    %matplotlib inline\n    ```", "```py\n    df = pd.read_csv('mtcars.csv')\n    df.head()\n    ```", "```py\n    plt.hist(df['mpg'], bins=20)\n    plt.ylabel('Frequency')\n    plt.xlabel('Values')\n    plt.title('Frequency distribution of mpg')\n    plt.show()\n    ```", "```py\n    ## Calculate count of records for each gear\n    data = pd.DataFrame([[3,4,5],df['gear'].value_counts().tolist()]).T\n    data.columns = ['gear','gear_counts']\n    ## Visualising percentage contribution of each gear in data using pie chart\n    plt.pie(data.gear_counts, labels=data.gear, startangle=90, autopct='%.1f%%')\n    plt.title('Percentage contribution of each gear')\n    plt.show()\n    ```", "```py\n    sns.boxplot(x = 'gear', y = 'disp', data = df)\n    plt.show()\n    ```", "```py\n    data = df[['hp']]\n    data.plot(linestyle='-')\n    plt.title('Line Chart for hp')\n    plt.ylabel('Values')\n    plt.xlabel('Row number')\n    plt.show()\n    ```", "```py\n    data = df[['hp','disp', 'mpg']]\n    data.plot(linestyle='-')\n    plt.title('Line Chart for hp, disp & mpg')\n    plt.ylabel('Values')\n    plt.xlabel('Row number')\n    plt.show()\n    ```", "```py\n    import findspark\n    findspark.init()\n    import pyspark\n    import random\n    ```", "```py\n    sc = pyspark.SparkContext(appName = \"chapter5\")\n    ```", "```py\n    from pyspark.sql import SQLContext\n    sqlc = SQLContext(sc)\n    ```", "```py\n    df = sqlc.read.format('com.databricks.spark.csv').options(header = 'true', inferschema = 'true').load('iris.csv')\n    df.show(5)\n    ```", "```py\n    from pyspark.sql.functions import mean\n    avg_sl = df.select(mean('Sepallength')).toPandas()['avg(Sepallength)']\n    ```", "```py\n    y = df\n    y = y.na.fill(float(avg_sl),['Sepallength'])\n    y.describe().show(1)\n    ```", "```py\n    from pyspark.mllib.stat import Statistics\n    import pandas as pd\n    ```", "```py\n    z = y.fillna(1)\n    ```", "```py\n    a = z.drop('Species') \n    features = a.rdd.map(lambda row: row[0:])\n    ```", "```py\n    correlation_matrix = Statistics.corr(features, method=\"pearson\")\n    ```", "```py\n    correlation_df = pd.DataFrame(correlation_matrix)\n    correlation_df.index, correlation_df.columns = a.columns, a.columns\n    correlation_df\n    ```", "```py\n    import pandas as pd\n    dat = y.toPandas()\n    type(dat)\n    ```", "```py\n    pandas.core.frame.DataFrame\n    ```", "```py\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    %matplotlib inline\n    sns.lmplot(x = \"Sepallength\", y = \"Petallength\", data = dat)\n    plt.show()\n    ```", "```py\n    import seaborn as sns\n    sns.lmplot(x = \"Sepallength\", y = \"Petalwidth\", data = dat)\n    plt.show()\n    ```", "```py\n    sns.lmplot(x = \"Petallength\", y = \"Petalwidth\", data = dat)\n    plt.show()\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    import seaborn as sns\n    import time\n    import re\n    import os\n    import matplotlib.pyplot as plt\n    sns.set(style=\"ticks\")\n    # import libraries required for preprocessing\n    import sklearn as sk\n    from scipy import stats\n    from sklearn import preprocessing\n    # set the working directory to the following\n    os.chdir(\"/Users/svk/Desktop/packt_exercises\")\n    # read the downloaded input data (marketing data)\n    df = pd.read_csv('bank.csv', sep=';')\n    ```", "```py\n    numeric_df = df._get_numeric_data()\n    numeric_df.head()\n    ```", "```py\n    numeric_df_array = np.array(numeric_df) # converting to numpy arrays for more efficient computation\n    loop_c = -1\n    col_for_normalization = list()\n    for column in numeric_df_array.T:\n        loop_c+=1\n        x = column\n        k2, p = stats.normaltest(x) \n        alpha = 0.001\n        print(\"p = {:g}\".format(p))\n\n        # rules for printing the normality output\n        if p < alpha:\n            test_result = \"non_normal_distr\"\n            col_for_normalization.append((loop_c)) # applicable if yeo-johnson is used\n\n            #if min(x) > 0: # applicable if box-cox is used\n                #col_for_normalization.append((loop_c)) # applicable if box-cox is used\n            print(\"The null hypothesis can be rejected: non-normal distribution\")\n\n        else:\n            test_result = \"normal_distr\"\n            print(\"The null hypothesis cannot be rejected: normal distribution\")\n    ```", "```py\n    columns_to_normalize = numeric_df[numeric_df.columns[col_for_normalization]]\n    names_col = list(columns_to_normalize)\n    # density plots of the features to check the normality\n    columns_to_normalize.plot.kde(bw_method=3)\n    ```", "```py\n    pt = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n    normalized_columns = pt.fit_transform(columns_to_normalize)\n    normalized_columns = pd.DataFrame(normalized_columns, columns=names_col)\n    ```", "```py\n    normalized_columns.plot.kde(bw_method=3)\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    import seaborn as sns\n    import time\n    import re\n    import os\n    import matplotlib.pyplot as plt\n    sns.set(style=\"ticks\")\n    ```", "```py\n    import sklearn as sk\n    from scipy import stats\n    from sklearn import preprocessing\n    ```", "```py\n    os.chdir(\"/Users/svk/Desktop/packt_exercises\")\n    ```", "```py\n    df = pd.read_csv('bank.csv', sep=';')\n    ```", "```py\n    DV = 'y'\n    df[DV]= df[DV].astype('category')\n    df[DV] = df[DV].cat.codes\n    ```", "```py\n    msk = np.random.rand(len(df)) < 0.8\n    train = df[msk]\n    test = df[~msk]\n    ```", "```py\n    # selecting the target variable (dependent variable) as y\n    y_train = train[DV]\n    ```", "```py\n    train = train.drop(columns=[DV])\n    train.head()\n    ```", "```py\n    numeric_df = train._get_numeric_data()\n    ```", "```py\n    numeric_df_array = np.array(numeric_df)\n    loop_c = -1\n    col_for_normalization = list()\n    for column in numeric_df_array.T:\n        loop_c+=1\n        x = column\n        k2, p = stats.normaltest(x) \n        alpha = 0.001\n        print(\"p = {:g}\".format(p))\n\n        # rules for printing the normality output\n        if p < alpha:\n            test_result = \"non_normal_distr\"\n            col_for_normalization.append((loop_c)) # applicable if yeo-johnson is used\n\n            #if min(x) > 0: # applicable if box-cox is used\n                #col_for_normalization.append((loop_c)) # applicable if box-cox is used\n            print(\"The null hypothesis can be rejected: non-normal distribution\")\n\n        else:\n            test_result = \"normal_distr\"\n            print(\"The null hypothesis cannot be rejected: normal distribution\")\n    ```", "```py\n    pt = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n    ```", "```py\n    columns_to_normalize = numeric_df[numeric_df.columns[col_for_normalization]]\n    names_col = list(columns_to_normalize)\n    ```", "```py\n    columns_to_normalize.plot.kde(bw_method=3)\n    ```", "```py\n    normalized_columns = pt.fit_transform(columns_to_normalize)\n    normalized_columns = pd.DataFrame(normalized_columns, columns=names_col)\n    ```", "```py\n    normalized_columns.plot.kde(bw_method=3)\n    ```", "```py\n    numeric_df_array = np.array(normalized_columns) \n    loop_c = -1\n    for column in numeric_df_array.T:\n        loop_c+=1\n        x = column\n        k2, p = stats.normaltest(x) \n        alpha = 0.001\n        print(\"p = {:g}\".format(p))\n\n        # rules for printing the normality output\n        if p < alpha:\n            test_result = \"non_normal_distr\"\n            print(\"The null hypothesis can be rejected: non-normal distribution\")\n\n        else:\n            test_result = \"normal_distr\"\n            print(\"The null hypothesis cannot be rejected: normal distribution\")\n    ```", "```py\n    columns_to_notnormalize = numeric_df\n    columns_to_notnormalize.drop(columns_to_notnormalize.columns[col_for_normalization], axis=1, inplace=True)\n    ```", "```py\n    numeric_df_normalized = pd.concat([columns_to_notnormalize.reset_index(drop=True), normalized_columns], axis=1)\n    numeric_df_normalized\n    ```", "```py\n    import plotly.graph_objs as go\n    from plotly.plotly import iplot\n    import plotly as py\n    ```", "```py\n    from plotly import __version__\n    from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n    print(__version__)\n    ```", "```py\n    import plotly.plotly as py\n    import plotly.graph_objs as go\n    init_notebook_mode(connected=True)\n    ```", "```py\n    df = pd.read_csv('bank.csv', sep=';')\n    data = [go.Bar(x=df.y,\n                y=df.balance)]\n    py.iplot(data)\n    ```", "```py\npy.iplot([go.Histogram2dContour(x=df.balance, y=df.age, contours=dict(coloring='heatmap')),\n       go.Scatter(x=df.balance, y=df.age, mode='markers', marker=dict(color='red', size=8, opacity=0.3))], show_link=False)\n```", "```py\nplot1 = go.Box(\n    y=df.age,\n    name = 'age of the customers',\n    marker = dict(\n        color = 'rgb(12, 12, 140)',\n    )\n)\npy.iplot([plot1])\n```"]