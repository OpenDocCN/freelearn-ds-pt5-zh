<html><head></head><body><div><div><div><div><h1 class="title"><a id="pref07"/>Preface</h1></div></div></div><p>The volume of data that enterprises acquire every day is increasing exponentially. It is now possible to store these vast amounts of information on low cost platforms such as Hadoop.</p><p>The conundrum these organizations now face is what to do with all this data and how to glean key insights from this data. Thus R comes into picture. R is a very amazing tool that makes it a snap to run advanced statistical models on data, translate the derived models into colorful graphs and visualizations, and do a lot more functions related to data science.</p><p>One key drawback of R, though, is that it is not very scalable. The core R engine can process and work on very limited amount of data. As Hadoop is very popular for Big Data processing, corresponding R with Hadoop for scalability is the next logical step.</p><p>This book is dedicated to R and Hadoop and the intricacies of how data analytics operations of R can be made scalable by using a platform as Hadoop.</p><p>With this agenda in mind, this book will cater to a wide audience including data scientists, statisticians, data architects, and engineers who are looking for solutions to process and analyze vast amounts of information using R and Hadoop.</p><p>Using R with Hadoop will provide an elastic data analytics platform that will scale depending on the size of the dataset to be analyzed. Experienced programmers can then write Map/Reduce modules in R and run it using Hadoop's parallel processing Map/Reduce mechanism to identify patterns in the dataset.</p><div><div><div><div><h1 class="title"><a id="ch00lvl1sec02"/>Introducing R</h1></div></div></div><p>R is an open source software package to perform statistical analysis on data. R is a programming language used by data scientist statisticians and others who need to make statistical analysis of data and glean key insights from data using mechanisms, such as regression, clustering, classification, and text analysis. R is registered under <strong>GNU</strong> (<strong>General Public License</strong>). It was developed by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, which is currently handled by the R Development Core Team. It can be considered as a different implementation of S, developed by Johan Chambers at Bell Labs. There are some important differences, but a lot of the code written in S can be unaltered using the R interpreter engine.</p><p>R provides a wide variety of statistical, machine learning (linear and nonlinear modeling, classic statistical tests, time-series analysis, classification, clustering) and graphical techniques, and is highly extensible. R has various built-in as well as extended functions for statistical, machine learning, and visualization tasks such as:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data extraction</li><li class="listitem" style="list-style-type: disc">Data cleaning</li><li class="listitem" style="list-style-type: disc">Data loading</li><li class="listitem" style="list-style-type: disc">Data transformation</li><li class="listitem" style="list-style-type: disc">Statistical analysis</li><li class="listitem" style="list-style-type: disc">Predictive modeling</li><li class="listitem" style="list-style-type: disc">Data visualization</li></ul></div><p>It is one of the most popular open source statistical analysis packages available on the market today. It is crossplatform, has a very wide community support, and a large and ever-growing user community who are adding new packages every day. With its growing list of packages, R can now connect with other data stores, such as MySQL, SQLite, MongoDB, and Hadoop for data storage activities.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec03"/>Understanding features of R</h1></div></div></div><p>Let's see different useful features of R:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Effective programming language</li><li class="listitem" style="list-style-type: disc">Relational database support</li><li class="listitem" style="list-style-type: disc">Data analytics</li><li class="listitem" style="list-style-type: disc">Data visualization</li><li class="listitem" style="list-style-type: disc">Extension through the vast library of R packages</li></ul></div></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec04"/>Studying the popularity of R</h1></div></div></div><p>The graph provided from KD suggests that R is the most popular language for data analysis and mining:</p><div><img src="img/3282OS_Preface_01.jpg" alt="Studying the popularity of R"/></div><p>The following graph provides details about the total number of R packages released by R users from 2005 to 2013. This is how we explore R users. The growth was exponential in 2012 and it seems that 2013 is on track to beat that.</p><p>R allows performing Data analytics by various statistical and machine learning operations as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Regression</li><li class="listitem" style="list-style-type: disc">Classification</li><li class="listitem" style="list-style-type: disc">Clustering</li><li class="listitem" style="list-style-type: disc">Recommendation</li><li class="listitem" style="list-style-type: disc">Text mining</li></ul></div><div><img src="img/3282OS_Preface_02.jpg" alt="Studying the popularity of R"/></div></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec05"/>Introducing Big Data</h1></div></div></div><p>Big Data has to deal with large and complex datasets that can be structured, semi-structured, or unstructured and will typically not fit into memory to be processed. They have to be processed in place, which means that computation has to be done where the data resides for processing. When we talk to developers, the people actually building Big Data systems and applications, we get a better idea of what they mean about 3Vs. They typically would mention the 3Vs model of Big Data, which are velocity, volume, and variety.</p><p>Velocity refers to the low latency, real-time speed at which the analytics need to be applied. A typical example of this would be to perform analytics on a continuous stream of data originating from a social networking site or aggregation of disparate sources of data.</p><p>Volume refers to the size of the dataset. It may be in KB, MB, GB, TB, or PB based on the type of the application that generates or receives the data.</p><p>Variety refers to the various types of the data that can exist, for example, text, audio, video, and photos.</p><p>Big Data usually includes datasets with sizes. It is not possible for such systems to process this amount of data within the time frame mandated by the business. Big Data volumes are a constantly moving target, as of 2012 ranging from a few dozen terabytes to many petabytes of data in a single dataset. Faced with this seemingly insurmountable challenge, entirely new platforms are called Big Data platforms.</p><div><img src="img/3282OS_Preface_03.jpg" alt="Introducing Big Data"/></div></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec06"/>Getting information about popular organizations that hold Big Data</h1></div></div></div><p>Some of the popular organizations that hold Big Data are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Facebook: It has 40 PB of data and captures 100 TB/day</li><li class="listitem" style="list-style-type: disc">Yahoo!: It has 60 PB of data</li><li class="listitem" style="list-style-type: disc">Twitter: It captures 8 TB/day</li><li class="listitem" style="list-style-type: disc">EBay: It has 40 PB of data and captures 50 TB/day</li></ul></div><p>How much data is considered as Big Data differs from company to company. Though true that one company's Big Data is another's small, there is something common: doesn't fit in memory, nor disk, has rapid influx of data that needs to be processed and would benefit from distributed software stacks. For some companies, 10 TB of data would be considered Big Data and for others 1 PB would be Big Data. So only you can determine whether the data is really Big Data. It is sufficient to say that it would start in the low terabyte range.</p><p>Also, a question well worth asking is, as you are not capturing and retaining enough of your data do you think you do not have a Big Data problem now? In some scenarios, companies literally discard data, because there wasn't a cost effective way to store and process it. With platforms as Hadoop, it is possible to start capturing and storing all that data.</p></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec07"/>Introducing Hadoop</h1></div></div></div><p>Apache Hadoop is an open source Java framework for processing and querying vast amounts of data on large clusters of commodity hardware. Hadoop is a top level Apache project, initiated and led by Yahoo! and Doug Cutting. It relies on an active community of contributors from all over the world for its success.</p><p>With a significant technology investment by Yahoo!, Apache Hadoop has become an enterprise-ready cloud computing technology. It is becoming the industry de facto framework for Big Data processing.</p><p>Hadoop changes the economics and the dynamics of large-scale computing. Its impact can be boiled down to four salient characteristics. Hadoop enables scalable, cost-effective, flexible, fault-tolerant solutions.</p></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec08"/>Exploring Hadoop features</h1></div></div></div><p>Apache Hadoop has two main features:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">HDFS (Hadoop Distributed File System)</li><li class="listitem" style="list-style-type: disc">MapReduce</li></ul></div><div><div><div><div><h2 class="title"><a id="ch00lvl2sec02"/>Studying Hadoop components</h2></div></div></div><p>Hadoop includes an ecosystem of other products built over the core HDFS and MapReduce layer to enable various types of operations on the platform. A few popular Hadoop components are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Mahout</strong>: This is an extensive library of machine learning algorithms.</li><li class="listitem" style="list-style-type: disc"><strong>Pig</strong>: Pig is a high-level language (such as PERL) to analyze large datasets with its own language syntax for expressing data analysis programs, coupled with infrastructure for evaluating these programs.</li><li class="listitem" style="list-style-type: disc"><strong>Hive</strong>: Hive is a data warehouse system for Hadoop that facilitates easy data summarization, ad hoc queries, and the analysis of large datasets stored in HDFS. It has its own SQL-like query language called <strong>Hive Query Language</strong> (<strong>HQL</strong>), which is used to issue query commands to Hadoop.</li><li class="listitem" style="list-style-type: disc"><strong>HBase</strong>: <strong>HBase</strong> (<strong>Hadoop Database</strong>) is a distributed, column-oriented database. HBase uses HDFS for the underlying storage. It supports both batch style computations using MapReduce and atomic queries (random reads).</li><li class="listitem" style="list-style-type: disc"><strong>Sqoop</strong>: Apache Sqoop is a tool designed for efficiently transferring bulk data between Hadoop and Structured Relational Databases. <strong>Sqoop</strong> is an abbreviation for (<strong>SQ</strong>)L to Had(<strong>oop</strong>).</li><li class="listitem" style="list-style-type: disc"><strong>ZooKeper</strong>: ZooKeeper is a centralized service to maintain configuration information, naming, providing distributed synchronization, and group services, which are very useful for a variety of distributed systems.</li><li class="listitem" style="list-style-type: disc"><strong>Ambari</strong>: A web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters, which includes support for Hadoop HDFS, Hadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig, and Sqoop.</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch00lvl2sec03"/>Understanding the reason for using R and Hadoop together</h2></div></div></div><p>I would also say that sometimes the data resides on the HDFS (in various formats). Since a lot of data analysts are very productive in R, it is natural to use R to compute with the data stored through Hadoop-related tools.</p><p>As mentioned earlier, the strengths of R lie in its ability to analyze data using a rich library of packages but fall short when it comes to working on very large datasets. The strength of Hadoop on the other hand is to store and process very large amounts of data in the TB and even PB range. Such vast datasets cannot be processed in memory as the RAM of each machine cannot hold such large datasets. The options would be to run analysis on limited chunks also known as sampling or to correspond the analytical power of R with the storage and processing power of Hadoop and you arrive at an ideal solution. Such solutions can also be achieved in the cloud using platforms such as Amazon EMR.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec09"/>What this book covers</h1></div></div></div><p><a class="link" href="ch01.html" title="Chapter 1. Getting Ready to Use R and Hadoop">Chapter 1</a>, <em>Getting Ready to Use R and Hadoop</em>, gives an introduction as well as the process of installing R and Hadoop.</p><p><a class="link" href="ch02.html" title="Chapter 2. Writing Hadoop MapReduce Programs">Chapter 2</a>, <em>Writing Hadoop MapReduce Programs</em>, covers basics of Hadoop MapReduce and ways to execute MapReduce using Hadoop.</p><p><a class="link" href="ch03.html" title="Chapter 3. Integrating R and Hadoop">Chapter 3</a>, <em>Integrating R and Hadoop</em>, shows deployment and running of sample MapReduce programs for RHadoop and RHIPE by various data handling processes.</p><p><a class="link" href="ch04.html" title="Chapter 4. Using Hadoop Streaming with R">Chapter 4</a>, <em>Using Hadoop Streaming with R</em>, shows how to use Hadoop Streaming with R.</p><p><a class="link" href="ch05.html" title="Chapter 5. Learning Data Analytics with R and Hadoop">Chapter 5</a>, <em>Learning Data Analytics with R and Hadoop</em>, introduces the Data analytics project life cycle by demonstrating with real-world Data analytics problems.</p><p><a class="link" href="ch06.html" title="Chapter 6. Understanding Big Data Analysis with Machine Learning">Chapter 6</a>, <em>Understanding Big Data Analysis with Machine Learning</em>, covers performing Big Data analytics by machine learning techniques with RHadoop.</p><p><a class="link" href="ch07.html" title="Chapter 7. Importing and Exporting Data from Various DBs">Chapter 7</a>, <em>Importing and Exporting Data from Various DBs</em>, covers how to interface with popular relational databases to import and export data operations with R.</p><p>  <a class="link" href="apa.html" title="Appendix A. References">Appendix</a>, <em>References</em>, describes links to additional resources regarding the content of all the chapters being present.</p></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec10"/>What you need for this book</h1></div></div></div><p>As we are going to perform Big Data analytics with R and Hadoop, you should have basic knowledge of R and Hadoop and how to perform the practicals and you will need to have R and Hadoop installed and configured. It would be great if you already have a larger size data and problem definition that can be solved with data-driven technologies, such as R and Hadoop functions.</p></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec11"/>Who this book is for</h1></div></div></div><p>This book is great for R developers who are looking for a way to perform Big Data analytics with Hadoop. They would like all the techniques of integrating R and Hadoop, how to write Hadoop MapReduce, and tutorials for developing and running Hadoop MapReduce within R. Also this book is aimed at those who know Hadoop and want to build some intelligent applications over Big Data with R packages. It would be helpful if readers have basic knowledge of R.</p></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec12"/>Conventions</h1></div></div></div><p>In this book, you will find a number of styles of text that distinguish between different kinds of information. Here are some examples of these styles, and an explanation of their meaning.</p><p>Code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles are shown as follows: "Preparing the <code class="literal">Map()</code> input."</p><p>A block of code is set as follows:</p><div><pre class="programlisting">&lt;property&gt;
&lt;name&gt;mapred.job.tracker&lt;/name&gt;
&lt;value&gt;localhost:54311&lt;/value&gt;
&lt;description&gt;The host and port that the MapReduce job tracker runs
at. If "local", then jobs are run in-process as a single map
and reduce task.
&lt;/description&gt;
&lt;/property&gt;</pre></div><p>Any command-line input or output is written as follows:</p><div><pre class="programlisting">
<strong>// Setting the environment variables for running Java and Hadoop commands</strong>
<strong>export HADOOP_HOME=/usr/local/hadoop</strong>
<strong>export JAVA_HOME=/usr/lib/jvm/java-6-sun</strong>
</pre></div><p><strong>New terms</strong> and <strong>important words</strong> are shown in bold. Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: "Open the <strong>Password</strong> tab. ".</p><div><div><h3 class="title"><a id="note01"/>Note</h3><p>Warnings or important notes appear in a box like this.</p></div></div><div><div><h3 class="title"><a id="tip01"/>Tip</h3><p>Tips and tricks appear like this.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec13"/>Reader feedback</h1></div></div></div><p>Feedback from our readers is always welcome. Let us know what you think about this book—what you liked or may have disliked. Reader feedback is important for us to develop titles that you really get the most out of.</p><p>To send us general feedback, simply send an e-mail to <code class="email">&lt;<a class="email" href="mailto:feedback@packtpub.com">feedback@packtpub.com</a>&gt;</code>, and mention the book title via the subject of your message.</p><p>If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, see our author guide on <a class="ulink" href="http://www.packtpub.com/authors">www.packtpub.com/authors</a>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch00lvl1sec14"/>Customer support</h1></div></div></div><p>Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.</p><div><div><div><div><h2 class="title"><a id="ch00lvl2sec04"/>Downloading the example code</h2></div></div></div><p>You can download the example code files for all Packt books you have purchased from your account at <a class="ulink" href="http://www.packtpub.com">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div><div><div><div><div><h2 class="title"><a id="ch00lvl1sec014"/>Errata</h2></div></div></div><p>Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be grateful if you would report this to us. By doing so, you can save other readers from frustration and help us improve subsequent versions of this book. If you find any errata, please report them by visiting <a class="ulink" href="http://www.packtpub.com/submit-errata">http://www.packtpub.com/submit-errata</a>, selecting your book, clicking on the <strong>errata</strong> <strong>submission</strong> <strong>form</strong> link, and entering the details of your errata. Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title. Any existing errata can be viewed by selecting your title from <a class="ulink" href="http://www.packtpub.com/support">http://www.packtpub.com/support</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch00lvl2sec05"/>Piracy</h2></div></div></div><p>Piracy of copyright material on the Internet is an ongoing problem across all media. At Packt, we take the protection of our copyright and licenses very seriously. If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.</p><p>Please contact us at <code class="email">&lt;<a class="email" href="mailto:copyright@packtpub.com">copyright@packtpub.com</a>&gt;</code> with a link to the suspected pirated material.</p><p>We appreciate your help in protecting our authors, and our ability to bring you valuable content.</p></div><div><div><div><div><h2 class="title"><a id="ch00lvl2sec06"/>Questions</h2></div></div></div><p>You can contact us at <code class="email">&lt;<a class="email" href="mailto:questions@packtpub.com">questions@packtpub.com</a>&gt;</code> if you are having a problem with any aspect of the book, and we will do our best to address it.</p></div></div></body></html>