<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Speaking the Lingua Franca – Data Conversions</h1></div></div></div><p>Last summer, I took a cheese-making class at a local cooking school. One of the first things we made was ricotta cheese. I was thrilled to learn that ricotta can be made in about an hour using just milk and buttermilk, and that buttermilk itself can be made from milk and lemon juice. In a kitchen, ingredients are constantly transformed into other ingredients, which will in turn be transformed into delicious meals. In our data science kitchen, we will routinely perform conversions from one data format to another. We might need to do this in order to perform various analyses, when we want to merge datasets together, or if we need to store a dataset in a new way.</p><p><strong>A lingua franca</strong> is<a id="id256" class="indexterm"/> a language that is adopted as a common standard in a conversation between speakers of different languages. In converting data, there are several data formats that can serve as a common standard. We covered some of these in <a class="link" title="Chapter 2. Fundamentals – Formats, Types, and Encodings" href="part0020.xhtml#aid-J2B82">Chapter 2</a>, <em>Fundamentals – Formats, Types, and Encodings</em>. JSON and CSV are two of the most common. In this chapter, we will spend some time learning:</p><div><ul class="itemizedlist"><li class="listitem">How to perform some quick conversions into JSON and CSV from software tools and languages (Excel, Google Spreadsheets, and phpMyAdmin).</li><li class="listitem">How to write Python and PHP programs to generate different text formats and convert between them.</li><li class="listitem">How to implement data conversions in order to accomplish a real<code class="literal">-</code>world task. In this project, we will download a friend network from Facebook using the netvizz software, and we will clean the data and convert it into the JSON format needed to build a visualization of your social network in D3. Then, we will clean the data in a different way, converting it into the Pajek format needed by<a id="id257" class="indexterm"/> the social network package called <strong>networkx</strong>.</li></ul></div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec22"/>Quick tool-based conversions</h1></div></div></div><p>One of the<a id="id258" class="indexterm"/> quickest and easiest ways to convert a small to medium amount of data is just to ask whatever software tool you are using to do it for you. Sometimes, the application you are using will already have the option to convert the data into the format you want. Just as with the tips and tricks in <a class="link" title="Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors" href="part0024.xhtml#aid-MSDG2">Chapter 3</a>, <em>Workhorses of Clean Data – Spreadsheets and Text Editors</em>, we want to take advantage of these hidden features in our tools, if at all possible. If you have too much data for an application-based conversion, or if the particular conversion you want is not available, we will cover programmatic solutions in the upcoming sections, <em>Converting with PHP</em> and <em>Converting with Python</em>.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec29"/>Spreadsheet to CSV</h2></div></div></div><p>Saving a<a id="id259" class="indexterm"/> spreadsheet as a delimited file is quite straightforward. Both Excel and Google spreadsheets have <strong>File</strong> menu options for <strong>Save As</strong>; in this option, select <strong>CSV (MS DOS)</strong>. Additionally, Google Spreadsheets has the options to save as an Excel file and save as a tab-delimited file. There are a few limitations with saving something as CSV:</p><div><ul class="itemizedlist"><li class="listitem">In both Excel and Google Spreadsheets, when you use the <strong>Save As</strong> feature, only the current sheet will be saved. This is because, by nature, a CSV file describes only one set of data; therefore, it cannot have multiple sheets in it. If you have a multiple-sheet spreadsheet, you will need to save each sheet as a separate CSV file.</li><li class="listitem">In both these tools, there are relatively few options for how to customize the CSV file, for example, Excel saves the data with commas as the separator (which makes sense as it is a CSV file) and gives no options to enclose data values in quotation marks or for different line terminators.</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec30"/>Spreadsheet to JSON</h2></div></div></div><p>JSON is <a id="id260" class="indexterm"/>a little trickier to contend with than<a id="id261" class="indexterm"/> CSV. Excel does not have an easy JSON converter, though there are several converter tools online that purport to convert CSV files for you into JSON.</p><p>Google Spreadsheets, however, has a JSON converter available via a URL. There are a few downsides to this method, the first of which is that you have to publish your document to the Web (at least temporarily) in order to access the JSON version of it. You will also have to customize the URL with some very long numbers that identify your spreadsheet. It also produces a lot of information in the JSON dump—probably more than you will want or need. Nonetheless, here are some step-by-step instructions to convert a<a id="id262" class="indexterm"/> Google Spreadsheet into its JSON representation.</p><div><div><div><div><h3 class="title"><a id="ch04lvl3sec33"/>Step one – publish Google spreadsheet to the Web</h3></div></div></div><p>After your Google<a id="id263" class="indexterm"/> spreadsheet is created and saved, select <strong>Publish to the Web</strong> from the <strong>File</strong> menu. Click through the subsequent dialogue boxes (I took all the default selections for mine). At this point, you will be ready to access the JSON for this file via a URL.</p></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec34"/>Step two – create the correct URL</h3></div></div></div><p>The URL <a id="id264" class="indexterm"/>pattern to create JSON from a published Google spreadsheet looks like this:</p><p><a class="ulink" href="http://spreadsheets.google.com/feeds/list/key/sheet/public/basic?alt=json">http://spreadsheets.google.com/feeds/list/key/sheet/public/basic?alt=json</a></p><p>There <a id="id265" class="indexterm"/>are three parts of this URL that you will need to alter to match your specific spreadsheet file:</p><div><ul class="itemizedlist"><li class="listitem"><strong>list</strong>: (optional) You can change <code class="literal">list</code> to, say, cells if you would prefer to see each cell<a id="id266" class="indexterm"/> listed separately with its reference (A1, A2, and so on) in the JSON file. If you want each row as an entity, leave <code class="literal">list</code> in the URL.</li><li class="listitem"><strong>key</strong>: Change <code class="literal">key</code> in this URL to match the long, unique number that Google internally<a id="id267" class="indexterm"/> uses to represent your file. In the URL of your spreadsheet, as you are looking at it in the browser, this key is shown as a long identifier between two slashes, just after the <strong>/spreadsheets/d</strong> portion of the URL, shown as follows:<div><img src="img/image00268.jpeg" alt="Step two – create the correct URL"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem"><strong>sheet</strong>: Change<a id="id268" class="indexterm"/> the word sheet in the sample URL to <code class="literal">od6</code> to indicate that you are interested in converting the first sheet.<div><h3 class="title"><a id="note08"/>Note</h3><p>What does <code class="literal">od6</code> mean? Google uses a code to represent each of the sheets. However, the codes are not strictly in numeric order. There is a lengthy discussion <a id="id269" class="indexterm"/>about the numbering scheme on the question on this Stack Overflow post and its answers: <a class="ulink" href="http://stackoverflow.com/questions/11290337/">http://stackoverflow.com/questions/11290337/</a></p></div></li></ul></div><p>To test this procedure, we can create a Google spreadsheet for the universities and the counts that we generated from the exercise at the end of the example project in <a class="link" title="Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors" href="part0024.xhtml#aid-MSDG2">Chapter 3</a>, <em>Workhorses of Clean Data – Spreadsheets and Text Editors</em>. The first three rows of this spreadsheet look like this:</p><div><table border="1"><colgroup><col/><col/></colgroup><tbody><tr><td valign="top">
<p>Yale University</p>
</td><td valign="top">
<p>26</p>
</td></tr><tr><td valign="top">
<p>Princeton University</p>
</td><td valign="top">
<p>25</p>
</td></tr><tr><td valign="top">
<p>Cornell University</p>
</td><td valign="top">
<p>24</p>
</td></tr></tbody></table></div><p>My URL to <a id="id270" class="indexterm"/>access this file via JSON looks like this:</p><p><a class="ulink" href="http://spreadsheets.google.com/feeds/list/1mWIAk_5KNoQHr4vFgPHdm7GX8Vh22WjgAUYYHUyXSNM/od6/public/basic?alt=json">http://spreadsheets.google.com/feeds/list/1mWIAk_5KNoQHr4vFgPHdm7GX8Vh22WjgAUYYHUyXSNM/od6/public/basic?alt=json</a></p><p>Pasting <a id="id271" class="indexterm"/>this URL into the browser yields a JSON representation of the data. It has 231 entries in it, each of which looks like the following snippet. I have formatted this entry with added line breaks for easier reading:</p><div><pre class="programlisting">{
  "id":{
    "$t":"https://spreadsheets.google.com/feeds/list/1mWIAk_5KN oQHr4vFgPHdm7GX8Vh22WjgAUYYHUyXSNM/od6/public/basic/cokwr"
  },
  "updated":{"$t":"2014-12-17T20:02:57.196Z"},
  "category":[{
    "scheme":"http://schemas.google.com/spreadsheets/2006",
    "term"  :"http://schemas.google.com/spreadsheets/2006#list"
  }],
  "title":{
    "type":"text",
    "$t"  :"Yale University "
  },
  "content":{
    "type":"text",
    "$t"  :"_cokwr: 26"
  },
  "link": [{
    "rel" :"self",
    "type":"application/atom+xml",
    "href":"https://spreadsheets.google.com/feeds/list/1mWIAk_5KN oQHr4vFgPHdm7GX8Vh22WjgAUYYHUyXSNM/od6/public/basic/cokwr"
  }]
}</pre></div><p>Even with my reformatting, this JSON is not very pretty, and many of these name-value pairs will be uninteresting to us. Nonetheless, we have successfully generated a functional JSON. If we are using a program to consume this JSON, we will ignore all the extraneous information <a id="id272" class="indexterm"/>about the spreadsheet itself and just go after the title and content entities and the <code class="literal">$t</code> values (<code class="literal">Yale University</code> and <code class="literal">_cokwr: 26</code>, in this case). These values are highlighted in the JSON shown in the preceding example. If you are wondering whether there is a way to go from a spreadsheet to CSV to JSON, the answer is yes. We will cover how to do exactly that in the <em>Converting with PHP</em> and <em>Converting with Python</em> sections later in this chapter.</p></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec31"/>SQL to CSV or JSON using phpMyAdmin</h2></div></div></div><p>In this<a id="id273" class="indexterm"/> section, we'll discuss <a id="id274" class="indexterm"/>two options for writing JSON and CSV directly from a database, MySQL in our case, without using any programming.</p><p>First, phpMyAdmin is a very common web-based frontend for MySQL databases. If you are using a modern version of this tool, you will be able to export an entire table or the results of a query as a CSV or JSON file. Using the same enron database we first visited in <a class="link" title="Chapter 1. Why Do You Need Clean Data?" href="part0014.xhtml#aid-DB7S1">Chapter 1</a>, <em>Why Do You Need Clean Data?</em>, consider the following screenshot of the <strong>Export</strong> tab, with <strong>JSON</strong> selected as the target format for the entire <strong>employeelist</strong> table (CSV is also available in this select box):</p><div><img src="img/image00269.jpeg" alt="SQL to CSV or JSON using phpMyAdmin"/><div><p>PhpMyAdmin JSON export for entire tables</p></div></div><p style="clear:both; height: 1em;"> </p><p>The<a id="id275" class="indexterm"/> process to export the results<a id="id276" class="indexterm"/> of a query is very similar, except that instead of using the <strong>Export</strong> tab on the top of the screen, run the SQL query and then use the <strong>Export</strong> option under <strong>Query results operations</strong> at the bottom of the page, shown as follows:</p><div><img src="img/image00270.jpeg" alt="SQL to CSV or JSON using phpMyAdmin"/><div><p>PhpMyAdmin can export the results of a query as well</p></div></div><p style="clear:both; height: 1em;"> </p><p>Here is a simple query we can run on the <code class="literal">employeelist</code> table to test this process:</p><div><pre class="programlisting">SELECT concat(firstName,  " ", lastName) as name, email_id
FROM employeelist
ORDER BY lastName;</pre></div><p>When <a id="id277" class="indexterm"/>we export the results <a id="id278" class="indexterm"/>as JSON, phpMyAdmin shows us 151 values formatted like this:</p><div><pre class="programlisting">{
  "name": "Lysa Akin",
  "email_id": "lysa.akin@enron.com"
}</pre></div><p>The phpMyAdmin tool is a good one, and it is effective for converting moderate amounts of data<a id="id279" class="indexterm"/> stored in MySQL, especially as the results of a query. If you are using a different RDBMS, your SQL interface will likely have a few formatting options of its own that you should explore.</p><p>Another strategy is to bypass phpMyAdmin entirely and just use your MySQL command line to write out a CSV file that is formatted the way you want:</p><div><pre class="programlisting">SELECT concat(firstName,  " ", lastName) as name, email_id
INTO OUTFILE 'enronEmployees.csv'
FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"'
LINES TERMINATED BY '\n'
FROM employeelist;</pre></div><p>This will write a comma-delimited file with the name specified (<code class="literal">employees.csv</code>). It will be written into the current directory.</p><p>What about JSON? There is no very clean way to output JSON with this strategy, so you should either use the phpMyAdmin solution shown previously, or use a more robust solution written in PHP or Python. These programmatic solutions are covered in further sections, so keep reading.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec23"/>Converting with PHP</h1></div></div></div><p>In our <a class="link" title="Chapter 2. Fundamentals – Formats, Types, and Encodings" href="part0020.xhtml#aid-J2B82">Chapter 2</a>, <em>Fundamentals – Formats, Types, and Encodings</em>, in a discussion on JSON <a id="id280" class="indexterm"/>numeric formatting, we briefly showed how to use PHP to connect to a database, run a query, build a PHP array from the results, and then print the JSON results to the screen. Here, we will first extend this example to write a file rather than print to the screen and also to write a CSV file. Next, we will show how to use PHP to read in JSON files and convert to CSV files, and vice versa.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec32"/>SQL to JSON using PHP</h2></div></div></div><p>In this<a id="id281" class="indexterm"/> section, we will write a PHP script to connect to the <code class="literal">enron</code> database, run a SQL query, and export is as a JSON-formatted file. Why <a id="id282" class="indexterm"/>write a PHP script for this instead of using phpMyAdmin? Well, this strategy will be useful in cases where we need to perform additional processing on the data before exporting it or where we suspect that we have more data than what a web-based application (such as phpMyAdmin) can run:</p><div><pre class="programlisting">&lt;?php
// connect to db, set up query, run the query
$dbc = mysqli_connect('localhost','username','password','enron')
or die('Error connecting to database!' . mysqli_error());

$select_query = "SELECT concat(firstName,  \" \", lastName) as name, email_id FROM  employeelist ORDER BY lastName";

$select_result = mysqli_query($dbc, $select_query);

if (!$select_result)
    die ("SELECT failed! [$select_query]" .  mysqli_error());

// ----JSON output----
// build a new array, suitable for json
$counts = array();
while($row = mysqli_fetch_array($select_result))
{
// add onto the json array
    array_push($counts, array('name'     =&gt; $row['name'],
    'email_id' =&gt; $row['email_id']));
}
// encode query results array as json
$json_formatted = json_encode($counts);

// write out the json file
file_put_contents("enronEmail.json", $json_formatted);
?&gt;</pre></div><p>This code writes a JSON-formatted output file to the location you specify in the <code class="literal">file_put_contents()</code> line.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec33"/>SQL to CSV using PHP</h2></div></div></div><p>The<a id="id283" class="indexterm"/> following code snippet shows how to use the PHP file output stream to create a CSV-formatted file of the results of a SQL query. Save this code as a <code class="literal">.php</code> file in the script-capable directory on your web server, and then request the file in the browser. It will automatically download a CSV file with the correct values in it:</p><div><pre class="programlisting">&lt;?php
// connect to db, set up query, run the query
  $dbc = mysqli_connect('localhost','username','password','enron')
  or die('Error connecting to database!' . mysqli_error());

$select_query = "SELECT concat(firstName,  \" \", lastName) as name, email_id FROM  employeelist ORDER BY lastName";

$select_result = mysqli_query($dbc, $select_query);

if (!$select_result)
    die ("SELECT failed! [$select_query]" .  mysqli_error());

// ----CSV output----
// set up a file stream
$file = fopen('php://output', 'w');
if ($file &amp;&amp; $select_result)
{
    header('Content-Type: text/csv');
    header('Content-Disposition: attachment;
    filename="enronEmail.csv"');
    // write each result row to the file in csv format
    while($row = mysqli_fetch_assoc($select_result))
    {
      fputcsv($file, array_values($row));
    }
}
?&gt;</pre></div><p>The <a id="id284" class="indexterm"/>results are formatted as follows (these are the first three lines only):</p><div><pre class="programlisting">"Lysa Akin",lysa.akin@enron.com
"Phillip Allen",k..allen@enron.com
"Harry Arora",harry.arora@enron.com</pre></div><p>If you are wondering whether Phillip's e-mail is really supposed to have two dots in it, we can run a quick query to find out how many of Enron's e-mails are formatted like that:</p><div><pre class="programlisting">SELECT CONCAT(firstName,  " ", lastName) AS name, email_id
FROM employeelist
WHERE email_id LIKE "%..%"
ORDER BY name ASC;</pre></div><p>It turns out that 24 of the e-mail addresses have double dots like that.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec34"/>JSON to CSV using PHP</h2></div></div></div><p>Here, we <a id="id285" class="indexterm"/>will use PHP to read in a JSON file and convert it to CSV and output a file:</p><div><pre class="programlisting">&lt;?php
// read in the file
$json = file_get_contents("outfile.json");
// convert JSON to an associative array
$array = json_decode ($json, true);
// open the file stream
$file = fopen('php://output', 'w');
header('Content-Type: text/csv');
header('Content-Disposition: attachment;
filename="enronEmail.csv"');
// loop through the array and write each item to the file
foreach ($array as $line)
{
    fputcsv($file, $line);
}
?&gt;</pre></div><p>This code will create a CSV with each line in it, just like the previous example. We should be aware that the <code class="literal">file_get_contents()</code> function reads the file into the memory as a string, so you may find that for extremely large files, you will need to use a combination of the <code class="literal">fread()</code>, <code class="literal">fgets()</code>, and <code class="literal">fclose()</code>PHP functions instead.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec35"/>CSV to JSON using PHP</h2></div></div></div><p>Another <a id="id286" class="indexterm"/>common task is to read in a CSV file and write it out as a JSON file. Most of the time, we have a CSV in which the first row is a header row. The header row lists the column name for each column in the file, and we would like each item in the header row to become the keys for the JSON-formatted version of the file:</p><div><pre class="programlisting">&lt;?php
$file = fopen('enronEmail.csv', 'r');
$headers = fgetcsv($file, 0, ',');
$complete = array();

while ($row = fgetcsv($file, 0, ','))
{
    $complete[] = array_combine($headers, $row);
}
fclose($file);
$json_formatted = json_encode($complete);
file_put_contents('enronEmail.json',$json_formatted);
?&gt;</pre></div><p>The result <a id="id287" class="indexterm"/>of this code on the <code class="literal">enronEmail.csv</code> file created earlier, with a header row, is as follows:</p><div><pre class="programlisting">[{"name":"Lysa Akin","email_id":"lysa.akin@enron.com"},
{"name":"Phillip Allen","email_id":"k..allen@enron.com"},
{"name":"Harry Arora","email_id":"harry.arora@enron.com"}…]</pre></div><p>For this example, of the 151 results in the actual CSV file, only the first three rows are shown.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec24"/>Converting with Python</h1></div></div></div><p>In this <a id="id288" class="indexterm"/>section, we describe a variety of ways to manipulate CSV into JSON, and vice versa, using Python. In these examples, we will explore different ways to accomplish this goal, both using specially installed libraries and using more plain-vanilla Python code.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec36"/>CSV to JSON using Python</h2></div></div></div><p>We <a id="id289" class="indexterm"/>have found several ways to convert CSV files to JSON using Python. The first of these uses the built-in <code class="literal">csv</code> and <code class="literal">json</code> libraries. Suppose we have a CSV file that has rows like this (only the first three rows shown):</p><div><pre class="programlisting">name,email_id
"Lysa Akin",lysa.akin@enron.com
"Phillip Allen",k..allen@enron.com
"Harry Arora",harry.arora@enron.com</pre></div><p>We can write a Python program to read these rows and convert them to JSON:</p><div><pre class="programlisting">import json
import csv

# read in the CSV file
with open('enronEmail.csv') as file:
    file_csv = csv.DictReader(file)
    output = '['
    # process each dictionary row
    for row in file_csv:
      # put a comma between the entities
      output += json.dumps(row) + ','
    output = output.rstrip(',') + ']'
# write out a new file to disk
f = open('enronEmailPy.json','w')
f.write(output)
f.close()</pre></div><p>The <a id="id290" class="indexterm"/>resulting JSON will look like this (only the first two rows are shown):</p><div><pre class="programlisting">[{"email_id": "lysa.akin@enron.com", "name": "Lysa Akin"},
{"email_id": "k..allen@enron.com", "name": "Phillip Allen"},…]</pre></div><p>One nice thing about using this method is that it does not require any special installations of libraries or any command-line access, apart from getting and putting the files you are reading (CSV) and writing (JSON).</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec37"/>CSV to JSON using csvkit</h2></div></div></div><p>The <a id="id291" class="indexterm"/>second method of changing CSV into JSON relies<a id="id292" class="indexterm"/> on a very interesting Python toolkit called <strong>csvkit</strong>. To install csvkit using Canopy, simply launch the Canopy terminal window (you can find it inside Canopy by navigating to <strong>Tools</strong> | <strong>Canopy Terminal</strong>) and then run the <code class="literal">pip install csvkit</code> command. All the dependencies for using csvkit will be installed for you. At this point, you have the option of accessing csvkit via a Python program as a library using <code class="literal">import csvkit</code> or via the command line, as we will do in the following snippet:</p><div><pre class="programlisting"><strong>csvjson enronEmail.csv &gt; enronEmail.json</strong>
</pre></div><p>This command takes a <code class="literal">enronEmail.csv</code> CSV file and transforms it to a JSON <code class="literal">enronEmail.csvkit.json</code> file quickly and painlessly.</p><p>There are several other extremely useful command-line programs that come with the csvkit package, including <code class="literal">csvcut</code>, which can extract an arbitrary list of columns from a CSV file, and <code class="literal">csvformat</code>, which can perform delimiter exchanges on CSV files or alter line endings or similar cleaning procedures. The <code class="literal">csvcut</code> program is particularly helpful if you want to extract just a few columns for processing. For any of these command-line tools, you can redirect its output to a new file. The following command line takes a file called <code class="literal">bigFile.csv</code>, cuts out the first and third column, and saves the result as a new CSV file:</p><div><pre class="programlisting"><strong>csvcut bigFile.csv –c 1,3 &gt; firstThirdCols.csv</strong>
</pre></div><div><h3 class="title"><a id="tip15"/>Tip</h3><p>Additional information about csvkit, including full documentation, downloads, and examples, is available at <a class="ulink" href="http://csvkit.rtfd.org/">http://csvkit.rtfd.org/</a>.</p></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec38"/>Python JSON to CSV</h2></div></div></div><p>It is<a id="id293" class="indexterm"/> quite straightforward to use Python to read in a JSON file and convert it to CSV for processing:</p><div><pre class="programlisting">import json
import csv

with open('enronEmailPy.json', 'r') as f:
    dicts = json.load(f)
out = open('enronEmailPy.csv', 'w')
writer = csv.DictWriter(out, dicts[0].keys())
writer.writeheader()
writer.writerows(dicts)
out.close()</pre></div><p>This program takes a JSON file called <code class="literal">enronEmailPy.json</code> and exports a CSV-formatted version of this file using the keys for the JSON as the header row new file, called <code class="literal">enronEmailPy.csv</code>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec25"/>The example project</h1></div></div></div><p>In this chapter, we have focused on converting data from one format to another, which is a common data cleaning task that will need to be done time and again before the rest of the data<a id="id294" class="indexterm"/> analysis project can be completed. We focused on some very common text formats (CSV and JSON) and common locations for data (files and SQL databases). Now, we are ready to extend our basic knowledge of data conversions with a sample project that will ask us to make conversions between some less standardized— but still text-based—data formats.</p><p>In this project, we want to investigate our <a id="id295" class="indexterm"/>Facebook social network. We will:</p><div><ol class="orderedlist arabic"><li class="listitem">Download our Facebook social network (friends and relationships between them) using<a id="id296" class="indexterm"/> netvizz into a text-based file format called <strong>Graph Description Format</strong> (<strong>GDF</strong>).</li><li class="listitem">Build a graphical representation of a Facebook social network showing the people in our network as nodes and their friendships as connecting lines (called <em>edges</em>) between these nodes. To do this, we will use the D3 JavaScript graphing library. This library expects a JSON representation of the data in the network.</li><li class="listitem">Calculate some metrics about the social network, such as the size of the network (known as the <em>degree</em> of the network) and the shortest path between two people our network. To do this, we will use the <code class="literal">networkx</code> package in Python. This package expects data in a text-based format, called the <strong>Pajek</strong> format.</li></ol><div></div><p>The primary goal of this <a id="id297" class="indexterm"/>project will be to show how to reconcile all these different expected <a id="id298" class="indexterm"/>formats (GDF, Pajek, and JSON) and perform conversions from one format to another. Our secondary goal will be to actually provide enough sample code and guidance to perform a small analysis of our social network.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec39"/>Step one – download Facebook data as GDF</h2></div></div></div><p>For <a id="id299" class="indexterm"/>this step, you will need to be logged into your Facebook account. Use Facebook's search box to find the <a id="id300" class="indexterm"/>netvizz app, or use this URL to directly link to the netvizz app: <a class="ulink" href="https://apps.facebook.com/netvizz/">https://apps.facebook.com/netvizz/</a>.</p><p>Once on the netvizz page, click on <strong>personal network</strong>. The page that follows explains that clicking on the <strong>start</strong> button will provide a downloadable file with two items in it: a GDF format file that lists all your friends and the connections between them and a tab-delimited <strong>Tab Separated Values</strong> (<strong>TSV</strong>) stats file. We are primarily interested in the GDF <a id="id301" class="indexterm"/>file for this project. Click on the <strong>start</strong> button, and on the subsequent page, right-click on the GDF file to save it to your local disk, as shown in the following screenshot:</p><div><img src="img/image00271.jpeg" alt="Step one – download Facebook data as GDF"/><div><p>The netvizz Facebook app allows us to download our social network as a GDF file</p></div></div><p style="clear:both; height: 1em;"> </p><p>It may be helpful to also give the file a shorter name at this point. (I called my file <code class="literal">personal.gdf</code> and saved it in a directory created just for this project.)</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec40"/>Step two – look at the GDF file format in a text editor</h2></div></div></div><p>Open<a id="id302" class="indexterm"/> the file in your text editor (I am using Text Wrangler for this), and note a few things about the format of this file:</p><div><ol class="orderedlist arabic"><li class="listitem">The file is divided into two parts: nodes and edges.</li><li class="listitem">The nodes are found in the first part of the file, preceded by the word <code class="literal">nodedef</code>. The list of nodes is a list of all my friends and some basic facts about them (their gender and their internal Facebook identification number). The nodes are listed in the order of the date when the person joined Facebook.</li><li class="listitem">The second part of the file shows the edges or connections between my friends. Sometimes, these are also called links. This section of the file is preceded by the word <code class="literal">edgedef</code>. The edges describe which of my friends are linked to which other friends.</li></ol><div></div><p>Here is an excerpt of what a nodes section looks like:</p><div><pre class="programlisting">nodedef&gt;name VARCHAR,label VARCHAR,sex VARCHAR,locale VARCHAR,agerank INT
  1234,Bugs Bunny,male,en_US,296
  2345,Daffy Duck,male,en_US,295
  3456,Minnie Mouse,female,en_US,294</pre></div><p>Here is an excerpt of what an edges section looks like. It shows that <code class="literal">Bugs</code> (<code class="literal">1234</code>) and <code class="literal">Daffy</code> (<code class="literal">2345</code>) are friends, and <code class="literal">Bugs</code> is also friends with <code class="literal">Minnie</code> (<code class="literal">3456</code>):</p><div><pre class="programlisting">edgedef&gt;node1 VARCHAR,node2 VARCHAR 
1234,2345
1234,3456
3456,9876</pre></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec41"/>Step three – convert the GDF file into JSON</h2></div></div></div><p>The task<a id="id303" class="indexterm"/> we want to perform is to build a representation of this data as a social network in D3. First, we need to look at the dozens of available examples of D3 to build a social network, such as those available in<a id="id304" class="indexterm"/> the D3 galleries of examples, <a class="ulink" href="https://github.com/mbostock/d3/wiki/Gallery">https://github.com/mbostock/d3/wiki/Gallery</a> and <a class="ulink" href="http://christopheviau.com/d3list/">http://christopheviau.com/d3list/</a>.</p><p>These examples of social network diagrams rely on JSON files. Each JSON file shows nodes and the edges between them. Here is an example of what one of these JSON files should look like:</p><div><pre class="programlisting">{"nodes": [
  {"name":"Bugs Bunny"},
  {"name":"Daffy Duck"},
  {"name":"Minnie Mouse"}],
  "edges": [
  {"source": 0,"target": 2},
  {"source": 1,"target": 3},
  {"source": 2,"target": 3}]}</pre></div><p>The most important thing about this JSON code is to note that it has the same two main chunks as the GDF file did: nodes and edges. The nodes are simply the person's name. The edges are a list of number pairs representing friendship relations. Instead of using the Facebook identification number, though, these pairs use an index for each item in the nodes list, starting with <code class="literal">0</code>.</p><p>We <a id="id305" class="indexterm"/>do not have a JSON file at this point. We only have a GDF file. How will we build this JSON file? When we look closely at the GDF file, we can see that it looks a lot like two CSV files stacked on top of one another. From earlier in this chapter, we know we have several different strategies to convert from CSV to JSON.</p><p>Therefore, we decide to convert GDF to CSV and then CSV to JSON.</p><div><h3 class="title"><a id="note09"/>Note</h3><p>Wait; what if that JSON example doesn't look like the JSON files I found online to perform a social network diagram in D3?</p><p>Some of the examples of D3 social network visualizations that you may find online will show many additional values for each node or link, for example, they may include extra attributes that can be used to signify a difference in size, a hover feature, or a color change, as shown in this sample: <a class="ulink" href="http://bl.ocks.org/christophermanning/1625629">http://bl.ocks.org/christophermanning/1625629</a>. This visualization shows relationships between paid political lobbyists in Chicago. In this example, the code takes into account information in the JSON file to determine the size of the circles for the nodes and the text that is displayed when you hover over the nodes. It makes a really nice diagram, but it is complicated. As our primary goal is to learn how to clean the data, we will work with a pared down, simple example here that does not have many of these extras. Do not worry, though; our example will still build a nifty D3 diagram!</p></div><p>To convert the GDF file to JSON in the format we want, we can follow these steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Use a text editor to split the <code class="literal">personal.gdf</code> file into two files, <code class="literal">nodes.gdf</code> and <code class="literal">links.gdf.</code></li><li class="listitem">Alter the header row in each file to match the column names we eventually want in the JSON file:<div><pre class="programlisting">id,name,gender,lang,num
1234,Bugs Bunny,male,en_US,296
2345,Daffy Duck,male,en_US,295
9876,Minnie Mouse,female,en_US,294

source,target
1234,2345
1234,9876
2345,9876</pre></div></li><li class="listitem">Use the <code class="literal">csvcut</code> utility (part of csvkit discussed previously) to extract the first and second columns from <a id="id306" class="indexterm"/>the <code class="literal">nodes.gdf</code> file and redirect the output to a new file called <code class="literal">nodesCut.gdf</code>:<div><pre class="programlisting"><strong>csvcut -c 1,2 nodes.gdf &gt; nodesCut.gdf</strong>
</pre></div></li><li class="listitem">Now, we need to give each edge pair an indexed value rather than their full Facebook ID value. The index just identifies this node by its position in the node list. We need to perform this transformation so that the data will easily feed into the D3 force network code examples that we have, with as little refactoring as possible. We need to convert this:<div><pre class="programlisting">source,target
1234,2345
1234,9876
2345,9876</pre></div><p>into this:</p><div><pre class="programlisting">source,target
0,1
0,2
1,2</pre></div><p>Here is a small Python script that will create these index values automatically:</p><div><pre class="programlisting">import csv

# read in the nodes
with open('nodesCut.gdf', 'r') as nodefile:
    nodereader = csv.reader(nodefile)
    nodeid, name = zip(*nodereader)

# read in the source and target of the edges
with open('edges.gdf', 'r') as edgefile:
    edgereader = csv.reader(edgefile)
    sourcearray, targetarray = zip(*edgereader)
slist = list(sourcearray)
tlist = list(targetarray)

# find the node index value for each source and target
for n,i in enumerate(nodeid):
    for j,s in enumerate(slist):
        if s == i:
            slist[j]=n-1
    for k,t in enumerate(tlist):
        if t == i: 
            tlist[k]=n-1
# write out the new edge list with index values
with open('edgelistIndex.csv', 'wb') as indexfile:
    iwriter = csv.writer(indexfile)
    for c in range(len(slist)):
        iwriter.writerow([ slist[c], tlist[c]])</pre></div></li><li class="listitem">Now, go back to<a id="id307" class="indexterm"/> the <code class="literal">nodesCut.csv</code> file and remove the <code class="literal">id</code> column:<div><pre class="programlisting"><strong>csvcut -c 2 nodesCut.gdf &gt; nodesCutName.gdf</strong>
</pre></div></li><li class="listitem">Construct a small Python script that takes each of these files and writes them out to a complete JSON file, ready for D3 processing:<div><pre class="programlisting">import csv
import json

# read in the nodes file
with open('nodesCutName.gdf') as nodefile:
    nodefile_csv = csv.DictReader(nodefile)
    noutput = '['
    ncounter = 0;

    # process each dictionary row
    for nrow in nodefile_csv:
        # look for ' in node names, like O'Connor
        nrow["name"] = \
        str(nrow["name"]).replace("'","")
        # put a comma between the entities
        if ncounter &gt; 0:
            noutput += ','
        noutput += json.dumps(nrow)
        ncounter += 1
    noutput += ']'
    # write out a new file to disk
    f = open('complete.json','w')
    f.write('{')
    f.write('\"nodes\":' )
    f.write(noutput)
    
# read in the edge file
with open('edgelistIndex.csv') as edgefile:
    edgefile_csv = csv.DictReader(edgefile)
    eoutput = '['
    ecounter = 0;
    # process each dictionary row
    for erow in edgefile_csv:
        # make sure numeric data is coded as number not # string
        for ekey in erow:
            try:
                erow[ekey] = int(erow[ekey])
            except ValueError:
                # not an int
                pass
        # put a comma between the entities
        if ecounter &gt; 0:
            eoutput += ','
        eoutput += json.dumps(erow)
        ecounter += 1
    eoutput += ']'
    
    # write out a new file to disk
    f.write(',')
    f.write('\"links\":')
    f.write(eoutput)
    f.write('}')
    f.close()</pre></div></li></ol><div></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec42"/>Step four – build a D3 diagram</h2></div></div></div><p>This <a id="id308" class="indexterm"/>section shows how to feed our JSON file of nodes and links into a boilerplate example of building a force-directed graph in D3. This code example came from the D3 website and builds a simple graph using the JSON file provided. Each node is shown as a circle, and when you hover your mouse over the node, the person's name shows up as a tooltip:</p><div><pre class="programlisting">&lt;!DOCTYPE html&gt;
&lt;!-- this code is based on the force-directed graph D3 example given at : https://gist.github.com/mbostock/4062045 --&gt;

&lt;meta charset="utf-8"&gt;
&lt;style&gt;

.node {
  stroke: #fff;
  stroke-width: 1.5px;
}

.link {
  stroke: #999;
  stroke-opacity: .6;
}

&lt;/style&gt;
&lt;body&gt;
&lt;!-- make sure you have downloaded the D3 libraries and stored them locally --&gt;
&lt;script src="img/d3.min.js"&gt;&lt;/script&gt;
&lt;script&gt;

var width = 960, height = 500;
var color = d3.scale.category20();
var force = d3.layout.force()
    .charge(-25)
    .linkDistance(30)
    .size([width, height]);

var svg = d3.select("body").append("svg")
    .attr("width", width)
    .attr("height", height);

d3.json("complete.json", function(error, graph) {
  force
      .nodes(graph.nodes)
      .links(graph.links)
      .start();

  var link = svg.selectAll(".link")
      .data(graph.links)
    .enter().append("line")
      .attr("class", "link")
      .style("stroke-width", function(d) { return Math.sqrt(d.value); });

  var node = svg.selectAll(".node")
      .data(graph.nodes)
    .enter().append("circle")
      .attr("class", "node")
      .attr("r", 5)
      .style("fill", function(d) { return color(d.group); })
      .call(force.drag);

  node.append("title")
      .text(function(d) { return d.name; });

  force.on("tick", function() {
    link.attr("x1", function(d) { return d.source.x; })
      .attr("y1", function(d) { return d.source.y; })
      .attr("x2", function(d) { return d.target.x; })
      .attr("y2", function(d) { return d.target.y; });

    node.attr("cx", function(d) { return d.x; })
      .attr("cy", function(d) { return d.y; });
  });
});
&lt;/script&gt;</pre></div><p>The <a id="id309" class="indexterm"/>following screenshot shows an example of this social network. One of the nodes has been hovered over, showing the tooltip (name) of that node.</p><div><img src="img/image00272.jpeg" alt="Step four – build a D3 diagram"/><div><p>Social network built with D3</p></div></div><p style="clear:both; height: 1em;"> </p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec43"/>Step five – convert data to the Pajek file format</h2></div></div></div><p>So<a id="id310" class="indexterm"/> far, we have converted a GDF file to CSV, and then to JSON, and built a D3 diagram of it. In the next two steps, we will continue to pursue our goal of getting the data in such a format that we can calculate some social network metrics on it.</p><p>For this step, we will take the original GDF file and tweak it to become a valid <code class="literal">Pajek</code> file, which is the<a id="id311" class="indexterm"/> format that is needed by the social network tool called networkx.</p><div><h3 class="title"><a id="note10"/>Note</h3><p>The word <em>pajek</em> means <em>spider</em> in Slovenian. A social network can be thought of as a web made up of nodes and the links between them.</p></div><p>The format of our Facebook GDF file converted to a Pajek file looks like this:</p><div><pre class="programlisting">*vertices 296
1234 Bugs_Bunny male en_US 296
2456 Daffy_Duck male en_US 295
9876 Minnie_Mouse female en_US 294
*edges
1234 2456
2456 9876
2456 3456</pre></div><p>Here are<a id="id312" class="indexterm"/> a few important things to notice right away about this Pajek file format:</p><div><ul class="itemizedlist"><li class="listitem">It is space-delimited, not comma-delimited.</li><li class="listitem">Just like in the GDF file, there are two main sections of data, and these are labeled, starting with an asterisk <code class="literal">*</code>. The two sections are the vertices (another word for nodes) and the edges.</li><li class="listitem">There is a count of how many total vertices (nodes) there are in the file, and this count goes next to the word vertices on the top line.</li><li class="listitem">Each person's name has spaces removed and replaced with underscores.</li><li class="listitem">The other columns are optional in the node section.</li></ul></div><p>To convert<a id="id313" class="indexterm"/> our GDF file into Pajek format, let's use the text editor, as these changes are fairly straightforward and our file is not very large. We <a id="id314" class="indexterm"/>will perform the data cleaning tasks as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Save a copy of your GDF file as a new file and call it something like <code class="literal">fbPajek.net</code> (the <code class="literal">.net</code> extension is commonly used for Pajek network files).</li><li class="listitem">Replace the top line in your file. Currently, it looks like this:<div><pre class="programlisting">nodedef&gt;name VARCHAR,label VARCHAR,sex VARCHAR,locale VARCHAR,agerank INT</pre></div><p>You will need to change it to something like this:</p><div><pre class="programlisting">*vertices 296</pre></div><p>Make sure the number of vertices matches the number you have in your actual file. This is the count of nodes. There should be one per line in your GDF file.</p></li><li class="listitem">Replace <a id="id315" class="indexterm"/>the edges line in your file. Currently, it looks like this:<div><pre class="programlisting">edgedef&gt;node1 VARCHAR,node2 VARCHAR</pre></div><p>You will need to change it to look like this:</p><div><pre class="programlisting">*edges</pre></div></li><li class="listitem">Starting at line 2, replace every instance of a space with an underscore. This works because the only spaces in this file are in the names. Take a look at this:<div><pre class="programlisting">1234,Bugs Bunny,male,en_US,296
2456,Daffy Duck,male,en_US,295
3456,Minnie Mouse,female,en_US,294</pre></div><p>This action will turn the preceding into this:</p><div><pre class="programlisting">1234,Bugs_Bunny,male,en_US,296
2456,Daffy_Duck,male,en_US,295
3456,Minnie_Mouse,female,en_US,294</pre></div></li><li class="listitem">Now, use find and replace to replace all the instances of a comma with a space. The result for the nodes section will be:<div><pre class="programlisting">*vertices 296
1234 Bugs_Bunny male en_US 296
2456 Daffy_Duck male en_US 295
3456 Minnie_Mouse female en_US 294</pre></div><p>The result for the edges section will be:</p><div><pre class="programlisting">*edges
1234 2456
2456 9876
2456 3456</pre></div></li><li class="listitem">One last thing; use the find feature of the text editor to locate any of your Facebook friends who have an apostrophe in their name. Replace this apostrophe with <a id="id316" class="indexterm"/>nothing. Thus, <code class="literal">Cap'n_Crunch</code> becomes:<div><pre class="programlisting">1998988 Capn_Crunch male en_US 137</pre></div><p>This is now a fully cleaned, Pajek-formatted file.</p></li></ol><div></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec44"/>Step six – calculate simple network metrics</h2></div></div></div><p>At this point, we are ready to<a id="id317" class="indexterm"/> run some simple social network metrics using a Python package like networkx. Even<a id="id318" class="indexterm"/> though <strong>Social Network Analysis</strong> (<strong>SNA</strong>) is beyond the scope of this book, we can still perform a few calculations quite easily without delving too deeply into the mysteries of SNA.</p><p>First, we should make sure that we have the <code class="literal">networkx</code> package installed. I am using Canopy for my Python editor, so I will use the Package Manager to search for networkx and install it.</p><p>Then, once networkx is installed, we can write some quick Python code to read our Pajek file and output a few interesting facts about the structure of my Facebook network:</p><div><pre class="programlisting">import networkx as net

# read in the file
g = net.read_pajek('fb_pajek.net')

# how many nodes are in the graph?
# print len(g)

# create a degree map: a set of name-value pairs linking nodes
# to the number of edges in my network
deg = net.degree(g)
# sort the degree map and print the top ten nodes with the
# highest degree (highest number of edges in the network)
print sorted(deg.iteritems(), key=lambda(k,v): (-v,k))[0:9]</pre></div><p>The result for my network looks like the following output. The top ten nodes are listed, along with a count of how many of my other nodes each of these links to:</p><div><pre class="programlisting">[(u'Bambi', 134), (u'Cinderella', 56), (u'Capn_Crunch', 50), (u'Bugs_Bunny', 47), (u'Minnie_Mouse', 47), (u'Cruella_Deville', 46), (u'Alice_Wonderland', 44), (u'Prince_Charming', 42), (u'Daffy_Duck', 42)]</pre></div><p>This shows that <code class="literal">Bambi</code> is connected to <code class="literal">134</code> of my other friends, but <code class="literal">Prince_Charming</code> is only connected to <code class="literal">42</code> of my other friends.</p><div><h3 class="title"><a id="tip16"/>Tip</h3><p>If you get any Python errors about missing quotations, double-check your Pajek format file to ensure that all node labels are free of spaces and other special characters. In the cleaning procedure explained in the preceding example, we removed spaces and the quotation character, but your friends may have more exotic characters in their names!</p></div><p>Of course, there are many more <a id="id319" class="indexterm"/>interesting things you can do with networkx and D3 visualizations, but this sample project was designed to give us a sense of how critical data-cleaning processes are to the successful outcome of any larger analysis effort.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec26"/>Summary</h1></div></div></div><p>In this chapter, we learned many different ways to convert data from one format to another. Some of these techniques are simple, such as just saving a file in the format you want or looking for a menu option to output the correct format. At other times, we will need to write our own programmatic solution.</p><p>Many projects, such as the sample project we implemented in this chapter, will require several different cleaning steps, and we will have to carefully plan out our cleaning steps and write down what we did. Both networkx and D3 are really nifty tools, but they do require data to be in a certain format before we are ready to use them. Likewise, Facebook data is easily available through netvizz, but it too has its own data format. Finding easy ways to convert from one file format to the other is a critical skill in data science.</p><p>In this chapter, we performed a lot of conversions between structured and semistructured data. But what about cleaning messy data, such as unstructured text?</p><p>In <a class="link" title="Chapter 5. Collecting and Cleaning Data from the Web" href="part0033.xhtml#aid-VF2I1">Chapter 5</a>, <em>Collecting and Cleaning Data from the Web</em>, we will continue to fill up our data science cleaning toolbox by learning some of the ways in which we can clean pages that we find on the Web.</p></div></body></html>