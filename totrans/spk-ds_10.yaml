- en: Chapter 10.  Putting It All Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data analytics is revolutionizing the way businesses are run and has paved
    the way for several hitherto unimagined opportunities. Almost every enterprise,
    individual researcher, or investigative journalist has lots of data to process.
    We need a concise approach to start from raw data and arrive at meaningful insights
    based on the questions at hand.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered various aspects of data science using Apache Spark in previous
    chapters. We started off discussing big data analytics requirements and how Apache
    spark fits in. Gradually, we looked into the Spark programming model, RDDs, and
    DataFrame abstractions and learnt how unified data access is enabled by Spark
    datasets along with the streaming aspect of continuous applications. Then we covered
    the entire breadth of the data analysis life cycle using Apache Spark followed
    by machine learning. We learnt structured and unstructured data analytics on Spark
    and explored the visualization aspects for data engineers and scientists, as well
    as business users.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the previously discussed chapters helped us understand one concise aspect
    per chapter. We are now equipped to traverse the entire data science life cycle.
    In this chapter, we shall take up an end-to-end case study and apply all that
    we have learned so far. We will not introduce any new concepts; this will help
    apply the knowledge gained so far and strengthen our understanding. However, we
    have reiterated some concepts without going into too much detail, to make this
    chapter self-contained. The topics covered in this chapter are roughly the same
    as the steps in the data analytics life cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: A quick recap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing a case study
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Framing the business problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data acquisition and data cleansing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the hypothesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicating the results to business users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We already discussed in detail the various steps involved in a typical data
    science project separately in different chapters. Let us quickly glance through
    what we have covered already and touch upon some important aspects. A high-level
    overview of the steps involved may appear as in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A quick recap](img/image_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding pictorial representation, we have tried to explain the steps
    involved in a data science project at a higher level, mostly generic to many data
    science assignments. Many more substeps are actually present at every stage, but
    may differ from project to project.
  prefs: []
  type: TYPE_NORMAL
- en: It is very difficult for data scientists to find the best approach and steps
    to follow in the beginning. Generally, data science projects do not have a well-defined
    life cycle such as the **Software Development Life Cycle** (**SDLC**). It is usually
    the case that data science projects get tramped into delivery delays with repeated
    hold-ups, as most of the steps in the life cycle are iterative. Also, there could
    be cyclic dependencies across teams that add to the complexity and cause delay
    in execution. However, while working on big data analytics projects, it is important
    as well as advantageous for data scientists to follow a well-defined data science
    workflow, irrespective of different business cases. This not only helps in an
    organized execution, but also helps us stay focused on the objective, as data
    science projects are inherently agile in most cases. Also, it is recommended that
    you plan for some level of research on data, domain, and algorithms for any given
    project.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we may not be able to accommodate all the granular steps in
    a single flow, but will address the important areas to give you a heads-up. We
    will try to look at some different coding examples that we have not covered in
    the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing a case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be exploring Academy Awards demographics in this chapter. You can download
    the data from the GitHub repository at [https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv](https://www.crowdflower.com/wp-content/uploads/2016/03/Oscars-demographics-DFE.csv).
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is based on the data provided at [http://www.crowdflower.com/data-for-everyone](http://www.crowdflower.com/data-for-everyone).
    It contains demographic details such as race, birthplace, and age. Rows are around
    400 and it can be easily processed on a simple home computer, so you can do a
    **Proof of Concept** (**POC**) on executing a data science project on Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Just start by downloading the file and inspecting the data. The data may look
    fine but as you take a closer look, you will notice that it is not "clean". For
    example, the date of birth column does not follow the same format. Some years
    are in two-digit format whereas some are in four-digit format. Birthplace does
    not have country for locations within the USA.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, you will also notice that the data looks skewed, with more "white"
    race people from the USA. But you might have felt that the trend has changed toward
    later years. You have not used any tools or techniques so far, just had a quick
    glance at the data. In the real world of data science, this seemingly trivial
    activity can be quite helpful further down the life cycle. You get to develop
    a feel for the data at hand and simultaneously hypothesize about the data. This
    brings you to the very first step in the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: The business problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As iterated before, the most important aspect of any data science project is
    the question at hand. Having a clear understanding on *what problem are we trying
    to solve?* This is critical to the success of the project. It also drives what
    is considered as relevant data and what is not. For example, in the current case
    study, if what we want to look at is the demographics, then movie name and person
    name are irrelevant. At times, there is no specific question at hand! *What then?*
    Even when there is no specific question, the business may still have some objective,
    or data scientists and domain experts can work together to find the area of business
    to work on. To understand the business, functions, problem statement, or data,
    the data scientists start with "Questioning". It not only helps in defining the
    workflow, but helps in sourcing the right data to work on.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, if the business focus is on demographics information, a formal
    business problem statement can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What is the impact of the race and country of origin among Oscar award winners?*'
  prefs: []
  type: TYPE_NORMAL
- en: In real-world, scenarios this step will not be this straightforward. Framing
    the right question is the collective responsibility of the data scientist, strategy
    team, domain experts, and the project owner. Since the whole exercise is futile
    if it does not serve the purpose, a data scientist has to consult all stakeholders
    and try to elicit as much information as possible from them. However, they may
    end up getting invaluable insights or "hunches". All of these combined form the
    core of the initial hypothesis and also help the data scientist to understand
    what exactly they should look for.
  prefs: []
  type: TYPE_NORMAL
- en: The situations where there is no specific question at hand that the business
    is trying to find an answer for are even more interesting to deal with, but can
    be complex in executing!
  prefs: []
  type: TYPE_NORMAL
- en: Data acquisition and data cleansing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data acquisition** is the logical next step. It may be as simple as selecting
    data from a single spreadsheet or it may be an elaborate several months project
    in itself. A data scientist has to collect as much relevant data as possible.
    ''Relevant'' is the keyword here. Remember, more relevant data beats clever algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: We have already covered how to source data from heterogeneous data sources and
    consolidate it to form a single data matrix, so we will not iterate the same fundamentals
    here. Instead, we source our data from a single source and extract a subset of
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is time to view the data and start cleansing it. The scripts presented
    in this chapter tend to be longer than the previous examples but still are no
    means of production quality. Real-world work requires a lot more exception checks
    and performance tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For the dataset at hand, you might also have noticed that `date_of_birth` and
    `birthplace` require a lot of cleaning. The following code shows two **user-defined
    functions** (**UDFs**) that clean `date_of_birth` and `birthplace` respectively.
    These UDFs work on a single data element at a time and they are just ordinary
    Scala/Python functions. These user defined functions should be registered so that
    they can be used from within a SQL statement. The final step is to create a cleaned
    data frame that will participate in further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the following logic for cleaning `birthplace.` It is a weak logic because
    we are assuming that any string ending with two characters is an American state.
    We have to compare them against a list of valid abbreviations. Similarly, assuming
    two-digit years are always from the twentieth century is another error-prone assumption.
    Depending on the use case, a data scientist/data engineer has to take a call whether
    retaining more rows is important or only quality data should be included. All
    such decisions should be neatly documented for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The UDF to clean date accepts a hyphenated date string and splits it. If the
    last component, which is the year, is two digits long, then it is assumed to be
    a twentieth-century date and 1900 is added to bring it to four-digit format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following UDF appends the country as USA if the country string is either
    New York City or the last component is two characters long, where it is assumed
    to be a state in the USA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The UDFs should be registered if you want to access them from SELECT strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Clean the data frame using the UDFs. Perform the following cleanup operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Call UDFs `fncleanDate` and `fncleanBirthplace` to fix birthplace and country.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract birth year from `award_year` to get `age` at the time of receiving
    the award.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retain `race` and `award` as they are.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The last line requires some explanation. The UDFs are used similar to SQL functions
    and the expressions are aliased to meaningful names. We have added a computed
    column `age` because we would like to validate the impact of age also. The `substring_index`
    function  searches the first argument for the second argument. `-1` indicates
    to look for the first occurrence from the right.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the hypothesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A hypothesis is your best guess about what the outcome will be. You form your
    initial hypothesis based on the question, conversations with stakeholders, and
    also by looking at the data. You may form one or more hypotheses for a given problem.
    This initial hypothesis serves as a roadmap that guides you through the exploratory
    analysis. Developing a hypothesis is very important to statistically approve or
    not approve a statement, and not just by looking at the data as a data matrix
    or even through visuals. This is because our perception built by just looking
    at the data may be incorrect and rather deceptive at times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you know that your final result may or may not prove the hypothesis to
    be correct. Coming to the case study we have considered for this lesson, we arrive
    at the following initial hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: Award winners are mostly white
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the award winners are from the USA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best actors and actresses tend to be younger than best directors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have formalized our hypotheses, we are all set to move forward with
    the next steps in the life cycle..
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a clean data frame with relevant data and the initial hypothesis,
    it is time to really explore what we have. The DataFrames abstraction provides
    functions such as `group by` out of the box for you to look around. You may register
    the cleaned data frame as a table and run the time-tested SQL statements to do
    just the same.
  prefs: []
  type: TYPE_NORMAL
- en: This is also the time to plot a few graphs. This phase of visualization is the
    exploratory analysis mentioned in the data visualization chapter. The objectives
    of this exploration are greatly influenced by the initial information you garner
    from the business stakeholders and the hypothesis. In other words, your discussions
    with the stakeholders help you know what to look for.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some general guidelines that are applicable for almost all data science
    assignments, but again subjective to different use cases. Let us look at some
    generic ones:'
  prefs: []
  type: TYPE_NORMAL
- en: Look for missing data and treat it. We have already discussed various ways to
    do this in [Chapter 5](ch05.xhtml "Chapter 5. Data Analysis on Spark"), *Data
    Analysis on Spark*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the outliers in the dataset and treat them. We have discussed this aspect
    as well. Please note that there are cases where what we think of as outliers and
    normal data points may change depending on the use case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform univariate analysis, wherein you explore each variable in the dataset
    separately. Frequency distribution or percentile distribution are quite common.
    Perhaps plot some graphs to get a better idea. This will also help you prepare
    your data before getting into data modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate your initial hypothesis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check minimum and maximum values of numerical data. If the variation is too
    high in any column, that could be a candidate for data normalization or scaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check distinct values in categorical data (string values such as city names)
    and their frequencies. If there are too many distinct values (aka levels) in any
    column, you may have to look for ways to reduce the number of levels. If one level
    is occurring almost always, then this column is not helping the model to differentiate
    between the possible outcomes. Such columns are likely candidates for removal.
    At the exploration stage, you just figure out such candidate columns and let the
    data preparation phase take care of the actual action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our current dataset, we do not have any missing data and we do not have
    any numerical data that might create any challenge. However, some missing values
    might creep in when invalid dates are processed. So, the following code covers
    the remaining action items. This code assumes that `cleaned_df` is already created:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala/Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following visualizations correspond to the initial hypotheses. Note that
    two of our hypotheses were found to be correct but the third one was not. These
    visualizations are created using zeppelin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data exploration](img/image_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note here that the all hypotheses cannot just be validated through visuals,
    as they can be deceptive at times. So proper statistical tests such as t-tests,
    ANOVA, Chi-squared tests, correlation tests, and so on need to be performed as
    applicable. We will not get into the details in this section. Please refer to
    [Chapter 5](ch05.xhtml "Chapter 5. Data Analysis on Spark"), *Data Analysis on
    Spark*, for further details.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data exploration stage helped us identify all the issues that needed to
    be fixed before proceeding to the modeling stage. Each individual issue requires
    careful thought and deliberation to choose the best fix. Here are some common
    issues and the possible fixes. The best fix is dependent on the problem at hand
    and/or the business context.
  prefs: []
  type: TYPE_NORMAL
- en: Too many levels in a categorical variable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is one of the most common issues we face. The treatment of this issue
    is dependent on multiple factors:'
  prefs: []
  type: TYPE_NORMAL
- en: If the column is almost always unique, for example, it is a transaction ID or
    timestamp, then it does not participate in modeling unless you are deriving new
    features from it. You may safely drop the column without losing any information
    content. You usually drop it during the data cleansing stage itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is possible to replace the levels with coarser-grained levels (for example,
    state or country instead of city) that make sense in the current context, then
    usually that is the best way to fix this issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may want to add dummy columns with 0 or 1 values for each distinct level.
    For example, if you have 100 levels in a single column, you add 100 columns instead.
    At most, one column will have 1 at any observation (row). This is called **one-hot
    encoding** and Spark provides this out of the box through the `ml.features` package.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another option is to retain the most frequent levels. You may even attach each
    of these levels to one of the dominant levels that is somehow considered "nearer"
    to this level. Also, you may bundle up the remaining into a single bucket, say,
    `Others`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no hard and fast rule for an absolute limit to the number of levels.
    It depends on what granularity you require in each individual feature and the
    performance constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The current dataset has too many levels in the categorical variable `country`.
    We chose to retain the most frequent levels and bundle the remaining into `Others`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Numerical variables with too much variation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes numerical data values may vary by several orders of magnitude. For
    example, if you are looking at the annual income of individuals, it may vary a
    lot. Z-score normalization (standardization) and min-max scaling are two popular
    choices to deal with such data. Spark includes both of these transformations out
    of the box in the `ml.features` package.
  prefs: []
  type: TYPE_NORMAL
- en: Our current dataset does not have any such variable. The only numerical variable
    we have is age and its value is uniformly two digits. That's one less issue to
    fix.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that it is not always necessary to normalize such data. If you are
    comparing two variables that are in two different scales, or if you are using
    a clustering algorithm or SVM classifier, or any other scenario where there is
    really a need to normalize the data, you may normalize the data.
  prefs: []
  type: TYPE_NORMAL
- en: Missing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a major area of concern. Any observations where the target itself is
    missing should be removed from the training data. The remaining observations may
    be retained with some imputed values or removed as per the requirements. You should
    be very careful in imputing the missing values; it may lead to misleading output
    otherwise! It may seem very easy to just go ahead and substitute average values
    in the blank cells of a continuous variable, but this may not be the right approach.
  prefs: []
  type: TYPE_NORMAL
- en: Our current case study does not have any missing data so there is no scope for
    treating it. However, let us look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume you have a student's dataset that you are dealing with, and it
    has data from class-1 to class-5\. If there are some missing `Age` values and
    you just find the average of the whole column and substitute, then that would
    rather become an outlier and could lead to vague results. You may choose to find
    the average of only the class that the student is in, and then impute that value.
    This is at least a better approach, but may not be a perfect one. In most of the
    cases, you will have to give weightage to other variables as well. If you do so,
    you may end up building a predictive model to find the missing values and this
    can be a great approach!
  prefs: []
  type: TYPE_NORMAL
- en: Continuous data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Numerical data is often continuous and must be discretized because it is a
    prerequisite to some of the algorithms. It is usually split into different buckets
    or ranges of values. However, there could be cases where you may not just uniformly
    bucket based on the range of your data, you may have to consider the variance
    or standard deviation or any other applicable reason to bucket properly. Now,
    deciding the number of buckets is also at the discretion of the data scientist,
    but that too needs careful analysis. Too few buckets reduces granularity and too
    many buckets is just about the same as having too many categorical levels. In
    our case study, `age` is an example of such data and we need to discretize it.
    We split it into different buckets. For example, look at this pipeline stage,
    which converts `age` to 10 buckets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Categorical data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have discussed the need for discretizing continuous data and converting it
    to categories or buckets. We have also discussed the introduction of dummy variables,
    one for each distinct value of a categorical variable. There is one more common
    data preparation practice where we convert categorical levels to numerical (discrete)
    data. This is required because many machine learning algorithms work with numerical
    data, integers, and real-valued numbers, or some other situation may demand it.
    So, we need to convert categorical data into numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: There can be downsides to this approach. Introducing an order into inherently
    unordered data may not be logical at times. For example, assigning numbers such
    as 0, 1, 2, 3 to the colors "red", "green", "blue", and "black", respectively,
    does not make sense. This is because we cannot say that red is one unit distant
    from "green" and so is "green" from "blue"! If applicable, introducing dummy variables
    makes more sense in many such cases.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having discussed the common issues and possible fixes, let us see how to prepare
    our current dataset. We have already covered the too many levels issue related
    code fix. The following example shows the rest. It converts all the features into
    a single features column. It also sets aside some data for testing the models.
    This code heavily relies on the `ml.features` package, which was designed to support
    the data preparation phase. Note that this piece of code is just defining what
    needs to be done. The transformations are not carried out as yet. These will become
    stages in subsequently defined pipelines. Execution is deferred as late as possible,
    until the actual model is built. The Catalyst optimizer finds the optimal route
    to implement the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: After carrying out all data preparation activity, you will end up with a completely
    numeric data with no missing values and with manageable levels in each attribute.
    You may have already dropped any attributes that may not add much value to the
    analysis on hand. This is what we call the **final data matrix**. You are all
    set now to start modeling your data. So, first you split your source data into
    train data and test data. Models are "trained" using train data and "tested" using
    test data. Note that the split is random and you may end up with different train
    and test partitions if you redo the split.
  prefs: []
  type: TYPE_NORMAL
- en: Model building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A model is a representation of things, a rendering or description of reality.
    Just like a model of a physical building, data science models attempt to make
    sense of the reality; in this case, the reality is the underlying relationships
    between the features and the predicted variable. They may not be 100 percent accurate,
    but still very useful to give some deep insights into our business space based
    on the data.
  prefs: []
  type: TYPE_NORMAL
- en: There are several machine learning algorithms that help us model data and Spark
    provides many of them out of the box. However, which model to build is still a
    million dollar question. It depends on various factors, such as interpretability-accuracy
    trade-off, how much data you have at hand, categorical or numerical variables,
    time and memory constraints, and so on. In the following code example, we have
    just trained a few models at random to show you how it can be done.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be predicting the award type based on race, age, and country. We''ll
    be using the DecisionTreeClassifier, RandomForestClassifier, and OneVsRest algorithms.
    These three are chosen arbitrarily. All of them work with multiclass labels and
    are simple to understand. We have used the following evaluation metrics provided
    by the `ml` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: The ratio of correctly predicted observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted Precision**: Precision is the ratio of correct positive observations
    to all positive observations. Weighted precision takes the frequency of individual
    classes into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted Recall**: Recall is the ratio of positives to actual positives.
    Actual positives are the sum of true positives and false negatives. Weighted Recall
    takes the frequency of individual classes into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1**: The default evaluation measure. This is the weighted average of Precision
    and Recall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, we have tried a few models and found that they gives us roughly the
    same performance. There are various other ways to validate the model performance.
    This again depends on the algorithm you have used, the business context, and the
    outcome produced. Let us look at some metrics that are offered out of the box
    in the `spark.ml.evaluation` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Decision tree** | **Random Forest** | **OneVsRest** |'
  prefs: []
  type: TYPE_TB
- en: '| F1 | 0.29579 | 0.26451 | 0.25649 |'
  prefs: []
  type: TYPE_TB
- en: '| WeightedPrecision | 0.32654 | 0.26451 | 0.25295 |'
  prefs: []
  type: TYPE_TB
- en: '| WeightedRecall | 0.30827 | 0.29323 | 0.32330 |'
  prefs: []
  type: TYPE_TB
- en: Upon validating the model performance, you will have to tune the model as much
    as possible. Now, tuning can happen both ways, at the data level and at the algorithm
    level. Feeding the right data that an algorithm expects is very important. The
    problem is that whatever data you feed in, the algorithm may still give some output
    - it never complains! So, apart from cleaning the data properly by treating missing
    values, treating univariate and multivariate outliers, and so on, you can create
    many more relevant features. This feature engineering is usually treated as the
    most important aspect of data science. Having decent domain expertise helps to
    engineer better features. Now, coming to the algorithmic aspect of tuning, there
    is always scope for working on optimizing the parameters that we pass to an algorithm.
    You may choose to use grid search to find the optimal parameters. Also, data scientists
    should question themselves on which loss function to use and why, and, out of
    GD, SGD, L-BFGS, and so on, which algorithm to use to optimize the loss function
    and why.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the preceding approach is intended just to demonstrate how
    to perform the steps on Spark. Selecting one algorithm over the other by just
    looking at the accuracy level may not be the best way. Selecting an algorithm
    depends on the type of data you are dealing with, the outcome variable, the business
    problem/requirement, computational challenges, interpretability, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data visualization** is something which is needed every now and then from
    the time you take on a data science assignment. Before building any model, preferably,
    you will have to visualize each variable to see their distributions to understand
    their characteristics and also find outliers so you can treat them. Simple tools
    such as scatterplot, box plot, bar chart, and so on are a few versatile, handy
    tools for such purposes. Also, you will have to use the visuals in most of the
    steps to ensure you are heading in the right direction.'
  prefs: []
  type: TYPE_NORMAL
- en: Every time you want to collaborate with business users or stakeholders, it is
    always a good practice to convey your analysis through visuals. Visuals can accommodate
    more data in them in a more meaningful way and are inherently intuitive in nature.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that most data science assignment outcomes are preferably represented
    through visuals and dashboards to business users. We already have a dedicated
    chapter on this topic, so we won't go deeper into it.
  prefs: []
  type: TYPE_NORMAL
- en: Communicating the results to business users
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In real-life scenarios, it is mostly the case that you have to keep communicating
    with the business intermittently. You might have to build several models before
    concluding on a final production-ready model and communicate the results to the
    business.
  prefs: []
  type: TYPE_NORMAL
- en: 'An implementable model does not always depend on accuracy; you might have to
    bring in other measures such as sensitivity, specificity, or an ROC curve, and
    also represent your results through visuals such as a Gain/Lift chart or an output
    of a K-S test with statistical significance. Note that these techniques require
    business users'' input. This input often guides the way you build the models or
    set thresholds. Let us look at a few examples to better understand how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: If a regressor predicts the probability of an event occurring, then blindly
    setting the threshold to 0.5 and assuming anything above 0.5 is 1 and less than
    0.5 is 0 may not be the best way! You may use an ROC curve and take a rather more
    scientific or logical decision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False-negative predictions for diagnosis of a cancer test may not be desirable
    at all! This is an extreme case of life risk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: E-mail campaigning is cheaper compared to delivery of hard copies. So the business
    may decide to send e-mails to the recipients who are predicted with less than
    0.5 (say 0.35) probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that the preceding decisions are influenced heavily by business users
    or the problem owners, and data scientists work closely with them to take a call
    on such cases.
  prefs: []
  type: TYPE_NORMAL
- en: Again, as discussed already, the right visuals are the most preferred way to
    communicate the results to the business.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have taken up a case study and completed the data analytics
    life cycle end to end. During the course of building a data product, we have applied
    the knowledge gained so far in the previous chapters. We have stated a business
    problem, formed an initial hypothesis, acquired data, and prepared it for model
    building. We have tried building multiple models and found a suitable model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, which is also the final chapter, we will discuss building
    real-world applications using Spark.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[http://www2.sas.com/proceedings/forum2007/073-2007.pdf](http://www2.sas.com/proceedings/forum2007/073-2007.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/](https://azure.microsoft.com/en-in/documentation/articles/machine-learning-algorithm-choice/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf](http://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf).'
  prefs: []
  type: TYPE_NORMAL
