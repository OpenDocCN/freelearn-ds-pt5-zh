- en: Chapter 4.  Unified Data Access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data integration from disparate data sources had always been a daunting feat.
    The three V's of big data and ever-shrinking processing time frames have made
    the task even more challenging. Delivering a clear view of well-curated data in
    near real time is extremely important for business. However, real-time curated
    data along with the ability to perform different operations such as ETL, ad hoc
    querying, and machine learning in a unified fashion is what is emerging as a key
    business differentiator.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark was created to offer a single general-purpose engine that can process
    data from a variety of data sources and support large-scale data processing for
    various different operations. Spark enables developers to combine SQL, Streaming,
    graphs, and machine learning algorithms in a single workflow!
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapters, we discussed **Resilient Distributed Datasets** (**RDDs**)
    as well as DataFrames. In [Chapter 3](ch03.xhtml "Chapter 3.  Introduction to
    DataFrames"), *Introduction to DataFrames*, we introduced Spark SQL and the Catalyst
    optimizer. This chapter builds on this foundation and delves deeper into these
    topics to help you realize the real essence of unified data access. We''ll introduce
    new constructs such as Datasets and Structured Streaming. Specifically, we''ll
    discuss the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data abstractions in Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Datasets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset API limitations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL operations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured Streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark streaming programming model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison with other streaming engines
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data abstractions in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MapReduce framework and its popular open source implementation Hadoop enjoyed
    widespread adoption in the past decade. However, iterative algorithms and interactive
    ad-hoc querying are not well supported. Any data sharing between jobs or stages
    within an algorithm is always through disk writes and reads as against in-memory
    data sharing. So, the logical next step would be to have a mechanism that facilitates
    reuse of intermediate results across multiple jobs. RDD is a general-purpose data
    abstraction that was developed to address this requirement.
  prefs: []
  type: TYPE_NORMAL
- en: RDD is the core abstraction in Apache Spark. It is an immutable, fault-tolerant
    distributed collection of statically typed objects that are usually stored in-memory.
    RDD API offer simple operations such as map, reduce, and filter that can be composed
    in arbitrary ways.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame abstraction is built on top of RDD and it adds "named" columns. So,
    a Spark DataFrame has rows of named columns similar to relational database tables
    and DataFrames in R and Python (pandas). This familiar higher level abstraction
    makes the development effort much easier because it lets you perceive data like
    an SQL table or an Excel file. Moreover, the Catalyst optimizer, under the hood,
    compiles the operations and generates JVM bytecode for efficient execution. However,
    the named columns approach gives rise to a new problem. Static type information
    is no longer available to the compiler, and hence we lose the advantage of compile-time
    type safety.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset API was introduced to combine the best traits from both RDDs and DataFrames
    plus some more features of its own. Datasets provide row and column data abstraction
    similar to the DataFrames, but with a structure defined on top of them. This structure
    may be defined by a case class in Scala or a class in Java. They provide type
    safety and lambda functions like RDDs. So, they support both typed methods such
    as `map` and `groupByKey` as well as untyped methods such as `select` and `groupBy`.
    In addition to the Catalyst optimizer, Datasets leverage in-memory encoding provided
    by the Tungsten execution engine, which improves performance even further.
  prefs: []
  type: TYPE_NORMAL
- en: The data abstractions introduced so far form the core abstractions. There are
    some more specialized data abstractions that work on top of these abstractions.
    Streaming APIs are introduced to process real-time streaming data from various
    sources such as Flume and Kafka. These APIs work together to provide data engineers
    a unified, continuous DataFrame abstraction that can be used for interactive and
    batch queries. Another example of specialized data abstraction is a GraphFrame.
    This enables developers to analyze social networks and any other graphs alongside
    Excel-like two-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now with the basics of the available data abstractions in mind, let''s understand
    what we exactly mean by a unified data access platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data abstractions in Apache Spark](img/image_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The intention behind this unified platform is that it not only lets you combine
    the static and streaming data together, but also allows various different kinds
    of operations on the data in a unified way! From the developer's perspective,
    a Dataset is the core abstraction to work with, and Spark SQL is the main interface
    to the Spark functionality. A two-dimensional data structure coupled with a SQL
    declarative programming interface had been a familiar way of dealing with data,
    thereby shortening the learning curve for the data engineers. So, understanding
    the unified platform translates to understanding Datasets and Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark **Datasets** are an extension of the DataFrame API that provide
    a type-safe object-oriented programming interface. This API was first introduced
    in the 1.6 release. Spark 2.0 version brought out unification of DataFrame and
    Dataset APIs. DataFrame becomes a generic, untyped Dataset; or a Dataset is a
    DataFrame with an added structure. The term "structure" in this context refers
    to a pattern or an organization of underlying data, more like a table schema in
    RDBMS parlance. The structure imposes a limit on what can be expressed or contained
    in the underlying data. This in turn enables better optimizations in memory organization
    as well as physical execution. Compile-time type checking leads to catching errors
    earlier than during runtime. For example, a type mismatch in a SQL comparison
    does not get caught until runtime, whereas it would be caught during compile time
    itself if it were expressed as a sequence of operations on Datasets. However,
    the inherent dynamic nature of Python and R implies that there is no compile-time
    type safety, and hence the concept Datasets does not apply to those languages.
    The unification of Datasets and DataFrames applies to Scala and Java API only.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of Dataset abstraction are the **encoders**. These encoders translate
    between JVM objects and Spark's internal Tungsten binary format. This internal
    representation bypasses JVM's memory management and garbage collection. Spark
    has its own C-style memory access that is specifically written to address the
    kind of workflows it supports. The resultant internal representations take less
    memory and have efficient memory management. Compact memory representation leads
    to reduced network load during shuffle operations. The encoders generate compact
    byte code that directly operates on serialized objects without de-serializing,
    thereby enhancing performance. Knowing the schema early on results in a more optimal
    layout in memory when caching Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will create Datasets and perform transformations and actions,
    much like DataFrames and RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1-creating a Dataset from a simple collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the last example in the preceding code, `case class` adds structure
    information. Spark uses this structure to create the best data layout and encoding.
    The following code shows us the structure and the plan for execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example shows the structure and the implementation physical plan
    as anticipated. If you want to get a more detailed execution plan, you have to
    pass explain (true), which prints extended information, including the logical
    plan as well.
  prefs: []
  type: TYPE_NORMAL
- en: We have examined Dataset creation from simple collections and RDDs. We have
    already discussed that DataFrames are just untyped Datasets. The following examples
    show conversion between Datasets and DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-converting the Dataset to a DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This example looks very much like the examples we have seen in [Chapter 3](ch03.xhtml
    "Chapter 3.  Introduction to DataFrames"), *Introduction to DataFrames*. These
    conversions become very handy in the real world. Consider adding a structure (aka
    case class) to imperfect data. You may first read that data into a DataFrame,
    perform cleansing, and then convert it to a Dataset. Another use case could be
    that you want to expose only a subset (rows and columns) of the data based on
    some runtime information, such as `user_id`. You could read the data into a DataFrame,
    register it as a temporary table, apply conditions, and expose the subset as a
    Dataset. The following example creates a `DataFrame` first and then converts it
    into `Dataset`. Note that the DataFrame column names must match the case class.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-convert a DataFrame to a Dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The explain command response shows `WholeStageCodegen`, which fuses multiple
    operations into a single Java function call. This enhances performance due to
    reduction in multiple virtual function calls. Code generation had been around
    in Spark engine since 1.1, but at that time it was limited to expression evaluation
    and a small number of operations such as filter. In contrast, whole stage code
    generation from Tungsten generates code for the entire query plan.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Datasets from JSON
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets can be created from JSON files, similar to DataFrames. Note that a
    JSON file may contain several records, but each record has to be on one line.
    If your source JSON has newlines, you have to programmatically remove them. The
    JSON records may have arrays and may be nested. They need not have uniform schema.
    The following example file has JSON records with one record having an additional
    tag and an array of data.
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-creating a Dataset from JSON
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Datasets API's limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even though the Datasets API is created using the best of both RDDs and DataFrames,
    it still has some limitations as of its current stage of development:'
  prefs: []
  type: TYPE_NORMAL
- en: While querying the dataset, the selected fields should be given specific data
    types as in the case class, or else the output will become a DataFrame. An example
    is `auth.select(col("first_name").as[String])`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python and R are inherently dynamic in nature, and hence typed Datasets do not
    fit in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Spark SQL** is a Spark module for structured data processing that was introduced
    in Spark 1.0\. This module is a tightly integrated relational engine that inert-operates
    with the core Spark API. It enables data engineers to write applications that
    load structured data from disparate sources and join them as a unified, and possibly
    continuous, Excel-like data frames; and then they can implement complex ETL workflows
    and advanced analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: The Spark 2.0 release brought in significant unification of APIs and expanded
    the SQL capabilities, including support for subqueries. The Dataset API and DataFrames
    API are now unified, with DataFrames being a "kind" of Datasets. The unified APIs
    build the foundation for Spark's future, spanning across all libraries. Developers
    can impose "structure" onto their data and can work with high-level declarative
    APIs, thereby improving performance as well as their productivity. The performance
    gains come as a result of the underlying optimization layer. DataFrames, Datasets,
    and SQL share the same optimization and execution pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: SQL operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SQL operations are most widely used constructs for data manipulation. Some of
    most used operations are, selecting all or some of the columns, filtering based
    on one or more conditions, sorting and grouping operations, and computing summary
    functions such as `average` on GroupedData. The  `JOIN` operations on multiple
    data sources and `set` operations such as `union`, `intersect` and `minus` are
    some other operations that are widely performed. Furthermore, data frames are
    registered as temporary tables and passed traditional SQL statements to perform
    the aforementioned operations. **User-Defined Functions** (**UDF**) are defined
    and used with and without registration. We'll be focusing on window operations,
    which have been just introduced in Spark 2.0\. They address sliding window operations.
    For example, if you want to report the average peak temperature every day in the
    past seven days, then you are operating on a sliding window of seven days until
    today. Here is an example that computes average sales per month for the past three
    months. The data file contains 24 observations showing monthly sales for two products,
    P1 and P2.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-window example with moving average computation
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Under the hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a developer is writing programs using RDD API, efficient execution for
    the workload on hand is his/her responsibility. The data types and computations
    are not available for Spark. In contrast, when a developer is using DataFrames
    and Spark SQL, the underlying engine has information about the schema and operations.
    In this case, the developer can write less code while the optimizer does all the
    hard work.
  prefs: []
  type: TYPE_NORMAL
- en: The Catalyst optimizer contains libraries for representing trees and applying
    rules to transform the trees. These tree transformations are applied to create
    the most optimized logical and physical execution plans. In the final phase, it
    generates Java bytecode using a special feature of the Scala language called **quasiquotes**.
    The optimizer also enables external developers to extend the optimizer by adding
    data-source-specific rules that result in pushing operations to external systems,
    or support for new data types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Catalyst optimizer arrives at the most optimized plan to execute the operations
    on hand. The actual execution and related improvements are provided by the Tungsten
    engine. The goal of Tungsten is to improve the memory and CPU efficiency of Spark
    backend execution. The following are some salient features of this engine:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the memory footprint and eliminating garbage collection overheads by
    bypassing (off-heap) Java memory management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code generation fuses across multiple operators and too many virtual function
    calls are avoided. The generated code looks like hand-optimized code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory layout is in columnar, in-memory parquet format because that enables
    vectorized processing and is also closer to usual data access operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory encoding using encoders. Encoders use runtime code generation to build
    custom byte code for faster and compact serialization and deserialization. Many
    operations can be performed in-place without deserialization because they are
    already in Tungsten binary format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming is a seemingly broad topic! If you take a closer look at the real-world
    problems, businesses do not just want a streaming engine to make decisions in
    real time. There has always been a need to integrate both batch stack and streaming
    stack, and integrate with external storage systems and applications. Also, the
    solution should be such that it should adapt to dynamic changes in business logic
    to address new and changing business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark 2.0 has the first version of the higher level stream processing
    API called the **Structured Streaming** engine. This scalable and fault-tolerant
    engine leans on the Spark SQL API to simplify the development of real-time, continuous
    big data applications. It is probably the first successful attempt in unifying
    the batch and streaming computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a technical level, Structured Streaming leans on the Spark SQL API, which
    extends DataFrames/Datasets, which we already discussed in the previous sections.
    Spark 2.0 lets you perform radically different activities in a unified way, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Building ML models and applying them on streaming data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining streaming data with other static data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing ad hoc, interactive, and batch queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing queries at runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating data streams and serving using Spark SQL JDBC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike other streaming engines, Spark lets you combine real-time **Streaming
    Data** with **Static data** and lets you perform the preceding operations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Structured Streaming](img/image_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fundamentally, Structured Streaming is empowered by Spark SQL's Catalyst optimizer.
    So, it frees up the developers from worrying about the underlying plumbing of
    making queries more efficient while dealing with static or real-time streams of
    data.
  prefs: []
  type: TYPE_NORMAL
- en: As of this writing, Structured Streaming of Spark 2.0 is focused on ETL, and
    later versions will have more operators and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Let us look at a simple example. The following example listens to **System Activity
    Report** (**sar**) on Linux on a local machine and computes the average free memory.
    System Activity Report gives system activity statistics and the current example
    collects memory usage, reported 20 times at a 2-second interval. The Spark stream
    reads this streaming output and computes average memory. We use a handy networking
    utility **netcat** (**nc**) to redirect the `sar` output onto a given port. The
    options `l` and `k` specify that `nc` should listen for an incoming connection
    and it has to keep listening for another connection even after its current connection
    is completed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-Streaming example
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Python:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example defined a continuous data frame (also known as stream)
    to listen to a particular port, perform some transformations, and aggregations
    and show continuous output.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark streaming programming model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As demonstrated earlier in this chapter, there is just a single API to take
    care of both static and streaming data. The idea is to treat the real-time data
    stream as a table that is continuously being appended, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Spark streaming programming model](img/image_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So whether for static or streaming data, you just fire up the batch-like queries
    as you would do on static data tables, and Spark runs it as an incremental query
    on the unbounded input table, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Spark streaming programming model](img/image_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the developers define a query on the input table, in the same way for both
    static-bounded as well as dynamic-unbounded table. Let us understand the various
    technical jargons for this whole process to understand how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input:** Data from sources as an append-only table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trigger:** When to check the input for new data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query:** What operation to perform on the data, such as filter, group, and
    so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Result:** The resultant table at every trigger interval'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output:** Choose what part of the result to write to the data sink after
    every trigger'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now look at how the Spark SQL planner treats the whole process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Spark streaming programming model](img/image_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Courtesy: Databricks'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot is very simply explained in the structured programming
    guide at the official Apache Spark site, as indicated in the *References* section.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Spark streaming programming model](img/image_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we need to know about the supported output models. Every time
    the result table is updated, the changes need to be written to an external system,
    such as HDFS, S3, or any other database. We usually prefer to write output incrementally.
    For this purpose, Structured Streaming provides three output modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Append:** In the external storage, only the new rows appended to the result
    table since the last trigger will be written. This is applicable only on queries
    where existing rows in the result table cannot change (for example, a map on an
    input stream).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete:** In the external storage, the entire updated result table will
    be written as is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update:** In the external storage, only the rows that were updated in the
    result table since the last trigger will be changed. This mode works for output
    sinks that can be updated in place, such as a MySQL table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our example, we used complete mode, which was straightaway writing to the
    console. You may want to write into some external file such as Parquet to get
    a better understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you look at the "behind the screen" execution mechanism of the operations
    performed on **DataFrames/Datasets**, it would appear as the following figure
    suggests:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Under the hood](img/image_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Please note here that the **Planner** knows apriori how to convert a streaming
    **Logical Plan** to a continuous series of **Incremental Execution Plans**. This
    can be represented by the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Under the hood](img/image_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The **Planner** can poll the data sources for new data to be able to plan the
    execution in an optimized way.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with other streaming engines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have discussed many unique features of Structured Streaming. Let us now
    have a comparative view with other available streaming engines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparison with other streaming engines](img/image_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Courtesy: Databricks'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed how unified data access is empowered by Spark. It lets you process
    data in a myriad of ways to build end-to-end continuous applications by enabling
    various analytic workloads, such as ETL processing, ad hoc queries, online machine
    learning modeling, or to generate necessary reports... all of this in a unified
    way by letting you work on both static as well as streaming data using a high-level,
    SQL-like API. In this way, Structured Streaming has substantially simplified the
    development and maintenance of real-time, continuous applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous applications](img/image_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Courtesy: Databricks'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed what is really meant by unified data access and
    how Spark serves this purpose. We took a closer look at the Datasets API and how
    real-time streaming is empowered through it. We learned the advantages of Datasets
    and also their limitations. We also looked at the fundamentals behind continuous
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will look at the various ways in which we can leverage
    the Spark platform for data analysis operations at scale.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf](http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf)
    : Spark SQL: Relational Data Processing in Spark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)
    : A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets - When to
    use them and why'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html](https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html)
    : Introducing Apache Spark Datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)
    : Deep Dive into Spark SQL''s Catalyst Optimizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)
    : Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)
    : Bringing Spark closer to baremetal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)
    : Structured Streaming API details'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
    : Spark Structured Streaming Programming Guide'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/](https://spark-summit.org/east-2016/events/structuring-spark-dataframes-datasets-and-streaming/):
    Structuring Apache Spark SQL, DataFrames, Datasets, and Streaming by Michael Armbrust'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html):
    Apache Spark Key terms explained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
