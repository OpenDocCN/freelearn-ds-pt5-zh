<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Text Analytics Using Spark ML</h1>
                
            
            
                
<p>"Programs must be written for people to read, and only incidentally for machines to execute."</p>
<p class="cdpalignright">- Harold Abelson</p>
<p class="mce-root">In this chapter, we will discuss the wonderful field of text analytics using Spark ML. Text analytics is a wide area in machine learning and is useful in many use cases, such as sentiment analysis, chat bots, email spam detection, and natural language processing. We will learn how to use Spark for text analysis with a focus on use cases of text classification using a 10,000 sample set of Twitter data.</p>
<p class="mce-root">In a nutshell, the following topics will be covered in this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Understanding text analytics</li>
<li class="mce-root1">Transformers and Estimators</li>
<li class="mce-root1">Tokenizer</li>
<li class="mce-root1">StopWordsRemover</li>
<li class="mce-root1">NGrams</li>
<li class="mce-root1">TF-IDF</li>
<li class="mce-root1">Word2Vec</li>
<li class="mce-root1">CountVectorizer</li>
<li class="mce-root1">Topic modeling using LDA</li>
<li class="mce-root1">Implementing text classification</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Understanding text analytics</h1>
                
            
            
                
<p class="mce-root">We have explored the world of machine learning and Apache Spark's support for machine learning in the last few chapters. As we discussed, machine learning has a workflow, which is explained in the following steps:</p>
<ol class="calibre14">
<li value="1" class="mce-root1">Loading or ingesting data.</li>
<li value="2" class="mce-root1">Cleansing the data.</li>
<li value="3" class="mce-root1">Extracting features from the data.</li>
<li value="4" class="mce-root1">Training a model on the data to generate desired outcomes based on features.</li>
<li value="5" class="mce-root1">Evaluate or predict some outcome based on the data.</li>
</ol>
<p class="mce-root">A simplified view of a typical pipeline is as shown in the following diagram:</p>
<div><img class="image-border200" src="img/00310.jpeg"/></div>
<p class="mce-root">Hence, there are several stages of transformation of data possible before the model is trained and then subsequently deployed. Moreover, we should expect refinement of the features and model attributes. We could even explore a completely different algorithm repeating the entire sequence of tasks as part of a new workflow.</p>
<p class="mce-root">A pipeline of steps can be created using several steps of transformation, and for this purpose, we use a <strong class="calibre1">domain specific language</strong> (<strong class="calibre1">DSL</strong>) to define the nodes (data transformation steps) to create a <strong class="calibre1">DAG</strong> (<strong class="calibre1">Directed Acyclic Graph</strong>) of nodes. Hence, the ML pipeline is a sequence of Transformers and Estimators to fit a Pipeline model to an input dataset. Each stage in the pipeline is known as <em class="calibre8">Pipeline stage</em>, which are listed as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Estimator</li>
<li class="mce-root1">Model</li>
<li class="mce-root1">Pipeline</li>
<li class="mce-root1">Transformer</li>
<li class="mce-root1">Predictor</li>
</ul>
<p class="mce-root">When you look at a line of text, we see sentences, phrases, words, nouns, verbs, punctuation, and so on, which when put together, have a meaning and purpose. Humans are very good at understanding sentences, words, and slangs and annotations or contexts extremely well. This comes from years of practice and learning how to read/write, proper grammar, punctuation, exclamations, and so on. So, how can we write a computer program to try to replicate this kind of capability?</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Text analytics</h1>
                
            
            
                
<p class="mce-root">Text analytics is the way to unlock the meaning from a collection of text. By using various techniques and algorithms to process and analyze the text data, we can uncover patterns and themes in the data. The goal of all this is to make sense of the unstructured text in order to derive contextual meaning and relationships.</p>
<p class="mce-root">Text analytics utilizes several broad categories of techniques, which we will cover next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Sentiment analysis</h1>
                
            
            
                
<p class="mce-root">Analyzing the political opinions of people on Facebook, Twitter, and other social media is a good example of sentiment analysis. Similarly, analyzing the reviews of restaurants on Yelp is also another great example of sentiment analysis.</p>
<p class="mce-root"><strong class="calibre1">Natural Language Processing</strong> (<strong class="calibre1">NLP</strong>) frameworks and libraries, such as OpenNLP and Stanford NLP, are typically used to implement sentiment analysis.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Topic modeling</h1>
                
            
            
                
<p class="mce-root">Topic modeling is a useful technique for detecting the topics or themes in a corpus of documents. This is an unsupervised algorithm, which can find themes in a set of documents. An example is to detect topics covered in a news article. Another example is to detect the ideas in a patent application.</p>
<p class="mce-root">The <strong class="calibre1">latent dirichlet allocation</strong> (<strong class="calibre1">LDA</strong>) is a popular clustering model using unsupervised algorithm, while <strong class="calibre1">latent semantic analysis</strong> (<strong class="calibre1">LSA</strong>) uses a probabilistic model on co-occurrence data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">TF-IDF (term frequency - inverse document frequency)</h1>
                
            
            
                
<p class="mce-root">TF-IDF measures how frequently words appear in documents and the relative frequency across the set of documents. This information can be used in building classifiers and predictive models. The examples are spam classification, chat conversations, and so on.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Named entity recognition (NER)</h1>
                
            
            
                
<p class="mce-root">Named entity recognition detects the usage of words and nouns in sentences to extract information about persons, organizations, locations, and so on. This gives important contextual information on the actual content of the documents rather than just treating words as the primary entities.</p>
<p class="mce-root">Stanford NLP and OpenNLP have implementation for NER algorithms.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Event extraction</h1>
                
            
            
                
<p class="mce-root">Event extraction expands on the NER establishing relationships around the entities detected. This can be used to make inferences on the relationship between two entities. Hence, there is an additional layer of semantic understanding to make sense of the document content.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Transformers and Estimators</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre1">Transformer</strong> is a function object that transforms one dataset to another by applying the transformation logic (function) to the input dataset yielding an output dataset. There are two types of Transformers the standard Transformer and the Estimator Transformer.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Standard Transformer</h1>
                
            
            
                
<p class="mce-root">A standard Transformer transforms the input dataset into the output dataset, explicitly applying transformation function to the input data. There is no dependency on the input data other than reading the input column and generating the output column.</p>
<p class="mce-root">Such Transformers are invoked as shown next:</p>
<pre class="calibre19">
<em class="calibre8">outputDF = transfomer.</em><strong class="calibre1">transform</strong><em class="calibre8">(inputDF)</em>
</pre>
<p class="mce-root">Examples of standard Transformers are as follows and will be explained in detail in the subsequent sections:</p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">Tokenizer</kbd>: This splits sentences into words using space as the delimiter</li>
<li class="mce-root1"><kbd class="calibre11">RegexTokenizer</kbd>: This splits sentences into words using regular expressions to split</li>
<li class="mce-root1"><kbd class="calibre11">StopWordsRemover</kbd>: This removes commonly used stop words from the list of words</li>
<li class="mce-root1"><kbd class="calibre11">Binarizer</kbd>: This converts the strings to binary numbers 0/1</li>
<li class="mce-root1"><kbd class="calibre11">NGram</kbd>: This creates N word phrases from the sentences</li>
<li class="mce-root1"><kbd class="calibre11">HashingTF</kbd>: This creates Term frequency counts using hash table to index the words</li>
<li class="mce-root1"><kbd class="calibre11">SQLTransformer</kbd>: This implements the transformations, which are defined by SQL statements</li>
<li class="mce-root1"><kbd class="calibre11">VectorAssembler</kbd>: This combines a given list of columns into a single vector column</li>
</ul>
<p class="mce-root">The diagram of a standard Transformer is as follows, where the input column from an input dataset is transformed into an output column generating the output dataset:</p>
<div><img class="image-border201" src="img/00247.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Estimator Transformer</h1>
                
            
            
                
<p class="mce-root">An Estimator Transformer transforms the input dataset into the output dataset by first generating a Transformer based on the input dataset. Then the Transformer processes the input data, reading the input column and generating the output column in the output dataset.</p>
<p class="mce-root">Such Transformers are invoked as shown next:</p>
<pre class="calibre19">
<em class="calibre8">transformer = estimator.</em><strong class="calibre1">fit</strong><em class="calibre8">(inputDF)<br class="title-page-name"/></em><em class="calibre8">outputDF = transformer.</em><strong class="calibre1">transform</strong><em class="calibre8">(inputDF)</em>
</pre>
<p class="mce-root">The examples of Estimator Transformers are as follows:</p>
<ul class="calibre9">
<li class="mce-root1">IDF</li>
<li class="mce-root1">LDA</li>
<li class="mce-root1">Word2Vec</li>
</ul>
<p class="mce-root">The diagram of an Estimator Transformer is as follows, where the input column from an input dataset is transformed into an output column generating the output dataset:</p>
<div><img class="image-border202" src="img/00287.jpeg"/></div>
<p class="mce-root">In the next few sections, we will look deeper into text analytics using a simple example dataset, which consists of lines of text (sentences), as shown in the following screenshot:</p>
<div><img class="image-border203" src="img/00330.jpeg"/></div>
<p class="mce-root">The upcoming code is used to load the text data into the input dataset.</p>
<p class="mce-root">Initialize a sequence of sentences called lines using a sequence of pairs of ID and text as shown next.</p>
<pre class="calibre19">
<strong class="calibre1">val lines = Seq(</strong><br class="title-page-name"/><strong class="calibre1"> | (1, "Hello there, how do you like the book so far?"),</strong><br class="title-page-name"/><strong class="calibre1"> | (2, "I am new to Machine Learning"),</strong><br class="title-page-name"/><strong class="calibre1"> | (3, "Maybe i should get some coffee before starting"),</strong><br class="title-page-name"/><strong class="calibre1"> | (4, "Coffee is best when you drink it hot"),</strong><br class="title-page-name"/><strong class="calibre1"> | (5, "Book stores have coffee too so i should go to a book store")</strong><br class="title-page-name"/><strong class="calibre1"> | )</strong><br class="title-page-name"/>lines: Seq[(Int, String)] = List((1,Hello there, how do you like the book so far?), (2,I am new to Machine Learning), (3,Maybe i should get some coffee before starting), (4,Coffee is best when you drink it hot), (5,Book stores have coffee too so i should go to a book store))
</pre>
<p class="mce-root">Next, invoke the <kbd class="calibre11">createDataFrame()</kbd> function to create a DataFrame from the sequence of sentences we saw earlier.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val sentenceDF = spark.createDataFrame(lines).toDF("id", "sentence")</strong><br class="title-page-name"/>sentenceDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string]
</pre>
<p class="mce-root">Now you can see the newly created dataset, which shows the Sentence DataFrame containing two column IDs and sentences.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; sentenceDF.show(false)</strong><br class="title-page-name"/>|id|sentence |<br class="title-page-name"/>|1 |Hello there, how do you like the book so far? |<br class="title-page-name"/>|2 |I am new to Machine Learning |<br class="title-page-name"/>|3 |Maybe i should get some coffee before starting |<br class="title-page-name"/>|4 |Coffee is best when you drink it hot |<br class="title-page-name"/>|5 |Book stores have coffee too so i should go to a book store|
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Tokenization</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre1">Tokenizer</strong> converts the input string into lowercase and then splits the string with whitespaces into individual tokens. A given sentence is split into words either using the default space delimiter or using a customer regular expression based Tokenizer. In either case, the input column is transformed into an output column. In particular, the input column is usually a String and the output column is a Sequence of Words.</p>
<p class="mce-root">Tokenizers are available by importing two packages shown next, the <kbd class="calibre11">Tokenizer</kbd> and the <kbd class="calibre11">RegexTokenize</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.ml.feature.Tokenizer</strong><br class="title-page-name"/><strong class="calibre1">import org.apache.spark.ml.feature.RegexTokenizer</strong>
</pre>
<p class="mce-root">First, you need to initialize a <kbd class="calibre11">Tokenizer</kbd> specifying the input column and the output column:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val tokenizer = new Tokenizer().setInputCol("sentence").setOutputCol("words")</strong><br class="title-page-name"/>tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_942c8332b9d8
</pre>
<p class="mce-root">Next, invoking the <kbd class="calibre11">transform()</kbd> function on the input dataset yields an output dataset:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val wordsDF = tokenizer.transform(sentenceDF)</strong><br class="title-page-name"/>wordsDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 1 more field]
</pre>
<p class="mce-root">The following is the output dataset showing the input column IDs, sentence, and the output column words, which contain the sequence of words:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; wordsDF.show(false)</strong><br class="title-page-name"/>|id|sentence |words |<br class="title-page-name"/>|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |<br class="title-page-name"/>|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |<br class="title-page-name"/>|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |<br class="title-page-name"/>|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |<br class="title-page-name"/>|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|
</pre>
<p class="mce-root">On the other hand, if you wanted to set up a regular expression based <kbd class="calibre11">Tokenizer</kbd>, you have to use the <kbd class="calibre11">RegexTokenizer</kbd> instead of <kbd class="calibre11">Tokenizer</kbd>. For this, you need to initialize a <kbd class="calibre11">RegexTokenizer</kbd> specifying the input column and the output column along with the regex pattern to be used:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val regexTokenizer = new RegexTokenizer().setInputCol("sentence").setOutputCol("regexWords").setPattern("\\W")</strong><br class="title-page-name"/>regexTokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_15045df8ce41
</pre>
<p class="mce-root">Next, invoking the <kbd class="calibre11">transform()</kbd> function on the input dataset yields an output dataset:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val regexWordsDF = regexTokenizer.transform(sentenceDF)</strong><br class="title-page-name"/>regexWordsDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 1 more field]
</pre>
<p class="mce-root">The following is the output dataset showing the input column IDs, sentence, and the output column <kbd class="calibre11">regexWordsDF</kbd>, which contain the sequence of words:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; regexWordsDF.show(false)</strong><br class="title-page-name"/>|id|sentence |regexWords |<br class="title-page-name"/>|1 |Hello there, how do you like the book so far? |[hello, there, how, do, you, like, the, book, so, far] |<br class="title-page-name"/>|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |<br class="title-page-name"/>|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |<br class="title-page-name"/>|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |<br class="title-page-name"/>|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|
</pre>
<p class="mce-root">The diagram of a <kbd class="calibre11">Tokenizer</kbd> is as follows, wherein the sentence from the input text is split into words using the space delimiter:</p>
<div><img class="image-border204" src="img/00150.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">StopWordsRemover</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">StopWordsRemover</kbd> is a Transformer that takes a <kbd class="calibre11">String</kbd> array of words and returns a <kbd class="calibre11">String</kbd> array after removing all the defined stop words. Some examples of stop words are I, you, my, and, or, and so on which are fairly commonly used in the English language. You can override or extend the set of stop words to suit the purpose of the use case. Without this cleansing process, the subsequent algorithms might be biased because of the common words.</p>
<p class="mce-root">In order to invoke <kbd class="calibre11">StopWordsRemover</kbd>, you need to import the following package:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.ml.feature.StopWordsRemover</strong>
</pre>
<p class="mce-root">First, you need to initialize a <kbd class="calibre11">StopWordsRemover</kbd> , specifying the input column and the output column. Here, we are choosing the words column created by the <kbd class="calibre11">Tokenizer</kbd> and generate an output column for the filtered words after removal of stop words:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val remover = new StopWordsRemover().setInputCol("words").setOutputCol("filteredWords")</strong><br class="title-page-name"/>remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_48d2cecd3011
</pre>
<p class="mce-root">Next, invoking the <kbd class="calibre11">transform()</kbd> function on the input dataset yields an output dataset:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val noStopWordsDF = remover.transform(wordsDF)</strong><br class="title-page-name"/>noStopWordsDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 2 more fields]
</pre>
<p class="mce-root">The following is the output dataset showing the input column IDs, sentence, and the output column <kbd class="calibre11">filteredWords</kbd>, which contains the sequence of words:</p>
<p class="mce-root"> </p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; noStopWordsDF.show(false)</strong><br class="title-page-name"/>|id|sentence |words |filteredWords |<br class="title-page-name"/>|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |<br class="title-page-name"/>|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |<br class="title-page-name"/>|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |<br class="title-page-name"/>|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |<br class="title-page-name"/>|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|
</pre>
<p class="mce-root">The following is the output dataset showing just the sentence and the <kbd class="calibre11">filteredWords</kbd>, which contains the sequence of filtered words:</p>
<p class="mce-root"> </p>
<pre class="calibre19">
<br class="title-page-name"/><strong class="calibre1">scala&gt; noStopWordsDF.select("sentence", "filteredWords").show(5,false)</strong><br class="title-page-name"/>|sentence |filteredWords |<br class="title-page-name"/>|Hello there, how do you like the book so far? |[hello, there,, like, book, far?] |<br class="title-page-name"/>|I am new to Machine Learning |[new, machine, learning] |<br class="title-page-name"/>|Maybe i should get some coffee before starting |[maybe, get, coffee, starting] |<br class="title-page-name"/>|Coffee is best when you drink it hot |[coffee, best, drink, hot] |<br class="title-page-name"/>|Book stores have coffee too so i should go to a book store|[book, stores, coffee, go, book, store]|<br class="title-page-name"/><br class="title-page-name"/>
</pre>
<p class="mce-root">The diagram of the <kbd class="calibre11">StopWordsRemover</kbd> is as follows, which shows the words filtered to remove stop words such as I, should, some, and before:</p>
<div><img class="image-border205" src="img/00021.jpeg"/></div>
<p class="mce-root">Stop words are set by default, but can be overridden or amended very easily, as shown in the following code snippet, where we will remove hello from the filtered words considering hello as a stop word:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val noHello = Array("hello") ++ remover.getStopWords</strong><br class="title-page-name"/>noHello: Array[String] = Array(hello, i, me, my, myself, we, our, ours, ourselves, you, your, yours, yourself, yourselves, he, him, his, himself, she, her, hers, herself, it, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, these, those, am, is, are, was, were ...<br class="title-page-name"/>scala&gt;<br class="title-page-name"/><br class="title-page-name"/>//create new transfomer using the amended Stop Words list<br class="title-page-name"/><strong class="calibre1">scala&gt; val removerCustom = new StopWordsRemover().setInputCol("words").setOutputCol("filteredWords").setStopWords(noHello)</strong><br class="title-page-name"/>removerCustom: org.apache.spark.ml.feature.StopWordsRemover = stopWords_908b488ac87f<br class="title-page-name"/><br class="title-page-name"/>//invoke transform function<br class="title-page-name"/><strong class="calibre1">scala&gt; val noStopWordsDFCustom = removerCustom.transform(wordsDF)</strong><br class="title-page-name"/>noStopWordsDFCustom: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 2 more fields]<br class="title-page-name"/><br class="title-page-name"/>//output dataset showing only sentence and filtered words - now will not show hello<br class="title-page-name"/><strong class="calibre1">scala&gt; noStopWordsDFCustom.select("sentence", "filteredWords").show(5,false)</strong><br class="title-page-name"/>+----------------------------------------------------------+---------------------------------------+<br class="title-page-name"/>|sentence |filteredWords |<br class="title-page-name"/>+----------------------------------------------------------+---------------------------------------+<br class="title-page-name"/>|Hello there, how do you like the book so far? |[there,, like, book, far?] |<br class="title-page-name"/>|I am new to Machine Learning |[new, machine, learning] |<br class="title-page-name"/>|Maybe i should get some coffee before starting |[maybe, get, coffee, starting] |<br class="title-page-name"/>|Coffee is best when you drink it hot |[coffee, best, drink, hot] |<br class="title-page-name"/>|Book stores have coffee too so i should go to a book store|[book, stores, coffee, go, book, store]|<br class="title-page-name"/>+----------------------------------------------------------+---------------------------------------+
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">NGrams</h1>
                
            
            
                
<p class="mce-root">NGrams are word combinations created as sequences of words. N stands for the number of words in the sequence. For example, 2-gram is two words together, 3-gram is three words together. <kbd class="calibre11">setN()</kbd> is used to specify the value of <kbd class="calibre11">N</kbd>.</p>
<p class="mce-root">In order to generate NGrams, you need to import the package:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.ml.feature.NGram</strong>
</pre>
<p class="mce-root">First, you need to initialize an <kbd class="calibre11">NGram</kbd> generator specifying the input column and the output column. Here, we are choosing the filtered words column created by the <kbd class="calibre11">StopWordsRemover</kbd> and generating an output column for the filtered words after removal of stop words:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val ngram = new NGram().setN(2).setInputCol("filteredWords").setOutputCol("ngrams")</strong><br class="title-page-name"/>ngram: org.apache.spark.ml.feature.NGram = ngram_e7a3d3ab6115
</pre>
<p class="mce-root">Next, invoking the <kbd class="calibre11">transform()</kbd> function on the input dataset yields an output dataset:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val nGramDF = ngram.transform(noStopWordsDF)</strong><br class="title-page-name"/>nGramDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 3 more fields]
</pre>
<p class="mce-root">The following is the output dataset showing the input column ID, sentence, and the output column <kbd class="calibre11">ngram</kbd>, which contain the sequence of n-grams:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; nGramDF.show(false)</strong><br class="title-page-name"/>|id|sentence |words |filteredWords |ngrams |<br class="title-page-name"/>|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |[hello there,, there, like, like book, book far?] |<br class="title-page-name"/>|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |[new machine, machine learning] |<br class="title-page-name"/>|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |[maybe get, get coffee, coffee starting] |<br class="title-page-name"/>|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |[coffee best, best drink, drink hot] |<br class="title-page-name"/>|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|[book stores, stores coffee, coffee go, go book, book store]|
</pre>
<p class="mce-root">The following is the output dataset showing the sentence and 2-grams:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; nGramDF.select("sentence", "ngrams").show(5,false)</strong><br class="title-page-name"/>|sentence |ngrams |<br class="title-page-name"/>|Hello there, how do you like the book so far? |[hello there,, there, like, like book, book far?] |<br class="title-page-name"/>|I am new to Machine Learning |[new machine, machine learning] |<br class="title-page-name"/>|Maybe i should get some coffee before starting |[maybe get, get coffee, coffee starting] |<br class="title-page-name"/>|Coffee is best when you drink it hot |[coffee best, best drink, drink hot] |<br class="title-page-name"/>|Book stores have coffee too so i should go to a book store|[book stores, stores coffee, coffee go, go book, book store]|<br class="title-page-name"/><br class="title-page-name"/>
</pre>
<p class="mce-root">The diagram of an NGram is as follows, which shows 2-grams generated from the sentence after tokenizing and removing stop words:</p>
<div><img class="image-border206" src="img/00163.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">TF-IDF</h1>
                
            
            
                
<p class="mce-root">TF-IDF stands for term frequency-inverse document frequency, which measures how important a word is to a document in a collection of documents. It is used extensively in informational retrieval and reflects the weightage of the word in the document. The TF-IDF value increases in proportion to the number of occurrences of the words otherwise known as frequency of the word/term and consists of two key elements, the term frequency and the inverse document frequency.</p>
<p class="mce-root">TF is the term frequency, which is the frequency of a word/term in the document.<br class="title-page-name"/>
For a term <em class="calibre8">t</em>, <em class="calibre8">tf</em> measures the number of times term <em class="calibre8">t</em> occurs in document <em class="calibre8">d</em>. <em class="calibre8">tf</em> is implemented in Spark using hashing where a term is mapped into an index by applying a hash function.</p>
<p class="mce-root">IDF is the inverse document frequency, which represents the information a term provides about the tendency of the term to appear in documents. IDF is a log-scaled inverse function of documents containing the term:</p>
<p class="mce-root">IDF = TotalDocuments/Documents containing Term</p>
<p class="mce-root">Once we have <em class="calibre8">TF</em> and <em class="calibre8">IDF</em>, we can compute the <em class="calibre8">TF-IDF</em> value by multiplying the <em class="calibre8">TF</em> and <em class="calibre8">IDF</em>:</p>
<p class="mce-root">TF-IDF = TF * IDF</p>
<p class="mce-root">We will now look at how we can generate <em class="calibre8">TF</em> using the HashingTF Transformer in Spark ML.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">HashingTF</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre1">HashingTF</strong> is a Transformer, which takes a set of terms and converts them into vectors of fixed length by hashing each term using a hash function to generate an index for each term. Then, term frequencies are generated using the indices of the hash table.</p>
<p>In Spark, the HashingTF uses the <strong class="calibre27">MurmurHash3</strong> algorithm to hash terms.</p>
<p class="mce-root">In order to use <kbd class="calibre11">HashingTF</kbd>, you need to import the following package:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.ml.feature.HashingTF</strong>
</pre>
<p class="mce-root">First, you need to initialize a <kbd class="calibre11">HashingTF</kbd> specifying the input column and the output column. Here, we choose the filtered words column created by the <kbd class="calibre11">StopWordsRemover</kbd> Transformer and generate an output column <kbd class="calibre11">rawFeaturesDF</kbd>. We also choose the number of features as 100:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val hashingTF = new HashingTF().setInputCol("filteredWords").setOutputCol("rawFeatures").setNumFeatures(100)</strong><br class="title-page-name"/>hashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_b05954cb9375
</pre>
<p class="mce-root">Next, invoking the <kbd class="calibre11">transform()</kbd> function on the input dataset yields an output dataset:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val rawFeaturesDF = hashingTF.transform(noStopWordsDF)</strong><br class="title-page-name"/>rawFeaturesDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 3 more fields]
</pre>
<p class="mce-root">The following is the output dataset showing the input column IDs, sentence, and the output column <kbd class="calibre11">rawFeaturesDF</kbd>, which contains the features represented by a vector:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; rawFeaturesDF.show(false)</strong><br class="title-page-name"/>|id |sentence |words |filteredWords |rawFeatures |<br class="title-page-name"/>|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |(100,[30,48,70,93],[2.0,1.0,1.0,1.0]) |<br class="title-page-name"/>|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |(100,[25,52,72],[1.0,1.0,1.0]) |<br class="title-page-name"/>|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |(100,[16,51,59,99],[1.0,1.0,1.0,1.0]) |<br class="title-page-name"/>|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |(100,[31,51,63,72],[1.0,1.0,1.0,1.0]) |<br class="title-page-name"/>|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|(100,[43,48,51,77,93],[1.0,1.0,1.0,1.0,2.0])|
</pre>
<p class="mce-root">Let's look at the preceding output to have a better understanding. If you just look at columns <kbd class="calibre11">filteredWords</kbd> and <kbd class="calibre11">rawFeatures</kbd> alone, you can see that,</p>
<ol class="calibre14">
<li value="1" class="mce-root1">The array of words <kbd class="calibre11">[hello, there, like, book, and far]</kbd> is transformed to raw feature vector <kbd class="calibre11">(100,[30,48,70,93],[2.0,1.0,1.0,1.0])</kbd>.</li>
<li value="2" class="mce-root1">The array of words <kbd class="calibre11">(book, stores, coffee, go, book, and store)</kbd> is transformed to raw feature vector <kbd class="calibre11">(100,[43,48,51,77,93],[1.0,1.0,1.0,1.0,2.0])</kbd>.</li>
</ol>
<p class="mce-root">So, what does the vector represent here? The underlying logic is that each word is hashed into an integer and counted for the number of occurrences in the word array.</p>
<p class="mce-root">Spark internally uses a <kbd class="calibre11">hashMap</kbd> for this <kbd class="calibre11">mutable.HashMap.empty[Int, Double]</kbd>, which stores the hash value of each word as <kbd class="calibre11">Integer</kbd> key and the number of occurrences as double value. Double is used so that we can use it in conjunction with IDF (we'll talk about it in the next section). Using this map, the array <kbd class="calibre11">[book, stores, coffee, go, book, store]</kbd> can be seen as <kbd class="calibre11">[hashFunc(book), hashFunc(stores), hashFunc(coffee), hashFunc(go), hashFunc(book), hashFunc(store)]</kbd><em class="calibre8">,</em> which is equal to <kbd class="calibre11">[43,48,51,77,93]</kbd><em class="calibre8">.</em> Then, if you count the number of occurrences too, that is, <kbd class="calibre11">book-2, coffee-1,go-1,store-1,stores-1</kbd>.</p>
<p class="mce-root">Combining the preceding information, we can generate a vector <kbd class="calibre11">(numFeatures, hashValues, Frequencies)</kbd><strong class="calibre1">,</strong> which in this case will be <kbd class="calibre11">(100,[43,48,51,77,93],[1.0,1.0,1.0,1.0,2.0])</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Inverse Document Frequency (IDF)</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre1">Inverse Document Frequency</strong> (<strong class="calibre1">IDF</strong>) is an estimator, which is fit onto a dataset and then generates features by scaling the input features. Hence, IDF works on output of a HashingTF Transformer.</p>
<p class="mce-root">In order to invoke IDF, you need to import the package:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.ml.feature.IDF</strong>
</pre>
<p class="mce-root">First, you need to initialize an <kbd class="calibre11">IDF</kbd> specifying the input column and the output column. Here, we are choosing the words column <kbd class="calibre11">rawFeatures</kbd> created by the HashingTF and generate an output column feature:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features")</strong><br class="title-page-name"/>idf: org.apache.spark.ml.feature.IDF = idf_d8f9ab7e398e
</pre>
<p class="mce-root">Next, invoking the <kbd class="calibre11">fit()</kbd> function on the input dataset yields an output Transformer:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val idfModel = idf.fit(rawFeaturesDF)</strong><br class="title-page-name"/>idfModel: org.apache.spark.ml.feature.IDFModel = idf_d8f9ab7e398e
</pre>
<p class="mce-root">Further, invoking the <kbd class="calibre11">transform()</kbd> function on the input dataset yields an output dataset:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val featuresDF = idfModel.transform(rawFeaturesDF)</strong><br class="title-page-name"/>featuresDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 4 more fields]
</pre>
<p class="mce-root">The following is the output dataset showing the input column ID and the output column features, which contain the vector of scaled features produced by HashingTF in the previous transformation:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; featuresDF.select("id", "features").show(5, false)</strong><br class="title-page-name"/>|id|features |<br class="title-page-name"/>|1 |(20,[8,10,13],[0.6931471805599453,3.295836866004329,0.6931471805599453]) |<br class="title-page-name"/>|2 |(20,[5,12],[1.0986122886681098,1.3862943611198906]) |<br class="title-page-name"/>|3 |(20,[11,16,19],[0.4054651081081644,1.0986122886681098,2.1972245773362196]) |<br class="title-page-name"/>|4 |(20,[3,11,12],[0.6931471805599453,0.8109302162163288,0.6931471805599453]) |<br class="title-page-name"/>|5 |(20,[3,8,11,13,17],[0.6931471805599453,0.6931471805599453,0.4054651081081644,1.3862943611198906,1.0986122886681098])|<br class="title-page-name"/><br class="title-page-name"/>
</pre>
<p class="mce-root">The following is the output dataset showing the input column IDs, sentence, <kbd class="calibre11">rawFeatures</kbd>, and the output column features, which contain the vector of scaled features produced by HashingTF in the previous transformation:</p>
<pre class="calibre19">
<br class="title-page-name"/><strong class="calibre1">scala&gt; featuresDF.show(false)</strong><br class="title-page-name"/>|id|sentence |words |filteredWords |rawFeatures |features |<br class="title-page-name"/>|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |(20,[8,10,13],[1.0,3.0,1.0]) |(20,[8,10,13],[0.6931471805599453,3.295836866004329,0.6931471805599453]) |<br class="title-page-name"/>|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |(20,[5,12],[1.0,2.0]) |(20,[5,12],[1.0986122886681098,1.3862943611198906]) |<br class="title-page-name"/>|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |(20,[11,16,19],[1.0,1.0,2.0]) |(20,[11,16,19],[0.4054651081081644,1.0986122886681098,2.1972245773362196]) |<br class="title-page-name"/>|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |(20,[3,11,12],[1.0,2.0,1.0]) |(20,[3,11,12],[0.6931471805599453,0.8109302162163288,0.6931471805599453]) |<br class="title-page-name"/>|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|(20,[3,8,11,13,17],[1.0,1.0,1.0,2.0,1.0])|(20,[3,8,11,13,17],[0.6931471805599453,0.6931471805599453,0.4054651081081644,1.3862943611198906,1.0986122886681098])|
</pre>
<p class="mce-root">The diagram of the TF-IDF is as follows, which shows the generation of <strong class="calibre1">TF-IDF Features</strong>:</p>
<div><img class="image-border207" src="img/00089.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Word2Vec</h1>
                
            
            
                
<p class="mce-root">Word2Vec is a sophisticated neural network style natural language processing tool and uses a technique called <strong class="calibre1">skip-grams</strong> to convert a sentence of words into an embedded vector representation. Let's look at an example of how this can be used by looking at a collection of sentences about animals:</p>
<ul class="calibre9">
<li class="mce-root1">A dog was barking</li>
<li class="mce-root1">Some cows were grazing the grass</li>
<li class="mce-root1">Dogs usually bark randomly</li>
<li class="mce-root1">The cow likes grass</li>
</ul>
<p class="mce-root">Using neural network with a hidden layer (machine learning algorithm used in many unsupervised learning applications), we can learn (with enough examples) that <em class="calibre8">dog</em> and <em class="calibre8">barking</em> are related, <em class="calibre8">cow</em> and <em class="calibre8">grass</em> are related in the sense that they appear close to each other a lot, which is measured by probabilities. The output of <kbd class="calibre11">Word2vec</kbd> is a vector of <kbd class="calibre11">Double</kbd> features.</p>
<p class="mce-root">In order to invoke <kbd class="calibre11">Word2vec</kbd>, you need to import the package:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.ml.feature.Word2Vec</strong>
</pre>
<p class="mce-root">First, you need to initialize a <kbd class="calibre11">Word2vec</kbd> Transformer specifying the input column and the output column. Here, we are choosing the words column created by the <kbd class="calibre11">Tokenizer</kbd> and generate an output column for the word vector of size 3:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val word2Vec = new Word2Vec().setInputCol("words").setOutputCol("wordvector").setVectorSize(3).setMinCount(0)</strong><br class="title-page-name"/>word2Vec: org.apache.spark.ml.feature.Word2Vec = w2v_fe9d488fdb69
</pre>
<p class="mce-root">Next, invoking the <kbd class="calibre11">fit()</kbd> function on the input dataset yields an output Transformer:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val word2VecModel = word2Vec.fit(noStopWordsDF)</strong><br class="title-page-name"/>word2VecModel: org.apache.spark.ml.feature.Word2VecModel = w2v_fe9d488fdb69
</pre>
<p class="mce-root">Further, invoking the <kbd class="calibre11">transform()</kbd> function on the input dataset yields an output dataset:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val word2VecDF = word2VecModel.transform(noStopWordsDF)</strong><br class="title-page-name"/>word2VecDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 3 more fields]
</pre>
<p class="mce-root">The following is the output dataset showing the input column IDs, sentence, and the output column <kbd class="calibre11">wordvector</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; word2VecDF.show(false)</strong><br class="title-page-name"/>|id|sentence |words |filteredWords |wordvector |<br class="title-page-name"/>|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |[0.006875938177108765,-0.00819675214588642,0.0040686681866645815]|<br class="title-page-name"/>|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |[0.026012470324834187,0.023195965060343344,-0.10863214979569116] |<br class="title-page-name"/>|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |[-0.004304863978177309,-0.004591284319758415,0.02117823390290141]|<br class="title-page-name"/>|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |[0.054064739029854536,-0.003801364451646805,0.06522738828789443] |<br class="title-page-name"/>|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|[-0.05887459063281615,-0.07891856770341595,0.07510609552264214] |
</pre>
<p class="mce-root">The diagram of the <strong class="calibre1">Word2Vec Features</strong> is as follows, which shows the words being converted into a vector:</p>
<div><img class="image-border208" src="img/00347.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">CountVectorizer</h1>
                
            
            
                
<p class="mce-root"><kbd class="calibre11">CountVectorizer</kbd> is used to convert a collection of text documents to vectors of token counts essentially producing sparse representations for the documents over the vocabulary. The end result is a vector of features, which can then be passed to other algorithms. Later on, we will see how to use the output from the <kbd class="calibre11">CountVectorizer</kbd> in LDA algorithm to perform topic detection.</p>
<p class="mce-root">In order to invoke <kbd class="calibre11">CountVectorizer</kbd>, you need to import the package:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.ml.feature.CountVectorizer</strong>
</pre>
<p class="mce-root">First, you need to initialize a <kbd class="calibre11">CountVectorizer</kbd> Transformer specifying the input column and the output column. Here, we are choosing the <kbd class="calibre11">filteredWords</kbd> column created by the <kbd class="calibre11">StopWordRemover</kbd> and generate output column features:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val countVectorizer = new CountVectorizer().setInputCol("filteredWords").setOutputCol("features")</strong><br class="title-page-name"/>countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_555716178088
</pre>
<p class="mce-root">Next, invoking the <kbd class="calibre11">fit()</kbd> function on the input dataset yields an output Transformer:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val countVectorizerModel = countVectorizer.fit(noStopWordsDF)</strong><br class="title-page-name"/>countVectorizerModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_555716178088
</pre>
<p class="mce-root">Further, invoking the <kbd class="calibre11">transform()</kbd> function on the input dataset yields an output dataset.</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val countVectorizerDF = countVectorizerModel.transform(noStopWordsDF)</strong><br class="title-page-name"/>countVectorizerDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 3 more fields]
</pre>
<p class="mce-root">The following is the output dataset showing the input column IDs, sentence, and the output column features:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; countVectorizerDF.show(false)</strong><br class="title-page-name"/>|id |sentence |words |filteredWords |features |<br class="title-page-name"/>|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |(18,[1,4,5,13,15],[1.0,1.0,1.0,1.0,1.0])|<br class="title-page-name"/>|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |(18,[6,7,16],[1.0,1.0,1.0]) |<br class="title-page-name"/>|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |(18,[0,8,9,14],[1.0,1.0,1.0,1.0]) |<br class="title-page-name"/>|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |(18,[0,3,10,12],[1.0,1.0,1.0,1.0]) |<br class="title-page-name"/>|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|(18,[0,1,2,11,17],[1.0,2.0,1.0,1.0,1.0])|
</pre>
<p class="mce-root">The diagram of a <kbd class="calibre11">CountVectorizer</kbd> is as follows, which shows the features generated from <kbd class="calibre11">StopWordsRemover</kbd> transformation:</p>
<div><img class="image-border209" src="img/00205.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Topic modeling using LDA</h1>
                
            
            
                
<p class="mce-root">LDA is a topic model, which infers topics from a collection of text documents. LDA can be thought of as an unsupervised clustering algorithm as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Topics correspond to cluster centers and documents correspond to rows in a dataset</li>
<li class="mce-root1">Topics and documents both exist in a feature space, where feature vectors are vectors of word counts</li>
<li class="mce-root1">Rather than estimating a clustering using a traditional distance, LDA uses a function based on a statistical model of how text documents are generated</li>
</ul>
<p class="mce-root">In order to invoke LDA, you need to import the package:</p>
<pre class="calibre19">
<strong class="calibre1">import org.apache.spark.ml.clustering.LDA</strong>
</pre>
<p class="mce-root"><strong class="calibre1">Step 1.</strong> First, you need to initialize an LDA model setting 10 topics and 10 iterations of clustering:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val lda = new LDA().setK(10).setMaxIter(10)</strong><br class="title-page-name"/>lda: org.apache.spark.ml.clustering.LDA = lda_18f248b08480
</pre>
<p class="mce-root"><strong class="calibre1">Step 2.</strong> Next invoking the <kbd class="calibre11">fit()</kbd> function on the input dataset yields an output transformer:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val ldaModel = lda.fit(countVectorizerDF)</strong><br class="title-page-name"/>ldaModel: org.apache.spark.ml.clustering.LDAModel = lda_18f248b08480
</pre>
<p class="mce-root"><strong class="calibre1">Step 3.</strong> Extract <kbd class="calibre11">logLikelihood</kbd>, which calculates a lower bound on the provided documents given the inferred topic:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val ll = ldaModel.logLikelihood(countVectorizerDF)</strong><br class="title-page-name"/>ll: Double = -275.3298948279124
</pre>
<p class="mce-root"><strong class="calibre1">Step 4.</strong> Extract <kbd class="calibre11">logPerplexity</kbd>, which calculates an upper bound on the perplexity of the provided documents given the inferred topics:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val lp = ldaModel.logPerplexity(countVectorizerDF)</strong><br class="title-page-name"/>lp: Double = 12.512670220189033
</pre>
<p class="mce-root"><strong class="calibre1">Step 5.</strong> Now, we can use <kbd class="calibre11">describeTopics()</kbd> to get the topics generated by LDA:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val topics = ldaModel.describeTopics(10)</strong><br class="title-page-name"/>topics: org.apache.spark.sql.DataFrame = [topic: int, termIndices: array&lt;int&gt; ... 1 more field]
</pre>
<p class="mce-root"><strong class="calibre1">Step 6.</strong> The following is the output dataset showing the <kbd class="calibre11">topic</kbd>, <kbd class="calibre11">termIndices</kbd>, and <kbd class="calibre11">termWeights</kbd> computed by LDA model:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; topics.show(10, false)</strong><br class="title-page-name"/>|topic|termIndices |termWeights |<br class="title-page-name"/>|0 |[2, 5, 7, 12, 17, 9, 13, 16, 4, 11] |[0.06403877783050851, 0.0638177222807826, 0.06296749987731722, 0.06129482302538905, 0.05906095287220612, 0.0583855194291998, 0.05794181263149175, 0.057342702589298085, 0.05638654243412251, 0.05601913313272188] |<br class="title-page-name"/>|1 |[15, 5, 13, 8, 1, 6, 9, 16, 2, 14] |[0.06889315890755099, 0.06415969116685549, 0.058990446579892136, 0.05840283223031986, 0.05676844625413551, 0.0566842803396241, 0.05633554021408156, 0.05580861561950114, 0.055116582320533423, 0.05471754535803045] |<br class="title-page-name"/>|2 |[17, 14, 1, 5, 12, 2, 4, 8, 11, 16] |[0.06230542516700517, 0.06207673834677118, 0.06089143673912089, 0.060721809302399316, 0.06020894045877178, 0.05953822260375286, 0.05897033457363252, 0.057504989644756616, 0.05586725037894327, 0.05562088924566989] |<br class="title-page-name"/>|3 |[15, 2, 11, 16, 1, 7, 17, 8, 10, 3] |[0.06995373276880751, 0.06249041124300946, 0.061960612781077645, 0.05879695651399876, 0.05816564815895558, 0.05798721645705949, 0.05724374708387087, 0.056034215734402475, 0.05474217418082123, 0.05443850583761207] |<br class="title-page-name"/>|4 |[16, 9, 5, 7, 1, 12, 14, 10, 13, 4] |[0.06739359010780331, 0.06716438619386095, 0.06391509491709904, 0.062049068666162915, 0.06050715515506004, 0.05925113958472128, 0.057946856127790804, 0.05594837087703049, 0.055000929117413805, 0.053537418286233956]|<br class="title-page-name"/>|5 |[5, 15, 6, 17, 7, 8, 16, 11, 10, 2] |[0.061611492476326836, 0.06131944264846151, 0.06092975441932787, 0.059812552365763404, 0.05959889552537741, 0.05929123338151455, 0.05899808901872648, 0.05892061664356089, 0.05706951425713708, 0.05636134431063274] |<br class="title-page-name"/>|6 |[15, 0, 4, 14, 2, 10, 13, 7, 6, 8] |[0.06669864676186414, 0.0613859230159798, 0.05902091745149218, 0.058507882633921676, 0.058373998449322555, 0.05740944364508325, 0.057039150886628136, 0.057021822698594314, 0.05677330199892444, 0.056741558062814376]|<br class="title-page-name"/>|7 |[12, 9, 8, 15, 16, 4, 7, 13, 17, 10]|[0.06770789917351365, 0.06320078344027158, 0.06225712567900613, 0.058773135159638154, 0.05832535181576588, 0.057727684814461444, 0.056683575112703555, 0.05651178333610803, 0.056202395617563274, 0.05538103218174723]|<br class="title-page-name"/>|8 |[14, 11, 10, 7, 12, 9, 13, 16, 5, 1]|[0.06757347958335463, 0.06362319365053591, 0.063359294927315, 0.06319462709331332, 0.05969320243218982, 0.058380063437908046, 0.057412693576813126, 0.056710451222381435, 0.056254581639201336, 0.054737785085167814] |<br class="title-page-name"/>|9 |[3, 16, 5, 7, 0, 2, 10, 15, 1, 13] |[0.06603941595604573, 0.06312775362528278, 0.06248795574460503, 0.06240547032037694, 0.0613859713404773, 0.06017781222489122, 0.05945655694365531, 0.05910351349013983, 0.05751269894725456, 0.05605239791764803] |
</pre>
<p class="mce-root">The diagram of an LDA is as follows, which shows the topics created from the features of TF-IDF:</p>
<div><img class="image-border210" src="img/00175.jpeg"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Implementing text classification</h1>
                
            
            
                
<p class="mce-root">Text classification is one of the most widely used paradigms in the field of machine learning and is useful in use cases such as spam detection and email classification and just like any other machine learning algorithm, the workflow is built of Transformers and algorithms. In the field of text processing, preprocessing steps such as stop-word removal, stemming, tokenizing, n-gram extraction, TF-IDF feature weighting come into play. Once the desired processing is complete, the models are trained to classify the documents into two or more classes.</p>
<p class="mce-root">Binary classification is the classification of inputting two output classes such as spam/not spam and a given credit card transaction is fraudulent or not. Multiclass classification can generate multiple output classes such as hot, cold, freezing, and rainy. There is another technique called Multilabel classification, which can generate multiple labels such as speed, safety, and fuel efficiency can be produced from descriptions of car features.</p>
<p class="mce-root">For this purpose, we will using a 10k sample dataset of tweets and we will use the preceding techniques on this dataset. Then, we will tokenize the text lines into words, remove stop words, and then use <kbd class="calibre11">CountVectorizer</kbd> to build a vector of the words (features).</p>
<p class="mce-root">Then we will split the data into training (80%)-testing (20%) and train a Logistic Regression model. Finally, we will evaluate against the test data and look at how it is performed.</p>
<p class="mce-root">The steps in the workflow are shown in the following diagram:</p>
<div><img class="image-border211" src="img/00177.jpeg"/></div>
<p class="mce-root"><strong class="calibre1">Step 1.</strong> Load the input text data containing 10k tweets along with label and ID:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val inputText = sc.textFile("Sentiment_Analysis_Dataset10k.csv")</strong><br class="title-page-name"/>inputText: org.apache.spark.rdd.RDD[String] = Sentiment_Analysis_Dataset10k.csv MapPartitionsRDD[1722] at textFile at &lt;console&gt;:77
</pre>
<p class="mce-root"><strong class="calibre1">Step 2.</strong> Convert the input lines to a DataFrame:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val sentenceDF = inputText.map(x =&gt; (x.split(",")(0), x.split(",")(1), x.split(",")(2))).toDF("id", "label", "sentence")</strong><br class="title-page-name"/>sentenceDF: org.apache.spark.sql.DataFrame = [id: string, label: string ... 1 more field]
</pre>
<p class="mce-root"><strong class="calibre1">Step 3.</strong> Transform the data into words using a <kbd class="calibre11">Tokenizer</kbd> with white space delimiter:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; import org.apache.spark.ml.feature.Tokenizer</strong><br class="title-page-name"/>import org.apache.spark.ml.feature.Tokenizer<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val tokenizer = new Tokenizer().setInputCol("sentence").setOutputCol("words")</strong><br class="title-page-name"/>tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_ebd4c89f166e<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val wordsDF = tokenizer.transform(sentenceDF)</strong><br class="title-page-name"/>wordsDF: org.apache.spark.sql.DataFrame = [id: string, label: string ... 2 more fields]<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; wordsDF.show(5, true)</strong><br class="title-page-name"/>| id|label| sentence| words|<br class="title-page-name"/>| 1| 0|is so sad for my ...|[is, so, sad, for...|<br class="title-page-name"/>| 2| 0|I missed the New ...|[i, missed, the, ...|<br class="title-page-name"/>| 3| 1| omg its already ...|[, omg, its, alre...|<br class="title-page-name"/>| 4| 0| .. Omgaga. Im s...|[, , .., omgaga.,...|<br class="title-page-name"/>| 5| 0|i think mi bf is ...|[i, think, mi, bf...|
</pre>
<p class="mce-root"><strong class="calibre1">Step 4.</strong> Remove stop words and create a new DataFrame with the filtered words:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; import org.apache.spark.ml.feature.StopWordsRemover</strong><br class="title-page-name"/>import org.apache.spark.ml.feature.StopWordsRemover<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val remover = new StopWordsRemover().setInputCol("words").setOutputCol("filteredWords")</strong><br class="title-page-name"/>remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_d8dd48c9cdd0<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val noStopWordsDF = remover.transform(wordsDF)</strong><br class="title-page-name"/>noStopWordsDF: org.apache.spark.sql.DataFrame = [id: string, label: string ... 3 more fields]<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; noStopWordsDF.show(5, true)</strong><br class="title-page-name"/>| id|label| sentence| words| filteredWords|<br class="title-page-name"/>| 1| 0|is so sad for my ...|[is, so, sad, for...|[sad, apl, friend...|<br class="title-page-name"/>| 2| 0|I missed the New ...|[i, missed, the, ...|[missed, new, moo...|<br class="title-page-name"/>| 3| 1| omg its already ...|[, omg, its, alre...|[, omg, already, ...|<br class="title-page-name"/>| 4| 0| .. Omgaga. Im s...|[, , .., omgaga.,...|[, , .., omgaga.,...|<br class="title-page-name"/>| 5| 0|i think mi bf is ...|[i, think, mi, bf...|[think, mi, bf, c...|
</pre>
<p class="mce-root"><strong class="calibre1">Step 5.</strong> Create a feature vector from the filtered words:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; import org.apache.spark.ml.feature.CountVectorizer</strong><br class="title-page-name"/>import org.apache.spark.ml.feature.CountVectorizer<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val countVectorizer = new CountVectorizer().setInputCol("filteredWords").setOutputCol("features")</strong><br class="title-page-name"/>countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_fdf1512dfcbd<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val countVectorizerModel = countVectorizer.fit(noStopWordsDF)</strong><br class="title-page-name"/>countVectorizerModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_fdf1512dfcbd<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val countVectorizerDF = countVectorizerModel.transform(noStopWordsDF)</strong><br class="title-page-name"/>countVectorizerDF: org.apache.spark.sql.DataFrame = [id: string, label: string ... 4 more fields]<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; countVectorizerDF.show(5,true)</strong><br class="title-page-name"/>| id|label| sentence| words| filteredWords| features|<br class="title-page-name"/>| 1| 0|is so sad for my ...|[is, so, sad, for...|[sad, apl, friend...|(23481,[35,9315,2...|<br class="title-page-name"/>| 2| 0|I missed the New ...|[i, missed, the, ...|[missed, new, moo...|(23481,[23,175,97...|<br class="title-page-name"/>| 3| 1| omg its already ...|[, omg, its, alre...|[, omg, already, ...|(23481,[0,143,686...|<br class="title-page-name"/>| 4| 0| .. Omgaga. Im s...|[, , .., omgaga.,...|[, , .., omgaga.,...|(23481,[0,4,13,27...|<br class="title-page-name"/>| 5| 0|i think mi bf is ...|[i, think, mi, bf...|[think, mi, bf, c...|(23481,[0,33,731,...|
</pre>
<p class="mce-root"><strong class="calibre1">Step 6.</strong> Create the <kbd class="calibre11">inputData</kbd> DataFrame with just a label and the features:</p>
<pre class="calibre19">
<br class="title-page-name"/><strong class="calibre1">scala&gt; val inputData=countVectorizerDF.select("label", "features").withColumn("label", col("label").cast("double"))</strong><br class="title-page-name"/>inputData: org.apache.spark.sql.DataFrame = [label: double, features: vector]
</pre>
<p class="mce-root"><strong class="calibre1">Step 7.</strong> Split the data using a random split into 80% training and 20% testing datasets:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val Array(trainingData, testData) = inputData.randomSplit(Array(0.8, 0.2))</strong><br class="title-page-name"/>trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]<br class="title-page-name"/>testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]
</pre>
<p class="mce-root"><strong class="calibre1">Step 8.</strong> Create a Logistic Regression model:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; import org.apache.spark.ml.classification.LogisticRegression</strong><br class="title-page-name"/>import org.apache.spark.ml.classification.LogisticRegression<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val lr = new LogisticRegression()</strong><br class="title-page-name"/>lr: org.apache.spark.ml.classification.LogisticRegression = logreg_a56accef5728
</pre>
<p class="mce-root"><strong class="calibre1">Step 9.</strong> Create a Logistic Regression model by fitting the <kbd class="calibre11">trainingData</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; var lrModel = lr.fit(trainingData)</strong><br class="title-page-name"/>lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_a56accef5728<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; lrModel.coefficients</strong><br class="title-page-name"/>res160: org.apache.spark.ml.linalg.Vector = [7.499178040193577,8.794520490564185,4.837543313917086,-5.995818019393418,1.1754740390468577,3.2104594489397584,1.7840290776286476,-1.8391923375331787,1.3427471762591,6.963032309971087,-6.92725055841986,-10.781468845891563,3.9752.836891070557657,3.8758544006087523,-11.760894935576934,-6.252988307540...<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; lrModel.intercept</strong><br class="title-page-name"/>res161: Double = -5.397920610780994
</pre>
<p class="mce-root"><strong class="calibre1">Step 10.</strong> Examine the model summary especially <kbd class="calibre11">areaUnderROC</kbd>, which should be <em class="calibre8">&gt; 0.90</em> for a good model:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary</strong><br class="title-page-name"/>import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val summary = lrModel.summary</strong><br class="title-page-name"/>summary: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary@1dce712c<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val bSummary = summary.asInstanceOf[BinaryLogisticRegressionSummary]</strong><br class="title-page-name"/>bSummary: org.apache.spark.ml.classification.BinaryLogisticRegressionSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary@1dce712c<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; bSummary.areaUnderROC</strong><br class="title-page-name"/>res166: Double = 0.9999231930196596<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; bSummary.roc</strong><br class="title-page-name"/>res167: org.apache.spark.sql.DataFrame = [FPR: double, TPR: double]<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; bSummary.pr.show()</strong><br class="title-page-name"/>| recall|precision|<br class="title-page-name"/>| 0.0| 1.0|<br class="title-page-name"/>| 0.2306543172990738| 1.0|<br class="title-page-name"/>| 0.2596354944726621| 1.0|<br class="title-page-name"/>| 0.2832387212429041| 1.0|<br class="title-page-name"/>|0.30504929787869733| 1.0|<br class="title-page-name"/>| 0.3304451747833881| 1.0|<br class="title-page-name"/>|0.35255452644158947| 1.0|<br class="title-page-name"/>| 0.3740663280549746| 1.0|<br class="title-page-name"/>| 0.3952793546459516| 1.0|
</pre>
<p class="mce-root"><strong class="calibre1">Step 11.</strong> Transform both training and testing datasets using the trained model:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; val training = lrModel.transform(trainingData)</strong><br class="title-page-name"/>training: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; val test = lrModel.transform(testData)</strong><br class="title-page-name"/>test: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]
</pre>
<p class="mce-root"><strong class="calibre1">Step 12.</strong> Count the number of records with matching label and prediction columns. They should match for correct model evaluation else they will mismatch:</p>
<pre class="calibre19">
<strong class="calibre1">scala&gt; training.filter("label == prediction").count</strong><br class="title-page-name"/>res162: Long = 8029<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; training.filter("label != prediction").count</strong><br class="title-page-name"/>res163: Long = 19<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; test.filter("label == prediction").count</strong><br class="title-page-name"/>res164: Long = 1334<br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">scala&gt; test.filter("label != prediction").count</strong><br class="title-page-name"/>res165: Long = 617
</pre>
<p class="mce-root">The results can be put into a table as shown next:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Dataset</strong></td>
<td class="calibre7"><strong class="calibre1">Total</strong></td>
<td class="calibre7"><strong class="calibre1">label == prediction</strong></td>
<td class="calibre7"><strong class="calibre1">label != prediction</strong></td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Training</strong></td>
<td class="calibre7">8048</td>
<td class="calibre7">8029 ( 99.76%)</td>
<td class="calibre7">19 (0.24%)</td>
</tr>
<tr class="calibre6">
<td class="calibre7"><strong class="calibre1">Testing</strong></td>
<td class="calibre7">1951</td>
<td class="calibre7">1334 (68.35%)</td>
<td class="calibre7">617 (31.65%)</td>
</tr>
</tbody>
</table>
<p class="mce-root">While training data produced excellent matches, the testing data only had 68.35% match. Hence, there is room for improvement which can be done by exploring the model parameters.</p>
<p class="mce-root">Logistic regression is an easy-to-understand method for predicting a binary outcome using a linear combination of inputs and randomized noise in the form of a logistic random variable. Hence, Logistic Regression model can be tuned using several parameters. (The full set of parameters and how to tune such a Logistic Regression model is out of scope for this chapter.)</p>
<p class="mce-root">Some parameters that can be used to tune the model are:</p>
<ul class="calibre9">
<li class="mce-root1">Model hyperparameters include the following parameters:
<ul class="calibre38">
<li class="mce-root1"><kbd class="calibre11">elasticNetParam</kbd>: This parameter specifies how you would like to mix L1 and L2 regularization</li>
<li class="mce-root1"><kbd class="calibre11">regParam</kbd>: This parameter determines how the inputs should be regularized before being passed in the model</li>
</ul>
</li>
<li class="mce-root1">Training parameters include the following parameters:
<ul class="calibre38">
<li class="mce-root1"><kbd class="calibre11">maxIter</kbd>: This is total number of interactions before stopping</li>
<li class="mce-root1"><kbd class="calibre11">weightCol</kbd>: This is the name of the weight column to weigh certain rows more than others</li>
</ul>
</li>
<li class="mce-root1">Prediction parameters include the following parameter:
<ul class="calibre38">
<li class="mce-root1"><kbd class="calibre11">threshold</kbd>: This is the probability threshold for binary prediction. This determines the minimum probability for a given class to be predicted.</li>
</ul>
</li>
</ul>
<p class="mce-root">We have now seen how to build a simple classification model, so any new tweet can be labeled based on the training set. Logistic Regression is only one of the models that can be used.</p>
<p class="mce-root">Other models which can be used in place of Logistic Regression are as follows:</p>
<ul class="calibre9">
<li class="mce-root1">Decision trees</li>
<li class="mce-root1">Random Forest</li>
<li class="mce-root1">Gradient Boosted Trees</li>
<li class="mce-root1">Multilayer Perceptron</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, we have introduced the world of text analytics using Spark ML with emphasis on text classification. We have learned about Transformers and Estimators. We have seen how Tokenizers can be used to break sentences into words, how to remove stop words, and generate n-grams. We also saw how to implement <kbd class="calibre11">HashingTF</kbd> and <kbd class="calibre11">IDF</kbd> to generate TF-IDF-based features. We also looked at <kbd class="calibre11">Word2Vec</kbd> to convert sequences of words into vectors.</p>
<p class="mce-root">Then, we also looked at LDA, a popular technique used to generate topics from documents without knowing much about the actual text. Finally, we implemented text classification on the set of 10k tweets from the Twitter dataset to see how it all comes together using Transformers, Estimators, and the Logistic Regression model to perform binary classification.</p>
<p class="mce-root">In the next chapter, we will dig even deeper toward tuning Spark applications for better performance.</p>


            

            
        
    </body></html>