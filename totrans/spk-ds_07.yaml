- en: Chapter 7.  Extending Spark with SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statisticians and data scientists have been using R to solve challenging problems
    in almost every field, ranging from bioinformatics to election campaigns. They
    prefer R due to its powerful visualization capabilities, strong community, and
    rich package ecosystem for statistics and machine learning. Many academic institutions
    around the world teach data science and statistics using the R language.
  prefs: []
  type: TYPE_NORMAL
- en: R was originally created by and for statisticians in around the mid-1990s with
    a goal to deliver a better and more user-friendly way to perform data analysis.
    R was initially used in academics and research. As businesses became increasingly
    aware of the role of data science in their business growth, the number of data
    analysts using R in the corporate sector started growing as well. The R language
    user base is considered to be more than two million strong, after being in existence
    for two decades.
  prefs: []
  type: TYPE_NORMAL
- en: One of the driving factors behind all this success is the fact that R is designed
    to make the life of the analyst easier but not that of the computer. R is inherently
    single-threaded and it can only process datasets that completely fit in a single
    machine's memory. But nowadays, R users are working with increasingly larger datasets.
    Seamless integration of modern-day distributed processing power underneath the
    well-established R language allows data scientists to leverage the best of both
    worlds. They can keep up with their ever-increasing business demands and continue
    to benefit from the flexibility of their favorite R language.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter introduces SparkR, an R API to Spark for R programmers so that
    they can harness the power of Spark, without learning a new language. Since prior
    knowledge of R, R Studio, and data analysis skills are already assumed, this chapter
    does not attempt to introduce R. A very brief overview of the Spark compute engine
    is provided as a quick recap. The reader should go through the first three chapters
    of this book to gain a deeper understanding of the Spark programming model and
    DataFrames. This knowledge is extremely important because the developer has to
    understand which part of his code is executing in the local R environment and
    which part is being handled by the Spark compute engine. The topics covered in
    this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: SparkR basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of R with Spark and its limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programming with SparkR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkR DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkR basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R is a language and environment for statistical computing and graphics. SparkR
    is an R package that provides a lightweight frontend to enable Apache Spark access
    from R. The goal of SparkR is to combine the flexibility and ease of use provided
    by the R environment and the scalability and fault tolerance provided by the Spark
    compute engine. Let us recap the Spark architecture before discussing how SparkR
    realizes its goal.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark is a fast, general-purpose, fault-tolerant framework for interactive
    and iterative computations on large, distributed datasets. It supports a wide
    variety of data sources as well as storage layers. It provides unified data access
    to combine different data formats, streaming data and defining complex operations
    using high-level, composable operators. You can develop your applications interactively
    using Scala, Python, or R shell (or Java without a shell). You can deploy it on
    your home desktop or you can run it on large clusters of thousands of nodes crunching
    petabytes of data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SparkR originated in the AMPLab ([https://amplab.cs.berkeley.edu/](https://amplab.cs.berkeley.edu/))
    to explore different techniques to integrate the usability of R with the scalability
    of Spark. It was released as an alpha component in Apache Spark 1.4, which was
    released in June 2015\. The Spark 1.5 release had improved R usability and introduced
    the MLlib machine learning package with **Generalized Linear Models** (**GLMs**).
    The Spark 1.6 release that happened in January 2016 added some more features,
    such as model summary and feature interactions. The Spark 2.0 release that happened
    in July 2016 brought several important features, such as UDF, improved model coverage,
    DataFrames Window functions API, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing SparkR from the R environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can start SparkR from R shell or R Studio. The entry point to SparkR is
    the SparkSession object, which represents the connection to the Spark cluster.
    The node on which R is running becomes the driver. Any objects created by the
    R program reside on this driver. Any objects created via SparkSession are created
    on the worker nodes in the cluster. The following diagram depicts the runtime
    view of R interaction with Spark running on a cluster. Note that R interpreter
    exists on every worker node in the cluster. The following figure does not show
    the cluster manager and it does not show the storage layer either. You could use
    any cluster manager (for example, Yarn or Mesos) and any storage option, such
    as HDFS, Cassandra, or Amazon S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Accessing SparkR from the R environment](img/image_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: http://www.slideshare.net/Hadoop_Summit/w-145p210-avenkataraman.'
  prefs: []
  type: TYPE_NORMAL
- en: A SparkSession object is created by passing information such as application
    name, memory, number of cores, and the cluster manager to connect to. Any interaction
    with the Spark engine is initiated via this SparkSession object. A SparkSession
    object is already created for you if you use SparkR shell. You have to explicitly
    create it otherwise. This object replaces SparkContext and SQLContext objects
    that existed in Spark 1.x releases. These objects still exist for backward compatibility.
    Even the preceding figure depicts SparkContext, which you should treat as SparkSession
    post Spark 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood how to access Spark from the R environment, let
    us examine the core data abstractions provided by the Spark engine.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs and DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the core of the Spark engine is its main data abstraction, called a **Resilient
    Distributed Dataset** (**RDD**). An RDD is composed of one or more data sources
    and is defined by the user as a series of transformations (aka lineage) on one
    or more stable (concrete) data sources. Every RDD or RDD partition knows how to
    recreate itself on failure using the lineage graph, thereby providing fault tolerance.
    RDD is an immutable data structure, implying that it is sharable between threads
    without synchronization overheads and hence amenable for parallelization. Operations
    on RDDs are either transformations or actions. Transformations are individual
    steps in the lineage. In other words, they are operations that create RDDs because
    every transformation is getting data from a stable data source or transforming
    an immutable RDD and creating another RDD. Transformations are simply declarations;
    they are not evaluated until an *action* operation is applied on that RDD. Actions
    are the operations that utilize the RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Spark optimizes RDD computation based on the action on hand. For example, if
    the action is to read the first line, only one partition is computed, skipping
    the rest. It automatically performs in-memory computation with graceful degradation
    (spills it to disk when memory is insufficient) and distributes processing across
    all the cores. You may cache an RDD if it is frequently accessed in your program
    logic, thereby avoiding recomputing overhead.
  prefs: []
  type: TYPE_NORMAL
- en: The R language provides a two-dimensional data structure called a *DataFrame*
    which makes data manipulation convenient. Apache Spark comes with its own DataFrames
    that are inspired by the DataFrame in R and Python (through Pandas). A Spark DataFrame
    is a specialized data structure that is built on top of the RDD data structure
    abstraction. It provides distributed DataFrame implementation that looks very
    similar to R DataFrame from the developer perspective and at the same time can
    support very large datasets. The Spark dataset API adds structure to DataFrames
    and this structure provides information for more optimization under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have understood the underlying data structures and the runtime
    view, it is time to run a few commands. In this section, we assume that you already
    have R and Spark successfully installed and added to the path. We also assume
    that the `SPARK_HOME` environment variable is set. Let us see how to access SparkR
    from R shell or R Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is all you need to do to access the power of Spark DataFrames from within
    the R environment.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The R language has long been the lingua franca of data scientists. Its simple-to-understand
    DataFrame abstraction, expressive APIs, and vibrant package ecosystem are exactly
    what the analysts needed. The main challenge was with the scalability. SparkR
    bridges that gap by providing distributed in-memory DataFrames without leaving
    the R eco-system. Such a symbiotic relationship allows users to gain the following
    benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no need for the analyst to learn a new language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SparkR APIs are similar to R APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can access SparkR from R studio, along with the autocomplete feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing interactive, exploratory analysis of a very large dataset is no longer
    hindered by memory limitations or long turnaround times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing data from different types of data sources becomes a lot easier. Most
    of the tasks which were imperative before have become declarative. Check [Chapter
    4](http://Chapter%204), *Unified Data Access*, to learn more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can freely mix dplyr such as Spark functions, SQL, and R libraries that
    are still not available in Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In spite of all the exciting advantages of combining the best of both worlds,
    there are still some limitations with this combination. These limitations may
    not impact every use case, but we need to be aware of them anyway:'
  prefs: []
  type: TYPE_NORMAL
- en: The inherent dynamic nature of R limits the information available for the catalyst
    optimizer. We may not get the full advantage of optimizations such as predicate
    pushback when compared to statically typed languages such as Scala.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkR does not have support for all the machine learning algorithms that are
    already available in other APIs such as the Scala API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, using Spark for data preprocessing and using R for analysis and
    visualization seems to be the best approach in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: Programming with SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have understood the runtime model of SparkR and the basic data abstractions
    that provide the fault tolerance and scalability. We have understood how to access
    the Spark API from R shell or R studio. It''s time to try out some basic and familiar
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The operations look very similar to R DataFrame functions because spark DataFrames
    are modeled based on R DataFrames and Python (Pandas) DataFrames. But the similarity
    may create confusion if you are not careful. You may accidentally end up choking
    your local machine by running a compute-intensive function on an R `data.frame`,
    thinking that the load will be distributed. For example, the intersect function
    has the same signature in both packages. You need to pay attention to whether
    the object is of class `SparkDataFrame` (Spark DataFrame) or `data.frame` (R DataFrame).
    You also need to minimize back and forth conversions between local R `data.frame`
    objects and Spark DataFrame objects. Let us get a feel for this distinction by
    trying out some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Function name masking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have tried some basic operations, let us digress a little bit.
    We have to understand what happens when a loaded library has overlapping function
    names with the base package or some other package that was already loaded. This
    is sometimes referred to as function name overlapping, function masking, or name
    conflict. You might have noticed the messages mentioning the objects masked when
    the SparkR package is loaded. This is common for any package loaded into the R
    environment, and is not specific to SparkR alone. If the R environment already
    contains any function that has the same name as a function in the package being
    loaded, then any subsequent calls to that function exhibit the behavior of the
    function in the latest package loaded. If you want to access the previous function
    instead of the `SparkR` function, you need to explicitly prefix that function
    with its package name, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Subsetting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Subsetting operations on R DataFrames are quite flexible and SparkR tries to
    retain these operations with the same or similar equivalents. We have already
    seen some operations in the preceding examples but this section presents them
    in an ordered fashion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the time of writing this book (Apache Spark 2.o release), row index based
    slicing is not available. You will not be able to get a specific row or range
    of rows using the `df[n,]` or `df[m:n,]` syntax.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Column functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will have already noticed the column functions `between` in the subsetting
    data section. These functions operate on the `Column` class. As the name suggests,
    these functions operate on a single column at a time and are usually used in subsetting
    DataFrames. There are several other handy column functions for common operations
    such as sorting, casting, and formatting. In addition to working on the values
    within a column, you can append columns to a DataFrame or drop one or more columns
    from a DataFrame. Negative column subscripts may be used to omit columns, similar
    to R. The following examples show the use of `Column` class functions in subset
    operations followed by adding and dropping columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Grouped data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DataFrame data can be subgrouped using the `group_by` function similar to SQL.
    There are multiple ways of performing such operations. We introduce a slightly
    complex example in this section. Moreover, we use `%>%`, aka the forward pipe
    operator, provided by the `magrittr` library, which provides a mechanism for chaining
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can keep chaining the operations using the forward pipe operator. Look at
    the column renamed part of the code carefully. The column name argument is the
    output of previous operations, which would have completed before commencement
    of this operation and thus you can safely assume that the `avg(sepal_len)` column
    already exists. The `format_number` works as expected, and this is yet another
    handy `Column` operation.
  prefs: []
  type: TYPE_NORMAL
- en: The next section has another similar example with `GroupedData` and its equivalent
    implementation using `dplyr`.
  prefs: []
  type: TYPE_NORMAL
- en: SparkR DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we try out some useful, commonly used operations. First, we
    try out the traditional R/`dplyr` operations and then show equivalent operations
    using the SparkR API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This operation is very similar to the SQL group and is followed by order. Its
    equivalent implementation in SparkR is also very similar to the `dplyr` example.
    Look at the following example. Pay attention to the method names and compare their
    positioning with respect to the preceding `dplyr` example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: SparkR is intended to be as close to the existing R API as possible. So, the
    method names look very similar to `dplyr` methods. For example, look at the example
    which has `groupBy` whereas `dplyr` has `group_by`. SparkR supports redundant
    function names. For example, it has `group_by` as well as `groupBy` to cater to
    developers coming from different programming environments. The method names in
    `dplyr` and SparkR are again very close to the SQL keyword `GROUP BY`. But the
    sequence of these method calls is not the same. The example also showed an additional
    step of converting a Spark DataFrame to an R `data.frame` using `collect`. The
    methods are arranged inside out, in the sense that first the data is grouped,
    then summarized, and then arranged. This is understandable because in SparkR,
    the DataFrame created in the innermost method becomes the argument for its immediate
    predecessor and so on.
  prefs: []
  type: TYPE_NORMAL
- en: SQL operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are not very happy with the syntax in the preceding example, you may
    want to try writing an SQL string as shown, which does exactly the same as the
    preceding but uses the good old SQL syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example looks like the most natural way of implementing the operation
    on hand, if you are used to fetching data from RDBMS tables. But how are we doing
    this? The first statement tells Spark to register a temporary table (or, as the
    name suggests, a view, a logical abstraction of a table). This is not exactly
    the same as a database table. It is temporary in the sense that it is destroyed
    when the SparkSession object is destroyed. You are not explicitly writing data
    into any RDBMS datastore (you have to use `SaveAsTable` for that). But when once
    you register a Spark DataFrame as a temporary table, you are free to use SQL syntax
    to operate on that DataFrame. The next statement is a basic `SELECT` statement
    that displays column names followed by five rows, as dictated by the `LIMIT` keyword.
    The next SQL statement created a Spark DataFrame containing a Species column followed
    by two average columns sorted on the average sepal length. This DataFrame is in
    turn collected as an R `data.frame` by using collect. The final result is exactly
    the same as the preceding example. You are free to use either syntax. For more
    information and examples, check out the SQL section in[Chapter 4](http://chapter%204),
    *Unified Data Access*.
  prefs: []
  type: TYPE_NORMAL
- en: Set operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The usual set operations, such as `union`, `intersection`, and `minus`, are
    available out of the box in SparkR. In fact, when SparkR is loaded, the warning
    message shows `intersect` as one of the masked functions. The following examples
    are based on `beaver` datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Merging DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next example illustrates the joining of two DataFrames using the `merge`
    command. The first part of the example shows the R implementation and the next
    part shows the SparkR implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding piece of code completely relies on R''s base package. We have
    used the same names for join columns in both DataFrames for simplicity. The next
    piece of code demonstrates the same example using SparkR. It looks similar to
    the preceding code so look carefully for the differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You may want to play with different types of joins, such as left outer join
    and right outer join, or different column names to get a better understanding
    of this function.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SparkR provides wrappers on existing MLLib functions. R formulas are implemented
    as MLLib feature transformers. A transformer is an ML pipeline (`spark.ml`) stage
    that takes a DataFrame as input and produces another DataFrame as output, which
    generally contains some appended columns. Feature transformers are a type of transformers
    that convert input columns to feature vectors and these feature vectors are appended
    to the source DataFrame. For example, in linear regression, string input columns
    are one-hot encoded and numeric values are converted to doubles. A label column
    will be appended (if not there in the data frame already) as a replica of the
    response variable.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we cover example code for the Naive Bayes and Gaussian GLM
    models. We do not explain the models as such or the summaries they produce. Instead,
    we go straight away to how it can be done using SparkR.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The NaÃ¯ve Bayes model is an intuitively simple model that works with categorical
    data. We'll be training a sample dataset using the NaÃ¯ve Bayes model. We will
    not explain how the model works but move straight away to training the model using
    SparkR. If you want more information, please refer to [Chapter 6](ch06.xhtml "Chapter 6. 
    Machine Learning"), *Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: This example takes a dataset with the average marks and attendance of twenty
    students. In fact, this dataset has already been introduced in [Chapter 6](http://Chapter%206),
    *Machine Learning*, for training ensembles. However, let us revisit its contents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The students are awarded `Pass` or `Fail` based on a set of well-defined rules.
    Two students with IDs `1009` and `1020` are granted `Pass`, even though they would
    have failed otherwise. Even though we do not provide the actual rules to the model,
    we expect the model to predict these two students'' result as `Fail`. Here are
    the `Pass` / `Fail` criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Marks < 40 => Fail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poor attendance => Fail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marks above 40 and attendance Full => Pass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Marks > 60 and attendance at least Enough => PassThe following is an example
    to train Naive Bayes model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The Gaussian GLM model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we try to predict temperature based on the values of ozone,
    solar radiation, and wind:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To date, SparkR does not support all algorithms available in Spark, but active
    development is happening to bridge the gap. The Spark 2.0 release has improved
    algorithm coverage, including NaÃ¯ve Bayes, k-means clustering, and survival regression.
    Check out the latest documentation for the supported algorithms. More work is
    underway in bringing out a CRAN release of SparkR, with better integration with
    R packages and Spark packages, and better RFormula support.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*SparkR: The Past, Present and Future* by *Shivaram Venkataraman: *[http://shivaram.org/talks/sparkr-summit-2015.pdf](http://shivaram.org/talks/sparkr-summit-2015.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Enabling Exploratory Data Science with Spark and R* by *Shivaram Venkataraman*
    and *Hossein Falaki:*[http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r](http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SparkR: Scaling R Programs with Spark* by *Shivaram Venkataraman* and others:
    [http://shivaram.org/publications/sparkr-sigmod.pdf](http://shivaram.org/publications/sparkr-sigmod.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recent Developments in SparkR for Advanced Analytics* by *Xiangrui Meng*:
    [http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf](http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand RFormula, try out the following links:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/ml-features.html#rformula](http://spark.apache.org/docs/latest/ml-features.html#rformula)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
