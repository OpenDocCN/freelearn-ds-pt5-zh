- en: Learning Machine Learning - Spark MLlib and Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Each of us, actually every animal, is a data scientist. We collect data from
    our sensors, and then we process the data to get abstract rules to perceive our
    environment and control our actions in that environment to minimize pain and/or
    maximize pleasure. We have memory to store those rules in our brains, and then
    we recall and use them when needed. Learning is lifelong; we forget rules when
    they no longer apply or revise them when the environment changes."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Ethem Alpaydin, Machine Learning: The New AI'
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of this chapter is to provide a conceptual introduction to statistical
    machine learning (ML) techniques for those who might not normally be exposed to
    such approaches during their typical required statistical training. This chapter
    also aims to take a newcomer from having minimal knowledge of machine learning
    all the way to being a knowledgeable practitioner in a few steps. We will focus
    on Spark''s machine learning APIs, called Spark MLlib and ML, in theoretical and
    practical ways. Furthermore, we will provide some examples covering feature extraction
    and transformation, dimensionality reduction, regression, and classification analysis.
    In a nutshell, we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark machine learning APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extractor and transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction using PCA for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary and multiclass classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will try to define machine learning from computer science,
    statistics, and data analytical perspectives. **Machine learning (ML)** is the
    branch of computer science that provides the computers the ability to learn without
    being explicitly programmed (Arthur Samuel in 1959). This field of study being
    evolved from the study of pattern recognition and computational learning theory
    in artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, ML explores the study and construction of algorithms that
    can learn from heuristics and make predictions on data. This kind of algorithms
    overcome the strictly static program instructions by making data-driven predictions
    or decisions, through building a model from sample inputs. Now let''s more explicit
    and versatile definition from Prof. Tom M. Mitchell, who explained what machine
    learning really means from the computer science perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: A computer program is said to learn from experience E with respect to some class
    of tasks T and performance measure P, if its performance at tasks in T, as measured
    by P, improves with experience E.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on that definition, we can conclude that a computer program or machine
    can:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn from data and histories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be improved with experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactively enhance a model that can be used to predict the outcomes of questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typical machine learning tasks are concept learning, predictive modeling, clustering,
    and finding useful patterns. The ultimate goal is to improve learning in such
    a way that it becomes automatic so that no human interactions are needed anymore,
    or to reduce the level of human interaction as much as possible. Although machine
    learning is sometimes conflated with **Knowledge Discovery and Data Mining** (**KDDM**),
    but KDDM, focuses more on exploratory data analysis and is known as unsupervised
    learning. Typical machine learning applications can be classified into scientific
    knowledge discovery and more commercial applications, ranging from Robotics or
    **Human-Computer Interaction** (**HCI**) to anti-spam filtering and recommender
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Typical machine learning workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A typical machine learning application involves several steps ranging from
    the input, processing, to output, which forms a scientific workflow, as shown
    in *Figure 1*. The following steps are involved in a typical machine learning
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the sample data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the data into the input format for the algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the data and handle the missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Split the data into two sets: one for building the model (training dataset)
    and one for testing the model (validation dataset).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the algorithm to build or train your ML model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make predictions with the training data and observe the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test and evaluate the model with the test data or, alternatively, validate the
    model using a cross-validator technique using the third dataset, called the validation
    dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the model for better performance and accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale up the model so that it will be able to handle massive datasets in future.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the ML model in commercialization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/00266.jpeg)**Figure 1:** Machine learning workflow'
  prefs: []
  type: TYPE_NORMAL
- en: Often, the machine learning algorithms have some ways to handle skewness in
    the datasets. That skewness is sometimes immense though. In step 4, the experimental
    dataset is randomly split, often into a training set and a test set, which is
    called sampling. The training dataset is used to train the model, whereas the
    test dataset is used to evaluate the performance of the best model at the very
    end. The better practice is to use the training dataset as much as you can to
    increase generalization performance. On the other hand, it is recommended to use
    the test dataset only once, to avoid the overfitting problem while computing the
    prediction error and the related metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'depending on the nature of the learning feedback available to a learning system,
    ML tasks or process are typically classified into three broad categories: supervised
    learning, unsupervised learning, and reinforcements learning shown in figure 2\.
    Furthermore, there are other machine learning tasks as well, for example, dimensionality
    reduction, recommendation system, frequent pattern mining, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00272.jpeg)**Figure 2:** Machine learning tasks'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A supervised learning application makes predictions based on a set of examples,
    and the goal is to learn general rules that map inputs to outputs aligning with
    the real world. For example, a dataset for spam filtering usually contains spam
    messages as well as non-spam messages. Therefore, we are able to know whether
    messages in the training set are spam or ham. Nevertheless, we might have the
    opportunity to use this information to train our model in order to classify new
    unseen messages. The following figure shows the schematic diagram of supervised
    learning. After the algorithm has found the required patterns, those patterns
    can be used to make predictions for unlabeled test data. This is the most popular
    and useful type of machine learning task, that is not an exception for Spark as
    well, where most of the algorithms are supervised learning techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00278.jpeg)**Figure 3**: Supervised learning in action'
  prefs: []
  type: TYPE_NORMAL
- en: Examples include classification and regression for solving supervised learning
    problems. We will provide several examples of supervised learning, such as logistic
    regression, random forest, decision trees, Naive Bayes, One-vs-the-Rest, and so
    on in this book. However, to make the discussion concrete, only logistic regression
    and the random forest will be discussed, and other algorithms will be discussed
    in [Chapter 12](part0383.html#BD87E1-21aec46d8593429cacea59dbdcd64e1c), *Advanced
    Machine Learning Best Practices*, with some practical examples. On the other hand,
    linear regression will be discussed for the regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In unsupervised learning, data points have no labels related with them. Therefore,
    we need to put labels on it algorithmically, as shown in the following figure.
    In other words, the correct classes of the training dataset in unsupervised learning
    are unknown. Consequently, classes have to be inferred from the unstructured datasets,
    which imply that the goal of an unsupervised learning algorithm is to preprocess
    the data in some structured ways by describing its structure.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this obstacle in unsupervised learning, clustering techniques are
    commonly used to group the unlabeled samples based on certain similarity measures.
    Therefore, this task also involves mining hidden patterns toward feature learning.
    Clustering is the process of intelligently categorizing the items in your dataset.
    The overall idea is that two items in the same cluster are “closer” to each other
    than items that belong to separate clusters. That is the general definition, leaving
    the interpretation of “closeness” open.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00282.jpeg)**Figure 4**: Unsupervised learning'
  prefs: []
  type: TYPE_NORMAL
- en: Examples include clustering, frequent pattern mining, and dimensionality reduction
    for solving unsupervised learning problems (it can be applied to supervised learning
    problems too). We will provide several examples of unsupervised learning, such
    as k-means, bisecting k-means, Gaussian mixture model, **Latent dirichlet allocation**
    (**LDA**), and so on, in this book. We will also show how to use a dimensionality
    reduction algorithm such as **Principal Component Analysis** (**PCA**) or **Singular
    Value Decomposition** (**SVD**) in supervised learning through regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction** (**DR**): Dimensionality reduction is a technique
    used to reduce the number of random variables under certain considerations. This
    technique is used for both supervised and unsupervised learning. Typical advantages
    of using DR techniques are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It reduces the time and storage space required in machine learning tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It helps remove multicollinearity and improves the performance of the machine
    learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data visualization becomes easier when reduced to very low dimensions such as
    2D or 3D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a human being, you and we also learn from past experiences. We haven't got
    so charming by accident. Years of positive compliments as well as negative criticism
    have all helped shape us who we are today. You learn what makes people happy by
    interacting with friends, family, or even strangers, and you figure out how to
    ride a bike by trying out different muscle movements until it just clicks. When
    you perform actions, you're sometimes rewarded immediately. For example, finding
    a shopping mall nearby might yield instant gratification. Other times, the reward
    doesn't appear right away, such as traveling a long distance to find an exceptional
    place to eat. These are all about Reinforcement Learning (RL)**.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus RL is a technique, where the model itself learns from a series of actions
    or behaviors. The complexity of the dataset, or sample complexity, is very important
    in the reinforcement learning needed for the algorithms to learn a target function
    successfully. Moreover, in response to each data point for achieving the ultimate
    goal, maximization of the reward function should be ensured while interacting
    with an external environment, as demonstrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00288.jpeg)**Figure 5**: Reinforcement learning'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reinforcement learning techniques are being used in many areas. Here''s a very
    short list includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Advertising helps in learning rank, using one-shot learning for emerging items,
    and new users will bring more money
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teaching robots new tasks, while retaining prior knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deriving complex hierarchical schemes, from chess gambits to trading strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Routing problems, for example, management of a shipping fleet, which trucks/truckers
    to assign to which cargo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In robotics, the algorithm must choose the robot's next action based on a set
    of sensor readings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also a natural fit for **Internet of Things** (**IoT**) applications,
    where a computer program interacts with a dynamic environment in which it must
    perform a certain goal without an explicit mentor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the simplest RL problems is called n-armed bandits. The thing is there
    are n-many slot machines but each has different fixed pay-out probability. The
    goal is to maximize the profit by always choosing the machine with the best payout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An emerging area for applying is the stock market trading. Where a trader acts
    like a reinforcement agent since buying and selling (i.e. action) particular stock
    changes the state of the trader by generating profit or loss i.e. reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommender system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A recommender system is a subclass of an information filtering system that looks
    to predict the rating or preference that users usually provide for an item. The
    concept of recommender systems has become very common in recent years subsequently
    being applied in different applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00294.jpeg)**Figure 6**: Different recommender system'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular ones are probably products (for example, movies, music, books,
    research articles, news, search queries, social tags, and so on). Recommender
    systems can be classified into the following four categories typically:'
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering, also referred to as social filtering that filters information
    by using the recommendations of other people. The thing is people who agreed in
    their evaluation of certain items in the past are likely to agree again in the
    future. Therefore, a person who wants to see a movie for example, might ask for
    recommendations from his/her friends. Now once he received the recommendations
    from some of his/her friends who have similar interests, are trusted more than
    recommendations from others. This information is used in the decision on which
    movie to see.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Content-based filtering (also known as cognitive filtering), which recommends
    items based on a comparison between the content of the items and a user profile.
    The content of each item is represented as a set of descriptors or terms, typically
    the words that occur in a document. The user profile is represented with the same
    terms and built up by analyzing the content of items that have been seen by the
    user. However, while implementing these types of recommendation systems, some
    issues that need to be considered are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, terms can be assigned automatically or manually. For automatic assignment,
    a method has to be chosen so that these items can be extracted from the item list.
    Second, terms have to be represented in a way so that both the user profile and
    the items can be compared in a meaningful way. The learning algorithm itself has
    to be chosen wisely so that it's going to be able to learn a user profile based
    on already observer (that is, seen) items and makes appropriate recommendations
    based on this user profile. Content-based filtering systems are mostly used with
    text documents, where term parsers are used to select single words from the documents.
    The vector space model and latent semantic indexing are two methods that use these
    terms to represent documents as vectors in a multidimensional space. Furthermore,
    it is also used in relevance feedback, genetic algorithms, neural networks, and
    the Bayesian classifier for learning a user profile.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A hybrid recommender system is a recent research and hybrid approach (that is,
    combining collaborative filtering and content-based filtering). Netflix is a good
    example of such a recommendation system that uses the **Restricted Boltzmann Machines**
    (**RBM**) and a form of the matrix factorization algorithm for large movie database
    like IMDb (see more at [https://pdfs.semanticscholar.org/789a/d4218d1e2e920b4d192023f840fe8246d746.pdf](https://pdfs.semanticscholar.org/789a/d4218d1e2e920b4d192023f840fe8246d746.pdf)).
    This recommendation which simply recommends movies, dramas, or streaming by comparing
    the watching and searching habits of similar users, is called rating prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge-based systems, where knowledge about users and products is used to
    reason what fulfills a user's requirements, using perception tree, decision support
    systems, and case-based reasoning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the collaborative filtering based recommender
    system for the movie recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Semisupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Between supervised and unsupervised learning, there is a small place for semi-supervised
    learning. In this case, the ML model usually receives an incomplete training signal.
    More statistically, the ML model receives a training set with some of the target
    outputs missing. Semi-supervised learning is more or less assumption based and
    often uses three kinds of assumption algorithms as the learning algorithm for
    the unlabeled datasets. The following assumptions are used: smoothness, cluster,
    and manifold. In other words, semi-supervised learning can furthermore be denoted
    as weakly supervised or a bootstrapping technique for using the hidden wealth
    of unlabeled examples to enhance the learning from a small amount of labeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: As already mentioned that the acquisition of labeled data for a learning problem
    often requires a skilled human agent. Therefore, the cost associated with the
    labeling process thus may render a fully labeled training set infeasible, whereas
    acquisition of unlabeled data is relatively inexpensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example: to transcribe an audio segment, in determining the 3D structure
    of a protein or determining whether there is oil at a particular location, expectation
    minimization and human cognition, and transitive. The In such situations, semi-supervised
    learning can be of great practical value.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark machine learning APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will describe two key concepts introduced by the Spark machine
    learning libraries (Spark MLlib and Spark ML) and the most widely used implemented
    algorithms that align with the supervised and unsupervised learning techniques
    we discussed in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: Spark machine learning libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As already stated, in the pre-Spark era, big data modelers typically used to
    build their ML models using statistical languages such as R, STATA, and SAS. However,
    this kind of workflow (that is, the execution flow of these ML algorithms) lacks
    efficiency, scalability, and throughput, as well as accuracy, with, of course,
    extended execution times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, data engineers used to reimplement the same model in Java, for example,
    to deploy on Hadoop. Using Spark, the same ML model can be rebuilt, adopted, and
    deployed, making the whole workflow much more efficient, robust, and faster, allowing
    you to provide hands-on insight to increase the performance. Moreover, implementing
    these algorithms in Hadoop means that these algorithms can run in parallel that
    cannot be run on R, STATA and SAS and so on. The Spark machine learning library
    is divided into two packages: Spark MLlib (`spark.mllib`) and Spark ML (`spark.ml`).'
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib is Spark's scalable machine learning library and is an extension of the
    Spark Core API which provides a library of easy-to-use machine learning algorithms.
    Spark algorithms are implemented in Scala and then expose the API for Java, Scala,
    Python, and R. Spark provides support of local vectors and matrix data types stored
    on a single machine, as well as distributed matrices backed by one or multiple
    RDDs. The beauties of Spark MLlib are numerous. For example, algorithms are highly
    scalable and leverage Spark's ability to work with a massive amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: They are fast foward designed for parallel computing with an in-memory based
    operation that is 100 times faster compared to MapReduce data processing (they
    also support disk-based operation, which is 10 times faster compared to what MapReduce
    has as normal data processing).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are diverse, since they cover common machine learning algorithms for regression
    analysis, classification, clustering, recommender systems, text analytics, and
    frequent pattern mining, and obviously cover all the steps required to build a
    scalable machine learning application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark ML adds a new set of machine learning APIs to let users quickly assemble
    and configure practical machine learning pipelines on top of datasets. Spark ML
    aims to offer a uniform set of high-level APIs built on top of DataFrames rather
    than RDDs that help users create and tune practical machine learning pipelines.
    Spark ML API standardizes machine learning algorithms to make the learning tasks
    easier to combine multiple algorithms into a single pipeline or data workflow
    for data scientists. The Spark ML uses the concepts of DataFrame and Datasets,
    which are much newer concepts introduced (as experimental) in Spark 1.6 and then
    used in Spark 2.0+.
  prefs: []
  type: TYPE_NORMAL
- en: In Scala and Java, DataFrame and Dataset have been unified, that is, DataFrame
    is just a type alias for a dataset of row. In Python and R, given the lack of
    type safety, DataFrame is the main programming interface.
  prefs: []
  type: TYPE_NORMAL
- en: The datasets hold diverse data types such as columns storing text, feature vectors,
    and true labels for the data. In addition to this, Spark ML also uses the transformer
    to transform one DataFrame into another or vice-versa, where the concept of the
    estimator is used to fit on a DataFrame to produce a new transformer. The pipeline
    API, on the other hand, can restrain multiple transformers and estimators together
    to specify an ML data workflow. The concept of the parameter was introduced to
    specify all the transformers and estimators to share a common API under an umbrella
    during the development of an ML application.
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib or Spark ML?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark ML provides a higher-level API built on top of DataFrames for constructing
    ML pipelines. Basically, Spark ML provides you with a toolset to create pipelines
    of different machine learning related transformations on your data. It makes it
    easy to, for example, chain feature extraction, dimensionality reduction, and
    the training of a classifier into one model, which as a whole can be later used
    for classification. MLlib, however, is older and has been in development longer,
    it has more features because of this. Therefore, using Spark ML is recommended
    because, the API is more versatile and flexible with DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction and transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you are going to build a machine learning model that will predict whether
    a credit card transaction is fraudulent or not. Now, based on the available background
    knowledge and data analysis, you might decide which data fields (aka features)
    are important for training your model. For example, amount, customer name, buying
    company name, and the address of the credit card owners are worth to providing
    for the overall learning process. These are important to consider since, if you
    just provide a randomly generated transaction ID, that will not carry any information
    so would not be useful at all. Thus, once you have decided which features to include
    in your training set, you then need to transform those features to train the model
    for better learning. The feature transformations help you add additional background
    information to the training data. The information enables the machine learning
    model to benefit from this experience eventually. To make the preceding discussion
    more concrete, suppose you have the following address of one of the customers
    represented in the string:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"123 Main Street, Seattle, WA 98101"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you see the preceding address, the address lacks proper semantics. In other
    words, the string has limited expressive power. This address will be useful only
    for learning address patterns associated with that exact address in a database,
    for example. However, breaking it up into fundamental parts can provide additional
    features such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Address" (123 Main Street)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"City" (Seattle)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"State" (WA)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"Zip" (98101)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you see the preceding patterns, your ML algorithm can now group more different
    transactions together and discover broader patterns. This is normal, since some
    customer''s zip codes contribute to more fraudulent activity than others. Spark
    provides several algorithms implemented for the feature extractions and to make
    transformation easier. For example, the current version provides the following
    algorithms for feature extractions:'
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CountVectorizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, a feature transformer is an abstraction that includes feature
    transformers and learned models. Technically, a transformer implements a method
    named `transform()`, which converts one DataFrame into another, generally by appending
    one or more columns. Spark supports the following transformers to RDD or DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StopWordsRemover
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: n-gram
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binarizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PolynomialExpansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discrete cosine transform (DCT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StringIndexer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IndexToString
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OneHotEncoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VectorIndexer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StandardScaler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MinMaxScaler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MaxAbsScaler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucketizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ElementwiseProduct
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQLTransformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VectorAssembler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QuantileDiscretizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to page limitations, we cannot describe all of them. But we will discuss
    some widely used algorithms such as `CountVectorizer`, `Tokenizer`, `StringIndexer`,
    `StopWordsRemover`, `OneHotEncoder`, and so on. PCA, which is commonly used in
    dimensionality reduction, will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: CountVectorizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`CountVectorizer` and `CountVectorizerModel` aim to help convert a collection
    of text documents to vectors of token counts. When the prior dictionary is not
    available, `CountVectorizer` can be used as an estimator to extract the vocabulary
    and generates a `CountVectorizerModel`. The model produces sparse representations
    for the documents over the vocabulary, which can then be passed to other algorithms
    such LDA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have the text corpus as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00296.gif)**Figure 7**: Text corpus containing name only'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we want to convert the preceding collection of texts to vectors of
    token counts, Spark provides the `CountVectorizer ()` API for doing so. First,
    let''s create a simple DataFrame for the earlier table, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In many cases, you can set the input column with `setInputCol`. Let''s look
    at an example of it and let''s fit a `CountVectorizerModel` object from the corpus,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s downstream the vectorizer using the extractor, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s check to make sure it works properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00306.gif)**Figure 8**: Name text corpus has been featurized'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move to the feature transformers. One of the most important transformers
    is the tokenizer, which is frequently used in the machine learning task for handling
    categorical data. We will see how to work with this transformer in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tokenization is the process of enchanting important components from raw text,
    such as words, and sentences, and breaking the raw texts into individual terms
    (also called words). If you want to have more advanced tokenization on regular
    expression matching, `RegexTokenizer` is a good option for doing so. By default,
    the parameter *pattern* (regex, default: `s+`) is used as delimiters to split
    the input text. Otherwise, you can also set parameter *gaps* to false, indicating
    the regex *pattern* denotes *tokens* rather than splitting gaps. This way, you
    can find all matching occurrences as the tokenization result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you have the following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization,is the process of enchanting words,from the raw text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want,to have more advance tokenization, `RegexTokenizer`,is a good option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here,will provide a sample example on how to tokenize sentences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, you can find all matching occurrences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, you want to tokenize each meaningful word from the preceding four sentences.
    Let''s create a DataFrame from the earlier sentences, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s create a tokenizer by instantiating the `Tokenizer ()` API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, count the number of tokens in each sentence using a UDF, as follows: `import
    org.apache.spark.sql.functions._`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now tokenize words form each sentence, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, show each token against each raw sentence, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code prints a snap from the tokenized DataFrame containing
    the raw sentence, bag of words, and number of tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00315.gif)**Figure 9**: Tokenized words from the raw texts'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you use `RegexTokenizer` API, you will get better results. This
    goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a regex tokenizer by instantiating the `RegexTokenizer ()` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now tokenize words from each sentence, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code prints a snap from the tokenized DataFrame using
    RegexTokenizer containing the raw sentence, bag of words, and number of tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00318.gif)**Figure 10**: Better tokenization using RegexTokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: StopWordsRemover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stop words are words that should be excluded from the input, typically because
    the words appear frequently and don't carry as much meaning. Spark's `StopWordsRemover`
    takes as input a sequence of strings, which is tokenized by `Tokenizer` or `RegexTokenizer`.
    Then, it removes all the stop words from the input sequences. The list of stop
    words is specified by the `stopWords` parameter. The current implementation for
    the `StopWordsRemover` API provides the options for the Danish, Dutch, Finnish,
    French, German, Hungarian, Italian, Norwegian, Portuguese, Russian, Spanish, Swedish,
    Turkish, and English languages. To provide an example, we can simply extend the
    preceding `Tokenizer` example in the previous section, since they are already
    tokenized. For this example, however, we will use the `RegexTokenizer` API.
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, create a stop word remover instance from the `StopWordsRemover ()`
    API, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s remove all the stop words and print the results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code prints a snap from the filtered DataFrame excluding
    the stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00324.gif)**Figure 11**: Filtered (that is, without stop words) tokens'
  prefs: []
  type: TYPE_NORMAL
- en: StringIndexer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'StringIndexer encodes a string column of labels to a column of label indices.
    The indices are in `[0, numLabels)`, ordered by label frequencies, so the most
    frequent label gets index 0\. If the input column is numeric, we cast it to string
    and index the string values. When downstream pipeline components such as estimator
    or transformer make use of this string-indexed label, you must set the input column
    of the component to this string-indexed column name. In many cases, you can set
    the input column with `setInputCol`. Suppose you have some categorical data in
    the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00158.gif)**Figure 12**: DataFrame for applying String Indexer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want to index the name column so that the most frequent name (that
    is, Jason in our case) gets index 0\. To make this, Spark provides `StringIndexer`
    API for doing so. For our example, this can be done, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, let''s create a simple DataFrame for the preceding table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s index the name column, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s downstream the indexer using the transformer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s check to make sure if it works properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00381.gif)**Figure 13**: Label creation using StringIndexer'
  prefs: []
  type: TYPE_NORMAL
- en: Another important transformer is the OneHotEncoder, which is frequently used
    in machine learning tasks for handling categorical data. We will see how to work
    with this transformer in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: OneHotEncoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A one-hot encoding maps a column of label indices to a column of binary vectors,
    with at most a single value. This encoding allows algorithms that expect continuous
    features, such as Logistic Regression, to use categorical features. Suppose you
    have some categorical data in the following format (the same that we used for
    describing the `StringIndexer` in the previous section):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00253.gif)**Figure 14:** DataFrame for applying OneHotEncoder'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want to index the name column so that the most frequent name in the
    dataset (that is, **Jason** in our case) gets index **0**. However, what''s the
    use of just indexing them? In other words, you can further vectorize them and
    then you can feed the DataFrame to any ML models easily. Since we have already
    seen how to create a DataFrame in the previous section, here, we will just show
    how to encode them toward Vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s transform it into a vector using `Transformer` and then see the
    contents, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting DataFrame containing a snap is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00228.gif)**Figure 15**: Creating category index and vector using OneHotEncoder'
  prefs: []
  type: TYPE_NORMAL
- en: Now you can see that a new column containing feature vectors has been added
    in the resulting DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib's goal is to make practical machine learning (ML) scalable and easy. Spark
    introduced the pipeline API for the easy creation and tuning of practical ML pipelines.
    As discussed previously, extracting meaningful knowledge through feature engineering
    in an ML pipeline creation involves a sequence of data collection, preprocessing,
    feature extraction, feature selection, model fitting, validation, and model evaluation
    stages. For example, classifying the text documents might involve text segmentation
    and cleaning, extracting features, and training a classification model with cross-validation
    toward tuning. Most ML libraries are not designed for distributed computation
    or they do not provide native support for pipeline creation and tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset abstraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When running SQL queries from another programming language (for example, Java),
    the result is returned as a DataFrame. A DataFrame is a distributed collection
    of data organized into named columns. A dataset, on the other hand, is an interface
    that tries to provide the benefits of RDDs out of the Spark SQL. A dataset can
    be constructed from some JVM objects such as primitive types (for example, `String`,
    `Integer`, and `Long`), Scala case classes, and Java Beans. An ML pipeline involves
    a number of the sequences of dataset transformations and models. Each transformation
    takes an input dataset and outputs the transformed dataset, which becomes the
    input to the next stage. Consequently, the data import and export are the start
    and end points of an ML pipeline. To make these easier, Spark MLlib and Spark
    ML provide import and export utilities of a dataset, DataFrame, RDD, and model
    for several application-specific types, including:'
  prefs: []
  type: TYPE_NORMAL
- en: LabeledPoint for classification and regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LabeledDocument for cross-validation and Latent Dirichlet Allocation (LDA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rating and ranking for collaborative filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, real datasets usually contain numerous types, such as user ID, item
    IDs, labels, timestamps, and raw records. Unfortunately, the current utilities
    of Spark implementation cannot easily handle datasets consisting of these types,
    especially time-series datasets. The feature transformation usually forms the
    majority of a practical ML pipeline. A feature transformation can be viewed as
    appending or dropping a new column created from existing columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, you will see that the text tokenizer breaks a document
    into a bag of words. After that, the TF-IDF algorithm converts a bag of words
    into a feature vector. During the transformations, the labels need to be preserved
    for the model-fitting stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00137.jpeg)**Figure 16**: Text processing for machine learning model
    (DS indicates data sources)'
  prefs: []
  type: TYPE_IMG
- en: Here, the ID, text, and words are conceded during the transformations steps.
    They are useful in making predictions and model inspection. However, they are
    actually unnecessary for model fitting to state. These also don't provide much
    information if the prediction dataset contains only the predicted labels. Consequently,
    if you want to inspect the prediction metrics, such as the accuracy, precision,
    recall, weighted true positives, and weighted false positives, it is quite useful
    to look at the predicted labels along with the raw input text and tokenized words.
    The same recommendation also applies to other machine learning applications using
    Spark ML and Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, an easy conversion between RDDs, dataset, and DataFrames has been
    made possible for in-memory, disk, or external data sources such as Hive and Avro.
    Although creating new columns from existing columns is easy with user-defined
    functions, the manifestation of dataset is a lazy operation. In contrast, the
    dataset supports only some standard data types. However, to increase the usability
    and to make a better fit for the machine learning model, Spark has also added
    the support for the `Vector` type as a user-defined type that supports both dense
    and sparse feature vectors under `mllib.linalg.DenseVector` and `mllib.linalg.Vector`.
  prefs: []
  type: TYPE_NORMAL
- en: Complete DataFrame, dataset, and RDD examples in Java, Scala, and Python can
    be found in the `examples/src/main/` folder under the Spark distribution. Interested
    readers can refer to Spark SQL's user guide at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)
    to learn more about DataFrame, dataset, and the operations they support.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark provides pipeline APIs under Spark ML. A pipeline comprises a sequence
    of stages consisting of transformers and estimators. There are two basic types
    of pipeline stages, called transformer and estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: A transformer takes a dataset as an input and produces an augmented dataset
    as the output so that the output can be fed to the next step. For example, **Tokenizer**
    and **HashingTF** are two transformers. Tokenizer transforms a dataset with text
    into a dataset with tokenized words. A HashingTF, on the other hand, produces
    the term frequencies. The concept of tokenization and HashingTF is commonly used
    in text mining and text analytics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the contrary, an estimator must be the first on the input dataset to produce
    a model. In this case, the model itself will be used as the transformer for transforming
    the input dataset into the augmented output dataset. For example, a **Logistic
    Regression** or linear regression can be used as an estimator after fitting the
    training dataset with corresponding labels and features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After that, it produces a logistic or linear regression model, which implies
    that developing a pipeline is easy and simple. Well, all you need to do is to
    declare required stages, then configure the related stage''s parameters; finally,
    chain them in a pipeline object, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00374.jpeg)**Figure 17**: Spark ML pipeline model using logistic regression
    estimator (DS indicates data store, and the steps inside the dashed line only
    happen during pipeline fitting)'
  prefs: []
  type: TYPE_IMG
- en: If you look at *Figure 17*, the fitted model consists of a Tokenizer, a HashingTF
    feature extractor, and a fitted logistic regression model. The fitted pipeline
    model acts as a transformer that can be used for prediction, model validation,
    model inspection, and, finally, model deployment. However, to increase the performance
    in terms of prediction accuracy, the model itself needs to be tuned.
  prefs: []
  type: TYPE_NORMAL
- en: Now we know about the available algorithms in Spark MLlib and ML, now it's time
    to get prepared before starting to use them in a formal way for solving supervised
    and unsupervised learning problems. In the next section, we will start on feature
    extraction and transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, to make the discussion concrete, only the dimensionality reduction
    using PCA and the LDA for topic modeling will be discussed for text clustering.
    Other algorithms for unsupervised learning will be discussed in [Chapter 13](part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c),
    *My Name is Bayes, Naive Bayes* with some practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality reduction is the process of reducing the number of variables
    under consideration. It can be used to extract latent features from raw and noisy
    features or to compress data while maintaining the structure. Spark MLlib provides
    support for dimensionality reduction on the `RowMatrix` class. The most commonly
    used algorithms for reducing the dimensionality of data are PCA and SVD. However,
    in this section, we will discuss PCA only to make the discussion more concrete.
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PCA is a statistical procedure that uses an orthogonal transformation to convert
    a set of observations of possibly correlated variables into a set of values of
    linearly uncorrelated variables called principal components. A PCA algorithm can
    be used to project vectors to a low-dimensional space using PCA. Then, based on
    the reduced feature vectors, an ML model can be trained. The following example
    shows how to project 6D feature vectors into four-dimensional principal components.
    Suppose, you have a feature vector as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s create a DataFrame from it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces a feature DataFrame having 6D feature vector for
    the PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00291.gif)**Figure 18**: Creating a feature DataFrame (6-dimensional
    feature vectors) for PCA'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s instantiate the PCA model by setting necessary parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to make a difference, we set the output column as `pcaFeatures` using
    the `setOutputCol()` method. Then, we set the dimension of the PCA. Finally, we
    fit the DataFrame to make the transformation. Note that the PCA model includes
    an `explainedVariance` member. A model can be loaded from such older data but
    will have an empty vector for `explainedVariance`. Now let''s show the resulting
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces a feature DataFrame having 4D feature vectors as
    principal components using the PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00384.gif)**Figure 19**: Four-dimensional principal components (PCA
    features)'
  prefs: []
  type: TYPE_IMG
- en: Using PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA, which is used widely in dimensionality reduction, is a statistical method
    that helps to find the rotation matrix. For example, if we want to check if the
    first coordinate has the largest variance possible. Also it helps to check if
    there is any succeeding coordinate that will turn the largest variance possible.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, the PCA model calculates such parameters and returns them as a rotation
    matrix. The columns of the rotation matrix are called principal components. Spark
    MLlib supports PCA for tall and skinny matrices stored in a row-oriented format
    and any vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Regression Analysis - a practical use of PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will first explore the **MSD** (**Million Song Dataset**)
    that will be used for the regression analysis. Then we will show how to use PCA
    to reduce the dimensions of the dataset. Finally, we will evaluate the linear
    regression model for the regression quality.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset collection and exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will describe the very famous MNIST dataset. This dataset
    will be used throughout this chapter. The MNIST database of handwritten digits
    (downloaded from [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html))
    has a training set of 60,000 examples and a test set of 10,000 examples. It is
    a subset of a larger set available from NIST. The digits have been size-normalized
    and centered in a fixed-size image. Consequently, this is a very good example
    dataset for those who are trying to learn techniques and pattern recognition methods
    on real-world data while spending minimal efforts on preprocessing and formatting.
    The original black and white (bi-level) images from NIST were size-normalized
    to fit in a 20 x 20 pixel box while preserving their aspect ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MNIST database was constructed from NIST''s special database 3 and special
    database 1, which contain binary images of handwritten digits. A sample of the
    dataset is given in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00121.gif)**Figure 20**: A snap of the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that there are 780 features altogether. Consequently, sometimes,
    many machine learning algorithms will fail due to the high-dimensional nature
    of your dataset. Therefore, to address this issue, in the next section, we will
    show you how to reduce the dimensions without sacrificing the qualities machine
    learning tasks, such as classification. However, before diving into the problem,
    let's get some background knowledge on regression analysis first.
  prefs: []
  type: TYPE_NORMAL
- en: What is regression analysis?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear regression belongs to the family of regression algorithms. The goal
    of regression is to find relationships and dependencies between variables. It
    is modeling the relationship between a continuous scalar dependent variable *y*
    (also, label or target in machine learning terminology) and one or more (a D-dimensional
    vector) explanatory variables (also, independent variables, input variables, features,
    observed data, observations, attributes, dimensions, data point, and so on) denoted
    *x* using a linear function. In regression analysis, the goal is to predict a
    continuous target variable, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00171.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: A regression algorithm is meant to produce continuous output. The
    input is allowed to be either'
  prefs: []
  type: TYPE_NORMAL
- en: 'discrete or continuous (source: Nishant Shukla, Machine Learning with TensorFlow,
    Manning Publications co. 2017)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you might have some confusion in your mind about what the basic difference
    between a classification and a regression problem is. The following information
    box will make it clearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression versus classification:** On the other hand, another area, called
    classification, is about predicting a label from a finite set but with discrete
    values. This distinction is important to know because discrete-valued output is
    handled better by classification, which will be discussed in upcoming sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model for a multiple regression that involves a linear combination of input
    variables takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: y = ss[0] + ss[1]x[1] + ss[2]x[2] + ss[3]x[3] +..... + e
  prefs: []
  type: TYPE_NORMAL
- en: Figure 22 shows an example of simple linear regression with one independent
    variable (*x* axis). The model (red line) is calculated using training data (blue
    points), where each point has a known label (*y* axis) to fit the points as accurately
    as possible by minimizing the value of a chosen loss function. We can then use
    the model to predict unknown labels (we only know *x* value and want to predict
    *y* value).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00383.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Regression graph that separates data points (the dots [.] refer
    to data points in the graph and the red line refers to the regression)'
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides an RDD-based implementation of the linear regression algorithm.
    You can train a linear regression model with no regularization using stochastic
    gradient descent. This solves the least squares regression formulation *f (weights)
    = 1/n ||A weights-y||^2* (which is the mean squared error). Here, the data matrix
    has *n* rows, and the input RDD holds the set of rows of *A*, each with its corresponding
    right-hand side label *y*. For more information, refer to [https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/regression/LinearRegression.scala](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/regression/LinearRegression.scala).
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Load the dataset and create RDD**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For loading the MNIST dataset in LIBSVM format, here we used the built-in API
    called MLUtils from Spark MLlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2\. Compute the number of features to make the dimensionality reduction
    easier:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: So the dataset has 780 columns -i.e. features so this can be considered as high-dimensional
    one (features). Therefore, sometimes it is worth reducing the dimensions of the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3\. Now prepare the training and test set as follows:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The thing is that we will train the `LinearRegressionwithSGD` model twice.
    First, we will use the normal dataset with the original dimensions of the features,
    secondly, using half of the features. With the original one, the training and
    test set preparation go as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, for the reduced features, the training goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4\. Training the linear regression model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now iterate 20 times and train the `LinearRegressionWithSGD` for the normal
    features and reduced features, respectively, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Beware! Sometimes, `LinearRegressionWithSGD()` returns `NaN`. In my opinion,
    there are two reasons for this happening:'
  prefs: []
  type: TYPE_NORMAL
- en: If the `stepSize` is big. In that case, you should use something smaller, such
    as 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your train data has `NaN`. If so, the result will likely be `NaN`. So, it is
    recommended to remove the null values prior to training the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 5\. Evaluating both models**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we evaluate the classification model, first, let''s prepare for computing
    the MSE for the normal to see the effects of dimensionality reduction on the original
    predictions. Obviously, if you want a formal way to quantify the accuracy of the
    model and potentially increase the precision and avoid overfitting. Nevertheless,
    you can do from residual analysis. Also it would be worth to analyse the selection
    of the training and test set to be used for the model building and then the evaluation.
    Finally, selection techniques help you to describe the various attributes of a
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now compute the prediction sets for the PCA one as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now compute the MSE and print them for each case as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the MSE is actually calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00238.gif)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 6.** **Observing the model coefficient for both models**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the model coefficient as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you should observer the following output on your terminal/console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Binary and multiclass classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Binary classifiers are used to separate the elements of a given dataset into
    one of two possible groups (for example, fraud or not fraud) and are a special
    case of multiclass classification. Most binary classification metrics can be generalized
    to multiclass classification metrics. A multiclass classification describes a
    classification problem, where there are *M>2* possible labels for each data point
    (the case where *M=2* is the binary classification problem).
  prefs: []
  type: TYPE_NORMAL
- en: For multiclass metrics, the notion of positives and negatives is slightly different.
    Predictions and labels can still be positive or negative, but they must be considered
    in the context of a particular class. Each label and prediction takes on the value
    of one of the multiple classes and so they are said to be positive for their particular
    class and negative for all other classes. So, a true positive occurs whenever
    the prediction and the label match, while a true negative occurs when neither
    the prediction nor the label takes on the value of a given class. By this convention,
    there can be multiple true negatives for a given data sample. The extension of
    false negatives and false positives from the former definitions of positive and
    negative labels is straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Performance metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While there are many different types of classification algorithms, evaluation
    metrics more or less shares similar principles. In a supervised classification
    problem, there exists a true output and a model-generated predicted output for
    each data point. For this reason, the results for each data point can be assigned
    to one of four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive** (**TP**): Label is positive and prediction is also positive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negative** (**TN**): Label is negative and prediction is also negative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive** (**FP**): Label is negative but prediction is positive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative** (**FN**): Label is positive but prediction is negative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, to get a clearer idea about these parameters, refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00181.jpeg)**Figure 23**: Prediction classifier (that is, confusion
    matrix)'
  prefs: []
  type: TYPE_IMG
- en: The TP, FP, TN, FN are the building blocks for most classifier evaluation metrics.
    A fundamental point when considering classifier evaluation is that pure accuracy
    (that is, was the prediction correct or incorrect) is not generally a good metric.
    The reason for this is that a dataset may be highly unbalanced. For example, if
    a model is designed to predict fraud from a dataset where 95% of the data points
    are not fraud and 5% of the data points are fraud. Then suppose a naive classifier
    predicts not fraud (regardless of input) will be 95% accurate. For this reason,
    metrics such as precision and recall are typically used because they take into
    account the type of error. In most applications, there is some desired balance
    between precision and recall, which can be captured by combining the two into
    a single metric, called the **F-measure**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision signifies how many of the positively classified were relevant. On
    the other hand, recall signifies how good a test is at detecting the positives?
    In binary classification, recall is called sensitivity. It is important to note
    that the the precision may not decrease with recall. The relationship between
    recall and precision can be observed in the stair step area of the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: Receiver operating characteristic (ROC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area under ROC curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area under precision-recall curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These curves are typically used in binary classification to study the output
    of a classifier. However, sometimes it is good to combine precision and recall
    to choose between two models. In contrast, using precision and recall with multiple-number
    evaluation metrics makes it harder to compare algorithms. Suppose you have two
    algorithms that perform as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Classifier** | **Precision** | **Recall** |'
  prefs: []
  type: TYPE_TB
- en: '| X | 96% | 89% |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 99% | 84% |'
  prefs: []
  type: TYPE_TB
- en: 'Here, neither classifier is obviously superior, so it doesn''t immediately
    guide you toward picking the optimal one. But using F1 score, which is a measure
    that combines precision and recall (that is, the harmonic mean of precision and
    recall), balanced the F1 score. Let''s calculate it and place it in the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Classifier** | **Precision** | **Recall** | **F1 score** |'
  prefs: []
  type: TYPE_TB
- en: '| X | 96% | 89% | 92.36% |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 99% | 84% | 90.885% |'
  prefs: []
  type: TYPE_TB
- en: Therefore, having F1-score helps make a decision for selecting from a large
    number of classifiers. It gives a clear preference ranking among all of them and
    therefore a clear direction for progress, that is, classifier **X**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the binary classification, the preceding performance metrics can be calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00057.jpeg)**Figure 24**: Mathematical formula for computing performance
    metrics for binary classifiers (source: [https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html))'
  prefs: []
  type: TYPE_IMG
- en: 'However, in multiclass classification problems where more than two predicted
    labels are associated, computing the earlier metrics is more complex but can be
    computed using the following mathematical equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00169.jpeg)**Figure 25**: Mathematical formula for computing performance
    metrics for multiclass classifiers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where *δ*^(*x*) is called modified delta function and that can be defined as
    follows (source: [https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Binary classification using logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression is widely used to predict a binary response. This is a
    linear method that can be written mathematically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00160.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *L(w; x, y)* is the loss function is called logistic
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'For binary classification problems, the algorithm will output a binary logistic
    regression model. Given a new data point, denoted by *x*, the model makes predictions
    by applying the logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00233.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Where *z = w^Tx,* and by default, if *f(w^Tx)>0.5*, the outcome is positive,
    or negative otherwise, though unlike linear SVMs, the raw output of the logistic
    regression model, *f(z)*, has a probabilistic interpretation (that is, the probability
    that *x* is positive).
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear SVM** is the newest extremely fast machine learning (data mining)
    algorithm for solving multiclass classification problems from ultralarge datasets
    that implements an original proprietary version of a cutting plane algorithm for
    designing a linear support vector machine (source: [www.linearsvm.com/](http://www.linearsvm.com/)
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: Breast cancer prediction using logistic regression of Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at how to develop a cancer diagnosis pipeline
    with Spark ML. A real dataset will be used to predict the probability of breast
    cancer. To be more specific, Wisconsin Breast Cancer Dataset will be used.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we have used simpler datasets that are structured and manually curated
    for machine learning application development, and, of course, many of them show
    good classification accuracy. The Wisconsin Breast Cancer Dataset from the UCI
    machine learning repository ([https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)),
    contains data that was donated by researchers at the University of Wisconsin and
    includes measurements from digitized images of fine-needle aspirations of breast
    masses. The values represent characteristics of the cell nuclei present in the
    digital images described in the following subsection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'To read more about the Wisconsin Breast Cancer Dataset, refer to the authors''
    publication: *Nuclear feature extraction for breast tumor diagnosis*, *IS&T/SPIE*
    1993 *International Symposium on Electronic Imaging: Science and Technology*,
    volume 1905, pp 861-870 by *W.N. Street*, *W.H. Wolberg*, and *O.L. Mangasarian*,
    1993.'
  prefs: []
  type: TYPE_NORMAL
- en: Developing the pipeline using Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will show you how to predict the possibility of breast cancer with step-by-step
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Load and parse the data**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `parseRDD()` method goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The `parseCancer()` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that here we have simplified the dataset. For the value 4.0, we have converted
    them to 1.0, and 0.0 otherwise. The `Cancer` class is a case class that can be
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2: Convert RDD to DataFrame for the ML pipeline**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The DataFrame looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00334.gif)**Figure 26:** A snap of the cancer dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Feature extraction and transformation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, let''s select the feature column, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s assemble them into a feature vector, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now transform them into a DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the structure of the transformed DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you should observe a DataFrame containing the features calculated based
    on the columns on the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00361.gif)**Figure 27:** New DataFrame containing features'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s use the `StringIndexer` and create the label for the training
    dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you should observe a DataFrame containing the features and labels calculated
    based on the columns in the left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00204.gif)**Figure 28:** New DataFrame containing features and labels
    to training the ML models'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Create test and training set**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5: Creating an estimator using the training sets**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create an estimator for the pipeline using the logistic regression with
    `elasticNetParam`. We also specify the max iteration and regression parameter,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6: Getting raw prediction, probability, and prediction for the test
    set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform the model using the test set to get raw prediction, probability,
    and prediction for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting DataFrame is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00159.gif)**Figure 29:** New DataFrame with raw prediction and actual
    prediction against each row'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7: Generating objective history of training**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s generate the objective history of the model in each iteration, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment produces the following output in terms of training
    loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the loss gradually reduces in later iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8: Evaluating the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will have to make sure that the classifier that we used comes from
    the binary logistic regression summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s obtain the ROC as a `DataFrame` and `areaUnderROC`. A value approximate
    to 1.0 is better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding lines prints the value of `areaUnderROC`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This is excellent! Now let''s compute other metrics, such as true positive
    rate, false positive rate, false negative rate, and total count, and a number
    of instances correctly and wrongly predicted, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you should observe an output from the preceding code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Total Count: 209** **Correctly Predicted: 202** **Wrongly Identified: 7**
    **True Positive: 140** **False Negative: 4** **False Positive: 3** **ratioWrong:
    0.03349282296650718** **ratioCorrect: 0.9665071770334929**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s judge the accuracy of the model. However, first, we need to
    set the model threshold to maximize `fMeasure`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s compute the accuracy, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output, which is almost 99.64%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Multiclass classification using logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A binary logistic regression can be generalized into multinomial logistic regression
    to train and predict multiclass classification problems. For example, for *K*
    possible outcomes, one of the outcomes can be chosen as a pivot, and the other
    *K−1* outcomes can be separately regressed against the pivot outcome. In `spark.mllib`,
    the first class 0 is chosen as the `pivot` class.
  prefs: []
  type: TYPE_NORMAL
- en: For multiclass classification problems, the algorithm will output a multinomial
    logistic regression model, which contains *k−1binary* logistic regression models
    regressed against the first class. Given a new data point, *k−1models* will be
    run, and the class with the largest probability will be chosen as the predicted
    class. In this section, we will show you an example of a classification using
    the logistic regression with L-BFGS for faster convergence.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Load and parse the MNIST dataset in LIVSVM format**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2\. Prepare the training and test sets**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split data into training (75%) and test (25%), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3\. Run the training algorithm to build the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the training algorithm to build the model by setting a number of classes
    (10 for this dataset). For better classification accuracy, you can also specify
    intercept and validate the dataset using the Boolean true value, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Set intercept as true if the algorithm should add an intercept using `setIntercept()`.
    If you want the algorithm to validate the training set before the model building
    itself, you should set the value as true using the `setValidateData()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4\. Clear the default threshold**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clear the default threshold so that the training does not occur with the default
    setting, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5\. Compute raw scores on the test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute raw scores on the test set so that we can evaluate the model using
    the aforementioned performance metrics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6\. Instantiate a multiclass metrics for the evaluation**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 7\. Constructing the confusion matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'In a confusion matrix, each column of the matrix represents the instances in
    a predicted class, while each row represents the instances in an actual class
    (or vice versa). The name stems from the fact that it makes it easy to see if
    the system is confusing two classes. For more, refer to matrix ([https://en.wikipedia.org/wiki/Confusion_matrix.Confusion](https://en.wikipedia.org/wiki/Confusion_matrix.Confusion)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00065.gif)**Figure 30:** Confusion matrix generated by the logistic
    regression classifier'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 8\. Overall statistics**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s compute the overall statistics to judge the performance of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment produces the following output, containing some performance
    metrics, such as accuracy, precision, recall, true positive rate , false positive
    rate, and f1 score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s compute the overall, that is, summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment prints the following output containing weighted
    precision, recall, f1 score, and false positive rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The overall statistics say that the accuracy of the model is more than 92%.
    However, we can still improve it using a better algorithm such as **random forest**
    (**RF**). In the next section, we will look at the random forest implementation
    to classify the same model.
  prefs: []
  type: TYPE_NORMAL
- en: Improving classification accuracy using random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests (also sometimes called random decision forests) are ensembles
    of decision trees. Random forests are one of the most successful machine learning
    models for classification and regression. They combine many decision trees in
    order to reduce the risk of overfitting. Like decision trees, random forests handle
    categorical features, extend to the multiclass classification setting, do not
    require feature scaling, and are able to capture nonlinearities and feature interactions.
    There are numerous advantageous RFs. They can overcome the overfitting problem
    across their training dataset by combining many decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: A forest in the RF or RDF usually consists of hundreds of thousands of trees.
    These trees are actually trained on different parts of the same training set.
    More technically, an individual tree that has grown very deep tends to learn from
    highly unpredictable patterns. This kind of nature of the trees creates overfitting
    problems on the training sets. Moreover, low biases make the classifier a low
    performer even if your dataset quality is good in terms of features presented.
    On the other hand, an RF helps to average multiple decision trees together with
    the goal of reducing the variance to ensure consistency by computing proximities
    between pairs of cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this increases a small bias or some loss of the interpretability of
    the results. But, eventually, the performance of the final model is increased
    dramatically. While using the RF as a classifier, here goes the parameter setting:'
  prefs: []
  type: TYPE_NORMAL
- en: If the number of trees is 1, then no bootstrapping is used at all; however,
    if the number of trees is *> 1*, then bootstrapping is accomplished. The supported
    values are `auto`, `all`, `sqrt`, `log2`, and `onethird`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The supported numerical values are *(0.0-1.0]* and *[1-n]*. However, if `featureSubsetStrategy`
    is chosen as `auto`, the algorithm chooses the best feature subset strategy automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `numTrees == 1`, the `featureSubsetStrategy` is set to be `all`. However,
    if `numTrees > 1` (that is, forest), `featureSubsetStrategy` is set to be `sqrt`
    for classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, if a real value *n* is set in the range of *(0, 1.0]*, `n*number_of_features`
    will be used. However, if an integer value say *n* is in the `range (1, the number
    of features)`, only `n` features are used alternatively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `categoricalFeaturesInfo` parameter , which is a map, is used for storing
    arbitrary categorical features. An entry *(n -> k)* indicates that feature *n*
    is categorical with *k* categories indexed from *0: {0, 1,...,k-1}.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impurity criterion is used only for the information gain calculation. The
    supported values are *gini* and *variance* for classification and regression,
    respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `maxDepth` is the maximum depth of the tree (for example, depth 0 means
    1 leaf node, depth 1 means 1 internal node *+ 2* leaf nodes, and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `maxBins` signifies the maximum number of bins used for splitting the features,
    where the suggested value is 100 to get better results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the random seed is used for bootstrapping and choosing feature subsets
    to avoid the random nature of the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As already mentioned, since RF is fast and scalable enough for the large-scale
    dataset, Spark is a suitable technology to implement the RF to take the massive
    scalability. However, if the proximities are calculated, storage requirements
    also grow exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying MNIST dataset using random forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show an example of a classification using the random
    forest. We will break down the code step-by-step so that you can understand the
    solution easily.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1\. Load and parse the MNIST dataset in LIVSVM format**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2\. Prepare the training and test sets**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split data into training (75%) and test (25%) and also set the seed for the
    reproducibility, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3\. Run the training algorithm to build the model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a random forest model with an empty `categoricalFeaturesInfo. This required`
    since all the features are continuous in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Note that training a random forest model is very resource extensive. Consequently,
    it will take more memory, so beware of OOM. I would say increase the Java heap
    space prior to running this code.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4\. Compute raw scores on the test set**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute raw scores on the test set so that we can evaluate the model using
    the aforementioned performance metrics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5\. Instantiate a multiclass metrics for the evaluation**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 6\. Constructing the confusion matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code prints the following confusion matrix for our classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.gif)**Figure 31:** Confusion matrix generated by the random forest
    classifier'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 7\. Overall statistics**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s compute the overall statistics to judge the performance of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment produces the following output, containing some performance
    metrics, such as accuracy, precision, recall, true positive rate , false positive
    rate, and F1 score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s compute the overall statistics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment prints the following output, containing weighted
    precision, recall, F1 score, and false positive rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: The overall statistics say that the accuracy of the model is more than 96%,
    which is better than that of logistic regression. However, we can still improve
    it using better model tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we had a brief introduction to the topic and got a grasp of
    simple, yet powerful and common ML techniques. Finally, you saw how to build your
    own predictive model using Spark. You learned how to build a classification model,
    how to use the model to make predictions, and finally, how to use common ML techniques
    such as dimensionality reduction and One-Hot Encoding.
  prefs: []
  type: TYPE_NORMAL
- en: In the later sections, you saw how to apply the regression technique to high-dimensional
    datasets. Then, you saw how to apply a binary and multiclass classification algorithm
    for predictive analytics. Finally, you saw how to achieve outstanding classification
    accuracy using a random forest algorithm. However, we have other topics in machine
    learning that need to be covered too, for example, recommendation systems and
    model tuning for even more stable performance before you finally deploy the models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover some advanced topics of Spark. We will provide
    examples of machine learning model tuning for better performance, and we will
    also cover two examples for movie recommendation and text clustering, respectively.
  prefs: []
  type: TYPE_NORMAL
