<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 3. Data Analysis – Explore and Wrangle"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch03" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 3. Data Analysis – Explore and Wrangle</h1></div></div></div><p class="calibre11">We will cover the following recipes in this chapter:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Analyzing univariate data graphically</li><li class="listitem">Grouping the data and using dot plots</li><li class="listitem">Using scatter plots for multivariate data</li><li class="listitem">Using heat maps</li><li class="listitem">Performing summary statistics and plots</li><li class="listitem">Using a box-and-whisker plot</li><li class="listitem">Imputing the data</li><li class="listitem">Performing random sampling</li><li class="listitem">Scaling the data</li><li class="listitem">Standardizing the data</li><li class="listitem">Performing tokenization</li><li class="listitem">Removing stop words</li><li class="listitem">Stemming the words</li><li class="listitem">Performing word lemmatization</li><li class="listitem">Representing the text as a bag of words</li><li class="listitem">Calculating term frequencies and inverse document frequencies</li></ul></div><div class="calibre2" title="Introduction"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec37" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Introduction</h1></div></div></div><p class="calibre11">Before you venture into any data science application, it is always helpful in the long run to have a good understanding of the data that you are about to process. An understanding of the underlying data will help you choose the right algorithm to use for the problem at hand. Exploring the data at various levels of granularity is called <span class="strong1"><strong class="calibre12">Exploratory Data Analysis</strong></span> (<span class="strong1"><strong class="calibre12">EDA</strong></span>). In many cases, <span class="strong1"><strong class="calibre12">EDA</strong></span> can uncover patterns that are typically revealed by a data mining <a id="id160" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>algorithm <span class="strong1"><strong class="calibre12">EDA</strong></span> helps us understand data characteristics and provides you with the proper guidance in order to choose the right algorithm for the given problem.</p><p class="calibre11">In this chapter, we will cover <span class="strong1"><strong class="calibre12">EDA</strong></span> in detail. We will look into the practical techniques and tools that are used to perform <span class="strong1"><strong class="calibre12">EDA</strong></span> operations in an effective way.</p><p class="calibre11">Data preprocessing and transformation are two other important processes that can improve the quality of data science models and increase the success rate of data science projects.</p><p class="calibre11">Data preprocessing<a id="id161" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is the process of making the data ready in order to be ingested either by a data mining method or machine learning algorithm. It encompasses many things such as data cleaning, attribute subset selection, data transformation, and others. We will cover both numerical data preprocessing and text data preprocessing in this chapter.</p><p class="calibre11">Text data is a different beast than the numerical data. We need different transformation methods in order to make it suitable for ingestion in the machine learning algorithms. In this chapter, we will see how we can transform the text data. Typically, text transformation is a staged process with various components in the form of a pipeline.</p><p class="calibre11">Some of the components are as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Tokenization</li><li class="listitem">Stop word removal</li><li class="listitem">Base form conversion</li><li class="listitem">Feature derivation</li></ul></div><p class="calibre11">Typically, these components are applied to a given text in order to extract features. At the end of the pipeline, the text data is transformed in a way that it can be fed as input to the machine learning algorithms. In this chapter, we will see recipes for every component listed in the preceding pipeline.</p><p class="calibre11">Many times, a lot of errors may be introduced during the data collection phase. These may be due to human errors, limitations, or bugs in the data measuring or collective process/device. Data inconsistency is a big challenge. We will start our data preprocessing journey with data imputation is a way to handle errors in the incoming data and then proceed to other methods.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Analyzing univariate data graphically"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec38" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Analyzing univariate data graphically</h1></div></div></div><p class="calibre11">Datasets with <a id="id162" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>only one variable/column are called univariate data. Univariate is a general term in mathematics, which refers to any expression, equation, function, or polynomial with only one variable. In our case, we will restrict the univariate function to datasets. Let's say that we will measure the heights of a group of people in meters; the data will look as follows:</p><div class="calibre2"><pre class="programlisting">5, 5.2, 6, 4.7,…</pre></div><p class="calibre11">Our measurement is only about a single attribute of people, height. This is an example of univariate data.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec115" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Let's start our <span class="strong1"><strong class="calibre12">EDA</strong></span> recipe by looking at a sample univariate dataset through visualization. It is easy to analyze the data characteristics through the right visualization techniques. We will use <code class="literal">pyplot</code> to draw graphs in order to visualize the data. Pyplot is the state-machine interface to the matplotlib plotting library. Figures and axes are implicitly and automatically created to achieve the desired plot. The following link is a good reference<a id="id163" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> for <code class="literal">pyplot</code>:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://matplotlib.org/users/pyplot_tutorial.html">http://matplotlib.org/users/pyplot_tutorial.html</a>
</p><p class="calibre11">For this example, we <a id="id164" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will use a number of Presidential Requests of Congress in State of the Union Address. The following URL contains the data:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.presidency.ucsb.edu/data/sourequests.php">http://www.presidency.ucsb.edu/data/sourequests.php</a>
</p><p class="calibre11">The following is a sample of the data:</p><div class="calibre2"><pre class="programlisting">1946, 41
1947, 23
1948, 16
1949, 28
1950, 20
1951, 11
1952, 19
1953, 14
1954, 39
1955, 32
1956, 
1957, 14
1958, 
1959, 16
1960, 6</pre></div><p class="calibre11">We will visually look at this data and identify any outliers present in the data. We will follow a recursive approach with respect to the outliers. Once we have identified the outliers, we will remove them from the dataset and plot the remaining data in order to find any new outliers.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note05" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Recursively looking into the data after removing the perceived outlier in every iteration is a common approach in detection of outliers.</p></div></div></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec116" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">We will load the<a id="id165" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> data using NumPy's data loading utility. Then, we will address the data quality issues; in this case, we will address how to handle the null values. As you can see in the data, the years 1956 and 1958 have null entries. Let's replace the null values by <code class="literal">0</code> using the lambda function.</p><p class="calibre11">Following this, let's plot the data to look for any trends:</p><div class="calibre2"><pre class="programlisting"># Load libraries
import numpy as np
from matplotlib.pylab import frange
import matplotlib.pyplot as plt

fill_data = lambda x : int(x.strip() or 0)
data = np.genfromtxt('president.txt',dtype=(int,int),converters={1:fill_data},\
            delimiter=",")
x = data[:,0]
y = data[:,1]


# 2.Plot the data to look for any trends or values
plt.close('all')
plt.figure(1)
plt.title("All data")
plt.plot(x,y,'ro')
plt.xlabel('year')plt.ylabel('No Presedential Request')</pre></div><p class="calibre11">Let's calculate the percentile values and plot them as references in the plot that has been generated:</p><div class="calibre2"><pre class="programlisting">#3.Calculate percentile values (25th, 50th,75th) for the data to understand data distribution
perc_25 = np.percentile(y,25)
perc_50 = np.percentile(y,50)
perc_75 = np.percentile(y,75)
print
print "25th Percentile    = %0.2f"%(perc_25)
print "50th Percentile    = %0.2f"%(perc_50)
print "75th Percentile    = %0.2f"%(perc_75)
print
#4.Plot these percentile values as reference in the plot we generated in the previous step.
# Draw horizontal lines at 25,50 and 75th percentile
plt.axhline(perc_25,label='25th perc',c='r')
plt.axhline(perc_50,label='50th perc',c='g')
plt.axhline(perc_75,label='75th perc',c='m')plt.legend(loc='best')</pre></div><p class="calibre11">Finally, let's inspect the data visually for outliers and then remove them using the mask function. Let's <a id="id166" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>plot the data again without the outliers:</p><div class="calibre2"><pre class="programlisting">#5.Look for outliers if any in the data by visual inspection.
# Remove outliers using mask function 
# Remove outliers 0 and 54
y_masked = np.ma.masked_where(y==0,y)
#  Remove point 54
y_masked = np.ma.masked_where(y_masked==54,y_masked)

#6 Plot the data again.
plt.figure(2)
plt.title("Masked data")
plt.plot(x,y_masked,'ro')
plt.xlabel('year')
plt.ylabel('No Presedential Request')
plt.ylim(0,60)


# Draw horizontal lines at 25,50 and 75th percentile
plt.axhline(perc_25,label='25th perc',c='r')
plt.axhline(perc_50,label='50th perc',c='g')
plt.axhline(perc_75,label='75th perc',c='m')
plt.legend(loc='best')plt.show()</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec117" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In the first step, we will put some data loading techniques that we learnt in the previous chapter to action. You will have noticed that the years <code class="literal">1956</code> and <code class="literal">1958</code> are left blank. We will replace them with <code class="literal">0</code> using an anonymous function:</p><div class="calibre2"><pre class="programlisting">fill_data = lambda x : int(x.strip() or 0)</pre></div><p class="calibre11">The <code class="literal">fill_data</code> lambda function will replace any null value in the dataset; in this case, line no 11 and 13 with 0:</p><div class="calibre2"><pre class="programlisting">data = np.genfromtxt('president.txt',dtype=(int,int),converters={1:fill_data},delimiter=",")</pre></div><p class="calibre11">We will pass <code class="literal">fill_data</code> to the <code class="literal">genfromtxt</code> function's <code class="literal">converters</code> parameter. Note that <code class="literal">converters</code> takes a dictionary as its input. The key in the dictionary dictates which column<a id="id167" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> our function should be applied to. The value indicates the function. In this case, we specified <code class="literal">fill_data</code> as the function and set the key to 1 indicating that the <code class="literal">fill_data</code> function has to be applied to column 1. Now let's look at the data in the console:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; data[7:15]
array([[1953,   14],
       [1954,   39],
       [1955,   32],
       [1956,    0],
       [1957,   14],
       [1958,    0],
       [1959,   16],
       [1960,    6]])
&gt;&gt;&gt;</pre></div><p class="calibre11">As we can see, the years <code class="literal">1956</code> and <code class="literal">1958</code> have a <code class="literal">0</code> value added to them. For the ease of plotting, we will load the year data in x and the number of Presidential Requests to Congress in the State of Union Address to y:</p><div class="calibre2"><pre class="programlisting">x = data[:,0]
y = data[:,1]</pre></div><p class="calibre11">As you can see, in the first column, the year is loaded in <code class="literal">x</code> and the next column in <code class="literal">y</code>.</p><p class="calibre11">In step 2, we will plot the data with the x axis as the year and y axis representing the values:</p><div class="calibre2"><pre class="programlisting">plt.close('all')</pre></div><p class="calibre11">We will first close any previous graphs that are open from the previous programs:</p><div class="calibre2"><pre class="programlisting">plt.figure(1)</pre></div><p class="calibre11">We will give a number to our plot. This is very useful when we have a lot of graphs in a program:</p><div class="calibre2"><pre class="programlisting">plt.title("All data")</pre></div><p class="calibre11">We will specify a title for our plot:</p><div class="calibre2"><pre class="programlisting">plt.plot(x,y,'ro')</pre></div><p class="calibre11">Finally, we will plot x and y. The 'ro' parameter tells plyplot to plot x and y as dots (0) in the color red (r):</p><div class="calibre2"><pre class="programlisting">plt.xlabel('year')
plt.ylabel('No Presedential Request')</pre></div><p class="calibre11">Finally, the <span class="strong1"><em class="calibre15">x</em></span> and <span class="strong1"><em class="calibre15">y</em></span> axes labels are provided.</p><p class="calibre11">The <a id="id168" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>output looks as follows:</p><div class="mediaobject"><img src="Images/B04041_03_02.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">A casual look at this graph shows that the data is spread everywhere and no trends or patterns can be found in the first glance. However, with a keen eye, you can notice three points: one point at the top on the right-hand side and others to the immediate left of <span class="strong1"><strong class="calibre12">1960</strong></span> in the <span class="strong1"><em class="calibre15">x</em></span> axis. They are starkly different from all the other points in the sample, and hence, they are outliers.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note06" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">An outlier is an observation that lies outside the overall pattern of a distribution (Moore and McCabe 1999).</p></div></div><p class="calibre11">In order to<a id="id169" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> understand these points further, we will take the help of percentiles.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note07" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">If we have a vector V of length N, the qth percentile of V is the qth ranked value in a sorted copy of V. The values and distances of the two nearest neighbors as well as the <span class="strong1"><em class="calibre15">interpolation</em></span> parameter will determine the percentile if the normalized ranking does not match q exactly. This function is the same as the median if<code class="literal">q=50</code>, the same as the minimum if <code class="literal">q=0</code>, and the same as the maximum if <code class="literal">q=100</code>.</p><p class="calibre19">Refer<a id="id170" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html">http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html</a> for more information.</p></div></div><p class="calibre11">Why don't we use averages? We will look into averages in the summary statistics section; however, looking at the percentiles has its own advantages. Average values are typically skewed by outliers; outliers such as the one at the top on the right-hand side can drag the average to a higher value and the outliers near 1960 can do the opposite. Percentiles give us a better clarity about the range of values in our dataset. We can calculate the percentiles using NumPy.</p><p class="calibre11">In step 3, we will calculate the percentiles and print them.</p><p class="calibre11">The percentile values calculated and printed for this dataset are as follows:</p><div class="mediaobject"><img src="Images/B04041_03_12.jpg" alt="How it works…" class="calibre46"/></div><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note08" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Interpreting the percentiles:</p><p class="calibre19">25% of the points in the dataset are below 13.00 (25th percentile value).</p><p class="calibre19">50% of the points in the dataset are below 18.50 (50th percentile value).</p><p class="calibre19">75% of the points in the dataset are below 25.25 (75th percentile value).</p><p class="calibre19">A point to note is that the 50th percentile is the median. Percentiles give us a good idea of the range of our values.</p></div></div><p class="calibre11">In step 4, we will plot these percentile values as horizontal lines in our graph in order to enhance our visualization:</p><div class="calibre2"><pre class="programlisting"># Draw horizontal lines at 25,50 and 75th percentile
plt.axhline(perc_25,label='25th perc',c='r')
plt.axhline(perc_50,label='50th perc',c='g')
plt.axhline(perc_75,label='75th perc',c='m')
plt.legend(loc='best')</pre></div><p class="calibre11">We<a id="id171" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> used the <code class="literal">plt.axhline()</code> function to draw these horizontal lines. This function will draw a line at the given y value from the minimum of x to the maximum of x. Using the label parameter, we gave it a name and set the color of the line through the c parameter.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="tip05" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre19">A good way to understand any function is to pass the function name to <code class="literal">help()</code> in the Python console. In this case, help (plt.axhline) in the Python console will give you the details.</p></div></div><p class="calibre11">Finally, we will place the legend using <code class="literal">plt.legend()</code>, and using the <code class="literal">loc</code> parameter, ask pyplot to determine the best location to put the legend so that it does not affect the plot readability.</p><p class="calibre11">Our graph is now as follows:</p><div class="mediaobject"><img src="Images/B04041_03_03.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">In step 5, we <a id="id172" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will move on to remove the outliers using the mask function in NumPy:</p><div class="calibre2"><pre class="programlisting"># Remove zero values
y_masked = np.ma.masked_where(y==0,y)
#  Remove 54
y_masked = np.ma.masked_where(y_masked==54,y_masked)</pre></div><p class="calibre11">Masking is a convenient way to hide some of the values without removing them from our array. We used the <code class="literal">ma.masked_where</code> function, where we passed a condition and an array. The function then masks the values in the array that meet the condition. Our first condition was to mask all the points in the <code class="literal">y</code> array, where the array value was <code class="literal">0</code>. We stored the new masked array as <code class="literal">y_masked</code>. Then, we applied another condition on <code class="literal">y_masked</code> to remove point 54.</p><p class="calibre11">Finally, in step 6, we will repeat the plotting steps. Our final plot looks as follows:</p><div class="mediaobject"><img src="Images/B04041_03_04.jpg" alt="How it works…" class="calibre45"/></div></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec118" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Creating Anonymous functions </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Pre-processing columns </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Acquiring data with Python </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Outliers </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Data Analysis – Deep Dive">Chapter 4</a>,<span class="strong1"><em class="calibre15"> Analyzing Data - Deep Dive</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Grouping the data and using dot plots"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec39" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Grouping the data and using dot plots</h1></div></div></div><p class="calibre11">
<span class="strong1"><strong class="calibre12">EDA</strong></span> is about <a id="id173" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>zooming in and out of the data from multiple angles in order to get a better<a id="id174" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> grasp of the data. Let's now see the data from a different angle using dot plots. A dot plot is a simple plot where the data is grouped and plotted in a simple scale. It's up to us to decide how we want to group the data.</p><div class="note" title="Note"><div class="inner"><h3 class="title4"><a id="note09" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Dot plots are best used for small-sized to medium-sized datasets. For large-sized data, a histogram is usually used.</p></div></div><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec119" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">For this exercise, we will use the same data as the previous section.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec120" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the<a id="id175" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> necessary libraries. We will follow it up with the loading of our data <a id="id176" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and along the way, we will handle the missing values. Finally, we will group the data using a frequency counter:</p><div class="calibre2"><pre class="programlisting"># Load libraries
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
from collections import OrderedDict
from matplotlib.pylab import frange

# 1.Load the data and handle missing values.
fill_data = lambda x : int(x.strip() or 0)
data = np.genfromtxt('president.txt',dtype=(int,int),converters={1:fill_data},delimiter=",")
x = data[:,0]
y = data[:,1]

# 2.Group data using frequency (count of individual data points).
# Given a set of points, Counter() returns a dictionary, where key is a data point,
# and value is the frequency of data point in the dataset.
x_freq = Counter(y)
x_ = np.array(x_freq.keys())y_ = np.array(x_freq.values())</pre></div><p class="calibre11">We will proceed to group the data by the year range and plot it:</p><div class="calibre2"><pre class="programlisting"># 3.Group data by range of years
x_group = OrderedDict()
group= 5
group_count=1
keys = []
values = []
for i,xx in enumerate(x):
    # Individual data point is appended to list keys
    keys.append(xx)
    values.append(y[i])
    # If we have processed five data points (i.e. five years)
    if group_count == group:
        # Convert the list of keys to a tuple
        # use the new tuple as the ke to x_group dictionary
        x_group[tuple(keys)] = values
        keys= []
        values =[]
        group_count = 1
        
    group_count+=1
# Accomodate the last batch of keys and values
x_group[tuple(keys)] = values 

print x_group
# 4.Plot the grouped data as dot plot.
plt.subplot(311)
plt.title("Dot Plot by Frequency")
# Plot the frequency
plt.plot(y_,x_,'ro')
plt.xlabel('Count')
plt.ylabel('# Presedential Request')
# Set the min and max limits for x axis
plt.xlim(min(y_)-1,max(y_)+1)

plt.subplot(312)
plt.title("Simple dot plot")
plt.xlabel('# Presendtial Request')plt.ylabel('Frequency')</pre></div><p class="calibre11">Finally, we <a id="id177" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will prepare the data for a simple dot plot and proceed with<a id="id178" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> plotting it:</p><div class="calibre2"><pre class="programlisting"># Prepare the data for simple dot plot
# For every (item, frequency) pair create a 
# new x and y
# where x is a list, created using using np.repeat
# function, where the item is repeated frequency times.
# y is a list between 0.1 and frequency/10, incremented
# by 0.1
for key,value in x_freq.items():
    x__ = np.repeat(key,value)
    y__ = frange(0.1,(value/10.0),0.1)
    try:
        plt.plot(x__,y__,'go')
    except ValueError:
        print x__.shape, y__.shape
    # Set the min and max limits of x and y axis
    plt.ylim(0.0,0.4)
    plt.xlim(xmin=-1) 

plt.xticks(x_freq.keys())

plt.subplot(313)
x_vals =[]
x_labels =[]
y_vals =[]
x_tick = 1
for k,v in x_group.items():
    for i in range(len(k)):
        x_vals.append(x_tick)
        x_label = '-'.join([str(kk) if not i else str(kk)[-2:] for i,kk in enumerate(k)])
        x_labels.append(x_label)
    y_vals.extend(list(v))
    x_tick+=1

plt.title("Dot Plot by Year Grouping")
plt.xlabel('Year Group')
plt.ylabel('No Presedential Request')
try:
    plt.plot(x_vals,y_vals,'ro')
except ValueError:
    print len(x_vals),len(y_vals)
    
plt.xticks(x_vals,x_labels,rotation=-35)plt.show()</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec121" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we <a id="id179" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will load the data. This is the same as the data loading<a id="id180" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> discussed in the previous recipe. Before we start plotting the data, we want to group them in order to see the overall data characteristics.</p><p class="calibre11">In steps 2 and 3, we will group the data using different criteria.</p><p class="calibre11">Let's look at step 2.</p><p class="calibre11">Here, we will use a function called <code class="literal">Counter()</code> from the <code class="literal">collections</code> package.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note10" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Given a set of points, <code class="literal">Counter()</code> returns a dictionary where key is a data point and value is the frequency of the data points in the dataset.</p></div></div><p class="calibre11">We will pass our dataset to <code class="literal">Counter()</code> and extract the keys from the actual data point and values, the respective frequency from this dictionary into numpy arrays <code class="literal">x_</code> and <code class="literal">y_ </code>for ease of plotting. Thus, we have now grouped our data using frequency.</p><p class="calibre11">Before we <a id="id181" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>move on to plot this, we will perform another grouping with this data in step 3.</p><p class="calibre11">We know<a id="id182" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> that the x axis is years. Our data is also sorted by the year in an ascending order. In this step, we will group our data in a range of years, five in this case; that is, let's say that we will make a group from the first five years, our second group is the next five years, and so on:</p><div class="calibre2"><pre class="programlisting">group= 5
group_count=1
keys = []
values = []</pre></div><p class="calibre11">The <code class="literal">group</code> variable defines how many years we want in a single group; in this example, we have 5 groups and <code class="literal">keys</code> and <code class="literal">values</code> are two empty lists. We will proceed to fill them with values from <code class="literal">x</code> and <code class="literal">y</code> till <code class="literal">group_count</code> reaches <code class="literal">group</code>, that is, <code class="literal">5</code>:</p><div class="calibre2"><pre class="programlisting">for i,xx in enumerate(x):
keys.append(xx)
values.append(y[i])
if group_count == group:
x_group[tuple(keys)] = values
keys= []
values =[]
group_count = 0
    group_count+=1
x_group[tuple(keys)] = values </pre></div><p class="calibre11">The <code class="literal">x_group</code> is the name of the dictionary that now stores the group of values. We will need to preserve the order in which we will insert our records and so, we will use <code class="literal">OrderedDict</code> in this case.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note11" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19"><code class="literal">OrderedDict</code> preserves the order in which the keys are inserted.</p></div></div><p class="calibre11">Now let's proceed to plot these values.</p><p class="calibre11">We want to plot all our graphs in a single window; hence, we will use the <code class="literal">subplot</code> parameter to the subplot, which defines the number of rows (3, the number in the hundredth place), number of columns (1, the number in the tenth place), and finally the plot number (1 in the unit place). Our plot output is as follows:</p><div class="mediaobject"><img src="Images/B04041_03_05.jpg" alt="How it works…" class="calibre47"/></div><p class="calibre11">In the top <a id="id183" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>graph, the data is grouped by frequency. Here, our x axis is the<a id="id184" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> count and y axis is the number of Presidential Requests. We can see that 30 or more Presidential Requests have occurred only once. As said before, the dot plot is good at analyzing the range of the data points under different groupings.</p><p class="calibre11">The middle graph can be viewed as a very simple histogram. As the title of the graph (<code class="literal">in plt.title()</code>) says, it's the simplest form of a dot plot, where the <span class="strong1"><em class="calibre15">x</em></span> axis is the actual values and y axis is the number of times this x value occurs in the dataset. In a histogram, the bin size has to be set carefully; if not, it can distort the complete picture about the data. However, this can be avoided in this simple dot plot.</p><p class="calibre11">In the bottom graph, we have grouped the data by years.</p></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec122" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Creating Anonymous functions </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Pre-processing columns </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Acquiring data with Python </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Using Dictionary objects </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Using scatter plots for multivariate data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec40" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using scatter plots for multivariate data</h1></div></div></div><p class="calibre11">From a single<a id="id185" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> column, we will now move on to multiple <a id="id186" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>columns. In multivariate data analysis, we are interested in seeing if there any relationships between the columns that we are analyzing. In two column/variable cases, the best place to start is a standard scatter plot. There can be four types of relationships, as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">No relationship</li><li class="listitem">Strong</li><li class="listitem">Simple</li><li class="listitem">Multivariate (not simple) relationship</li></ul></div><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec123" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">We will use the Iris dataset. It's a multivariate dataset introduced by Sir Ronald Fisher. Refer to <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a> for more information.</p><p class="calibre11">The Iris dataset has 150 instances and four attributes/columns. The 150 instances are composed of 50 records from each of the three species of the Iris flower (Setosa, virginica, and versicolor). The four attributes are the sepal length in cm, sepal width in cm, petal length in cm, and petal width in cm. Thus, the Iris dataset also serves as a great classification dataset. A classification method can be written in such a way that, given a record, we can classify which species that record belongs to after appropriate training.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec124" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the necessary libraries and extract the Iris data:</p><div class="calibre2"><pre class="programlisting"># Load Librarires
from sklearn.datasets import load_iris
import numpy as np
import matplotlib.pyplot as plt
import itertools

# 1. Load Iris dataset
data = load_iris()
x = data['data']
y = data['target']col_names = data['feature_names']</pre></div><p class="calibre11">We will proceed with demonstrating with a scatter plot:</p><div class="calibre2"><pre class="programlisting"># 2.Perform a simple scatter plot. 
# Plot 6 graphs, combinations of our columns, sepal length, sepal width,
# petal length and petal width.
plt.close('all')
plt.figure(1)
# We want a plot with
# 3 rows and 2 columns, 3 and 2 in
# below variable signifies that.
subplot_start = 321
col_numbers = range(0,4)
# Need it for labeling the graph
col_pairs = itertools.combinations(col_numbers,2)
plt.subplots_adjust(wspace = 0.5)

for col_pair in col_pairs:
    plt.subplot(subplot_start)
    plt.scatter(x[:,col_pair[0]],x[:,col_pair[1]],c=y)
    plt.xlabel(col_names[col_pair[0]])
    plt.ylabel(col_names[col_pair[1]])
    subplot_start+=1plt.show()</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec125" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">The scikit library<a id="id187" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> provides a convenient function to <a id="id188" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>load the Iris dataset called <code class="literal">load_iris()</code>. We will use this to load the Iris data in the variable data in step 1. The <code class="literal">data</code> is a dictionary object. Using the data and target keys, we will retrieve the records and class labels. We will look at the <code class="literal">x</code> and <code class="literal">y</code> values:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; x.shape
(150, 4)
&gt;&gt;&gt; y.shape
(150,)
&gt;&gt;&gt;</pre></div><p class="calibre11">As you can see, <code class="literal">x</code> is a matrix with <code class="literal">150</code> rows and four columns; <code class="literal">y</code> is a vector of length <code class="literal">150</code>. The <code class="literal">data</code> dictionary can also be queried to view the column names using the <code class="literal">feature_names</code> keyword, as follows:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; data['feature_names']

['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
&gt;&gt;&gt;</pre></div><p class="calibre11">We will then create a scatter plot of the iris variables in step 2. As we did before, we will use subplot here to accommodate all the plots in a single figure. We will get two combinations of our column using <code class="literal">itertools.Combination</code>:</p><div class="calibre2"><pre class="programlisting">col_pairs = itertools.combinations(col_numbers,2)</pre></div><p class="calibre11">We can iterate <code class="literal">col_pairs</code> to get two combinations of our column and plot a scatter plot for each, as you can see in the following line of code:</p><div class="calibre2"><pre class="programlisting">plt.scatter(x[:,col_pair[0]],x[:,col_pair[1]],c=y)</pre></div><p class="calibre11">We will <a id="id189" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>pass a <code class="literal">c</code> parameter in order to indicate the<a id="id190" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> color of the points. In this case, we will pass our y variable (class label) so that the different species of iris are plotted in different colors in our scatter plot.</p><p class="calibre11">The resulting plot is as follows:</p><div class="mediaobject"><img src="Images/B04041_03_06.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">As you can see, we have plotted two combinations of our columns. We also have the class labels represented using three different colors. Let's look at the bottom left plot, petal length<a id="id191" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> versus petal width. We see that<a id="id192" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> different range of values belong to different class labels. Now, this gives us a great clue for classification; the petal width and length variables are good candidates if the problem in hand is classification.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note12" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">For the Iris dataset, the petal width and length can alone classify the records in their respective flower family.</p></div></div><p class="calibre11">These kinds of observations can be quickly made during the feature selection process with the help of bivariate scatter plots.</p></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec126" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Using iterables </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Working with itertools </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Using heat maps"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec41" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using heat maps</h1></div></div></div><p class="calibre11">Heat maps are another interesting visualization technique. In a heat map, the data is represented as a <a id="id193" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix where the range of values taken by attributes are represented as color gradients. Look at the following Wikipedia reference for a general <a id="id194" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>introduction to heat maps:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://en.wikipedia.org/wiki/Heat_map">http://en.wikipedia.org/wiki/Heat_map</a>
</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec127" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">We will again resort to the Iris dataset in order to demonstrate how to build a heat map. We will also see the various ways that heat maps can be used on this data.</p><p class="calibre11">In this recipe, we will see how we can represent the whole data as a heat map and how the various interpretations of the data can be made from the heat map. Let's proceed to build a heat map of the Iris dataset.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec128" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the necessary libraries and import the Iris dataset. We will proceed with scaling the variables in the data by their mean value:</p><div class="calibre2"><pre class="programlisting"># Load libraries
from sklearn.datasets import load_iris
from sklearn.preprocessing import scale
import numpy as np
import matplotlib.pyplot as plt

# 1. Load iris dataset
data = load_iris()
x = data['data']
y = data['target']
col_names = data['feature_names']

# 2. Scale the variables, with mean value
x = scale(x,with_std=False)
x_ = x[1:26,]y_labels = range(1,26)</pre></div><p class="calibre11">Let's plot our heat map:</p><div class="calibre2"><pre class="programlisting"># 3. Plot the Heat map
plt.close('all')

plt.figure(1)
fig,ax = plt.subplots()
ax.pcolor(x_,cmap=plt.cm.Greens,edgecolors='k')
ax.set_xticks(np.arange(0,x_.shape[1])+0.5)
ax.set_yticks(np.arange(0,x_.shape[0])+0.5)
ax.xaxis.tick_top()
ax.yaxis.tick_left()
ax.set_xticklabels(col_names,minor=False,fontsize=10)
ax.set_yticklabels(y_labels,minor=False,fontsize=10)plt.show()</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec129" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we will<a id="id195" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> load the Iris dataset. Similar to the other recipes, we will take the data dictionary objects and store them as x and y for clarity. In step 2, we will scale the variables by their means:</p><div class="calibre2"><pre class="programlisting">x = scale(x,with_std=False)</pre></div><p class="calibre11">With the parameter standard set to false, the scale function will use only the mean of the columns in order to normalize the data.</p><p class="calibre11">The reason for the scaling is to adjust the range of values that each column takes to a common scale, typically between 0 and 1. Having them in the same scale is very important for the heat map visualization as the values decide the color gradients.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="tip06" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre19">Don't forget to scale your variables to bring them to the same range. Not having a proper scaling may lead to variables with a bigger range and scale, thus dominating others.</p></div></div><p class="calibre11">In step 3, we will perform the actual plotting. Before we plot, we will subset the data:</p><div class="calibre2"><pre class="programlisting">x = x[1:26,]
col_names = data['feature_names']
y_labels = range(1,26)</pre></div><p class="calibre11">As you can see, we<a id="id196" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> selected only the first 25 records from the dataset. We did so in order to have the labels in the y axis to be readable. We will store the labels for the x and y axes in <code class="literal">col_names</code> and <code class="literal">y_labels</code>, respectively. Finally, we will use the <code class="literal">pcolor</code> function from pyplot to plot a heat map of the Iris data. We will do a little more tinkering with pcolor to make it look nice:</p><div class="calibre2"><pre class="programlisting">ax.set_xticks(np.arange(0,x.shape[1])+0.5)
ax.set_yticks(np.arange(0,x.shape[0])+0.5)</pre></div><p class="calibre11">The <span class="strong1"><em class="calibre15">x</em></span> and <span class="strong1"><em class="calibre15">y</em></span> axis ticks are set uniformly:</p><div class="calibre2"><pre class="programlisting">ax.xaxis.tick_top()</pre></div><p class="calibre11">The x axis ticks are displayed at the top of the graph:</p><div class="calibre2"><pre class="programlisting">ax.yaxis.tick_left()</pre></div><p class="calibre11">The y axis ticks are displayed to the left:</p><div class="calibre2"><pre class="programlisting">ax.set_xticklabels(col_names,minor=False,fontsize=10)
ax.set_yticklabels(y_labels,minor=False,fontsize=10)</pre></div><p class="calibre11">Finally, we will pass on the label values.</p><p class="calibre11">The output plot is shown as follows:</p><div class="mediaobject"><img src="Images/B04041_03_07.jpg" alt="How it works…" class="calibre45"/></div></div><div class="calibre2" title="There's more..."><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec130" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more...</h2></div></div></div><p class="calibre11">Another<a id="id197" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> interesting way to use a heat map is to view the variables separated by their respective classes; for example, in the Iris dataset, we will plot three different heat maps for the three classes that are present. The code is as follows:</p><div class="calibre2"><pre class="programlisting">x1 = x[0:50]
x2 = x[50:99]
x3 = x[100:149]


x1 = scale(x1,with_std=False)
x2 = scale(x2,with_std=False)
x3 = scale(x3,with_std=False)

plt.close('all')
plt.figure(2)
fig,(ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)
y_labels = range(1,51)


ax1.set_xticks(np.arange(0,x.shape[1])+0.5)
ax1.set_yticks(np.arange(0,50,10))

ax1.xaxis.tick_bottom()
ax1.set_xticklabels(col_names,minor=False,fontsize=2)

ax1.pcolor(x1,cmap=plt.cm.Greens,edgecolors='k')
ax1.set_title(data['target_names'][0])

ax2.pcolor(x2,cmap=plt.cm.Greens,edgecolors='k')
ax2.set_title(data['target_names'][1])

ax3.pcolor(x3,cmap=plt.cm.Greens,edgecolors='k')
ax3.set_title(data['target_names'][2])plt.show()   </pre></div><p class="calibre11">Let's look at the plot:</p><div class="mediaobject"><img src="Images/B04041_03_08.jpg" alt="There's more..." class="calibre45"/></div><p class="calibre11">The first 50 <a id="id198" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>records belong to the <code class="literal">setosa</code> class, the next 50 to <code class="literal">versicolor</code>, and the last 50 belong to <code class="literal">virginica</code>. We will make three heat maps for each of these classes.</p><p class="calibre11">The cells are filled with the actual values of the records. You can notice that, for <code class="literal">setosa</code>, the sepal width has a good variation but doesn't show any significance in the case of <code class="literal">versicolor</code> and <code class="literal">virginica</code>.</p></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec131" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Scaling the data </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <span class="strong1"><em class="calibre15">Analyzing Data - Explore &amp; Wrangle</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Performing summary statistics and plots"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec42" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Performing summary statistics and plots</h1></div></div></div><p class="calibre11">The primary<a id="id199" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> purpose of using summary statistics is to get a good <a id="id200" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>understanding of the location and dispersion of the data. By summary statistics, we refer to mean, median, and standard deviation. These quantities are quite easy to calculate. However, one should be careful when using them. If the underlying data is not unimodal, that is, it has multiple peaks, these quantities may not be of much use.</p><div class="note" title="Note"><div class="inner"><h3 class="title4"><a id="note13" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">If the given data is unimodal, that is, having only one peak, the mean, which gives the location, and standard deviation, which gives the variance, are valuable metrics.</p></div></div><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec132" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Let's use our Iris <a id="id201" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>dataset to explore some of these summary<a id="id202" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> statistics. In this section, we don't have a wholesome program producing a single output; however, we will have different steps demonstrating different summary measures.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec133" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's begin by importing the necessary libraries. We will follow it up with the loading of the Iris dataset:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
from sklearn.datasets import load_iris
import numpy as np
from scipy.stats import trim_mean

# Load iris data
data = load_iris()
x = data['data']
y = data['target']col_names = data['feature_names']</pre></div><p class="calibre11">Let's now demonstrate how to calculate the mean, trimmed mean, and range values:</p><div class="calibre2"><pre class="programlisting"># 1.	Calculate and print the mean value of each column in the Iris dataset
print "col name,mean value"
for i,col_name in enumerate(col_names):
    print "%s,%0.2f"%(col_name,np.mean(x[:,i]))
print    

# 2.	Trimmed mean calculation.
p = 0.1 # 10% trimmed mean
print
print "col name,trimmed mean value"
for i,col_name in enumerate(col_names):
    print "%s,%0.2f"%(col_name,trim_mean(x[:,i],p))
print

# 3.	Data dispersion, calculating and display the range values.
print "col_names,max,min,range"
for i,col_name in enumerate(col_names):
    print "%s,%0.2f,%0.2f,%0.2f"%(col_name,max(x[:,i]),min(x[:,i]),max(x[:,i])-min(x[:,i]))
print</pre></div><p class="calibre11">Finally, we will <a id="id203" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>show the variance, standard deviation, mean <a id="id204" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>absolute deviation, and median absolute deviation calculations:</p><div class="calibre2"><pre class="programlisting"># 4.	Data dispersion, variance and standard deviation
print "col_names,variance,std-dev"
for i,col_name in enumerate(col_names):
    print "%s,%0.2f,%0.2f"%(col_name,np.var(x[:,i]),np.std(x[:,i]))
print
    
# 5.	Mean absolute deviation calculation  
def mad(x,axis=None):
    mean = np.mean(x,axis=axis)
    return np.sum(np.abs(x-mean))/(1.0 * len(x))
        
print "col_names,mad"
for i,col_name in enumerate(col_names):
    print "%s,%0.2f"%(col_name,mad(x[:,i]))
print

# 6.	Median absolute deviation calculation
def mdad(x,axis=None):
    median = np.median(x,axis=axis)
    return np.median(np.abs(x-median))
       
print "col_names,median,median abs dev,inter quartile range"
for i,col_name in enumerate(col_names):
    iqr = np.percentile(x[:,i],75) - np.percentile(x[i,:],25)
    print "%s,%0.2f,%0.2f,%0.2f"%(col_name,np.median(x[:,i]), mdad(x[:,i]),iqr)
print</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec134" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">The loading of the Iris dataset is not repeated in this recipe. It's assumed that the reader can look at the <a id="id205" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>previous recipe to do the same. Further, we will<a id="id206" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> assume that the x variable is loaded with all the instance of the Iris records with each record having four columns.</p><p class="calibre11">Step 1 prints the mean value of each of the column in the Iris dataset. We used NumPy's <code class="literal">mean</code> function for the same. The output of the print statement is as follows:</p><div class="mediaobject"><img src="Images/B04041_03_13.jpg" alt="How it works…" class="calibre48"/></div><p class="calibre11">As you can see, we have the mean value for each column. The code to calculate the mean is as follows:</p><div class="calibre2"><pre class="programlisting">np.mean(x[:,i])</pre></div><p class="calibre11">We passed all the rows and columns in the loop. Thus, we get the mean value by columns.</p><p class="calibre11">Another interesting measure is what is called trimmed mean. It has its own advantages. The 10% trimmed mean of a given sample is computed by excluding the 10% largest and 10% smallest values from the sample and taking the arithmetic mean of the remaining 80% of the sample.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note14" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Compared to the regular mean, a trimmed mean is less sensitive to outliers.</p></div></div><p class="calibre11">SciPy provides us with a trim mean function. We will demonstrate the trimmed mean calculation in step 2. The output is as follows:</p><div class="mediaobject"><img src="Images/B04041_03_14.jpg" alt="How it works…" class="calibre49"/></div><p class="calibre11">With the Iris dataset, we don't see a lot of difference, but in real-world datasets, the trimmed mean is very handy as it gives a better picture of the location of the data.</p><p class="calibre11">Till now, what <a id="id207" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we saw was the location of the data and that the<a id="id208" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> mean and trimmed mean gives a good inference on the data location. Another important aspect to look at is the dispersion of the data. The simplest way to look at the data dispersion is range, which is defined as follows, given a set of values, x, the range is the maximum value of x – minimum value of x. In Step 3, we will calculate and print the same:</p><div class="mediaobject"><img src="Images/B04041_03_15.jpg" alt="How it works…" class="calibre50"/></div><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note15" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">If the data falls in a very narrow range, say, most of the values cluster around a single value and we have a few extreme values, then the range may be misleading.</p></div></div><p class="calibre11">When the data falls in a very narrow range and clusters around a single value, variance is used as a typical measure of the dispersion/spread of the data. Variance is the sum of the squared difference between the individual values and the mean value divided by the number of instances. In step 4, we will see the variance calculation.</p><p class="calibre11">In the preceding code, in addition to variance, we can see std-dev, that is, standard deviation. As variance is the square of the difference, it's not in the same measurement scale as the original data. We will use standard deviation, which is the square root of the variance, in order to get the data back into its original scale. Let's look at the output of the print statement, where we listed both the variance and standard deviation:</p><div class="mediaobject"><img src="Images/B04041_03_16.jpg" alt="How it works…" class="calibre49"/></div><p class="calibre11">As we mentioned earlier, the mean is very sensitive to outliers; variance also uses the mean, and hence, it's prone to the same issues as the mean. We can use other measures for variance to avoid this trap. One such measure is absolute average deviation; instead of taking the square <a id="id209" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the difference between the individual values<a id="id210" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> and mean and dividing it by the number of instances, we will take the absolute of the difference between the mean and individual values and divide it by the number of instances. In step 5, we will define a function for this:</p><div class="calibre2"><pre class="programlisting">def mad(x,axis=None):
mean = np.mean(x,axis=axis)
return np.sum(np.abs(x-mean))/(1.0 * len(x))</pre></div><p class="calibre11">As you can see, the function returns the absolute difference between the mean and individual values. The output of this step is as follows:</p><div class="mediaobject"><img src="Images/B04041_03_17.jpg" alt="How it works…" class="calibre51"/></div><p class="calibre11">With the data having many outliers, there is another set of metrics that come in handy. They are the median and percentiles. We already saw percentiles in the previous section while plotting the univariate data. Traditionally, median is defined as a value from the dataset such that half of all the points in the dataset are smaller and the other half is larger than the median value.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note16" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Percentiles are a generalization of the concept of median. The 50th percentile is the traditional median value.</p></div></div><p class="calibre11">We saw the 25th and 75th percentiles in the previous section. The 25th percentile is a value such that 25% of all the points in the dataset are smaller than this value:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; 
&gt;&gt;&gt; a = [8,9,10,11]
&gt;&gt;&gt; np.median(a)
9.5
&gt;&gt;&gt; np.percentile(a,50)
9.5</pre></div><p class="calibre11">The median is the measure of the location of the data distribution. Using percentiles, we can get a metric for the dispersion of the data, the interquartile range. The interquartile range is the distance between the 75th percentile and 25th percentile. Similar to the mean absolute deviation as explained previously, we also have the median absolute deviation.</p><p class="calibre11">In step 6, we will <a id="id211" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculate and display both the interquartile range <a id="id212" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and median absolute deviation. We will define the following function in order to calculate the median absolute deviation:</p><div class="calibre2"><pre class="programlisting">def mdad(x,axis=None):
median = np.median(x,axis=axis)
return np.median(np.abs(x-median))</pre></div><p class="calibre11">The output is as follows:</p><div class="mediaobject"><img src="Images/B04041_03_18.jpg" alt="How it works…" class="calibre52"/></div></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec135" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Grouping Data and Using Plots </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <span class="strong1"><em class="calibre15">Analyzing Data - Explore &amp; Wrangle</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Using a box-and-whisker plot"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec43" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using a box-and-whisker plot</h1></div></div></div><p class="calibre11">A box-and-whisker plot is<a id="id213" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> a good companion with the summary statistics to view the statistical summary of the data in hand. Box-and-whiskers can effectively represent quantiles in data and also outliers, if any, emphasizing the overall structure of the data. A box plot consists of the following features:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">A horizontal line indicating the median that indicates the location of the data</li><li class="listitem">A box spanning the interquartile range, measuring the dispersion</li><li class="listitem">A set of whiskers that extends from the central box horizontally and vertically, which indicates the tail of the distribution</li></ul></div><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec136" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Let's use the box plot to look at the Iris dataset.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec137" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the<a id="id214" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> necessary libraries to begin with. We will follow this with loading the Iris dataset:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Load Iris dataset
data = load_iris()
x = data['data']
plt.close('all')</pre></div><p class="calibre11">Let's demonstrate how to create a box-and-whisker plot:</p><div class="calibre2"><pre class="programlisting"># Plot the box and whisker
fig = plt.figure(1)
ax = fig.add_subplot(111)
ax.boxplot(x)
ax.set_xticklabels(data['feature_names'])
plt.show()    </pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec138" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">The code is very straightforward. We will load the Iris data in x and pass the x values to the box plot function from pyplot. As you know, our x has four columns. The box plot is as follows:</p><div class="mediaobject"><img src="Images/B04041_03_09.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">The box plot has <a id="id215" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>captured both the location and variation of all the four columns in a single plot.</p><p class="calibre11">The horizontal red line indicates the median, which is the location of the data. You can see that the sepal length has a higher median than the rest of the columns.</p><p class="calibre11">The box spanning the interquartile range measuring the dispersion can be seen for all the four variables.</p><p class="calibre11">You can see a set of whiskers that extends from the central box horizontally and vertically, which indicates the tail of the distribution. Whiskers help you to see the extreme values in the datasets.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec139" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">It will also be interesting to see how the data is distributed across the various class labels. Similar to how we did in the scatter plots, let's do the same with the box-and-whisker plot. The following code and chart explains how to plot a box plot across various class labels:</p><div class="calibre2"><pre class="programlisting">y=data['target']
class_labels = data['target_names']

fig = plt.figure(2,figsize=(18,10))
sub_plt_count = 321
for t in range(0,3):
    ax = fig.add_subplot(sub_plt_count)
    y_index = np.where(y==t)[0]
    x_ = x[y_index,:]
    ax.boxplot(x_)
    ax.set_title(class_labels[t])   
    ax.set_xticklabels(data['feature_names'])
    sub_plt_count+=1
plt.show()</pre></div><p class="calibre11">As you can <a id="id216" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see in the following chart, we now have a box-and-whisker plot for each class label:</p><div class="mediaobject"><img src="Images/B04041_03_10.jpg" alt="There's more…" class="calibre53"/></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Imputing the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec44" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Imputing the data</h1></div></div></div><p class="calibre11">In many real-world scenarios, we have the problem of incomplete or missing data. We need a strategy<a id="id217" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to handle the incomplete data. This strategy can be formulated either using the data alone or in conjunction with the class labels, if the labels are present.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec140" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Let's first look at the ways of imputing the data without using the class labels.</p><p class="calibre11">A simple technique is to ignore the missing value and hence, avoid the overhead of data imputation. However, this can be applied when the data is available in abundance, which is not always the case. If the dataset has very few missing values and the percentage of the missing values is minimal, we can ignore them. Typically, it's not about ignoring a single value of a <a id="id218" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>variable, it's about ignoring a tuple that contains this variable. We have to be more careful when ignoring a whole tuple, as the other attributes in this tuple may be very critical for our task.</p><p class="calibre11">A better way to handle the missing data is to estimate it. Now, the estimation process can be carried out considering only the data or in conjunction with the class label. In the case of a continuous variable, the mean, median, or the most frequent value can be used to replace the missing value. Scikit-learn provides you with an <code class="literal">Imputer()</code> function in module preprocessing to handle the missing data. Let's see an example where we will perform data imputation. To better understand the imputation technique, we will artificially introduce some missing values in the Iris dataset.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec141" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the necessary libraries to begin with. We will load the Iris dataset as usual and introduce some arbitrary missing values:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
from sklearn.datasets import load_iris
from sklearn.preprocessing import Imputer
import numpy as np
import numpy.ma as ma

# 1. Load Iris Data Set
data = load_iris()
x = data['data']
y = data['target']

# Make a copy of hte original x value
x_t = x.copy()

# 2.	Introduce missing values into second row
x_t[2,:] = np.repeat(0,x.shape[1])</pre></div><p class="calibre11">Let's see some data imputation in action:</p><div class="calibre2"><pre class="programlisting"># 3.	Now create an imputer object with strategy as mean, 
# i.e. fill the missing values with the mean value of the missing column.
imputer = Imputer(missing_values=0,strategy="mean")
x_imputed = imputer.fit_transform(x_t)


mask = np.zeros_like(x_t)
mask[2,:] = 1
x_t_m = ma.masked_array(x_t,mask)

print np.mean(x_t_m,axis=0)print x_imputed[2,:]</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec142" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Step 1 is <a id="id219" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>about loading the Iris data in memory. In step 2, we will introduce some missing values; in this case, we will set all the columns in the third row to <code class="literal">0</code>.</p><p class="calibre11">In step 3, we will use the Imputer object to handle the missing data:</p><div class="calibre2"><pre class="programlisting">imputer = Imputer(missing_values=0,strategy="mean")</pre></div><p class="calibre11">As you can see, we will need two parameters, <code class="literal">missing_valu</code>es to specify the missing values, and strategy, which is a way to impute these missing values. The Imputer object provides the following three strategies:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">mean</li><li class="listitem">median</li><li class="listitem">most_frequent</li></ul></div><p class="calibre11">Using the mean, any cell with the <code class="literal">0</code> value will be replaced by the mean value of the column that the cell belongs to. In the case of the median, the median value is used to replace <code class="literal">0</code>, and in <code class="literal">most_frequent</code>, as the name suggests, the most frequent value is used to replace <code class="literal">0</code>. Based on the context of our application, one of these strategies can be applied.</p><p class="calibre11">The intial value of x[2,:] is as follows:</p><div class="calibre2"><pre class="programlisting">
<span class="strong1"><strong class="calibre12">&gt;&gt;&gt; x[2,:]</strong></span>
<span class="strong1"><strong class="calibre12">array([ 4.7,  3.2,  1.3,  0.2])</strong></span>
</pre></div><p class="calibre11">We will make it <code class="literal">0</code> in all the columns and use an imputer with the mean strategy.</p><p class="calibre11">Before we look at the imputer output, let's calculate the mean values for all the columns:</p><div class="calibre2"><pre class="programlisting">import numpy.ma as ma
mask = np.zeros_like(x_t)
mask[2,:] = 1
x_t_m = ma.masked_array(x_t,mask)


print np.mean(x_t_m,axis=0)</pre></div><p class="calibre11">The output is as follows:</p><div class="calibre2"><pre class="programlisting">[5.851006711409397 3.053020134228189 3.7751677852349017 1.2053691275167793]</pre></div><p class="calibre11">Now, let's look at the imputed output for row number 2:</p><div class="calibre2"><pre class="programlisting">print x_imputed[2,:]</pre></div><p class="calibre11">The following is the output:</p><div class="calibre2"><pre class="programlisting">[ 5.85100671  3.05302013  3.77516779  1.20536913]</pre></div><p class="calibre11">As you can <a id="id220" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see, the imputer has filled the missing values with the mean value of the respective columns.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec143" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">As we discussed, we can also leverage the class labels and impute the missing values either using the mean or median:</p><div class="calibre2"><pre class="programlisting"># Impute based on class label
missing_y = y[2]
x_missing = np.where(y==missing_y)[0]
y = data['target']
# Mean stragegy 
print np.mean(x[x_missing,:],axis=0)
# Median stragegy
print np.median(x[x_missing,:],axis=0)</pre></div><p class="calibre11">Instead of using the mean or median of the whole dataset, what we did was to subset the data by the class variable of the missing tuple:</p><div class="calibre2"><pre class="programlisting">missing_y = y[2]</pre></div><p class="calibre11">We introduced the missing value in the third record. We will take the class label associated with this record to the <code class="literal">missing_y</code> variable:</p><div class="calibre2"><pre class="programlisting">x_missing = np.where(y==missing_y)[0]</pre></div><p class="calibre11">Now, we will take all the tuples that have the same class label:</p><div class="calibre2"><pre class="programlisting"># Mean stragegy 
print np.mean(x[x_missing,:],axis=0)
# Median stragegy
print np.median(x[x_missing,:],axis=0)</pre></div><p class="calibre11">We can now apply the mean or median strategy by replacing the missing tuple with the mean or median of all the tuples that belong to this class label.</p><p class="calibre11">We took the mean/median value of this subset for the data imputation process.</p></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec144" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Performing Summary Statistics </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <span class="strong1"><em class="calibre15">Analyzing Data - Explore &amp; Wrangle</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Performing random sampling"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec45" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Performing random sampling</h1></div></div></div><p class="calibre11">In this we will learn to how to perform a random sampling of data.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec145" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Typically, in<a id="id221" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> scenarios where it's very expensive to access the whole dataset, sampling can be effectively used to extract a portion of the dataset for analysis. Sampling can be effectively used in EDA as well. A sample should be a good representative of the underlying dataset. It should have approximately the same characteristics as the underlying dataset. For example, with respect to the mean, the sample mean should be as close to the original data's mean value as possible. There are several sampling techniques; we will cover one of them here.</p><p class="calibre11">In simple random sampling, there is an equal chance of selecting any tuple. For our example, we want to sample ten records randomly from the Iris dataset.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec146" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">We will begin with loading the necessary libraries and importing the Iris dataset:</p><div class="calibre2"><pre class="programlisting"># Load libraries
from sklearn.datasets import load_iris
import numpy as np

# 1.	Load the Iris data set
data = load_iris()
x = data['data']</pre></div><p class="calibre11">Let's demonstrate how sampling is performed:</p><div class="calibre2"><pre class="programlisting"># 2.	Randomly sample 10 records from the loaded dataset
no_records = 10
x_sample_indx = np.random.choice(range(x.shape[0]),no_records)
print x[x_sample_indx,:]</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec147" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we will load the Iris dataset. In step 2, we will do a random selection using the <code class="literal">choice</code> function from <code class="literal">numpy.random</code>.</p><p class="calibre11">The two parameters that we will pass to the choice functions are a range variable for the total number of rows in the original dataset and the sample size that we require. From zero to the total number of rows in the original dataset, the choice function randomly picks n integers, where n is the size of the sample, which is dictated by <code class="literal">no_records</code> in our case.</p><p class="calibre11">Another important aspect is that one of the parameters to the choice function is <code class="literal">replace</code> and it's set <a id="id222" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to True by default; it specifies whether we need to sample with replacement or without replacement. Sampling without replacement removes the sampled item from the original list so it will not be a candidate for future sampling. Sampling with replacement does the opposite; every element has an equal chance to be sampled in future sampling even though it's been sampled before.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec148" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><div class="calibre2" title="Stratified sampling"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title6"><a id="ch03lvl3sec01" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Stratified sampling</h3></div></div></div><p class="calibre11">If the underlying <a id="id223" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>dataset consists of different groups, a simple random sampling may fail to capture adequate samples in order to be able to represent the data. For example, in a two-class classification problem, 10% of the data belongs to the positive class and 90% belongs to the negative class. This kind of problem is called class imbalance problem in machine learning. When we do sampling on such imbalanced datasets, the sample should also reflect the preceding percentages. This kind of sampling is called stratified sampling. We will look more into stratified sampling in future chapters on machine learning.</p></div><div class="calibre2" title="Progressive sampling"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title6"><a id="ch03lvl3sec02" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Progressive sampling</h3></div></div></div><p class="calibre11">How do we<a id="id224" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> determine the correct sample size that we need for a given problem? We discussed several sampling techniques before but we don't have a strategy to select the correct sample size. There is no simple answer for this. One way to do this is to use progressive sampling. Select a sample size and get the samples through any of the sampling techniques, apply the desired operation on the data, and record the results. Now, increase the sample size and repeat the steps. This iterative process is called progressive sampling.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Scaling the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec46" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Scaling the data</h1></div></div></div><p class="calibre11">In this we will learn to how to scale the data.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec149" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Scaling is an important type of data transformation. Typically, by doing scaling on a dataset, we can <a id="id225" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>control the range of values that the data type can assume. In a dataset with multiple columns, the columns with a bigger range and scale tend to dominate other columns. We will perform scaling of the dataset in order to avoid these interferences.</p><p class="calibre11">Let's say that we are comparing two software products based on the number of features and the number of lines of code. The difference in the number of lines of code will be very high compared to the difference in the number of features. In this case, our comparison will be dominated by the number of lines of code. If we use any similarity measure, the similarity or difference will be dominated by the number of lines of code. To avoid such a situation, we will adopt scaling. The simplest scaling is min-max scaling. Let's look at min-max scaling on a randomly generated dataset.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec150" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's generate some random data in order to test our scaling functionality:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
import numpy as np

# 1.	Generate some random data for scaling
np.random.seed(10)
x = [np.random.randint(10,25)*1.0 for i in range(10)]</pre></div><p class="calibre11">Now, we will demonstrate scaling:</p><div class="calibre2"><pre class="programlisting"># 2.Define a function, which can perform min max scaling given a list of numbers
def min_max(x):
    return [round((xx-min(x))/(1.0*(max(x)-min(x))),2) for xx in x]

# 3.Perform scaling on the given input list.    
print x 
print min_max(x)    </pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec151" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we will generate a list of random numbers between 10 and 25. In step 2, we will define a function to perform min-max scaling on the given input. Min-max scaling is defined as follows:</p><div class="calibre2"><pre class="programlisting">x_scaled = x – min(x) / max(x) –min (x)</pre></div><p class="calibre11">In step 2 we define a function to do the above task.</p><p class="calibre11">This transforms the range of the given value. After transformation, the values will fall in the [ 0,1 ] range.</p><p class="calibre11">In step 3, we will<a id="id226" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> first print the original input list. The output is as follows:</p><div class="calibre2"><pre class="programlisting">[19, 23, 14, 10, 11, 21, 22, 19, 23, 10]</pre></div><p class="calibre11">We will pass this list to our <code class="literal">min_max</code> function in order to get the scaled output, which is as follows:</p><div class="calibre2"><pre class="programlisting">[0.69, 1.0, 0.31, 0.0, 0.08, 0.85, 0.92, 0.69, 1.0, 0.0]</pre></div><p class="calibre11">You can see the scaling in action; <code class="literal">10</code>, which is the smallest number, has been assigned a value of <code class="literal">0.0</code> and <code class="literal">23</code>, the highest number, is assigned a value of <code class="literal">1.0</code>. Thus, we scaled the data in the [0,1] range.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec152" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">Scikit-learn provides a MinMaxScaler function for the same:</p><div class="calibre2"><pre class="programlisting">from sklearn.preprocessing import MinMaxScaler
import numpy as np

np.random.seed(10)
x = np.matrix([np.random.randint(10,25)*1.0 for i in range(10)])
x = x.T
minmax = MinMaxScaler(feature_range=(0.0,1.0))
print x
x_t = minmax.fit_transform(x)
print x_t</pre></div><p class="calibre11">The output is as follows:</p><div class="calibre2"><pre class="programlisting">[19.0, 23.0, 14.0, 10.0, 11.0, 21.0, 22.0, 19.0, 23.0, 10.0]
[0.69, 1.0, 0.31, 0.0, 0.08, 0.85, 0.92, 0.69, 1.0, 0.0]</pre></div><p class="calibre11">We saw examples where we scaled the data to a range (0,1); this can be extended to any range. Let's say that our new range is <code class="literal">nr_min,nr_max</code>, then the min-max formula is modified as follows:</p><div class="calibre2"><pre class="programlisting">x_scaled =  ( x – min(x) / max(x) –min (x) ) * (nr_max- nr_min) + nr_min</pre></div><p class="calibre11">The following will be the Python code:</p><div class="calibre2"><pre class="programlisting">import numpy as np

np.random.seed(10)
x = [np.random.randint(10,25)*1.0 for i in range(10)]

def min_max_range(x,range_values):
    return [round( ((xx-min(x))/(1.0*(max(x)-min(x))))*(range_values[1]-range_values[0]) \
    + range_values[0],2) for xx in x]

print min_max_range(x,(100,200))</pre></div><p class="calibre11">where, range_values is<a id="id227" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> a tuple of two elements, where the 0th element is the new range's lower end and the first element is the higher end. Let's invoke this function on our input and see how the output is, as follows:</p><div class="calibre2"><pre class="programlisting">print min_max_range(x,(100,200))

[169.23, 200.0, 130.77, 100.0, 107.69, 184.62, 192.31, 169.23, 200.0, 100.0]</pre></div><p class="calibre11">The lowest value, <code class="literal">10</code>, is now scaled to <code class="literal">100</code> and the highest value, <code class="literal">23</code>, is scaled to <code class="literal">200</code>.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Standardizing the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec47" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Standardizing the data</h1></div></div></div><p class="calibre11">Standardization<a id="id228" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is the process of converting the input so that it has a mean of <code class="literal">0</code> and standard deviation of 1.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec153" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">If you are given a <a id="id229" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>vector X, the mean of <code class="literal">0</code> and standard deviation of 1 for X can be achieved by the following equation:</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note17" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Standardized X  =  x– mean(value) / standard deviation (X)</p></div></div><p class="calibre11">Let's see how this can be achieved in Python.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec154" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's import the necessary libraries to begin with. We will follow this with the generation of the input data:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
import numpy as np
from sklearn.preprocessing import scale

# Input data generation
np.random.seed(10)
x = [np.random.randint(10,25)*1.0 for i in range(10)]</pre></div><p class="calibre11">We are now ready to demonstrate standardization:</p><div class="calibre2"><pre class="programlisting">x_centered = scale(x,with_mean=True,with_std=False)
x_standard = scale(x,with_mean=True,with_std=True)

print x
print x_centered
print x_standard
print "Orginal x mean = %0.2f, Centered x mean = %0.2f, Std dev of \
        standard x =%0.2f"%(np.mean(x),np.mean(x_centered),np.std(x_standard))</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec155" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">We will generate <a id="id230" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>some random data using np.random:</p><div class="calibre2"><pre class="programlisting">x = [np.random.randint(10,25)*1.0 for i in range(10)]</pre></div><p class="calibre11">We will perform standardization using the <code class="literal">scale</code> function from scikit-learn:</p><div class="calibre2"><pre class="programlisting">x_centered = scale(x,with_mean=True,with_std=False)
x_standard = scale(x,with_mean=True,with_std=True)</pre></div><p class="calibre11">The <code class="literal">x_centered</code> is scaled using only the mean; you can see the <code class="literal">with_mean</code> parameter set to <code class="literal">True</code> and <code class="literal">with_std</code> set to <code class="literal">False</code>.</p><p class="calibre11">The <code class="literal">x_standard</code> is standardized using both mean and standard deviation.</p><p class="calibre11">Now let us look at the output.</p><p class="calibre11">The original data is as follows:</p><div class="calibre2"><pre class="programlisting">[19.0, 23.0, 14.0, 10.0, 11.0, 21.0, 22.0, 19.0, 23.0, 10.0]

Next, we will print x_centered, where we centered it with the mean value:

[ 1.8  5.8 -3.2 -7.2 -6.2  3.8  4.8  1.8  5.8 -7.2]

Finally we will print x_standardized, where we used both the mean and standard deviation:

[ 0.35059022  1.12967961 -0.62327151 -1.4023609  -1.20758855  0.74013492
  0.93490726  0.35059022  1.12967961 -1.4023609 ]

Orginal x mean = 17.20, Centered x mean = 0.00, Std dev of standard x =1.00</pre></div></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec156" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note18" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">Standardization can be generalized to any level and spread, as follows:</p><p class="calibre19">Standardized value = value – level / spread</p></div></div><p class="calibre11">Let's break the <a id="id231" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>preceding equation in two parts: just the numerator part, which is called centering, and the whole equation, which is called standardization. Using the mean values, centering plays a critical role in regression. Consider a dataset that has two attributes, weight and height. We will center the data such that the predictor, weight, has a mean of <code class="literal">0</code>. This makes the interpretation of intercept easier. The intercept will be interpreted as what is the expected height when the predictor values are set to their mean.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Performing tokenization"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec48" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Performing tokenization</h1></div></div></div><p class="calibre11">When you are given any text, the first job is to tokenize the text into a format that is based on the given problem requirements. Tokenization is a very broad term; we can tokenize the text <a id="id232" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>at the following various levels of granularity:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The paragraph level</li><li class="listitem">The sentence level</li><li class="listitem">The word level</li></ul></div><p class="calibre11">In this section, we will see sentence level and word level tokenization. The methods are similar and can be easily applied to a paragraph level or any other level of granularity as required by the problem at hand.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec157" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">We will see how to perform sentence level and word level tokenization in a single recipe.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec158" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's start with the demonstration of sentence tokenization:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from collections import defaultdict


# 1.Let us use a very simple text to demonstrate tokenization
# at sentence level and word level. You have seen this example in the
# dictionary recipe, except for some punctuation which are added.

sentence = "Peter Piper picked a peck of pickled peppers. A peck of pickled \
peppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled \
peppers, Wheres the peck of pickled peppers Peter Piper picked ?"

# 2.Using nltk sentence tokenizer, we tokenize the given text into sentences
# and verify the output using some print statements.

sent_list = sent_tokenize(sentence)

print "No sentences = %d"%(len(sent_list))
print "Sentences"
for sent in sent_list: print sent

# 3.With the sentences extracted let us proceed to extract
# words from these sentences.
word_dict = defaultdict(list)
for i,sent in enumerate(sent_list):
    word_dict[i].extend(word_tokenize(sent))

print word_dict</pre></div><p class="calibre11">A quick<a id="id233" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> peek at how NLTK performs its sentence tokenization in the following way:</p><div class="calibre2"><pre class="programlisting">def sent_tokenize(text, language='english'):
    """
    Return a sentence-tokenized copy of *text*,
    using NLTK's recommended sentence tokenizer
    (currently :class:`.PunktSentenceTokenizer`
    for the specified language).

    :param text: text to split into sentences
    :param language: the model name in the Punkt corpus
    """
    tokenizer = load('tokenizers/punkt/{0}.pickle'.format(language))
    return tokenizer.tokenize(text)</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec159" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we <a id="id234" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will initialize a variable sentence with a paragraph. This is the same example that we used in the dictionary recipe. In step 2, we will use nltk's <code class="literal">sent_tokenize </code>function to extract sentences from the given text.<code class="literal"> </code>You can look into the <a id="id235" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>source of <code class="literal">sent_tokenize</code> in nltk in the documentation<a id="id236" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> found at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize">http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize</a>.</p><p class="calibre11">As you can see, <code class="literal">sent_tokenize</code> loads a prebuilt tokenizer model, and using this model, it tokenizes the given text and returns the output. The tokenizer model is an instance of PunktSentenceTokenizer from the <code class="literal">nltk.tokenize.punkt</code> module. There are several pretrained instances of this tokenizer available in different languages. In our case, you can see that the language parameter is set to English.</p><p class="calibre11">Let's look at the output of this step:</p><div class="calibre2"><pre class="programlisting">No sentences = 3
Sentences
Peter Piper picked a peck of pickled peppers.
A peck of pickled             peppers, Peter Piper picked !!!
If Peter Piper picked a peck of pickled             peppers, Wheres the peck of pickled peppers Peter Piper picked ?</pre></div><p class="calibre11">As you can see, the sentence tokenizer has split our input text into three sentences. Let's proceed to step 3, where we will tokenize these sentences into words. Here, we will use the <code class="literal">word_tokenize</code> function in order to extract the words from each of the sentences and store them in a dictionary, where the key is the sentence number and the value is the list of words for that sentence. Let's look at the output of the print statement:</p><div class="calibre2"><pre class="programlisting">defaultdict(&lt;type 'list'&gt;, {0: ['Peter', 'Piper', 'picked', 'a', 'peck', 'of', 'pickled', 'peppers', '.'], 1: ['A', 'peck', 'of', 'pickled', 'peppers', ',', 'Peter', 'Piper', 'picked', '!', '!', '!'], 2: ['If', 'Peter', 'Piper', 'picked', 'a', 'peck', 'of', 'pickled', 'peppers', ',', 'Wheres', 'the', 'peck', 'of', 'pickled', 'peppers', 'Peter', 'Piper', 'picked', '?']})</pre></div><p class="calibre11">The <code class="literal">word_tokenize</code> uses <a id="id237" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a regular expression to split the<a id="id238" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> sentences into words. It will be useful to look at the source of <code class="literal">word_tokenize</code> found at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize">http://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize</a>.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec160" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">For sentence tokenization, we saw a way of doing it in NLTK. There are other methods available. The <code class="literal">nltk.tokenize.simple</code> module has a <code class="literal">line_tokenize</code> method. Let's take the same <a id="id239" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>input sentence as before and run it using <code class="literal">line_tokenize</code>:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
from nltk.tokenize import line_tokenize


sentence = "Peter Piper picked a peck of pickled peppers. A peck of pickled \
peppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled \
peppers, Wheres the peck of pickled peppers Peter Piper picked ?"


sent_list = line_tokenize(sentence)
print "No sentences = %d"%(len(sent_list))
print "Sentences"
for sent in sent_list: print sent

# Include new line characters
sentence = "Peter Piper picked a peck of pickled peppers. A peck of pickled\n \
peppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled\n \
peppers, Wheres the peck of pickled peppers Peter Piper picked ?"

sent_list = line_tokenize(sentence)
print "No sentences = %d"%(len(sent_list))
print "Sentences"
for sent in sent_list: print sent</pre></div><p class="calibre11">The output is as follows:</p><div class="calibre2"><pre class="programlisting">No sentences = 1
Sentences
Peter Piper picked a peck of pickled peppers. A peck of pickled             peppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled             peppers, Wheres the peck of pickled peppers Peter Piper picked ?</pre></div><p class="calibre11">You can see that we have only the sentence retrieved from the input.</p><p class="calibre11">Let's now modify our input in order to include new line characters:</p><div class="calibre2"><pre class="programlisting">sentence = "Peter Piper picked a peck of pickled peppers. A peck of pickled\n \
peppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled\n \
peppers, Wheres the peck of pickled peppers Peter Piper picked ?"</pre></div><p class="calibre11">Note that<a id="id240" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> we have a new line character added. We will again apply <code class="literal">line_tokenize</code> to get the following output:</p><div class="calibre2"><pre class="programlisting">No sentences = 3
Sentences
Peter Piper picked a peck of pickled peppers. A peck of pickled
             peppers, Peter Piper picked !!! If Peter Piper picked a peck of pickled
             peppers, Wheres the peck of pickled peppers Peter Piper picked ?</pre></div><p class="calibre11">You can see that it has tokenized our sentences at the new line and now we have three sentences.</p><p class="calibre11">See <span class="strong1"><em class="calibre15">Chapter 3</em></span> of the <span class="strong1"><em class="calibre15">NLTK</em></span> book; it has more references for sentence and word tokenization. It can be found at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.nltk.org/book/ch03.html">http://www.nltk.org/book/ch03.html</a>.</p></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec161" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Using Dictionary object </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Writing list </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Removing stop words"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec49" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Removing stop words</h1></div></div></div><p class="calibre11">In text processing, we are interested in words or phrases that will help us differentiate the given text from the<a id="id241" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> other text in the corpus. Let's call these words or phrases as key phrases. Every text mining application needs a way to find out the key phrases. An information retrieval application needs key phrases for the easy retrieval and ranking of search results. A text classification system needs key phrases as its features that are to be fed to a classifier.</p><p class="calibre11">This is where stop words come into the picture.</p><div class="calibre2"><blockquote class="blockquote"><p class="calibre17"><span class="strong1"><em class="calibre15">"Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words."</em></span></p></blockquote></div><p class="calibre11">
<span class="strong1"><em class="calibre15">Introduction to Information Retrieval By Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze</em></span>
</p><p class="calibre11">The Python NLTK library provides us with a default stop word corpus that we can leverage, as follows:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import stopwords
&gt;&gt;&gt; stopwords.words('english')
[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now']
&gt;&gt;&gt;</pre></div><p class="calibre11">You can see that we have printed the list of stop words in English.</p><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec162" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the necessary library and introduce<a id="id242" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> our input text:</p><div class="calibre2"><pre class="programlisting"># Load libraries
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

text = "Text mining, also referred to as text data mining, roughly equivalent to text analytics,\
refers to the process of deriving high-quality information from text. High-quality information is \
typically derived through the devising of patterns and trends through means such as statistical \
pattern learning. Text mining usually involves the process of structuring the input text \
(usually parsing, along with the addition of some derived linguistic features and the removal \
of others, and subsequent insertion into a database), deriving patterns within the structured data, \
and finally evaluation and interpretation of the output. 'High quality' in text mining usually \
refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks \
include text categorization, text clustering, concept/entity extraction, production of granular \
taxonomies, sentiment analysis, document summarization, and entity relation modeling \
(i.e., learning relations between named entities).Text analysis involves information retrieval, \
lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, \
information extraction, data mining techniques including link and association analysis, \
visualization, and predictive analytics. The overarching goal is, essentially, to turn text \
into data for analysis, via application of natural language processing (NLP) and analytical \
methods.A typical application is to scan a set of documents written in a natural language and \
either model the document set for predictive classification purposes or populate a database \
or search index with the information extracted."</pre></div><p class="calibre11">Let's now <a id="id243" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>demonstrate the stop words removal process:</p><div class="calibre2"><pre class="programlisting">words = word_tokenize(text)
# 2.Let us get the list of stopwords from nltk stopwords english corpus.
stop_words = stopwords.words('english')


print "Number of words = %d"%(len(words)) 
# 3.	Filter out the stop words.
words = [w for w in words if w not in stop_words]
print "Number of words,without stop words = %d"%(len(words)) 


words = [w for w in words if w not in string.punctuation]
print "Number of words,without stop words and punctuations = %d"%(len(words))</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec163" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we will import the necessary libraries from nltk. We will need the list of English stop words, so we will import the stop word corpus. We will need to tokenize our input text into words. For this, we will import the <code class="literal">word_tokenize</code> function from the <code class="literal">nltk.tokenize</code> module.</p><p class="calibre11">For our input text, we took the introduction paragraph from Wikipedia on text mining, which can be<a id="id244" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> found at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://en.wikipedia.org/wiki/Text_mining">http://en.wikipedia.org/wiki/Text_mining</a>.</p><p class="calibre11">Finally, we will <a id="id245" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>tokenize the input text into words using the word_tokenize function. The words is now a list of all the words tokenized from the input. Let's look at the output of the print function, where we will print the length of the words list:</p><div class="calibre2"><pre class="programlisting">Number of words = 259</pre></div><p class="calibre11">We have a total of 259 words in our list.</p><p class="calibre11">In step 2, we will compile a list of the English stop words in a list called <code class="literal">stop_words</code>.</p><p class="calibre11">In step 2, we will use a list comprehension to get a final list of the words; only those words that are not in the stop word list that we created in step 2. This way, we can remove the stop words from our input. Let's now look at the output of our print statement, where we will print the final list where the stop words have been removed:</p><div class="calibre2"><pre class="programlisting">Number of words,without stop words = 195</pre></div><p class="calibre11">You can see that we chopped off nearly 64 words from our input text, which were the stop words.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec164" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">Stop words are not limited to proper English words. It's contextual, depending on the application in hand and how you want to program your system. Ideally, if we are not interested in special characters, we can include them in our stop word list. Let's look at the following code:</p><div class="calibre2"><pre class="programlisting">import string
words = [w for w in words if w not in string.punctuation]
print "Number of words,without stop words and punctuations = %d"%(len(words)) </pre></div><p class="calibre11">Here, we will run another list comprehension in order to remove punctuations from our words. Now, the output looks as follows:</p><div class="calibre2"><pre class="programlisting">Number of words, without stop words and punctuations = 156</pre></div><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="tip07" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre19">Remember that stop word removal is contextual and based on the application. If you are working on a sentiment analysis application on mobile or chat room text, emoticons are highly useful. You don't remove them as they form a very good feature set for the downstream machine learning application.</p><p class="calibre19">Typically, in a document, the frequency of stop words is very high. However, there may be other words in your corpus that may have a very high frequency. Based on<a id="id246" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> your context, you can add them to your stop word list.</p></div></div></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec165" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Performing Tokenization </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <span class="strong1"><em class="calibre15">Analyzing Data - Explore &amp; Wrangle</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">List generation </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Stemming the words"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec50" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Stemming the words</h1></div></div></div><p class="calibre11">In this we will see how to stem the word.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec166" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Standardization<a id="id247" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of the text is a different beast and we need different tools<a id="id248" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to tame it. In this section, we will look into how we can convert words to their base forms in order to bring consistency to our processing. We will start with traditional ways that include stemming and lemmatization. English grammar dictates how certain words are used in sentences. For example, perform, performing, and performs indicate the same action; they appear in different sentences based on the grammar rules.</p><div class="calibre2"><blockquote class="blockquote"><p class="calibre17"><span class="strong1"><em class="calibre15">The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.</em></span></p><p class="calibre17"><span class="strong1"><em class="calibre15">Introduction to Information Retrieval By Christopher D. Manning, Prabhakar Raghavan &amp; Hinrich Schütze</em></span></p></blockquote></div><p class="calibre11">Let's look into how we can perform word stemming using Python NLTK. NLTK provides us with a rich set of functions that can help us do the stemming pretty easily:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; import nltk.stem
&gt;&gt;&gt; dir(nltk.stem)
['ISRIStemmer', 'LancasterStemmer', 'PorterStemmer', 'RSLPStemmer', 'RegexpStemmer', 'SnowballStemmer', 'StemmerI', 'WordNetLemmatizer', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', 'api', 'isri', 'lancaster', 'porter', 'regexp', 'rslp', 'snowball', 'wordnet']
&gt;&gt;&gt;  </pre></div><p class="calibre11">We can see the list of functions in the module, and for our interest, we have the following stemmers:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Porter – porter stemmer</li><li class="listitem">Lancaster – Lancaster stemmer</li><li class="listitem">Snowball – snowball stemmer</li></ul></div><p class="calibre11">Porter is the most commonly used stemmer. The algorithm is not very aggressive when moving words to their root form.</p><p class="calibre11">Snowball is<a id="id249" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> an improvement over porter. It is also faster than porter in terms <a id="id250" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the computational time.</p><p class="calibre11">Lancaster is the most aggressive stemmer. With porter and snowball, the final word tokens would still be readable by humans, but with Lancaster, it is not readable. It's the fastest of the trio.</p><p class="calibre11">In this recipe, we will use some of them to see how the stemming of words can be performed.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec167" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">To begin with, let's load the necessary libraries and declare the dataset against which we would want to demonstrate stemming:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
from nltk import stem

#1. small input to figure out how the three stemmers perform.
input_words = ['movies','dogs','planes','flowers','flies','fries','fry','weeks','planted','running','throttle']</pre></div><p class="calibre11">Let's jump into the different stemming algorithms, as follows:</p><div class="calibre2"><pre class="programlisting">#2.Porter Stemming
porter = stem.porter.PorterStemmer()
p_words = [porter.stem(w) for w in input_words]
print p_words

#3.Lancaster Stemming
lancaster = stem.lancaster.LancasterStemmer()
l_words = [lancaster.stem(w) for w in input_words]
print l_words

#4.Snowball stemming
snowball = stem.snowball.EnglishStemmer()
s_words = [snowball.stem(w) for w in input_words]
print s_words

wordnet_lemm = stem.WordNetLemmatizer()
wn_words = [wordnet_lemm.lemmatize(w) for w in input_words]
print wn_words</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec168" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we <a id="id251" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will import the stem module from nltk. We will also create a list <a id="id252" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of words that we want to stem. If you observe carefully, the words have been chosen to have different suffixes, including s, ies, ed, ing, and so on. Additionally, there are some words in their root form already, such as throttle and fry. The idea is to see how the stemming algorithm treats them.</p><p class="calibre11">Steps 2, 3, and 4 are very similar; we will invoke the porter, lancaster, and snowball stemmers on the input and print the output. We will use a list comprehension to apply these words to our input and finally, print the output. Let's look at the print output to understand the effect of stemming:</p><div class="calibre2"><pre class="programlisting">[u'movi', u'dog', u'plane', u'flower', u'fli', u'fri', u'fri', u'week', u'plant', u'run', u'throttl']</pre></div><p class="calibre11">This is the output from step 2. Porter stemming was applied to our input words. We can see that the words with the suffixes ies, s, ed , and ing have been reduced to their root forms:</p><div class="calibre2"><pre class="programlisting">Movies – movi
Dogs   - dog
Planes – plane
Running – run and so on.</pre></div><p class="calibre11">It's interesting to note that throttle is changed to throttle.</p><p class="calibre11">In step 3, we will print the output of lancaster, which is as follows:</p><div class="calibre2"><pre class="programlisting">[u'movy', 'dog', 'plan', 'flow', 'fli', 'fri', 'fry', 'week', 'plant', 'run', 'throttle']</pre></div><p class="calibre11">The word throttle has been left as it is. Note what has happened to movies.</p><p class="calibre11">Similarly, let's look at the output produced by the snowball stemmer in step 4:</p><div class="calibre2"><pre class="programlisting">[u'movi', u'dog', u'plane', u'flower', u'fli', u'fri', u'fri', u'week', u'plant', u'run', u'throttl']</pre></div><p class="calibre11">The output is pretty similar to the porter stemmer.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec169" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">All the three algorithms are pretty involved; going into the details of these algorithms is beyond the scope of this book. I will recommend you to look to the web for more details on these algorithms.</p><p class="calibre11">For details<a id="id253" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of the <a id="id254" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>porter and <a id="id255" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>snowball stemmers, refer to the following link:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://snowball.tartarus.org/algorithms/porter/stemmer.html">http://snowball.tartarus.org/algorithms/porter/stemmer.html</a>
</p></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec170" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">List Comprehension </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Performing word lemmatization"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec51" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Performing word lemmatization</h1></div></div></div><p class="calibre11">In this we will learn how to perform word lemmatization.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec171" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Stemming is a <a id="id256" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>heuristic process, which goes about chopping the word suffixes in order to get to the root form of the word. In the previous recipe, we saw that it may end up chopping even the right words, that is, chopping the derivational affixes.</p><p class="calibre11">See the following <a id="id257" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Wikipedia link for the derivational patterns:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://en.wikipedia.org/wiki/Morphological_derivation#Derivational_patterns">http://en.wikipedia.org/wiki/Morphological_derivation#Derivational_patterns</a>
</p><p class="calibre11">On the other hand, lemmatization uses a morphological analysis and vocabulary to get the lemma of a <a id="id258" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>word. It tries to change only the inflectional endings and give the base word from a dictionary.</p><p class="calibre11">See Wikipedia for more information on inflection at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://en.wikipedia.org/wiki/Inflection">http://en.wikipedia.org/wiki/Inflection</a>.</p><p class="calibre11">In this recipe, we will use NLTK's <code class="literal">WordNetLemmatizer</code>.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec172" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">To begin with, we will load the necessary libraries. Once again, as we did in the previous recipes, we will prepare a text input in order to demonstrate lemmatization. We will then proceed to implement lemmantization in the following way:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
from nltk import stem

#1. small input to figure out how the three stemmers perform.
input_words = ['movies','dogs','planes','flowers','flies','fries','fry','weeks', 'planted','running','throttle']

#2.Perform lemmatization.
wordnet_lemm = stem.WordNetLemmatizer()
wn_words = [wordnet_lemm.lemmatize(w) for w in input_words]
print wn_words</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec173" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Step 1 is <a id="id259" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>very similar to our stemming recipe. We will provide the input. In step 2, we will do the lemmatization. This lemmatizer uses Wordnet's built-in morphy-function.</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://wordnet.princeton.edu/man/morphy.7WN.html">https://wordnet.princeton.edu/man/morphy.7WN.html</a>
</p><p class="calibre11">Let's look <a id="id260" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>at the output from the print statement:</p><div class="calibre2"><pre class="programlisting">[u'movie', u'dog', u'plane', u'flower', u'fly', u'fry', 'fry', u'week', 'planted', 'running', 'throttle']</pre></div><p class="calibre11">The first thing to strike is the word movie. You can see that it has got this right. Porter and the other algorithms had chopped the last letter e.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec174" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">Let's look into a small example using lemmatizer:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; wordnet_lemm.lemmatize('running')
'running'
&gt;&gt;&gt; porter.stem('running')
u'run'
&gt;&gt;&gt; lancaster.stem('running')
'run'
&gt;&gt;&gt; snowball.stem('running')
u'run'</pre></div><p class="calibre11">The word running should ideally be run and our lemmatizer should have gotten it right. We can see that it has not made any changes to running. However, our heuristic-based stemmers have got it right! Then, what has gone wrong with our lemmatizer?</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="tip08" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre19">By default, the lemmatizer assumes that the input is a noun; this can be rectified by passing the POS tag of the word to our lemmatizer, as follows:</p><div class="calibre2"><pre class="programlisting1">&gt;&gt;&gt; wordnet_lemm.lemmatize('running','v') u'run'</pre></div></div></div></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec175" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Performing Tokenization </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>, <span class="strong1"><em class="calibre15">Analyzing Data - Explore &amp; Wrangle</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Representing the text as a bag of words"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec52" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Representing the text as a bag of words</h1></div></div></div><p class="calibre11">In this we will learn how represent the text as a bag of words.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec176" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">In order to do<a id="id261" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> machine learning on text, we will need to convert the text to numerical feature vectors. In this section, we will look into the bag of words representation, where the text is converted to numerical vectors and the column names are the underlying words and values can be either of thw following points:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Binary, which indicates whether the word is present/absent in the given document</li><li class="listitem">Frequency, which indicates the count of the word in the given document</li><li class="listitem">TFIDF, which is a score that we will cover subsequently</li></ul></div><p class="calibre11">Bag of words is the most frequent way of representing the text. As the name suggests, the order of words is ignored and only the presence/absence of words are key to this representation. It is a two-step process, as follows:</p><div class="calibre2"><ol class="orderedlist"><li class="listitem1">For every word in the document that is present in the training set, we will assign an integer and store this as a dictionary.</li><li class="listitem1">For every document, we will create a vector. The columns of the vectors are the actual words itself. They form the features. The values of the cell are binary, frequency, or TFIDF.</li></ol></div></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec177" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the necessary libraries and prepare the dataset for the demonstration of bag of words:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords

# 1.	Our input text, we use the same input which we had used in stop word removal recipe.
text = "Text mining, also referred to as text data mining, roughly equivalent to text analytics,\
refers to the process of deriving high-quality information from text. High-quality information is \
typically derived through the devising of patterns and trends through means such as statistical \
pattern learning. Text mining usually involves the process of structuring the input text \
(usually parsing, along with the addition of some derived linguistic features and the removal \
of others, and subsequent insertion into a database), deriving patterns within the structured data, \
and finally evaluation and interpretation of the output. 'High quality' in text mining usually \
refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks \
include text categorization, text clustering, concept/entity extraction, production of granular \
taxonomies, sentiment analysis, document summarization, and entity relation modeling \
(i.e., learning relations between named entities).Text analysis involves information retrieval, \
lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, \
information extraction, data mining techniques including link and association analysis, \
visualization, and predictive analytics. The overarching goal is, essentially, to turn text \
into data for analysis, via application of natural language processing (NLP) and analytical \
methods.A typical application is to scan a set of documents written in a natural language and \
either model the document set for predictive classification purposes or populate a database \
or search index with the information extracted."</pre></div><p class="calibre11">Let's jump<a id="id262" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> into how to transform the text into a bag of words representation:</p><div class="calibre2"><pre class="programlisting">#2.Let us divide the given text into sentences
sentences = sent_tokenize(text)

#3.Let us write the code to generate feature vectors.
count_v = CountVectorizer()
tdm = count_v.fit_transform(sentences)


# While creating a mapping from words to feature indices, we can ignore
# some words by providing a stop word list.
stop_words = stopwords.words('english')
count_v_sw = CountVectorizer(stop_words=stop_words)
sw_tdm = count_v.fit_transform(sentences)


# Use ngrams
count_v_ngram = CountVectorizer(stop_words=stop_words,ngram_range=(1,2))
ngram_tdm = count_v.fit_transform(sentences)</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec178" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we <a id="id263" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will define the input. This is the same input that we used for the stop word removal recipe. In step 2, we will import the sentence tokenizer and tokenize the given input into sentences. We will treat every sentence here as a document:</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="tip09" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre19">Depending on your application, the notion of a document can change. In this case, our sentence is considered as a document. In some cases, we can also treat a paragraph as a document. In web page mining, a single web page can be treated as a document or parts of the web page separated by the &lt;p&gt; tags can also be treated as a document.</p><div class="calibre2"><pre class="programlisting1">&gt;&gt;&gt; len(sentences)
6
&gt;&gt;&gt;</pre></div></div></div><p class="calibre11">If we print the length of the sentence list, we will get six, and so in our case, we have six documents.</p><p class="calibre11">In step 3, we will import <code class="literal">CountVectorizer</code> from the <code class="literal">scikitlearn.feature_extraction</code> text package. It converts a collection of documents—in this case, a list of sentences—to a matrix, where the rows are sentences and the columns are the words in these sentences. The count of these words are inserted in the value of these cells.</p><p class="calibre11">We will transform the list of sentences into a term document matrix using <code class="literal">CountVectorizer</code>. Let's dissect the output one by one. First, we will look into <code class="literal">count_v</code>, which is a <code class="literal">CountVectorizer</code> object. We had mentioned in the introduction that we need to build a dictionary of all the words in the given text. The <code class="literal">vocabulary_</code> of <code class="literal">count_v</code> attribute provides us with the list of words and their associated IDs or feature indices:</p><div class="mediaobject"><img src="Images/B04041_03_19.jpg" alt="How it works…" class="calibre54"/></div><p class="calibre11">This <a id="id264" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>dictionary can be retrieved using the <code class="literal">vocabulary_</code> attribute. This is a map of the terms in order to feature indices. We can also use the following function to get the list of words (features):</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; count_v.get_feature_names()</pre></div><p class="calibre11">Let's now move on to look at <code class="literal">tdm</code>, which is the object that we received after transforming the given input using CountVectorizer:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; type(tdm)
&lt;class 'scipy.sparse.csr.csr_matrix'&gt;
&gt;&gt;&gt;</pre></div><p class="calibre11">As you can see, tdm is a sparse matrix object. Refer to the following link to understand more about<a id="id265" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the sparse matrix representation:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html">http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html</a>
</p><p class="calibre11">We can look into the shape of this object and also inspect some of the elements, as follows:</p><div class="mediaobject"><img src="Images/B04041_03_20.jpg" alt="How it works…" class="calibre55"/></div><p class="calibre11">We can<a id="id266" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> see that the shape of the matrix is 6 X 122. We have six documents, that is, sentences in our context and 122 words that form the vocabulary. Note that this is a sparse matrix representation; as all the sentences will not have all the words, a lot of the cell values will have zero as an entry and hence, we will print only the indices that have non-zero entries.</p><p class="calibre11">From <code class="literal">tdm.indptr</code>, we know that document 1's entry starts from <code class="literal">0</code> and ends at 18 in the <code class="literal">tdm.data</code> and <code class="literal">tdm.indices</code> arrays, as follows:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; tdm.data[0:17]
array([4, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)
&gt;&gt;&gt; tdm.indices[0:17]
array([107,  60,   2,  83, 110,   9,  17,  90,  28,   5,  84, 108,  77,
        67,  20,  40,  81])
&gt;&gt;&gt;</pre></div><p class="calibre11">We can verify this in the following way:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; count_v.get_feature_names()[107]
u'text'
&gt;&gt;&gt; count_v.get_feature_names()[60]
u'mining'</pre></div><p class="calibre11">We can<a id="id267" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> see that <code class="literal">107</code>, which corresponds to the word text, has occurred four times in the first sentence, and similarly, mining has occurred once. Thus, in this recipe, we converted a given text into a feature vector, where the features are words.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec179" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">The <code class="literal">CountVectorizer</code> class has a lot of other features to offer in order to transform the text into feature vectors. Let's look at some of them:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; count_v.get_params()
{'binary': False, 'lowercase': True, 'stop_words': None, 'vocabulary': None, 'tokenizer': None, 'decode_error': u'strict', 'dtype': &lt;type 'numpy.int64'&gt;, 'charset_error': None, 'charset': None, 'analyzer': u'word', 'encoding': u'utf-8', 'ngram_range': (1, 1), 'max_df': 1.0, 'min_df': 1, 'max_features': None, 'input': u'content', 'strip_accents': None, 'token_pattern': u'(?u)\\b\\w\\w+\\b', 'preprocessor': None}
&gt;&gt;&gt;	</pre></div><p class="calibre11">The first one is binary, which is set to <code class="literal">False</code>; we can also have it set to <code class="literal">True</code>. Then, the final matrix would not have the count but will have one or zero, based on the presence or absence of the word in the document.</p><p class="calibre11">The lowercase is set to <code class="literal">True</code> by default; the input text is transformed into lowercase before the mapping of the words to feature indices is performed.</p><p class="calibre11">While creating a mapping of the words to feature indices, we can ignore some words by providing a stop word list. Observe the following example:</p><div class="calibre2"><pre class="programlisting">from nltk.corpus import stopwords
stop_words = stopwords.words('english')

count_v = CountVectorizer(stop_words=stop_words)
sw_tdm = count_v.fit_transform(sentences)</pre></div><p class="calibre11">If we print the size of the vocabulary that has been built, we can see the following:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; len(count_v_sw.vocabulary_)
106
&gt;&gt;&gt;</pre></div><p class="calibre11">We can see that we have 106 now as compared to 122 that we had before.</p><p class="calibre11">We can also give a fixed set of vocabulary to <code class="literal">CountVectorizer</code>. The final sparse matrix columns will be only from these fixed sets and anything that is not in this set will be ignored.</p><p class="calibre11">The next <a id="id268" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>interesting parameter is the ngram range. You can see that a tuple (1,1) has been passed. This ensures that only one grams or single words are used while creating a feature set. For example, this can be changed to (1,2), which tells <code class="literal">CountVectorizer</code> to create both unigrams and bigrams. Let's look at the following code and the output:</p><div class="calibre2"><pre class="programlisting">count_v_ngram = CountVectorizer(stop_words=stop_words,ngram_range=(1,2))
ngram_tdm = count_v.fit_transform(sentences)</pre></div><p class="calibre11">Both the unigrams and bigrams are now a part of our feature set.</p><p class="calibre11">I will leave you to explore the other parameters. The documentation for these parameters is available at<a id="id269" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the following link:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html</a>
</p></div><div class="calibre2" title="See also"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec180" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre15">Using Dictionaries</em></span> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>,<span class="strong1"><em class="calibre15"> Using Python for Data Science</em></span></li><li class="listitem"><span class="strong1"><em class="calibre15">Removing Stop words, Stemming of words, Lemmatization of words </em></span>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Data Analysis – Explore and Wrangle">Chapter 3</a>,<span class="strong1"><em class="calibre15"> Analyzing Data - Explore &amp; Wrangle</em></span></li></ul></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Calculating term frequencies and inverse document frequencies"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch03lvl1sec53" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating term frequencies and inverse document frequencies</h1></div></div></div><p class="calibre11">In this we will learn how to calculate term frequencies and inverse document frequencies.</p><div class="calibre2" title="Getting ready"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec181" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Occurrences and <a id="id270" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>counts are good as feature values, but they suffer from<a id="id271" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> some problems. Let's say that we have four documents of unequal length. This will give a higher weightage to the terms in the longer documents than those in the shorter ones. So, instead of using the plain vanilla occurrence, we will normalize it; we will divide the number of occurrences of a word in a document by the total number of words in the document. This metric is called term frequencies. Term frequency is also not without problems. There are words that will occur in many documents. These words would dominate the feature vector but they are not informative enough to distinguish the documents in the corpus. Before we look into a new metric that can avoid this problem, let's define document frequency. Similar to word frequency, which is local with respect to a document, we can calculate a score called document frequency, which is the number of documents that the word occurs in the <a id="id272" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>corpus divided by the total number of documents in the corpus.</p><p class="calibre11">The final metric <a id="id273" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that we will use for the words is the product of the term frequency and the inverse of the document frequency. This is called the TFIDF score.</p></div><div class="calibre2" title="How to do it…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec182" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Load the necessary libraries and declare the input data that will be used for the demonstration of term frequencies and inverse document frequencies:</p><div class="calibre2"><pre class="programlisting"># Load Libraries
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer


# 1.	We create an input document as in the previous recipe.

text = "Text mining, also referred to as text data mining, roughly equivalent to text analytics,\
refers to the process of deriving high-quality information from text. High-quality information is \
typically derived through the devising of patterns and trends through means such as statistical \
pattern learning. Text mining usually involves the process of structuring the input text \
(usually parsing, along with the addition of some derived linguistic features and the removal \
of others, and subsequent insertion into a database), deriving patterns within the structured data, \
and finally evaluation and interpretation of the output. 'High quality' in text mining usually \
refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks \
include text categorization, text clustering, concept/entity extraction, production of granular \
taxonomies, sentiment analysis, document summarization, and entity relation modeling \
(i.e., learning relations between named entities).Text analysis involves information retrieval, \
lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, \
information extraction, data mining techniques including link and association analysis, \
visualization, and predictive analytics. The overarching goal is, essentially, to turn text \
into data for analysis, via application of natural language processing (NLP) and analytical \
methods.A typical application is to scan a set of documents written in a natural language and \
either model the document set for predictive classification purposes or populate a database \
or search index with the information extracted."</pre></div><p class="calibre11">Let's see how<a id="id274" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to find the term frequency and inverse document<a id="id275" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> frequency:</p><div class="calibre2"><pre class="programlisting"># 2.	Let us extract the sentences.
sentences = sent_tokenize(text)

# 3.	Create a matrix of term document frequency.
stop_words = stopwords.words('english')

count_v = CountVectorizer(stop_words=stop_words)
tdm = count_v.fit_transform(sentences)

#4.	Calcuate the TFIDF score.
tfidf = TfidfTransformer()
tdm_tfidf = tfidf.fit_transform(tdm)</pre></div></div><div class="calibre2" title="How it works…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec183" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Steps 1, 2, and 3 are the same as the previous recipe. Let's look at step 4, where we will pass the output of step 3 in order to calculate the TFIDF score:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; type(tdm)
&lt;class 'scipy.sparse.csr.csr_matrix'&gt;
&gt;&gt;&gt;</pre></div><p class="calibre11">Tdm is a sparse matrix. Now, let's look at the values of these matrices, using indices, data, and index pointer:</p><div class="mediaobject"><img src="Images/B04041_03_21.jpg" alt="How it works…" class="calibre56"/></div><p class="calibre11">The <a id="id276" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data <a id="id277" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>shows the values, we don't have the occurences, but the normalized TFIDF score for the words.</p></div><div class="calibre2" title="There's more…"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch03lvl2sec184" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">Once again, we <a id="id278" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can delve deeper into the TFIDF transformer<a id="id279" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> by looking into the parameters that can be passed:</p><div class="calibre2"><pre class="programlisting">&gt;&gt;&gt; tfidf.get_params()
{'use_idf': True, 'smooth_idf': True, 'sublinear_tf': False, 'norm': u'l2'}
&gt;&gt;&gt;</pre></div><p class="calibre11">The documentation<a id="id280" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> for this is available at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html</a>.</p></div></div></div>



  </body></html>