<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch02" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 2. The Spark Programming Model</h1></div></div></div><p class="calibre11">Large-scale data processing using thousands of nodes with built-in fault tolerance has become widespread due to the availability of open source frameworks, with Hadoop being a popular choice. These frameworks are quite successful in executing specific tasks such as <strong class="calibre19">Extract, Transform, and Load</strong> (<strong class="calibre19">ETL</strong>) and storage applications that deal with web-scale data. However, developers were left with a myriad of tools to work with, along with the well-established Hadoop ecosystem. There was a need for a single, general-purpose development platform that caters to batch, streaming, interactive, and iterative requirements. This was the motivation behind Spark.</p><p class="calibre11">The previous chapter outlined the big data analytics challenges and how Spark addressed most of them at a very high level. In this chapter, we will examine the design goals and choices involved in the making of Spark to get a clearer understanding of its suitability as a data science platform for big data. We will also cover the core abstraction <strong class="calibre19">Resilient Distributed Dataset</strong> (<strong class="calibre19">RDD</strong>) in depth with examples.</p><p class="calibre11">As a prerequisite for this chapter, a basic understanding of Python or Scala along with elementary understanding of Spark is needed. The topics covered in this chapter are as follows:</p><div><ul class="itemizedlist"><li class="listitem">The programming paradigm - language support and design benefits<div><ul class="itemizedlist1"><li class="listitem">Supported programming languages</li><li class="listitem">Choosing the right language</li></ul></div><p class="calibre31">
</p></li></ul></div><div><ul class="itemizedlist"><li class="listitem">The Spark engine - Spark core components and their implications<div><ul class="itemizedlist1"><li class="listitem">Driver program</li><li class="listitem">Spark shell</li><li class="listitem">SparkContext</li><li class="listitem">Worker nodes</li><li class="listitem">Executors</li><li class="listitem">Shared variables</li><li class="listitem">Flow of execution</li></ul></div><p class="calibre31">
</p></li><li class="listitem">The RDD API - understanding the RDD fundamentals<div><ul class="itemizedlist1"><li class="listitem">RDD basics</li><li class="listitem">Persistence</li></ul></div><p class="calibre31">
</p></li><li class="listitem">RDD operations - let's get your hands dirty<div><ul class="itemizedlist1"><li class="listitem">Getting started with the shell</li><li class="listitem">Creating RDDs</li><li class="listitem">Transformations on normal RDDs</li><li class="listitem">Transformations on pair RDDs</li><li class="listitem">Actions</li></ul></div><p class="calibre31">
</p></li></ul></div><div><div><div><div><h1 class="title1"><a id="ch02lvl1sec14" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The programming paradigm</h1></div></div></div><p class="calibre11">For Spark to address the big data challenges and serve as a platform for data science and other scalable applications, it was built with well-thought-out design considerations and language support.</p><p class="calibre11">There are Spark APIs designed for varieties of application developers to create Spark-based applications using standard API interfaces. Spark provides APIs for Scala, Java, R and Python programming languages, as explained in the following sections.</p><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec14" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Supported programming languages</h2></div></div></div><p class="calibre11">With built-in support for so many languages, Spark can be used interactively through a shell, which is otherwise known as <strong class="calibre19">Read-Evaluate-Print-Loop</strong> (<strong class="calibre19">REPL</strong>), in a way that will feel familiar to developers of any language. The developers can use the language of their choice, leverage existing libraries, and seamlessly interact with Spark and its ecosystem. Let us see the ones supported on Spark and how they fit into the Spark ecosystem.</p><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec0" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Scala</h3></div></div></div><p class="calibre11">Spark itself is written in Scala, a <strong class="calibre19">Java Virtual Machine</strong> (<strong class="calibre19">JVM</strong>) based functional programming language. The Scala compiler generates byte code that executes on the JVM. So, it can seamlessly integrate with any other JVM-based systems such as HDFS, Cassandra, HBase, and so on. Scala was the language of choice because of its concise programming interface, an interactive shell, and its ability to capture functions and efficiently ship them across the nodes in a cluster. Scala is an extensible (scalable, hence the name), statically typed, efficient multi-paradigm language that supports functional and object-oriented language features.</p><p class="calibre11">Apart from the full-blown applications, Scala also supports shell (Spark shell) for interactive data analysis on Spark.</p></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec1" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Java</h3></div></div></div><p class="calibre11">Since Spark is JVM based, it naturally supports Java. This helps existing Java developers to develop data science applications along with other scalable applications. Almost all the built-in library functions are accessible from Java. Coding in Java for data science assignments is comparatively difficult in Spark, but someone very hands-on with Java might find it easy.</p><p class="calibre11">This Java API only lacks a shell-based interface for interactive data analysis on Spark.</p></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec2" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Python</h3></div></div></div><p class="calibre11">Python is supported on Spark through PySpark, which is built on top of Spark's Java API (using Py4J). From now on, we will  be using the term <strong class="calibre19">PySpark</strong> to refer to the Python environment on Spark. Python was already very popular amongst developers for data wrangling, data munging, and other data science related tasks. Support for Python on Spark became even more popular as Spark could address the scalable computation challenge.</p><p class="calibre11">Through Python's interactive shell on Spark (PySpark), interactive data analysis at scale is possible.</p></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec3" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>R</h3></div></div></div><p class="calibre11">R is supported on Spark through SparkR, an R package through which Spark's scalability is accessible through R. SparkR empowered R to address its limitation of single-threaded runtime, because of which computation was limited only to a single node.</p><p class="calibre11">Since R was originally designed only for statistical analysis and machine learning, it was already enriched with most of the packages. Data scientists can now work on huge data at scale with a minimal learning curve. R is still a default choice for many data scientists.</p></div></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec15" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Choosing the right language</h2></div></div></div><p class="calibre11">Apart from the developer's language preference, at times there are other constraints that may draw attention. The following aspects could supplement your development experience while choosing one language over the other:</p><div><ul class="itemizedlist"><li class="listitem">An interactive shell comes in handy when developing complex logic. All languages supported by Spark except Java have an interactive shell.</li><li class="listitem">R is the lingua franca of data scientists. It is definitely more suitable for pure data analytics because of its richer set of libraries. R support was added in Spark 1.4.0 so that Spark reaches out to data scientists working on R.</li><li class="listitem">Java has a broader base of developers. Java 8 has included lambda expressions and hence the functional programming aspect. Nevertheless, Java tends to be verbose.</li><li class="listitem">Python is gradually gaining more popularity in the data science space. The availability of Pandas and other data processing libraries, and its simple and expressive nature, make Python a strong candidate. Python gives more flexibility than R in scenarios such as data aggregation from different sources, data cleaning, natural language processing, and so on.</li><li class="listitem">Scala is perhaps the best choice for real-time analytics because this is the closest to Spark. The initial learning curve for developers coming from other languages should not be a deterrent for serious production systems. The latest inclusions to Spark are usually first available in Scala. Its static typing and sophisticated type inference improve efficiency as well as compile-time checks. Scala can draw from Java's libraries as Scala's own library base is still at an early stage, but catching up.</li></ul></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch02lvl1sec15" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The Spark engine</h1></div></div></div><p class="calibre11">To program with Spark, a basic understanding of Spark components is needed. In this section, some of the important Spark components along with their execution mechanism will be explained so that developers and data scientists can write programs and build applications.</p><p class="calibre11">Before getting into the details, we suggest you take a look at the following diagram so that the descriptions of the Spark gears are more comprehensible as you read further:</p><p class="calibre11">
</p><div><img src="img/image_02_001.jpg" alt="The Spark engine" class="calibre32"/></div><p class="calibre11">
</p><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec16" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Driver program</h2></div></div></div><p class="calibre11">The Spark shell is an example of a driver program. A driver program is a process that executes in the JVM and runs the user's <em class="calibre22">main</em> function on it. It has a SparkContext object which is a connection to the underlying cluster manager. A Spark application is initiated when the driver starts and it completes when the driver stops. The driver, through an instance of SparkContext, coordinates all processes within a Spark application.</p><p class="calibre11">Primarily, an RDD lineage <strong class="calibre19">Directed Acyclic Graph</strong> (<strong class="calibre19">DAG</strong>) is built on the driver side with data sources (which may be RDDs) and transformations. This DAG is submitted to the DAG scheduler when an <em class="calibre22">action</em> method is encountered. The DAG scheduler then splits the DAG into logical units of work (for example, map or reduce) called stages. Each stage, in turn, is a set of tasks, and each task is assigned to an executor (worker) by the task scheduler. Jobs may be executed in FIFO order or round robin, depending on the configuration.</p><div><div><h3 class="title5"><a id="tip3" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Tip</h3><p class="calibre25">Inside a single Spark application, multiple parallel jobs can run simultaneously if they were submitted from separate threads.</p></div></div></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec17" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The Spark shell</h2></div></div></div><p class="calibre11">The Spark shell is none other than the interface provided by Scala and Python. It looks very similar to any other interactive shell. It has a SparkContext object (created by default for you) that lets you leverage the distributed cluster. An interactive shell is quite useful for exploratory or ad hoc analysis. You can develop your complex scripts step by step through the shell without going through the compile-build-execute cycle.</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec18" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>SparkContext</h2></div></div></div><p class="calibre11">SparkContext is the entry point to the Spark core engine. This object is required to create and manipulate RDDs and create shared variables on a cluster. The SparkContext object connects to a cluster manager, which is responsible for resource allocation. Spark comes with its own standalone cluster manager. Since the cluster manager is a pluggable component in Spark, it can be managed through external cluster managers such as Apache Mesos or YARN.</p><p class="calibre11">When you start a Spark shell, a SparkContext object is created by default for you. You can also create it by passing a SparkConf object that is used to set various Spark configuration parameters as key value pairs. Please note that there can be only one SparkContext object in a JVM.</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec19" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Worker nodes</h2></div></div></div><p class="calibre11">Worker nodes are the nodes that run the application code in a cluster, obeying the driver program. The real work is actually executed by the worker nodes. Each machine in the cluster may have one or more worker instances (default one). A worker node executes one or more executors that belong to one or more Spark applications. It consists of a <em class="calibre22">block manager</em> component, which is responsible for managing data blocks. The blocks can be cached RDD data, intermediate shuffled data, or broadcast data. When the available RAM is not sufficient, it automatically moves some data blocks to disk. Data replication across nodes is another responsibility of block manager.</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec20" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Executors</h2></div></div></div><p class="calibre11">Each application has a set of executor processes. Executors reside on worker nodes and communicate directly with the driver once the connection is made by the cluster manager. All executors are managed by SparkContext. An executor is a single JVM instance that serves a single Spark application. An executor is responsible for managing computation through tasks, storage, and caching on each worker node. It can run multiple tasks concurrently.</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec21" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Shared variables</h2></div></div></div><p class="calibre11">Normally, the code is shipped to partitions along with separate copies of variables. These variables cannot be used to propagate results (for example, intermediate work counts) back to the driver program. Shared variables are used for this purpose. There are two kinds of shared variables, <strong class="calibre19">broadcast variables</strong> and <strong class="calibre19">accumulators</strong>.</p><p class="calibre11">Broadcast variables enable the programmers to retain a read-only copy cached on each node rather than shipping a copy of it with tasks. If large, read-only data is used in multiple operations, it can be designated as broadcast variables and shipped only once to all worker nodes. The data broadcast in this way is cached in serialized form and is deserialized before running each task. Subsequent operations can access these variables along with the local variables moved along with the code. Creating broadcast variables is not necessary in all cases, except the ones where tasks across multiple stages need the same read-only copy of the data.</p><p class="calibre11">Accumulators are variables that are always incremented, such as counters or cumulative sums. Spark natively supports accumulators of numeric types, but allows programmers to add support for new types. Please note that the worker nodes cannot read the value of accumulators; they can only modify their values.</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec22" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Flow of execution</h2></div></div></div><p class="calibre11">A Spark application consists of a set of processes with one <em class="calibre22">driver</em> program and multiple <em class="calibre22">worker</em> (<em class="calibre22">executor</em>) programs. The driver program contains the application's <em class="calibre22">main</em> function and a SparkContext object, which represents a connection to the Spark cluster. Coordination between driver and the other processes happens through the SparkContext object.</p><p class="calibre11">A typical Spark client program performs the following steps:</p><div><ol class="orderedlist"><li class="listitem1">When a program is run on a Spark shell, it is called the driver program with the user's <code class="literal">main</code> method in it. It gets executed in the JVM of the system where you are running the driver program.</li><li class="listitem1">The first step is to create a SparkContext object with the required configuration parameters. When you run the PySpark or Spark shell, it is instantiated by default, but for other applications, you have to create it explicitly. SparkContext is actually the gateway to Spark.</li><li class="listitem1">The next step is to define one or more RDDs, either by loading a file or programmatically by passing an array of items, referred to parallel collection</li><li class="listitem1">Then more RDDs can be defined by a sequence of transformations, which are tracked and managed by a <strong class="calibre19">lineage graph</strong>. These RDD transformations may be viewed as piped UNIX commands where the output of one command becomes the input to the next command and so on. Each resulting RDD of a <em class="calibre22">transformation</em> step has a pointer to its parent RDD and also has a function for calculating its data. The RDD is acted on only after encountering an <em class="calibre22">action</em> statement. So, the <em class="calibre22">transformations</em> are lazy operations used to define new RDDs and <em class="calibre22">actions</em> launch a computation to return a value to the program or write data to external storage. We will discuss this aspect a little more in the following sections.</li><li class="listitem1">At this stage, Spark creates an execution graph where nodes represent the RDDs and edges represent the transformation steps. Spark breaks the job into multiple tasks to run on separate machines. This is how Spark sends the <strong class="calibre19">compute</strong> to the data across the nodes in a cluster, rather than getting all the data together and computing it.</li></ol></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch02lvl1sec16" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The RDD API</h1></div></div></div><p class="calibre11">The RDD is a read-only, partitioned, fault-tolerant collection of records. From a design perspective, there was a need for a single data structure abstraction that hides the complexity of dealing with a wide variety of data sources, be it HDFS, filesystems, RDBMS, NOSQL data structures, or any other data source. The user should be able to define the RDD from any of these sources. The goal was to support a wide array of operations and let users compose them in any order.</p><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec23" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>RDD basics</h2></div></div></div><p class="calibre11">Each dataset is represented as an object in Spark's programming interface called RDD. Spark provides two ways for creating RDDs. One way is to parallelize an existing collection. The other way is to reference a dataset in an external storage system such as a filesystem.</p><p class="calibre11">An RDD is composed of one or more data sources, maybe after performing a series of transformations including several operators. Every RDD or RDD partition knows how to recreate itself in case of failure. It has the log of transformations, or a <em class="calibre22">lineage</em> that is required to recreate itself from stable storage or another RDD. Thus, any program using Spark can be assured of built-in fault tolerance, regardless of the underlying data source and the type of RDD.</p><p class="calibre11">There are two kinds of methods available on RDDs: transformations, and actions. Transformations are the methods that are used to create RDDs. Actions are the methods that utilize RDDs. RDDs are usually partitioned. Users may choose to persist RDDs that may be reused in their programs.</p><p class="calibre11">RDDs are immutable (read-only) data structures, so any transformation results in the creation of a new RDD. The transformations are applied lazily, only when any action is applied on them, and not when an RDD is defined. An RDD is recomputed every time it is used in an action unless the user explicitly persists the RDD in memory. Saving in memory saves a lot of time. If the memory is not sufficient to accommodate the RDD fully, the remaining portion of that RDD will be stored (spilled) on the hard disk automatically. One advantage of lazy transformations is that it is possible to optimize the transformation steps. For example, if the action is to return the first line, Spark computes only a single partition and skips the rest.</p><p class="calibre11">An RDD may be viewed as a set of partitions (splits) with a list of dependencies on parent RDDs and a function to compute a partition given its parents. Sometimes, each partition of a parent RDD is used by a single child RDD. This is called <em class="calibre22">narrow dependency</em>. Narrow dependency is desirable because when a parent RDD partition is lost, only a single child partition needs to be recomputed. On the other hand, computing a single child RDD partition that involves operations such as <em class="calibre22">group-by-keys</em> depends on several parent RDD partitions. Data from each parent RDD partition in turn is required in creating data in several child RDD partitions. Such a dependency is called <em class="calibre22">wide dependency</em>. In the case of narrow dependency, it is possible to keep both parent and child RDD partitions on a single node (co-partition). But this is not possible in the case of wide dependency because parent data is scattered across several partitions. In such cases, data should be <em class="calibre22">shuffled</em> across partitions. Data shuffling is a resource-intensive operation that should be avoided to the extent possible. Another issue with wide dependency is that all child RDD partitions need to be recomputed even when a single parent RDD partition is lost.</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec24" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Persistence</h2></div></div></div><p class="calibre11">RDDs are computed on the fly every time they are acted upon through an action method. The developer has the ability to override this default behavior and instruct to <em class="calibre22">persist</em> or <em class="calibre22">cache</em> a dataset in memory across partitions. If this dataset is required to participate in several actions, then persisting saves a significant amount of time, CPU cycles, disk I/O, and network bandwidth. The fault-tolerance mechanism is applicable to the cached partitions too. When any partition is lost due to node failure, it is recomputed using a lineage graph. If the available memory is insufficient, Spark gracefully spills the persisted partitions on to the disk. The developer may remove unwanted RDDs using <em class="calibre22">unpersist</em>. Nevertheless, Spark automatically monitors the cache and removes old partitions using <strong class="calibre19">Least Recently Used</strong> (<strong class="calibre19">LRU</strong>) algorithms.</p><div><div><h3 class="title5"><a id="tip4" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Tip</h3><p class="calibre25">
<code class="literal">Cache()</code> is the same as <code class="literal">persist()</code> or <code class="literal">persist (MEMORY_ONLY)</code>. While the <code class="literal">persist()</code> method can have many other arguments for different levels of persistence, such as only memory, memory and disk, only disk, and so on, the <code class="literal">cache()</code> method is designed only for persistence in the memory.</p></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch02lvl1sec17" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>RDD operations</h1></div></div></div><p class="calibre11">Spark programming usually starts by choosing a suitable interface that you are comfortable with. If you intend to do interactive data analysis, then a shell prompt would be the obvious choice. However, choosing a Python shell (PySpark) or Scala shell (Spark-Shell) depends on your proficiency with these languages to some extent. If you are building a full-blown scalable application then proficiency matters a great deal, so you should develop the application in your language of choice between Scala, Java, and Python, and submit it to Spark. We will discuss this aspect in more detail later in the book.</p><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec25" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Creating RDDs</h2></div></div></div><p class="calibre11">In this section, we will use both a Python shell (PySpark) and a Scala shell (Spark-Shell) to create an RDD. Both of these shells have a predefined, interpreter-aware SparkContext that is assigned to a variable <code class="literal">sc</code>.</p><p class="calibre11">Let us get started with some simple code examples. Note that the code assumes the current working directory is Spark's home directory. The following code snippet initiates the Spark interactive shell, reads a file from the local filesystem, and prints the first line from that file:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">&gt; bin/pyspark  // Start pyspark shell  
&gt;&gt;&gt; _         // For simplicity sake, no Log messages are shown here 
 
&gt;&gt;&gt; type(sc)    //Check the type of Predefined SparkContext object 
&lt;class 'pyspark.context.SparkContext'&gt; 
 
//Pass the file path to create an RDD from the local file system 
&gt;&gt;&gt; fileRDD = sc.textFile('RELEASE') 
 
&gt;&gt;&gt; type(fileRDD)  //Check the type of fileRDD object  
&lt;class 'pyspark.rdd.RDD'&gt; 
 
&gt;&gt;&gt;fileRDD.first()   //action method. Evaluates RDD DAG and also returns the first item in the RDD along with the time taken 
took 0.279229 s 
u'Spark Change Log' 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">&gt; bin/Spark-Shell  // Start Spark-shell  
Scala&gt; _      // For simplicity sake, no Log messages are shown here 
 
Scala&gt; sc   //Check the type of Predefined SparkContext object 
res1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@70884875 
 
//Pass the file path to create an RDD from the local file system 
 
Scala&gt; val fileRDD = sc.textFile("RELEASE") 
 
Scala&gt; fileRDD  //Check the type of fileRDD object  
res2: org.apache.spark.rdd.RDD[String] = ../ RELEASE
MapPartitionsRDD[1] at textFile at &lt;console&gt;:21 
 
Scala&gt;fileRDD.first()   //action method. Evaluates RDD DAG and also returns the first item in the RDD along with the time taken 
0.040965 s 
res6: String = Spark Change Log 
</pre><p class="calibre11">In both the preceding examples, the first line has invoked the interactive shell. The SparkContext variable <code class="literal">sc</code> is already defined as expected. We have created an RDD by the name <code class="literal">fileRDD</code> that points to a file <code class="literal">RELEASE</code>. This statement is just a transformation and will not be executed until an action is encountered. You can try giving a nonexistent filename but you will not get any error until you execute the next statement, which happens to be an <em class="calibre22">action</em> statement.</p><p class="calibre11">We have completed the whole cycle of initiating a Spark application (shell), creating an RDD, and consuming it. Since RDDs are recomputed every time an action is executed, <code class="literal">fileRDD</code> is not persisted in the memory or hard disk. This allows Spark to optimize the sequence of steps and execute intelligently. In fact, in the previous example, the optimizer would have just read one partition of the input file because <code class="literal">first()</code> does not require a complete file scan.</p><p class="calibre11">Recall that there are two ways to create an RDD: one way is to create a pointer to a data source and the other is to parallelize an existing collection. The previous examples covered one way, by loading a file from a storage system. We will now see the second way, which is parallelizing an existing collection. RDD creation by passing in-memory collections is simple but may not work very well for large collections, because the input collection should fit completely in the driver node's memory.</p><p class="calibre11">The following example creates an RDD by passing a Python/Scala list with the <code class="literal">parallelize</code> function:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">// Pass a Python collection to create an RDD 
&gt;&gt;&gt; numRDD = sc.parallelize([1,2,3,4],2) 
&gt;&gt;&gt; type(numRDD) 
&lt;class 'pyspark.rdd.RDD'&gt; 
&gt;&gt;&gt; numRDD 
ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:396 
&gt;&gt;&gt; numRDD.first() 
1 
&gt;&gt;&gt; numRDD.map(lambda(x) : x*x).collect() 
[1,4,9,16] 
&gt;&gt;&gt; numRDD.map(lambda(x) : x * x).reduce(lambda a,b: a+b) 
30 
</pre><div><div><h3 class="title5"><a id="tip5" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Tip</h3><p class="calibre25">A lambda function is an unnamed function, typically used as function arguments to other functions. A Python lambda function can be a single expression only. If your logic requires multiple steps, create a separate function and use it in the lambda expression.</p></div></div><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">// Pass a Scala collection to create an RDD 
Scala&gt; val numRDD = sc.parallelize(List(1,2,3,4),2) 
numRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:21 
 
Scala&gt; numRDD 
res15: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:21 
 
Scala&gt; numRDD.first() 
res16: Int = 1 
 
Scala&gt; numRDD.map(x =&gt; x*x).collect() 
res2: Array[Int] = Array(1, 4, 9, 16) 
 
Scala&gt; numRDD.map(x =&gt; x * x).reduce(_+_) 
res20: Int = 30 
</pre><p class="calibre11">As we saw in the previous example, we were able to pass a Scala/Python collection to create an RDD and we also had the liberty to specify the number of partitions to cut those collections into. Spark runs one task for each partition of the cluster, so it has to be carefully decided to optimize the computation effort. Though Spark sets the number of partitions automatically based on the cluster, we have the liberty to set it manually by passing it as a second argument to the <code class="literal">parallelize</code> function (for example, <code class="literal">sc.parallelize(data, 3)</code>). The following is a diagrammatic representation of an RDD which is created with a dataset with, say, 14 records (or tuples) and is partitioned into 3, distributed across 3 nodes:</p><p class="calibre11">
</p><div><img src="img/1-1.jpg" alt="Creating RDDs" class="calibre33"/></div><p class="calibre11">
</p><p class="calibre11">Writing a Spark program usually consists of transformations and actions. Transformations are lazy operations that define how to build an RDD. Most of the transformations accept a single function argument. All these methods convert one data source to another. Every time you perform a transformation on any RDD, a new RDD will be generated, even if it is a small change as shown in the following diagram:</p><p class="calibre11">
</p><div><img src="img/image_02_003.jpg" alt="Creating RDDs" class="calibre34"/></div><p class="calibre11">
</p><p class="calibre11">This is because the RDDs are immutable (read-only) abstractions by design. The resulting output from an action can either be written back to the storage system or it can be returned to the driver program for local computation if needed to produce the final output.</p><p class="calibre11">So far, we have seen some simple transformations that define RDDs and some actions to process them and generate some output. Let us go on a quick tour of some handy transformations and actions followed by transformations on pair RDDs.</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec26" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Transformations on normal RDDs</h2></div></div></div><p class="calibre11">The Spark API includes a rich set of transformation operators, and developers can compose them in arbitrary ways. Try out the following examples on the interactive shell to gain a better understanding of these operations.</p><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec4" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The filter operation</h3></div></div></div><p class="calibre11">The <code class="literal">filter</code> operation returns an RDD with only those elements that satisfy a <code class="literal">filter</code> condition, similar to the <code class="literal">WHERE</code> condition in SQL.</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">a = sc.parallelize([1,2,3,4,5,6], 3) 
b = a.filter(lambda x: x % 3 == 0) 
b.collect() 
[3,6] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val a = sc.parallelize(1 to 10, 3) 
val b = a.filter(_ % 3 == 0) 
b.collect 
 
res0: Array[Int] = Array(3, 6, 9) 
</pre></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec5" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The distinct operation</h3></div></div></div><p class="calibre11">The distinct (<code class="literal">[numTasks]</code>) operation returns an RDD with a new dataset after eliminating duplicates:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">c = sc.parallelize(["John", "Jack", "Mike", "Jack"], 2) 
c.distinct().collect() 
 
['Mike', 'John', 'Jack'] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val c = sc.parallelize(List("John", "Jack", "Mike", "Jack"), 2) 
c.distinct.collect 
res6: Array[String] = Array(Mike, John, Jack) 
 
val a = sc.parallelize(List(11,12,13,14,15,16,17,18,19,20)) 
a.distinct(2).partitions.length      //create 2 tasks on two partitions of the same RDD for parallel execution 
 
res16: Int = 2 
</pre></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec6" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The intersection operation</h3></div></div></div><p class="calibre11">The intersection operation takes another dataset as input. It returns a dataset that contains common elements:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">x = sc.parallelize([1,2,3,4,5,6,7,8,9,10]) 
y = sc.parallelize([5,6,7,8,9,10,11,12,13,14,15]) 
z = x.intersection(y) 
z.collect() 
 
[8, 9, 10, 5, 6, 7] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val x = sc.parallelize(1 to 10) 
val y = sc.parallelize(5 to 15) 
val z = x.intersection(y) 
z.collect 
 
res74: Array[Int] = Array(8, 9, 5, 6, 10, 7) 
</pre></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec7" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The union operation</h3></div></div></div><p class="calibre11">The union operation takes another dataset as input. It returns a dataset that contains elements of itself and the input dataset supplied to it. If there are common values in both sets, then they will appear as duplicate values in the resulting set after union:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">a = sc.parallelize([3,4,5,6,7], 1) 
b = sc.parallelize([7,8,9], 1) 
c = a.union(b) 
c.collect() 
 
[3, 4, 5, 6, 7, 7, 8, 9] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val a = sc.parallelize(3 to 7, 1) 
val b = sc.parallelize(7 to 9, 1) 
val c = a.union(b)     // An alternative way is (a ++ b).collect 
 
res0: Array[Int] = Array(3, 4, 5, 6, 7, 7, 8, 9) 
</pre></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec8" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The map operation</h3></div></div></div><p class="calibre11">The map operation returns a distributed dataset formed by executing an input function on each of the elements in the input dataset:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">a = sc.parallelize(["animal", "human", "bird", "rat"], 3) 
b = a.map(lambda x: len(x)) 
c = a.zip(b) 
c.collect() 
 
[('animal', 6), ('human', 5), ('bird', 4), ('rat', 3)] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val a = sc.parallelize(List("animal", "human", "bird", "rat"), 3) 
val b = a.map(_.length) 
val c = a.zip(b) 
c.collect 
 
res0: Array[(String, Int)] = Array((animal,6), (human,5), (bird,4), (rat,3)) 
</pre></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec9" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The flatMap operation</h3></div></div></div><p class="calibre11">The flatMap operation is similar to the <code class="literal">map</code> operation. While <code class="literal">map</code> returns one element per input element, <code class="literal">flatMap</code> returns a list of zero or more elements for each input element:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">a = sc.parallelize([1,2,3,4,5], 4) 
a.flatMap(lambda x: range(1,x+1)).collect() 
   // Range(1,3) returns 1,2 (excludes the higher boundary element) 
[1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5] 
 
sc.parallelize([5, 10, 20], 2).flatMap(lambda x:[x, x, x]).collect() 
[5, 5, 5, 10, 10, 10, 20, 20, 20] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val a = sc.parallelize(1 to 5, 4) 
a.flatMap(1 to _).collect 
res47: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5) 
 
//One more example 
sc.parallelize(List(5, 10, 20), 2).flatMap(x =&gt; List(x, x, x)).collect 
res85: Array[Int] = Array(5, 5, 5, 10, 10, 10, 20, 20, 20) 
</pre></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec10" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The keys operation</h3></div></div></div><p class="calibre11">The keys operation returns an RDD with the key of each tuple:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">a = sc.parallelize(["black", "blue", "white", "green", "grey"], 2) 
b = a.map(lambda x:(len(x), x)) 
c = b.keys() 
c.collect() 
 
[5, 4, 5, 5, 4] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val a = sc.parallelize(List("black", "blue", "white", "green", "grey"), 2) 
val b = a.map(x =&gt; (x.length, x)) 
b.keys.collect 
 
res2: Array[Int] = Array(5, 4, 5, 5, 4) 
</pre></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec11" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The cartesian operation</h3></div></div></div><p class="calibre11">The <code class="literal">cartesian</code> operation takes another dataset as argument and returns the Cartesian product of both datasets. This can be an expensive operation, returning a dataset of size <code class="literal">m</code> x <code class="literal">n</code> where <code class="literal">m</code> and <code class="literal">n</code> are the sizes of input datasets:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">x = sc.parallelize([1,2,3]) 
y = sc.parallelize([10,11,12]) 
x.cartesian(y).collect() 
 
[(1, 10), (1, 11), (1, 12), (2, 10), (2, 11), (2, 12), (3, 10), (3, 11), (3, 12)] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val x = sc.parallelize(List(1,2,3)) 
val y = sc.parallelize(List(10,11,12)) 
x.cartesian(y).collect 
 
res0: Array[(Int, Int)] = Array((1,10), (1,11), (1,12), (2,10), (2,11), (2,12), (3,10), (3,11), (3,12))  
</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec27" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Transformations on pair RDDs</h2></div></div></div><p class="calibre11">Some Spark operations are available only on RDDs of key value pairs. Note that most of these operations, except counting operations, usually involve shuffling, because the data related to a key may not always reside on a single partition.</p><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec12" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The groupByKey operation</h3></div></div></div><p class="calibre11">Similar to the SQL <code class="literal">groupBy</code> operation, this groups input data based on the key and you can use <code class="literal">aggregateKey</code> or <code class="literal">reduceByKey</code> to perform aggregate operations:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">a = sc.parallelize(["black", "blue", "white", "green", "grey"], 2) 
b = a.groupBy(lambda x: len(x)).collect() 
sorted([(x,sorted(y)) for (x,y) in b]) 
 
[(4, ['blue', 'grey']), (5, ['black', 'white', 'green'])] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val a = sc.parallelize(List("black", "blue", "white", "green", "grey"), 2) 
val b = a.keyBy(_.length) 
b.groupByKey.collect 
 
res11: Array[(Int, Iterable[String])] = Array((4,CompactBuffer(blue, grey)), (5,CompactBuffer(black, white, green))) 
</pre></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec13" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The join operation</h3></div></div></div><p class="calibre11">The join operation takes another dataset as input. Both datasets should be of the key value pairs type. The resulting dataset is yet another key value dataset having keys and values from both datasets:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">a = sc.parallelize(["blue", "green", "orange"], 3) 
b = a.keyBy(lambda x: len(x)) 
c = sc.parallelize(["black", "white", "grey"], 3) 
d = c.keyBy(lambda x: len(x)) 
b.join(d).collect() 
[(4, ('blue', 'grey')), (5, ('green', 'black')), (5, ('green', 'white'))] 
 
//leftOuterJoin 
b.leftOuterJoin(d).collect() 
[(6, ('orange', None)), (4, ('blue', 'grey')), (5, ('green', 'black')), (5, ('green', 'white'))] 
 
//rightOuterJoin 
b.rightOuterJoin(d).collect() 
[(4, ('blue', 'grey')), (5, ('green', 'black')), (5, ('green', 'white'))] 
 
//fullOuterJoin 
b.fullOuterJoin(d).collect() 
[(6, ('orange', None)), (4, ('blue', 'grey')), (5, ('green', 'black')), (5, ('green', 'white'))] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val a = sc.parallelize(List("blue", "green", "orange"), 3) 
val b = a.keyBy(_.length) 
val c = sc.parallelize(List("black", "white", "grey"), 3) 
val d = c.keyBy(_.length) 
b.join(d).collect 
res38: Array[(Int, (String, String))] = Array((4,(blue,grey)), (5,(green,black)), (5,(green,white))) 
 
//leftOuterJoin 
b.leftOuterJoin(d).collect 
res1: Array[(Int, (String, Option[String]))] = Array((6,(orange,None)), (4,(blue,Some(grey))), (5,(green,Some(black))), (5,(green,Some(white)))) 
 
//rightOuterJoin 
b.rightOuterJoin(d).collect 
res1: Array[(Int, (Option[String], String))] = Array((4,(Some(blue),grey)), (5,(Some(green),black)), (5,(Some(green),white))) 
 
//fullOuterJoin 
b.fullOuterJoin(d).collect 
res1: Array[(Int, (Option[String], Option[String]))] = Array((6,(Some(orange),None)), (4,(Some(blue),Some(grey))), (5,(Some(green),Some(black))), (5,(Some(green),Some(white))))  
</pre></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec14" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The reduceByKey operation</h3></div></div></div><p class="calibre11">The reduceByKey operation merges the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer and producing hash-partitioned output:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">a = sc.parallelize(["black", "blue", "white", "green", "grey"], 2) 
b = a.map(lambda x: (len(x), x)) 
b.reduceByKey(lambda x,y: x + y).collect() 
[(4, 'bluegrey'), (5, 'blackwhitegreen')] 
 
a = sc.parallelize(["black", "blue", "white", "orange"], 2) 
b = a.map(lambda x: (len(x), x)) 
b.reduceByKey(lambda x,y: x + y).collect() 
[(4, 'blue'), (6, 'orange'), (5, 'blackwhite')] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val a = sc.parallelize(List("black", "blue", "white", "green", "grey"), 2) 
val b = a.map(x =&gt; (x.length, x)) 
b.reduceByKey(_ + _).collect 
res86: Array[(Int, String)] = Array((4,bluegrey), (5,blackwhitegreen)) 
 
val a = sc.parallelize(List("black", "blue", "white", "orange"), 2) 
val b = a.map(x =&gt; (x.length, x)) 
b.reduceByKey(_ + _).collect 
res87: Array[(Int, String)] = Array((4,blue), (6,orange), (5,blackwhite))  
</pre></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec15" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The aggregate operation</h3></div></div></div><p class="calibre11">The aggregrate operation returns an RDD with the keys of each tuple:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">z = sc.parallelize([1,2,7,4,30,6], 2) 
z.aggregate(0,(lambda x, y: max(x, y)),(lambda x, y: x + y)) 
37 
z = sc.parallelize(["a","b","c","d"],2) 
z.aggregate("",(lambda x, y: x + y),(lambda x, y: x + y)) 
'abcd' 
z.aggregate("s",(lambda x, y: x + y),(lambda x, y: x + y)) 
'ssabsscds' 
z = sc.parallelize(["12","234","345","56789"],2) 
z.aggregate("",(lambda x, y: str(max(len(str(x)), len(str(y))))),(lambda x, y: str(y) + str(x))) 
'53' 
z.aggregate("",(lambda x, y: str(min(len(str(x)), len(str(y))))),(lambda x, y: str(y) + str(x))) 
'11' 
z = sc.parallelize(["12","234","345",""],2) 
z.aggregate("",(lambda x, y: str(min(len(str(x)), len(str(y))))),(lambda x, y: str(y) + str(x))) 
'01' 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">val z = sc.parallelize(List(1,2,7,4,30,6), 2) 
z.aggregate(0)(math.max(_, _), _ + _) 
res40: Int = 37 
 
val z = sc.parallelize(List("a","b","c","d"),2) 
z.aggregate("")(_ + _, _+_) 
res115: String = abcd 
 
z.aggregate("x")(_ + _, _+_) 
res116: String = xxabxcd 
 
val z = sc.parallelize(List("12","234","345","56789"),2) 
z.aggregate("")((x,y) =&gt; math.max(x.length, y.length).toString, (x,y) =&gt; x + y) 
res141: String = 53 
 
z.aggregate("")((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y) 
res142: String = 11 
 
val z = sc.parallelize(List("12","234","345",""),2) 
z.aggregate("")((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y) 
res143: String = 01 
</pre><div><div><h3 class="title6"><a id="note6" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Note</h3><p class="calibre25">Note that in the preceding aggregate examples, the resultant strings (for example, <code class="literal">abcd</code>, <code class="literal">xxabxcd</code>, <code class="literal">53</code>, <code class="literal">01</code>) you get need not match the output shown here exactly. It depends on the order in which the individual tasks return their output.</p></div></div></div></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec28" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Actions</h2></div></div></div><p class="calibre11">Once an RDD has been created, the various transformations get executed only when an <em class="calibre22">action</em> is performed on it. The result of an action can either be data written back to the storage system or returned to the driver program that initiated this for further computation locally to produce the final result.</p><p class="calibre11">We have already covered some of the action functions in the previous examples of transformations. The following are a few more, but there are a lot more that you have to explore.</p><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec16" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The collect() function</h3></div></div></div><p class="calibre11">The <code class="literal">collect()</code> function returns all the results of an RDD operation as an array to the driver program. This is usually useful for operations that produce sufficiently small datasets. Ideally, the result should easily fit in the memory of the system that's hosting the driver program.</p></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec17" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The count() function</h3></div></div></div><p class="calibre11">This returns the number of elements in a dataset or the resulting output of an RDD operation.</p></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec18" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The take(n) function</h3></div></div></div><p class="calibre11">The <code class="literal">take(n)</code> function returns the first (<code class="literal">n</code>) elements of a dataset or the resulting output of an RDD operation.</p></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec19" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The first() function</h3></div></div></div><p class="calibre11">The <code class="literal">first()</code> function returns the first element of the dataset or the resulting output of an RDD operation. It works similarly to the <code class="literal">take(1)</code> function.</p></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec20" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The takeSample() function</h3></div></div></div><p class="calibre11">The <code class="literal">takeSample(withReplacement, num, [seed])</code> function returns an array with a random sample of elements from a dataset. It has three arguments as follows:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">withReplacement</code>/<code class="literal">withoutReplacement</code>: This indicates sampling with or without replacement (while taking multiple samples, it indicates whether to replace the old sample back to the set and then take a fresh sample or sample without replacing). For <code class="literal">withReplacement</code>, argument should be <code class="literal">True</code> and <code class="literal">False</code> otherwise.</li><li class="listitem"><code class="literal">num</code>: This indicates the number of elements in the sample.</li><li class="listitem"><code class="literal">Seed</code>: This is a random number generator seed (optional).</li></ul></div></div><div><div><div><div><h3 class="title4"><a id="ch02lvl3sec21" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The countByKey() function</h3></div></div></div><p class="calibre11">The <code class="literal">countByKey()</code> function is available only on RDDs of type key value. It returns a table of (<code class="literal">K</code>, <code class="literal">Int</code>) pairs with the count of each key.</p><p class="calibre11">The following are some example code snippets on Python and Scala:</p><p class="calibre11">
<strong class="calibre19">Python</strong>:</p><pre class="programlisting">&gt;&gt;&gt; sc.parallelize([2, 3, 4]).count() 
3 
 
&gt;&gt;&gt; sc.parallelize([2, 3, 4]).collect() 
[2, 3, 4] 
 
&gt;&gt;&gt; sc.parallelize([2, 3, 4]).first() 
2 
 
&gt;&gt;&gt; sc.parallelize([2, 3, 4]).take(2) 
[2, 3] 
</pre><p class="calibre11">
<strong class="calibre19">Scala</strong>:</p><pre class="programlisting">Scala&gt; sc.parallelize(List(2, 3, 4)).count() 
res0: Long = 3 
 
Scala&gt; sc.parallelize(List(2, 3, 4)).collect() 
res1: Array[Int] = Array(2, 3, 4) 
 
Scala&gt; sc.parallelize(List(2, 3, 4)).first() 
res2: Int = 2 
 
Scala&gt; sc.parallelize(List(2, 3, 4)).take(2) 
res3: Array[Int] = Array(2, 3)  
</pre></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch02lvl1sec18" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we touched upon the supported programming languages, their advantages and when to choose one language over the other. We discussed the design of the Spark engine along with its core components and their execution mechanism. We saw how Spark sends the data to be computed across many cluster nodes. We then discussed some RDD concepts. We learnt how to create RDDs and perform transformations and actions on them through both Scala and Python. We also discussed some advanced operations on RDDs.</p><p class="calibre11">In the next chapter, we will learn about DataFrames in detail and how they justify their suitability for all sorts of data science requirements.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch02lvl1sec19" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><p class="calibre11">Scala language:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.scala-lang.org">http://www.scala-lang.org</a></li></ul></div><p class="calibre11">Apache Spark architecture:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://lintool.github.io/SparkTutorial/slides/day1_context.pdf">http://lintool.github.io/SparkTutorial/slides/day1_context.pdf</a></li></ul></div><p class="calibre11">The Spark programming guide is the primary resource for concepts; refer to the language-specific API documents for a complete list of operations available:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://spark.apache.org/docs/latest/programming-guide.html">http://spark.apache.org/docs/latest/programming-guide.html</a></li></ul></div><p class="calibre11">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing by Matei Zaharia and others is the original source for RDD basics:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf">https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf">http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf</a></li></ul></div><p class="calibre11">Spark Summit, the official event series of Apache Spark, has a wealth of the latest information. Check out past events' presentations and videos:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://spark-summit.org/2016/">https://spark-summit.org/2016/</a></li></ul></div></div></div>



  </body></html>