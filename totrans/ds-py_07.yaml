- en: 'Chapter 7. Analytics Study: NLP and Big Data with Twitter Sentiment Analysis'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：分析研究：Twitter情感分析与NLP及大数据
- en: '|   | *"Data is the new oil."* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *“数据是新的石油。”* |   |'
- en: '|   | --*Unknown* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*未知* |'
- en: 'In this chapter we are going to look at two important fields of AI and data
    science: **natural language processing** (**NLP**) and big data analysis. For
    the supporting sample application, we re-implement the *Sentiment analysis of
    Twitter hashtags* project described in [Chapter 1](ch01.xhtml "Chapter 1. Programming
    and Data Science – A New Toolset"), *Programming and Data Science – A New Toolset*,
    but this time we leverage Jupyter Notebooks and PixieDust to build live dashboards
    that analyze data from a stream of tweets related to a particular entity, such
    as a product offered by a company, for example, to provide sentiment information
    as well as information about other trending entities extracted from the same tweets.
    At the end of this chapter, the reader will learn how to integrate cloud-based
    NLP services such as *IBM Watson Natural Language Understanding* into their application
    as well as perform data analysis at (Twitter) scale with frameworks such as Apache
    Spark.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探讨人工智能和数据科学的两个重要领域：**自然语言处理**（**NLP**）和大数据分析。为了支持示例应用程序，我们重新实现了[第1章](ch01.xhtml
    "第1章 编程与数据科学——一种新工具集")中描述的*Twitter标签情感分析*项目，*编程与数据科学——一种新工具集*，但这次我们利用Jupyter Notebooks和PixieDust构建实时仪表盘，分析来自与特定实体（例如公司提供的某个产品）相关的推文流中的数据，提供情感信息以及从相同推文中提取的其他趋势实体的信息。在本章的结尾，读者将学习如何将基于云的NLP服务，如*IBM
    Watson自然语言理解*，集成到他们的应用程序中，并使用像Apache Spark这样的框架在（Twitter）规模上执行数据分析。
- en: As always, we'll show how to operationalize the analytics by implementing a
    live dashboard as a PixieApp that runs directly in the Jupyter Notebook.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我们将展示如何通过实现一个作为PixieApp的实时仪表盘，直接在Jupyter Notebook中运行来使分析工作可操作化。
- en: Getting started with Apache Spark
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Apache Spark
- en: 'The term *big data* can rightly feel vague and imprecise. What is the cut-off
    for considering any dataset big data? Is it 10 GB, 100 GB, 1 TB or more? One definition
    that I like is: big data is when the data cannot fit into the memory available
    in a single machine. For years, data scientists have been forced to sample large
    datasets, so they could fit into a single machine, but that started to change
    as parallel computing frameworks that are able to distribute the data into a cluster
    of machines made it possible to work with the dataset in its entirety, provided
    of course that the cluster had enough machines. At the same time, advances in
    cloud technologies made it possible to provision on demand a cluster of machines
    that are adapted to the size of the dataset.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*大数据*这一术语常常给人模糊不清和不准确的感觉。什么样的数据集才算是大数据呢？是10 GB、100 GB、1 TB还是更多？我喜欢的一个定义是：大数据是当数据无法完全装入单个机器的内存时。多年来，数据科学家被迫对大数据集进行抽样处理，以便能够在单台机器上处理，但随着并行计算框架的出现，这些框架能够将数据分布到多台机器的集群中，使得可以在整个数据集上进行工作，当然，前提是集群有足够的机器。与此同时，云技术的进步使得可以按需提供适合数据集大小的机器集群。'
- en: Today, there are multiple frameworks (most of the time available as open source)
    that can provide robust, flexible parallel computing capabilities. Some of the
    most popular include Apache Hadoop ([http://hadoop.apache.org](http://hadoop.apache.org)),
    Apache Spark ([https://spark.apache.org](https://spark.apache.org)) and Dask ([https://dask.pydata.org](https://dask.pydata.org)).
    For our *Twitter Sentiment Analysis* application, we'll use Apache Spark, which
    provides excellent performances in the area of scalability, programmability, and
    speed. In addition, many cloud providers offer some flavor of Spark as a Service
    giving the ability to create on demand an appropriately sized Spark cluster in
    minutes.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有多种框架（大多数通常以开源形式提供）可以提供强大且灵活的并行计算能力。最受欢迎的一些包括Apache Hadoop ([http://hadoop.apache.org](http://hadoop.apache.org))、Apache
    Spark ([https://spark.apache.org](https://spark.apache.org)) 和 Dask ([https://dask.pydata.org](https://dask.pydata.org))。对于我们的*Twitter情感分析*应用程序，我们将使用Apache
    Spark，它在可扩展性、可编程性和速度方面表现出色。此外，许多云服务提供商提供了某种形式的Spark即服务，能够在几分钟内按需创建一个合适大小的Spark集群。
- en: 'Some Spark as a Service cloud providers include:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一些Spark即服务的云服务提供商包括：
- en: 'Microsoft Azure: [https://azure.microsoft.com/en-us/services/hdinsight/apache-spark](https://azure.microsoft.com/en-us/services/hdinsight/apache-spark)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Microsoft Azure: [https://azure.microsoft.com/en-us/services/hdinsight/apache-spark](https://azure.microsoft.com/en-us/services/hdinsight/apache-spark)'
- en: 'Amazon Web Services: [https://aws.amazon.com/emr/details/spark](https://aws.amazon.com/emr/details/spark)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊网络服务：[https://aws.amazon.com/emr/details/spark](https://aws.amazon.com/emr/details/spark)
- en: 'Google Cloud: [https://cloud.google.com/dataproc](https://cloud.google.com/dataproc)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud：[https://cloud.google.com/dataproc](https://cloud.google.com/dataproc)
- en: 'Databricks: [https://databricks.com](https://databricks.com)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks：[https://databricks.com](https://databricks.com)
- en: 'IBM Cloud: [https://www.ibm.com/cloud/analytics-engine](https://www.ibm.com/cloud/analytics-engine)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM Cloud：[https://www.ibm.com/cloud/analytics-engine](https://www.ibm.com/cloud/analytics-engine)
- en: Note
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: Apache Spark can also be easily installed on a local machine for testing
    purposes, in which case, the cluster nodes are simulated using threads.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：Apache Spark也可以轻松地在本地机器上安装用于测试，在这种情况下，集群节点通过线程进行模拟。'
- en: Apache Spark architecture
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Spark架构
- en: 'The following diagram shows the main components of the Apache Spark framework:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了Apache Spark框架的主要组件：
- en: '![Apache Spark architecture](img/B09699_07_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark架构](img/B09699_07_01.jpg)'
- en: Spark high level architecture
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark高层架构
- en: '**Spark SQL**: The core data structure of this component is the Spark DataFrame,
    which enables users who know the SQL language, to effortlessly work with structured
    data.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**：该组件的核心数据结构是Spark DataFrame，使得熟悉SQL语言的用户能够轻松地处理结构化数据。'
- en: '**Spark Streaming**: Module used to work with streaming data. As we''ll see
    later on, we''ll use this module and more specifically Structured Streaming (which
    was introduced in Spark 2.0) in our sample application.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**：用于处理流式数据的模块。正如我们稍后所看到的，我们将在示例应用中使用该模块，特别是Spark 2.0引入的结构化流处理（Structured
    Streaming）。'
- en: '**MLlib**: Module that provides a feature-rich machine learning library that
    works on a Spark scale.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLlib**：提供一个功能丰富的机器学习库，在Spark规模上运行。'
- en: '**GraphX**: Module used for performing the graph-parallel computation.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**：用于执行图并行计算的模块。'
- en: 'There are mainly two ways of working with a Spark cluster as illustrated in
    the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，主要有两种方式可以与Spark集群工作：
- en: '![Apache Spark architecture](img/B09699_07_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Apache Spark架构](img/B09699_07_02.jpg)'
- en: Two ways to work with a Spark cluster
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与Spark集群工作的两种方式
- en: '**spark-submit**: Shell script used to launch Spark applications on a cluster'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**spark-submit**：用于在集群上启动Spark应用的Shell脚本'
- en: '**Notebooks**: Interactively execute code statements against a Spark cluster'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Notebooks**：与Spark集群交互式执行代码语句'
- en: 'Covering the `spark-submit` shell script is beyond the scope of this book,
    but official documentation can be found at: [https://spark.apache.org/docs/latest/submitting-applications.html](https://spark.apache.org/docs/latest/submitting-applications.html).
    For the rest of this chapter, we''ll focus on interacting with the Spark cluster
    via Jupyter Notebooks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不涵盖` spark-submit` shell脚本的内容，但可以在以下网址找到官方文档：[https://spark.apache.org/docs/latest/submitting-applications.html](https://spark.apache.org/docs/latest/submitting-applications.html)。在本章的其余部分，我们将重点介绍通过Jupyter
    Notebooks与Spark集群进行交互。
- en: Configuring Notebooks to work with Spark
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Notebooks以便与Spark一起使用
- en: The instructions in this section only cover installing Spark locally for development
    and testing. Manually installing Spark in a cluster is beyond the scope of this
    book. If a real cluster is needed, it is highly recommended to use a cloud-based
    service.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的说明仅涵盖在本地安装Spark用于开发和测试。手动在集群中安装Spark超出了本书的范围。如果需要真正的集群，强烈建议使用基于云的服务。
- en: 'By default, local Jupyter Notebooks are installed with plain Python Kernels.
    To work with Spark, users must use the following steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，本地Jupyter Notebooks会安装普通的Python内核。为了与Spark一起使用，用户必须执行以下步骤：
- en: Install Spark locally by downloading a binary distribution from [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)下载二进制分发包，安装Spark到本地。
- en: 'Generate a kernel specification in a temporary directory using the following
    command:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在临时目录中生成内核规范：
- en: '[PRE0]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: The preceding command may generate a warning message that can be
    safely ignored as long as the following message is stated:'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**注意**：上述命令可能会生成警告消息，只要显示以下信息，这些警告可以安全忽略：'
- en: '`Installed kernelspec python3 in /tmp/share/jupyter/kernels/python3`'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`已在/tmp/share/jupyter/kernels/python3中安装kernelspec python3`'
- en: 'Go to `/tmp/share/jupyter/kernels/python3`, and edit the `kernel.json` file
    to add the following key to the JSON object (replace `<<spark_root_path>>` with
    the directory path where you installed Spark and `<<py4j_version>>` with the version
    installed on your system):'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到 `/tmp/share/jupyter/kernels/python3`，编辑 `kernel.json` 文件，向 JSON 对象中添加以下键（将
    `<<spark_root_path>>` 替换为你安装 Spark 的目录路径，将 `<<py4j_version>>` 替换为你系统上安装的版本）：
- en: '[PRE1]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You may also want to customize the `display_name` key to make it unique and
    easily recognizable from the Juptyer UI. If you need to know the list of existing
    kernels, you can use the following command:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能还想自定义 `display_name` 键，以使其在 Juptyer 界面中独特且易于识别。如果你需要查看现有内核的列表，可以使用以下命令：
- en: '[PRE2]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding command will give you a list of kernel names and associated paths
    on the local filesystem. From the path, you can open the `kernel.json` file to
    access the `display_name` value. For example:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前述命令将为你提供内核名称和相关路径的列表。从路径中，你可以打开 `kernel.json` 文件，访问 `display_name` 值。例如：
- en: '[PRE3]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Install the kernel with the edited files using the following command:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装带有编辑文件的内核：
- en: '[PRE4]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: Depending on the environment, you may receive a "permission denied" error
    when running the preceding command. In this case, you may want to run the command
    with the admin privileges using `sudo` or use the `--user` switch as follows:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：根据环境不同，你可能在运行前述命令时会遇到“权限拒绝”的错误。在这种情况下，你可能需要使用管理员权限运行该命令，使用 `sudo` 或者按如下方式使用
    `--user` 开关：
- en: '`jupyter kernelspec install --user /tmp/share/jupyter/kernels/python3`'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`jupyter kernelspec install --user /tmp/share/jupyter/kernels/python3`'
- en: 'For more information about install ation options, you can use the `-h` switch.
    For example:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如需了解更多安装选项的信息，可以使用 `-h` 开关。例如：
- en: '[PRE5]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Restart the Notebook server and start using the new PySpark kernel.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重启 Notebook 服务器并开始使用新的 PySpark 内核。
- en: Fortunately, PixieDust provides an `install` script to automate the preceding
    manual steps.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，PixieDust 提供了一个 `install` 脚本来自动化前述的手动步骤。
- en: Note
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find detailed documentation for this script here:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到该脚本的详细文档：
- en: '[https://pixiedust.github.io/pixiedust/install.html](https://pixiedust.github.io/pixiedust/install.html)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://pixiedust.github.io/pixiedust/install.html](https://pixiedust.github.io/pixiedust/install.html)'
- en: 'In short, using the automated PixieDust `install` script requires the following
    command to be issued and the on-screen instructions to be followed:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用自动化 PixieDust `install` 脚本需要发出以下命令并按照屏幕上的说明操作：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We'll dive deeper into the Spark programming model later in this chapter, but
    for now, let's define in the next section, the MVP requirements for our *Twitter
    Sentiment Analysis* application.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本章稍后会深入探讨 Spark 编程模型，但现在让我们在下一节定义我们 *Twitter 情感分析* 应用的 MVP 要求。
- en: Twitter sentiment analysis application
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter 情感分析应用
- en: 'As always, we start by defining the requirements for our MVP version:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们首先定义 MVP 版本的要求：
- en: Connect to Twitter to get a stream of real-time tweets filtered by a query string
    provided by the user
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接 Twitter，获取由用户提供的查询字符串过滤的实时推文流
- en: Enrich the tweets to add sentiment information and relevant entities extracted
    from the text
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丰富推文，添加情感信息和从文本中提取的相关实体
- en: Display a dashboard with various statistics about the data using live charts that
    are updated at specified intervals
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用实时图表显示有关数据的各种统计信息，并在指定的时间间隔内更新图表
- en: The system should be able to scale up to Twitter data size
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统应该能够扩展到 Twitter 数据规模
- en: 'The following diagram shows the first version of our application architecture:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了我们应用架构的第一个版本：
- en: '![Twitter sentiment analysis application](img/B09699_07_03.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![Twitter 情感分析应用](img/B09699_07_03.jpg)'
- en: Twitter sentiment architecture version 1
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 情感分析架构版本 1
- en: For version 1, the application will be entirely implemented in a single Python
    Notebook and will call out to an external service for the NLP part. To be able
    to scale, we will certainly have to externalize some of the processing outside
    of the Notebook, but for development and testing, I found that being able to contain
    the whole application in a single Notebook significantly increases productivity.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个版本，应用将完全在一个 Python Notebook 中实现，并调用外部服务处理 NLP 部分。为了能够扩展，我们肯定需要将一些处理外部化，但对于开发和测试，我发现能够将整个应用封装在一个
    Notebook 中显著提高了生产力。
- en: As for libraries and frameworks, we'll use Tweepy ([http://www.tweepy.org](http://www.tweepy.org))
    for connecting to Twitter, Apache Spark Structured Streaming ([https://spark.apache.org/streaming](https://spark.apache.org/streaming))
    for processing the streaming data in a distributed cluster and the Watson Developer
    Cloud Python SDK ([https://github.com/watson-developer-cloud/python-sdk](https://github.com/watson-developer-cloud/python-sdk))
    to access the IBM Watson Natural Language Understanding ([https://www.ibm.com/watson/services/natural-language-understanding](https://www.ibm.com/watson/services/natural-language-understanding))
    service.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 至于库和框架，我们将使用Tweepy（[http://www.tweepy.org](http://www.tweepy.org)）连接到Twitter，使用Apache
    Spark结构化流处理（[https://spark.apache.org/streaming](https://spark.apache.org/streaming)）处理分布式集群中的流数据，使用Watson
    Developer Cloud Python SDK（[https://github.com/watson-developer-cloud/python-sdk](https://github.com/watson-developer-cloud/python-sdk)）访问IBM
    Watson自然语言理解（[https://www.ibm.com/watson/services/natural-language-understanding](https://www.ibm.com/watson/services/natural-language-understanding)）服务。
- en: Part 1 – Acquiring the data with Spark Structured Streaming
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1部分 – 使用Spark结构化流处理获取数据
- en: 'To acquire the data, we use Tweepy which provides an elegant Python client
    library to access the Twitter APIs. The APIs covered by Tweepy are very extensive
    and covering them in detail is beyond the scope of this book, but you can find
    the complete API reference at the Tweepy official website: [http://tweepy.readthedocs.io/en/v3.6.0/cursor_tutorial.html](http://tweepy.readthedocs.io/en/v3.6.0/cursor_tutorial.html).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取数据，我们使用Tweepy，它提供了一个优雅的Python客户端库来访问Twitter API。Tweepy支持的API非常广泛，详细介绍超出了本书的范围，但你可以在Tweepy官方网站找到完整的API参考：[http://tweepy.readthedocs.io/en/v3.6.0/cursor_tutorial.html](http://tweepy.readthedocs.io/en/v3.6.0/cursor_tutorial.html)。
- en: 'You can install the Tweepy library directly from PyPi using the `pip install`
    command. The following command shows how to install it from a Notebook using the
    `!` directive:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接通过PyPi安装Tweepy库，使用`pip install`命令。以下命令展示了如何通过Notebook使用`!`指令安装：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: The current Tweepy version used is 3.6.0\. Do not forget to restart
    the kernel after installing the library.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：当前使用的Tweepy版本是3.6.0。安装完库后，别忘了重启内核。'
- en: Architecture diagram for the data pipeline
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管道架构图
- en: Before we start diving into each component of the data pipeline, it would be
    good to take a look at its overall architecture and understand the computation
    flow.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解数据管道的每个组件之前，最好先了解其整体架构并理解计算流。
- en: 'As shown in the following diagram, we start by creating a Tweepy stream that
    writes raw data in CSV files. We then create a Spark Streaming DataFrame that
    reads the CSV files and is periodically updated with new data. From the Spark
    Streaming DataFrame, we create a Spark structured query using SQL and store its
    results in a Parquet database:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们首先创建一个Tweepy流，将原始数据写入CSV文件。然后，我们创建一个Spark Streaming数据框，读取CSV文件并定期更新新数据。从Spark
    Streaming数据框中，我们使用SQL创建一个Spark结构化查询，并将其结果存储在Parquet数据库中：
- en: '![Architecture diagram for the data pipeline](img/B09699_07_04.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![数据管道架构图](img/B09699_07_04.jpg)'
- en: Streaming computation flow
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 流计算流程
- en: Authentication with Twitter
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Twitter身份验证
- en: 'Before using any of the Twitter APIs, it is recommended to authenticate with
    the system. One of the most commonly used authentication mechanism is the OAuth
    2.0 protocol ([https://oauth.net](https://oauth.net)) which enables third-party
    applications to access a service on the web. The first thing you need to do is
    acquire a set of key strings that are used by the OAuth protocol to authenticate
    you:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用任何Twitter API之前，建议先进行身份验证。最常用的身份验证机制之一是OAuth 2.0协议（[https://oauth.net](https://oauth.net)），该协议使第三方应用程序能够访问网络服务。你需要做的第一件事是获取一组密钥字符串，这些字符串由OAuth协议用于对你进行身份验证：
- en: '**Consumer key**: String that uniquely identifies the client app (a.k.a. the
    API Key).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者密钥**：唯一标识客户端应用程序的字符串（即API密钥）。'
- en: '**Consumer secret**: Secret string known only to the application and the Twitter
    OAuth server. It can be thought of like a password.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者密钥**：仅应用程序和Twitter OAuth服务器知道的密钥字符串。可以将其视为密码。'
- en: '**Access token**: String used to authenticate your requests. This token is
    also used during the authorization phase to determine the level of access for
    the application.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问令牌**：用于验证请求的字符串。该令牌也在授权阶段用于确定应用程序的访问级别。'
- en: '**Access token secret**: Similar to the consumer secret, this is a secret string
    sent with the access token to be used as a password.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问令牌密钥**：与消费者密钥类似，这是与访问令牌一起发送的密码字符串，用作密码。'
- en: 'To generate the preceding key strings, you need to go to [http://apps.twitter.com](http://apps.twitter.com),
    provide authentication with your regular Twitter user ID and password and follow
    these steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成前面的密钥字符串，您需要访问[http://apps.twitter.com](http://apps.twitter.com)，使用您的常规Twitter用户ID和密码进行身份验证，并按照以下步骤操作：
- en: Create a new Twitter app using the **Create New App** button.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**创建新应用**按钮创建一个新的Twitter应用。
- en: Fill out the application details, agree to the Developer agreement and click
    on **Create your Twitter application** button.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写应用程序详情，同意开发者协议，然后点击**创建您的Twitter应用**按钮。
- en: Tip
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Note**: Make sure that your mobile phone number is added to your profile or
    you''ll get an error when creating the Twitter application.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**注意**：确保您的手机号码已添加到个人资料中，否则在创建Twitter应用时会出现错误。'
- en: You can provide a random URL for the mandatory **Website** input and leave the
    **URL** input blank as this is an optional callback URL.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以为必填项**网站**输入提供一个随机URL，并将**URL**输入留空，因为这是一个可选的回调URL。
- en: Click on the **Keys and Access Tokens** tab to get the consumer and access token.
    At any time, you can regenerate these tokens using the buttons available on this
    page. If you do so, you'll need to also update the value in your application code.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**密钥和访问令牌**标签以获取消费者和访问令牌。您可以随时使用页面上提供的按钮重新生成这些令牌。如果您这么做，您还需要在应用程序代码中更新这些值。
- en: 'For easier code maintenance, let''s put these tokens in their own variables
    at the top of the Notebook and create the `tweepy.OAuthHandler` class that we''ll
    use later on:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易进行代码维护，我们将把这些令牌放在Notebook顶部的单独变量中，并创建我们稍后将使用的`tweepy.OAuthHandler`类：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Creating the Twitter stream
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Twitter流
- en: 'For implementing our application, we only need to use the Twitter streaming
    API that is documented here: [http://tweepy.readthedocs.io/en/v3.5.0/streaming_how_to.html](http://tweepy.readthedocs.io/en/v3.5.0/streaming_how_to.html).
    In this step, we create a Twitter stream that stores the incoming data into CSV
    files on the local filesystem. This is done using a custom `RawTweetsListener`
    class that inherits from `tweepy.streaming.StreamListener`. Custom processing
    of the incoming data is done by overriding the `on_data` method.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我们的应用程序，我们只需要使用这里文档化的Twitter流API：[http://tweepy.readthedocs.io/en/v3.5.0/streaming_how_to.html](http://tweepy.readthedocs.io/en/v3.5.0/streaming_how_to.html)。在此步骤中，我们创建一个Twitter流，将传入的数据存储到本地文件系统中的CSV文件中。通过继承自`tweepy.streaming.StreamListener`的自定义`RawTweetsListener`类完成此操作。通过重写`on_data`方法来处理传入数据的自定义处理。
- en: In our case, we want to transform the incoming data from JSON to CSV using `DictWriter`
    from the standard Python `csv` module. Because the Spark Streaming file input
    source triggers only when new files are created in the input directory, we can't
    simply append the data into an existing file. Instead, we buffer the data into
    an array and write it to disk once the buffer has reached capacity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们希望使用标准Python `csv`模块中的`DictWriter`将传入的JSON数据转换为CSV格式。由于Spark Streaming文件输入源仅在输入目录中创建新文件时触发，因此我们不能简单地将数据追加到现有文件中。相反，我们将数据缓冲到一个数组中，并在缓冲区达到容量时将其写入磁盘。
- en: Note
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For simplicity, the implementation doesn't include cleaning up the files once
    they have been processed. Another minor limitation of this implementation is that
    we currently wait until the buffer is filled to write the file which theoretically
    could take a long time if no new tweets appear.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，实施中没有包括处理完文件后的清理工作。另一个小的限制是，我们目前等待缓冲区填满后再写入文件，理论上如果没有新推文出现，这可能需要很长时间。
- en: 'The code for the `RawTweetsListener` is shown here:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`RawTweetsListener`的代码如下所示：'
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode1.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode1.py)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode1.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode1.py)'
- en: 'A few important things to notice from the preceding code are:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中有几个重要的地方需要注意：
- en: 'Each tweet coming from the Twitter API contains a lot of data, and we pick which
    field to keep using the `field_metadata` variable. We also define a global variable
    `fieldnames` that holds the list of fields to capture from the stream, and a `transforms`
    variable that contains a dictionary with all the field names that have a transform
    function as a key and the transform function itself as a value:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每条来自Twitter API的推文都包含大量数据，我们使用`field_metadata`变量选择保留的字段。我们还定义了一个全局变量`fieldnames`，它保存了要从流中捕获的字段列表，以及一个`transforms`变量，它包含一个字典，字典的键是所有具有变换函数的字段名，值是变换函数本身：
- en: '[PRE10]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode2.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode2.py)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode2.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode2.py)'
- en: 'The CSV files are written in `output_dir` which is defined in its own variable.
    At start time, we first remove the directory and its contents:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV文件被写入定义在自己的变量中的`output_dir`目录。在启动时，我们首先删除该目录及其内容：
- en: '[PRE11]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode3.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode3.py)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode3.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode3.py)'
- en: The `field_metadata` contains the Spark DataType that we'll use later on to build
    the schema when creating the Spark streaming query.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`field_metadata`包含了Spark DataType，我们稍后将在创建Spark流查询时使用它来构建模式。'
- en: 'The `field_metadata` also contains an optional transform `lambda` function
    to cleanse the value before being written to disk. For reference, a lambda function
    in Python is an anonymous function defined inline (see [https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)).
    We use it for the source field that is often returned as an HTML fragment. In
    this lambda function, we use the BeautifulSoup library (which was also used in
    the previous chapter) to extract only the text as shown in the following snippet:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`field_metadata`还包含一个可选的变换`lambda`函数，用于在将值写入磁盘之前清理数据。作为参考，Python中的lambda函数是一个内联定义的匿名函数（请参见[https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)）。我们在此使用它来处理常常以HTML片段形式返回的源字段。在这个lambda函数中，我们使用了BeautifulSoup库（它也在上一章中使用过）来提取只有文本的内容，如以下代码片段所示：'
- en: '[PRE12]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that the `RawTweetsListener` is created, we define a `start_stream` function
    that we''ll use later on in the PixieApp. This function takes an array of search
    terms as input and starts a new stream using the `filter` method:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`RawTweetsListener`已经创建，我们定义了一个`start_stream`函数，稍后将在PixieApp中使用。此函数接受一个搜索词数组作为输入，并使用`filter`方法启动一个新的流：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Notice the `async=True` parameter passed to `stream.filter`. This is needed
    to make sure that the function doesn't block, which would prevent us from running
    any other code in the Notebook.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到传递给`stream.filter`的`async=True`参数。这是必要的，确保该函数不会阻塞，这样我们就可以在Notebook中运行其他代码。
- en: 'You can find the code file here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode4.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode4.py)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode4.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode4.py)'
- en: 'The following code starts the stream that will receive tweets containing the
    word `baseball` in it:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码启动了一个流，它将接收包含单词`baseball`的推文：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When running the preceding code, no output is generated in the Notebook. However,
    you can see the files (that is, `tweets0.csv`, `tweets1.csv`, and so on.) being generated
    in the output directory (that is, `../output/raw`) from the path where the Notebook
    is being run.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行上述代码时，Notebook中不会生成任何输出。然而，你可以在输出目录（即`../output/raw`）中看到生成的文件（如`tweets0.csv`、`tweets1.csv`等），这些文件位于Notebook运行的路径下。
- en: 'To stop the stream, we simply call the `disconnect` method, as shown here:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止流，我们只需调用`disconnect`方法，如下所示：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Creating a Spark Streaming DataFrame
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个 Spark Streaming DataFrame
- en: Referring to the architecture diagram, the next step is to create a Spark Streaming
    DataFrame `tweets_sdf` that uses the `output_dir` as the source file input. We
    can think of a Streaming DataFrame as an unbounded table where new rows are continuously
    added as new data arrives from the stream.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 根据架构图，下一步是创建一个 Spark Streaming DataFrame `tweets_sdf`，该 DataFrame 使用 `output_dir`
    作为源文件输入。我们可以把 Streaming DataFrame 看作一个没有边界的表格，随着新数据从流中到达，新的行会不断被添加进来。
- en: Note
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: Spark Structured Streaming supports multiple types of input source including
    File, Kafka, Socket, and Rate. (Both Socket and Rate are used only for testing.)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：Spark Structured Streaming 支持多种类型的输入源，包括文件、Kafka、Socket 和 Rate。（Socket
    和 Rate 仅用于测试。）'
- en: 'The following diagram is taken from the Spark website and does a great job explaining
    how new data is appended to the Streaming DataFrame:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表摘自 Spark 网站，能够很好地解释新数据是如何被添加到 Streaming DataFrame 中的：
- en: '![Creating a Spark Streaming DataFrame](img/B09699_07_05.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![创建 Spark Streaming DataFrame](img/B09699_07_05.jpg)'
- en: Streaming DataFrame flow
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Streaming DataFrame 流程
- en: 'Source: [https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)'
- en: The Spark Streaming Python API provides an elegant way to create the Streaming
    DataFrame using the `spark.readStream` property which creates a new `pyspark.sql.streamingreamReader`
    object that conveniently lets you chain method calls with the added benefit of
    creating clearer code (see [https://en.wikipedia.org/wiki/Method_chaining](https://en.wikipedia.org/wiki/Method_chaining)
    for more details on this pattern).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming Python API 提供了一种优雅的方式来使用 `spark.readStream` 属性创建 Streaming DataFrame，该属性会创建一个新的
    `pyspark.sql.streamingreamReader` 对象，方便你链式调用方法，并能让代码更加清晰（有关此模式的更多细节，请参见 [https://en.wikipedia.org/wiki/Method_chaining](https://en.wikipedia.org/wiki/Method_chaining)）。
- en: 'For example, to create a CSV file stream, we call the format method with `csv`,
    chain the applicable options and call the `load` method with the path of the directory:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要创建一个 CSV 文件流，我们调用 `format` 方法并传入 `csv`，接着链式调用适用的选项，并通过指定目录路径调用 `load` 方法：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此处找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode5.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode5.py)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode5.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode5.py)'
- en: '`spark.readStream` also provides a convenient high-level `csv` method that
    takes the path as the first argument and keyword arguments for the options:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.readStream` 还提供了一个方便的高阶 `csv` 方法，它将路径作为第一个参数，并为选项提供关键字参数：'
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the code file here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此处找到代码文件：
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode6.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode6.py)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode6.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode6.py)'
- en: 'You can verify that the `csv_sdf` DataFrame is indeed a Streaming DataFrame
    by calling the `isStreaming` method which should return `true`. The following
    code also adds a call to `printSchema` to verify that the schema follows the `field_metadata`
    configuration as expected:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过调用 `isStreaming` 方法来验证 `csv_sdf` DataFrame 是否真的是一个 Streaming DataFrame，返回值应为
    `true`。以下代码还添加了 `printSchema` 方法的调用，以验证 schema 是否按照 `field_metadata` 配置如预期那样：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Returns:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Before continuing to the next step, it is important to understand how the `csv_sdf`
    Streaming DataFrame fits in the Structured Streaming programming model and what
    limitations it has. At its core, the Spark low-level APIs define the **Resilient
    Distributed Dataset** (**RDD**) data structure which encapsulates all the underlying
    complexity of managing the distributed data. Features like fault-tolerance (cluster
    nodes that crashes for any reason are transparently restarted with no intervention
    from the developer) are automatically handled by the framework. There are two
    types of RDD operations: transformations and actions. **Transformations** are
    logical operations on an existing RDD that are not immediately executed on the
    cluster until an action is invoked (lazy execution). The output of a transformation
    is a new RDD. Internally, Spark maintains an RDD acyclic directed graph that keeps
    track of all the lineage resulting in the creation of the RDD, which is useful
    when recovering from server failure. Example transformations include `map`, `flatMap`,
    `filter`, `sample`, and `distinct`. The same goes for transformations on DataFrames
    (which internally are backed by RDDs) that have the benefit of including SQL queries.
    On the other hand, **actions** do not produce other RDDs, but rather perform an
    operation on the actual distributed data to return a non-RDD value. Examples of
    actions include `reduce`, `collect`, `count`, and `take`.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续下一步之前，理解`csv_sdf`流数据框如何适应结构化流编程模型及其局限性非常重要。从本质上讲，Spark的低级API定义了**弹性分布式数据集**（**RDD**）数据结构，它封装了管理分布式数据的所有底层复杂性。像容错（集群节点因任何原因崩溃时，框架会自动重启节点，无需开发者干预）等特性都由框架自动处理。RDD操作有两种类型：转换和动作。**转换**是对现有RDD的逻辑操作，直到调用动作操作时，转换才会在集群上立即执行（懒执行）。转换的输出是一个新的RDD。内部，Spark维护一个RDD有向无环图（DAG），记录所有生成RDD的血统，这在从服务器故障恢复时非常有用。常见的转换操作包括`map`、`flatMap`、`filter`、`sample`和`distinct`。对数据框的转换（数据框在内部由RDD支持）也适用，且它们具有包括SQL查询的优点。另一方面，**动作**不会生成其他RDD，而是对实际分布式数据执行操作，返回非RDD值。常见的动作操作包括`reduce`、`collect`、`count`和`take`。
- en: As mentioned before, `csv_sdf` is a Streaming DataFrame, which means that the
    data is continuously added to it and as such we are only able to apply transformations
    to it, not actions. To circumvent this problem, we must first create a streaming
    query using `csv_sdf.writeStream` which is a `pyspark.sql.streaming.DataStreamWriter`
    object. The streaming query is responsible for sending the results to an output
    sink. We can then run the streaming query using the `start()` method.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`csv_sdf`是一个流式数据框（Streaming DataFrame），这意味着数据会持续被添加到其中，因此我们只能对其应用转换，而不能执行操作。为了解决这个问题，我们必须先使用`csv_sdf.writeStream`创建一个流查询，这是一个`pyspark.sql.streaming.DataStreamWriter`对象。流查询负责将结果发送到输出接收器。然后，我们可以通过`start()`方法运行流查询。
- en: 'Spark Streaming supports multiple output sink types:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming支持多种输出接收器类型：
- en: '**File**: All the classic file formats are supported, including JSON, CSV,
    and Parquet'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文件**：支持所有经典文件格式，包括JSON、CSV和Parquet'
- en: '**Kafka**: Write directly to one or more Kafka topics'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka**：直接写入一个或多个Kafka主题'
- en: '**Foreach**: Run arbitrary computations on each element in the collection'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Foreach**：对集合中的每个元素执行任意计算'
- en: '**Console**: Prints the output to the system console (used mainly for debugging)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制台**：将输出打印到系统控制台（主要用于调试）'
- en: '**Memory**: Output is stored in memory'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存**：输出存储在内存中'
- en: In the next section, we'll create and run a structured query on `csv_sdf` with
    an output sink that stores the output in Parquet format.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将创建并运行一个结构化查询，针对`csv_sdf`使用输出接收器将结果存储为Parquet格式。
- en: Creating and running a structured query
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建并运行结构化查询
- en: Using the `tweets_sdf` Streaming DataFrame, we create a streaming query `tweet_streaming_query`
    that writes the data into a Parquet format using the *append* output mode.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tweets_sdf`流数据框，我们创建一个流查询`tweet_streaming_query`，该查询将数据以*append*输出模式写入Parquet格式。
- en: Note
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Note**: Spark streaming queries support three output modes: **complete**
    where the entire table is written at each trigger, **append** where only the delta
    rows since the last trigger are written, and **update** where only the rows that
    were modified are written.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：Spark流查询支持三种输出模式：**complete**，每次触发时写入整个表；**append**，只写入自上次触发以来的增量行；以及**update**，只写入已修改的行。'
- en: 'Parquet is a columnar database format that provides an efficient, scalable
    storage for distributed analytics. You can find more information about the Parquet
    format at: [https://parquet.apache.org](https://parquet.apache.org).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code creates and starts the `tweet_streaming_query` streaming
    query:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode7.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode7.py)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, you can stop the streaming query by using the `stop()` method as
    follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code, we use the `path` option to specify the location of the
    Parquet files, and the `checkpointLocation` to specify the location of the recovery
    data that would be used in case of a server failure. We also specify the trigger
    interval for new data to be read from the stream and new rows to be added to the
    Parquet database.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'For testing purpose, you can also use the `console` sink to see the new rows
    being read every time a new raw CSV file is generated in the `output_dir` directory:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode8.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode8.py)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: You can see the results in the system output of the master node of your Spark
    cluster (you will need to physically access the master node machine and look at
    the log files, since, unfortunately, the output is not printed into the Notebook
    itself because the operation is executed in a different process. Location of the
    log files depends on the cluster management software; please refer to the specific
    documentation for more information).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are sample results displayed for a particular batch (identifiers have
    been masked):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Monitoring active streaming queries
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When a streaming query is started, cluster resources are allocated by Spark.
    Therefore, it is important to manage and monitor these queries to make sure that
    you don''t run out of cluster resources. At any time, you can get a list of all
    the running queries as shown in the following code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Results:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can then dive into the details of each query by using the following query
    monitoring properties:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '`id`: Returns a unique identifier for the query that persists across restarts
    from checkpoint data'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runId`: Returns a unique ID generated for the current session'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`explain()`: Prints detailed explanations of the query'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recentProgress`: Returns an array of the most recent progress updates'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lastProgress:` Returns the most recent progress'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code prints the most recent progress for each active query:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode9.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode9.py)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Results for the first query are shown here:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As an exercise for the reader, it would be useful to build a PixieApp that provides
    a live dashboard with updated details about each active streaming query.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: We''ll show how to build this PixieApp in *Part 3 – Create a real-time
    dashboard PixieApp*.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Creating a batch DataFrame from the Parquet files
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: For the rest of this chapter, we define a batch Spark DataFrame as
    a classic Spark DataFrame, that is non-streaming.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The last step of this streaming computation flow is to create one or more batch
    DataFrames that we can use for building our analytics and data visualizations.
    We can think of this last step as taking a snapshot of the data for deeper analysis.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to programmatically load a batch DataFrame from a Parquet
    file:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `spark.read` (notice that we don''t use `spark.readStream` as we did earlier):'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Using `spark.sql`:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode10.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode10.py)'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The benefit of this method is that we can use any ANSI SQL query to load the
    data, instead of using the equivalent low-level DataFrame APIs that we would have
    to use in the first method.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then periodically refresh the data by rerunning the preceding code and
    recreating the DataFrame. We are now ready to create further analysis on the data
    by, for example, running the PixieDust `display()` method on it in order to create
    visualizations:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We select the **Bar Chart** menu and drag and drop the `source` field in the
    **Keys** field area. Since we want to show only the top 10 tweets, we set this
    value in the **# of Rows to Display** field. The following screenshot shows the
    PixieDust options dialog:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a batch DataFrame from the Parquet files](img/B09699_07_06.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: Options dialog for showing the top 10 sources of tweets
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'After clicking **OK**, we see the following results:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a batch DataFrame from the Parquet files](img/B09699_07_07.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: Chart showing the number of tweets related to baseball by source
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we've seen how to use the Tweepy library to create a Twitter
    stream, clean the raw data and store it in CSV files, create a Spark Streaming
    DataFrame, run streaming queries on it and store the output in a Parquet database,
    create a batch DataFrame from the Parquet file, and visualize the data using PixieDust
    `display()`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The complete notebook for *Part 1 – Acquiring the data with Spark Structured
    Streaming* can be found here:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%201.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%201.ipynb)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: In the next part, we'll look at enriching the data with sentiment and entity
    extraction using the IBM Watson Natural Language Understanding service.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Part 2 – Enriching the data with sentiment and most relevant extracted entity
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we enrich the Twitter data with sentiment information, for example,
    *positive*, *negative*, and *neutral*. We also want to extract the most relevant
    entity from the tweet, for example, sport, organization, and location. This extra
    information will be analyzed and visualized by the real-time dashboard that we'll
    build in the next section. The algorithms used to extract sentiment and entity
    from an unstructured text belong to a field of computer science and artificial
    intelligence called **natural language processing** (**NLP**). There are plenty
    of tutorials available on the web that provide algorithm examples on how to extract
    sentiment. For example, you can find a comprehensive text analytic tutorial on
    the scikit-learn repo at [https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/working_with_text_data.rst](https://github.com/scikit-learn/scikit-learn/blob/master/doc/tutorial/text_analytics/working_with_text_data.rst).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: However, for this sample application, we are not going to build our own NLP
    algorithm. Instead, we'll choose a cloud-based service that provides text analytics
    such as sentiment and entity extraction. This approach works very well when you
    have generic requirements such as do not require training custom models, but even
    then, most of the service providers now provide tooling to do so. There are major
    advantages to use a cloud-based provider over creating your own model such as
    saving on the development time and much better accuracy and performance. With
    a simple REST call, we'll be able to generate the data we need and integrate it
    into the flow of our application. Also, it would be very easy to change providers
    if needed as the code responsible for interfacing with the service is well isolated.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: For this sample application, we'll use the **IBM Watson Natural Language Understanding**
    (**NLU**) service which is a part of the IBM Watson family of cognitive services,
    and available on IBM Cloud.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the IBM Watson Natural Language Understanding service
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of provisioning a new service is usually the same for every cloud
    provider. After logging in, you go to a service catalog page where you can search for
    a particular service.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'To log in to the IBM Cloud, just go to [https://console.bluemix.net](https://console.bluemix.net)
    and create a free IBM account if you don''t already have one. Once in the dashboard,
    there are multiple ways to search for the IBM Watson NLU service:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Click on the top left-hand menu, and select **Watson**, select **Browse services**,
    and find the **Natural Language Understanding** entry in the list of services.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click on the **Create Resource** button in the top-right corner to get to the
    catalog. Once in the catalog, you can search for `Natural Language Understanding`
    in the search bar as shown in the following screenshot:![Getting started with
    the IBM Watson Natural Language Understanding service](img/B09699_07_08.jpg)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching for Watson NLU in the service catalog
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can then click on **Natural Language Understanding** to provision a new
    instance. It is not unusual that cloud providers offer a free or trial-based plan
    for some services and fortunately Watson NLU provides one of these, with the limitation
    that you can train only one custom model with a maximum of 30,000 NLU items processed
    per month (which is adequate for our sample application). After selecting the
    **Lite** (free) plan and clicking on the **Create** button, the newly provisioned
    instance will appear on the dashboard and is ready to accept requests.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: After creating the service, you may be redirected to the NLU service *getting
    started document*. If so, simply navigate back to the dashboard where you should
    see the new service instance listed.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to test the service from our Notebook by making a REST call.
    Every service provides detailed documentation on how to use it including the API
    reference. From the Notebook, we could use the requests package to make GET, POST,
    PUT, or DELETE calls according to the API reference, but it is highly recommended
    to check whether the service offers SDKs with high-level programmatic access to
    the APIs.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, IBM Watson provides the `watson_developer_cloud` open source library
    which includes multiple open source SDKs supporting some of the most popular languages,
    including Java, Python, and Node.js. For this project, we''ll use the Python SDK
    with source code and code examples located here: [https://github.com/watson-developer-cloud/python-sdk](https://github.com/watson-developer-cloud/python-sdk).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `pip` command installs the `watson_developer_cloud` package directly from
    the Jupyter Notebook:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Notice the `!` in front of the command that signifies that it's a shell command.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: Don''t forget to restart the kernel once installation is complete.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Most cloud service providers use a common pattern to let consumers authenticate
    with the service, which consists of generating a set of credentials from the service
    console dashboard that will be embedded in the client application. To generate
    the credentials, simply click on the **Service credentials** tab of your Watson
    NLU instance and click on the **New credential** button.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'This will generate a new set of credentials in JSON format as shown in the
    following screenshot:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting started with the IBM Watson Natural Language Understanding service](img/B09699_07_09.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: Generating new credentials for the Watson NLU service
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the credentials to our service, we can create a `NaturalLanguageUnderstandingV1`
    object that will provide programmatic access to the REST APIs, as shown in the
    following code:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode11.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode11.py)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: In the preceding code, replace the `XXXX` text with the appropriate
    username and password from the service credentials.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'The `version` argument refers to a specific version of the API. To know the
    latest version, go to the official documentation page located here:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1](https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'Before continuing with building the application, let''s take a moment to understand
    the text analytics capabilities offered by the Watson Natural Language service
    which include:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entities
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concepts
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categories
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emotion
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keywords
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relations
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic roles
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our application, enriching the Twitter data happens in the `RawTweetsListener`
    where we create an `enrich` method that will be invoked from the `on_data` handler
    method. In this method, we call the `nlu.analyze` method with the Twitter data
    and a feature list that includes sentiment and entities only as shown in the following
    code:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: The `[[RawTweetsListener]]` notation means that the following code
    is part of a class called `RawTweetsListener` and that the user should not attempt
    to run the code as is without the complete class. As always, you can always refer
    to the complete notebook for reference.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode12.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode12.py)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: The results are then stored in the `data` object which will be written to the
    CSV files. We also guard against unexpected exceptions skipping the current tweet
    and logging a warning message instead of letting the exception bubble up which
    would stop the Twitter stream.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: The most common exception happens when the tweet data is in a language
    that is not supported by the service.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: We use the `@Logger` decorator described in [Chapter 5](ch05.xhtml "Chapter 5. Python
    and PixieDust Best Practices and Advanced Concepts"), *Python and PixieDust Best
    Practices and Advanced Concepts* to log messages against the PixieDust logging
    framework. As a reminder, you can use the `%pixiedustLog` magic from another cell
    to view the log messages.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'We still need to change the schema metadata to include the new fields as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode13.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode13.py)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we update `on_data` handler to invoke the `enrich` method as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode14.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode14.py)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'When we restart the Twitter stream and create the Spark Streaming DataFrame,
    we can verify that we have the correct schema using the following code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode15.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode15.py)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Which shows the following results as expected:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Similarly, when we run the structured query with the `console` sink, data is
    displayed in batches in the console of the Spark master node as shown here:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, we run the structured query with the Parquet `output` sink, create
    a batch DataFrame, and explore the data using the PixieDust `display()` to show,
    for example, a count of tweets by sentiment (`positive`, `negative`, `neutral`)
    clustered by the entity as shown in the following chart:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting started with the IBM Watson Natural Language Understanding service](img/B09699_07_10.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: Bar chart showing the number of tweets by sentiment clustered by entities
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The complete notebook for *Part 2 – Enrich the data with sentiment and most relevant
    extracted entity* is located here:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%202.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%202.ipynb)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: If you are running it, I encourage you to experiment by adding more fields to
    the schema, run different SQL queries, and visualize the data with PixieDust `display()`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll build a dashboard that displays multiple metrics
    about the Twitter data.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Part 3 – Creating a real-time dashboard PixieApp
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As always, we first need to define the requirements for the MVP version of
    the dashboard. This time we''ll borrow a tool from the agile methodology called
    a **user story** which describes the features we want to build from the perspective
    of the user. The agile methodology also prescribes fully understanding the context
    of the different users that will interact with the software by categorizing them
    into personas. In our case, we will only use one persona: *Frank the marketing
    director who wants to get real-time insights from what consumers are talking about
    on social media*.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'The user story goes like this:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Frank enters a search query like for example a product name
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dashboard is then presented that displays a set of charts showing metrics about
    user sentiments (positive, negative, neutral)
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dashboard also contains a word cloud of all the entities being uttered in the
    tweets
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, the dashboard has an option to display the real-time progress
    of all the Spark Streaming queries that are currently active
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: The last feature is not really needed for Frank, but we show it here
    anyway as an example implementation of the exercise given earlier.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring the analytics into their own methods
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start, we need to refactor the code that starts the Twitter stream
    and creates the Spark Streaming DataFrame into their own method that we will invoke
    in the PixieApp.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'The `start_stream,` `start_streaming_dataframe`, and `start_parquet_streaming_query`
    methods are as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode16.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode16.py)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode17.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode17.py)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode18.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode18.py)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: As part of the preparation work, we also need to manage the life cycle of the
    different streams that will be created by the PixieApp and make sure that the
    underlying resources are correctly stopped when the user restarts the dashboard.
    To help with that, we create a `StreamsManager` class that encapsulates the Tweepy
    `twitter_stream` and the CSV Streaming DataFrame. This class has a `reset` method
    that will stop the `twitter_stream`, stop all the active streaming queries, delete
    all the output files created from the previous queries, and start a new one with
    a new query string. If the `reset` method is called without a query string, then
    we don't start new streams.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: We also create a global `streams_manager` instance that will keep track of the
    current state even if the dashboard is restarted. Since the user can rerun the
    cell that contains the global `streams_manager` we need to make sure that the
    `reset` method is automatically invoked when the current global instance is deleted.
    For that, we override the object's `__del__` method, which is Python's way of
    implementing a destructor and call `reset`.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for `StreamsManager` is shown here:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode19.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode19.py)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Creating the PixieApp
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like in [Chapter 6](ch06.xhtml "Chapter 6. Analytics Study: AI and Image Recognition
    with TensorFlow"), *Analytics Study: AI and Image Recognition with TensorFlow*,
    we''ll use the `TemplateTabbedApp` class again to create a tab layout with two
    PixieApps:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '`TweetInsightApp`: Lets the user specify a query string and shows the real-time
    dashboard associated with it'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StreamingQueriesApp`: Monitors the progress of the active structured queries'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the default route of the `TweetInsightApp`, we return a fragment that asks
    the user for the query string as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode20.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode20.py)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the results of running the preceding code:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: We''ll create the main `TwitterSentimentApp` PixieApp that has the
    tabbed layout and includes this class later on in this section. For now, we are
    only showing the `TweetInsightApp` child app in isolation.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the PixieApp](img/B09699_07_11.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: Welcome screen for the Twitter Sentiment Dashboard
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `Go` button, we invoke the `search_query` route with the query string
    provided by the user. In this route, we first start the various streams and create
    a batch DataFrame stored in a class variable called `parquet_df` from the output
    directory where the Parquet database is located. We then return the HTML fragment
    that is composed of three widgets showing the following metrics:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Bar chart for each of the three sentiments clustered by entities
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line chart subplots showing the distribution of the tweets by sentiment
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A word cloud for the entities
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the widgets is calling a specific route at a regular interval using
    the `pd_refresh_rate` attribute documented in [Chapter 5](ch05.xhtml "Chapter 5. Python
    and PixieDust Best Practices and Advanced Concepts"), *Python and PixieDust Best
    Practices and Advanced Concepts*. We also make sure to reload the `parquet_df`
    variable to pick up the new data that has arrived since the last time. This variable
    is then referenced in the `pd_entity` attribute for displaying the chart.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the implementation for the `search_query` route:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode21.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode21.py)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple things to notice from the preceding code:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The output directory for the Parquet files may not be ready when we try to load
    the `parquet_df` batch DataFrame, which would cause an exception. To solve this
    timing issue, we wrap the code into a `try...except` statement and wait for 5
    seconds using `time.sleep(5)`.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also display the current count of tweets in the header. To do this we add
    a `<div>` element that refreshes every 5 seconds, with a `<pd_script>` that prints
    the current count of tweets using `streams_manager.twitter_stream.listener.tweet_count`
    which is a variable we added to the `RawTweetsListener` class. We also updated
    the `on_data()` method to increment the `tweet_count` variable every time a new
    tweet arrives as shown in the following code:'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Also, to avoid flickering, we prevent the displaying of the *loading spinner*
    image using `class="no_loading_msg"` in the `<div>` element.
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We invoke three different routes (`display_metric1`, `display_metric2`, and
    `display_wc`) that are responsible for displaying the three widgets respectively.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `display_metric1` and `display_metric2` routes are very similar. They return
    a div with `parquet_df` as the `pd_entity` and a custom `<pd_options>` child element
    that contains the JSON configuration passed to the PixieDust `display()` layer.
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following code shows the implementation for the `display_metric1` route:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode22.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode22.py)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: The `display_metric2` route follows a similar pattern but with a different set
    of `pd_options` attributes.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'The last route is `display_wc` and is responsible for displaying the word cloud
    for the entities. This route uses the `wordcloud` Python library that you can
    install with the following command:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Note
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: As always, don''t forget to restart the kernel once installation
    is complete.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `@captureOutput` decorator documented in [Chapter 5](ch05.xhtml
    "Chapter 5. Python and PixieDust Best Practices and Advanced Concepts"), *Python
    and PixieDust Best Practices and Advanced Concepts* as shown here:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Note
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode23.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode23.py)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: The text passed to the `WordCloud` class is generated from collecting all the
    entities in the `parquet_df` batch DataFrame.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the dashboard after letting a Twitter stream,
    created with the search query `baseball`, run for a little while:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the PixieApp](img/B09699_07_12.jpg)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
- en: Twitter Sentiment Dashboard for the search query "baseball"
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'The second PixieApp is used to monitor the streaming queries that are actively
    running. The main route returns an HTML fragment that has a `<div>` element that invokes
    the `show_progress` route at regular intervals (5000 ms) as shown in the following
    code:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Note
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode24.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode24.py)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `show_progress` route we use the `query.lastProgress` monitoring API described
    earlier in this chapter, iterate over the JSON object using Jinja2 `{%for%}` loop
    and display the results in a table as shown in the following code:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode25.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode25.py)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the streaming query monitoring PixieApp:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the PixieApp](img/B09699_07_13.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
- en: Live monitoring of the active Spark streaming queries
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to put together the complete application using the `TemplateTabbedApp`
    class as shown in the following code:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Note
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode26.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode26.py)'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3 of our sample application is now complete; you can find the fully-built
    Notebook here:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%203.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%203.ipynb)'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we discuss ways to make the data pipeline of our application
    more scalable by using Apache Kafka for event streaming and IBM Streams Designer for
    data enrichment of the streaming data.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Part 4 – Adding scalability with Apache Kafka and IBM Streams Designer
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: This section is optional. It demonstrates how to re-implement parts
    of the data pipeline with cloud-based streaming services to achieve greater scalability'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the entire data pipeline in a single Notebook gave us high productivity
    during development and testing. We can experiment with the code and test the changes
    very rapidly with a very small footprint. Also, performances have been reasonable
    because we have been working with a relatively small amount of data. However,
    it is quite obvious that we wouldn't use this architecture in production and the
    next question we need to ask ourselves is where are the bottlenecks that would
    prevent the application from scaling as the quantity of streaming data coming
    from Twitter increases dramatically.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we identify two areas for improvement:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: In the Tweepy stream, the incoming data is sent to the `RawTweetsListener` instance
    for processing using the `on_data` method. We need to make sure to spend as little
    time as possible in this method otherwise the system will fall behind as the amount
    of incoming data increases. In the current implementation, the data is enriched
    synchronously by making an external call to the Watson NLU service; it is then
    buffered and eventually written to disk. To fix this issue, we send the data to
    a Kafka service, which is a highly scalable, fault tolerant streaming platform
    using a publish/subscribe pattern for processing a high volume of data. We also
    use the Streaming Analytics service, which will consume data from Kafka and enrich
    it by invoking the Watson NLU service. Both services are available on the IBM
    Cloud.
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: There are alternative open source frameworks that we could have used
    for processing the streaming data, such as, for example, Apache Flink ([https://flink.apache.org](https://flink.apache.org))
    or Apache Storm ([http://storm.apache.org](http://storm.apache.org)).'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the current implementation, the data is stored as CSV files, and we create a
    Spark Streaming DataFrame with the output directory as the source. This step consumes
    time and resources on the Notebook and the local environment. Instead, we can
    have the Streaming Analytics write back the enriched events in a different topic
    and create a Spark Streaming DataFrame with the Message Hub service as the Kafka
    input source.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the updated architecture for our sample application:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '![Part 4 – Adding scalability with Apache Kafka and IBM Streams Designer](img/B09699_07_14.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
- en: Scaling the architecture with Kafka and Streams Designer
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will implement the updated architecture, starting
    with streaming the tweets to Kafka.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Streaming the raw tweets to Kafka
  id: totrans-427
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Provisioning a Kafka / Message Hub service instance on IBM Cloud follows the
    same pattern as the steps we used to provision the Watson NLU service. We first
    locate and select the service in the catalog, pick a pricing plan and click **Create**.
    We then open the service dashboard and select the **Service credentials** tab
    to create new credentials as shown in the following screenshot:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '![Streaming the raw tweets to Kafka](img/B09699_07_15.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
- en: Creating new credentials for the Message Hub service
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: 'As is the case for all the services available on IBM Cloud, the credentials
    come in the form of a JSON object that we''ll need to store in its own variable
    in the Notebook as shown in the following code (again, don''t forget to replace
    the `XXXX` text with your username and password from the service credentials):'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note
  id: totrans-433
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode27.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode27.py)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: As for interfacing with Kafka, we have a choice between multiple good client
    libraries. I have tried many of them, but the one I ended up using most often
    is `kafka-python` ([https://github.com/dpkp/kafka-python](https://github.com/dpkp/kafka-python))
    which has the advantage of being a pure Python implementation and is thereby easier
    to install.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'To install it from the Notebook, use the following command:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: As always, do not forget to restart the kernel after installing any
    libraries.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: The `kafka-python` library provides a `KafkaProducer` class for writing the
    data as messages into the service, which we'll need to configure with the credentials
    we created earlier. There are multiple Kafka configuration options available and
    going over all of them is beyond the scope of this book. The required options
    are related to authentication, host servers, and API version.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is implemented in the `__init__` constructor of `RawTweetsListener`
    class. It creates a `KafkaProducer` instance and stores it as a class variable:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Note
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode28.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode28.py)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: We configure a lambda function for the `value_serializer` key that serializes
    JSON objects which is the format we'll be using for our data.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: We need to specify the `api_version` key because otherwise, the library
    would try to autodiscover its value which would cause a `NoBrokerAvailable` exception
    to be raised due to a bug in the `kafka-python` library reproducible only on Macs.
    A fix for this bug has not yet been provided at the time of writing this book.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to update the `on_data` method to send the tweets data to Kafka
    using the `tweets` topic. A Kafka topic is like a channel that applications can
    publish or subscribe to. It is important to have the topic already created before
    attempting to write into it otherwise an exception will be raised. This is done
    in the following `ensure_topic_exists` method:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Note
  id: totrans-452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode29.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode29.py)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we make a POST request into the path `/admin/topic` with
    a JSON payload that contains the name of the topic we want to create. The request
    must be authenticated using the API key provided in the credentials and the `X-Auth-Token`
    header. We also make sure to ignore HTTP error codes 422 and 403 which indicate
    that the topic already exists.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the `on_data` method now looks much simpler as shown here:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Note
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode30.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode30.py)'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, with this new code, we're spending as little time as possible
    in the `on_data` method, which is the goal we wanted to achieve. The tweet data
    is now flowing into the Kafka `tweets` topic, ready to be enriched by the Streaming
    Analytics service which we'll discuss in the next section.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Enriching the tweets data with the Streaming Analytics service
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this step, we'll need to use Watson Studio which is an integrated cloud-based
    IDE that provides various tools for working with data, including machine learning
    / deep learning models, Jupyter Notebooks, stream flows, and more. Watson Studio
    is a companion tool to IBM Cloud accessible at [https://datascience.ibm.com](https://datascience.ibm.com),
    and therefore no extra sign up is required.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Once logged in to Watson Studio, we create a new project which we'll call `Thoughtful
    Data Science`.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: It is OK to select the default options when creating a project.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: We then go to the **Settings** tab to create a Streaming Analytics service,
    which will be the engine that powers our enrichment process and associate it with
    the project. Note that we could also have created the service in the IBM Cloud
    catalog as we did for the other services used in this chapter, but since we still
    have to associate it with the project, we might as well do the creation in Watson
    Studio too.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Settings** tab, we scroll to the **Associated services** section and
    click on the **Add service** drop-down to select **Streaming Analytics**. In the
    next page, you have the choice between **Existing** and **New**. Select **New**
    and follow the steps to create the service. Once done, the newly created service
    should be associated with the project as shown in the following screenshot:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: If there are multiple free options, it is OK to pick any one of them.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_16.jpg)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
- en: Associating the Streaming Analytics service with the project
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to create the stream flow that defines the enrichment processing
    of our tweet data.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: We go to the **Assets** tab, scroll down to the **Streams flows** section and
    click on the **New streams flow** button. In the next page, we give a name, select
    the Streaming Analytics service, select **Manually** and click on the **Create**
    button.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now in the Streams Designer which is composed of a palette of operators
    on the left and a canvas where we can graphically build our stream flow. For our
    sample application, we''ll need to pick three operators from the palette and drag
    and drop them into the canvas:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '**Message Hub from the Sources section of the palette**: Input source for our
    data. Once in the canvas, we rename it `Source Message Hub` (by double- clicking
    on it to enter edit mode).'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code from the Processing and analytics section**: It will contain the data
    enrichment Python code that invokes the Watson NLU service. We rename the operator
    to `Enrichment`.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Message Hub from the Targets section of the palette**: Output source for the enriched
    data. We rename it to `Target Message Hub`.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we create a connection between the **Source Message Hub** and **Enrichment**
    and between **Enrichment** and the **Target Message Hub**. To create a connection
    between two operators, simply grab the output port at the end of the first operator
    and drag it to the input port of the other operator. Notice that a source operator
    has only one output port on the right of the box to denote that it only supports
    outgoing connections, while a target operator has only one input port on the left
    to denote that it only supports incoming connections. Any operator from the **PROCESSING
    AND ANALYTICS** section has two ports on the left and right as they accept both
    incoming and outgoing connections.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the fully completed canvas:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_17.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
- en: Tweet enrichment stream flow
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at the configuration of each of these three operators.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: To complete this section, make sure to run the code that generates
    topics to the Message Hub instance that we discussed in the previous section.
    Otherwise, the Message Hub instance will be empty, and no schema will be detected.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the source Message Hub. An animated pane on the right appears with
    the options to select the Message Hub instance that contains the tweets. The first
    time, you''ll need to create a connection to the Message Hub instance. Select
    `tweets` as the topic. Click on the **Edit Output Schema** and then **Detect Schema**
    to have the schema autopopulated from the data. You can also preview the live
    streaming data using the **Show Preview** button as shown in the following screenshot:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_18.jpg)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
- en: Setting the schema and previewing the live streaming data
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: Now select the **Code** operator to implement the code that invokes the Watson
    NLU. The animated contextual right-hand pane contains a Python code editor with
    boilerplate code that includes the required functions to implement, namely `init(state)`
    and `process(event, state)`.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `init` method, we instantiate the `NaturalLanguageUnderstandingV1` instance
    as shown in the following code:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note
  id: totrans-492
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode31.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode31.py)'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: We need to install the `Watson_developer_cloud` library via the **Python
    packages** link located above the Python editor window in the right-hand contextual
    pane as shown in the following screenshot:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_19.jpg)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
- en: Adding the watson_cloud_developer package to the stream flow
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: 'The process method is invoked on every event data. We use it to invoke the
    Watson NLU and add the extra information to the event object as shown in the following
    code:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Note
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode32.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode32.py)'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: We must also declare all output variables by using the **Edit Output Schema**
    link as shown in the following screenshot:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_20.jpg)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
- en: Declaring all output variables for the Code operator
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we configure the target Message Hub to use the `enriched_tweets` topic.
    Note that you'll need to manually create the topic the first time by going into
    the dashboard of the Message Hub instance on the IBM Cloud and clicking on the
    **Add Topic** button.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: We then save the stream flow using the **Save** button in the main toolbar.
    Any errors in the flow, whether it be a compile error in the code, a service configuration
    error or any other errors, will be shown in the notification pane. After we make
    sure that there is no error, we can run the flow using the **Run** button which
    takes us to the streams flow live monitoring screen. This screen is composed of
    multiple panes. The main pane shows the different operators with the data represented
    as little balls flowing in a virtual pipe between operators. We can click on a
    pipe to show the events payload in a pane on the right. This is really useful
    for debugging as we can visualize how the data is transformed through each operator.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-508
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: Streams Designer also supports adding Python logging messages in the
    code operator which can then be downloaded on your local machine for analysis.
    You can learn more about this functionality here:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '[https://dataplatform.cloud.ibm.com/docs/content/streaming-pipelines/downloading_logs.html](https://dataplatform.cloud.ibm.com/docs/content/streaming-pipelines/downloading_logs.html)'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the stream flow live monitoring screen:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: '![Enriching the tweets data with the Streaming Analytics service](img/B09699_07_21.jpg)'
  id: totrans-512
  prefs: []
  type: TYPE_IMG
- en: Live monitoring screen for the Twitter Sentiment Analysis stream flow
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: We now have our enriched tweets flowing in the Message Hub instance using the
    `enriched_tweets` topic. In the next section, we show how to create a Spark Streaming
    DataFrame using the Message Hub instance as the input source.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Spark Streaming DataFrame with a Kafka input source
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this final step, we create a Spark Streaming DataFrame that consumes the
    enriched tweets from the `enriched_tweets` Kafka topic of the Message Hub service.
    For this, we use the built-in Spark Kafka connector specifying the topic we want
    to subscribe to in the `subscribe` option. We also need to specify the list of
    Kafka servers in the `kafka.bootstrap.servers` option, by reading it from the
    global `message_hub_creds` variable that we created earlier.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-517
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: You have probably noticed that different systems use different names
    for this option making it more error prone. Fortunately, in case of a misspelling,
    an exception with an explicit root cause message will be displayed.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding options are for Spark Streaming, and we still need to configure
    the Kafka credentials so that the lower level Kafka consumer can be properly authenticated
    with the Message Hub service. To properly pass these consumer properties to Kafka,
    we do not use the `.option` method, but rather we create a `kafka_options` dictionary
    that we pass to the load method as shown in the following code:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Note
  id: totrans-521
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode33.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode33.py)'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: You would think that we're done with the code at this point since the rest of
    the Notebook should work unchanged from *Part 3 – Create a real-time dashboard
    PixieApp*. This would be correct until we run the Notebook and start seeing exceptions
    with Spark complaining that the Kafka connector cannot be found. This is because
    the Kafka connector is not included in the core distribution of Spark and must
    be installed separately.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, these types of problems which are infrastructural in nature
    and are not directly related to the task at hand, happen all the time and we end
    up spending a lot of time trying to fix them. Searching on Stack Overflow or any
    other technical site usually yields a solution rapidly, but in some cases, the
    answer is not obvious. In this case, because we are running in a Notebook and
    not in a `spark-submit` script, there isn''t much help available, and we have
    to experiment ourselves until we find the solution. To install the `spark-sql-kafka`,
    we need to edit the `kernel.json` file discussed earlier in this chapter, and
    add the following option to the `"PYSPARK_SUBMIT_ARGS"` entry:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: When the kernel restarts, this configuration will automatically download the
    dependencies and cache them locally.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: It should all work now right? Well, not yet. We still have to configure Kafka
    security to use the credentials to our Message Hub service which uses SASL as
    the security protocol. For that, we need to provide a **JAAS** (short for, **Java
    Authentication and Authorization Service**) configuration file that will contain
    the username and password for the service. The latest version of Kafka provides
    a flexible mechanism to programmatically configure the security using a consumer
    property called `sasl.jaas.config`. Unfortunately, the latest version of Spark
    (2.3.0 as of the time of writing) has not yet updated to the latest version of
    Kafka. So, we have to fall back to the other way of configuring JAAS which is
    to set a JVM system property called `java.security.auth.login.config` with the
    path to a `jaas.conf` configuration file.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create the `jaas.conf` in a directory of our choice and add the following
    content to it:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: In the preceding content, replace the `XXXX` text with the username and password
    taken from the Message Hub service credentials.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: 'We then add the following configuration to the `"PYSPARK_SUBMIT_ARGS"` entry
    of `kernel.json`:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'For reference, here is a sample `kernel.json` that contains these configurations:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Note
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode34.json](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/sampleCode34.json)'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: We should always restart the Notebook server when modifying `kernel.json`
    to make sure that all new configurations are properly reloaded.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the Notebook code doesn't change, and the PixieApp dashboard should
    work the same.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-541
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have now completed Part 4 of our sample application; you can find the complete
    notebook here:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%204.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%207/Twitter%20Sentiment%20Analysis%20-%20Part%204.ipynb)'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: 'The extra code we had to write at the end of this section reminds us that the
    journey of working with data is never a straight line. We have to be prepared
    to deal with obstacles that can be different in nature: a bug in a dependency
    library or a limitation in an external service. Surmounting these obstacles doesn''t
    have to stop the project for a long time. Since we''re using mostly open-source
    components, we can leverage a large community of like-minded developers on social
    sites such as Stack Overflow, get new ideas and code samples, and experiment quickly
    on a Jupyter Notebook.'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-545
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've built a data pipeline that analyzes large quantities
    of streaming data containing unstructured text and applies NLP algorithms coming
    from external cloud services to extract sentiment and other important entities
    found in the text. We also built a PixieApp dashboard that displays live metrics
    with insights extracted from the tweets. We've also discussed various techniques
    for analyzing data at scale, including Apache Spark Structured Streaming, Apache
    Kafka, and IBM Streaming Analytics. As always, the goal of these sample applications
    is to show the art of the possible in building data pipelines with a special focus
    on leveraging existing frameworks, libraries, and cloud services.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll discuss time series analysis, which is another great
    data science topic with a lot of industry applications, which we'll illustrate
    by building a *Financial Portfolio* analysis application.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
