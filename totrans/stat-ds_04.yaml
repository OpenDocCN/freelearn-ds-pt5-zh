- en: Data Mining and the Database Developer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces the data developer to mining (not to be confused with
    querying) data, providing an understanding of exactly what data mining is and
    why it is an integral part of data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll provide working examples to help the reader feel comfortable using R
    for the most common statistical data mining methods: dimensional reduction, frequent
    patterns, and sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve broken things into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition and purpose of data mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the developer for data mining rather than data querying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using R for dimensional reduction, frequent patterns, and sequence mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is always prudent to start explaining things with a high-level definition.
  prefs: []
  type: TYPE_NORMAL
- en: Data mining can be explained simply as assembling information concerning a particular
    topic or belief in an understandable (and further useable) format. Keep in mind
    though that the information assembled is not the data itself (as with data querying)
    but information from the data (more on this later in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Data mining should also not be confused with analytics, information extraction,
    or data analysis. Also, it can be manual or by hand, a semi-automatic, or automatic
    process. When working with new data, it will typically be a manual process that
    the data scientist will perform. Later, when working with newer versions of the
    same data (source), it may become automated to some level or degree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data mining is the probing carried out by a data scientist to find previously
    unknown information within the data, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Patterns, such as groups of data records, known as **clusters**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unusual records, known as **anomalies**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependencies in the form of association rules or sequential patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This new information (or insights) can be thought of as a kind of data summary
    and can be used in further analysis or, for example, in machine learning and predictive
    analytics. For example, with data mining, a data scientist might identify various
    groups that can then be used to obtain more accurate prediction results by a decision
    support system.
  prefs: []
  type: TYPE_NORMAL
- en: Data developers can liken the insights derived from data mining to descriptive
    or structural metadata, which is understood within the industry as data that defines
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Once the probing is completed and the data scientist has mined the information,
    that information must then be transformed into a comprehensible and usable structure,
    given the objectives of the effort.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection, data preparation, results from interpretation and visualization,
    and reporting is not part of data mining.
  prefs: []
  type: TYPE_NORMAL
- en: Common techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some of the most common and widely accepted and used data mining (statistical)
    analysis methods are explained in the following sub-sections.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualization of averages, measures of variation, counts, percentages, cross-tabbing,
    and simple correlations help the data scientists in understanding the structure
    of the data. This is also referred to as **data profiling**.
  prefs: []
  type: TYPE_NORMAL
- en: Area, temporal, multidimensional, and hierarchical are typical, commonly used,
    and easily understood formats for data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster analysis is used by the data scientists to place data variables into
    defined collections (that is, clusters) as a way of summarizing the data. Clusters
    should be both internally homogeneous (the variables are like one another) as
    well as externally heterogeneous (the variables are not like members of other
    clusters).
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical agglomerative, partitioning, and model-based are all very common
    methods of cluster analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Correlation analysis is a method where the data scientists measure the relationship
    between two data variables. This results in something called a **correlation coefficient**,
    which shows if changes to one variable (the independent variable) will result
    in changes to the other (the dependent variable).
  prefs: []
  type: TYPE_NORMAL
- en: Common correlation approaches are positive/negative and linear and non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminant analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discriminant analysis is used when there is no obvious natural ordering of groups
    to determine if a data variable is a member. With this method, there are predetermined
    groups with specific scores or measures that are used in the classification or
    grouping of the data variables process.
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear discriminant analysis** (**LDA**) is one of the most common approaches
    to discriminate analysis where the data scientist attempts to find a linear combination
    of features that characterize or separates a data variable (into groups).'
  prefs: []
  type: TYPE_NORMAL
- en: Factor analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Factor analysis is useful for understanding the reasons for the associations
    (amongst a group of data variables). The main goal is to try and reduce the number
    of variables and to detect structure in the relationships among them (this method
    also results in an overall data reduction).
  prefs: []
  type: TYPE_NORMAL
- en: Types of factor analysis used by data scientists are the principal component,
    common, image, alpha, and factor regression.
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression analysis uses the relationship between two or more quantitative variables
    so that one variable (dependent variable) can be predicted from the other(s) (independent
    variables).
  prefs: []
  type: TYPE_NORMAL
- en: There are many kinds of regression analysis, including simple linear, multiple
    linear, curvilinear, and multiple curvilinear, as well as logistic regression
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A logistic analysis is a method that is used when the response variable is binary
    or qualitative and attempts to find a best fitting equation using a maximum likelihood
    method to maximize the probability of obtaining the observed results given the
    fitted regression coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the common flavours that logistic regression comes in include simple,
    multiple, polytomous, and Poisson logistic regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Purpose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through the practice of data mining, a data scientist can achieve (the aforementioned)
    goal of deriving actionable information from long information or data.
  prefs: []
  type: TYPE_NORMAL
- en: Some have said that the objective of data mining is to discover structure inside
    unstructured (data). For example, you might use data mining to identify customer
    segments to design a promotion targeting high-value customers or an inventory
    control plan to ensure short product shelf life.
  prefs: []
  type: TYPE_NORMAL
- en: One might confuse data querying with data mining. But if we consider the example
    of generating a plan for controlling inventory at a newly opened home improvement
    store, then by simply querying sales transactions to determine the fastest selling
    products from the past few months (from other store locations), we might not be
    successful. Mining demographical information might, however, yield better results,
    as we might identify valid novel, potentially useful, and understandable correlations
    and patterns in the data, which can then be used to predict local consumer purchasing.
    In other words, the objective or purpose of data mining is not reporting but uncovering.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll take a closer look at the differences between data
    mining and data querying.
  prefs: []
  type: TYPE_NORMAL
- en: Mining versus querying
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data querying is the process of asking specific, structured questions of data
    in search of a specific answer, while data mining is the process of sifting through
    data to identify patterns and relationships using statistical algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following matrix may help the data developers in gaining an understanding
    of the differences between data querying and data mining:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Example** | **Data querying or mining** |'
  prefs: []
  type: TYPE_TB
- en: '| What was the total number of technical books sold last month worldwide? |
    Data querying |'
  prefs: []
  type: TYPE_TB
- en: '| What factors affected the type of technical books sold worldwide last month?
    | Data mining |'
  prefs: []
  type: TYPE_TB
- en: '| How many different technologies'' technical books were sold last quarter?
    | Data querying |'
  prefs: []
  type: TYPE_TB
- en: '| Which technologies were purchased as part of a set? | Data mining |'
  prefs: []
  type: TYPE_TB
- en: '| Does a technology tend to be purchased in hardcopy or electronic version?
    | Data mining |'
  prefs: []
  type: TYPE_TB
- en: '| Which technical books have repeat customers? | Data mining |'
  prefs: []
  type: TYPE_TB
- en: '| Which is the most sold technical book overall? | Data querying |'
  prefs: []
  type: TYPE_TB
- en: Once again, data querying is about reporting the results of events while data
    mining is the process of identifying relationships that may help to understand
    which factors affected the outcome of those events, or they may be used to predict
    future outcomes of similar events.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing R for data mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although there are many good options to choose from, R is a language and environment
    that has a short learning curve, is very flexible in nature, and is also very
    focused on statistical computing, making it great for manipulating, cleaning,
    summarizing, producing probability statistics, and so on (as well as actually
    creating visualizations with your data); thus, it's a great choice for the exercises
    data mining.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, here are a few more reasons to learn and use R for data mining
    projects:'
  prefs: []
  type: TYPE_NORMAL
- en: R is used by a large number of academic statisticians, so it's a tool that is
    not going away
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R is pretty much platform independent; what you develop will run almost anywhere
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R has awesome help resources. Just google it and you'll see!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To illustrate, we'll explore a few practical data mining examples using the
    R programming language throughout the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin, let''s look at creating a simple visualization of our data, using
    R. In this use case scenario, we have data collected from a theoretical hospital,
    whereupon admission, patient medical history information is collected through
    an online survey. Information is also added to a patient''s file as treatment
    is provided. The file includes many fields, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic descriptive data for the patient, such as sex, date of birth, height,
    weight, blood type, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vital statistics, such as blood pressure, heart rate, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical history, such as number of hospital visits, surgeries, major illnesses
    or conditions, is currently under a doctor's care, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demographical statistics, such as occupation, home state, educational background,
    and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some additional information is also collected in the file to develop patient
    characteristics and habits, such as the number of times the patient included beef,
    pork, and fowl in his or her weekly diet, if he or she typically uses a butter
    replacement product, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming we have been given no further information about the data (other than
    a brief field name list and the knowledge that the data is captured by hospital
    personnel upon patient admission), the next step would be to perform some data
    mining, that is identifying or grouping data and perhaps locating relationships
    between the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, we can read out hospital survey data into an R data frame and
    then use two available R functions to reveal information about our file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/950b0084-7b4d-4367-949b-3883f928494a.png)'
  prefs: []
  type: TYPE_IMG
- en: The code shown here reads our text file (named `Chapter4.txt`) into an R data
    frame (also named `chapter4`) and then uses the functions `dim` and `names`. The
    `dim` function shows us the file's data structure (there are `5994` records or
    cases in this file and `107` data points or variables, as shown in the screenshot
    we just saw). The `names` function simply lists all the field or variable names
    in our file (partially shown in the screenshot we just saw).
  prefs: []
  type: TYPE_NORMAL
- en: R function attributes and `str` are also some very useful R data mining functions
    and would be worth the readers' time to investigate and experiment with further.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, a data scientist might begin by looking through the field names for
    some ideas to start with; perhaps the common groups, such as sex, age, and state
    (insured is also a pretty interesting attribute these days!).
  prefs: []
  type: TYPE_NORMAL
- en: Current smokers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, a data scientist has an objective in mind when performing data mining.
    So in this example, let''s suppose we are interested in grouping patients who
    are smokers into age groups. Using the variable `current_smoker`, we can use the
    R table function and run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31da38d7-af84-4551-8b2a-015d5a9b1d39.png)'
  prefs: []
  type: TYPE_IMG
- en: From the results shown here, it seems like we have more non-smokers (`5466`)
    than smokers (`528`), at least in this file or population.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, what we''d like to see (that is, visualize) is the smoker patients in
    our population organized into (or by) age groups. To do this, a logical next step
    as a data scientist would be to understand the `range` of values present in the
    `age` variable. In other words, part of our data mining effort will be to see
    the age of the youngest patient, as well as the age of the oldest patient, within
    our population. Rather than having to slice and dice the data to find this information,
    we can use the R range function, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63fac424-9842-4140-be51-90b52d3dac33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From these results, the data scientist can now see that we have cases with
    patient ages ranging from 1 to 99 years! Another good idea would be to visualize
    the frequency of the ages of our patients. The data scientist might want to again
    use the R table function to create a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/876062c3-b0f2-4597-9d7c-f9ce1fe997df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This R code will generate the following visualization, which provides, even
    more, visibility to our patients'' ages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19b94e0e-09b7-4773-bdac-4cd0b846e07f.png)'
  prefs: []
  type: TYPE_IMG
- en: Another interesting bit of information is density estimation. Without much effort,
    we can nest the three R functions, `plot`, `density`, and `table`, to create another
    visual of patient age.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66d1d045-173a-4da2-84d4-132131283177.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This will generate the following visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a607895f-588a-421b-995d-d3d159b5078a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Given all this new-found knowledge, perhaps the data scientist will want to
    go ahead and group our cases into six distinct age groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Under 22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 22 to 34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 35 to 44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 45 to 54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 55 to 64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 65 and older
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To begin speaking the language of the data scientist, the data developer should
    start using the word cases rather than records (in the file) and population rather
    than the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following R program code groups our cases into current smokers by their
    recorded age and then creates a simple pie chart to visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is our simple pie chart generated by using the R `pie` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/845383f3-5fb7-4068-b846-c48a87672a67.png)'
  prefs: []
  type: TYPE_IMG
- en: Missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Critical to the outcome of any analysis is the availability of data.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose there are cases within our population that have values that are missing.
    You may want to ignore (or omit) those cases in your analysis. Rather than spending
    time writing code to deal with these cases, you can use the handy R generic function
    `na`. The `na.omit` function evaluates each case in your file and if it is missing
    any values for any of the variables, it drops that case automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows the use of the R functions `na.omit` and `nrow`
    on a file with missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87268f9b-8acb-49d2-a27a-671ee5bc0956.png)'
  prefs: []
  type: TYPE_IMG
- en: Note the row (case) count before and after the use of `na.omit` (five records
    were dropped).
  prefs: []
  type: TYPE_NORMAL
- en: I've overwritten the object `Chapter4` with the updated file; in reality, it
    is a good habit to create a new object so that you have an audit of your data
    before and after any processing.
  prefs: []
  type: TYPE_NORMAL
- en: A cluster analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this next example, the data scientist wants to take a closer look at our
    cases, but only those that are smokers. So, with R, let''s first create a subset
    of our original cases, including only those cases that are current smokers. As
    we did in the previous example, after we create our subset (named `mysub`), we
    will use the R `nrow` function to verify the number of records in our new population,
    so that we can get an idea of the number of cases in our new population:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d05eac96-f095-4229-98cf-502e1c5fefc9.png)'
  prefs: []
  type: TYPE_IMG
- en: As per the output we just saw, there are still over 500 cases in our new population.
    So, as a data scientist, we make the decision to pull a random sample of our cases
    (which we will then perform a cluster analysis on) and then, again, validate the
    number of records in our newest population.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the R `sample` command to create a sample of just 30 cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9fd427d2-4235-486e-9f54-b6b0c9f402f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, our data scientist feels that he now has a small enough sampling of
    our cases that is easy enough to work with, so let's go ahead and perform a cluster
    analysis with it. As mentioned earlier in this chapter, hierarchical agglomerative
    is one of the most popular cluster analysis techniques, and so we will use it
    with our random sample of cases.
  prefs: []
  type: TYPE_NORMAL
- en: We can perform the hierarchical agglomerative cluster analysis of our random
    sample of cases using a combination of the R's `dist` and `hclust` functions.
    The `dist` function calculates a distance matrix for your dataset, giving the
    Euclidean distance between any two observations. The `hclust` function performs
    hierarchical clustering on that distance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, the easiest way to review and understand the results of a hierarchical
    cluster analysis is through visualization of the results. This visualization is
    known as a **dendrogram** (a tree diagram frequently used to illustrate the arrangement
    of the clusters), so we''ll add that code as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48739aaa-fd9f-4b8a-b6fe-1c04ef942a5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This code sample creates the following visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ddf3744-4d87-44f5-a1f1-ae718bbb2ab1.png)'
  prefs: []
  type: TYPE_IMG
- en: R offers a long list of options for creating rich visualizations based upon
    data and statistics. It is important for a data scientist to be familiar with
    these options and, perhaps more importantly, understand which type of visualization
    best fits the objective of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensional reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is intended to group data variables that are found to be interrelated,
    based on observations of their attributes' values. However, given a scenario with
    a large number of attributes, the data scientist will find that some of the attributes
    will usually not be meaningful for a given cluster. In the example, we used earlier
    in this chapter (dealing with patient cases), we could have found this situation.
    Recall that we performed a hierarchical cluster analysis on smokers only. Those
    cases include many attributes, such as, sex, age, weight, height, no_hospital_visits,
    heartrate, state, relationship, Insurance blood type, blood_pressure, education,
    date of birth, current_drinker, currently_on_medications, known_allergies, currently_under_doctors_care,
    ever_operated_on, occupation, heart_attack, rheumatic_fever, heart_murmur, diseases_of_the_arteries,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, you can use the R function `names`, as we did earlier in
    this chapter, to see the complete list of attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensional reduction is a process where the data scientist attempts to reduce
    or limit the number of attributes, or dimensions, within a case. This is referred
    to as reducing the number of random variables under consideration, but what that
    translates to is simply removing columns from a data file, based upon scientific
    theory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently accepted and commonly used approaches to eliminating dimensions include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Missing data: If a variable (column) has many cases (records) with no values,
    it is not going to add much value; therefore, the column can be removed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, earlier in this chapter, we used the R function `na.omit`. This function
    comes in handy for removing entire cases; however, with dimensional reduction,
    we want to omit the entire variable for all cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Little variance: Like a variable having a great number of missing values, variables
    with little variation do not add value and can be removed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'High correlation: Data columns with very similar trends are also likely to
    carry very similar information. In this case, only one of them is needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decision trees: It is a technique that may require a bit more work. It is an
    approach to dimensionality reduction where the data scientist generates a set
    of decision trees against a target attribute and then uses each attribute''s usage
    statistics to find the most informative features (or columns). The columns with
    the lowest statistics may be dropped.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**): It is a process that transforms
    variables in a dataset into a new set of variables called **principal components**.
    The components are ordered by the variables'' possible variance and only those
    with the highest variance are kept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Backward elimination and forward construction: These techniques involve focusing
    on one or more variables, and sequentially removing or adding one additional variable
    at a time and observing the effect. Backward elimination measures effect with
    a tolerable error rate, while forward construction measures by the effect on performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating statistical significance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now look at a simple example of using a data variables calculated variance
    to determine if it should be removed from an analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Again, using our same patient cases example that we've used throughout this
    chapter, we can use the R function `var` to determine the statistical significance
    of the variables within our population.
  prefs: []
  type: TYPE_NORMAL
- en: The R function `var` only works with numeric values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next code, we use the R `var` function to calculate the variance of
    the variable named:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that it has a low variance percentage (it doesn''t vary often or
    is not found to have many different values, case by case):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75c4e61f-31b4-4065-b509-fb90f93a9c70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we look at the calculated variance percentage of yet another variable, this
    one named: `No_servings_per_week_regular_or_diet_soda`, we see that it has a higher
    calculated variance (than the previous variable):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/361f94a8-c1d3-454e-83d2-682bd8f1d3c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And finally, if we look at a third variable, this one named `No_servings_per_week_water`,
    we get a third calculated variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cb8fc1e-7e8f-4461-8ea9-dca10adf3eaf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From these individual variance calculations, we can see how statistically significant
    each of these variables may be in our case analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data Variable** | **Calculated Variance** |'
  prefs: []
  type: TYPE_TB
- en: '| `No_servings_per_week_skim_milk` | .003160316 |'
  prefs: []
  type: TYPE_TB
- en: '| `No_servings_per_week_regular_or_diet_soda` | 8.505655 |'
  prefs: []
  type: TYPE_TB
- en: '| `No_servings_per_week_water` | 24.10477 |'
  prefs: []
  type: TYPE_TB
- en: The data variable named `No_servings_per_week_skim_milk` could be certainly
    eliminated from the analysis, and depending upon our data scientists tolerance
    levels, possibly the data variable named `No_servings_per_week_regular_or_diet_soda`
    could be eliminated from our analysis as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using simple R functions, we can visualize our calculated variance data for
    a better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a95c2d43-5c75-44d5-ac74-b9dad173e743.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we generate the following visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ba20498-03cb-40ec-9a11-647e77adff0f.png)'
  prefs: []
  type: TYPE_IMG
- en: When we eliminate a variable, it is eliminated from all cases within our population.
  prefs: []
  type: TYPE_NORMAL
- en: Frequent patterning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To gain an understanding of statistical patterning, let us begin with thinking
    about what happens when an urban area is threatened by severe weather and potentially
    hazardous travelingâ€”all the local stores sell out of bread, milk, and eggs!
  prefs: []
  type: TYPE_NORMAL
- en: Patterning (which is a subfield of data mining) is the process of looking through
    data in an effort to identify previously unknown but potentially useful patterns
    consisting of frequently co-occurring events (such as the stormy weather event
    triggering the sale of bread, milk, and eggs) or objects (such as the products
    bread, milk, and eggs being typically purchased together or bundled together in
    the same shopping cart).
  prefs: []
  type: TYPE_NORMAL
- en: Pattern mining is the process that consists of using or developing custom pattern
    mining logic. This logic might be applied to various types of data sources (such
    as transaction and sequence databases, streams, strings, spatial data, graphs,
    and so on) in an effort to look for various types of patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a higher level, data scientists look for:'
  prefs: []
  type: TYPE_NORMAL
- en: Interesting patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequent patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rare patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterns with a high confidence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The top patterns, and others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the more specific types of patterns that may exist in data include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Subgraphs**: The discovery of an interesting graph(s) within a graph or in
    a set of graphs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct and indirect associations**: Identifying couplings or dependencies
    between objects or events; either implicit or explicit defined'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trends**: This is also sometimes known as trend analysis, and is the practice
    of collecting seemingly unrelated information and attempting to spot a pattern'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Periodic patterns**: This is defined as a trend or change in the character
    of an element, either across a period or group'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential rules**: This is an add-on to sequential pattern mining, taking
    into account the probability that an identified pattern will be followed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lattices**: A partially ordered set in which every two elements have a unique
    least upper bound and a unique greatest lower bound'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential patterns**: A subsequence that appears in several sequences of
    a data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-utility patterns**: High utility patterns are those patterns that have
    been determined to have higher, greater, or equal to a threshold value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequent item-setting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on the idea in the previous section (of finding frequent patterns)
    is frequent item-setting. To the data developer, by far the most applicable patterning
    concept is that of frequent item-setting or finding items that are found to be
    frequently a member of a group or set.
  prefs: []
  type: TYPE_NORMAL
- en: Using our stormy weather example from earlier in this chapter, one can envision
    the process of searching through a file or database of sales transactions, looking
    for the occasion (that is, the event) where milk, bread, and eggs were purchased
    together as one sale (or one set of products).
  prefs: []
  type: TYPE_NORMAL
- en: Frequent item setting also involves determining a minsup or minimum supported
    threshold to be used within an analysis. What this means is that the data scientist
    will determine the minimum occurrence of items that constitute a set.
  prefs: []
  type: TYPE_NORMAL
- en: Again, going back to our stormy weather example, if the data scientist sets
    a minsup of two to be used, then sales, where just two of the member products
    exist, would be considered a set or pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider the following sales transactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sales ID** | **Items Purchased** | **Qualifies (as a Frequent Item set)**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sale 1 | Milk, Bread, Eggs | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Sale 2 | Milk, Potatoes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Sale 3 | Bread, Eggs, Tea | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Sale 4 | Eggs, Orange Juice | No |'
  prefs: []
  type: TYPE_TB
- en: The most known algorithm for pattern mining, without a doubt, is Apriori, designed
    to be applied to a transaction database to discover patterns in transactions made
    by customers in stores. This algorithm takes as input a minsup threshold set by
    the user and a transaction database containing a set of transactions and outputs
    all the frequent item-sets.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sequence mining evolves the preceding concepts even further. This is a process
    that the data scientist uses to discover a set of patterns that are shared among
    objects but which also have between them a specific order.
  prefs: []
  type: TYPE_NORMAL
- en: With sequence mining, we acknowledge that there are sequence rules associated
    with identified sequences. These rules define the pattern's objects and order.
    A sequence can have multiple rules. The support of a sequence rule can be calculated
    or determined by the data scientist by the number of sequences containing the
    rule divided by the total number of sequences. The confidence of a sequence rule
    will be the number of sequences containing the rule divided by the number of sequences
    containing its antecedent.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the objective of sequential rule mining is to discover all sequential
    rules having a support and confidence no less than two thresholds, given by the
    user named minsup and minconf.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided a universal definition for data mining, listed
    the most common techniques used by data scientists, and stated the overall objective
    of the efforts. Data mining was also compared to data querying and, using R, various
    working examples were given to illustrate certain key techniques. Finally, the
    concepts of dimensional reduction, frequent patterning, and sequence mining were
    explored.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will be a hands-on introduction to statistical analysis of
    data through the eyes of a data developer, providing instructions for describing
    the nature of data, exploring relationships presented in data, creating a summarization
    model from data, proving the validly of a data model, and employing predictive
    analytics on a data developed model.
  prefs: []
  type: TYPE_NORMAL
