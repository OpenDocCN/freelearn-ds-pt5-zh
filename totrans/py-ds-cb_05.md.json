["```py\nimport numpy as np\n\ndef euclidean_distance(x,y):\n    if len(x) == len(y):\n        return np.sqrt(np.sum(np.power((x-y),2)))\n    else:\n        print \"Input should be of equal length\"\n    return None\n\ndef lrNorm_distance(x,y,power):\n    if len(x) == len(y):\n        return np.power(np.sum (np.power((x-y),power)),(1/(1.0*power)))\n    else:\n        print \"Input should be of equal length\"\n    return None\n\ndef cosine_distance(x,y):\n    if len(x) == len(y):\n        return np.dot(x,y) / np.sqrt(np.dot(x,x) * np.dot(y,y))\n    else:\n        print \"Input should be of equal length\"\n    return None\n\ndef jaccard_distance(x,y):\n    set_x = set(x)\n    set_y = set(y)\n    return 1 - len(set_x.intersection(set_y)) / len(set_x.union(set_y))\n\ndef hamming_distance(x,y):\n    diff = 0\n    if len(x) == len(y):\n        for char1,char2 in zip(x,y):\n            if char1 != char2:\n                diff+=1\n        return diff\n    else:\n        print \"Input should be of equal length\"\n    return None\n```", "```py\nif __name__ == \"__main__\":\n\n    # Sample data, 2 vectors of dimension 3\n    x = np.asarray([1,2,3])\n    y = np.asarray([1,2,3])\n    # print euclidean distance    \n    print euclidean_distance(x,y)\n    # Print euclidean by invoking lr norm with\n    # r value of 2    \n    print lrNorm_distance(x,y,2)\n    # Manhattan or citi block Distance\n    print lrNorm_distance(x,y,1)\n\n    # Sample data for cosine distance\n    x =[1,1]\n    y =[1,0]\n    print 'cosine distance'\n    print cosine_distance(x,y)\n\n    # Sample data for jaccard distance    \n    x = [1,2,3]\n    y = [1,2,3]\n    print jaccard_distance(x,y)\n\n    # Sample data for hamming distance    \n    x =[11001]\n    y =[11011]\n    print hamming_distance(x,y)\n```", "```py\nnp.sqrt(np.sum(np.power((x-y),2)))\n```", "```py\nx = np.asarray([1,2,3])\ny = np.asarray([1,2,3])\n\nprint euclidean_distance(x,y)\n```", "```py\nlrNorm_distance(x,y,power):\n```", "```py\nprint lrNorm_distance(x,y,2)\n```", "```py\nnp.dot(x,y) / np.sqrt(np.dot(x,x) * np.dot(y,y))\n```", "```py\nnp.dot(x,y)\n```", "```py\nnp.sqrt(np.dot(x,x) * np.dot(y,y))\n```", "```py\nnp.dot(x,x) is equivalent to \n\ntot = 0\nfor i in range(len(x)):\ntot+=x[i] * x[i]\n```", "```py\nset_x = set(x)\nset_y = set(y)\n```", "```py\n1 - len(set_x.intersection(set_y)) / (1.0 * len(set_x.union(set_y)))\n```", "```py\nfor char1,char2 in zip(x,y):\n    if char1 != char2:\n        diff+=1\nreturn diff\n```", "```py\nimport numpy as np\n# Simple example to illustrate Kernel Function concept.\n# 3 Dimensional input space\nx = np.array([10,20,30])\ny = np.array([8,9,10])\n\n# Let us find a mapping function to transform this space\n# phi(x1,x2,x3) = (x1x2,x1x3,x2x3,x1x1,x2x2,x3x3)\n# this will transorm the input space into 6 dimesions\n\ndef mapping_function(x):\n    output_list  =[]\n    for i in range(len(x)):\n        output_list.append(x[i]*x[i])\n\n    output_list.append(x[0]*x[1])\n    output_list.append(x[0]*x[2])\n    output_list.append(x[1]*x[0])\n    output_list.append(x[1]*x[2])\n    output_list.append(x[2]*x[1])\n    output_list.append(x[2]*x[0])\n    return np.array(output_list)\n```", "```py\nif __name_ == \"__main__\"\n    # Apply the mapping function\n    tranf_x = mapping_function(x)\n    tranf_y = mapping_function(y)\n    # Print the output\n    print tranf_x\n    print np.dot(tranf_x,tranf_y)\n\n    # Print the equivalent kernel functions\n    # transformation output.\n    output = np.power((np.dot(x,y)),2)\n    print output\n```", "```py\n[100 400 900 200 300 200 600 600 300]\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef get_random_data():\n    x_1 = np.random.normal(loc=0.2,scale=0.2,size=(100,100))\n    x_2 = np.random.normal(loc=0.9,scale=0.1,size=(100,100))\n    x = np.r_[x_1,x_2]\n    return x\n```", "```py\nx = get_random_data()\n\nplt.cla()\nplt.figure(1)\nplt.title(\"Generated Data\")\nplt.scatter(x[:,0],x[:,1])\nplt.show()\n```", "```py\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\ndef form_clusters(x,k):\n    \"\"\"\n    Build clusters\n    \"\"\"\n    # k = required number of clusters\n    no_clusters = k\n    model = KMeans(n_clusters=no_clusters,init='random')\n    model.fit(x)\n    labels = model.labels_\n    print labels\n    # Cacluate the silhouette score\n    sh_score = silhouette_score(x,labels)\n    return sh_score\n```", "```py\nsh_scores = []\nfor i in range(1,5):\n    sh_score = form_clusters(x,i+1)\n    sh_scores.append(sh_score)\n\nno_clusters = [i+1 for i in range(1,5)]\n```", "```py\nno_clusters = [i+1 for i in range(1,5)]\n\nplt.figure(2)\nplt.plot(no_clusters,sh_scores)\nplt.title(\"Cluster Quality\")\nplt.xlabel(\"No of clusters k\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()\n```", "```py\n    model = KMeans(n_clusters=no_clusters,init='random')\n    model.fit(x)\n    ```", "```py\nlabels = model.labels_\nsh_score = silhouette_score(x,labels)\nreturn sh_score\n```", "```py\nsh_scores = []\nfor i in range(1,5):\nsh_score = form_clusters(x,i+1)\nsh_scores.append(sh_score)\n```", "```py\nno_clusters = [i+1 for i in range(1,5)]\n\nplt.figure(2)\nplt.plot(no_clusters,sh_scores)\nplt.title(\"Cluster Quality\")\nplt.xlabel(\"No of clusters k\")\nplt.ylabel(\"Silhouette Coefficient\")\nplt.show()\n```", "```py\nfrom collections import defaultdict\nimport numpy as np\n\ninstances = np.matrix([[0,0],[0,1],[1,1],[1,0],[5,0]])\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.squeeze(np.asarray(instances[:,0]))\ny = np.squeeze(np.asarray(instances[:,1]))\nplt.cla()\nplt.figure(1)\nplt.scatter(x,y)\nplt.show()\n```", "```py\n    Reachability distance (P ß Q) = > maximum(K-Distance(P), Distance(P,Q))\n    ```", "```py\n    k = 2\n    distance = 'manhattan'\n\n    from sklearn.metrics import pairwise_distances\n    dist = pairwise_distances(instances,metric=distance)\n    ```", "```py\n    # Calculate K distance\n    import heapq\n    k_distance = defaultdict(tuple)\n    # For each data point\n    for i in range(instances.shape[0]):\n        # Get its distance to all the other points.\n        # Convert array into list for convienience\n        distances = dist[i].tolist()\n        # Get the K nearest neighbours\n        ksmallest = heapq.nsmallest(k+1,distances)[1:][k-1]\n        # Get their indices\n        ksmallest_idx = distances.index(ksmallest)\n        # For each data point store the K th nearest neighbour and its distance\n        k_distance[i]=(ksmallest,ksmallest_idx)\n    ```", "```py\n    def all_indices(value, inlist):\n        out_indices = []\n        idx = -1\n        while True:\n            try:\n                idx = inlist.index(value, idx+1)\n                out_indices.append(idx)\n            except ValueError:\n                break\n        return out_indices\n    # Calculate K distance neighbourhood\n    import heapq\n    k_distance_neig = defaultdict(list)\n    # For each data point\n    for i in range(instances.shape[0]):\n        # Get the points distances to its neighbours\n        distances = dist[i].tolist()\n        print \"k distance neighbourhood\",i\n        print distances\n        # Get the 1 to K nearest neighbours\n        ksmallest = heapq.nsmallest(k+1,distances)[1:]\n        print ksmallest\n        ksmallest_set = set(ksmallest)\n        print ksmallest_set\n        ksmallest_idx = []\n        # Get the indices of the K smallest elements\n        for x in ksmallest_set:\n                ksmallest_idx.append(all_indices(x,distances))\n        # Change a list of list to list\n        ksmallest_idx = [item for sublist in ksmallest_idx for item in sublist]\n        # For each data pont store the K distance neighbourhood\n        k_distance_neig[i].extend(zip(ksmallest,ksmallest_idx))\n    ```", "```py\n    #Local reachable density\n    local_reach_density = defaultdict(float)\n    for i in range(instances.shape[0]):\n        # LRDs numerator, number of K distance neighbourhood\n        no_neighbours = len(k_distance_neig[i])\n        denom_sum = 0\n        # Reachability distance sum\n        for neigh in k_distance_neig[i]:\n            # maximum(K-Distance(P), Distance(P,Q))\n            denom_sum+=max(k_distance[neigh[1]][0],neigh[0])\n        local_reach_density[i] = no_neighbours/(1.0*denom_sum)\n    ```", "```py\n    lof_list =[]\n    #Local Outlier Factor\n    for i in range(instances.shape[0]):\n        lrd_sum = 0\n        rdist_sum = 0\n        for neigh in k_distance_neig[i]:\n            lrd_sum+=local_reach_density[neigh[1]]\n            rdist_sum+=max(k_distance[neigh[1]][0],neigh[0])\n        lof_list.append((i,lrd_sum*rdist_sum))\n    ```", "```py\n>>> dist.shape\n(5, 5)\n>>>\n```", "```py\nimport heapq\n```", "```py\nk_distance = defaultdict(tuple)\n```", "```py\ndistances = dist[i].tolist()\n```", "```py\n>>> dist\narray([[ 0.,  1.,  2.,  1.,  5.],\n       [ 1.,  0.,  1.,  2.,  6.],\n       [ 2.,  1.,  0.,  1.,  5.],\n       [ 1.,  2.,  1.,  0.,  4.],\n       [ 5.,  6.,  5.,  4.,  0.]]) \n```", "```py\n[ 0.,  1.,  2.,  1.,  5.]\n```", "```py\n# Get the Kth nearest neighbours\nksmallest = heapq.nsmallest(k+1,distances)[1:][k-1]\n```", "```py\n>>> help(heapq.nsmallest)\nHelp on function nsmallest in module heapq:\n\nnsmallest(n, iterable, key=None)\n    Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]\n```", "```py\n# Get their indices\nksmallest_idx = distances.index(ksmallest)\n# For each data point store the K th nearest neighbour and its distance\nk_distance[i]=(ksmallest,ksmallest_idx)\n```", "```py\nprint k_distance\ndefaultdict(<type 'tuple'>, {0: (1.0, 1), 1: (1.0, 0), 2: (1.0, 1), 3: (1.0, 0), 4: (5.0, 0)})\n```", "```py\n# Calculate K distance neighbourhood\nimport heapq\nk_distance_neig = defaultdict(list)\n```", "```py\ndistances = dist[i].tolist()\n# Get the 1 to K nearest neighbours\nksmallest = heapq.nsmallest(k+1,distances)[1:]\nksmallest_set = set(ksmallest)\n```", "```py\n[0.0, 1.0, 2.0, 1.0, 5.0]\n```", "```py\n[1.0, 1.0]\n```", "```py\ndef all_indices(value, inlist):\n    out_indices = []\n    idx = -1\n    while True:\n        try:\n            idx = inlist.index(value, idx+1)\n            out_indices.append(idx)\n        except ValueError:\n            break\n    return out_indices\n```", "```py\nksmallest_set = set(ksmallest)\n```", "```py\n# Get the indices of the K smallest elements\nfor x in ksmallest_set:\nksmallest_idx.append(all_indices(x,distances))\n```", "```py\nksmallest_idx = [item for sublist in ksmallest_idx for item in sublist]\n```", "```py\nk_distance_neig[i].extend(zip(ksmallest,ksmallest_idx))\n```", "```py\ndefaultdict(<type 'list'>, {0: [(1.0, 1), (1.0, 3)], 1: [(1.0, 0), (1.0, 2)], 2: [(1.0, 1), (1.0, 3)], 3: [(1.0, 0), (1.0, 2)], 4: [(4.0, 3), (5.0, 0)]})\n```", "```py\n    Reachability distance (P ß Q) = > maximum(K-Distance(P), Distance(P,Q))\n    ```", "```py\n    #Local reachable density\n    local_reach_density = defaultdict(float)\n    ```", "```py\nfor i in range(instances.shape[0]):\n# LRDs numerator, number of K distance neighbourhood\nno_neighbours = len(k_distance_neig[i])\ndenom_sum = 0\n# Reachability distance sum\nfor neigh in k_distance_neig[i]:\n# maximum(K-Distance(P), Distance(P,Q))\ndenom_sum+=max(k_distance[neigh[1]][0],neigh[0])\n   local_reach_density[i] = no_neighbours/(1.0*denom_sum)\n```", "```py\nfor i in range(instances.shape[0]):\nlrd_sum = 0\nrdist_sum = 0\nfor neigh in k_distance_neig[i]:\nlrd_sum+=local_reach_density[neigh[1]]\nrdist_sum+=max(k_distance[neigh[1]][0],neigh[0])\nlof_list.append((i,lrd_sum*rdist_sum))\n```", "```py\n[(0, 4.0), (1, 4.0), (2, 4.0), (3, 4.0), (4, 18.0)]\n```", "```py\nfrom sklearn.datasets import load_iris\nimport numpy as np\nfrom sklearn.metrics import euclidean_distances\n\ndata = load_iris()\nx = data['data']\ny = data['target']\n\n# Scale the variables\nfrom sklearn.preprocessing import MinMaxScaler\nminmax = MinMaxScaler()\nx = minmax.fit_transform(x)\n```", "```py\n    R = 2\n    n_classes = 3\n    epsilon = 0.9\n    epsilon_dec_factor = 0.001\n    ```", "```py\n    class prototype(object):\n        \"\"\"\n        Class to hold prototype vectors\n        \"\"\"\n\n        def __init__(self,class_id,p_vector,eplsilon):\n            self.class_id = class_id\n            self.p_vector = p_vector\n            self.epsilon = epsilon\n\n        def update(self,u_vector,increment=True):\n            if increment:\n                # Move the prototype vector closer to input vector\n                self.p_vector = self.p_vector + self.epsilon*(u_vector - self.p_vector)\n            else:\n                # Move the prototype vector away from input vector\n                self.p_vector = self.p_vector - self.epsilon*(u_vector - self.p_vector)\n    ```", "```py\n    def find_closest(in_vector,proto_vectors):\n        closest = None\n        closest_distance = 99999\n        for p_v in proto_vectors:\n            distance = euclidean_distances(in_vector,p_v.p_vector)\n            if distance < closest_distance:\n                closest_distance = distance\n                closest = p_v\n        return closest\n    ```", "```py\n    def find_class_id(test_vector,p_vectors):\n        return find_closest(test_vector,p_vectors).class_id\n    ```", "```py\n    # Choose R initial prototypes for each class        \n    p_vectors = []\n    for i in range(n_classes):\n        # Select a class\n        y_subset = np.where(y == i)\n        # Select tuples for choosen class\n        x_subset  = x[y_subset]\n        # Get R random indices between 0 and 50\n        samples = np.random.randint(0,len(x_subset),R)\n        # Select p_vectors\n        for sample in samples:\n            s = x_subset[sample]\n            p = prototype(i,s,epsilon)\n            p_vectors.append(p)\n\n    print \"class id \\t Initial protype vector\\n\"\n    for p_v in p_vectors:\n        print p_v.class_id,'\\t',p_v.p_vector\n           print\n    ```", "```py\n    while epsilon >= 0.01:\n        # Sample a training instance randonly\n        rnd_i = np.random.randint(0,149)\n        rnd_s = x[rnd_i]\n        target_y = y[rnd_i]\n\n        # Decrement epsilon value for next iteration\n        epsilon = epsilon - epsilon_dec_factor    \n        # Find closes prototype vector to given point\n        closest_pvector = find_closest(rnd_s,p_vectors)\n\n        # Update closes prototype vector\n        if target_y == closest_pvector.class_id:\n            closest_pvector.update(rnd_s)\n        else:\n            closest_pvector.update(rnd_s,False)\n        closest_pvector.epsilon = epsilon\n\n    print \"class id \\t Final Prototype Vector\\n\"\n    for p_vector in p_vectors:\n        print p_vector.class_id,'\\t',p_vector.p_vector\n    ```", "```py\n    predicted_y = [find_class_id(instance,p_vectors) for instance in x ]\n\n    from sklearn.metrics import classification_report\n\n    print\n    print classification_report(y,predicted_y,target_names=['Iris-Setosa','Iris-Versicolour', 'Iris-Virginica'])\n    ```", "```py\nself.class_id = class_id\nself.p_vector = p_vector\nself.epsilon = epsilon\n```", "```py\ndef update(self,u_vector,increment=True):\nif increment:\n# Move the prototype vector closer to input vector\nself.p_vector = self.p_vector + self.epsilon*(u_vector - self.p_vector)\nelse:\n# Move the prototype vector away from input vector\nself.p_vector = self.p_vector - self.epsilon*(u_vector - self.p_vector)\n```", "```py\nfor p_v in proto_vectors:\ndistance = euclidean_distances(in_vector,p_v.p_vector)\nif distance < closest_distance:\nclosest_distance = distance\nclosest = p_v\n```", "```py\nsamples = np.random.randint(0,len(x_subset),R)\n# Select p_vectors\nfor sample in samples:\ns = x_subset[sample]\np = prototype(i,s,epsilon)\np_vectors.append(p)\n```", "```py\n# Sample a training instance randonly\nrnd_i = np.random.randint(0,149)\nrnd_s = x[rnd_i]\ntarget_y = y[rnd_i]\n```", "```py\nclosest_pvector = find_closest(rnd_s,p_vectors)\n```", "```py\n# Update closes prototype vector\nif target_y == closest_pvector.class_id:\nclosest_pvector.update(rnd_s)\nelse:\nclosest_pvector.update(rnd_s,False)\n```", "```py\nclosest_pvector.epsilon = epsilon\n```", "```py\nprint \"class id \\t Final Prototype Vector\\n\"\nfor p_vector in p_vectors:\nprint p_vector.class_id,'\\t',p_vector.p_vector\n```", "```py\npredicted_y = [find_class_id(instance,p_vectors) for instance in x ]\n```", "```py\nprint classification_report(y,predicted_y,target_names=['Iris-Setosa','Iris-Versicolour', 'Iris-Virginica'])\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn_samples = 100\nfraction_of_outliers = 0.1\nnumber_inliers = int ( (1-fraction_of_outliers) * n_samples )\nnumber_outliers = n_samples - number_inliers\n```", "```py\n# Get some samples from a normal distribution\nnormal_data = np.random.randn(number_inliers,1)\n```", "```py\n# Print the mean and standard deviation\n# to confirm the normality of our input data.\nmean = np.mean(normal_data,axis=0)\nstd = np.std(normal_data,axis=0)\nprint \"Mean =(%0.2f) and Standard Deviation (%0.2f)\"%(mean[0],std[0])\n```", "```py\nMean =(0.24) and Standard Deviation (0.90)\n```", "```py\n# Create outlier data\noutlier_data = np.random.uniform(low=-9,high=9,size=(number_outliers,1))\ntotal_data = np.r_[normal_data,outlier_data]\nprint \"Size of input data = (%d,%d)\"%(total_data.shape)\n# Eyeball the data\nplt.cla()\nplt.figure(1)\nplt.title(\"Input points\")\nplt.scatter(range(len(total_data)),total_data,c='b')\n```", "```py\n    # Median Absolute Deviation\n    median = np.median(total_data)\n    b = 1.4826\n    mad = b * np.median(np.abs(total_data - median))\n    outliers = []\n    # Useful while plotting\n    outlier_index = []\n    print \"Median absolute Deviation = %.2f\"%(mad)\n    lower_limit = median - (3*mad)\n    upper_limit = median + (3*mad)\n    print \"Lower limit = %0.2f, Upper limit = %0.2f\"%(lower_limit,upper_limit)\n    for i in range(len(total_data)):\n        if total_data[i] > upper_limit or total_data[i] < lower_limit:\n            print \"Outlier %0.2f\"%(total_data[i])\n            outliers.append(total_data[i])\n            outlier_index.append(i)\n\n    plt.figure(2)\n    plt.title(\"Outliers using mad\")\n    plt.scatter(range(len(total_data)),total_data,c='b')\n    plt.scatter(outlier_index,outliers,c='r')\n    plt.show()\n    ```", "```py\n    # Standard deviation\n    std = np.std(total_data)\n    mean = np.mean(total_data)\n    b = 3\n    outliers = []\n    outlier_index = []\n    lower_limt = mean-b*std\n    upper_limt = mean+b*std\n    print \"Lower limit = %0.2f, Upper limit = %0.2f\"%(lower_limit,upper_limit)\n    for i in range(len(total_data)):\n        x = total_data[i]\n        if x > upper_limit or x < lower_limt:\n            print \"Outlier %0.2f\"%(total_data[i])\n            outliers.append(total_data[i])\n            outlier_index.append(i)\n\n    plt.figure(3)\n    plt.title(\"Outliers using std\")\n    plt.scatter(range(len(total_data)),total_data,c='b')\n    plt.scatter(outlier_index,outliers,c='r')\n    plt.savefig(\"B04041 04 10.png\")\n    plt.show()\n    ```", "```py\nmedian = np.median(total_data)\nb = 1.4826\nmad = b * np.median(np.abs(total_data - median))\n```", "```py\nlower_limit = median - (3*mad)\nupper_limit = median + (3*mad)\n\nprint \"Lower limit = %0.2f, Upper limit = %0.2f\"%(lower_limit,upper_limit)\n```", "```py\nfor i in range(len(total_data)):\nif total_data[i] > upper_limit or total_data[i] < lower_limit:\nprint \"Outlier %0.2f\"%(total_data[i])\noutliers.append(total_data[i])\noutlier_index.append(i)\n```", "```py\nstd = np.std(total_data)\nmean = np.mean(total_data)\nb = 3\n```", "```py\nlower_limt = mean-b*std\nupper_limt = mean+b*std\n\nprint \"Lower limit = %0.2f, Upper limit = %0.2f\"%(lower_limit,upper_limit)\n\nfor i in range(len(total_data)):\nx = total_data[i]\nif x > upper_limit or x < lower_limt:\nprint \"Outlier %0.2f\"%(total_data[i])\noutliers.append(total_data[i])\noutlier_index.append(i)\n```", "```py\nnp.random.randn(8)\n```", "```py\n-1.76334861, -0.75817064,  0.44468944, -0.07724717,  0.12951944,0.43096092, -0.05436724, -0.23719402\n```", "```py\n-1.763348607322289, -0.7581706357821458, 0.4446894368956213, -0.07724717210195432, 0.1295194428816003, 0.4309609200681169, -0.05436724238743103, -0.23719402072058543, 45, 69\n```"]