<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Machine Learning Deep Dive</h1>
                </header>
            
            <article>
                
<p>The prior chapter on machine learning provided a preliminary overview of the subject, including the different classes and core concepts in the subject area. This chapter will delve deeper into the theoretical aspects of machine learning such as the limits of algorithms and how different algorithms work.</p>
<p><strong>Machine learning</strong> is a vast and complex subject, and to that end, this chapter focuses on the breadth of different topics, rather than the depth. The concepts are introduced at a high level and the reader may refer to other sources to further their understanding of the topics.</p>
<p>We will start out by discussing a few fundamental theories in machine learning, such as Gradient Descent and VC Dimension. Next, we will look at Bias and Variance, two of the most important factors in any modelling process and the concept of bias-variance trade-off.</p>
<p>We'll then discuss the various machine learning algorithms, their strengths and areas of applications.</p>
<p>We'll conclude with exercises that leverage real-world datasets to perform machine learning operations using R.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>The bias, variance, and regularization properties</li>
<li>Gradient descent and VC dimension theories</li>
<li>Machine learning algorithms</li>
<li>Tutorial: Machine learning with R</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The bias, variance, and regularization properties</h1>
                </header>
            
            <article>
                
<p>Bias, variance, and the closely related topic of regularization hold very special and fundamental positions in the field of machine learning.</p>
<p>Bias happens when a machine learning model is too 'simple', leading to results that are consistently off from the actual values.</p>
<p>Variance happens when a model is too 'complex', leading to results that are very accurate on test datasets, but do not perform well on unseen/new datasets.</p>
<p>Once users become familiar with the process of creating machine learning models, it would seem that the process is quite simplistic - get the data, create a training set and a test set, create a model, apply the model on the test dataset, and the exercise is complete. Creating models is easy; creating a <em>good</em> model is a much more challenging topic. But how can one test the quality of a model? And, perhaps more importantly, how does one go about building a 'good' model?</p>
<p>The answer lies in a term called regularization. It's arguably a fancy word, but all it means is that during the process of creating a model, one benefits from penalizing an overly impressive performance on a training dataset and relaxing the same on a poorly performing model.</p>
<p>To understand regularization, it would help to know the concepts of overfitting and underfitting. For this, let us look at a simple but familiar example of drawing lines of best fit. For those who have used Microsoft Excel, you may have noticed the option to draw the <em>line of best fit</em> - in essence, given a set of points, you can draw a line that represents the data and approximates the function that the points represent.</p>
<p>The following table shows the prices vs square footage of a few properties. In order to determine the relationship between house prices and the size of the house, we can draw a line of best fit, or a trend line, as shown as follows:</p>
<table class="GridTable2">
<tbody>
<tr>
<td>
<p><strong>Sq. ft.</strong></p>
</td>
<td>
<p><strong>Price ($)</strong></p>
</td>
</tr>
<tr>
<td>
<p>862</p>
</td>
<td>
<p>170,982</p>
</td>
</tr>
<tr>
<td>
<p>1235</p>
</td>
<td>
<p>227,932</p>
</td>
</tr>
<tr>
<td>
<p>932</p>
</td>
<td>
<p>183,280</p>
</td>
</tr>
<tr>
<td>
<p>1624</p>
</td>
<td>
<p>237,945</p>
</td>
</tr>
<tr>
<td>
<p>1757</p>
</td>
<td>
<p>275,921</p>
</td>
</tr>
<tr>
<td>
<p><strong>1630</strong></p>
</td>
<td>
<p>274,713</p>
</td>
</tr>
<tr>
<td>
<p><strong>1236</strong></p>
</td>
<td>
<p>201,428</p>
</td>
</tr>
<tr>
<td>
<p><strong>1002</strong></p>
</td>
<td>
<p>193,128</p>
</td>
</tr>
<tr>
<td>
<p><strong>1118</strong></p>
</td>
<td>
<p>187,073</p>
</td>
</tr>
<tr>
<td>
<p><strong>1339</strong></p>
</td>
<td>
<p>202,422</p>
</td>
</tr>
<tr>
<td>
<p><strong>1753</strong></p>
</td>
<td>
<p>283,989</p>
</td>
</tr>
<tr>
<td>
<p><strong>1239</strong></p>
</td>
<td>
<p>228,170</p>
</td>
</tr>
<tr>
<td>
<p><strong>1364</strong></p>
</td>
<td>
<p>230,662</p>
</td>
</tr>
<tr>
<td>
<p><strong>995</strong></p>
</td>
<td>
<p>169,369</p>
</td>
</tr>
<tr>
<td>
<p><strong>1000</strong></p>
</td>
<td>
<p>157,305</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>If we were to draw a <em>line of best</em> <em>fit</em> using a linear trend line, the chart would look somewhat like this:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/c352bb42-90f4-43a0-ab7a-05d53e2916ae.png"/></div>
<p>Excel provides an useful additional feature that allows users to draw an extension of the trend line which can provide an estimate, or a <em>prediction</em>, of unknown variables. In this case, extending the trendline will show us, based on the function, what the prices for houses in the 1,800-2,000 sq. ft. range are likely to be.</p>
<p>The linear function that describes the data is as follows:</p>
<p><em>y=126.13x + 54,466.81</em></p>
<p>The following chart with an extended trend line shows that the price is most likely between <kbd>$275,000</kbd> and <kbd>$300,000</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/70b446ec-c433-4589-8aea-731806451a1c.png"/></div>
<p>However, one may argue that the line is not the best approximation and that it may be possible to increase the value of R2, which in this case is 0.87. In general, the higher the R^2, the better the model that describes the data. There are various different types of <em>R<sup>2</sup></em> values, but for the purpose of this section, we'll assume that the higher the <em>R<sup>2</sup></em>, the better the model.</p>
<p>In the next section, we will draw a new trend line that has a much higher R^2, but using a polynomial function. This function has a higher R^2 (0.91 vs 0.87) and visually appears to be closer to the points on average.</p>
<p>The function in this case is a 6<sup>th</sup>-order polynomial:</p>
<p><em>y = -0.00x<sup>6</sup> + 0.00x<sup>5</sup> - 0.00x<sup>4</sup> + 2.50x<sup>3</sup> - 2,313.40x<sup>2</sup> + 1,125,401.77x - 224,923,813.17</em></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/98b77763-7482-4c42-9e49-62c7c75e2021.png"/></div>
<p>But, even though the line has a higher R^2, if we extend the trend line, intending to find what the prices of houses in the 1,800-2,000 sq. ft. range are likely to be, we get the following result.</p>
<p>Houses in the 1,800-2,000 sq. ft. range go from approx. $280,000 to negative $2 million at the 2,000<sup>th</sup> sq. ft. In other words, people purchasing houses with 1800 sq. ft. are expected to spend $ 280,000 and those purchasing houses with 2,000 sq. ft. should, according to this function, with a 'higher R^2', receive $2 million! This, of course, is not accurate, but what we have just witnessed is what is known as <strong>over-fitting</strong>. The image below illustrates this phenomenon.</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/8f39c591-a89e-499e-921c-900f60f31753.png"/></div>
<p>At the other end of the spectrum is <strong>under-fitting</strong>. This happens when the model built does not describe the data. In the following chart, the function y = 0.25x - 200 is one such example:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/60b80a9d-cace-4bbb-ac95-ed3eac84763b.png"/></div>
<p>In brief, this section can be abbreviated as follows:</p>
<ul>
<li>A function that fits the data too well, such that the function can approximate nearly all of the points in the training dataset is considered overfitting.</li>
<li>A function that does not fit the data at all, or in other words is far from the actual points in the training dataset, is considered underfitting.</li>
<li>Machine learning is the process of balancing between overfitting and underfitting the data. This is arguably not an easy exercise, which is why even though building a model may be trivial, building a model that is reasonably good is a much more difficult challenge.</li>
<li>Underfitting is when your function is <em>not thinking at all</em> - it has a high bias.</li>
<li>Overfitting is when your function is <em>thinking too hard</em> - it has a high variance.</li>
<li>Another example for underfitting and overfitting is given in coming example.</li>
</ul>
<p>Say we are tasked with determining if a bunch of fruit are oranges or apples, and have been given their location in a fruit basket (left-side or right-side), size and weight:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<div class="packt_figure"><img class=" image-border" src="assets/be16688e-a356-4b63-923b-d9e25b01abb1.png"/></div>
</td>
<td>
<div class="packt_figure"><img class=" image-border" src="assets/f6035a45-cd01-4de4-be71-f428126da2b1.png"/></div>
</td>
</tr>
<tr>
<td>
<p><strong>Basket 1 (Training Dataset)</strong></p>
</td>
<td>
<p><strong>Basket 2 (Test Dataset)</strong></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>An example of overfitting could be that, based on the training dataset, with regard to Basket 1 we could conclude that the only fruits located on the right hand side of the basket are oranges and those on the left are all apples.</p>
<p>An example of underfitting could be that I conclude that the basket has only oranges.</p>
<p><strong>Model 1</strong>: In the first case - for overfitting - I have, in essence, memorized the locations.</p>
<p><strong>Model 2</strong>: In the second case - for underfitting - I could not remember anything precisely at all.</p>
<p>Now, given a second basket - the test dataset where the positions of the apples and oranges are switched - if I were to use Model 1, I would incorrectly conclude that all the fruits on the right hand side are oranges and those on the left hand side are apples (since I memorized the training data).</p>
<p>If I were to use Model 2, I would, again, incorrectly conclude that all the fruits are oranges.</p>
<p>There are, however, ways to manage the balance between underfitting and overfitting - or in other words, between high bias and high variance.</p>
<p>One of the methods commonly used for bias-variance trade-off is known as regularization. This refers to the process of penalizing the model (for example, the model's coefficients in a regression) in order to produce an output that generalizes well across a range of data points.</p>
<p>The table on the next page illustrates some of the key concepts of bias and variance and illustrates options for remedial steps when a model has high bias or high variance:</p>
<div class="CDPAlignCenter CDPAlign"><img height="235" width="415" class=" image-border" src="assets/7fd11d61-930f-4e67-a737-7ba72e754400.png"/></div>
<p>In terms of the modeling process, a high bias is generally indicated by the fact that both the training set error as well as the test set error remain consistently high. For high variance (overfitting), the training set error decreases rapidly, but the test set error remains unchanged.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The gradient descent and VC Dimension theories</h1>
                </header>
            
            <article>
                
<p>Gradient descent and VC Dimension are two fundamental theories in machine learning. In general, <strong>gradient descent</strong> gives a structured approach to finding the optimal co-efficients of a function. The hypothesis space of a function can be large and with gradient descent, the algorithm tries to find a minimum (<em>a minima</em>) where the cost function (for example, the squared sum of errors) is the lowest.</p>
<p><strong>VC Dimension</strong> provides an upper bound on the maximum number of points that can be classified in a system. It is in essence the measure of the richness of a function and provides an assessment of what the limits of a hypothesis are in a structured way. The number of points that can be exactly classified by a function or hypothesis is known as the VC Dimension of the hypothesis. For example, a linear boundary can accurately classify 2 or 3 points but not 4. Hence, the VC Dimension of this 2-dimensional space would be 3.</p>
<p>VC Dimension, like many other topics in computational learning theory, is both complex and interesting. It is a lesser known (and discussed) topic, but one that has a profound implication as it attempts to answer questions about what the limits of learning can be.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Popular machine learning algorithms</h1>
                </header>
            
            <article>
                
<p>There are various different classes of machine learning algorithms. As such, since algorithms can belong to multiple 'classes' or categories at the same time at a conceptual level, it is hard to specifically state that an algorithm belongs exclusively to a single class. In this section, we will briefly discuss a few of the most commonly used and well-known algorithms.</p>
<p>These include:</p>
<ul>
<li>Regression models</li>
<li>Association rules</li>
<li>Decision trees</li>
<li>Random forest</li>
<li>Boosting algorithms</li>
<li>Support vector machines</li>
<li>K-means</li>
<li>Neural networks</li>
</ul>
<p>Note that in the examples, we have shown the basic use of the R functions using the entire dataset. In practice, we'd split the data into a training and test set, and once we have built a satisfactory model apply the same on the test dataset to evaluate the model's performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression models</h1>
                </header>
            
            <article>
                
<p>Regression models range from commonly used linear, logistic, and multiple regression algorithms used in statistics to Ridge and Lasso regression, which penalizes co-efficients to improve model performance.</p>
<p>In our earlier examples, we saw the application of <strong>linear regression</strong> when we created trend-lines. <strong>Multiple linear regression</strong> refers to the fact that the process of creating the model requires multiple independent variables.</p>
<p>For instance:</p>
<p><strong>Total Advertising Cost = x* Print Ads</strong>, would be a simple linear regression; whereas</p>
<p><strong>Total Advertising Cost = X + Print Ads + Radio Ads + TV Ads</strong>, due to the presence of more than one independent variable (Print, Radio, and TV), would be a multiple linear regression.</p>
<p><strong>Logistic regression</strong> is another commonly used statistical regression modelling technique that predicts the outcome of a discrete categorical value, mainly for cases where the outcome variable is dichotomous (for example, 0 or 1, Yes or No, and so on). There can, however, be more than 2 discrete outcomes (for example, State NY, NJ, CT) and this type of logistic regression is known as <strong>multinomial logistic regression</strong>.</p>
<p><strong>Ridge and Lasso Regressions</strong> include a regularization term (λ) in addition to the other aspects of Linear Regression. The regularization term, Ridge Regression, has the effect of reducing the β coefficients (thus 'penalizing' the co-efficients). In Lasso, the regularization term tends to reduce some of the co-efficients to 0, thus eliminating the effect of the variable on the final model:</p>
<div class="CDPAlignCenter CDPAlign"><img height="352" width="385" class=" image-border" src="assets/c0734ce4-09ff-4e4f-9ae6-150d20324b53.png"/></div>
<pre># Load mlbench and create a regression model of glucose (outcome/dependent variable) with pressure, triceps and insulin as the independent variables.<br/><br/>&gt; library("mlbench") 
&gt;lm_model&lt;- lm(glucose ~ pressure + triceps + insulin, data=PimaIndiansDiabetes[1:100,]) 
&gt; plot(lm_model) </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Association rules</h1>
                </header>
            
            <article>
                
<p>Association rules mining, or <strong>apriori</strong>, attempts to find relationships between variables in a dataset. Association rules are frequently used for various practical real-world use cases. Given a set of variables, apriori can indicate the patterns inherent in a transactional dataset. One of our tutorials will be based on implementing an R Shiny Application for apriori and hence, more emphasis is being provided for the same in this section.</p>
<p>For instance, let's say a supermarket chain is deciding the order for placing items on the shelves. An apriori algorithm run against a database containing sales transactions would identify the items that, say, are most often bought together. This permits the supermarket to determine which items, when placed strategically in close proximity to one another, can yield the most sales. This is also often referred to as <em>market basket analysis</em>.</p>
<p>A simple example that reflects this could be as follows:</p>
<pre># The LHS (left-hand side) leads to the RHS (right-hand side) in the relationships shown below.<br/><br/># For instance, {Milk, Bread} --&gt; {Butter} indicates that someone purchasing milk and bread is also likely to purchase butter.<br/><br/>{Milk, Bread} --&gt; {Butter}<br/>{Butter, Egg} --&gt; {Baking Tray}<br/>{Baking Tray, Butter} --&gt; {Sugar}<br/>...</pre>
<p>In all these cases, the act of purchasing something on the left-hand side led to the purchase of the item mentioned on the right-hand side of the expression.</p>
<p>It is also possible to derive association rules from databases that do not necessarily contain <em>transactions</em>, but instead use a sliding window to go through events along a temporal attribute, such as with the WINEPI algorithm.</p>
<p>There are 3 primary measures in apriori. To illustrate them, let us use a sample dataset containing items purchased in 4 separate transactions:</p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Transaction</strong></p>
</td>
<td>
<p><strong>Item 1</strong></p>
</td>
<td>
<p><strong>Item 2</strong></p>
</td>
<td>
<p><strong>Item 3</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>Milk</p>
</td>
<td>
<p>Bread</p>
</td>
<td>
<p>Butter</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>Milk</p>
</td>
<td>
<p>Egg</p>
</td>
<td>
<p>Butter</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>Bread</p>
</td>
<td>
<p>Egg</p>
</td>
<td>
<p>Cheese</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>Butter</p>
</td>
<td>
<p>Bread</p>
</td>
<td>
<p>Egg</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confidence</h1>
                </header>
            
            <article>
                
<p>Confidence refers to how often the right-hand side of the apriori expression is valid when the left-hand side is valid. For instance, given an expression:</p>
<pre>{Milk} à {Bread}</pre>
<p>We would like to know how often Bread was purchased <em>when Milk was also purchased</em>.</p>
<p>In this case:</p>
<ul>
<li><strong>Transaction 1</strong>: Milk and Bread are both present</li>
<li><strong>Transaction 2</strong>: Milk is present, but not Bread</li>
<li><strong>Transactions 3 and 4</strong>: Milk is not present</li>
</ul>
<p>Hence, based on the what we saw, there were 2 transactions where Milk was present and of them, Bread was present in 1 transaction. Hence, the confidence for the rule {Milk} à {Bread} would be ½ = 50%</p>
<p>Taking another expression:</p>
<pre>{Bread} à {Butter}</pre>
<p>We would like to know, when Bread was purchased, how often was Butter also purchased?:</p>
<ul>
<li><strong>Transaction 1</strong>: Bread and Butter are both present</li>
<li><strong>Transaction 2</strong>: There is no Bread (Butter is present, but our point of reference is Bread and hence this does not count)</li>
<li><strong>Transaction 3</strong>: Bread is present but no Butter</li>
<li><strong>Transaction 4</strong>: Bread and Butter are both present</li>
</ul>
<p>Hence, we have Bread in 3 of the transactions, and Bread &amp; Butter in 2 of the 3 transactions. Hence, in this case, the 'confidence' of the rule <kbd>{Bread} à {Butter}</kbd> is <em>2/3 = 66.7</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support</h1>
                </header>
            
            <article>
                
<p>Support refers to the number of times the rule is satisfied relative to the total number of transactions in the dataset.</p>
<p>For instance:</p>
<p>{Milk} --&gt; {Bread}, occurs in 1 out of 4 Transactions (in Transaction 1). Hence, the support for this rule is ¼ = 0.25 (or 25%).</p>
<p>{Bread} <span>--&gt;</span> {Butter}, occurs in 2 out of 4 Transactions (in Transaction 1 and 4). Hence, the support for this rule is ½ = 0.50 (or 50%).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Lift</h1>
                </header>
            
            <article>
                
<p>Lift is arguably the most important of the 3 measures; it measures the support of the rule relative to the support of the individual sides of the expression; put differently, it measures how strong the rule is with respect to a random occurrence of the LHS and RHS of the expression. It is formally defined as:</p>
<p><em>Lift = Support (Rule)/(Support(LHS) * Support (RHS))</em></p>
<p>A low value for lift (say, less than or equal to 1) indicates that the LHS and RHS occurrence are independent of one another, whereas a higher lift measure indicates that the co-occurrence is significant.</p>
<p>In our prior example,</p>
<p>{Bread} <span>--&gt;</span> {Butter} has a lift of:</p>
<p>Support ({Bread} <span>--&gt;</span> {Butter})<br/>
Support {Bread} * Support {Butter}</p>
<p>= 0.50/((3/4) * (3/4)) = 0.50/(0.75 * 0.75) = 0.89.</p>
<p>This indicates that although the Confidence of the rule was high, the rule in and of itself is not significant relative to other rules that may be higher than 1.</p>
<p>An example of a rule with a Lift higher than 1 would be:</p>
<p>{Item 1: Bread} <span>--&gt;</span> {Item 3: Cheese}</p>
<p>This has a Lift of:</p>
<p>Support {Item 1: Bread <span>--&gt;</span> Item 3: Cheese}/(Support {Item 1: Cheese} * Support {Item 3: Cheese})</p>
<p>= (1/4)/((1/4)*(1/4) = 4.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision trees</h1>
                </header>
            
            <article>
                
<p>Decision Trees are a predictive modeling technique that generates rules that derive the likelihood of a certain outcome based on the likelihood of the preceding outcomes. In general, decision trees are typically constructed similar to a <strong>flowchart</strong>, with a series of nodes and leaves that denote a parent-child relationship. Nodes that do not link to other nodes are known as leaves.</p>
<p>Decision Trees belong to a class of algorithms that are often known as <strong>CART</strong> (<strong>Classification and Regression Trees</strong>). If the outcome of interest is a categorical variable, it falls under a classification exercise, whereas if the outcome is a number, it is known as a regression tree.</p>
<p>An example will help to make this concept clearer. Take a look at the chart:</p>
<div class="CDPAlignCenter CDPAlign"><img height="254" width="397" class=" image-border" src="assets/7ac42b3c-4ec1-4a83-8c52-0e51853d030b.png"/></div>
<p>The chart shows a hypothetical scenario: if school is closed/not closed. The rectangular boxes (in blue) represent the nodes. The first rectangle (School Closed) represent the <em>root</em> node, whereas the inner rectangles represent the <em>internal</em> nodes. The rectangular boxes with angled edges (in green and italic letters) represent the '<em>leaves</em>' (or <em>terminal</em> nodes).</p>
<p>Decision Trees are simple to understand and one of the few algorithms that are not a 'black box'. Algorithms such as those used to create Neural Networks are often considered black boxes, as it is very hard - if not impossible - to intuitively determine the exact path by which a final outcome was reached due to the complexity of the model.</p>
<p>In R, there are various facilities for creating Decision Trees. A commonly used library for creating them in R is <kbd>rpart</kbd>. We'll revisit our <kbd>PimaIndiansDiabetes</kbd> dataset to see how a decision tree can be created using the package.</p>
<p>We would like to create a model to determine how glucose, insulin, (body) mass, and age are related to diabetes. Note that in the dataset, diabetes is a categorical variable with a yes/no response.</p>
<p>For visualizing the decision tree, we will use the <kbd>rpart.plot</kbd> package. The code for the same is given as follows:</p>
<pre>install.packages("rpart") 
install.packages("rpart.plot") 
 
library(rpart) 
library(rpart.plot) 
 
rpart_model&lt;- rpart (diabetes ~ glucose + insulin + mass + age, data = PimaIndiansDiabetes) 
 
 
&gt;rpart_model 
n= 768  
 
node), split, n, loss, yval, (yprob) 
      * denotes terminal node 
 
  1) root 768 268 neg (0.6510417 0.3489583)   
    2) glucose&lt; 127.5 485  94neg (0.8061856 0.1938144) * 
    3) glucose&gt;=127.5 283 109 pos (0.3851590 0.6148410)   
      6) mass&lt; 29.95 76  24neg (0.6842105 0.3157895)   
       12) glucose&lt; 145.5 41   6 neg (0.8536585 0.1463415) * 
       13) glucose&gt;=145.5 35  17pos (0.4857143 0.5142857)   
         26) insulin&lt; 14.5 21   8 neg (0.6190476 0.3809524) * 
         27) insulin&gt;=14.5 14   4 pos (0.2857143 0.7142857) * 
      7) mass&gt;=29.95 207  57pos (0.2753623 0.7246377)   
       14) glucose&lt; 157.5 115  45pos (0.3913043 0.6086957)   
         28) age&lt; 30.5 50  23neg (0.5400000 0.4600000)   
           56) insulin&gt;=199 14   3 neg (0.7857143 0.2142857) * 
           57) insulin&lt; 199 36  16pos (0.4444444 0.5555556)   
            114) age&gt;=27.5 10   3 neg (0.7000000 0.3000000) * 
            115) age&lt; 27.5 26   9 pos (0.3461538 0.6538462) * 
         29) age&gt;=30.5 65  18pos (0.2769231 0.7230769) * 
       15) glucose&gt;=157.5 92  12pos (0.1304348 0.8695652) * 
 
&gt;rpart.plot(rpart_model, extra=102, nn=TRUE)<br/><br/># The plot shown below illustrates the decision tree that the model, rpart_model represents.</pre>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/27b2b5d4-4128-4690-8f2c-328712a01cfe.png"/></div>
<p>Reading from the top, the graph shows that that there are 500 cases of <kbd>diabetes=neg</kbd> in the dataset (out of a total of 768 records).</p>
<pre>&gt; sum(PimaIndiansDiabetes$diabetes=="neg") 
[1] 500 </pre>
<p>Of the total number of records in the dataset (768) with value of glucose &lt; 128, there were 485 records marked as negative. Of these, the model correctly predicted 391 cases as negative (Node Number 2, the first one on the left from the bottom).</p>
<p>For the records which had a glucose reading of &gt; 128, there were 283 records marked as positive (Node Number 3, the node immediately below the topmost/root node). The model correctly classified 174 of these cases.</p>
<p>Another, more recent package for intuitive decision trees with comprehensive visual information is <strong>FFTrees</strong> (<strong>Fast and Frugal Decision Trees</strong>). The following example has been provided for informational purposes:</p>
<pre>install.packages("FFTrees") 
library(caret) 
library(mlbench) 
library(FFTrees) 
set.seed(123) 
 
data("PimaIndiansDiabetes") 
diab&lt;- PimaIndiansDiabetes 
diab$diabetes&lt;- 1 * (diab$diabetes=="pos") 
 
train_ind&lt;- createDataPartition(diab$diabetes,p=0.8,list=FALSE,times=1) 
 
training_diab&lt;- diab[train_ind,] 
test_diab&lt;- diab[-train_ind,] 
 
diabetes.fft&lt;- FFTrees(diabetes ~.,data = training_diab,data.test = test_diab) 
plot(diabetes.fft)<br/><br/># The plot below illustrates the decision tree representing diabetes.fft using the FFTrees package.</pre>
<p class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/c984b3e7-7aa5-4ff3-a53e-d2b4d2eb1943.png"/></p>
<p>Decision Trees work by splitting the data recursively until a stopping criterion, such as when a certain depth has been reached, or the number of cases, is below a specified value. Each split is done based on the variable that will lead to a 'purer subset'.</p>
<p>In principle, we can grow an endless number of trees from a given set of variables, which makes it a particularly hard and intractable problem. Numerous algorithms exist which provide an efficient method for splitting and creating decision trees. One such method is Hunt's Algorithm.</p>
<p>Further details about the algorithm can be found at: <a href="https://www-users.cs.umn.edu/~kumar/dmbook/ch4.pdf" target="_blank"><span class="URLPACKT">https://www-users.cs.umn.edu/~kumar/dmbook/ch4.pdf</span></a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Random forest extension</h1>
                </header>
            
            <article>
                
<p>Random forest is an extension of the decision tree model that we just discussed. In practice, Decision Trees are simple to understand, simple to interpret, fast to create using available algorithms, and overall, intuitive. However, Decision Trees are sensitive to small changes in the data, permit splits only along an axis (linear splits) and can lead to overfitting. To mitigate some of the drawbacks of decision trees, whilst still getting the benefit of their elegance, algorithms such as Random Forest create multiple decision trees and sample random features to leverage and build an aggregate model.</p>
<p>Random forest works on the principle of <strong>bootstrap aggregating</strong> or <strong>bagging</strong>. Bootstrap is a statistical term indicating random sampling with replacement. Bootstrapping a given set of records means taking a random number of records and possibly including the same record multiple times in a sample. Thereafter, the user would measure their metric of interest on the sample and then repeat the process. In this manner, the distribution of the values of the metric calculated from random samples multiple times is expected to represent the distribution of the population, and so the entire dataset.</p>
<p>An example of Bagging a set of 3 numbers, such as (1,2,3,4), would be:</p>
<p>(1,2,3), (1,1,3), (1,3,3), (2,2,1), and others.</p>
<p>Bootstrap Aggregating, or <em>bagging</em>, implies leveraging a voting method using <em>multiple bootstrap samples</em> at a time, building a model on each individual sample (set of n records) and then finally aggregating the results.</p>
<p>Random forests also implement another level of operation beyond simple bagging. It also randomly selects the variables to be included in the model building process at each split. For instance, if we were to create a random forest model using the <kbd>PimaIndiansDiabetes</kbd> dataset with the variables pregnant, glucose, pressure, triceps, insulin, mass, pedigree, age, and diabetes, in each bootstrap sample (draw of n records), we would select a random subset of features with which to build the model--for instance, glucose, pressure, and insulin; insulin, age, and pedigree; triceps, mass, and insulin; and others.</p>
<p>In R, the package commonly used for RandomForest is called by its namesake, RandomForest. We can use it via the package as is or via caret. Both methods are shown as follows:</p>
<ol>
<li>Using Random Forest using the RandomForest package:</li>
</ol>
<pre class="mce-root">&gt; rf_model1 &lt;- randomForest(diabetes ~ ., data=PimaIndiansDiabetes) &gt; rf_model1 Call: randomForest(formula = diabetes ~ ., data = PimaIndiansDiabetes) <br/>Type of random forest: classification Number of trees: 500 No. of variables tried at each split: 2 OOB estimate of error rate: 23.44% Confusion matrix: negposclass.error neg430 70 0.1400000 pos 110 158 0.4104478</pre>
<ol start="2">
<li>Using Random Forest via caret using the <kbd>method="rf"</kbd> function:</li>
</ol>
<pre>&gt; library(caret) 
&gt; library(doMC) 
 
# THE NEXT STEP IS VERY CRITICAL - YOU DO 'NOT' NEED TO USE MULTICORE 
# NOTE THAT THIS WILL USE ALL THE CORES ON THE MACHINE THAT YOU ARE 
# USING TO RUN THE EXERCISE 
 
# REMOVE THE # MARK FROM THE FRONT OF registerDoMC BEFORE RUNNING 
# THE COMMAND 
 
&gt;# registerDoMC(cores = 8) # CHANGE NUMBER OF CORES TO MATCH THE NUMBER OF CORES ON YOUR MACHINE  
 
&gt;rf_model&lt;- train(diabetes ~ ., data=PimaIndiansDiabetes, method="rf") 
&gt;rf_model 
Random Forest  
 
768 samples 
  8 predictor 
  2 classes: 'neg', 'pos'  
 
No pre-processing 
Resampling: Bootstrapped (25 reps)  
Summary of sample sizes: 768, 768, 768, 768, 768, 768, ...  
Resampling results across tuning parameters: 
 
mtry  Accuracy   Kappa     
  2     0.7555341  0.4451835 
  5     0.7556464  0.4523084 
  8     0.7500721  0.4404318 
 
Accuracy was used to select the optimal model using  the largest value. 
The final value used for the model was mtry = 5. 
 
&gt;getTrainPerf(rf_model) 
 
TrainAccuracyTrainKappa method 
1     0.7583831  0.4524728rf </pre>
<p>It is also possible to see the splits and other related information in each tree of the original Random Forest model (which did not use caret). This can be done using the <kbd>getTree</kbd> function as follows:</p>
<pre>&gt;getTree(rf_model1,1,labelVar = TRUE) 
    left daughter right daughter split var split point status prediction 
1               2              3      mass     27.8500      1       &lt;NA&gt; 
2               4              5       age     28.5000      1       &lt;NA&gt; 
3               6              7   glucose    155.0000      1       &lt;NA&gt; 
4               8              9       age     27.5000      1       &lt;NA&gt; 
5              10             11      mass      9.6500      1       &lt;NA&gt; 
6              12             13  pregnant      7.5000      1       &lt;NA&gt; 
7              14             15   insulin     80.0000      1       &lt;NA&gt; 
8               0              0      &lt;NA&gt;      0.0000     -1        neg 
9              16             17  pressure     68.0000      1       &lt;NA&gt; 
10              0              0      &lt;NA&gt;      0.0000     -1        pos 
11             18             19   insulin    131.0000      1       &lt;NA&gt; 
12             20             21   insulin     87.5000      1       &lt;NA&gt; 
<br/> [...]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boosting algorithms</h1>
                </header>
            
            <article>
                
<p>Boosting is a technique that uses weights and a set of <em>weak learners</em>, such as decision trees, in order to improve model performance. Boosting assigns weights to data based on model misclassification and future learner's (created during the boosting machine learning process) focus on the misclassified examples. Examples that were correctly classified will be reassigned new weights which will generally be lower than those that were not correctly classified. The weight can be based on a cost function, such as a majority vote, using subsets of the data.</p>
<p>In simple and non-technical terms, boosting uses <em>a series of weak learners, and each learner 'learns' from the mistakes of the prior learners</em>.</p>
<p>Boosting is generally more popular compared to bagging as it assigns weights relative to model performance rather than assigning equal weights to all data points as in bagging. This is conceptually similar to the difference between a weighted average versus an average function with no weighting criteria.</p>
<p>There are several packages in R for boosting algorithms and some of the commonly used ones are as follows:</p>
<ul>
<li>Adaboost</li>
<li><strong>GBM</strong> (<strong>Stochastic Gradient Boosting</strong>)</li>
<li>XGBoost</li>
</ul>
<p>Of these, XGBoost is a widely popular machine learning package that has been used very successfully in competitive machine learning platforms such as Kaggle. XGBoost has a very elegant and computationally efficient way to creating ensemble models. Because it is both accurate and extremely fast, users have often used XGBoost for compute-intensive ML challenges. You can learn more about Kaggle at <a href="http://www.kaggle.com" target="_blank">http://www.kaggle.com</a>.</p>
<pre># Creating an XGBoost model in R<br/><br/>library(caret)
library(xgboost) 
 
set.seed(123) 
train_ind&lt;- sample(nrow(PimaIndiansDiabetes),as.integer(nrow(PimaIndiansDiabetes)*.80)) 
 
training_diab&lt;- PimaIndiansDiabetes[train_ind,] 
test_diab&lt;- PimaIndiansDiabetes[-train_ind,] 
 
diab_train&lt;- sparse.model.matrix(~.-1, data=training_diab[,-ncol(training_diab)]) 
diab_train_dmatrix&lt;- xgb.DMatrix(data = diab_train, label=training_diab$diabetes=="pos") 
 
diab_test&lt;- sparse.model.matrix(~.-1, data=test_diab[,-ncol(test_diab)]) 
diab_test_dmatrix&lt;- xgb.DMatrix(data = diab_test, label=test_diab$diabetes=="pos") 
 
 
 
param_diab&lt;- list(objective = "binary:logistic", 
eval_metric = "error", 
              booster = "gbtree", 
max_depth = 5, 
              eta = 0.1) 
 
xgb_model&lt;- xgb.train(data = diab_train_dmatrix, 
param_diab, nrounds = 1000, 
watchlist = list(train = diab_train_dmatrix, test = diab_test_dmatrix), 
print_every_n = 10) 
 
 
predicted &lt;- predict(xgb_model, diab_test_dmatrix) 
predicted &lt;- predicted &gt; 0.5 
 
actual &lt;- test_diab$diabetes == "pos" 
confusionMatrix(actual,predicted) 
 
 
# RESULT 
 
Confusion Matrix and Statistics 
 
          Reference 
Prediction FALSE TRUE 
     FALSE    80   17 
     TRUE     21   36 
 
Accuracy : 0.7532           
                 95% CI : (0.6774, 0.8191) 
    No Information Rate : 0.6558           
    P-Value [Acc&gt; NIR] : 0.005956         
 
Kappa : 0.463            
Mcnemar's Test P-Value : 0.626496         
 
Sensitivity : 0.7921           
Specificity : 0.6792           
PosPredValue : 0.8247           
NegPredValue : 0.6316           
Prevalence : 0.6558           
         Detection Rate : 0.5195           
   Detection Prevalence : 0.6299           
      Balanced Accuracy : 0.7357           
 
       'Positive' Class : FALSE       </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector machines</h1>
                </header>
            
            <article>
                
<p>Support vector machines, commonly known as <strong>SVMs</strong>, are another class of machine learning algorithm that are used to classify data into one or another category using a concept called <strong>hyperplane</strong>, which is used to demarcate a linear boundary between points.</p>
<p>For instance, given a set of black and white points on an x-y axis, we can find multiple lines that will separate them. The line, in this case, represents the function that delineates the category that each point belongs to. In the following image, lines H1 and H2 both separate the points accurately. In this case, how can we determine which one of H1 and H2 would be the optimal line?:</p>
<div class="CDPAlignCenter CDPAlign"><img height="235" width="246" class=" image-border" src="assets/086d5f41-630d-4146-b5bc-34957e49f501.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">Intuitively, we can say the line that is closest to the points - for instance, the vertical line H1 - might <em>not</em> be the optimal line to separate the points. Since the line is too close to the points, and so too specific to the points on the given dataset, a new point may be misclassified if it is even slightly off to the right or the left side of the line. In other words, the line is too sensitive to small changes in the data (which could be due to stochastic/deterministic noise, such as imperfections in the data).</p>
<p>On the other hand, the line H2 manages to separate the data whilst maintaining the maximum possible distance from the points closest to the line. Slight imperfections in the data are unlikely to affect the classification of the points to the extent line H1 may have done. This, in essence, describes the principle of the maximum margin of separation as shown in the image below.</p>
<div class="CDPAlignCenter CDPAlign"><strong><img height="278" width="272" class=" image-border" src="assets/6f17c69e-6d57-4497-a95a-c8b36d1cdb19.png"/></strong></div>
<p>The points close to the line, also known as the hyperplane, are known as the 'support vectors' (hence the name). In the image, the points that lie on the dashed line are therefore the support vectors.</p>
<p>In the real world, however, not all points may be 'linearly separable'. SVMs leverage a concept known as the 'kernel trick'. In essence, points that might not be linearly separable can be projected or mapped onto a higher dimensional surface. For example, given a set of points on a 2D x-y space that are not linearly separable, it may be possible to separate them if we were to project the points on a 3-dimensional space as shown in the following image. The points colored in red were not separable by a 2D line, but when mapped to a 3-dimensional surface, they can be separated by a hyperplane as shown in the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img height="138" width="247" class=" image-border" src="assets/10a74760-3d89-464f-b9c2-685247b84ba6.png"/></div>
<p>There are several packages in R that let users leverage SVM, such as <kbd>kernlab</kbd>, <kbd>e1071</kbd>, <kbd>klaR</kbd>, and others. Here, we illustrate the use of SVM from the <kbd>e1071</kbd> package, as shown as follow:</p>
<pre>library(mlbench) 
library(caret) 
library(e1071) 
set.seed(123) 
 
 
data("PimaIndiansDiabetes") 
diab&lt;- PimaIndiansDiabetes 
 
train_ind&lt;- createDataPartition(diab$diabetes,p=0.8,list=FALSE,times=1) 
 
training_diab&lt;- diab[train_ind,] 
test_diab&lt;- diab[-train_ind,] 
 
svm_model&lt;- svm(diabetes ~ ., data=training_diab) 
plot(svm_model,training_diab, glucose ~ mass) <br/><br/># The plot below illustrates the areas that are classified 'positive' and 'negative'</pre>
<div class="CDPAlignCenter CDPAlign"><img height="286" width="356" class=" image-border" src="assets/469a697c-7c5c-457e-a232-e330b01b3e66.png"/></div>
<pre># Creating and evaluating the Confusion Matrix for the SVM model<br/><br/>svm_predicted&lt;- predict(svm_model,test_diab[,-ncol(test_diab)]) 
confusionMatrix(svm_predicted,test_diab$diabetes) <br/><br/>Confusion Matrix and Statistics 
 
          Reference 
Prediction negpos 
neg  93  26 
pos7  27 
 
Accuracy : 0.7843           
                 95% CI : (0.7106, 0.8466) 
    No Information Rate : 0.6536           
    P-Value [Acc&gt; NIR] : 0.0003018        
 
Kappa : 0.4799           
Mcnemar's Test P-Value : 0.0017280        
 
Sensitivity : 0.9300           
Specificity : 0.5094           
PosPredValue : 0.7815           
NegPredValue : 0.7941           
Prevalence : 0.6536           
         Detection Rate : 0.6078           
   Detection Prevalence : 0.7778           
      Balanced Accuracy : 0.7197           
 
       'Positive' Class :neg </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The K-Means machine learning technique</h1>
                </header>
            
            <article>
                
<p>K-Means is one of the most popular unsupervised machine learning techniques that is used to create clusters, and so categorizes data.</p>
<p>An intuitive example could be posed as follows:</p>
<p>Say a university was offering a new course on American History and Asian History. The university maintains a 15:1 student-teacher ratio, so there is 1 teacher per 15 students. It has conducted a survey which contains a 10-point numeric score that was assigned by each student to their preference of studying American History or Asian History.</p>
<p>We can use the in-built K-Means algorithm in R to create 2 clusters and presumably, by the number of points in each cluster, it may be possible to get an estimate of the number of students who may sign up for each course. The code for the same is given as follows:</p>
<pre>library(data.table) 
library(ggplot2) 
library() 
 
historyData&lt;- fread("~/Desktop/history.csv") 
ggplot(historyData,aes(american_history,asian_history)) + geom_point() + geom_jitter() 
 
historyCluster&lt;- kmeans(historyData,2) # Create 2 clusters 
historyData[,cluster:=as.factor(historyCluster$cluster)] 
ggplot(historyData, aes(american_history,asian_history,color=cluster)) + geom_point() + geom_jitter()<br/><br/># The image below shows the output of the ggplot command. Note that the effect of geom_jitter can be seen in the image below (the points are nudged so that overlapping points can be easily visible)</pre>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">The following image could provide an intuitive estimate of the number of students who may sign up for each course (and thereby determine how many teachers may be required):</p>
<div class="CDPAlignCenter CDPAlign"><img style="color: #333333;font-size: 1em;border: 1em solid black;text-align: center" height="311" width="386" class=" image-border" src="assets/33e06eed-9895-473c-a410-34899e68a800.png"/></div>
<p>There are several variations of the K-Means algorithm, but the standard and the most commonly used one is Lloyd's Algorithm. The algorithm steps are as follows:</p>
<p>Given a set of n points (say in an x-y axis), in order to find k clusters:</p>
<ol>
<li>Select k points at random from the dataset to represent the mid-points for k clusters (say, the <em>initial centroids</em>).</li>
<li>The distance from each of the other points to the selected k points (representing k clusters) is measured and assigned to the cluster that has the lowest distance from the point.</li>
<li>The cluster centers are recalculated as the mean of the points in the cluster.</li>
<li>The distance between the centroids and all the other points are again calculated as in Step 2 and new centroids are calculated as in Step 3. In this manner, Steps 2 and 3 are repeated until no new data is re-assigned.</li>
</ol>
<p>Various <em>distance and similarity measures</em> exist for clustering, such as <strong>Euclidean Distance</strong> (straight-line distance), <strong>Cosine Similarity</strong> (Cosine of angles between vectors), <strong>Hamming Distance</strong> (generally used for categorical variables), <strong>Mahalanobis Distance</strong> (named after P.C. Mahalanobis; this measures the distance between a point and the mean of a distribution), and others.</p>
<p>Although the optimal number of clusters cannot always be unambiguously identified, there are various methods that attempt to find an estimate. In general, clusters can be measured by how close points within a cluster are to one another (within cluster variance, such as the sum of squares--WSS) and how far apart the clusters are (so higher distances between clusters would make the clusters more readily distinguishable). One such method that is used to determine the optimal number is known as the <strong>elbow method</strong>. The following chart illustrates the concept:</p>
<div class="CDPAlignCenter CDPAlign"><img height="184" width="349" class=" image-border" src="assets/c0a29271-8d0e-4b07-9d74-bd3c2dc982c5.png"/></div>
<p>The chart shows a plot of the WSS (within the cluster sum of squares that we're seeking to minimize) versus the number of clusters. As is evident, increasing the number of clusters from 1 to 2 decreases the WSS value substantially. The value for WSS decreases rapidly up until the 4<sup>th</sup> or 5<sup>th</sup> cluster, when adding more clusters does not lead to a significant improvement in WSS. By visual assessment, the machine learning practitioner can conclude that the ideal number of clusters that can be created is between 3-5, based on the image.</p>
<div class="packt_infobox">Note that a low WSS score is not enough to determine the optimal number of clusters. It has to be done by inspecting the improvement in the metric. The WSS will eventually reduce to 0 when each point becomes an independent cluster.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The neural networks related algorithms</h1>
                </header>
            
            <article>
                
<p>Neural Network related algorithms have existed for many decades. The first computational model was described by Warren McCulloch and Walter Pitts in 1943 in the Bulletin of Mathematical Biophysics.</p>
<div class="packt_tip">You can learn more about these concepts at <a href="https://pdfs.semanticscholar.org/5272/8a99829792c3272043842455f3a110e841b1.pdf" target="_blank">https://pdfs.semanticscholar.org/5272/8a99829792c3272043842455f3a110e841b1.pdf</a> and <a href="https://en.wikipedia.org/wiki/Artificial_neural_network" target="_blank">https://en.wikipedia.org/wiki/Artificial_neural_network</a>.</div>
<p>Various man-made objects in the physical world, such as aeroplanes, have drawn inspiration from nature. A neural network is in essence a representation of the phenomenon of data exchange between the axons and dendrons (also known as dendrites) of neurons in the <em>human nervous system</em>. Just as data passes between one neuron to multiple other neurons to make complex decisions, an artificial neural network in similar ways creates a network of neurons that receive input from other neurons.</p>
<p>At a high level, an artificial neural network consists of 4 main components:</p>
<ul>
<li>Input Layer</li>
<li>Hidden Layer(s)</li>
<li>Output Layer</li>
<li>Nodes and Weights</li>
</ul>
<p>This is depicted in the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img height="255" width="361" class=" image-border" src="assets/1664ecf7-b0e7-4a0b-bcca-457fa1f07629.png"/></div>
<p>Each node in the diagram produces an output based on the input from the preceding layer. The output is produced using an <strong>activation function</strong>. There are various types of activation functions and the output produced depends on the type of function used. Examples include binary step (0 or 1), tanh (between -1 and +1), sigmoid, and others.</p>
<p>The following diagram illustrates the concept:</p>
<div class="CDPAlignCenter CDPAlign"><img style="color: #333333;font-size: 1em" height="109" width="293" class=" image-border" src="assets/a84f0aef-960d-4e7a-b280-d68dd85b9e11.png"/></div>
<p><span>The values x1 and x2 are the inputs, w1 and w2 represent the weights, and the node represents the point at which the inputs and their weights are evaluated and a specific output is produced by the activation function. The output f can thus be represented by:</span></p>
<p style="padding-left: 210px"><img height="35" width="130" class=" image-border" src="assets/889c62d8-9180-48a5-bfe3-06c3cdcd33ed.png"/></p>
<p>Here, f represents the activation function, and b represents the bias term. The bias term is independent of the weights and the input values and allows the user to shift the output to achieve a better model performance.</p>
<p>Neural networks with multiple hidden layers (generally 2 or more) are computationally intensive, and in recent days, neural networks with multiple hidden layers, also known as deep neural networks or more generally deep learning, have become immensely popular.</p>
<p>A lot of the developments in the industry, driven by machine learning and artificial intelligence, have been the direct result of the implementation of such multi-layer neural networks.</p>
<p>In R, the package <kbd>nnet</kbd> provides a readily usable interface to neural networks. Although in practice, neural networks generally require sophisticated hardware, GPU cards, and so on for illustration purposes, we have leveraged the <kbd>nnet</kbd> package to run the earlier classification exercise on the <kbd>PimaIndiansDiabetes</kbd> dataset. In the example, we will leverage caret in order to execute the <kbd>nnet</kbd> model:</p>
<pre>library(mlbench) 
library(caret) 
set.seed(123) 
 
 
data("PimaIndiansDiabetes") 
diab&lt;- PimaIndiansDiabetes 
 
train_ind&lt;- createDataPartition(diab$diabetes,p=0.8,list=FALSE,times=1) 
 
training_diab&lt;- diab[train_ind,] 
test_diab&lt;- diab[-train_ind,] 
 
nnet_grid&lt;- expand.grid(.decay = c(0.5,0.1), .size = c(3,5,7)) 
 
nnet_model&lt;- train(diabetes ~ ., data = training_diab, method = "nnet", metric = "Accuracy", maxit = 500, tuneGrid = nnet_grid) 
<br/># Generating predictions using the neural network model
nnet_predicted &lt;- predict(nnet_model, test_diab)<br/><br/>&gt; plot (nnet_model)<br/><br/><br/></pre>
<div class="CDPAlignCenter CDPAlign"><img height="305" width="379" class=" image-border" src="assets/c6baa443-294b-4514-8dcb-38bb030d0fbf.png"/></div>
<pre># Confusion Matrix for the Neural Network model<br/><br/>confusionMatrix(nnet_predicted,test_diab$diabetes)<br/><br/>Confusion Matrix and Statistics 
 
          Reference 
Prediction negpos 
neg  86  22 
pos  14  31 
 
Accuracy : 0.7647           
                 95% CI : (0.6894, 0.8294) 
    No Information Rate : 0.6536           
    P-Value [Acc&gt; NIR] : 0.001988         
 
Kappa : 0.4613           
Mcnemar's Test P-Value : 0.243345         
 
Sensitivity : 0.8600           
Specificity : 0.5849           
PosPredValue : 0.7963           
NegPredValue : 0.6889           
Prevalence : 0.6536           
         Detection Rate : 0.5621           
   Detection Prevalence : 0.7059           
      Balanced Accuracy : 0.7225           
 
       'Positive' Class :neg </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tutorial - associative rules mining with CMS data</h1>
                </header>
            
            <article>
                
<p>This tutorial will implement an interface for accessing rules created using the Apriori Package in R.</p>
<p>We'll be downloading data from the CMS OpenPayments website. The site hosts data on payments made to physicians and hospitals by companies:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/f21aafc2-9e34-4665-865d-54eda8ee3aad.png"/></div>
<p>The site provides various ways of downloading data. Users can select the dataset of interest and download it manually. In our case, we will download the data using one of the Web-based APIs that is available to all users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading the data</h1>
                </header>
            
            <article>
                
<p>The dataset can be downloaded either at the Unix terminal (in the virtual machine) or by accessing the site directly from the browser. If you are downloading the dataset in the Virtual Machine, run the following command in the terminal window:</p>
<pre>time wget -O cms2016_2.csv 'https://openpaymentsdata.cms.gov/resource/vq63-hu5i.csv?$query=select Physician_First_Name as firstName,Physician_Last_Name as lastName,Recipient_City as city,Recipient_State as state,Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name as company,Total_Amount_of_Payment_USDollars as payment,Nature_of_Payment_or_Transfer_of_Value as paymentNature,Product_Category_or_Therapeutic_Area_1 as category,Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_1 as product where covered_recipient_type like "Covered Recipient Physician" and Recipient_State like "NY" limit 1200000' </pre>
<p>Alternatively, if you are downloading the data from a browser, enter the following URL in the browser window and hit <em>Enter</em>:</p>
<div class="packt_infobox"><a href="https://openpaymentsdata.cms.gov/resource/vq63-hu5i.csv?%24query=select%20Physician_First_Name%20as%20firstName,Physician_Last_Name%20as%20lastName,Recipient_City%20as%20city,Recipient_State%20as%20state,Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name%20as%20company,Total_Amount_of_Payment_USDollars%20as%20payment,Nature_of_Payment_or_Transfer_of_Value%20as%20paymentNature,Product_Category_or_Therapeutic_Area_1%20as%20category,Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_1%20as%20product%20where%20covered_recipient_type%20like%20%22Covered%20Recipient%20Physician%22%20and%20Recipient_State%20like%20%22NY%22"><span class="URLPACKT">https://openpaymentsdata.cms.gov/resource/vq63-hu5i.csv?$query=select Physician_First_Name as firstName,Physician_Last_Name as lastName,Recipient_City as city,Recipient_State as state,Submitting_Applicable_Manufacturer_or_Applicable_GPO_Name as company,Total_Amount_of_Payment_USDollars as payment,Nature_of_Payment_or_Transfer_of_Value as paymentNature,Product_Category_or_Therapeutic_Area_1 as category,Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_1 as product where covered_recipient_type like "Covered Recipient Physician" and Recipient_State like "NY"</span></a></div>
<p>As shown in the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/2f67c791-65e6-4e7c-aca8-8011b4e0eb33.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the R code for Apriori</h1>
                </header>
            
            <article>
                
<p>The Apriori algorithm, as explained earlier, allows users to find relationships or patterns inherent in a dataset. For this, we will use the arules package in R/RStudio. The code will read the dataset downloaded (called <kbd>cms2016_2.csv</kbd> in the example) and run the apriori algorithm in order to find associative rules.</p>
<p>Create a new R file in RStudio and enter the following code. Make sure that you change the location of the csv file that you downloaded to the appropriate directory where the file has been stored:</p>
<pre>library(data.table) 
library(arules) 
 
cms&lt;- fread("~/cms2016_2.csv") # CHANGE THIS TO YOUR LOCATION OF THE DATA 
 
cols &lt;- c("category","city","company","firstName","lastName","paymentNature","product") 
 
cms[ ,(cols) := lapply(.SD, toupper), .SDcols = cols] 
 
cms[,payment:=as.numeric(payment)] 
 
quantile_values&lt;- quantile(cms$payment,seq(0,1,.25)) 
interval_values&lt;- findInterval(cms$payment,quantile_values,rightmost.closed=TRUE) 
 
cms[,quantileVal:=factor(interval_values, labels=c("0-25","25-50","50-75","75-100"))] 
 
rules_cols&lt;- c("category","city","company","paymentNature","product","quantileVal") 
 
cms[ ,(rules_cols) := lapply(.SD, factor), .SDcols = rules_cols] 
 
cms_factor&lt;- cms[,.(category,city,company,paymentNature,product,quantileVal)] 
 
rhsVal&lt;- paste0("quantileVal","=",c("0-25","25-50","50-75","75-100")) 
 
cms_rules&lt;- apriori(cms_factor,parameter=list(supp=0.001,conf=0.25,target="rules",minlen=3)) 
 
cms_rules_dt&lt;- data.table(as(cms_rules,"data.frame")) 
cms_rules_dt[, c("LHS", "RHS") := tstrsplit(rules, "=&gt;", fixed=TRUE)] 
num_cols&lt;- c("support","confidence","lift") 
cms_rules_dt[,(num_cols) := lapply(.SD, function(x){round(x,2)}), .SDcols = num_cols] 
 
saveRDS(cms_rules_dt,"cms_rules_dt.rds") 
saveRDS(cms_factor,"cms_factor_dt.rds") </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shiny (R Code)</h1>
                </header>
            
            <article>
                
<p>In RStudio, select <span class="packt_screen">File</span> | <span class="packt_screen">New File</span> | <span class="packt_screen">Shiny Web App:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="191" width="374" class=" image-border" src="assets/f0044242-690a-47a1-92bd-7c09da18d9bb.png"/></div>
<p>Enter the following code in <kbd>app.R</kbd><span class="packt_screen">:</span></p>
<pre># Packt: Big Data Analytics 
# Chapter 8 Tutorial 
 
library(shiny) 
library(shinydashboard) 
library(data.table) 
library(DT) 
library(shinyjs) 
 
 
cms_factor_dt&lt;- readRDS("~/r/rulespackt/cms_factor_dt.rds") 
cms_rules_dt&lt;- readRDS("~/r/rulespackt/cms_rules_dt.rds") 
 
# Define UI for application that draws a histogram 
ui&lt;- dashboardPage (skin="green",    
dashboardHeader(title = "Apriori Algorithm"), 
dashboardSidebar( 
useShinyjs(), 
sidebarMenu( 
uiOutput("company"), 
uiOutput("searchlhs"), 
uiOutput("searchrhs"), 
uiOutput("support2"), 
uiOutput("confidence"), 
uiOutput("lift"), 
downloadButton('downloadMatchingRules', "Download Rules") 
 
         ) 
),dashboardBody( 
tags$head( 
tags$link(rel = "stylesheet", type = "text/css", href = "packt2.css"), 
tags$link(rel = "stylesheet", type = "text/css", href = "//fonts.googleapis.com/css?family=Fanwood+Text"), 
tags$link(rel = "stylesheet", type = "text/css", href = "//fonts.googleapis.com/css?family=Varela"), 
tags$link(rel = "stylesheet", type = "text/css", href = "fonts.css"), 
 
tags$style(type="text/css", "select { max-width: 200px; }"), 
tags$style(type="text/css", "textarea { max-width: 185px; }"), 
tags$style(type="text/css", ".jslider { max-width: 200px; }"), 
tags$style(type='text/css', ".well { max-width: 250px; padding: 10px; font-size: 8px}"), 
tags$style(type='text/css', ".span4 { max-width: 250px; }") 
 
         ), 
fluidRow( 
dataTableOutput("result") 
) 
       ), 
       title = "Aprior Algorithm" 
) 
 
# Define server logic required to draw a histogram 
server &lt;- function(input, output, session) { 
 
  PLACEHOLDERLIST2 &lt;- list( 
    placeholder = 'Select All', 
onInitialize = I('function() { this.setValue(""); }') 
  ) 
 
output$company&lt;- renderUI({ 
datasetList&lt;- c("Select All",as.character(unique(sort(cms_factor_dt$company)))) 
selectizeInput("company", "Select Company" ,  
datasetList, multiple = FALSE,options = PLACEHOLDERLIST2,selected="Select All") 
  }) 
 
output$searchlhs&lt;- renderUI({ 
textInput("searchlhs", "Search LHS", placeholder = "Search") 
  }) 
 
output$searchrhs&lt;- renderUI({ 
textInput("searchrhs", "Search RHS", placeholder = "Search") 
  }) 
 
  output$support2 &lt;- renderUI({ 
sliderInput("support2", label = 'Support',min=0,max=0.04,value=0.01,step=0.005) 
  }) 
 
output$confidence&lt;- renderUI({ 
sliderInput("confidence", label = 'Confidence',min=0,max=1,value=0.5) 
  }) 
 
output$lift&lt;- renderUI({ 
sliderInput("lift", label = 'Lift',min=0,max=10,value=0.8) 
  }) 
 
dataInput&lt;- reactive({ 
    print(input$support2) 
    print(input$company) 
    print(identical(input$company,"")) 
 
    temp &lt;- cms_rules_dt[support &gt; input$support2 &amp; confidence &gt;input$confidence&amp; lift &gt;input$lift] 
 
    if(!identical(input$searchlhs,"")){ 
searchTerm&lt;- paste0("*",input$searchlhs,"*") 
      temp &lt;- temp[LHS %like% searchTerm] 
    } 
 
    if(!identical(input$searchrhs,"")){ 
searchTerm&lt;- paste0("*",input$searchrhs,"*") 
      temp &lt;- temp[RHS %like% searchTerm] 
    } 
 
if(!identical(input$company,"Select All")){ 
      # print("HERE") 
      temp &lt;- temp[grepl(input$company,rules)] 
    } 
    temp[,.(LHS,RHS,support,confidence,lift)] 
  }) 
 
output$downloadMatchingRules&lt;- downloadHandler( 
    filename = "Rules.csv", 
    content = function(file) { 
      write.csv(dataInput(), file, row.names=FALSE) 
    } 
  ) 
 
output$result&lt;- renderDataTable({ 
    z = dataInput() 
    if (nrow(z) == 0) { 
      z &lt;- data.table("LHS" = '', "RHS"='', "Support"='', "Confidence"='', "Lift" = '') 
    } 
setnames(z, c("LHS", "RHS", "Support", "Confidence", "Lift")) 
datatable(z,options = list(scrollX = TRUE)) 
  }) 
 
}  shinyApp(ui = ui, server = server)</pre>
<p>The following image shows the code being copied and saved in a file called <kbd>app.R</kbd>.</p>
<div class="CDPAlignCenter CDPAlign"><img height="432" width="554" class=" image-border" src="assets/fd2894d0-c621-4d9e-b169-450f2408f273.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using custom CSS and fonts for the application</h1>
                </header>
            
            <article>
                
<p>For our application, we will use a custom CSS File. We will also use custom fonts in order to give the application a nice look-and-feel.</p>
<p>You can download the custom CSS File from the software repository for this book.</p>
<p>The CSS, Fonts, and other related files should be stored in a folder called <kbd>www</kbd> in the directory where you created the R Shiny Application:</p>
<div class="CDPAlignCenter CDPAlign"><img height="246" width="404" class=" image-border" src="assets/429dc22c-98bc-473a-89dc-7961b7811cc9.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the application</h1>
                </header>
            
            <article>
                
<p>If all goes well, you should be now able to run the application by clicking on the <span class="packt_screen">Run App</span> option on the top of the page, as shown in the following images:</p>
<div class="CDPAlignCenter CDPAlign"><img height="238" width="512" class=" image-border" src="assets/ebc58a2c-7459-439d-9364-d22e13b55c10.png"/></div>
<p>Upon clicking the "Run" button, the user will see a popup window similar to the one shown below. Note that popups should be enabled in the browser for this to function.</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/fa672f9b-88b0-4b6b-8da1-970452c79e7a.png"/></div>
<p>The app has multiple controls, such as:</p>
<ul>
<li><strong>Search LHS/RHS</strong>: Enter any test that you want to filter for, in the Left-Hand Side or the Right-Hand Side of the rule.</li>
<li><strong>Support</strong>: Indicates the prevalence of the rule in the dataset.</li>
<li><strong>Confidence</strong>: Of the rules, how many were exact matches.</li>
<li><strong>Lift</strong>: Variable defining the importance of a rule. Numbers above 1 are considered significant.</li>
</ul>
<p>You can use this app for any other rules file as long as they are processed in a way similar to the one outlined before in the R Script section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Machine learning practitioners are often of the opinion that creating models is easy, but creating a good one is much more difficult. Indeed, not only is creating a <em>good</em> model important, but perhaps more importantly, knowing how to identify a <em>good</em> model is what distinguishes successful versus less successful Machine Learning endeavors.</p>
<p>In this chapter, we read up on some of the deeper theoretical concepts in Machine Learning. Bias, Variance, Regularization, and other common concepts were explained with examples as and where needed. With accompanying R code, we also learnt about some of the common machine learning algorithms such as Random Forest, Support Vector Machines, and others. We concluded with a tutorial on how to create an exhaustive web-based application for Association Rules Mining against CMS OpenPayments data.</p>
<p>In the next chapter, we will read about some of the technologies that are being used in enterprises for both big data as well as machine learning. We will also discuss the merits of cloud computing and how they are influencing the selection of enterprise software and hardware stacks.</p>


            </article>

            
        </section>
    </body></html>