- en: Cleaning Messy Data
  prefs: []
  type: TYPE_NORMAL
- en: Data analysts and scientists spend most of their time cleaning data and pre-processing
    messy datasets. While this activity is less talked about, it is one of the most
    performed activities and one of the most important skills for any data professional.
    Mastering the skill of data cleaning is necessary for any aspiring data scientist.
    Data cleaning and pre-processing is the process of identifying, updating, and
    removing corrupt or incorrect data. Cleaning and pre-processing results in high-quality
    data for robust and error-free analysis. Quality data can beat complex algorithms
    and outperform simple and less complex algorithms. In this context, high quality
    means accurate, complete, and consistent data. Data cleaning is a set of activities
    such as handling missing values, removing outliers, feature encoding, scaling,
    transformation, and splitting.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on data cleaning, manipulation, and wrangling. Data preparation,
    manipulation, wrangling, and munging are all terms for the same thing, and the
    main objective is to clean up the data in order to get valuable insights. We will
    start by exploring employee data and then start filtering the data and handling
    missing values and outliers. After cleaning, we will focus on performing data
    transformation activities such as encoding, scaling, and splitting. We will mostly
    be using `pandas` and `scikit-learn` in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering data to weed out the noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature encoding techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature splitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code and the datasets that will be used in this chapter in
    this book's GitHub repository at [https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter07](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter07).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code is available in the `ch7.ipynb` file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter uses only one CSV file (`employee.csv`) for practice purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will use the `pandas` and `scikit-learn` Python libraries,
    so please ensure you have them installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore data by performing **Exploratory Data Analysis**
    (**EDA**). EDA is the most critical and most important component of the data analysis
    process. EDA offers the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: It provides an initial glimpse of data and its context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It captures quick insights and identifies the potential drivers from the data
    for predictive analysis. It finds the queries and questions that can be answered
    for decision-making purposes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It assesses the quality of the data and helps us build the road map for data
    cleaning and preprocessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It finds missing values, outliers, and the importance of features for analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EDA uses descriptive statistics and visualization techniques to explore data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In EDA, the first step is to read the dataset. We can read the dataset using
    `pandas`. The `pandas` library offers various options for reading data. It can
    read files in various formats, such as CSV, Excel, JSON, parquet, HTML, and pickle.
    All these methods were covered in the previous chapter. After reading the data,
    we can explore the data. This initial exploration will help us understand the
    data and gain some domain insights. Let's start with the EDA process.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will read the `employee.csv` file (you can find this file in the
    `Chapter-7` folder of this book''s GitHub repository at [https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter07/employee.csv](https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter07/employee.csv)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the first five records in the file using the `head()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d02b414-5c80-4d3a-8499-ecaa7ec598c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, let''s look at the last five records using the `head()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51201083-8298-496b-9d0c-b5ace193c3b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can check the list of columns using the `columns` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check out the list of columns by using the shape of the DataFrame by
    using the `shape` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the dataset has `9` rows and `7` columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check the table schema, its columns, rows, data types, and missing values
    in the DataFrame by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/166eeb97-d7d1-43f2-ae99-8eeafbbcdcca.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding output, you can see that there are 7 columns in the data. Out
    of these 7 columns, 3 columns (age, income, and gender) have missing values. Out
    of these 7 columns, 4 are objects, 2 are floats, and 1 is an integer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at the descriptive statistics of the data by using
    the `describe` function. This function will describe numerical objects. In our
    example, the age, income, and performance scores will describe the count, mean,
    standard deviation, min-max, and the first, second, and third quartiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3817ae7-9822-4147-a125-27a142c0b676.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code block, we have checked the descriptive statistics values
    of the data using the `describe()` function. From these results, we can interpret
    that the employee's age is ranging from 23 to 54 years. Here, the mean age is
    40 years and the median age is 45 years. Similarly, we can draw conclusions for
    income and performance scores. Now that we've described the data, let's learn
    how to filter noise from data.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering data to weed out the noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last two decades, the data size of companies and government agencies
    has increased due to digitalization. This also caused an increase in consistency,
    errors, and missing values. Data filtering is responsible for handling such issues
    and optimizing them for management, reporting, and predictions. The filtering
    process boosts the accuracy, relevance, completeness, consistency, and quality
    of the data by processing dirty, messy, or coarse datasets. It is a very crucial
    step for any kind of data management because it can make or break a competitive
    edge of business. Data scientists need to master the skill of data filtering.
    Different kinds of data need different kinds of treatment. That's why a systematic
    approach to data filtering needs to be taken.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we learned about data exploration, while in this section,
    we will learn about data filtering. Data can be filtered either column-wise or
    row-wise. Let's explore them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Column-wise filtration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will learnhow to filter column-wise data. We can filter
    columns using the `filter()` method. The `slicing []. filter()` method selects
    the columns when they''re passed as a list of columns. Take a look at the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/acc285e8-6ca1-49dd-84d5-445a40c8fd8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can also filter columns using slicing. In slicing, a single column
    does not need a list, but when we are filtering multiple columns, then they should
    be on the list. The output of a single column is a pandas Series. If we want the
    output as a DataFrame, then we need to put the name of the single column into
    a list. Take a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we have selected a single column without passing it
    into the list and the output is a pandas Series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s select a single column using a Python list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f42be3d8-d872-45b4-b40c-c39aa268a364.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, a single column can be selected using a Python list. The output
    of this filter is a pandas DataFrame with a single column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s filter multiple columns from the pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea038b30-cb77-48f8-bd72-7a063d4a0579.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have filtered the two columns without using the `filter()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Row-wise filtration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s filter row-wise data. We can filter data using indices, slices,
    and conditions. In indices, you have to pass the index of the record, while for
    slicing, we need to pass the slicing range. Take a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d16add6-cd38-4581-ab1c-96fc90668c35.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding example, we have filtered the data based on indexes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of filtering data by slicing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be28f328-311c-4eb3-b66b-68902c7adfff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In condition-based filtration, we have to pass some conditions in square brackets,
    `[ ]`, or brackets, `( )`. For a single value, we use the `==` (double equal to)
    condition, while for multiple values, we use the `isin()` function and pass the
    list of values. Let''s take a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f23ef39-6cf0-413a-8d85-64ff83f73846.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding code, we filtered the department sales in the first line of
    code using `==` (double equal to) as a condition. Now, let''s filter multiple
    columns using the `isin()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d775a08-dd7b-4a96-9775-284254f7d2a0.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding example, we filtered the department sales and finance department
    using the `isin()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the `>=` and `<=` conditions for continuous variables.
    We can have single or multiple conditions. Let''s take a look at the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb7360aa-658f-4f1c-80a4-429e794ff40c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding example, we filtered employees on the basis of their performance
    score (performance_score >=700). Now, let''s filter data using multiple conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac661fab-a43c-4c91-b0e9-23a07e3b9c75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also try the `query()` method. This method queries the columns using
    a boolean expression. Let''s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/050ffa66-9c67-4c42-b988-b30fa16c3d76.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding example, we filtered the employees who have performance scores
    less than 500\. Now, let's learn how to handle missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Missing values are the values that are absent from the data. Absent values
    can occur due to human error, privacy concerns, or the value not being filled
    in by the respondent filling in the survey. This is the most common problem in
    data science and the first step of data preprocessing. Missing values affect a
    machine learning model''s performance. Missing values can be handled in the following
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Drop the missing value records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill in the missing value manually.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill in the missing values using the measures of central tendency, such as mean,
    median, and mode. The mean is used to impute the numeric feature, the median is
    used to impute the ordinal feature, and the mode or highest occurring value is
    used to impute the categorical feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill in the most probable value using machine learning models such as regression,
    decision trees, KNNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to understand that in some cases, missing values will not impact
    the data. For example, driving license numbers, social security numbers, or any
    other unique identification numbers will not impact the machine learning models
    because they can't be used as features in the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will look at how missing values can be handled
    in more detail. First, we'll learn how to drop missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Python, missing values can be dropped using the `dropna()` function. `dropna`
    takes one argument: `how`. `how` can take two values: `all` or `any`. `any` drops
    certain rows that contain NAN or missing values, while `all` drops all the rows
    contains NAN or missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7ec72c4-477c-4d59-9eb6-a5214c6913f0.png)'
  prefs: []
  type: TYPE_IMG
- en: This summarizes the dataset as a dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: Filling in a missing value
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Python, missing values can be dropped using the `fillna()` function. The
    `fillna()` function takes one value that we want to fill at the missing place.
    We can fill in the missing values using the mean, median, and mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d5464ea-c441-4d9d-9b0d-20a15aed67ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding example, the missing values in the age column have been filled
    in with the mean value of the age column. Let''s learn how to fill in the missing
    values using the median:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03216565-ff03-4564-a983-d8d3f062abc7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding example, the missing values in the income column have been
    filled in with the median value of the income column. Let''s learn how to fill
    in missing values using the mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07110f3e-266b-4b0a-ad09-b26324b99100.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code example, the missing values in the gender column have
    been filled in with the mode value of the gender column. As you have seen, the
    mean, median, and mode help us handle missing values in pandas DataFrames. In
    the next section, we will focus on how to handle outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Handling outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outliers are those data points that are distant from most of the similar points
    – in other words, we can say the outliers are entities that are different from
    the crowd. Outliers cause problems when it comes to building predictive models,
    such as long model training times, poor accuracy, an increase in error variance,
    a decrease in normality, and a reduction in the power of statistical tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of outliers: univariate and multivariate. Univariate outliers
    can be found in single variable distributions, while multivariates can be found
    in n-dimensional spaces. We can detect and handle outliers in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Box Plot**: We can use a box plot to create a bunch of data points through
    quartiles. It groups the data points between the first and third quartile into
    a rectangular box. The box plot also displays the outliers as individual points
    using the interquartile range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scatter Plot**: A scatter plot displays the points (or two variables) on
    the two-dimensional chart. One variable is placed on the x-axis, while the other
    is placed on the y-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Z-Score**: The Z-score is a kind of parametric approach to detecting outliers.
    It assumes a normal distribution of the data. The outlier lies in the tail of
    the normal curve distribution and is far from the mean:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/94e69c69-f0e5-4091-b438-416b93b77c12.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Interquartile Range (IQR)**: IQR is a robust statistical measure of data
    dispersion. It is the difference between the third and first quartile. These quartiles
    can be visualized in a box plot. This is also known as the midspread, the middle
    50%, or H-spread:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/60d1ecea-dfd3-41bd-a8ca-50b0eb2ce248.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Percentile**: A percentile is a statistical measure that divides data into
    100 groups of equal size. Its value indicates the percentage of the population
    below that value. For example, the 95th percentile means 95% of people fall under
    this category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s drop some outliers using standard deviation and the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ae1c2b9-ca13-4b8f-af45-3d4e6c245f0a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding example, we are handling the outliers using standard deviation
    and the mean. We are using ![](img/4a8e21b2-2dfc-4fd4-b71f-83efba392c62.png) as
    the upper limit and ![](img/d88fce77-3079-49f8-9481-b6a4bd758751.png) as the lower
    limit for filtering the outliers. We can also try the percentile values to remove
    the outliers. Let''s take a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98b31a6c-6ed8-4cfd-9ef2-91ebf8e8f072.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code example, we handled the outliers using percentiles. We
    removed the outliers by using a percentile of 1 for the lower limit and by using
    a percentile of 99 for the upper limit. This helps us handle outliers in pandas
    DataFrames. In the next section, we will focus on how to perform feature encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Feature encoding techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning models are mathematical models that required numeric and integer
    values for computation. Such models can't work on categorical features. That's
    why we often need to convert categorical features into numerical ones. Machine
    learning model performance is affected by what encoding technique we use. Categorical
    values range from 0 to N-1 categories.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One-hot encoding transforms the categorical column into labels and splits the
    column into multiple columns. The numbers are replaced by binary values such as
    1s or 0s. For example, let''s say that, in the `color` variable, there are three
    categories; that is, `red`, `green`, and `blue`. These three categories are labeled
    and encoded into binary columns, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/704e22f0-db8d-4d31-9459-d632d51797fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One-hot encoding can also be performed using the `get_dummies()` function.
    Let''s use the `get_dummies()` function as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51e5d4fd-f955-4cd3-96be-c331ebaeeba0.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see two extra columns, F and M. Both columns are dummy columns
    that were added by the Boolean encoder. We can also perform the same task with
    `OneHotEncoder` from the `scikit-learn` module. Let's look at an example of using
    `OneHotEncoder`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code example, we imported `OneHotEncoder`, initialized its
    object, and then fit and transformed the model on the gender column. We can see
    that the output array has two columns for female and male employees.
  prefs: []
  type: TYPE_NORMAL
- en: Label encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Label encoding is also known as integer encoding. Integer encoding replaces
    categorical values with numeric values. Here, the unique values in variables are
    replaced with a sequence of integer values. For example, let''s say there are
    three categories: red, green, and blue. These three categories were encoded with
    integer values; that is, `red` is 0, `green` is 1, and `blue` is 2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following label encoding example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we performed simple label encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we are encoding the department column using the `LabelEncoder`
    class. First, we must import and initialize the `LabelEncoder` object and then
    fit and transform the column that we want to encode. Let''s perform the inverse
    transformation on the encoded labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we reversed the encoding of the encoded values using
    `inverse_transformation()`. We can also use one-hot encoding with numerical variables.
    Here, each unique numeric value is encoded into an equivalent binary variable.
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ordinal encoding is similar to label encoding, except there's an order to the
    encoding. The output encoding will start from 0 and end at one less than the size
    of the categories. Let's look at an example containing employee grades such as
    G0, G1, G2, G3, and G4\. These five grades have been encoded with ordinal integer
    values; that is, `G0` is 0, `G1` is 1, `G2` is 2, `G3` is 3, and `G4` is 4\. We
    can define the order of the values as a list and pass it to the category parameter.
    The ordinal encoder uses the integer or numeric values to encode. Here, the integer
    and numeric values are ordinal in nature. This encoding helps machine learning
    algorithms take advantage of this ordinal relationship.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following `OrdinalEncoder` example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a85a5ae-87a2-47e7-9b6f-75b4814c48ec.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding example is similar to the `LabelEncoder` example, except for the
    order of the values that were passed when the `OrdinalEncoder` object was initialized.
    In this example, the `categories` parameters were passed alongside the `grade`
    order at the time of initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In real life, most features have different ranges, magnitudes, and units, such
    as age being between 0-200 and salary being between 0 to thousands or millions.
    From a data analyst or data scientist's point of view, how can we compare these
    features when they are on different scales? High-magnitude features will weigh
    more on machine learning models than lower magnitude features. Thankfully, feature
    scaling or feature normalization can solve such issues.
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling brings all the features to the same level of magnitude. This
    is not compulsory for all kinds of algorithms; some algorithms clearly need scaled
    data, such as those that rely on Euclidean distance measures such as K-nearest
    neighbor and the K-means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Methods for feature scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s look at the various methods we can use for feature scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standard Scaling or Z-Score Normalization**: This method computes the scaled
    values of a feature by using the mean and standard deviation of that feature.
    It is best suited for normally distributed data. Suppose [![](img/c70a5dad-c951-4138-b04c-706796d00869.png)]
    is the mean and [![](img/b2a11c1a-d40a-41ce-a8cb-d762bf9132ee.png)] is the standard
    deviation of the feature column. This results in the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1438fcd1-7e86-409c-8c14-e034bd0a83ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the following standard scaling example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4d91332-cd72-4946-9f97-2c0b19da5675.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we need to import and initialize the `StandardScaler` object. After initialization,
    we must perform fit and transform operations on the column that we want to scale.
  prefs: []
  type: TYPE_NORMAL
- en: '**Min-Max Scaling**: This method linearly transforms the original data into
    the given range. It preserves the relationships between the scaled data and the
    original data. If the distribution is not normally distributed and the value of
    the standard deviation is very small, then the min-max scaler works better since
    it is more sensitive to outliers. Let''s say that [![](img/88f1ca2e-0121-4f60-9263-574826d461ad.png)]
    is the minimum value and [![](img/7d5098d6-0da7-4477-9c34-f6f2646aaffa.png)]is
    the maximum value of a feature column, while ![](img/1d4830e9-dcbc-4692-b7d8-423e060735e3.png)
    and ![](img/a6fc4c91-4f05-4b5f-8486-1a1d98183a7e.png) are the new minimum and
    new maximum. This results in the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a13a857a-ff6e-4795-9e10-df7d5865fd52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the following min-max scaling example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77655e38-fc1e-40e7-b6cf-e018e3f3f644.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we need to import and initialize the `MinMaxScaler` object. After initialization,
    we, must perform the fit and transform operations on the column that we want to
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: '**Robust Scaling**: This method is similar to the min-max scaler method. Instead
    of min-max, this method uses an interquartile range. That''s why it is robust
    to outliers. Suppose ![](img/460af998-c191-4b5d-8e94-ce27f70f91dc.png)and ![](img/47c520a2-9875-422b-9fa7-7d9926f6fb43.png)
    are the first and third quartiles of column x. This results in the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/bed79988-4f47-4f1d-a43e-bd34e33ff648.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the following robust scaling example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/098a721a-ae9c-4f03-8d07-f40c52064ae0.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we need to import and initialize the `RobustScaler` object. After initialization,
    we must fit and transform the column that we want to scale.
  prefs: []
  type: TYPE_NORMAL
- en: Feature transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feature transformation alters features so that they''re in the required form.
    It also reduces the effect of outliers, handles skewed data, and makes the model
    more robust. The following list shows the different kinds of feature transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: Log transformation is the most common mathematical transformation used to transform
    skewed data into a normal distribution. Before applying the log transform, ensure
    that all the data values ​​only contain positive values; otherwise, this will
    throw an exception or error message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Square and cube transformation has a moderate effect on distribution shape.
    It can be used to reduce left skewness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Square and cube root transformation has a fairly strong transformation effect
    on the distribution shape but it is weaker than logarithms. It can be applied
    to right-skewed data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discretization can also be used to transform a numeric column or attribute.
    For example, the age of a group of candidates can be grouped into intervals such
    as 0-10, 11-20, and so on. We can also use discretization to assign conceptual
    labels instead of intervals such as youth, adult, and senior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the feature is right-skewed or positively skewed or grouped at lower values,
    then we can apply the square root, cube root, and logarithmic transformations,
    while if the feature is left-skewed or negative skewed or grouped at higher values,
    then we can apply the cube, square, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at an example of discretization transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb1d7b77-c332-45a4-ad61-c18d0418f707.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding example, we loaded the dataset and created the `performance_grade()`
    function. The `performance_grade()` function takes the performance score and converts
    it into grades; that is, `A`, `B`, and `C`.
  prefs: []
  type: TYPE_NORMAL
- en: Feature splitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature splitting helps data analysts and data scientists create more new features
    for modeling. It allows machine learning algorithms to comprehend features and
    uncover potential information for decision-making; for example, splitting name
    features into first, middle, and last name and splitting an address into house
    number, locality, landmark, area, city, country, and zip code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Composite features such as string and date columns violate the tidy data principles.
    Feature splitting is a good option if you wish to generate more features from
    a composite feature. We can utilize the components of a column to do this. For
    example, from a date object, we can easily get the year, month, and weekday. These
    features may directly affect the prediction model. There is no rule of thumb when
    it comes to breaking the features into components; this depends on the characteristics
    of the feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bf00656-9c59-40e1-b7d1-c2d6ec146ee1.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding example, we split the name column using the `split()` and `map()`
    functions. The `split()` function splits the name column using a space, while
    the `map()` function assigns the first divided string to the first name and the
    second divided string to the last name.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored data preprocessing and feature engineering with
    Python. This had helped you gain important skills for data analysis. The main
    focus of this chapter was on cleaning and filtering out dirty data. We started
    with EDA and discussed data filtering, handling missing values, and outliers.
    After this, we focused on feature engineering tasks such as transformation, feature
    encoding, feature scaling, and feature splitting. We then explored various methods
    and techniques we can use when it comes to feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, Chapter 8, *Signal Processing and Time Series*, we will
    focus on the importance of signal processing and time series data in Python. We'll
    start this chapter by analyzing time series data and discussing moving averages,
    autocorrelations, autoregressive models, and ARMA models. Then, we will look at
    signal processing and discuss Fourier transform, spectral transform, and filtering
    on signals.
  prefs: []
  type: TYPE_NORMAL
