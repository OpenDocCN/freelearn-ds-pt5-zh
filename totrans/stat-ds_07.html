<html><head></head><body><div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Regularization for Database Improvement</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this chapter, we will introduce the idea of statistical regularization to improve data models in an effort to help comprehend what statistical regularization is, why it is important as well as to feel comfortable with the various statistical regularization methods.</p>
<p class="calibre4">In this chapter, we've organized information into the following areas:</p>
<ul class="calibre18">
<li class="calibre19">Statistical regularization</li>
<li class="calibre19">Using data to understand statistical regularization</li>
<li class="calibre19">Improving data or a data model</li>
<li class="calibre19">Using R for statistical regularization</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Statistical regularization</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">So, what is statistical regularization?</p>
<p class="calibre4">With regularization, whether we are speaking about mathematics, statistics, or machine learning, we are essentially talking about a process of adding additional information in order to solve a problem.</p>
<p class="calibre4">The term <strong class="calibre7">regularization</strong> has been described as an abstract concept of management of complex systems<em class="calibre21"> </em>(according to a set of rules or accepted concepts). These rules will define how one can add or modify values in order to satisfy a requirement or solve a problem.</p>
<p class="calibre4">Does adding or modifying values mean changing data? (More about this will be studied later in this chapter.)</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Various statistical regularization methods</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Within the statistical community, the most popular statistical regularization methods may include the following:</p>
<ul class="calibre18">
<li class="calibre19">Ridge</li>
<li class="calibre19">Lasso</li>
<li class="calibre19">Least angles</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Ridge</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Ridge regression is a statistical technique that is used when analyzing regression data or models that suffer from a condition known as <strong class="calibre7">multicollinearity</strong>. When multicollinearity occurs, estimates may be unbiased but their variances are usually large and far from the true value. This technique adds a degree of bias to the regression estimates to reduce standard errors (to produce estimates that are more dependable).</p>
<div class="packt_infobox">Multicollinearity is a condition within statistics in which a predictor (variable) in multiple regression models can be linearly predicted from the others with a significant accuracy.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Lasso</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4"><strong class="calibre7">Least absolute shrinkage and selection operator</strong> (<strong class="calibre7">Lasso</strong>) is a statistical technique that performs both variable selection and regularization in an effort to enhance prediction accuracies within a model.</p>
<div class="packt_infobox">The process of choosing or selecting variables within a statistical model results, obviously, in reducing the number of variables, which is also referred to as variable shrinkage.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Least angles</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4"><strong class="calibre7">Least Angle Regression</strong> (<strong class="calibre7">LARS</strong>) is a statistical technique used by data scientists when dealing with high-dimensional data. If there is speculation that a response variable is determined by a particular subset of predictors, then the LARS technique can help with determining which variables to include in the regression process.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Opportunities for regularization</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">So, when would you, a data scientist, consider using any type of regularization method?</p>
<p class="calibre4">Well, the truth is that there is no absolute rule that dictates the use of regularization; however, there are certain indicators to observe that should cause you to consider <span class="calibre14">regularization</span>, for example:</p>
<ul class="calibre18">
<li class="calibre19">If your data contains a high variable count</li>
<li class="calibre19">If there is a low ratio of the number of observations to the number of variables in your data</li>
</ul>
<p class="calibre4">In <a href="8a0b3272-dfa0-46ca-9e90-f6050f2007cd.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">Chapter 6</a>, <em class="calibre21">Database Progression to Database Regression</em> (on statistical regression), we reviewed some sample data consisting of consulting project results. In that example, we explored the relationship between the total hours billed to the project, the total project management hours spent on the project, and the project's supposed profitability.</p>
<p class="calibre4">Looking closer at that same data, perhaps we may now see additional variables, such as the following:</p>
<ul class="calibre18">
<li class="calibre19">Number of consultants assigned to the project full time</li>
<li class="calibre19">Number of consultants assigned to the project part-time</li>
<li class="calibre19">Number of sub-contractors assigned to the project (full time or part time)</li>
<li class="calibre19">Number of customer resources assigned to the project full time</li>
<li class="calibre19">Number of customer resources assigned to the project part-time</li>
<li class="calibre19">Number of local resources assigned to the project</li>
<li class="calibre19">Years of experience with the projects core technology</li>
<li class="calibre19">Total project management hours</li>
<li class="calibre19">Total development hours</li>
<li class="calibre19">Hourly bill rate</li>
<li class="calibre19">Total hours invoiced</li>
<li class="calibre19">Number of technologies used in the project</li>
<li class="calibre19">Project style (time and materials, not to exceed, or staff augment)</li>
</ul>
<p class="calibre4">Here, we can see more than twelve possible independent or predictor variables—certainly a manageable number—especially given that the number of observations (records) in the file is over 100 (the ratio of variables to observations is about 12%).</p>
<div class="packt_infobox">An independent variable (or experimental or predictor variable) is a variable that is being manipulated in a model to observe the effect on a dependent variable, or an outcome variable.</div>
<p class="calibre4">When a data scientist speaks of high variable counts, they are really referring to an excessive number or, if the number of variables is approaching the number of observations, (not so in this example) but suppose we had more than 50 possible predictor variables in our data of only 100 observations? This is what can be referred to as an overly complex model and warrants consideration of using a common regulation method.</p>
<div class="packt_infobox">What constitutes as overly complex is often a subject for debate and often differs based on the data and objectives of the statistical model.</div>
<p class="calibre4">Experience shows us that when a model is excessively complex, a model may fit but have a poor predicting performance (which is ultimately the goal). When this occurs, a data scientist will recognize overfitting.</p>
<p class="calibre4">Regularization is the statistical technique used by data scientists to avoid or address this overfitting problem. The idea behind regularization is that models that overfit the data are complex statistical models that have, for example, too many parameters.</p>
<p class="calibre4">Other known opportunities for the use of regulation include the following:</p>
<ul class="calibre18">
<li class="calibre19">Instances involving high collinearity</li>
<li class="calibre19">When a project objective is a sparse solution</li>
<li class="calibre19">Accounting for variables grouping in high-dimensional data</li>
<li class="calibre19">Classification</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Collinearity</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The term <strong class="calibre7">collinearity</strong> describes a statistical situation when a selected predictor variable can be linearly predicted from the others with a considerable degree of accuracy.</p>
<div class="packt_infobox">Linear prediction is a procedure where future values of a variable are estimated based on a linear function of previous samples.</div>
<p class="calibre4">This typically allows very small changes to the data to produce unreliable results regarding individual predictor variables. That is, multiple regression models with collinear predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Sparse solutions</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">A <strong class="calibre7">sparse solution</strong> or <strong class="calibre7">approximation</strong> is a sparse vector that approximately solves an arrangement of equations. Techniques to find sparse approximations have found a wide use in applications such as image processing and document analysis.</p>
<div class="packt_infobox">You should recall that a <span class="calibre5">vector</span> is a sequence of data points of the same basic type. Members of a vector are officially called <span class="calibre5"><strong class="calibre3">components</strong>.</span></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">High-dimensional data</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4"><strong class="calibre7">High-dimensional statistics</strong> is the study of data where the number of dimensions is higher than the dimensions considered in the classical <strong class="calibre7">multivariate analysis </strong>(<strong class="calibre7">MVA</strong>).</p>
<div class="packt_infobox">In statistical studies, a <strong class="calibre3">multivariate random variable</strong> (or <strong class="calibre3">random vector</strong>) is a list of variables, each of whose value is unknown. MVA is defined as the study of this occasion.</div>
<p class="calibre4">High-dimensional statistics relies on the theory of random vectors. In many applications, the dimension of the data vectors may be larger than the sample size.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Classification</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4"><strong class="calibre7">Classification</strong> is the process of identifying to which of a set of categories or groups a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.</p>
<p class="calibre4">Regularization is a common statistical technique used to address the mentioned (as well as other) scenarios. In the next section, we'll look at some simple examples of each of these.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Using data to understand statistical regularization</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Variable selection is an imperative process within the field of statistics as it tries to make models simpler to understand, easier to train, and free of misassociations--by eliminating variables unrelated to the output.</p>
<p class="calibre4">This (variable selection) is one possible approach to dealing with the problem of overfitting. In general, we don't expect a model to completely fit our data; in fact, the problem of overfitting often means that it may be disadvantageous to our predictive model's accuracy on unseen data if we fit our training or test data too well.</p>
<p class="calibre4">Rather than using variable selection, the process of regularization is an alternative approach to reducing the number of variables in the data in order to deal with the issue of overfitting and is essentially a process of introducing an intentional bias or constraint in a training of a model that (hopefully) prevents our coefficients from exhibiting very high variances.</p>
<p class="calibre4">When the number of parameters (in a population) is deemed very large—particularly compared to the number of available observations—linear regression tends to allow small changes in a few of the observations to cause the coefficients to change drastically (or, as we already put it, exhibit very high variances).</p>
<p class="calibre4"><strong class="calibre7">Ridge regression</strong> is a statistical method that introduces a controlled bias (through or using a constraint) to the model's regression estimates but is effective at reducing the model's variance as well.</p>
<div class="packt_infobox">Ridge regression is sometimes referred to within the data scientist community as a penalized regression technique.</div>
<p class="calibre4">There are a number of different R functions and packages that implement ridge regression, such as <kbd class="calibre22">lm.ridge()</kbd> from the <kbd class="calibre22">MASS</kbd> package and <kbd class="calibre22">ridge()</kbd> from the <kbd class="calibre22">genridge</kbd> package.</p>
<div class="packt_infobox">You might be familiar with the <kbd class="calibre22">MASS</kbd> R package but perhaps not <kbd class="calibre22">genridge</kbd>. The <kbd class="calibre22">genridge</kbd> package introduces generalizations of the standard univariate ridge trace plot used in ridge regression and related methods and is worthy of additional investigation.</div>
<p class="calibre4">In <a href="8a0b3272-dfa0-46ca-9e90-f6050f2007cd.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2">Chapter 6</a>, <em class="calibre21">Database Progression to Database Regression</em> we proposed an example where we created a linear regression model on data from a consulting company's project results in an effort to predict a project's profitability. We used the R function: <kbd class="calibre22">lm()</kbd>, which takes in two main arguments: <kbd class="calibre22">formula</kbd> (an object of class formula) and <kbd class="calibre22">data</kbd> (typically a <kbd class="calibre22">data.frame</kbd>), as shown in the following R code:</p>
<pre class="calibre29"># --- build linear regression model using all the<br class="calibre2"/># --- project results data<br class="calibre2"/>alinearMod &lt;- lm(ProjectManagement ~ Profit, data=MyData)</pre>
<p class="calibre4">In this chapter, we are going to work with the <kbd class="calibre22">lm.ridge()</kbd> function in an attempt to acceptably fit the preceding linear model using ridge regression. The preceding code generated a linear model using our R object named <kbd class="calibre22">MyData</kbd>, using the <kbd class="calibre22">ProjectManagment</kbd> <span class="calibre14">variable </span>to predict <kbd class="calibre22">Profit</kbd>.</p>
<p class="calibre4">The <kbd class="calibre22">lm.ridge</kbd> function uses the following syntax:</p>
<pre class="calibre29">lm.ridge(formula, data, subset, na.action, lambda = 0, model = FALSE,<br class="calibre2"/>x = FALSE, y = FALSE, contrasts = NULL, ...)</pre>
<p class="calibre4">The arguments are included here for later reference:</p>
<ul class="calibre18">
<li class="calibre19"><kbd class="calibre22">formula</kbd>: This a formula expression as for regression models, of the form <kbd class="calibre22">response ~ predictors</kbd></li>
<li class="calibre19"><kbd class="calibre22">data</kbd>: This is an optional data frame in which to interpret the variables occurring in the formula</li>
<li class="calibre19"><kbd class="calibre22">subset</kbd>: This is an expression saying which subset of the rows of the data should be used in the fit. All observations are included by default</li>
<li class="calibre19"><kbd class="calibre22">na.action</kbd>: This a function to filter missing data</li>
<li class="calibre19"><kbd class="calibre22">lambda</kbd>: This is a scalar or vector of ridge constants</li>
<li class="calibre19"><kbd class="calibre22">model</kbd>: Should the model frame be returned?</li>
<li class="calibre19"><kbd class="calibre22">x</kbd>: Should the design matrix be returned?</li>
<li class="calibre19"><kbd class="calibre22">y</kbd>: Should the response be returned?</li>
<li class="calibre19"><kbd class="calibre22">contrasts</kbd>: A list of contrasts to be used for some or all of the factor terms in the formula</li>
</ul>
<div class="packt_infobox">The term <kbd class="calibre22">lambda</kbd> (here, a parameter in the <kbd class="calibre22">lm.ridge</kbd> function) is typically defined as a comparison of a group means on a combination of dependent variables.</div>
<p class="calibre4">To set up our next example, let's recall that our project data had a ratio of variables to observations of 12%. Suppose we've been provided with a new data file, one which has only 50 observations. Now our ratio of variables to observations goes up to 24%.</p>
<p class="calibre4">What about a file with only 12 observations? Further, suppose we are told that management believes that these 12 observations are based upon the key, high-visibility projects and therefore are unwilling to provide a bigger population to the data scientist (at least at this time)? Is it even worthwhile to model this data? Would the results be valuable in any way?</p>
<p class="calibre4">In general terms, it is said that the more the variables present in a regression model, the more flexible a model is considered to be, or that it will become. It is very likely that a model of this type will be able to achieve a low error by fitting random fluctuations in the training data but the outcome or results won't represent the true, underlying distribution of the variables within the data and in other words, performance will, therefore, be poor when the model is run on future data drawn from the same distribution. (Management would not be happy if our predictions for project profitability were based upon flawed logic!)</p>
<p class="calibre4">Given the preceding scenario, how should a data scientist proceed? Well, it is certainly possible to fit good models when there are more variables than data points, but it must be done very carefully.</p>
<p class="calibre4">As a rule, when the data contains more variables than observations, the results may seemingly yield acceptable performance, but as we already mentioned, the solution may achieve favorable results or even zero error on the training data. Such a model would certainly overfit on actual data because it's too flexible for the amount of training data. (This condition is called <strong class="calibre7">ill-posed</strong> or <strong class="calibre7">underdetermined</strong>.)</p>
<p class="calibre4">This problem is most often addressed by carefully setting limitations or imposing constraints on the parameters, either explicitly or via a logical process. The model then becomes a trade-off between fitting the data well and satisfying these set limits or constraints. Ridge regression constraints or penalizes data parameters and can yield better predictive performance by limiting the model's flexibility, thereby reducing the tendency to overfit.</p>
<p class="calibre4">However, simply setting limits or imposing constraints doesn't imply that the resulting solution will be good or acceptable. Constraints will only produce good solutions when they're actually suited to the problem or objective at hand.</p>
<p class="calibre4">Let's get back to the <kbd class="calibre22">lm.ridge</kbd> function we mentioned earlier in the section. A little different from the use of the <kbd class="calibre22">lm</kbd> function, we can see the difference in the following use case examples.</p>
<p class="calibre4">Typical to most examples you'll find, we can utilize the <kbd class="calibre22">runif</kbd> and <kbd class="calibre22">rnom</kbd> R functions to generate some random number datasets (to be used for illustration), we can see the difference between executing <kbd class="calibre22">lm</kbd> and <kbd class="calibre22">lm.ridge</kbd>:</p>
<pre class="calibre29"># -- create a uniform random number series as X1, X2 and X3<br class="calibre2"/># --- using runif<br class="calibre2"/>x1 &lt;- runif(n=20)<br class="calibre2"/>x2 &lt;- runif(n=20)<br class="calibre2"/>x3 &lt;- runif(n=20)<br class="calibre2"/># --- Create a new variable from x1 and x2<br class="calibre2"/>x3c &lt;- 10*x1 + x3<br class="calibre2"/># --- create a random number<br class="calibre2"/>ep &lt;- rnorm(n=20)<br class="calibre2"/>y &lt;- x1 + x2 + ep</pre>
<p class="calibre4">As we know what we want to explore (for example, estimating the parameters in a linear regression model), we can take liberties with creating the testing data. The following is an example of R code that generates a linear regression model using our three made-up variables:</p>
<pre class="calibre29"># --- using the R lm function to create an ordinary least squares (OLS) # -- fit of 3-variable model using x3 as an independent x3 variable<br class="calibre2"/>ols &lt;- lm(y~ x1 + x2 + x3)<br class="calibre2"/>summary(ols)</pre>
<p class="calibre4">The following is the generated output from the preceding code:</p>
<div class="packt_figure"><img class="image-border36" src="Images/14a32907-43d6-400a-b36a-e6a0ef9f84dd.png"/></div>
<p class="calibre4">Now, let's move on.</p>
<p class="calibre4">Using our same made-up example data and similar thinking, we can use the R function <kbd class="calibre22">lm.ridge</kbd> to attempt to fit our linear model using ridge regression:</p>
<pre class="calibre29"># --- Fit model using ridge regression using independent variables<br class="calibre2"/>ridge &lt;- lm.ridge (y ~ x1 + x2 + x3, lambda = seq(0, .1, .001))<br class="calibre2"/>summary(ridge)</pre>
<p class="calibre4">The following is the output generated (note the difference in output generated by the <kbd class="calibre22">summary</kbd> function):</p>
<div class="packt_figure"><img class="image-border37" src="Images/b40c4f50-dc12-4ffe-acf7-87ea1a6da8f5.png"/></div>
<p class="calibre4">You'll find that the <kbd class="calibre22">summary</kbd> function does not yield the same output on a linear regression model as it does on a model using the ridge regression method. However, there are a variety of packages available to produce sufficient output on ridge regression models.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Improving data or a data model</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">There are various parameters which are used for improving data or data model. In this section, we will be studying about a few of them.</p>
<div class="packt_figure"><img class="image-border38" src="Images/161afca5-347a-4159-8d3b-8154f2c3cf63.png"/></div>
<p class="calibre4">There are much other acceptable or at least well-known methods or approaches that a data scientist may employ in an attempt to improve on a statistical model (other than regularization) and it's worth spending some time mentioning a few of the most popular:</p>
<ul class="calibre18">
<li class="calibre19">Simplification</li>
<li class="calibre19">Relevance</li>
<li class="calibre19">Speed </li>
<li class="calibre19">Transformation</li>
<li class="calibre19">Variation of coefficients</li>
<li class="calibre19">Casual inference</li>
<li class="calibre19">Back to regularization</li>
<li class="calibre19">Reliability</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Simplification</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The first may be just plain common sense. A simple model is just plain easier to interpret and understand. Algorithms run more efficiently on a simpler model, allowing the data scientist the luxury of higher iterations as well as more time to evaluate the outcomes.</p>
<p class="calibre4">Keep in mind, though, that a more complicated model is somewhat more believable, so beware of over-simplification. The approach to finding the right mix between complex and simple can be worked both ways; by starting simple and adding complexities or, more commonly, by starting complex and removing things out of the model, testing, and evaluating and then repeating, until successfully understanding the (fitting) process.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Relevance</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">This one also seems obvious as well. In other words, don't waste time on statistical noise. Using common statistical regression packages, you will have visuals (such as quantile-quantile plots, influence diagrams, box plots, and so on) to pour over and understand. Spending time on removing irrelevancies from a model or data will pay dividends. The trick is to be able to identify what is relevant.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Speed</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The faster a data scientist can fit models, the more models (and data) can be evaluated and understood (the ultimate goal!). The ways and means of model optimization can be costly--either in time or expertise--and can focus on the model or the data, or both.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Transformation</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">This is something that can have a substantial effect on a model, but is not without risk. Transformation of variables can create models that could make sense (and can then be fit and compared to data) and that includes all relevant information, but if done irrationally, may introduce bias and imply incorrect outcomes.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Variation of coefficients</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Testing coefficients to determine whether a coefficient should vary by group, and the estimated scale of variation, is a feasible approach to model improvement. Very small varying coefficients (across categories) have the propensity to be dropped out of consideration.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Casual inference</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">You may be tempted to set up a single large regression to answer several causal questions that exist in a model or data; however, in observational settings (including experiments in which certain conditions of interest are observational), this approach risks bias. The bottom line here is, don't assume anything about any perceived relationships (or coefficients), especially don't assume that a coefficient can be interpreted causally. However, a casual inference can be effective (where appropriate) as a method used to improve a statistical model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Back to regularization</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Getting to the point--the theme of regularization is an attempt to improve an outcome or performance of a statistical model or method. In other words, to improve the process of learning (from data of course) through direct and indirect observation.</p>
<div class="packt_infobox">The process of attempting to gain knowledge or learn from a finite dataset (also known as <strong class="calibre3">empirical learning</strong>) is said to be an <strong class="calibre3">underdetermined problem</strong>, because in general, it is an attempt to infer a function <kbd class="calibre22"><span class="calibre5">x {\displaystyle x}</span></kbd>, given only some examples of data observations.</div>
<p class="calibre4">Another possible method of improving a statistical model is to use <strong class="calibre7">additive smoothing</strong> (also known as <strong class="calibre7">Laplacian smoothing</strong>) during the training of a model. This is a form of regularization and it works by adding a fixed number to all the counts of feature and class combinations during model training.</p>
<div class="packt_infobox">It is a popular opinion that additive smoothing is more effective than other probability smoothing methods in several retrieval tasks such as language model-based applications.</div>
<p class="calibre4">Regularization fundamentally works to introduce additional information, or an intentional bias, or constraint in a training procedure—preventing coefficients from taking large values—in order to solve an ill-posed problem. This is a method that attempts to shrink coefficients, also known as a <strong class="calibre7">shrinkage method</strong>. The information introduced tends to be in the form of a penalty for complexity, such as restrictions for smoothness or bounds on the vector space norm. In other words, regularization A does what it implies, it regulates how or how much you can change a parameter within a statistical model or its data. Yes, that is right, you can change the actual data!</p>
<p class="calibre4">When is it justifiable to change the values of your data?</p>
<p class="calibre4">The statistical community respects that the theoretical justification for regularization might be that it attempts to impose the belief that among competing hypotheses, the one with the fewest assumptions will be the most effective (and therefore should be the one selected and used). This belief is rigorously known as <strong class="calibre7">Occam's razor</strong> (or the law of parsimony).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Reliability</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">Should one always (Of course, we are referring to those situations that are identified as we discussed in this chapter's section, <em class="calibre21">Opportunities for regulation</em>.) attempt to institute a regulation method on a statistical model? Will it always improve a model or data population?</p>
<p class="calibre4">Before considering an answer to this question, remember that regularization does not improve the performance on the dataset that the algorithm initially used to learn the model parameters (feature weights). However, it can improve the generalization performance (the performance on new, unseen data, which is what you are looking for).</p>
<p class="calibre4">Think of using regularization in a statistical model as the adding of bias as a countermeasure to overfitting; on the other hand, though, adding too much bias almost always results in underfitting and the model will perform badly.</p>
<p class="calibre4">Answer: Regularization doesn't always work and may cause a model to perform poorly (perhaps even worse than before!). S. Raschka, Author of Python Machine Learning, makes an interesting comment:</p>
<div class="packt_quote">In intuitive terms, you can think of regularization as a penalty against the complexity (of a model). Increasing the regularization strength penalizes large weight coefficients. Therefore, your goal is to prevent your model from picking up peculiarities or noise and to generalize well to new, unseen data.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Using R for statistical regularization</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">There are a number of different functions and packages that implement ridge regression, such as <kbd class="calibre22">lm.ridge()</kbd> from the <kbd class="calibre22">MASS</kbd> package and <kbd class="calibre22">ridge()</kbd> from the <kbd class="calibre22">genridge</kbd> package. For the lasso, there is also the <kbd class="calibre22">lars</kbd> package. Here, in this chapter, we are going to use R's <kbd class="calibre22">glmnet()</kbd> function (from the <kbd class="calibre22">glmnet</kbd> package) due to it being well-documented and having a consistent and friendly interface.</p>
<p class="calibre4">The key to working with regularization is to determine an appropriate <kbd class="calibre22">lambda</kbd> value to use. The approach that the <kbd class="calibre22">glmnet()</kbd> function uses is to use a grid of different <kbd class="calibre22">lambda</kbd> values, training a regression model for each value. Then, one can either pick a value manually or use a technique to estimate the best <kbd class="calibre22">lambda</kbd>.</p>
<div class="packt_tip">You can specify the sequence of the values to try (via the <kbd class="calibre22">lambda</kbd> parameter); otherwise, a default sequence with 100 values will be used.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Parameter Setup</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">The first parameter to the <kbd class="calibre22">glmnet()</kbd> function must be a matrix of features (which we can create using the R function, <kbd class="calibre22">model.matrix()</kbd>). The second parameter is a vector with the output variable. Finally, the <kbd class="calibre22">alpha</kbd> parameter is a switch between ridge regression (0) and lasso (1). The following code sets up for our example:</p>
<pre class="calibre29"># --- load the package 
library(glmnet) 
# --- create our parameter data 
cars_train_mat &lt;- model.matrix(Price ~ .-Saturn, cars_train)[,-1] 
lambdas &lt;- 10 ^ seq(8, -4, length = 250) 
  </pre>
<div class="packt_infobox">The <kbd class="calibre22">model.matrix</kbd> R function creates a matrix by expanding factors to a set of summary variables (depending on the contrasts) and expanding interactions similarly.</div>
<pre class="calibre29"> 
 # --- create regression model 
cars_models_ridge &lt;-  
  glmnet(cars_train_mat, cars_train$Price, alpha = 0, lambda = lambdas) 
 
# --- create a lasso model 
cars_models_lasso &lt;-  
  glmnet(cars_train_mat, cars_train$Price, alpha = 1, lambda = lambdas) </pre>
<p class="calibre4">The preceding code that we used to set up the data to be used in this example (specifically, <kbd class="calibre22">length = 250</kbd>) provided a sequence of 250 values. This means that (in the preceding code) actually trained 250 ridge regression models and another 250 lasso models!</p>
<p class="calibre4">We can review the value of the <kbd class="calibre22">lambda</kbd> attribute (of the <kbd class="calibre22">cars_models_ridge</kbd> object) that is produced by <kbd class="calibre22">glmnet()</kbd> and then apply the <kbd class="calibre22">coef()</kbd> function to this object to retrieve the corresponding coefficients for the 100<sup class="calibre31">th</sup> model, as follows:</p>
<pre class="calibre29"># --- print the value of the lambda object of the 100th model 
# --- generated by glmnet 
cars_models_ridge$lambda[100] 
[1] 1694.009 
 
# --- use coef to see 100th model's coefficient values 
coef(cars_models_ridge)[,100] 
  (Intercept)       Mileage      Cylinder         Doors  
 6217.5498831    -0.1574441  2757.9937160   371.2268405  
       Cruise         Sound       Leather         Buick  
 1694.6023651   100.2323812  1326.7744321  -358.8397493  
     Cadillac         Chevy       Pontiac          Saab  
11160.4861489 -2370.3268837 -2256.7482905  8416.9209564  
  convertible     hatchback         sedan  
10576.9050477 -3263.4869674 -2058.0627013 </pre>
<p class="calibre4">Finally, we can use the R <kbd class="calibre22">plot()</kbd> function to obtain a plot showing how the values of the coefficients change as the logarithm values change.</p>
<p class="calibre4">As shown in the following code, it is very helpful to show the corresponding plot for ridge regression and lasso side by side:</p>
<pre class="calibre29"># --- visualize our model data 
# --- set matrix column-widths and the row-heights 
layout(matrix(c(1, 2), 1, 2)) 
 
# --- create ridge regression plot 
plot(cars_models_ridge, xvar = "lambda", main = "Ridge  
   Regression\n")</pre>
<p class="calibre4">The following is the plot graphic generated by the preceding R code:</p>
<div class="packt_figure"><img class="image-border39" src="Images/966624e3-2d01-46cd-a46d-bf473ac56cc6.png"/></div>
<p class="calibre4">This is the R code to generate the <kbd class="calibre22">lasso</kbd> plot:</p>
<pre class="calibre29"># --- create lasso plot 
plot(cars_models_lasso, xvar = "lambda", main = "Lasso\n") </pre>
<p class="calibre4">This is the corresponding output:</p>
<div class="packt_figure"><img class="image-border40" src="Images/ce26b845-7e47-48f0-839c-b26deda890ed.png"/></div>
<p class="calibre4">The significant difference between the preceding two graphs is that <kbd class="calibre22">lasso</kbd> forces many coefficients to fall to zero exactly, whereas, in ridge regression, they tend to drop off smoothly and only become zero altogether at extreme values. Note the values on the top horizontal axis of both of the graphs, which show the number of non-zero coefficients as values vary.</p>
<div class="packt_infobox">Along with applying regularization to minimize the issue of overfitting, the <kbd class="calibre22">lasso</kbd> function is often used to perform feature selection as a feature with a zero coefficient would not be included in the model.</div>
<p class="calibre4">As a part of the <kbd class="calibre22">glmnet</kbd> package, the <kbd class="calibre22">predict()</kbd> function operates in a variety of contexts. We can, for example, determine the <strong class="calibre7">coefficient variance</strong> (<strong class="calibre7">CV</strong>) percentages (the strength and direction of a linear relationship between two variables) of a model for a <kbd class="calibre22">lambda</kbd> value that was not in our original list.</p>
<div class="packt_infobox">Predict is a generic function for predictions from the results of various model fitting functions.</div>
<p class="calibre4">Let's try using <kbd class="calibre22">predict</kbd> on our lasso model (created earlier).</p>
<p class="calibre4">We can write the following R code on our previously created lasso model, <kbd class="calibre22">cars_models_lasso</kbd>:</p>
<pre class="calibre29"># --- use predict function on the lasso model 
predict(cars_models_lasso, type = "coefficients", s = lambda_lasso) 
 
   
Below is the generated output, a list of the coefficient values: 
 
 (Intercept)  -521.3516739 
Mileage        -0.1861493 
Cylinder     3619.3006985 
Doors        1400.7484461 
Cruise        310.9153455 
Sound         340.7585158 
Leather       830.7770461 
Buick        1139.9522370 
Cadillac    13377.3244020 
Chevy        -501.7213442 
Pontiac     -1327.8094954 
Saab        12306.0915679 
convertible 11160.6987522 
hatchback   -6072.0031626 
sedan       -4179.9112364 </pre>
<p class="calibre4">From the preceding output, you can see that <kbd class="calibre22">lasso</kbd> has not forced any coefficients to zero, in this case, suggesting that none should be removed (and therefore remain as features in the model) from the data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article class="calibre2">
                
<p class="calibre4">In this chapter, we provided an explanation of statistical regularization and then used sample data in an example to illustrate and better understand statistical regularization. Later, we had a discussion of various methods on how to improve (the performance of) data or a data model with regulation. Finally, we saw how well the R language supports the concepts and methods of regulation.</p>
<p class="calibre4">In the next chapter, we're looking to cover the idea of data model assessment and using statistics for assessment. We'll compare the concepts of data assessment and data quality assurance, and finally, apply the idea of statistical assessment to data using R.</p>
<p class="calibre4"/>
<p class="calibre4"/>


            </article>

            
        </section>
    </div>



  </body></html>