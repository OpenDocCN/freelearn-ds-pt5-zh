<html><head></head><body><div><div><p class="hidden">2</p>
		</div>
		<div><h1 id="_idParaDest-36"><a id="_idTextAnchor036"/>Data Cleaning and Advanced Machine Learning</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Plan a machine learning classification strategy</li>
				<li class="bullets">Preprocess data to prepare it for machine learning</li>
				<li class="bullets">Train classification models</li>
				<li class="bullets">Use validation curves to tune model parameters</li>
				<li class="bullets">Use dimensionality reduction to enhance model performance</li>
			</ul>
			<p>In this chapter, you will learn data preprocessing and machine learning by completing several practical exercises.</p>
		</div>
		<div><h2 id="_idParaDest-37"><a id="_idTextAnchor037"/>Introduction</h2>
			<p>Consider a small food-delivery business that is looking to optimize their product. An analyst might look at the appropriate data and determine what type of food people are enjoying most. Perhaps they find a large amount of people are ordering the spiciest food options, indicating the business might be losing out on customers who desire something even more spicy. This is quite basic, or as some might say, "vanilla" analytics.</p>
			<p>In a separate task, the analyst could employ predictive analytics by modeling the order volumes over time. With enough data, they could predict the future order volumes and therefore guide the restaurant as to how many staff are required each day. This model could take factors such as the weather into account to make the best predictions. For instance, a heavy rainstorm could be an indicator to staff more delivery personnel to make up for slow travel times. With historical weather data, that type of signal could be encoded into the model. This prediction model would save a business the time of having to consider these problems manually, and money by keeping customers happy and thereby increasing customer retention.</p>
			<p>The goal of data analytics in general is to uncover actionable insights that result in positive business outcomes. In the case of predictive analytics, the aim is to do this by determining the most likely future outcome of a target, based on previous trends and patterns.</p>
			<p>The benefits of predictive analytics are not restricted to big technology companies. Any business can find ways to benefit from machine learning, given the right data.</p>
			<p>Companies all around the world are collecting massive amounts of data and using predictive analytics to cut costs and increase profits. Some of the most prevalent examples of this are from the technology giants Google, Facebook, and Amazon, who utilize big data on a huge scale. For example, Google and Facebook serve you personalized ads based on predictive algorithms that guess what you are most likely to click on. Similarly, Amazon recommends personalized products that you are most likely to buy, given your previous purchases.</p>
			<p>Modern predictive analytics is done with machine learning, where computer models are trained to learn patterns from data. As we saw briefly in the previous chapter, software such as scikit-learn can be used with Jupyter Notebooks to efficiently build and test machine learning models. As we will continue to see, Jupyter Notebooks are an ideal environment for doing this type of work, as we can perform ad-hoc testing and analysis, and easily save the results for reference later.</p>
			<p>In this chapter, we will again take a hands-on approach by running through various examples and activities in a Jupyter Notebook. Where we saw a couple of examples of machine learning in the previous chapter, here we'll take a much slower and more thoughtful approach. Using an employee retention problem as our overarching example for the chapter, we will discuss how to approach predictive analytics, what things to consider when preparing the data for modeling, and how to implement and compare a variety of models using Jupyter Notebooks.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor038"/>Preparing to Train a Predictive Model</h2>
			<p>Here, we will cover the preparation required to train a predictive model. Although not as technically glamorous as training the models themselves, this step should not be taken lightly. It's very important to ensure you have a good plan before proceeding with the details of building and training a reliable model. Furthermore, once you've decided on the right plan, there are technical steps in preparing the data for modeling that should not be overlooked.</p>
			<h4>Note</h4>
			<p class="callout">We must be careful not to go so deep into the weeds of technical tasks that we lose sight of the goal. Technical tasks include things that require programming skills, for example, constructing visualizations, querying databases, and validating predictive models. It's easy to spend hours trying to implement a specific feature or get the plots looking just right. Doing this sort of thing is certainly beneficial to our programming skills, but we should not forget to ask ourselves if it's really worth our time with respect to the current project.</p>
			<p>Also, keep in mind that Jupyter Notebooks are particularly well-suited for this step, as we can use them to document our plan, for example, by writing rough notes about the data or a list of models we are interested in training. Before starting to train models, its good practice to even take this a step further and write out a well- structured plan to follow. Not only will this help you stay on track as you build and test the models, but it will allow others to understand what you're doing when they see your work.</p>
			<p>After discussing the preparation, we will also cover another step in preparing to train the predictive model, which is cleaning the dataset. This is another thing that Jupyter Notebooks are well-suited for, as they offer an ideal testing ground for performing dataset transformations and keeping track of the exact changes. The data transformations required for cleaning raw data can quickly become intricate and convoluted; therefore, it's important to keep track of your work. As discussed in the first chapter, tools other than Jupyter Notebooks just don't offer very good options for doing this efficiently.</p>
			<p>Before we progress to the next section, let's pause and think about these ideas in the context of a real-life example. </p>
			<p>Consider the following situation:</p>
			<p>You are hired by an online video game marketplace who want to increase the conversion rate of people visiting their site. They ask you to use predictive analytics to determine what genre of game the user will like, so they can display specialized content that will encourage the user to make a purchase. They want to do this without having to ask the customer their preference of game genre.</p>
			<p>Is this a problem that can be solved? What type of data would be required? What would be the business implications?</p>
			<p>To address this challenge, we could consider making the prediction based on users' browsing cookies. For example, if they have a cookie from previously visiting a World of Warcraft website, this would act as an indicator that they like role playing games.</p>
			<p>Another valuable piece of data would be a history of the games that user has previously bought in the marketplace. This could be the target variable in a machine learning algorithm, for example, a model that could predict which games the user would be interested in, based on the type of cookies in their browsing session. An alternate target variable could be constructed by setting up a survey in the marketplace to collect data on user preferences.</p>
			<p>In terms of the business implications, being able to accurately predict the genre of game is very important to the success of the campaign. In fact, getting the prediction wrong is doubly problematic: not only do we miss out on the opportunity to target users, but we may end up showing users content that would be negatively perceived. This could lead to more people leaving the site and fewer sales.</p>
			<h3 id="_idParaDest-39"><a id="_idTextAnchor039"/>Determining a Plan for Predictive Analytics</h3>
			<p>When formulating a plan for doing predictive modeling, one should start by considering stakeholder needs. A perfect model will be useless if it doesn't solve a relevant problem. Planning a strategy around business needs ensures that a successful model will lead to actionable insights.</p>
			<p>Although it may be possible in principle to solve many business problems, the ability to deliver the solution will always depend on the availability of the necessary data. Therefore, it's important to consider the business needs in the context of the available data sources. When data is plentiful, this will have little effect, but as the amount of available data becomes smaller, so too does the scope of problems that can be solved.</p>
			<p>These ideas can be formed into a standard process for determining a predictive analytics plan, which goes as follows:</p>
			<ol>
				<li><strong class="bold">Look at the available data</strong> to understand the range of realistically solvable business problems. At this stage, it might be too early to think about the exact problems that can be solved. Make sure you understand the data fields available and the timeframes they apply to.</li>
				<li><strong class="bold">Determine the business needs</strong> by speaking with key stakeholders. Seek out a problem where the solution will lead to actionable business decisions.</li>
				<li><strong class="bold">Assess the data for suitability</strong> by considering the availability of a sufficiently diverse and large feature space. Also, take into account the condition of the data: are there large chunks of missing values for certain variables or time ranges?</li>
			</ol>
			<p>Steps 2 and 3 should be repeated until a realistic plan has taken shape. At this point, you will already have a good idea of what the model input will be and what you might expect as output.</p>
			<p>Once <a id="_idTextAnchor040"/>you've identified a problem that can be solved with machine learning, along with the appropriate data sources, we should answer the following questions to lay a framework for the project. Doing this will help us determine which types of machine learning models we can use to solve the problem. The following image provides an overview of the choices available depending on the type of data: </p>
			<div><div><img src="img/C13018_02_01.jpg" alt="Figure 2.1: A flow chart for machine learning strategy based on the type of data" width="1800" height="995"/>
				</div>
			</div>
			<h6>Figure 2.1: A flow chart for machine learning strategy based on the type of data</h6>
			<p>The above image describes the path you can choose depending of the type of data: labeled or unlabeled. </p>
			<p>As can be seen, either one can chose supervised or unsupervised learning. Supervised learning comprises either classification or regression problem. In regression, variables are continuous; for example, the amount of rainfall. In regression, the variables are discrete and we predict class labels. Simplest type of classification problem is binary; for example, will it rain today? (yes/no)</p>
			<p>For unsupervised learning, cluster analysis is a commonly used method. Here, labels are assigned to the nearest cluster for each sample.</p>
			<p>However, not only the type but also the size and origin of data sources would be a factor while deciding on machine learning strategy. Specifically, following points should be note:</p>
			<ul>
				<li>The size of data in terms of the width (no. of columns) and height (no. of rows) should be considered before apply a machine learning algorithm.</li>
				<li>Certain algorithms are better at handling certain features than the others.</li>
				<li>General, the larger the dataset, the better in terms of accuracy. However, this can be time consuming</li>
				<li>One can reduce time by using dimensionality reduction techniques.</li>
				<li>For multiple data sources, one can consider merging them in a single table.</li>
				<li>If this cannot be done, we can train models for each and consider an ensemble average for final prediction.</li>
			</ul>
			<p>An example where we may want to do this is with various sets of times series data on different scales. Consider we have the following data sources: a table with the AAPL stock closing prices on a daily time scale and iPhone sales data on a monthly time scale. We could merge the data by adding the monthly sales data to each sample in the daily time scale table, or grouping the daily data by month, but it might be better to build two models, one for each dataset, and use a combination of the results from each in the final prediction model.</p>
			<p>Data preprocessing has a huge impact on machine learning. Like the saying "you are what you eat," the model's performance is a direct reflection of the data it's trained on. Many models depend on the data being transformed so that the continuous feature values have comparable limits. Similarly, categorical features should be encoded into numerical values. Although important, these steps are relatively simple and do not take very long.</p>
			<p>The aspect of preprocessing that usually takes the longest is cleaning up messy data. Some estimates suggest that data scientists spend around two thirds of their work time cleaning and organizing datasets:</p>
			<div><div><img src="img/C13018_02_02.jpg" alt="Figure 2.2: A pie chart distribution of the time spend on different data tasks" width="1800" height="770"/>
				</div>
			</div>
			<h6>Figure 2.2: A pie chart distribution of the time spend on different data tasks</h6>
			<p>To know more about the preprocessing stage, refer to: <a href="http://www.forbes.com/sites/gilpress/2016/03/23/data-">https://www.forbes.com/sites/gilpress/2016/03/23/data-</a> preparation-most-time-consuming-least-enjoyable-data- science-task-survey-says/2/#17c66c7e1492.</p>
			<p>Another thing to consider is the size of the datasets being used by many data scientists. As the dataset size increases, the prevalence of messy data increases as well, along with the difficulty in cleaning it.</p>
			<p>Simply dropping the missing data is usually not the best option, because it's hard to justify throwing away samples where most of the fields have values. In doing so, we could lose valuable information that may hurt final model performance.</p>
			<h4>Note</h4>
			<p class="callout">In this exercise, we practice preprocessing the data by creating two DataFrames, and performing an inner merge and outer merge on the DataFrames and remove the null (<code>NaN</code>) values.</p>
			<p>The steps involved in data preprocessing can be grouped as follows:</p>
			<ul>
				<li><strong class="bold">Merging data</strong> sets on common fields to bring all data into a single table</li>
				<li><strong class="bold">Feature engineering</strong> to improve the quality of data, for example, the use of dimensionality reduction techniques to build new features</li>
				<li><strong class="bold">Cleaning the data</strong> by dealing with duplicate rows, incorrect or missing values, and other issues that arise</li>
				<li><strong class="bold">Building the training data sets</strong> by standardizing or normalizing the required data and splitting it into training and testing sets</li>
			</ul>
			<p>Let's explore some of the tools and methods for doing the preprocessing.</p>
			<h3 id="_idParaDest-40"><a id="_idTextAnchor041"/>Exercise 8: Explore Data Preprocessing Tools and Methods</h3>
			<ol>
				<li value="1">Start the <code>NotebookApp</code> from the project directory by executing <code>jupyter notebook</code>. Navigate to the <code>Lesson-2</code> directory and open up the <code>lesson- 2-workbook.ipynb</code> file. Find the cell near the top where the packages are loaded, and run it.<p>We are going to start by showing off some basic tools from Pandas and sci-kit learn. Then, we'll take a deeper dive into methods for rebuilding missing data.</p></li>
				<li>Scroll down to <code>Subtopic B: Preparing data for machine learning</code> and run the cell containing <code>pd.merge?</code> to display the docstring for the merge function in the notebook: <div><img src="img/C13018_02_03.jpg" alt="Figure 2.3: Docstring for the merge function" width="1647" height="1112"/></div><h6>Figure 2.3: Docstring for the merge function</h6><p>As we can see, the function accepts a left and right DataFrame to merge. You can specify one or more columns to group on as well as how they are grouped, that is, to use the left, right, outer, or inner sets of values. Let's see an example of this in use.</p></li>
				<li>Exit the help popup and run the cell containing the following sample DataFrames:<pre>df_1 = pd.DataFrame({'product': ['red shirt', 'red shirt', 'red shirt', 'white dress'],\n",
'price': [49.33, 49.33, 32.49,
199.99]})\n",
df_2 = pd.DataFrame({'product': ['red shirt', 'blue pants', 'white tuxedo', 'white dress'],\n",
'in_stock': [True, True, False,
False]})</pre><p>Here, we will build two simple DataFrames from scratch. As can be seen, they contain a <code>product</code> column with some shared entries.</p></li>
				<li>Run the next cell to perform the inner merge:<div><img src="img/C13018_02_04.jpg" alt="Figure 2.4: Inner merge of columns" width="1220" height="496"/></div><h6>Figure 2.4: Inner merge of columns</h6><p>Note how only the shared items, <strong class="bold">red shirt</strong> and <strong class="bold">white dress</strong>, are included. To include all entries from both tables, we can do an outer merge instead. Let's do this now.</p></li>
				<li>Run the next cell to perform an outer merge:<div><img src="img/C13018_02_05.jpg" alt="Figure 2.5: Outer merge of columns" width="1220" height="626"/></div><h6>Figure 2.5: Outer merge of columns</h6><p>This returns all of the data from each table where missing values have been labeled with <code>NaN</code>.</p><div><img src="img/C13018_02_06.jpg" alt="Figure 2.6: Code for using NumPy to test for quality" width="1605" height="752"/></div><h6>Figure 2.6: Code for using NumPy to test for quality</h6><p>You may have noticed that our most recently merged table has duplicated data in the first few rows. This will be addressed in the next step.</p></li>
				<li>Run the cell containing <code>df.drop_duplicates()</code> to return a version of the DataFrame with no duplicate rows:<div><img src="img/C13018_02_07.jpg" alt="Figure 2.7: Table with dropped duplicate rows" width="1800" height="698"/></div><h6>Figure 2.7: Table with dropped duplicate rows</h6><p>This is the easiest and "standard" way to drop duplicate rows. To apply these changes to <code>df</code>, we can either set <code>inplace=True</code> or do something like <code>df = df.drop_duplicated()</code>. Let's see another method, which uses masking to select or drop duplicate rows.</p></li>
				<li>Run the cell containing <code>df.duplicated()</code> to print the True/False series, marking duplicate rows:<div><img src="img/C13018_02_08.jpg" alt="Figure 2.8: Printing True/False values for duplicate rows" width="1241" height="320"/></div><h6>Figure 2.8: Printing True/False values for duplicate rows</h6></li>
				<li>Sum the result to determine how many rows have been duplicated by running the following code: <div><img src="img/C13018_02_09.jpg" alt="Figure 2.9: Summing the result to check the number of duplicate rows" width="1281" height="372"/></div><h6>Figure 2.9: Summing the result to check the number of duplicate rows</h6></li>
				<li>Run the following code and convince yourself the output is the same as that from <code>df.drop_duplicates()</code>:<pre>df[~df.duplicated()]</pre><div><img src="img/C13018_02_10.jpg" alt="Figure 2.10: Output from the df.[~df.duplicated()] function" width="1228" height="456"/></div><h6>Figure 2.10: Output from the df.[~df.duplicated()] function</h6></li>
				<li>Run the cell containing the following code to drop duplicates from a subset of the full DataFrame:<pre>df[~df['product'].duplicated()]</pre><div><img src="img/C13018_02_11.jpg" alt="Figure 2.11: Output after dropping duplicates" width="1252" height="404"/></div><h6>Figure 2.11: Output after dropping duplicates</h6><p>Here, we are doing the following things: </p><p>creating a mask (a True/False series) for the product row, where duplicates are marked with <code>True</code>; </p><p>using the tilde (~) to take the opposite of that mask, so that duplicates are instead marked with False and everything else is <code>True</code>; </p><p>using that mask to filter out the <code>False</code> rows of <code>df</code>, which correspond to the duplicated products.</p><p>As expected, we now see that only the first <code>df</code> with a deduplicated version of itself. This can be done by running <code>drop_duplicates</code> and passing the parameter <code>inplace=True</code>.</p></li>
				<li>Deduplicate the DataFrame and save the result by running the cell containing the following code:<pre>df.drop_duplicates(inplace=True)</pre><p>Continuing on to other preprocessing methods, let's ignore the duplicated rows and first deal with the missing data. This is necessary because models cannot be trained on incomplete samples. Using the missing price data for blue pants and white tuxedo as an example, let's show some different options for handling <code>NaN</code> values.</p></li>
				<li>Drop rows, especially if your <code>NaN</code> samples are missing data, by running the cell containing <code>df.dropna()</code>:<div><img src="img/C13018_02_12.jpg" alt="Figure 2.12: Output after dropping incomplete rows" width="1138" height="404"/></div><h6>Figure 2.12: Output after dropping incomplete rows </h6></li>
				<li>Drop entire columns that have most values missing for a feature. Do this by running the cell containing the same method as before, but this time with the axes parameter passed to indicate columns instead of rows:<div><img src="img/C13018_02_13.jpg" alt="Figure 2.13: Output after dropping entire columns with missing values for a feature" width="1163" height="526"/></div><h6>Figure 2.13: Output after dropping entire columns with missing values for a feature</h6><p>Simply dropping the <code>NaN</code> values is usually not the best option, because losing data is never good, especially if only a small fraction of the sample values is missing. Pandas offers a method for filling in <code>NaN</code> entries in a variety of different ways, some of which we'll illustrate now.</p></li>
				<li>Run the cell containing <code>df.fillna?</code> to print the docstring for the Pandas <code>NaN-fill</code> method:<div><img src="img/C13018_02_14.jpg" alt="Figure 2.14: Docstring for the NaN-fill method" width="1800" height="778"/></div><h6>Figure 2.14: Docstring for the NaN-fill method</h6><p>Note the options for the value parameter; this could be, for example, a single value or a dictionary/series type map based on index. Alternatively, we can leave the value as <code>None</code> and pass a <code>fill</code> method instead. We'll see examples of each in this chapter.</p></li>
				<li>Fill in the missing data with the average product price by running the cell containing the following code:<pre>df.fillna(value=df.price.mean())</pre><div><img src="img/C13018_02_15.jpg" alt="Figure 2.15: Output after filling missing data with average product price" width="1178" height="520"/></div><h6>Figure 2.15: Output after filling missing data with average product price</h6></li>
				<li>Fill in the missing data using the pad method by running the cell containing the following code instead:<pre>df.fillna(method='pad')</pre><div><img src="img/C13018_02_16.jpg" alt="Figure 2.16: Output after filling data using the pad method" width="1491" height="558"/></div><h6>Figure 2.16: Output after filling data using the pad method</h6><p>Notice how the <strong class="bold">white dress</strong> price was used to pad the missing values below it.</p><p>To conclude this exercise, we will prepare our simple table to be used for training a machine learning algorithm. Don't worry, we won't actually try to train any models on such a small dataset! We start this process by encoding the class labels for the categorical data.</p></li>
				<li>Run the first cell in the <code>Building training data sets</code> section to add another column of data representing the average product ratings before encoding the labels:<div><img src="img/C13018_02_17.jpg" alt="Figure 2.17: Output after adding the rating column" width="1461" height="590"/></div><h6>Figure 2.17: Output after adding the rating column</h6><p>Considering we want to use this table to train a predictive model, we should first think about changing all the variables to numeric types.</p></li>
				<li>Convert the handle <code>in_stock</code>., which is a Boolean list, to numeric values; for example, <code>0</code> and <code>1</code>. This should be done before using it to train a predictive model. This can be done in many ways, for example, by running the cell containing the following code:<pre>df.in_stock = df.in_stock.map({False: 0, True: 1})</pre><div><img src="img/C13018_02_18.jpg" alt="Figure 2.18: Output after converting in_stock to binary" width="1800" height="756"/></div><h6>Figure 2.18: Output after converting in_stock to binary</h6></li>
				<li>Run the cell containing the following code to map class labels to integers at a higher level. We use sci-kit learn's <code>LabelEncoder</code> for this purpose:<pre>from sklearn.preprocessing import LabelEncoder rating_encoder = LabelEncoder()
_df = df.copy()
_df.rating = rating_encoder.fit_transform(df.rating)
_df</pre><div><img src="img/C13018_02_19_.jpg" alt="Figure 2.19: Output after mapping class labels to integers" width="1297" height="650"/></div><h6>Figure 2.19: Output after mapping class labels to integers</h6><p>This might bring to mind the preprocessing we did in the previous chapter, when building the polynomial model. Here, we instantiate a label encoder and then "train" it and "transform" our data using the <code>fit_transform</code> method. We apply the result to a copy of our DataFrame, <code>_df</code>.</p></li>
				<li>Re-convert the features using the class we reference with the variable <code>rating_encoder</code>, by running <code>rating_encoder.inverse_ transform(df.rating)</code>:<div><img src="img/C13018_02_20.jpg" alt="Figure 2.20: Output after performing inverse transform" width="1356" height="196"/></div><h6>Figure 2.20: Output after performing inverse transform</h6><p>You may notice a problem here. We are working with a so-called "ordinal" feature, where there's an inherent order to the labels. In this case, we should expect that a rating of "low" would be encoded with a 0 and a rating of "high" would be encoded with a 2. However, this is not the result we see. In order to achieve proper ordinal label encoding, we should again use map, and build the dictionary ourselves.</p></li>
				<li>Encode the ordinal labels properly by running the cell containing the following code:<pre>ordinal_map = {rating: index for index, rating in enumerate(['low', 'medium', 'high'])}
print(ordinal_map)
df.rating = df.rating.map(ordinal_map)</pre><div><img src="img/C13018_02_21.jpg" alt="Figure 2.21: Output after encoding ordinal labels" width="1800" height="797"/></div><h6>Figure 2.21: Output after encoding ordinal labels</h6><p>We first create the mapping dictionary. This is done using a dictionary comprehension and enumeration, but looking at the result, we see that it could just as easily be defined manually instead. Then, as done earlier for the <code>in_stock</code> column, we apply the dictionary mapping to the feature. Looking at the result, we see that rating now makes more sense than before, where <code>low</code> is labeled with <code>0</code>, <code>medium</code> with <code>1</code>, and <code>high</code> with <code>2</code>.</p><p>Now that you've discussed ordinal features, let's touch on another type called nominal features. These are fields with no inherent order, and in our case, we see that <code>product</code> is a perfect example.</p><p>Most scikit-learn models can be trained on data like this, where we have strings instead of integer-encoded labels. In this situation, the necessary conversions are done under the hood. However, this may not be the case for all models in scikit-learn, or other machine learning and deep learning libraries. Therefore, it's good practice to encode these ourselves during preprocessing.</p></li>
				<li>Convert the class labels from strings to numerical values by running the cell containing the following code:<pre>df = pd.get_dummies(df)</pre><p>The final DataFrame then looks as follows:</p><div><img src="img/C13018_02_201.jpg" alt="Figure 2.22: Final DataFrame" width="1220" height="196"/></div><h6>Figure 2.22: Final DataFrame</h6><p>Here, we see the result of one-hot encoding: the <code>product</code> column has been split into 4, one for each unique value. Within each column, we find either a <code>1</code> or <code>0</code> representing whether that row contains the particular value or product.</p><p>Moving on and ignoring any data scaling (which should usually be done), the final step is to split the data into training and test sets to use for machine learning. This can be done using scikit-learn's <code>train_test_split</code>. Let's assume we are going to try to predict whether an item is in stock, given the other feature values.</p><h4>Note</h4><p class="callout">When we call the values attribute in the preceding code, we are converting the Pandas series (that is, the DataFrame column) into a NumPy array. This is good practice because it strips out unnecessary information from the series object, such as the index and name.</p></li>
				<li>Split the data into training and test sets by running the cell containing the following code:<pre>features = ['price', 'rating', 'product_blue pants', 'product_red shirt', 'product_white dress', 'product_white tuxedo']
X = df[features].values target = 'in_stock'
y = df[target].values
from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = \
train_test_split(X, y, test_size=0.3)</pre></li>
			</ol>
			<div><div><img src="img/C13018_02_23.jpg" alt="Figure 2.23: Splitting data intro training and test sets" width="1130" height="482"/>
				</div>
			</div>
			<h6>Figure 2.23: Splitting data intro training and test sets</h6>
			<p>Here, we are selecting subsets of the data and feeding them into the <code>train_test_split</code> function. This function has four outputs, which are unpacked into the training and testing splits for features (<code>X</code>) and the target (<code>y</code>).</p>
			<p>Observe the shape of the output data, where the test set has roughly 30% of the samples and the training set has roughly 70%. </p>
			<p>We'll see similar code blocks later, when preparing real data to use for training predictive models.</p>
			<p>This concludes the training exercise on cleaning data for use in machine learning applications. Let's take a minute to note how effective our Jupyter Notebook was for testing various methods of transforming the data, and ultimately documenting the pipeline we decided upon. This could easily be applied to an updated version of the data by altering only specific cells of code, prior to processing. Also, should we desire any changes to the processing, these can easily be tested in the notebook, and specific cells may be changed to accommodate the alterations. The best way to achieve this would probably be to copy the notebook over to a new file, so that we can always keep a copy of the original analysis for reference.</p>
			<p>Moving on to an activity, we'll now apply the concepts from this section to a large dataset as we prepare it for use in training predictive models.</p>
			<h3 id="_idParaDest-41"><a id="_idTextAnchor042"/>Activity 2: Preparing to Train a Predictive Model for the Employee-Retention Problem</h3>
			<p>Suppose you are hired to do freelance work for a company who wants to find insights into why their employees are leaving. They have compiled a set of data they think will be helpful in this respect. It includes details on employee satisfaction levels, evaluations, time spent at work, department, and salary.</p>
			<p>The company shares their data with you by sending you a file called hr_data.csv and asking what you think can be done to help stop employees from leaving.</p>
			<p>Our aim is to</p>
			<p>apply the concepts you've learned thus far to a real-life problem. In particular, we seek to:</p>
			<ol>
				<li value="1">Determine a plan for using predictive analytics to provide impactful business insights, given the available data.</li>
				<li>Prepare the data for use in machine learning models.<h4>Note</h4><p class="callout">Starting with this activity and continuing through the remainder of this chapter, we'll be using Human Resources Analytics dataset, which is a Kaggle dataset. The link to the dataset can be found here: <a href="https://bit.ly/2OXWFUs">https://bit.ly/2OXWFUs</a>. The data is simulated, meaning the samples are artificially generated and do not represent real people. We'll ignore this fact as we analyze and model the data. There is a small difference between the dataset we use in this book and the online version. Our human resource analytics data contains some NaN values. These were manually removed from the online version of the dataset, for the purposes of illustrating data cleaning techniques. We have also added a column of data called is_smoker, for the same purposes.</p></li>
			</ol>
			<p>In order to achieve this, following steps have to be executed:</p>
			<ol>
				<li value="1">Scroll to the <code>Activity A</code> section of the <code>lesson-2-workbook.ipynb</code> notebook file.</li>
				<li>Check the head of the table to verify that it is in standard CSV format.</li>
				<li>Load the data with Pandas.</li>
				<li>Inspect the columns by printing <code>df.columns</code> and make sure the data has loaded as expected by printing the DataFrame <code>head</code> and <code>tail</code> with <code>df.head()</code> and <code>df.tail()</code>:</li>
				<li>Check the number of rows (including the header) in the CSV file.</li>
				<li>Compare this result to <code>len(df)</code> to make sure we've loaded all the data:</li>
				<li>Assess the target variable and check the distribution and number of missing entries.</li>
				<li>Print the data type of each feature.</li>
				<li>Display the feature distributions.</li>
				<li>Check how many <code>NaN</code> values are in each column by running the following code:</li>
				<li>Drop the <code>is_smoker</code> column as there is barely any information in this metric. </li>
				<li>Fill the <code>NaN</code> values in the <code>time_spend_company</code> column.</li>
				<li>Make a boxplot of <code>average_montly_hours</code> segmented by <code>number_project</code>. </li>
				<li>Calculate the mean of each group by running the following code:</li>
				<li>Fill the <code>NaN</code> values in <code>average_montly_hours</code>.</li>
				<li>Confirm that <code>df</code> has no more <code>NaN</code> values by running the assertion test. </li>
				<li>Transform the string and Boolean fields into integer representations. </li>
				<li>Print <code>df.columns</code> to show the fields</li>
				<li>Save our preprocessed data. <h4>Note</h4><p class="callout">The detailed steps along with the solutions are presented in the <em class="italics">Appendix A</em> (pg. no. 150).</p></li>
			</ol>
			<p>Again, we pause here to note how well the Jupyter Notebook suited our needs when performing this initial data analysis and clean-up. Imagine, for example, we left this project in its current state for a few months. Upon returning to it, we would probably not remember what exactly was going on when we left it. Referring back to this notebook though, we would be able to retrace our steps and quickly recall what we previously learned about the data. Furthermore, we could update the data source with any new data and re-run the notebook to prepare the new set of data for use in our machine learning algorithms. Recall that in this situation, it would be best to make a copy of the notebook first, so as not to lose the initial analysis.</p>
			<p>To summarize, you've learned and applied methods for preparing to train a machine learning model. We started by discussing steps for identifying a problem that can be solved with predictive analytics. This consisted of:</p>
			<ul>
				<li>Looking at the available data</li>
				<li>Determining the business needs</li>
				<li>Assessing the data for suitability</li>
			</ul>
			<p>We also discussed how to identify supervised versus unsupervised and regression versus classification problems.</p>
			<p>After identifying our problem, we learned techniques for using Jupyter Notebooks to build and test a data transformation pipeline. These techniques included methods and best practices for filling missing data, transforming categorical features, and building train/test data sets.</p>
			<p>In the remainder of this chapter, we will use this preprocessed data to train a variety of classification models. To avoid blindly applying algorithms we don't understand, we start by introducing them and overviewing how they work. Then, we use Jupyter to train and compare their predictive capabilities. Here, we have the opportunity to discuss more advanced topics in machine learning like overfitting, k-fold cross-validation, and validation curves.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor043"/>Training Classification Models</h2>
			<p>As you've already seen in the previous chapter, using libraries such as scikit-learn and platforms such as Jupyter, predictive models can be trained in just a few lines of code. This is possible by abstracting away the difficult computations involved with optimizing model parameters. In other words, we deal with a black box where the internal operations are hidden instead. With this simplicity also comes the danger of misusing algorithms, for example, by overfitting during training or failing to properly test on unseen data. We'll show how to avoid these pitfalls while training classification models and produce trustworthy results with the use of k-fold cross validation and validation curves.</p>
			<h3 id="_idParaDest-43"><a id="_idTextAnchor044"/>Introduction to Classification Algorithms</h3>
			<p>Recall the two types of supervised machine learning: regression and classification. In regression, we predict a continuous target variable. For example, recall the linear and polynomial models from the first chapter. In this chapter, we focus on the other type of supervised machine learning: classification. Here, the goal is to predict the class of a sample using the available metrics.</p>
			<p>In the simplest case, there are only two possible classes, which means we are doing binary classification. This is the case for the example problem in this chapter, where we try to predict whether an employee has left or not. If we have more than two class labels instead, we are doing multi-class classification.</p>
			<p>Although there is little difference between binary and multi-class classification when training models with scikit-learn, what's done inside the "black box" is notably different. In particular, multi-class classification models often use the one-versus-rest method. This works as follows for a case with three class labels. When the model is "fit" with the data, three models are trained, and each model predicts whether the sample is part of an individual class or part of some other class. This might bring to mind the one-hot encoding for features that we did earlier. When a prediction is made for a sample, the class label with the highest confidence level is returned.</p>
			<p>In this chapter, we'll train three types of classification models: Support Vector Machines, Random Forests, and k-Nearest Neighbors classifiers. Each of these algorithms are quite different. As we will see, however, they are quite similar to train and use for predictions thanks to scikit-learn. Before swapping over to the Jupyter Notebook and implementing these, we'll briefly see how they work.</p>
			<p>SVMs attempt to find the best hyperplane to divide classes by. This is done by maximizing the distance between the hyperplane and the closest samples of each class, which are called support vectors.</p>
			<p>This linear method can also be used to model nonlinear classes using the kernel trick. This method maps the features into a higher-dimensional space in which the hyperplane is determined. This hyperplane is also referred to as the decision surface, and we'll visualize it when training our models.</p>
			<p>K-Nearest Neighbors classification algorithms memorize the training data and make predictions depending on the K nearest samples in the feature space. With three features, this can be visualized as a sphere surrounding the prediction sample. Often, however, we are dealing with more than three features and therefore hyperspheres are drawn to find the closest K samples.</p>
			<p>Random Forests are an ensemble of decision trees, where each has been trained on different subsets of the training data.</p>
			<p>A decision tree algorithm classifies a sample based on a series of decisions. For example, the first decision might be "if feature x_1 is less than or greater than 0." The data would then be split on this condition and fed into descending branches of the tree. Each step in the decision tree is decided based on the feature split that maximizes the information gain. Essentially, this term describes the mathematics that attempts to pick the best possible split of the target variable.</p>
			<p>Training a Random Forest consists of creating bootstrapped (that is, randomly sampled data with replacement) datasets for a set of decision trees. Predictions are then made based on the majority vote. These have the benefit of less overfitting and better generalizability.</p>
			<h4>Note</h4>
			<p class="callout">Decision trees can be used to model a mix of continuous and categorical data, which make them very useful. Furthermore, as we will see later in this chapter, the tree depth can be limited to reduce overfitting. For a detailed (but brief) look into the decision tree algorithm, check out this popular StackOverflow answer: <a href="https://stackoverflow.com/a/1859910/3511819">https://stackoverflow.com/a/1859910/3511819</a>. There, the author shows a simple example and discusses concepts such as node purity, information gain, and entropy.</p>
			<h3 id="_idParaDest-44"><a id="_idTextAnchor045"/>Exercise 9: Training Two-Feature Classification Models With Scikit-learn</h3>
			<p>We'll continue working on the employee retention problem that we introduced in the first topic. We previously prepared a dataset for training a classification model, in which we predicted whether an employee has left or not. Now, we'll take that data and use it to train classification models:</p>
			<ol>
				<li value="1">Start the <code>NotebookApp</code> and open the <code>lesson-2-workbook.ipynb</code> file. Scroll down to <code>Topic B: Training classification models</code>. Run the first couple of cells to set the default figure size and load the processed data that we previously saved to a CSV file. For this example, we'll be training classification models on two continuous features: <code>satisfaction_level and last_evaluation</code>.</li>
				<li>Draw the bivariate and univariate graphs of the continuous target variables by running the cell with the following code:<pre>sns.jointplot('satisfaction_level', 'last_evaluation', data=df, kind='hex')</pre><div><img src="img/C13018_02_24.jpg" alt="Figure 2.24: Bivariate and univariate graphs for satisfaction_level and last_evaluation" width="1800" height="1077"/></div><h6>Figure 2.24: Bivariate and univariate graphs for satisfaction_level and last_evaluation</h6><p>As you can see in the preceding image, there are some very distinct patterns in the data.</p></li>
				<li>Re-plot the bivariate distribution, segmenting on the target variable, by running the cell containing the following code:<pre>plot_args = dict(shade=True, shade_lowest=False) for i, c in zip((0, 1), ('Reds', 'Blues')):
sns.kdeplot(df.loc[df.left==i, 'satisfaction_level'], df.loc[df.left==i, 'last_evaluation'], cmap=c, **plot_args)</pre><div><img src="img/C13018_02_25.jpg" alt="Figure 2.25: Bivariate distribution for satisfaction_level and last_evaluation" width="1800" height="964"/></div><h6>Figure 2.25: Bivariate distribution for satisfaction_level and last_evaluation</h6><p>Now, we can see how the patterns are related to the target variable. For the remainder of this exercise, we'll try to exploit these patterns to train effective classification models.</p></li>
				<li>Split the data into training and test sets by running the cell containing the following code:<pre>from sklearn.model_selection import train_test_split
features = ['satisfaction_level', 'last_evaluation'] X_train, X_test, y_train, y_test = train_test_split(
df[features].values, df['left'].values, test_size=0.3, random_state=1)</pre><p>Our first two models, the Support Vector Machine and k-Nearest Neighbors algorithm, are most effective when the input data is scaled so that all of the features are on the same order. We'll accomplish this with scikit-learn's <code>StandardScaler</code>.</p></li>
				<li>Load <code>StandardScaler</code> and create a new instance, as referenced by the scaler variable. Fit the scaler on the training set and transform it. Then, transform the test set. Run the cell containing the following code:<pre>from sklearn.preprocessing import StandardScaler scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train) X_test_std = scaler.transform(X_test)</pre><h4>Note</h4><p class="callout">An easy mistake to make when doing machine learning is to "fit" the scaler on the whole dataset, when in fact it should only be "fit" to the training data. For example, scaling the data before splitting into training and testing sets is a mistake. We don't want this because the model training should not be influenced in any way by the test data.</p></li>
				<li>Import the scikit-learn support vector machine class and fit the model on the training data by running the cell containing the following code:<pre>from sklearn.svm import SVC
svm = SVC(kernel='linear', C=1, random_state=1) svm.fit(X_train_std, y_train)</pre></li>
				<li>Compute the accuracy of this model on unseen data by running the cell containing the following code:<pre>from sklearn.metrics import accuracy_score y_pred = svm.predict(X_test_std)
acc = accuracy_score(y_test, y_pred) print('accuracy = {:.1f}%'.format(acc*100))
&gt;&gt; accuracy = 75.9%</pre></li>
				<li>We predict the targets for our test samples and then use scikit-learn's <code>accuracy_score</code> function to determine the accuracy. The result looks promising at ~75%! Not bad for our first model. Recall, though, the target is imbalanced. Let's see how accurate the predictions are for each class.</li>
				<li>Calculate the confusion matrix and then determine the accuracy within each class by running the cell containing the following code:<pre>from sklearn.metrics import confusion_matrix cmat = confusion_matrix(y_test, y_pred)
scores = cmat.diagonal() / cmat.sum(axis=1) * 100 print('left = 0 : {:.2f}%'.format(scores[0]))
print('left = 1 : {:.2f}%'.format(scores[1]))
&gt;&gt; left = 0 : 100.00%
&gt;&gt; left = 1 : 0.00%</pre><p>It looks like the model is simply classifying every sample as 0, which is clearly not helpful at all. Let's use a contour plot to show the predicted class at each point in the feature space. This is commonly known as the decision- regions plot.</p></li>
				<li>Plot the decision regions using a helpful function from the <code>mlxtend</code> library. Run the cell containing the following code:<pre>from mlxtend.plotting import plot_decision_regions N_samples = 200
X, y = X_train_std[:N_samples], y_train[:N_samples] plot_decision_regions(X, y, clf=svm)</pre><div><img src="img/C13018_02_26.jpg" alt="Figure 2.26: Plot of the decision regions" width="1675" height="942"/></div><h6>Figure 2.26: Plot of the decision regions</h6><p>The function plots decision regions along with a set of samples passed as arguments. In order to see the decision regions properly without too many samples obstructing our view, we pass only a 200-sample subset of the test data to the <code>plot_decision_regions</code> function. In this case, of course, it does not matter. We see the result is entirely red, indicating every point in the feature space would be classified as 0.</p><p>It shouldn't be surprising that a linear model can't do a good job of describing these nonlinear patterns. Recall earlier we mentioned the kernel trick for using SVMs to classify nonlinear problems. Let's see if doing this can improve the result.</p></li>
				<li>Print the docstring for scikit-learn's SVM by running the cell containing SVC. Scroll down and check out the parameter descriptions. Notice the kernel option, which is actually enabled by default as <code>rbf</code>. Use this <code>kernel</code> option to train a new SVM by running the cell containing the following code:<pre>check_model_fit(svm, X_test_std, y_test)</pre></li>
			</ol>
			<div><div><img src="img/C13018_02_27.jpg" alt="Figure 2.27: Training a new SVM " width="1800" height="1403"/>
				</div>
			</div>
			<h6>Figure 2.27: Training a new SVM </h6>
			<h6> </h6>
			<div><div><img src="img/C13018_02_28.jpg" alt="Figure 2.28: Enhanced results with non-linear patterns" width="1800" height="1018"/>
				</div>
			</div>
			<h6>Figure 2.28: Enhanced results with non-linear patterns</h6>
			<p>The result is much better. Now, we are able to capture some of the non-linear patterns in the data and correctly classify the majority of the employees who have left.</p>
			<h3 id="_idParaDest-45"><a id="_idTextAnchor046"/>The plot_decision_regions Function</h3>
			<p>The <code>plot_decision_regions</code> function is provided by <code>mlxtend</code>, a Python library developed by Sebastian Raschka. It's worth taking a peek at the source code (which is of course written in Python) to understand how these plots are drawn. It's really not too complicated.</p>
			<p>In a Jupyter Notebook, import the function with <code>from mlxtend.plotting import plot_decision_regions</code> and then pull up the help with <code>plot_decision_regions?</code> and scroll to the bottom to see the local file path:</p>
			<div><div><img src="img/C13018_02_29.jpg" alt="Figure 2.29: Local file path" width="1765" height="346"/>
				</div>
			</div>
			<h6>Figure 2.29: Local file path</h6>
			<p>Then, open up the file and read through it. For example, you could run <code>cat</code> in the notebook:</p>
			<div><div><img src="img/C13018_02_30.jpg" alt="Figure 2.30: Running cat in the notebook" width="1800" height="698"/>
				</div>
			</div>
			<h6>Figure 2.30: Running cat in the notebook</h6>
			<p>This is okay, but not ideal as there's no color markup for the code. It's better to copy it (so you don't accidentally alter the original) and open it with your favorite text editor.</p>
			<p>When drawing attention to the code responsible for mapping the decision regions, we see a contour plot of predictions Z over an array <code>X_predict</code> that spans the feature space.</p>
			<div><div><img src="img/C13018_02_31.jpg" alt="Figure 2.31: The screenshot of the code for mapping decision regions&#13;&#10;" width="1800" height="1013"/>
				</div>
			</div>
			<h6>Figure 2.31: The screenshot of the code for mapping decision regions</h6>
			<p>Let's move to training our model on k-Nearest Neighbors.</p>
			<h3 id="_idParaDest-46"><a id="_idTextAnchor047"/>Exercise 10: Training K-nearest Neighbors for Our Model</h3>
			<ol>
				<li value="1">Load the scikit-learn KNN classification model and print the docstring by running the cell containing the following code:<pre>from sklearn.neighbors import KNeighborsClassifier KNeighborsClassifier?</pre><p>The <code>n_neighbors</code> parameter decides how many samples to use when making a classification. If the weights parameter is set to uniform, then class labels are decided by majority vote. Another useful choice for the weights is distance, where closer samples have a higher weight in the voting. Like most model parameters, the best choice for this depends on the particular dataset.</p></li>
				<li>Train the KNN classifier with <code>n_neighbors=3</code>, and then compute the accuracy and decision regions. Run the cell containing the following code:<pre>knn = KNeighborsClassifier(n_neighbors=3) 
knn.fit(X_train_std, y_train)
check_model_fit(knn, X_test_std, y_test)</pre><div><img src="img/C13018_02_32.jpg" alt="Figure 2.32: Training the kNN classifier with n_negihbours=3" width="1800" height="1250"/></div><h6>Figure 2.32: Training the kNN classifier with n_negihbours=3</h6><div><img src="img/C13018_02_33.jpg" alt="Figure 2.33: Enhanced results after training" width="1800" height="965"/></div><h6>Figure 2.33: Enhanced results after training</h6><p>We see an increase in overall accuracy and a significant improvement for class 1 in particular. However, the decision region plot would indicate we are overfitting the data. This is evident by the hard, "choppy" decision boundary, and small pockets of blue everywhere. We can soften the decision boundary and decrease overfitting by increasing the number of nearest neighbors.</p></li>
				<li>Train a KNN model with <code>n_neighbors=25</code> by running the cell containing the following code:<pre>knn = KNeighborsClassifier(n_neighbors=25) knn.fit(X_train_std, y_train)
check_model_fit(knn, X_test_std, y_test)</pre></li>
			</ol>
			<div><div><img src="img/C13018_02_34.jpg" alt="Figure 2.34: Training the kNN classifier with n_negihbours=25" width="1800" height="1368"/>
				</div>
			</div>
			<h6>Figure 2.34: Training the kNN classifier with n_negihbours=25</h6>
			<div><div><img src="img/C13018_02_35.jpg" alt="Figure 2.35: Output after training with n_neighbours=25 " width="1800" height="1223"/>
				</div>
			</div>
			<h6>Figure 2.35: Output after training with n_neighbours=25 </h6>
			<p>As we can see, the decision boundaries are significantly less choppy, and there are far less pockets of blue. The accuracy for class 1 is slightly less, but we would need to use a more comprehensive method such as k-fold cross validation to decide if there's a significant difference between the two models.</p>
			<p>Note that increasing <code>n_neighbors</code> has no effect on training time, as the model is simply memorizing the data. The prediction time, however, will be greatly affected.</p>
			<h4>Note</h4>
			<p class="callout">When doing machine learning with real-world data, it's important for the algorithms to run quick enough to serve their purposes. For example, a script to predict tomorrow's weather that takes longer than a day to run is completely useless! Memory is also a consideration that should be taken into account when dealing with substantial amounts of data.</p>
			<p>We will now train a Random Forest.</p>
			<h3 id="_idParaDest-47"><a id="_idTextAnchor048"/>Exercise 11: Training a Random Forest</h3>
			<h4>Note</h4>
			<p class="callout">Observe how similar it is to train and make predictions on each model, despite them each being so different internally.</p>
			<ol>
				<li value="1">Train a Random Forest classification model composed of 50 decision trees, each with a max depth of 5. Run the cell containing the following code:<pre>from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=50, max_depth=5,
random_state=1)
forest.fit(X_train, y_train) check_model_fit(forest, X_test, y_test)</pre><div><img src="img/C13018_02_36.jpg" alt="Figure 2.36: Training a Random Forest with a max depth of 5" width="1800" height="1490"/></div><h6>Figure 2.36: Training a Random Forest with a max depth of 5</h6><div><img src="img/C13018_02_37.jpg" alt="Figure 2.37: Output after training with a max depth of 5" width="1800" height="989"/></div><p class="Normal" lang="en-US" xml:lang="en-US"> </p><h6>Figure 2.37: Output after training with a max depth of 5</h6><p>Note the distinctive axes-parallel decision boundaries produced by decision tree machine learning algorithms.</p><p>We can access any of the individual decision trees used to build the Random Forest. These trees are stored in the <code>estimators_attribute</code> of the model. Let's draw one of these decision trees to get a feel for what's going on. Doing this requires the <strong class="bold">graphviz</strong> dependency, which can sometimes be difficult to install.</p></li>
				<li>Draw one of the decision trees in the Jupyter Notebook by running the cell containing the following code:<pre>from sklearn.tree import export_graphviz import graphviz
dot_data = export_graphviz(
forest.estimators_[0], out_file=None, feature_names=features, class_names=['no', 'yes'], filled=True, rounded=True, special_characters=True)
graph = graphviz.Source(dot_data) graph</pre></li>
			</ol>
			<div><div><img src="img/C13018_02_38.jpg" alt="Figure 2.38: Decision tree obtained using graphviz" width="1129" height="856"/>
				</div>
			</div>
			<h6>Figure 2.38: Decision tree obtained using graphviz</h6>
			<p>We can see that each path is limited to five nodes as a result of setting max_depth=5. The orange boxes represent predictions of no (has not left the company), and the blue boxes represent yes (has left the company). The shade of each box (light, dark, and so on) indicates the confidence level, which is related to the gini value.</p>
			<p>To summarize, we have accomplished two of the learning objectives in this section:</p>
			<ul>
				<li>We gained a qualitative understanding of support vector machines (SVMs), k-Nearest Neighbor classifiers (kNNs), and Random Forest</li>
				<li>We are now able to train a variety of models using scikit-learn and Jupyter Notebooks so that we can confidently build and compare predictive models</li>
			</ul>
			<p>In particular, we used the preprocessed data from our employee retention problem to train classification models to predict whether an employee has left the company or not. For the purposes of keeping things simple and focusing on the algorithms, we built models to predict this given only two features: the satisfaction level and last evaluation value. This two-dimensional feature space also allowed us to visualize the decision boundaries and identify what overfitting looks like.</p>
			<p>In the following section, we will introduce two important topics in machine learning: k-fold cross-validation and validation curves.</p>
			<h3 id="_idParaDest-48"><a id="_idTextAnchor049"/>Assessing Models With K-fold Cross-Validation and Validation Curves</h3>
			<p>Thus far, we have trained models on a subset of the data and then assessed performance on the unseen portion, called the test set. This is good practice because the model performance on training data is not a good indicator of its effectiveness as a predictor. It's very easy to increase accuracy on a training dataset by overfitting a model, which can result in poorer performance on unseen data.</p>
			<p>That said, simply training models on data split in this way is not good enough. There is a natural variance in data that causes accuracies to be different (if even slightly) depending on the training and test splits. Furthermore, using only one training/test split to compare models can introduce bias towards certain models and lead to overfitting. </p>
			<p><strong class="bold">K-fold cross validation</strong> offers a solution to this problem and allows the variance to be accounted for by way of an error estimate on each accuracy calculation. This, in turn, naturally leads to the use of validation curves for tuning model parameters. These plot the accuracy as a function of a hyper parameter such as the number of decision trees used in a Random Forest or the max depth.</p>
			<h4>Note</h4>
			<p class="callout">This is our first time using the term hyperparameter. It references a parameter that is defined when initializing a model, for example, the C parameter of the SVM. This is in contradistinction to a parameter of the trained model, such as the equation of the decision boundary hyperplane for a trained SVM.</p>
			<p>The method is illustrated in the following diagram, where we see how the k-folds can be selected from the dataset:</p>
			<div><div><img src="img/C13018_02_39.jpg" alt="Figure 2.39: Selecting k-folds from a data set" width="1517" height="494"/>
				</div>
			</div>
			<h6>Figure 2.39: Selecting k-folds from a data set</h6>
			<p>The k-fold cross validation algorithm goes as follows:</p>
			<ol>
				<li value="1">Split data into k "folds" of near-equal size.</li>
				<li>Test and train k models on different fold combinations. Each model will include k - 1 folds of training data and the left-out fold is used for testing. In this method, each fold ends up being used as the validation data exactly once.</li>
				<li>Calculate the model accuracy by taking the mean of the k values. The standard deviation is also calculated to provide error bars on the value.</li>
			</ol>
			<p>It's standard to set <em class="italics">k = 10</em>, but smaller values for k should be considered if using a big data set.</p>
			<p>This validation method can be used to reliably compare model performance with different hyperparameters (for example, the C parameter for an SVM or the number of nearest neighbors in a KNN classifier). It's also suitable for comparing entirely different models.</p>
			<p>Once the <em class="italics">best model</em> has been identified, it should be re-trained on the entirety of the dataset before being used to predict actual classifications.</p>
			<p>When implementing this with scikit-learn, it's common to use a slightly improved variation of the normal k-fold algorithm instead. This is called <strong class="bold">stratified k-fold</strong>. The improvement is that stratified k-fold cross validation maintains roughly even class label populations in the folds. As you can imagine, this reduces the overall variance in the models and decreases the likelihood of highly unbalanced models causing bias.</p>
			<p><strong class="bold">Validation curves</strong> are plots of a training and validation metric as a function of some model parameter. They allow to us to make good model parameter selections. In this book, we will use the accuracy score as our metric for these plots.</p>
			<h4>Note</h4>
			<p class="callout">The documentation for plot validation curves is available here: <a href="http://scikit-learn.org/stable/auto_examples/">http://scikit-learn.org/stable/auto_examples/</a> model_selection/plot_validation_curve.html.</p>
			<p>Consider this validation curve, where the accuracy score is plotted as a function of the gamma SVM parameter:</p>
			<div><div><img src="img/C13018_02_40.jpg" alt="Figure 2.40: Validation curve with SVM" width="1357" height="570"/>
				</div>
			</div>
			<h6>Figure 2.40: Validation curve with SVM</h6>
			<p>Starting on the left side of the plot, we can see that both sets of data are agreeing on the score, which is good. However, the score is also quite low compared to other gamma values, so therefore we say the model is underfitting the data. Increasing the gamma, we can see a point where the error bars of these two lines no longer overlap. From this point on, we see the classifier overfitting the data as the models behave increasingly well on the training set compared to the validation set. The optimal value for the gamma parameter can be found by looking for a high validation score with overlapping error bars on the two lines.</p>
			<p>Keep in mind that a learning curve for some parameter is only valid while the other parameters remain constant. For example, if training the SVM in this plot, we could decide to pick gamma on the order of. However, we may want to optimize the <code>C</code> parameter as well. With a different value for <code>C</code>, the preceding plot would be different and our selection for gamma may no longer be optimal.</p>
			<h3 id="_idParaDest-49"><a id="_idTextAnchor050"/>Exercise 12: Using K-fold Cross Validation and Validation Curves in Python With Scikit-learn</h3>
			<ol>
				<li value="1">Start the <code>NotebookApp</code> and open the <code>lesson-2-workbook.ipynb</code> file. Scroll down to <code>Subtopic B: K-fold cross-validation and validation curves</code>.<p>The training data should already be in the notebook's memory, but let's reload it as a reminder of what exactly we're working with.</p></li>
				<li>Load the data and select the <code>satisfaction_level</code> and <code>last_evaluation</code> features for the training/validation set. We will not use the train-test split this time because we are going to use k-fold validation instead. Run the cell containing the following code:<pre>df = pd.read_csv('../data/hr-analytics/hr_data_processed. csv')
features = ['satisfaction_level', 'last_evaluation']
X = df[features].values y = df.left.values</pre></li>
				<li>Instantiate a Random Forest model by running the cell containing the following code:<pre>clf = RandomForestClassifier(n_estimators=100, max_depth=5)</pre></li>
				<li>To train the model with stratified k-fold cross validation, we'll use the <code>model_ selection.cross_val_score</code> function.<p>Train 10 variations of our model <code>clf</code> using stratified k-fold validation. Note that scikit-learn's <code>cross_val_score</code> does this type of validation by default. Run the cell containing the following code:</p><pre>from sklearn.model_selection import cross_val_score np.random.seed(1)
scores = cross_val_score(
estimator=clf, X=X,
y=y, cv=10)
print('accuracy = {:.3f} +/- {:.3f}'.format(scores.mean(), scores.std()))
&gt;&gt; accuracy = 0.923 +/- 0.005</pre><p>Note how we use <code>np.random.seed</code> to set the seed for the random number generator, therefore ensuring reproducibility with respect to the randomly selected samples for each fold and decision tree in the Random Forest.</p></li>
				<li>Calculate the accuracy as the average of each fold. We can also see the individual accuracies for each fold by printing scores. To see these, <code>run print(scores)</code>:<pre>&gt;&gt; array([ 0.93404397,	0.91533333,	0.92266667,
0.91866667,	0.92133333,
0.92866667,	0.91933333,	0.92	,
0.92795197,	0.92128085])</pre><p>Using <code>cross_val_score</code> is very convenient, but it doesn't tell us about the accuracies within each class. We can do this manually with the <code>model_ selection.StratifiedKFold</code> class. This class takes the number of folds as an initialization parameter, then the split method is used to build randomly sampled "masks" for the data. A mask is simply an array containing indexes of items in another array, where the items can then be returned by doing this: <code>data[mask]</code>.</p></li>
				<li>Define a custom class for calculating k-fold cross validation class accuracies. Run the cell containing the following code:<pre>from sklearn.model_selection import StratifiedKFold
…
…
print('fold: {:d} accuracy: {:s}'.format(k+1, str(class_acc)))
return class_accuracy</pre><h4>Note</h4><p class="callout">For the complete code, refer to the following: <a href="https://bit.ly/2O5uP3h">https://bit.ly/2O5uP3h</a>. </p></li>
				<li>We can then calculate the class accuracies with code that's very similar to step 4. Do this by running the cell containing the following code:<pre>from sklearn.model_selection import cross_val_score np.random.seed(1)
…
…
&gt;&gt; fold: 10 accuracy: [ 0.98861646	0.70588235]
&gt;&gt; accuracy = [ 0.98722476	0.71715647] +/- [ 0.00330026
0.02326823]</pre><h4>Note</h4><p class="callout">For the complete code, refer to the following: <a href="https://bit.ly/2EKK7Lp">https://bit.ly/2EKK7Lp</a>.</p></li>
				<li>Now we can see the class accuracies for each fold! Pretty neat, right?</li>
				<li>Calculate a validation curve using <code>model_selection.validation_curve</code>. This function uses stratified k-fold cross validation to train models for various values of a given parameter. <p>Do the calculations required to plot a validation curve by training Random Forests over a range of <code>max_depth</code> values. Run the cell containing the following code:</p><pre>from sklearn.model_selection import validation_curve
clf = RandomForestClassifier(n_estimators=10) max_depths = np.arange(3, 16, 3)
train_scores, test_scores = validation_curve( estimator=clf,
X=X,
y=y, param_name='max_depth', param_range=max_depths,
cv=10);</pre><p>This will return arrays with the cross validation scores for each model, where the models have different max depths. In order to visualize the results, we'll leverage a function provided in the scikit-learn documentation.</p></li>
				<li>Run the cell in which <code>plot_validation_curve</code> is defined. Then, run the cell containing the following code to draw the plot:<pre>plot_validation_curve(train_scores, test_scores,
max_depths, xlabel='max_depth')</pre></li>
			</ol>
			<div><div><img src="img/C13018_02_41.jpg" alt="Figure 2.41: Plot validation curve" width="1800" height="873"/>
				</div>
			</div>
			<h6>Figure 2.41: Plot validation curve</h6>
			<p>Recall how setting the max depth for decision trees limits the amount of overfitting. This is reflected in the validation curve, where we see overfitting taking place for large max depth values to the right. A good value for <code>max_depth</code> appears to be 6, where we see the training and validation accuracies in agreement. When <code>max_depth</code> is equal to 3, we see the model underfitting the data as training and validation accuracies are lower.</p>
			<p>To summarize, we have learned and implemented two important techniques for building reliable predictive models. The first such technique was k-foldcross-validation, which is used to split the data into various train/test batches and generate a set accuracy. From this set, we then calculated the average accuracy and the standard deviation as a measure of the error. This is important so that we have a gauge of the variability of our model and we can produce trustworthy accuracy.</p>
			<p>We also learned about another such technique to ensure we have trustworthy results: validation curves. These allow us to visualize when our model is overfitting based on comparing training and validation accuracies. By plotting the curve over a range of our selected hyperparameter, we are able to identify its optimal value.</p>
			<p>In the final section of this chapter, we take everything we have learned so far and put it together in order to build our final predictive model for the employee retention problem. We seek to improve the accuracy, compared to the models trained thus far, by including all of the features from the dataset in our model. We'll see now-familiar topics such as k-fold cross-validation and validation curves, but we'll also introduce something new: dimensionality reduction techniques.</p>
			<h3 id="_idParaDest-50"><a id="_idTextAnchor051"/>Dimensionality Reduction Techniques</h3>
			<p>Dimensionality reduction can simply involve removing unimportant features from the training data, but more exotic methods exist, such as <strong class="keyword">Principal Component Analysis</strong> (<strong class="keyword">PCA</strong>) and Linear Discriminant Analysis (LDA). These techniques allow for data compression, where the most important information from a large group of features can be encoded in just a few features.</p>
			<p>In this subtopic, we'll focus on PCA. This technique transforms the data by projecting it into a new subspace of orthogonal "principal components," where the components with the highest eigenvalues encode the most information for training the model. Then, we can simply select a few of these principal components in place of the original high-dimensional dataset. For example, PCA could be used to encode the information from every pixel in an image. In this case, the original feature space would have dimensions equal to the number of pixels in the image. This high-dimensional space could then be reduced with PCA, where the majority of useful information for training predictive models might be reduced to just a few dimensions. Not only does this save time when training and using models, it allows them to perform better by removing noise in the dataset.</p>
			<p>Like the models you've seen, it's not necessary to have a detailed understanding of PCA in order to leverage the benefits. However, we'll dig into the technical details of PCA just a bit further so that we can conceptualize it better. The key insight of PCA is to identify patterns between features based on correlations, so the PCA algorithm calculates the covariance matrix and then decomposes this into eigenvectors and eigenvalues. The vectors are then used to transform the data into a new subspace, from which a fixed number of principal components can be selected.</p>
			<p>In the following exercise, we'll see an example of how PCA can be used to improve our Random Forest model for the employee retention problem we have been working on. This will be done after training a classification model on the full feature space, to see how our accuracy is affected by dimensionality reduction.</p>
			<h3 id="_idParaDest-51"><a id="_idTextAnchor052"/>Exercise 13: Training a Predictive Model for the Employee Retention Problem</h3>
			<p>We have already spent considerable effort planning a machine learning strategy, preprocessing the data, and building predictive models for the employee retention problem. Recall that our business objective was to help the client prevent employees from leaving. The strategy we decided upon was to build a classification model that would predict the probability of employees leaving. This way, the company can assess the likelihood of current employees leaving and take action to prevent it.</p>
			<p>Given our strategy, we can summarize the type of predictive modeling we are doing as follows:</p>
			<ul>
				<li>Supervised learning on labeled training data</li>
				<li>Classification problems with two class labels (binary)</li>
			</ul>
			<p>In particular, we are training models to determine whether an employee has left the company, given a set of continuous and categorical features. After preparing the data for machine learning in <em class="italics">Activity 1, Preparing to Train a Predictive Model for the Employee-Retention Problem</em>, we went on to implement SVM, k-Nearest Neighbors, and Random Forest algorithms using just two features. These models were able to make predictions with over 90% overall accuracy. When looking at the specific class accuracies, however, we found that employees who had left (<code>class- label 1</code>) could only be predicted with 70-80% accuracy. </p>
			<p>Let's see how much this can be improved by utilizing the full feature space.</p>
			<ol>
				<li value="1">Scroll down to the code for this section in the <code>lesson-2-workbook.ipynb</code> notebook. We should already have the preprocessed data loaded from the previous exercises, but this can be done again, if desired, by executing <code>df = pd.read_csv</code>(<code>'../data/hr-analytics/hr_data_processed.csv'</code>). Then, print the DataFrame columns with <code>print(df.columns)</code>.</li>
				<li>Define a list of all the features by copy and pasting the output from <code>df.columns</code> into a new list (making sure to remove the target variable <code>left</code>). Then, define <code>X</code> and <code>Y</code> as we have done before. This goes as follows:<pre>features = ['satisfaction_level', 'last_evaluation', 'number_project',
'average_montly_hours', 'time_spend_company', 'work_ accident',
…
…
X = df[features].values y = df.left.values</pre><h4>Note</h4><p class="callout">For the complete code, refer to the following: <a href="https://bit.ly/2D3WOQ2">https://bit.ly/2D3WOQ2</a>. </p><p>Looking at the feature names, recall what the values look like for each one. Scroll up to the set of histograms we made in the first activity to help jog your memory. The first two features are continuous; these are what we used for training models in the previous two exercises. After that, we have a few discrete features, such as <code>number_project</code> and <code>time_spend_company</code>, followed by some binary fields such as <code>work_accident</code> and <code>promotion_last_5years</code>. We also have a bunch of binary features, such as <code>department_ IT</code> and <code>department_accounting</code>, which were created by one-hot encoding. </p><p>Given a mix of features like this, Random Forests are a very attractive type of model. For one thing, they're compatible with feature sets composed of both continuous and categorical data, but this is not particularly special; for instance, an SVM can be trained on mixed feature types as well (given proper preprocessing).</p><h4>Note</h4><p class="callout">If you're interested in training an SVM or k-Nearest Neighbors classifier on mixed-type input features, you can use the data-scaling prescription from this StackExchange answer: <a href="https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm/83086#83086">https://stats.stackexchange.com/questions/82923/mixing-continuous-and-binary-data-with-linear-svm/83086#83086</a>.</p><p>A simple approach would be to preprocess data as follows: </p><p>standardize continuous variables; one-hot-encode categorical features; shift binary values to -1 and 1 instead of 0 and 1. Finally, the mixed-feature data could be used to train a variety of classification models.</p></li>
				<li>Tune the <code>max_depth</code> hyperparameter using a validation curve to figure out the best parameters for our Random Forest model. Calculate the training and validation accuracies by running the following code:<pre>%%time np.random.seed(1)
clf = RandomForestClassifier(n_estimators=20) max_depths = [3, 4, 5, 6, 7,
9, 12, 15, 18, 21]
train_scores, test_scores = validation_curve( estimator=clf,
X=X,
y=y, param_name='max_depth', param_range=max_depths,
cv=5);</pre><p>We are testing 10 models with k-fold cross validation. By setting k = 5, we produce five estimates of the accuracy for each model, from which we extract the mean and standard deviation to plot in the validation curve. In total, we train 50 models, and since <code>n_estimators</code> is set to 20, we are training a total of 1,000 decision trees! All in roughly 10 seconds!</p></li>
				<li>Plot the validation curve using our custom <code>plot_validation_curve</code> function from the last exercise. Run the following code:<pre>plot_validation_curve(train_scores, test_scores,
max_depths, xlabel='max_depth');</pre><div><img src="img/C13018_02_42.jpg" alt="Figure 2.42: Plot validation curve for different values of max_depths" width="1800" height="993"/></div><h6>Figure 2.42: Plot validation curve for different values of max_depths</h6><p>For small max depths, we see the model underfitting the data. Total accuracies dramatically increase by allowing the decision trees to be deeper and encode more complicated patterns in the data. As the max depth is increased further and the accuracy approaches 100%, we find the model overfits the data, causing the training and validation accuracies to grow apart. Based on this figure, let's select a <code>max_depth</code> of 6 for our model.</p><p>We should really do the same for <code>n_estimators</code>, but in the spirit of saving time, we'll skip it. You are welcome to plot it on your own; you should find agreement between training and validation sets for a large range of values. Usually it's better to use more decision tree estimators in the random forest, but this comes at the cost of increased training times. We'll use 200 estimators to train our model.</p></li>
				<li>Use <code>cross_val_class_score</code>, the k-fold cross validation by class function we created earlier, to test the selected model, a Random Forest with <code>max_ depth = 6</code> and <code>n_estimators = 200:</code><pre>np.random.seed(1)
clf = RandomForestClassifier(n_estimators=200, max_depth=6) scores = cross_val_class_score(clf, X, y)
print('accuracy = {} +/- {}'\
.format(scores.mean(axis=0), scores.std(axis=0)))
&gt;&gt; accuracy = [ 0.99553722	0.85577359] +/- [ 0.00172575
0.02614334]</pre><p>The accuracies are way higher now that we're using the full feature set, compared to before when we only had the two continuous features!</p></li>
				<li>Visualize the accuracies with a boxplot by running the following code:<pre>fig = plt.figure(figsize=(5, 7)) sns.boxplot(data=pd.DataFrame(scores, columns=[0, 1]),
palette=sns.color_palette('Set1')) plt.xlabel('Left')
plt.ylabel('Accuracy')</pre><div><img src="img/C13018_02_43.jpg" alt="Figure 2.43: Visualizing the accuracy with a box plot" width="1800" height="1013"/></div><h6>Figure 2.43: Visualizing the accuracy with a box plot</h6><p>Random forests can provide an estimate of the feature performances.</p><h4>Note</h4><p class="callout">The feature importance in scikit-learn is calculated based on how the node impurity changes with respect to each feature. For a more detailed explanation, take a look at the following StackOverflow thread about how feature importance is determined in Random Forest Classifier: <a href="https://stackoverflow.com">https://stackoverflow.com</a></p></li>
				<li>Plot the feature importance, as stored in the attribute <code>feature_importances_</code>, by running the following code:<pre>pd.Series(clf.feature_importances_, name='Feature importance',
index=df[features].columns)\
.sort_values()\
.plot.barh() plt.xlabel('Feature importance')</pre><div><img src="img/C13018_02_44.jpg" alt="Figure 2.44: Plot of feature_importance" width="1800" height="1013"/></div><h6>Figure 2.44: Plot of feature_importance</h6></li>
				<li>It doesn't look like we're getting much in the way of useful contribution from the one-hot encoded variables: department and salary. Also, the <code>promotion_last_5years</code> and <code>work_accident</code> features don't appear to be very useful.<p>Let's use PCA to condense all of these weak features into just a few principal components.</p></li>
				<li>Import the <code>PCA</code> class from scikit-learn and transform the features. Run the following code<pre>from sklearn.decomposition import PCA pca_features = \
…
…
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_reduce)</pre><h4>Note</h4><pre>&gt;&gt; array([[-0.67733089,	0.75837169, -0.10493685],
&gt;&gt;	[ 0.73616575,	0.77155888, -0.11046422],
&gt;&gt;	[ 0.73616575,	0.77155888, -0.11046422],
&gt;&gt;	...,
&gt;&gt;	[-0.67157059, -0.3337546 ,	0.70975452],
&gt;&gt;	[-0.67157059, -0.3337546 ,	0.70975452],
&gt;&gt;	[-0.67157059, -0.3337546 ,	0.70975452]])</pre><p>Since we asked for the top three components, we get three vectors returned.</p></li>
				<li>Add the new features to our DataFrame with the following code:<pre>df['first_principle_component'] = X_pca.T[0] df['second_principle_component'] = X_pca.T[1]
df['third_principle_component'] = X_pca.T[2]</pre><p>Select our reduced-dimension feature set to train a new Random Forest with. Run the following code:</p><pre>features = ['satisfaction_level', 'number_project', 'time_spend_company',
'average_montly_hours', 'last_evaluation', 'first_principle_component', 'second_principle_component', 'third_principle_component']
X = df[features].values y = df.left.values</pre></li>
				<li>Assess the new model's accuracy with k-fold cross validation. This can be done by running the same code as before, where X now points to different features. The code is as follows:<pre>np.random.seed(1)
clf = RandomForestClassifier(n_estimators=200, max_depth=6) scores = cross_val_class_score(clf, X, y)
print('accuracy = {} +/- {}'\
.format(scores.mean(axis=0), scores.std(axis=0)))
&gt;&gt; accuracy = [ 0.99562463	0.90618594] +/- [ 0.00166047
0.01363927]</pre></li>
				<li>Visualize the result in the same way as before, using a box plot. The code is as follows:<pre>fig = plt.figure(figsize=(5, 7)) sns.boxplot(data=pd.DataFrame(scores, columns=[0, 1]), palette=sns.color_palette('Set1')) plt.xlabel('Left') plt.ylabel('Accuracy')</pre><div><img src="img/C13018_02_45.jpg" alt="Figure 2.45: Box plot to visualize accuracy" width="1800" height="989"/></div><h6>Figure 2.45: Box plot to visualize accuracy</h6><p>Comparing this to the previous result, we find an improvement in the class 1 accuracy! Now, the majority of the validation sets return an accuracy greater than 90%. The average accuracy of 90.6% can be compared to the accuracy of 85.6% prior to dimensionality reduction!</p><p>Let's select this as our final model. We'll need to re-train it on the full sample space before using it in production.</p></li>
				<li>Train the final predictive model by running the following code:<pre>np.random.seed(1)
clf = RandomForestClassifier(n_estimators=200, max_depth=6) clf.fit(X, y)</pre></li>
				<li>Save the trained model to a binary file using <code>externals.joblib.dump</code>. Run the following code:<pre>from sklearn.externals import joblib joblib.dump(clf, 'random-forest-trained.pkl')</pre></li>
				<li>Check that it's saved into the working directory, for example, by running:<p><code>!ls *.pkl</code>. Then, test that we can load the model from the file by running the following code:</p><pre>clf = joblib.load('random-forest-trained.pkl')</pre><p>Congratulations! You've trained the final predictive model! Now, let's see an example of how it can be used to provide business insights for the client. Say we have a particular employee, who we'll call Sandra. Management has noticed she is working very hard and reported low job satisfaction in a recent survey. They would therefore like to know how likely it is that she will quit. For the sake of simplicity, let's take her feature values as a sample from the training set (but pretend that this is unseen data instead).</p></li>
				<li>List the feature values for Sandra by running the following code:<pre>sandra = df.iloc[573]X = sandra[features]X
&gt;&gt; satisfaction_level              0.360000
&gt;&gt; number_project                  2.000000
&gt;&gt; time_spend_company              3.000000
&gt;&gt; average_montly_hours          148.000000
&gt;&gt; last_evaluation                 0.470000
&gt;&gt; first_principle_component       0.742801
&gt;&gt; second_principle_component     -0.514568
&gt;&gt; third_principle_component      -0.677421</pre><p>The next step is to ask the model which group it thinks she should be in.</p></li>
				<li>Predict the class label for Sandra by running the following code:<pre>clf.predict([X])
&gt;&gt; array([1])</pre><p>The model classifies her as having already left the company; not a good sign! We can take this a step further and calculate the probabilities of each class label.</p></li>
				<li>Use <code>clf.predict_proba</code> to predict the probability of our model predicting that Sandra has quit. Run the following code:<pre>clf.predict_proba([X])
&gt;&gt; array([[ 0.06576239,	0.93423761]])</pre><p>We see the model predicting that she has quit with 93% accuracy. Since this is clearly a red flag for management, they decide on a plan to reduce her number of monthly hours to 100 and the time spent at the company to 1.</p></li>
				<li>Calculate the new probabilities with Sandra's newly planned metrics. Run the following code:<pre>X.average_montly_hours = 100
X.time_spend_company = 1 clf.predict_proba([X])
&gt;&gt; array([[ 0.61070329,	0.38929671]])</pre><p>Excellent! We can now see that the model returns a mere 38% likelihood that she has quit! Instead, it now predicts she will not have left the company.</p></li>
			</ol>
			<p>Our model has allowed management to make a data-driven decision. By reducing her amount of time with the company by this particular amount, the model tells us that she will most likely remain an employee at the company!</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor053"/>Summary</h2>
			<p>In this chapter, we have seen how predictive models can be trained in Jupyter Notebooks. </p>
			<p>To begin with, we talked about how to plan a machine learning strategy. We thought about how to design a plan that can lead to actionable business insights and stressed the importance of using the data to help set realistic business goals. We also explained machine learning terminology such as supervised learning, unsupervised learning, classification, and regression. </p>
			<p>Next, we discussed methods for preprocessing data using scikit-learn and pandas. This included lengthy discussions and examples of a surprisingly time-consuming part of machine learning: dealing with missing data.</p>
			<p>In the latter half of the chapter, we trained predictive classification models for our binary problem, comparing how decision boundaries are drawn for various models such as the SVM, k-Nearest Neighbors, and Random Forest. We then showed how validation curves can be used to make good parameter choices and how dimensionality reduction can improve model performance. Finally, at the end of our activity, we explored how the final model can be used in practice to make data-driven decisions.</p>
			<p>In the next chapter, we will focus on data acquisition. Specifically, we will analyze HTTP requests, scrape tabular data from a web page, build and transform Pandas DataFrames, and finally create visualizations.</p>
		</div>
	</div></body></html>