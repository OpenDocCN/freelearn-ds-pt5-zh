<html><head></head><body>
		<div><h1 id="_idParaDest-163"><em class="italics"><a id="_idTextAnchor185"/>Chapter 7</em></h1>
		</div>
		<div><h1 id="_idParaDest-164"><a id="_idTextAnchor186"/>Reproducibility in Big Data Analysis</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Implement the concept of reproducibility with Jupyter notebooks</li>
				<li class="bullets">Perform data gathering in a reproducible way</li>
				<li class="bullets">Implement suitable code practices and standards to keep analysis reproducible</li>
				<li class="bullets">Avoid the duplication of work with IPython scripts</li>
			</ul>
			<p>In this chapter, we will discover how reproducibility plays a vital role in big data analysis.</p>
		</div>
		<div><h2 id="_idParaDest-165"><a id="_idTextAnchor187"/>Introduction</h2>
			<p>In the previous chapter, we learned how to define a business problem from a data science perspective through a very structured approach, which included how to identify and understand business requirements, an approach to solutioning it, and how to build data pipelines and carry out analysis.</p>
			<p>In this chapter, we will look at the reproducibility of computational work and research practices, which is a major challenge faced today across the industry, as well as by academicsâ€”especially in data science work, in which most of the data, complete datasets, and associated workflow cannot be accessed completely.</p>
			<p>Today, most research and technical papers conclude with the approach used on the sample data, a brief mention of the methodology used, and a theoretical approach to a solution. Most of these works lack detailed calculations and step-by-step approaches. This is a very limited amount of knowledge for anyone reading it to be able to reproduce the same work that was carried out. This is the basic objective of reproducible coding, where ease of reproducing the code is key. </p>
			<p>There have been advancements in notebooks in general, which can include text elements for commenting in detail; this improves the reproduction process. This is where Jupyter as a notebook is gaining traction within the data science and research communities.</p>
			<p>Jupyter was developed with the intention of being open source software with open standards and services for interactive computing across dozens of programming languages, including Python, Spark, and R.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor188"/>Reproducibility with Jupyter Notebooks</h2>
			<p>Let's start by learning what it is meant by <strong class="bold">computational reproducibility</strong>. Research, solutions, prototypes, and even a simple algorithm that is developed is said to be reproducible if access is provided to the original source code that was used to develop the solution, and the data that was used to build any related software should be able to produce the same results. However, today, the scientific community is experiencing some challenges in reproducing work developed previously by peers. This is mainly due to the lack of documentation and difficulty in understanding process workflows.</p>
			<p>The impact of a lack of documentation can be seen at every level, right from understanding the approach to the code level. Jupyter is one of the best tools for improvising this process, for better reproducibility, and for the reuse of developed code. This includes not just understanding what each line or snippet of code does, but also understanding and visualizing data.</p>
			<h4>Note</h4>
			<p class="callout">Jon Claerbout, who is considered the father of reproducible computational research, in the early 1990s required his students to develop research work and create results that could be regenerated in a single click. He believed that work that was completed took effort and tim, and should be left as it was so that further work on it could be done by reusing the earlier work without any difficulties. On a macro level, an economy's growth is strongly determined by the amount of innovation. The reproducibility of earlier work or research contributes to overall innovation growth.</p>
			<p>Now let's see how we can maintain effective computational reproducibility using the Jupyter notebook.</p>
			<p>The following pointers are broad ways to achieve reproducibility using a Jupyter notebook in Python:</p>
			<ul>
				<li>Provide a detailed introduction to the business problem</li>
				<li>Document the approach and workflow</li>
				<li>Explain the data pipelines</li>
				<li>Explain the dependencies</li>
				<li>Use source code version control</li>
				<li>Modularize the process</li>
			</ul>
			<p>In the following sections, let's explore and discuss the previously mentioned topics in a brief manner.</p>
			<h3 id="_idParaDest-167"><a id="_idTextAnchor189"/>Introduction to the Business Problem</h3>
			<p>One of the key advantages of using the Jupyter notebook is that it includes textual content along with the code to create a workflow.</p>
			<p>We must start with a good introduction to the business problem we have identified, and a summarization of it has to be documented in the Jupyter notebook to provide the gist of the problem. It has to include a problem statement, with the identified business problem from a data science perspective, concluding why we need to carry out this analysis, or what the objective of the process is.</p>
			<h3 id="_idParaDest-168"><a id="_idTextAnchor190"/>Documenting the Approach and Workflows</h3>
			<p>In data science, there can be a lot of back and forth in computational work, such as, for instance, the explorations that are carried out, the kind of algorithms used, and the parameter changes to tune.</p>
			<p>Once changes to the approach have been finalized, those changes need to be documented to avoid work being repeated. Documenting the approach and workflows helps to set up a process. Adding comments to code while developing is a must and this has to be a continuous practice rather than waiting until the end or the results to add comments. By the end of the process, you may have forgotten the details, and this could result in miscalculating the effort that goes into it. The advantages of maintaining the Jupyter notebook with good documentation are the following:</p>
			<ul>
				<li>Tracking the development effort</li>
				<li>Self-explanatory code with comments for each process</li>
				<li>A better understanding of the code workflow and the results of each step</li>
				<li>Avoiding back-and-forth work by making previous code snippets for specific tasks easy to find</li>
				<li>Avoiding duplicating work by understanding the repeated use of code</li>
				<li>Ease of knowledge transfer </li>
			</ul>
			<h3 id="_idParaDest-169"><a id="_idTextAnchor191"/>Explaining the Data Pipeline</h3>
			<p>The data for identifying and quantifying the problem can be generated from multiple data sources, databases, legacy systems, real-time data sources, and so on. The data scientist involved in this closely works with the data management teams of the client to extract and gather the required data and ingest it into the analytical tools for further analysis, and creates a strong data pipeline to acquire this data.</p>
			<p>It is important to document the data sources in detail (covered in the previous chapter) to maintain a data dictionary that explains the variables that are considered, why they are considered, what kind of data we have (structured or unstructured), and the type of data that we have; that is, whether we have a time-series, multivariate, or data that needs to be preprocessed and generated from raw sources such as image, text, speech, and so on.</p>
			<h3 id="_idParaDest-170"><a id="_idTextAnchor192"/>Explain the Dependencies</h3>
			<p>Dependencies are the packages and libraries that are available in a tool. For instance, you may use OpenCV (<a href="https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html">https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html</a>), a library in Python for image-related modeling, or you may use an API such as TensorFlow for deep-learning modeling. Here's another example: if you use Matplotlib ( <a href="https://matplotlib.org/">https://matplotlib.org/</a>) for visualization in Python, Matplotlib can be a part of dependencies. On other hand, dependencies can include the hardware and software specifications that are required for an analysis. You can manage your dependencies explicitly from the beginning by employing a tool such as a Conda environment to list all relevant dependencies (covered in previous chapters on dependencies for pandas, NumPy, and so on), including their package/library versions.</p>
			<h3 id="_idParaDest-171">Using <a id="_idTextAnchor193"/>Source Code Version Control</h3>
			<p>Version control is an important aspect when it comes to any kind of computational activity that involves code. When code is being developed, bugs or errors will arise. If previous versions of the code are available, we will then be able to pinpoint when the bug was identified, when it was resolved, and the amount of effort that went into it. This is possible through version control. At times, you may need to revert to older versions, because of scalability, performance, or for some other reasons. Using source code version control tools, you can always easily access previous versions of code.</p>
			<h3 id="_idParaDest-172"><a id="_idTextAnchor194"/>Modularizing the Process</h3>
			<p>Avoiding duplicate code is an effective practice for managing repetitive tasks, for maintaining code, and for debugging. To carry this out efficiently, you must modularize the process.</p>
			<p>Let's understand this in detail. Say you carry out a set of data manipulation processes, where you develop the code to complete a task. Now, suppose you need to use the same code in a later section of the code; you need to do add, copy, or run the same steps again, which is a repetitive task. The input data and variable names can change. To handle this, you can write the earlier steps as a function for a dataset or on a variable and save all such functions as a separate module. You can call it a functions file (for example, <code>functions.py</code>, a Python file).</p>
			<p>In the next section, we will look at this in more detail, particularly with respect to gathering and building an efficient data pipeline in a reproducible way.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor195"/>Gathering Data in a Reproducible Way</h2>
			<p>Once the problem is defined, the first step in an analysis task is gathering data. Data can be extracted from multiple sources: databases, legacy systems, real-time data, external data, and so on. Data sources and the way data can be ingested into the model needs to be documented.</p>
			<p>Let's understand how to use markdown and code block functionalities in the Jupyter notebook. Text can be added to Jupyter notebooks using markdown cells. These texts can be changed to bold or italic, like in any text editor. To change the cell type to markdown, you can use the <strong class="bold">Cell</strong> menu. We will look at the ways you can use various functionalities in markdown and code cells in Jupyter.</p>
			<h3 id="_idParaDest-174"><a id="_idTextAnchor196"/>Functionalities in Markdown and Code Cells</h3>
			<ul>
				<li><strong class="bold">Markdown in Jupyter</strong>: To select the markdown option in <strong class="bold">Jupyter</strong>, click on <strong class="bold">Widgets</strong> and <strong class="bold">Markdown</strong> from the drop-down menu:</li>
			</ul>
			<div><div><img alt="" src="img/C12913_07_01.jpg"/>
				</div>
			</div>
			<h6>Figure 7.1: The markdown option in the Jupyter notebook</h6>
			<ul>
				<li><code>&lt;h1&gt;</code> and <code>&lt;h2&gt;</code> tags:</li>
			</ul>
			<div><div><img alt="Figure 7.2: Heading levels in the Jupyter notebook" src="img/C12913_07_02.jpg"/>
				</div>
			</div>
			<h6>Figure 7.2: Heading levels in the Jupyter notebook</h6>
			<ul>
				<li><strong class="bold">Text in Jupyter</strong>: To add text just the way it is, we do not add any tags to it:<div><img alt="Figure 7.3: Using normal text in the Jupyter notebook" src="img/Image36337.jpg"/></div></li>
			</ul>
			<h6>Figure 7.3: Using normal text in the Jupyter notebook</h6>
			<ul>
				<li><code>**</code>) to the start and end of the text, for example, **Bold**:</li>
			</ul>
			<div><div><img alt="Figure 7.4: Using bold text in the Jupyter notebook" src="img/C12913_07_04.jpg"/>
				</div>
			</div>
			<h6>Figure 7.4: Using bold text in the Jupyter notebook</h6>
			<ul>
				<li><code>*</code>) to the start and end of the text:</li>
			</ul>
			<div><div><img alt="Figure 7.5: Using italicized text in the Jupyter notebook" src="img/C12913_07_05.jpg"/>
				</div>
			</div>
			<h6>Figure 7.5: Using italicized text in the Jupyter notebook</h6>
			<ul>
				<li><strong class="bold">Code in Jupyter</strong>: To make text appear as code, select the <strong class="bold">Code</strong> option from the dropdown:</li>
			</ul>
			<div><div><img alt="Figure 7.6: Code in the Jupyter notebook" src="img/C12913_07_06.jpg"/>
				</div>
			</div>
			<h6>Figure 7.6: Code in the Jupyter notebook</h6>
			<h3 id="_idParaDest-175">Explai<a id="_idTextAnchor197"/>ning the Business Problem in the Markdown</h3>
			<p>Provide a brief introduction to the business problem to understand the objective of the project. The business problem definition is a summarization of the problem statement and includes the way in which the problem can be resolved using a data science algorithm:</p>
			<div><div><img alt="Figure 7.7: Snippet of the problem definition" src="img/C12913_07_07.jpg"/>
				</div>
			</div>
			<h6>Figure 7.7: Snippet of the problem definition</h6>
			<h3 id="_idParaDest-176">Providi<a id="_idTextAnchor198"/>ng a Detailed Introduction to the Data Source</h3>
			<p>The data source needs to be documented properly to understand the data license for reproducibility and for further work. A sample of how the data source can be added is as follows:</p>
			<div><div><img alt="Figure 7.8: Data Source in Jupyter notebook" src="img/C12913_07_08.jpg"/>
				</div>
			</div>
			<h6>Figure 7.8: Data Source in Jupyter notebook</h6>
			<h3 id="_idParaDest-177">Explain <a id="_idTextAnchor199"/>the Data Attributes in the Markdown</h3>
			<p>A data dictionary needs to be maintained to understand the data on an attribute level. This can include defining the attribute with what type of data it is:</p>
			<div><div><img alt="Figure 7.9: Detailed attributes in markdown" src="img/C12913_07_09.jpg"/>
				</div>
			</div>
			<h6>Figure 7.9: Detailed attributes in markdown</h6>
			<p>To unders<a id="_idTextAnchor200"/>tand the data on an attribute level, we can use functions such as <code>info</code> and <code>describe</code>; however, <code>pandas_profiling</code> is a library that provides a lot of descriptive information in one function, from which we can extract the following information:</p>
			<div><div><img alt="Figure 7.10: Profiling report" src="img/C12913_07_10.jpg"/>
				</div>
			</div>
			<h6>Figure 7.10: Profiling report</h6>
			<p>On the DataFrame level, that is, for the overall data, which includes all the columns and rows that are considered:</p>
			<ul>
				<li>Number of variables</li>
				<li>Number of observations</li>
				<li>Total missing (%)</li>
				<li>Total size in memory</li>
				<li>Average record size in memory</li>
				<li>Correlation matrix</li>
				<li>Sample data</li>
			</ul>
			<p>On the attribute level, which is for a specific column, the specifications are as follows:</p>
			<ul>
				<li>Distinct count</li>
				<li>Unique (%)</li>
				<li>Missing (%) </li>
				<li>Missing (n) </li>
				<li>Infinite (%)</li>
				<li>Infinite (n)</li>
				<li>Histogram for distribution</li>
				<li>Extreme values</li>
			</ul>
			<div><div><img alt="Figure 7.11: Data profiling report on the attribute level" src="img/C12913_07_11.jpg"/>
				</div>
			</div>
			<h6>Figure 7.11: Data profiling report on the attribute level</h6>
			<h3 id="_idParaDest-178">Exercise 45:<a id="_idTextAnchor201"/> Performing Data Reproducibility</h3>
			<p>The aim of this exercise is to learn how to develop code for high reproducibility in terms of data understanding. We will be using the UCI Bank and Marketing dataset taken from this link: <a href="https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson07/Dataset/bank/bank.csv">https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson07/Dataset/bank/bank.csv</a>.</p>
			<p>Let's perform the following steps to achieve data reproducibility:</p>
			<ol>
				<li>Add headings and mention the business problem in the notebook using markup:<div><img alt="Figure 7.12: Introduction and business problem" src="img/C12913_07_12.jpg"/></div><h6>Figure 7.12: Introduction and business problem</h6></li>
				<li>Import the required libraries into the Jupyter notebook:<pre>import numpy as np
import pandas as pd
import time
import re
import os
import pandas_profiling</pre></li>
				<li>Now set the working directory, as illustrated in the following command:<pre>os.chdir("/Users/svk/Desktop/packt_exercises")</pre></li>
				<li>Import and read the input dataset as <code>df</code> using pandas' <code>read_csv</code> function from the dataset:<pre>df = pd.read_csv('bank.csv', sep=';')</pre></li>
				<li>Now view the first five rows of the dataset using the <code>head</code> function:<pre>df.head(5)</pre><p>The output is as follows:</p><div><img alt="Figure 7.13: Data in the CSV file" src="img/C12913_07_13.jpg"/></div><h6>Figure 7.13: Data in the CSV file</h6></li>
				<li>Add the <strong class="bold">Data Dictionary</strong> and <strong class="bold">Data Understanding</strong> sections in the Jupyter notebook:<div><img alt="Figure 7.14: Data Dictionary" src="img/C12913_07_14.jpg"/></div><h6>Figure 7.14: Data Dictionary</h6><p>The <strong class="bold">Data Understanding</strong> section is as follows:</p><div><img alt="Figure 7.15: Data Understanding" src="img/C12913_07_15.jpg"/></div><h6>Figure 7.15: Data Understanding</h6></li>
				<li>To understand the data specifications, use pandas profiling to generate the descriptive information:<pre>pandas_profiling.ProfileReport(df)</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img alt="Figure 7.16: Summary of the specifications with respect to data" src="img/C12913_07_16.jpg"/>
				</div>
			</div>
			<h6>Figure 7.16: Summary of the spe<a id="_idTextAnchor202"/>cifications with respect to data</h6>
			<h4>Instructor Note:</h4>
			<p class="callout">This exercise identifies how a Jupyter notebook is created and includes how to develop a reproducible Jupyter notebook for a bank marketing problem. This must include a good introduction to the business problem, data, data types, data sources, and so on.</p>
			<h2 id="_idParaDest-179">Code Practices a<a id="_idTextAnchor203"/>nd Standards</h2>
			<p>Writing code with a set of practices and standards is important for code reproducibility, as is explaining the workflow of the process descriptively in a step-wise manner. </p>
			<p>This is universally applicable across any coding tool that you may use, not just with Jupyter. Some coding practices and standards should be followed strictly and a few of these will be discussed in the next section.</p>
			<h3 id="_idParaDest-180">Environment Docu<a id="_idTextAnchor204"/>mentation</h3>
			<p>For installation purposes, you should maintain a snippet of code to install the necessary packages and libraries. The following practices help with code reproducibility:</p>
			<ul>
				<li>Include the versions used for libraries/packages.</li>
				<li>Download the original version of packages/libraries used and call the packages internally for installation in a new setup.</li>
				<li>Effective implementation by running it in a script that automatically installs dependencies.</li>
			</ul>
			<h3 id="_idParaDest-181">Writing Readable<a id="_idTextAnchor205"/> Code with Comments</h3>
			<p>Code commenting is an important aspect. Apart from the markdown option available on Jupyter, we must include comments for each code snippet. At times, we make changes in the code in a way that may not be used immediately but will be required for later steps. For instance, we can create an object that may not be used immediately for the next step but for later steps. This can confuse a new user in terms of understanding the flow. Commenting such specifics is crucial.</p>
			<p>When we use a specific method, we must provide a reason for using that particular method. For example, let's say, for the transformation of data for normal distribution, you can use <code>box-cox</code> or <code>yeo-johnson</code>. If there are negative values, you may prefer <code>yeo-johnson</code>, as it can handle negative values. It needs to be commented as in the following example:</p>
			<div><div><img alt="Figure 7.17: Comments with reasons" src="img/C12913_07_17.jpg"/>
				</div>
			</div>
			<h6>Figure 7.17: Comments with reasons</h6>
			<p>We should also follow a good practice for naming the objects that we create. For example, you can name raw data <code>raw_data</code>, and can do the same for model data, preprocessed data, analysis data, and so on. The same goes when creating objects such as models and methods, for example, we can call power transformations <code>pt</code>.</p>
			<h3 id="_idParaDest-182">Effective Segment<a id="_idTextAnchor206"/>ation of Workflows</h3>
			<p>When developing code, there are steps that you design to achieve end results. Each step can be part of a process. For instance, reading data, understanding data, carrying out various transformations, or building a model. Each of these steps, needs to be clearly separated for multiple reasons; firstly, code readability for how each stage is carried out and, secondly, how the result is generated at each stage.</p>
			<p>For example, here, we are looking at two sets of activities. One where the loop is generated to identify the columns that need to be normalized, and the second generating the columns that do not need to be normalized using the previous output:</p>
			<div><div><img alt="Figure 7.18: Effective segmentation of workflows" src="img/C12913_07_18.jpg"/>
				</div>
			</div>
			<h6>Figure 7.18: Effective segmentation of workflows</h6>
			<h3 id="_idParaDest-183">Workflow Documenta<a id="_idTextAnchor207"/>tion</h3>
			<p>When products and solutions are developed, they are mostly developed, monitored, deployed, and tested in a sandbox environment. To ensure a smooth deployment process in a new environment, we must provide sufficient support documents for technical, as well as non-technical, users. Workflow documentation includes requirements and design documents, product documentation, methodology documentation, installation guides, software user manuals, hardware and software requirements, troubleshooting management, and test documents. These are mostly required for a product or a solution development. We cannot just hand over a bunch of code to a client/user and ask them to get it running. Workflow documentation helps during the deployment and integration stages in a client/user environment, which is highly important for code reproducibility.</p>
			<p>At a high level, data science project documentation can be divided into two segments:</p>
			<ul>
				<li>Product documentation</li>
				<li>Methodology documentation</li>
			</ul>
			<p>Product documentation provides information on how each functionality is used in the UI/UX and the application of it. Product documentation can be further segmented into:</p>
			<ul>
				<li>Installation guides</li>
				<li>Software design and user manual</li>
				<li>Test documents</li>
				<li>Troubleshooting management</li>
			</ul>
			<p>Methodology documentation provides information on the algorithms that are used, the methods, the solution approach, and so on.</p>
			<h3 id="_idParaDest-184"><a id="_idTextAnchor208"/>Exercise 46: Missing Value Preprocessing with High Reproducibility</h3>
			<p>The aim of this exercise is to learn how to develop code for high reproducibility in terms of missing value treatment preprocessing. We will be using the UCI Bank and Marketing dataset taken from this link: <a href="https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson07/Dataset/bank/bank.csv">https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson07/Dataset/bank/bank.csv</a>.</p>
			<p>Perform the following steps to find the missing value preprocessing reproducibility:</p>
			<ol>
				<li value="1">Import the required libraries and packages in the Jupyter notebook, as illustrated here:<pre>import numpy as np
import pandas as pd
import collections
import random</pre></li>
				<li>Set the working directory of your choice as illustrated here:<pre>os.chdir("/Users/svk/Desktop/packt_exercises")</pre></li>
				<li>Import the dataset from <code>back.csv</code> to the Spark object using the <code>read_csv</code> function, as illustrated here:<pre>df = pd.read_csv('bank.csv', sep=';')</pre></li>
				<li>Now, view the first five rows of the dataset using the head function:<pre>df.head(5)</pre><p>The output is as follows:</p><div><img alt="Figure 7.19: Bank dataset" src="img/C12913_07_19.jpg"/></div><h6>Figure 7.19: Bank dataset</h6><p>As the dataset has no missing values, we have to introduce some into the dataset.</p></li>
				<li>First, set the loop parameters, as illustrated here:<pre>replaced = collections.defaultdict(set)
ix = [(row, col) for row in range(df.shape[0]) for col in range(df.shape[1])]
random.shuffle(ix)
to_replace = int(round(.1*len(ix)))</pre></li>
				<li>Create a <code>for</code> loop for generating missing values:<pre>for row, col in ix:
    if len(replaced[row]) &lt; df.shape[1] - 1:
        df.iloc[row, col] = np.nan
        to_replace -= 1
        replaced[row].add(col)
        if to_replace == 0:
            break</pre></li>
				<li>Use the following command to identify the missing values in the data by looking into each column's missing values:<pre>print(df.isna().sum())</pre><p>The output is as follows:</p><div><img alt="" src="img/C12913_07_20.jpg"/></div><h6>Figure 7.20: Find the missing values</h6></li>
				<li>Define the range of <strong class="bold">Interquartile Ranges</strong> (<strong class="bold">IQRs</strong>) and apply them to the dataset to identify the outliers:<pre>num = df._get_numeric_data()
Q1 = num.quantile(0.25)
Q3 = num.quantile(0.75)
IQR = Q3 - Q1
print(num &lt; (Q1 - 1.5 * IQR))
print(num &gt; (Q3 + 1.5 * IQR))</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img alt="Figure 7.21: Identifying outliers" src="img/C12913_07_21.jpg"/>
				</div>
			</div>
			<h6>Figure 7.21: Identifying outliers</h6>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor209"/>Avoiding Repetition</h2>
			<p>We all know that the duplication or repetition of code is not a good practice. It becomes difficult to handle bugs, and the length of code increases. Different versions of the same code can lead to difficulty after a point, in terms of understanding which version is correct. For debugging, a change in one position needs to be reflected across the code. To avoid bad practices and write and maintain high-level code, let's learn about some best practices in the following sections.</p>
			<h3 id="_idParaDest-186"><a id="_idTextAnchor210"/>Using Functions and Loops for Optimizing Code</h3>
			<p>A function confines a task which requires a set of steps that from a single of multiple inputs to single or multiple outputs and loops are used for repetitive tasks on the same block of code for a different set of sample or subsetted data. Functions can be written for a single variable, multiple variables, a DataFrame, or a multiple set of parameter inputs. </p>
			<p>For example, let's say you need to carry out some kind of transformation for only numeric variables in a DataFrame or matrix. A function can be written for a single variable and it can be applied to all numeric columns, or it can be written for a DataFrame, where the function identifies the set of numeric variables and applies them to generate the output. Once a function is written, it can be applied any future similar application in the proceeding code. This reduces duplicate work.</p>
			<p>The following are the challenges that need to be considered when writing a function:</p>
			<ul>
				<li><strong class="bold">Internal parameter changes</strong>: There can be changes in the input parameter from one task to another. This is a common challenge. To handle this, you can mention the dynamic variables or objects in the function inputs when defining the inputs for a function.</li>
				<li><strong class="bold">Variations in the calculation process for a future task</strong>: Write a function with internal functions that will not require many changes if any variations need to be captured. This way, rewriting the function for a new kind of task will be easy.</li>
				<li><strong class="bold">Avoiding loops in functions</strong>: If a process needs to be carried out across many subsets of the data by row, functions can be directly applied in each loop. This way, your function will be not be constrained by repetitive blocks of code on the same data.</li>
				<li><strong class="bold">Handing changes in data type changes</strong>: The return object in a function can be different for different tasks. Depending on the task, the return object can be converted to other data classes or data types as required. However, input data classes or data types can change from task to task. To handle this, you need to clearly provide comments to understand the inputs for a function.</li>
				<li><strong class="bold">Writing optimized functions</strong>: Arrays are efficient when it comes to repetitive tasks such as loops or functions. In Python, using NumPy arrays generates very efficient data processing for most arithmetic operations.</li>
			</ul>
			<h3 id="_idParaDest-187"><a id="_idTextAnchor211"/>Developing Libraries/Packages for Code/Algorithm Reuse</h3>
			<p>Packages or libraries encapsulate a collection of modules. They are highly dependable for code reproducibility and the modules that are generated. There are thousands of packages/libraries that are generated daily by developers and researchers around the globe. You can follow the package developing instructions from the Python project packaging guide for developing a new package (<a href="https://packaging.python.org/tutorials/packaging-projects/">https://packaging.python.org/tutorials/packaging-projects/</a>). This tutorial will provide you with information on how to upload and distribute your package publicly as well as for internal use.</p>
			<h3 id="_idParaDest-188"><a id="_idTextAnchor212"/>Activity 14: Carry normalisation of data</h3>
			<p>The aim of this activity is to apply various preprocessing techniques that were learned in previous exercises and to develop a model using preprocessed data.</p>
			<p>Now let's perform the following steps:</p>
			<ol>
				<li value="1">Import the required libraries and read the data from the <code>bank.csv</code> file.</li>
				<li>Import the dataset and read the CSV file into the Spark object.<p>Check the normality of the dataâ€”the following step is to identify the normality of data.</p></li>
				<li>Segment the data numerically and categorically and perform distribution transformation on the numeric data.</li>
				<li>Create a <code>for</code> loop that loops through each column to carry out a normality test to detect the normal distribution of data.</li>
				<li>Create a power transformer. A power transformer will transform the data from a non-normal distribution to a normal distribution. The model developed will be used to transform the previously identified columns, which are non-normal.</li>
				<li>Apply the created power transformer model to the non-normal data.</li>
				<li>To develop a model, first split the data into training and testing for cross-validation, train the model, then predict the model in test data for cross-validation. Finally, generate a confusion matrix for cross-validation.<h4>Note</h4><p class="callout">The solution of this activity can be found on page 240.</p></li>
			</ol>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor213"/>Summary</h2>
			<p>In this chapter, we have learned how to maintain code reproducibility from a data science perspective through structured standards and practices to avoid duplicate work using the Jupyter notebook.</p>
			<p>We started by gaining an understanding of what reproducibility is and how it impacts research and data science work. We looked into areas where we can improve code reproducibility, particularly looking at how we can maintain effective coding standards in terms of data reproducibility. Following that, we looked at important coding standards and practices to avoid duplicate work using the effective management of code through the segmentation of workflows, by developing functions for all key tasks, and how we can generalize coding to create libraries and packages from a reusability standpoint. </p>
			<p>In the next chapter, we will learn how to use all the functionalities we have learned about so far to generate a full analysis report. We will also learn how to use various PySpark functionalities for SQL operations and how to develop various visualization plots.</p>
		</div>
	</body></html>