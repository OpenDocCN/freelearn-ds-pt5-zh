<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">What's Next?</h1>
                </header>
            
            <article>
                
<p>We have come a long way. We started this book with the basics of data science and its applications in marketing and worked through numerous use cases of data science in marketing. Along the way, we have conducted descriptive analysis, where we used data science techniques to analyze and visualize data to identify patterns. We have also conducted explanatory analysis, where we used machine learning models to draw insights from data, such as finding the drivers behind certain customers' activities and the correlations between customer attributes and their actions. Lastly, we have also looked at predictive analytics, where we trained various machine learning algorithms to make forecasts on certain actions of customers.</p>
<p>The topics we have covered throughout this book are not trivial and were geared toward the practical usage of data science in marketing. Each chapter was meant to showcase how you can use data science and machine learning techniques in actual marketing use cases and guide you through how you might be able to apply the concepts discussed to your specific business cases. As the field of marketing analytics is growing and broadening its reach, we wanted to use this chapter to inform you of some potential challenges you might face and look at some other commonly used technologies, as well as review the topics that we have discussed in this book.</p>
<p>I<span>n this chapte</span><span>r, we will cover the following topics:</span></p>
<ul>
<li>Recap of the topics covered in this book</li>
<li>Real-life data science challenges</li>
<li>More machine learning models and packages</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recap of the topics covered in this book</h1>
                </header>
            
            <article>
                
<p>We have covered a large amount of material from the beginning of this book, from discussing the trends in marketing and how data science and machine learning have become a crucial part in building marketing strategies, to building various predictive machine learning models for more efficient marketing. It is worth reviewing what we have covered so far and refreshing our memory before we close this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Trends in marketing</h1>
                </header>
            
            <article>
                
<p>As you may recall, the first thing we discussed in <a href="c169428b-e0db-4624-896c-24316e9b29cc.xhtml">Chapter 1</a>, <em>Data Science and Marketing</em>, was the recent trends in marketing. It is important to try to understand and keep up with the trends that are occurring in the industry that you are working and specializing in. Especially in marketing, there is a lot of <span>demand for</span> more data-driven and quantitative marketing, and for the use of the latest and most intelligent technologies for developing more cost-effective marketing strategies.</p>
<p><span>According to the February, 2018, CMO survey (<a href="https://www.forbes.com/sites/christinemoorman/2018/02/27/marketing-analytics-and-marketing-technology-trends-to-watch/#4ec8a8431b8a">https://www.forbes.com/sites/christinemoorman/2018/02/27/marketing-analytics-and-marketing-technology-trends-to-watch/#4ec8a8431b8a</a>), reliance on marketing analytics has gone up from 30% to 42% in the past 5 years. The three main trends in marketing that can be easily observed are the following:</span></p>
<ul>
<li><strong>Rising importance of digital marketing</strong>: Lots of marketing activities are now happening more heavily on digital channels, such as search engines, social media, email, and websites, rather than on more traditional mass media, such as TV, radio, and banners at bus stations. As various digital marketing channels are gaining popularity as the choice of marketing channel, it has become more important to have a good understanding of how audience targeting works on social networks, such as Facebook and Instagram, or how to place advertisements on search engines and video streaming services, such as Google and YouTube.</li>
<li><strong>Marketing analytics</strong>: Marketing analytics is a way of monitoring and quantifying the results and performances of past marketing efforts. In <a href="1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml">Chapter 2</a>, <em>Key Performance Indicators and Visualizations</em>, we learned about various <strong>key performance indicators</strong> (<strong>KPIs</strong>) that we can use to track and quantify the returns from various marketing efforts. Marketing analytics does not just stop at analyzing KPIs. It can also be applied to product and customer analytics, which we discussed in <a href="73a716c6-6a84-4785-b04e-87651d0a29d1.xhtml" target="_blank">Chapter 5</a>, <em>Product Analytics</em>, and <a href="72e8f4ee-7f95-4acc-928d-d33c9fc31bd6.xhtml" target="_blank">Chapter 7</a>, <em>Exploratory Analysis for Customer Behavior</em>.</li>
</ul>
<ul>
<li><strong>Personalized and target marketing</strong>: As the accessibility of data science and machine learning has become easier, another trend in marketing has arisen: individual-level targeted marketing. Using predictive analytics, we can now predict what types of products that individual customers would like, which we have discussed in <a href="d3ba7047-2873-4b03-9a44-4c1d55b84178.xhtml">Chapter 6</a>, <em>Recommending the Right Products</em>. We have also seen how we can target those customers who are likely to churn by building predictive machine learning models in <a href="3d5c7798-6874-40e9-b7e9-6fe39592cc2a.xhtml">Chapter 11</a>, <em>Retaining Customers</em>. <span>As targeted marketing results in higher ROI, there are many <strong>software-as-a-Service</strong> (<strong>SaaS</strong>) companies, such as Sailthru and Oracle, that provide platforms for personalized and target marketing.</span></li>
</ul>
<p>As new strategies and technologies are developed, trends are destined to change. The trends that we have discussed in this book might not be applicable in 20-30 years, time. As a marketing professional, it is critical to follow and understand what others in the same industry do and what other approaches or technologies are being developed and used to achieve higher ROI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data science workflow</h1>
                </header>
            
            <article>
                
<p>As a marketing professional or an aspiring data scientist in marketing, it can be challenging to figure out where to start for a data science project. In <a href="c169428b-e0db-4624-896c-24316e9b29cc.xhtml">Chapter 1</a>, <em>Data Science and Marketing</em>, we have discussed a typical workflow for a data science project. It is worth reviewing the steps before you embark on your future marketing data science projects. You should be familiar with the following workflow diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c76e5d08-c952-4db3-bd8a-7321b80ba75d.png" style="width:13.33em;height:20.42em;"/></p>
<p>Let's talk a bit more in detail about these six steps:</p>
<ol>
<li><strong>Problem definition</strong>: Any data science and machine learning project should have a clear problem definition. You will need to have an in-depth understanding of the problem itself, the scope of the project, and approaches to coming up with solutions. This is where you brainstorm what types of analyses and data science techniques to use.</li>
<li><strong>Data collection</strong>: As it is for any data science project, having data is key for success. In this step, you will need to gather all the required data for your data science project. It is common that you will need to implement data collection processes for internal data, purchase third-party data, or scrape data from different websites. Depending on the cases, the data collection step can be trivial or it can also be tedious.</li>
<li><strong>Data preparation</strong>: With the data from the data collection step, the next step is to clean and prepare the data. As we have seen throughout this book, our programming exercises always started with data cleanup and preparation. In the data preparation step, we handled missing values, encoded categorical variables, or transformed other variables, so that this data can be understood by machine learning algorithms.</li>
<li><strong>Data analysis</strong><span>: As you may recall, we have discovered useful insights from this data analysis step in our programming exercises throughout the book. Through analyzing data, we gain a better understanding of the overall distributions of different variables, and it is often a good idea to visualize data with different plots to identify any noticeable patterns.</span></li>
<li><span><strong>Feature engineering</strong>: As we have seen and discussed throughout the book, there are many different ways to approach engineering the features for machine learning models. For monetary values, we have applied log transformations. In some cases, we have normalized the data so that the variables are on the same scale. We have also used one-hot encoding to encode categorical variables. Feature engineering is one of the most important steps in building machine learning models, as the algorithms are going to try to learn from these features to correctly predict the target.</span></li>
<li><strong>Model building</strong>: The final step in a typical data science workflow is, of course, model building. With the clean data and features that we have built from previous steps, this is where you train your machine learning models. Throughout this book, we have discussed how to evaluate the models. For classification models, we have often used accuracy, precision, recall, the ROC curve, and the AUC. For regression models, we have used MSE, <em>R</em><sup>2</sup>, or a scatterplot of predicted and actual values for model evaluations.</li>
</ol>
<p>During our programming exercises, our workflow looked almost the same as the workflow that we have just discussed. When unsure about what to do next, we hope this workflow diagram gives you some hints on the next steps.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning models</h1>
                </header>
            
            <article>
                
<p>As you may recall, we built a number of machine learning models in this book. For example, in <a href="4f5163a1-c34a-495f-bc5f-e02f9b2a2052.xhtml" target="_blank"/><a href="4f5163a1-c34a-495f-bc5f-e02f9b2a2052.xhtml" target="_blank">Chapter 8</a>, <em>Predicting the Likelihood of Marketing Engagement</em>, we trained a random forest model to predict how likely each customer is to engage with marketing calls. In <a href="3d5c7798-6874-40e9-b7e9-6fe39592cc2a.xhtml">Chapter 11</a>, <em>Retaining Customers</em>, we used an <strong>artificial neural network</strong> (<strong>ANN</strong>) model to identify which customers are likely to churn from the business. In this section, we will review those machine learning models that we have used in this book:</p>
<ul>
<li><strong>Logistic regression</strong>: In <a href="ce2c2775-9817-4b18-972c-db8e8c629b74.xhtml">Chapter 3</a>, <em>Drivers behind Marketing Engagement</em>, we have used a logistic regression model to extract the insights on which factors make customers more likely to engage with marketing campaigns. In Python, we used the <kbd>statsmodels</kbd> package to build a logistic regression model, and the code to train a logistic regression model looked like the following:</li>
</ul>
<pre>        import statsmodels.formula.api as sm<br/><br/>        logit = sm.Logit(<br/>            target_variable, <br/>            features<br/>        )<br/> <br/>        logit = logit.fit()</pre>
<p style="padding-left: 60px">From this trained model, we could look at the details and correlations between the features and the target variable by running <kbd>logit_fit.summary()</kbd>. On the other hand, in R, we used the following command to train a logistic regression model:</p>
<pre>        logit.fit &lt;- glm(Target ~ ., data = DF, family = binomial)</pre>
<p style="padding-left: 60px">Similar to how we used the <kbd>summary</kbd> function in Python, we could run the <kbd>summary(logit.fit)</kbd> command to get the details of the logistic regression fit and the correlations between the features and the target variable.</p>
<ul>
<li><strong>Random forest</strong>: As you may recall, we used a random forest algorithm in <a href="4f5163a1-c34a-495f-bc5f-e02f9b2a2052.xhtml" target="_blank">Chapter 8</a>, <em>Predicting the Likelihood of Marketing Engagement</em>, to predict which customers are likely to respond to marketing calls. In Python, we used the <kbd>scikit-learn</kbd> package to build random forest models. The code to train a random forest model looked like the following:</li>
</ul>
<pre>        from sklearn.ensemble import RandomForestClassifier<br/><br/>        rf_model = RandomForestClassifier()<br/><br/>        rf_model.fit(X=x_train, y=y_train)</pre>
<p style="padding-left: 60px"><br/>
As you may recall, there were numerous hyperparameters you could tune with the random forest algorithm. We have discussed how you can fine-tune the number of estimators in the forest, <kbd>n_estimators</kbd>, the maximum depth of the tree, <kbd>max_depth</kbd>, and the minimum of samples needed to be able to split into branches, <kbd>min_samples_split</kbd>. On the other hand, in R, we used the <kbd>randomForest</kbd> library to build random forest models. The code for training a random forest model in R looked like the following:</p>
<pre>        library(randomForest)<br/><br/>        rfModel &lt;- randomForest(x=trainX, y=factor(trainY))</pre>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">With the <kbd>randomForest</kbd> package, you could fine-tune the hyperparameters. You could use <kbd>ntree</kbd> to tune the number of trees in the forest, <kbd>sampsize</kbd> to tune the size of the sample to draw for training each tree, and <kbd>maxnodes</kbd> to define the maximum number of terminal nodes in the tree.</p>
<ul>
<li><strong>ANN</strong>: As you may recall, in <a href="3d5c7798-6874-40e9-b7e9-6fe39592cc2a.xhtml">Chapter 11</a>, <em>Retaining Customers</em>, we used an ANN model to predict the customers who are likely to churn from the business. In order to build an ANN model, we used the <kbd>keras</kbd> package for both Python and R. In Python, training an ANN model looked like the following:</li>
</ul>
<pre>        from keras.models import Sequential<br/>        from keras.layers import Dense<br/><br/>        model = Sequential()<br/>        model.add(Dense(16, input_dim=<br/>                        len(features), activation='relu'))<br/>        model.add(Dense(8, activation='relu'))<br/>        model.add(Dense(1, activation='sigmoid'))<br/><br/><br/>        model.compile(loss='binary_crossentropy', <br/>                      optimizer='adam', metrics=['accuracy'])<br/><br/>        model.fit(X_train, y_train, epochs=50, batch_size=100)</pre>
<p style="padding-left: 60px"><br/>
As you should know already, we first had to add input, hidden, and output layers to the model. Then, we could compile and train an ANN model. In R, the concept is the same, but the syntax looks a bit different. The R code to train an ANN model using the <kbd>keras</kbd> package looked like the following:</p>
<pre>        library(keras)<br/><br/>        model &lt;- keras_model_sequential() <br/>        model %&gt;% <br/>          layer_dense(units = 16, kernel_initializer =<br/>          "uniform", activation = 'relu', input_shape=ncol(train)-1) %&gt;% <br/>          layer_dense(units = 8, kernel_initializer = <br/>          "uniform", activation = 'relu') %&gt;%<br/>          layer_dense(units = 1, kernel_initializer =<br/>                      "uniform", activation = 'sigmoid') %&gt;% <br/>          compile(optimizer = 'adam',<br/>            loss = 'binary_crossentropy',<br/>            metrics = c('accuracy')<br/>          )<br/><br/>        history &lt;- model %&gt;% fit(<br/>          trainX, <br/>          trainY, <br/>          epochs = 50, <br/>          batch_size = 100, <br/>          validation_split = 0.2<br/>        )</pre>
<ul>
<li><strong>k-means clustering</strong>: In <a href="5955002d-2a75-4d5a-aa6a-86710a3bf00e.xhtml">Chapter 10</a>, <em>Data-Driven Customer Segmentation</em>, we used a k-means clustering algorithm to programmatically build different customer segments. We have seen how analyzing the attributes of these different customer segments can help us understand the different behaviors of the customers and find better ways to target different groups of customers. In Python, we could use the <kbd>scikit-learn</kbd> package to build a k-means clustering algorithm. The code looked like the following:</li>
</ul>
<pre>        from sklearn.cluster import KMeans<br/><br/>        kmeans = KMeans(n_clusters=4)<br/>        kmeans = kmeans.fit(data)</pre>
<p style="padding-left: 60px">As you may recall, you needed to define the number of clusters you would like to build from the data, using the <kbd>n_clusters</kbd> parameter. In order to get the cluster labels for each record and cluster centroids, we could use <kbd>kmeans.labels_</kbd> and <kbd>kmeans.cluster_centers_</kbd>. Similarly, in R, we used the <kbd>kmeans</kbd> function to build a clustering model, as shown in the following code:</p>
<pre>        cluster &lt;- kmeans(data, 4)</pre>
<p style="padding-left: 60px">In order to get the labels and cluster centroids, we could use <kbd>cluster$cluster</kbd> and <kbd>cluster$centers</kbd>.</p>
<p style="padding-left: 60px">With these algorithms, we were able to easily build various machine learning models for different use cases in marketing. We hope these brief reviews of the syntax of building these machine learning models helped to refresh your memory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-life data science challenges</h1>
                </header>
            
            <article>
                
<p>Applying data science and machine learning in marketing would be all glamorous and flawless if we were able to just build and use various machine learning models for different marketing use cases. However, that normally is not the case. Quite often, the end-to-end machine learning model building process can be tedious, with lots of barriers and bottlenecks on the way. We are going to discuss some of the most frequently appearing data science challenges in real life, including the following:</p>
<ul>
<li>Challenges in data</li>
<li>Challenges in infrastructure</li>
<li>Challenges in choosing the right model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Challenges in data</h1>
                </header>
            
            <article>
                
<p>One of the most challenging factors in using data science and machine learning for marketing is getting the right data. As may sound obvious to you, without data, there is no data science or machine learning. Moreover, if the quality of the data is not good, then the quality of your trained machine learning is also going to be bad.</p>
<p>In this section, we are going to discuss some of the common challenges that many data scientists face in getting the right data:</p>
<ul>
<li><strong>Existence of data</strong>: Sometimes you may come up with a great idea of applying data science techniques to solve one of the problems you have in marketing. However, the data you need might not even exist. For example, say your idea was to identify trending web content, such as which web pages are viewed the most and liked the most by your users. However, you might not have the page view data, if the web page tracking functionality was not implemented on your websites. In this case, you will need to implement tracking functionality in your websites to track which users viewed or liked which content. Then, it is only possible to work on your idea after some period of time, when you have gathered enough data for your analysis. This type of case happens relatively frequently, so it is critical to have a good understanding of how well you track user activities and which parts you are missing. If possible, obtaining third-party data is also an option, when the data does not exist internally. There are lots of data vendors who sell data that you might need. If using a third-party data vendor is an option, that can be a good solution when there is no data for your project. Also, there is a lot of publicly available data that you can use freely. It is always worthwhile to see whether the data you need is publicly available or not.</li>
<li><strong>Accessibility of data</strong>: Data accessibility can be a barrier for a data science project. Especially in big corporations, access to certain sets of data is strictly restricted to selected subgroups of teams. In this case, even if the required dataset exists, it can be difficult or even impossible for data scientists or marketing professionals to access and use the data. Where the data is being generated from can also cause data accessibility problems. For instance, if the data is streamed into other applications without being stored or archived, then this data can be lost after it has been streamed. The location of the data files can also be a barrier to accessing the data you need. If the data cannot be shared through a network or if you cannot reach the location that the data lives in, then that can also keep you from using this data. This is why the responsibility and importance of data engineering and data engineers is rising. Data engineers work with other data scientists or software developers to specifically work on building data pipelines through which data with accessibility issues can move to other parts of the business. If you are facing issues with data accessibility, it is crucial to first find out what the barrier is and consider working with data engineers to build data pipelines to make the data accessible for your future projects.</li>
</ul>
<p class="mce-root"/>
<ul>
<li><strong>Messy data</strong>: You can assume the majority of the data you will face in real-life data science projects will be messy. It may be in a format that you cannot easily understand. It may be segmented into smaller parts that cannot easily be joined to each other. Or, there may also be too many missing values or too many duplicate records in the data. The degree of messiness of datasets can significantly increase the amount of time you need to spend on cleaning up the raw data and making it usable. Conducting in-depth data analysis on this messy data is crucial in making the data usable for future steps. Sometimes, it may be worthwhile to work with data engineers to fix the source that causes the messiness in the data and make future data more clean.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Challenges in infrastructure</h1>
                </header>
            
            <article>
                
<p>When working with different datasets for applying data science techniques and using machine learning models for different projects in marketing, you may face some challenges in the system infrastructure that you use for developments. Quite often, datasets are too big to fit into your laptop or computer. As the size of data is grows bigger and bigger everyday, it becomes even more likely that sometime in the future, you will have issues with developing data science models on your laptop, even if you currently do not have this problem.</p>
<p>There are two main things that can slow you down when working on data science projects: shortage of CPU or processing power and shortage of RAM or memory. If you do not have enough processing power, your analysis could take long time. Especially when training machine learning models, it is not uncommon for model training to take days, weeks, or even months. On the other hand, if you do not have enough memory, you might end up getting <kbd>Out of Memory</kbd> errors while running your analysis. For example, tree-based models, such as decision trees or random forests, can take a large amount of memory, and training such models can fail after hours of training because of shortage of memory.</p>
<p>With the emerging popularity of and developments in cloud computing, there are solutions to these problems. Using one of the cloud computing service providers, such as AWS, Google, or Microsoft Azure, you can, theoretically, get an unlimited amount of computing power and memory. Of course, everything comes with a price. Running large data science jobs on these cloud platforms can cost a fortune, if you do not plan it right. When working with large datasets, it is wise to consider the amount of processing power and memory you would need to successfully run your tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Challenges in choosing the right model</h1>
                </header>
            
            <article>
                
<p>Choosing a machine learning algorithm for a given data science project is more difficult than it sounds. Some algorithms work more like a black box, where you do not know how an algorithm makes predictions or decisions. For example, it is quite difficult to understand how a trained random forest model makes predictions on the output from the input. The decisions are made from hundreds of different decision trees, where each tree works differently with different decision-making criteria, and this makes it difficult for a data scientist to fully understand what happens in between the input and the output. </p>
<p>On the other hand, linear models, such as logistic regression models, tell us exactly how they are making decisions. Once logistic regression models are trained, we know the coefficients given to each feature, and from these coefficients, we can deduce what the predicted output is going to be. Depending on your use cases, you might need to have this kind of explainability, where you need to be able to explain how each feature works and affects the prediction output to your business partners. Quite often, more advanced models work more like a black box, and you will need to make a trade-off between prediction accuracy and explainability.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">More machine learning models and packages</h1>
                </header>
            
            <article>
                
<p>In this book, we have mainly used the following five machine learning algorithms that fit into and work the best for our marketing use cases: logistic regression, random forests, ANN, k-means clustering, and collaborative filtering. However, there are many more readily available machine learning algorithms that you may find useful for your future data science and machine learning projects. We will be covering some of the other frequently used machine learning algorithms, what packages to use in Python and R, and where to find more information on these algorithms.</p>
<p>Some of the other machine learning algorithms to consider in your future projects are the following:</p>
<ul>
<li><strong>Nearest neighbors</strong>: This is a machine learning algorithm that finds the pre-defined number of closest samples to a new data point. Even though the concept of this algorithm sounds simple, the nearest neighbors algorithm has been used successfully in various areas, including image recognition. In the <kbd>scikit-learn</kbd> package of Python, you can use the <kbd>KNeighborsClassifier</kbd> class in the <span><kbd>neighbors</kbd> module to build classification models, or you can use the <kbd>KNeighborsRegressor</kbd> class to build regression models. For more details on the usage, we recommend you take a look at the following documentation page: <a href="https://scikit-learn.org/stable/modules/neighbors.html">https://scikit-learn.org/stable/modules/neighbors.html</a>. On the other hand, in R, you can use the <kbd>knn</kbd> function in the <kbd>class</kbd> library. For the documentation of this function in R, you can refer to this documentation page: <a href="https://www.rdocumentation.org/packages/class/versions/7.3-15/topics/knn">https://www.rdocumentation.org/packages/class/versions/7.3-15/topics/knn</a>.</span></li>
<li><strong>Support vector machine</strong> (<strong>SVM</strong>): SVM is another machine learning algorithm that you may find useful. The SVM algorithm tries to find a hyperplane that best splits the data into classes or groups. It is especially effective in high-dimensional space. The <kbd>scikit-learn</kbd> package has the <kbd>SVC</kbd> and <kbd>SVR</kbd> classes implemented in Python for classification and regression models. The documentation page can be found at the following link: <a href="https://scikit-learn.org/stable/modules/svm.html">https://scikit-learn.org/stable/modules/svm.html</a>. In R, the <kbd>e1071</kbd> library has the <kbd>svm</kbd> function, which you can use to train SVM models. More documentation on its usage can be found here: <a href="https://www.rdocumentation.org/packages/e1071/versions/1.7-0.1/topics/svm">https://www.rdocumentation.org/packages/e1071/versions/1.7-0.1/topics/svm</a>.</li>
<li><strong>Gradient-boosted trees</strong> (<strong>GBT</strong>): GBT is one of the tree-based machine learning algorithms. Unlike the random forest algorithm, the GBT algorithm learns and trains each tree sequentially, and each tree learns from the mistakes that the previous trees made. It is well known and frequently used for its prediction accuracy and robustness. In Python, you can use the <kbd>GradientBoostingClassifier</kbd> class in the <kbd>scikit-learn</kbd> package's <kbd>ensemble</kbd> module for classification problems and the <kbd>GradientBoostingRegressor</kbd> class for regression problems. More details about GBT in <kbd>scikit-learn</kbd> can be found here: <a href="https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting">https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting</a>. Similarly, in R, the <kbd>gbm</kbd> package has the GBT algorithm implemented for classification and regression problems. You can use the <kbd>gbm</kbd> function within the <kbd>gbm</kbd> package to train a GBT model. More information can be found at the following link: <a href="https://www.rdocumentation.org/packages/gbm/versions/2.1.5/topics/gbm">https://www.rdocumentation.org/packages/gbm/versions/2.1.5/topics/gbm</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we reviewed the topics that we discussed in this book. We briefly went through the trends that are observable in the marketing industry and how data science and machine learning are becoming more and more important in marketing. Then, we reviewed a typical data science workflow, where you start with problem definition, then move onto data collection, preparation, and analysis, and finally move to feature engineering and model building. While working on future data science projects, it will be worthwhile to keep the workflow diagram we looked at in the back of your head and when stuck with what to do next, refer back to this diagram for ideas. We have also shared some of the challenges you might face when working with real-world datasets. The three main challenges we covered were data issues, infrastructure issues, and choosing the right model. More specifically, we discussed the trade-off between explainability and model accuracy. We have suggested some workarounds and solutions to these challenges, so we hope they help when you face similar challenges. Lastly, we have discussed some other frequently used machine learning models that you may find useful in your future projects. We have briefly showed which Python and R packages to use for each of these models and where you can find more information about the usage of those models.</p>
<p>Throughout the 13 chapters in this book, we have covered the various data science and machine learning techniques you can use in marketing, with a focus on practicality. As you have worked through numerous examples for different use cases in marketing throughout this book, we hope you have gained more confidence in applying data science techniques and building machine learning models for developing more intelligent and efficient marketing strategies. We hope your journey throughout this book was worthwhile and rewarding and that you have gained many new and useful skills.</p>


            </article>

            
        </section>
    </body></html>