<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Unsupervised Learning in Anaconda</h1>
                </header>
            
            <article>
                
<p>Before discussing unsupervised learning, it might be a good idea to introduce supervised learning since most of us will be familiar with functions discussed in the previous chapters. For a function of <em>y=f(x)</em>, usually we have values for independent variables of <em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, ... <em>x<sub>n</sub></em> and a set of corresponding values for a dependent variable of <em>y</em>. In previous chapters, we have discussed various types of functions, such as the single-factor linear model. Our task is to figure out the format of the function, given a set of input values. For supervised learning, we have two datasets: the <strong>training data</strong> and <strong>test data</strong>. For the training dataset, it has a set of input variables and related output values (also called a <strong>supervisory signal</strong>). A supervised learning algorithm analyzes the training data and produces an inferred function. Then, we apply this inferred function to map the given test dataset.</p>
<p>Unlike supervised learning, we don't have <em>y</em> for unsupervised learning. In other words, unsupervised learning is used to draw inferences from datasets consisting of input data without labelled responses. In a sense, for supervised learning, we have both <em>x</em> and <em>y</em>, while for unsupervised learning, we have <em>x</em> only. In other words, for one algorithm, we have both input and output, while for the other, we have inputs only. The most common unsupervised learning method is Cluster Analysis, which is used for exploratory data analysis to find hidden patterns or groupings in data.</p>
<p>In this chapter, the following topics will be covered:</p>
<ul>
<li>Introduction to unsupervised learning</li>
<li>Hierarchical clustering</li>
<li>k-means clustering</li>
<li>Introduction to Python packages: <kbd>scipy</kbd>, <kbd>contrastive</kbd>, and <kbd>sklearn</kbd> (scikit-learn)</li>
<li>Introduction to R packages: <kbd>rattle</kbd>, <kbd>randomUniformForest</kbd>, and <kbd>Rmixmod</kbd></li>
<li>Implementation using Julia</li>
<li>Task view for Cluster Analysis</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to unsupervised learning</h1>
                </header>
            
            <article>
                
<p>For unsupervised learning, we try to reorganize data or classify it into different groups based on certain traits or characteristics. For this purpose, we can use certain rules to categorize our dataset. For example, <span>we could classify them into different groups </span><span>based on investors' characteristics, such as age, education level, background, job types, living city, profession, salary level, and house ownership. For instance, they could be classified into four types of investors: aggressive, risk averse, risk neutral, and extremely risk averse. After that, financial institutions could design and market specific financial products targeting different groups.</span></p>
<p>To plan an equitable income tax policy, governments could classify potential taxpayers based on various criteria, such as income level and whether a person has a certain disability. Then, they could design distinct tax plans to target various groups with varying social welfare status. Such a tax policy could be more equitable than a simple progressive tax scheme.</p>
<p>Another example is that investors could have over 5,000 stocks available to buy. It might be a good idea to group them according to various benchmarks, such as riskiness, profitability, familiarity, reputation, local/national/international, social responsibility, and transaction cost. After that, financial institutions could form different mutual funds which could be sold to different investors.</p>
<p>The following diagram is used to summarize the idea: grouping data according to certain criteria. The data is drawn from two normal distributions with the same standard deviation, but with different means. Later in the chapter, we will show the related code:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-789 image-border" src="Images/b4187cf8-03f4-4341-ad53-3f97dded5985.png" style="width:13.50em;height:11.75em;" width="322" height="278"/></div>
<p>The logic for clustering or grouping is that the distance between group members is less than that between groups. In other words, the similarity among members within each group is higher than that between groups. For instance, if we have ZIP codes of many houses, we could estimate the distance between each pair of houses and classify them into different communities. For numerical values, such as <em>(x<sub>1</sub>,y<sub>1</sub>), (x<sub>2</sub>,y<sub>2</sub>),</em> the difference could be defined by the following formula:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/3b10edad-8c57-48b6-b27c-c359b6bec3e9.png" style="width:19.00em;height:1.92em;" width="3760" height="380"/></div>
<p>The following code generates a graph that shows the distance between two points. This is also called Euclidean distance:</p>
<pre>&gt; a&lt;-c(2,5) 
&gt; b&lt;-c(3,7) 
&gt; z&lt;-rbind(a,b) 
&gt; dist(z, method = "euclidean") 
<strong>         a 
b 2.236068</strong> 
&gt; plot(z,lwd=20) </pre>
<p>In the previous code, <kbd>method="euclidean"</kbd> could be omitted since it is the default setting. The distance is <kbd>2.236</kbd> and the graph is ignored for brevity. The potential phrases for the method variable are <kbd>euclidean</kbd>, <em>maximum,</em> <em>manhattan,</em> <em>canberra, binary</em>, and <em>minkowski.</em> The general form is called the Minkowski distance, as shown here:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f61bc53c-d682-49d6-8419-fa2105049b87.png" style="width:22.58em;height:1.50em;" width="3930" height="260"/> </div>
<p>Obviously, when <em>n</em> takes a value of <kbd>2</kbd>, Equation (2) is the same as Equation (1). On the other hand, when <em>p = 1</em>, this is equivalent to the Manhattan distance, as shown in the following:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0c343a8b-dca2-439e-a8da-f0b2f48923e8.png" style="width:20.08em;height:1.33em;" width="3320" height="220"/></div>
<p>When <em>x</em> and <em>y</em> differ in terms of sizes, we could scale them by using the following formulae:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/f63d00d9-12c2-4936-a943-da2fa2912e76.png" style="width:15.25em;height:4.08em;" width="2390" height="650"/> </div>
<p>The previous scaling method is based on the assumption that <em>x</em> and <em>y</em> are independent, where <img src="Images/9dd0a259-0c4d-42a4-ba5d-d3053294379a.png" width="27" height="16"/> (<img src="Images/d7e8ba0b-c561-48da-94cb-a8ce3b7cc2d5.png" width="28" height="16"/>) is the new data point for <em>x (y)</em>, <em><img src="Images/c435e423-edbb-4d77-8d07-2ae9739095ee.png" width="8" height="16"/> (<img src="Images/f052afff-8055-4c10-bf01-ffd5be193128.png" width="8" height="16"/>)</em> is the mean for <em>x (y)</em>, and <em><img src="Images/fd28ba58-bedd-4525-b74d-0f5d66adca14.png" width="16" height="16"/>(<img src="Images/267ec792-db42-4961-a745-3f2f711035bc.png" width="14" height="17"/>)</em> is the standard deviation of <em>x(y)</em>. If researchers put more weight on the direction of the data rather than the magnitude, then the cosine distance could be used. The cosine of two non-zero vectors can be derived by using the Euclidean dot product formula:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/d3dc554e-a590-4c82-9854-0b85304a67f7.png" style="width:19.33em;height:1.42em;" width="3000" height="220"/></div>
<p>where <em>θ</em> is the angle between those two vectors. Also, we could view <em>cos(θ)</em> as a scaling factor, shown here:</p>
<pre>&gt; angles&lt;-seq(0,380,30) 
&gt; y&lt;-cos(angles) 
&gt; y2&lt;-round(y,2) 
&gt; z&lt;-cbind(angles,y2) 
&gt; colnames(z)&lt;-c("ANGLE","cos(angle)") 
&gt; print(z) 
<strong>      ANGLE cos(angle) 
 [1,]     0       1.00 
 [2,]    30       0.15 
 [3,]    60      -0.95 
 [4,]    90      -0.45 
 [5,]   120       0.81 
 [6,]   150       0.70 
 [7,]   180      -0.60 
 [8,]   210      -0.88 
 [9,]   240       0.33 
[10,]   270       0.98 
[11,]   300      -0.02 
[12,]   330      -0.99 
[13,]   360      -0.28</strong> </pre>
<p>Obviously, when the <em>x</em> and <em>y</em> vector has the exact same direction, <em>cos(θ)</em> has the highest value of 1. When they have opposite directions, <em>cos(θ)</em> has a negative value.</p>
<p>Given two vectors A and B, the cosine similarity, <em>cos(θ)</em>, is represented using a dot product and magnitude in the following:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/fdd66593-f9f3-462b-9208-28839851f4f1.png" style="width:32.00em;height:3.58em;" width="5890" height="660"/></div>
<p>For categorical data points, they could not be ranked because of non-numeric values. For these cases, we can calculate the following similarity index:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/188cf78b-353d-4ba4-b053-cf148e7599c7.png" style="width:23.08em;height:2.42em;" width="3720" height="390"/></div>
<p>Where <em>n<sub>matched</sub></em> is the number of matched treats and <em>n<sub>total</sub></em> is the number of total treats. When we have both categorical and numeric data, we can estimate both types of distances first when choosing a weighted final value:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/793aebfe-458d-417d-a729-797743f6ba10.png" style="width:7.83em;height:1.08em;" width="1370" height="190"/></div>
<p>Assuming <em>d<sub>num</sub></em> is the distance based on the numerical data and <em>d<sub>cat</sub></em> is the distance based on the categorical data, we have the following equivalent:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/97b9fe03-c9ef-4c8a-bc93-3a0cef53b2a5.png" style="width:29.08em;height:1.50em;" width="4260" height="220"/></div>
<p>where <em>w<sub>num</sub></em> is the weight of the numerical value.</p>
<p>Assume that we have two sets of data, <em>X</em> and <em>Y</em>. For the <em>X</em> set, we have <em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, ..., <em>x<sub>n</sub></em>, while for the <em>Y</em> set, we have <em>y<sub>1</sub></em>, <em>y<sub>2</sub></em>, ..., and <em>y<sub>m</sub></em>. In other words, for <em>X</em>, there are <em>n</em> observations, while for <em>Y</em>, there are <em>m</em> observations. For a pair of clusters, we have several ways to define their linkages (see the following table):</p>
<table style="border-collapse: collapse;width: 100%" class="MsoTableGrid" border="1">
<tbody>
<tr>
<td style="width: 102px">
<div class="CDPAlignCenter CDPAlign"><strong>Linkage</strong></div>
</td>
<td style="width: 166px">
<div class="CDPAlignCenter CDPAlign"><strong>Math formula</strong></div>
</td>
<td style="width: 437px">
<div class="CDPAlignCenter CDPAlign"><strong>Description</strong></div>
</td>
</tr>
<tr>
<td style="width: 102px">
<p>Single</p>
</td>
<td style="width: 166px">
<p><img class="fm-editor-equation" src="Images/8697c477-f14c-4853-828a-2e22494f5788.png" style="width:7.08em;height:2.33em;" width="1420" height="470"/></p>
</td>
<td style="width: 437px">
<p>Shortest distance between the closest members of the two clusters</p>
</td>
</tr>
<tr>
<td style="width: 102px">
<p>Complete</p>
</td>
<td style="width: 166px">
<p><img class="fm-editor-equation" src="Images/f3330f9d-1a50-4304-b9aa-1c5bd30acccd.png" style="width:8.58em;height:2.75em;" width="1450" height="470"/></p>
</td>
<td style="width: 437px">
<p>Longest distance between the members that are farthest apart (most dissimilar)</p>
</td>
</tr>
<tr>
<td style="width: 102px">
<p>Average</p>
</td>
<td style="width: 166px">
<p><img class="fm-editor-equation" src="Images/5276dbe0-2eb7-45e3-aa7a-91807909229f.png" style="width:10.75em;height:2.83em;" width="2150" height="570"/></p>
</td>
<td style="width: 437px">
<p>Step 1: Get distances between all pairs</p>
<p>Step 2: Calculate the average</p>
</td>
</tr>
<tr>
<td style="width: 102px">
<p>Centroid Method</p>
</td>
<td style="width: 166px">
<p><img class="fm-editor-equation" src="Images/d4109ed8-9024-4a5b-9881-dca7811cb3bd.png" style="width:5.33em;height:1.33em;" width="880" height="220"/></p>
</td>
<td style="width: 437px">
<p>Step 1: Find the mean vector locations</p>
<p>Step 2: Get the distance between the two locations</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Table 8.1 Four types of linkages between two clusters</div>
<p>Let's use some hypothetical values to see how to get those linkages. To make our estimate easier to understand, we have just five values (see the following code):</p>
<pre>&gt; data &lt;- rbind(c(180,20), c(160,5), c(60, 150), c(160,60), c(80,120)) 
&gt; plot(data, col = "red", lwd = 20) </pre>
<p>Refer to the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-790 image-border" src="Images/e5944b5d-2969-4c19-86dc-5cb6a77fcf73.png" style="width:21.33em;height:20.08em;" width="424" height="399"/></div>
<p>For the centers of these two clusters, we could guess that the first one should be about (65,140), while the second one should be around (170, 40) (see the following output code):</p>
<pre>&gt; library(cluster) 
&gt; data &lt;- rbind(c(180,20), c(160,5), c(60, 150), c(160,60), c(80,120)) 
&gt; output&lt;-clara(data,2) 
&gt; output$clustering 
<strong>[1] 1 1 2 1 2</strong> 
&gt; output$medoids 
<strong>     [,1] [,2] 
[1,]  180   20 
[2,]   80  120</strong> </pre>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hierarchical clustering</h1>
                </header>
            
            <article>
                
<p>In hierarchical clustering, the two most similar clusters are combined and continue to combine until all objects are in the same cluster. Hierarchical clustering produces a tree called a <strong>dendrogram</strong> that shows the hierarchy of the clusters.</p>
<p>To show this concept, let's start by looking at the dataset called <kbd>animals</kbd> embedded in the R package called <kbd>cluster</kbd>:</p>
<pre>&gt; library(cluster) 
&gt; data(animals) 
&gt; dim(animals) 
<strong>[1] 20  6</strong> 
&gt; head(animals) 
<strong>    war fly ver end gro hai 
ant   1   1   1   1   2   1 
bee   1   2   1   1   2   2 
cat   2   1   2   1   1   2 
cpl   1   1   1   1   1   2 
chi   2   1   2   2   2   2 
cow   2   1   2   1   2   2</strong> 
&gt; colnames(animals) 
<strong>[1] "war" "fly" "ver" "end" "gro" "hai"</strong> 
&gt; apply(animals,2, table) # simple overview 
<strong>  war fly ver end gro hai 
1  10  16   6  12   6  11 
2  10   4  14   6  11   9</strong> </pre>
<p>In total, we have 20 observations with 6 characteristics: <kbd>war</kbd>, <kbd>fly</kbd>, <kbd>ver</kbd>, <kbd>end</kbd>, <kbd>gro</kbd>, and <kbd>hai</kbd>, illustrated by their column names. The following table shows their definitions:</p>
<table style="border-collapse: collapse;width: 75%" class="MsoTableGrid" border="1">
<tbody>
<tr style="height: 57px">
<td style="width: 39px;height: 57px">
<div class="CDPAlignCenter CDPAlign"><strong>#</strong></div>
</td>
<td style="width: 198px;height: 57px">
<div class="CDPAlignCenter CDPAlign"><strong>Name</strong></div>
</td>
<td style="width: 461px;height: 57px">
<div class="CDPAlignCenter CDPAlign"><strong>Description</strong></div>
</td>
</tr>
<tr style="height: 37px">
<td style="width: 39px;height: 37px">1</td>
<td style="width: 198px;height: 37px"><kbd>war</kbd></td>
<td style="width: 461px;height: 37px">warm-blooded</td>
</tr>
<tr style="height: 33px">
<td style="width: 39px;height: 33px">2</td>
<td style="width: 198px;height: 33px"><kbd>fly</kbd></td>
<td style="width: 461px;height: 33px">can fly</td>
</tr>
<tr style="height: 24px">
<td style="width: 39px;height: 24px">3</td>
<td style="width: 198px;height: 24px"><kbd>ver</kbd></td>
<td style="width: 461px;height: 24px">vertebrate</td>
</tr>
<tr style="height: 29.3125px">
<td style="width: 39px;height: 29.3125px">4</td>
<td style="width: 198px;height: 29.3125px"><kbd>end</kbd></td>
<td style="width: 461px;height: 29.3125px">endangered</td>
</tr>
<tr style="height: 64px">
<td style="width: 39px;height: 64px">5</td>
<td style="width: 198px;height: 64px"><kbd>gro</kbd></td>
<td style="width: 461px;height: 64px">live in groups</td>
</tr>
<tr style="height: 64px">
<td style="width: 39px;height: 64px">6</td>
<td style="width: 198px;height: 64px"><kbd>hai</kbd></td>
<td style="width: 461px;height: 64px">have hair</td>
</tr>
</tbody>
</table>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Table 8.2 Definitions of column names for animals</div>
<p>Again, we have 20 observations with 6 characters to describe each of them:</p>
<pre>ma &lt;- mona(animals)<br/>ma</pre>
<p>The <kbd>mona()</kbd> function is <strong>MONothetic Analysis Clustering of Binary Variables</strong>, and returns a list representing a divisive hierarchical clustering of a dataset with only binary variables. The related output is shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-791 image-border" src="Images/0b8abf3f-6597-4204-bf00-40285101ff6f.png" style="width:35.58em;height:35.50em;" width="562" height="561"/></div>
<p>From the previous output, we can see that there are five groups. For example, the first observation called <kbd>ant</kbd> belongs to group 4. Its vector is <kbd>[0, 0, 0, 0,1,0]</kbd>. On the other hand, for the second observation called <kbd>bee</kbd>, it does not belong to any of those five groups (that is, its group is zero). Its six-value binary vector is <kbd>[0,1,0,0,1,1]</kbd>. To visually present the outcome, we have the following code:</p>
<pre>plot(ma) </pre>
<p>The output is shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-792 image-border" src="Images/8b03d7f2-c031-4386-8464-9e5dc3986394.png" style="width:22.58em;height:25.33em;" width="470" height="527"/></div>
<p>From the previous graph, it can be seen that the first observation belongs to group 4, while the second one does not belong to any of those five groups. To draw a dendrogram for the dataset called <kbd>animals</kbd>, we have the following code:</p>
<pre>&gt; require(cluster) 
&gt; data(animals) 
&gt; x&lt;-agnes(animals) 
&gt; pltree(x) </pre>
<p>Refer to the following graph:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-793 image-border" src="Images/97a7f5cd-723f-476c-b916-4611a44e919f.png" style="width:28.75em;height:29.83em;" width="621" height="644"/></div>
<p>For the next example, we generate two sets of random numbers drawn from a normal distribution with different means, <kbd>0</kbd> and <kbd>80</kbd>, with the same standard deviation of <kbd>8</kbd>:</p>
<pre>library(cluster) 
set.seed(123) 
n1&lt;-200; mean1&lt;-0; std1&lt;-8 
n2&lt;-300; mean2&lt;-80; std2&lt;-8 
set1&lt;-cbind(rnorm(n1,mean1,std1), rnorm(n1,mean1,std1)) 
set2&lt;-cbind(rnorm(n2,mean2,std2), rnorm(n2,mean2,std2)) 
x &lt;- rbind(set1,set2) 
data &lt;- clara(x, 2, samples=50) 
plot(data) </pre>
<p>The output graph is shown in the following diagram. From it, we can see that those random values belong to two distinct groups:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-794 image-border" src="Images/69b490fb-8dcb-46b3-9202-c6029484f852.png" style="width:28.08em;height:28.83em;" width="641" height="657"/></div>
<p>Also note that the  <kbd>clara()</kbd> function from the program has the following structure:</p>
<pre>clara(data, nCluster, metric = "euclidean", stand = FALSE, samples = 5, sampsize = min(n, 40 + 2 * k), trace = 0, medoids.x = TRUE,keep.data = medoids.x, rngR = FALSE, pamLike = FALSE, correct.d = TRUE) </pre>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">k-means clustering</h1>
                </header>
            
            <article>
                
<p>The purpose of k-means clustering is to partition <em>n</em> observations into <em>k</em> clusters, where each observation belongs to the cluster with the nearest mean. This results in a partitioning of the data space into a <strong>Voronoi diagram</strong>. In mathematics, a Voronoi diagram is a partitioning of a plane into regions based on the distance to points in a specific subset of the plane.</p>
<p>Borrowed from Priyadarsini (<a href="https://www.kaggle.com/maitree/kmeans-unsupervised-learning-using-wine-dataset/data">https://www.kaggle.com/maitree/kmeans-unsupervised-learning-using-wine-dataset/data</a>), the slightly modified code is given here:</p>
<pre>library(readr)  
library(corrplot) 
library(ggplot2) 
# 
path&lt;-"http://canisius.edu/~yany/RData/wine.RData" 
load(url(path)) 
red2&lt;-red 
red2$quality&lt;-NULL 
white2&lt;-white 
white2$quality&lt;-NULL 
red_cor&lt;-cor(red2) 
white_cor&lt;-cor(white2) 
class(red_cor) 
class(white_cor) 
# 
corrplot(red_cor,method="number") </pre>
<p>The output is shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-795 image-border" src="Images/d25be667-5e59-440c-88b7-dc5f4d1569f9.png" style="width:27.58em;height:25.17em;" width="588" height="537"/></div>
<p>This square matrix shows the similarity between each combination of treatments. Blue values are all positive, representing similarity, while red values are negative, representing dissimilarity.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to Python packages – scipy</h1>
                </header>
            
            <article>
                
<p>The submodule from the <kbd>scipy</kbd> package is called <kbd>scipy.cluster</kbd>. With the following code, we can find all embedded functions. The document for this submodule is available here at <a href="https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html">https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html</a>:</p>
<pre>import scipy.cluster as cluster 
x=dir(cluster) 
print(x) </pre>
<p>The next screenshot shows the related output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-796 image-border" src="Images/07935c9d-d3d2-4b69-9d2a-2ff805549fd1.png" style="width:30.42em;height:6.00em;" width="585" height="116"/></div>
<p>First, we use the <kbd>scipy.cluster._hierarchy</kbd> function. This example is borrowed as well (<a href="https://stackoverflow.com/questions/21638130/tutorial-for-scipy-cluster-hierarchy">https://stackoverflow.com/questions/21638130/tutorial-for-scipy-cluster-hierarchy</a>). The code is shown here:</p>
<pre>import numpy as np 
import matplotlib.pyplot as plt 
import scipy.cluster.hierarchy as hac 
a = np.array([[0.11, 2.5],[1.45, 0.40], 
              [0.3,  1.1],[0.9 , 0.8], 
              [0.5,  0.01],[0.1 , 0.5], 
              [0.6,  0.5],[2.6,  2.1], 
              [2.3,  3.2],[3.1,  2.2], 
              [3.2,  1.3]]) 
name='possiblen&lt;- knee point' 
yName='{}ncluster distance' 
title='Screeplot{}' 
t1=['#2200CC','#D9007E','#FF6600','#FFCC00'] 
t2=['#ACE600','#0099CC','#8900CC','#FF0000'] 
t3=['#FF9900' ,'#FFFF00','#00CC01' ,'#0055CC'] 
fig, axes23 = plt.subplots(2, 3) 
for method, axes in zip(['single','complete'], axes23): 
    z = hac.linkage(a, method=method) 
    axes[0].plot(range(1, len(z)+1), z[::-1, 2]) 
    knee = np.diff(z[::-1, 2], 2) 
    axes[0].plot(range(2, len(z)), knee) 
for method, axes in zip(['single','complete'], axes23): 
    z = hac.linkage(a, method=method) 
    axes[0].plot(range(1, len(z)+1), z[::-1, 2]) 
    knee = np.diff(z[::-1, 2], 2) 
    axes[0].plot(range(2, len(z)), knee) 
    num_clust1 = knee.argmax() + 2 
    knee[knee.argmax()] = 0 
    num_clust2 = knee.argmax() + 2 
    axes[0].text(num_clust1, z[::-1, 2][num_clust1-1], name) 
    part1 = hac.fcluster(z, num_clust1, 'maxclust') 
    part2 = hac.fcluster(z, num_clust2, 'maxclust') 
    clr = t1+t2+t3 
    for part,ax in zip([part1, part2], axes[1:]): 
        for cluster in set(part): 
            ax.scatter(a[part==cluster,0],a[part==cluster,1],  
                       color=clr[cluster]) 
    m = 'n(method: {})'.format(method) 
    plt.setp(axes[0 ],title=title.format(m),xlabel='partition', 
             ylabel=yName.format(m)) 
    plt.setp(axes[1], title='{} Clusters'.format(num_clust1)) 
    plt.setp(axes[2], title='{} Clusters'.format(num_clust2)) 
plt.tight_layout() 
plt.show() </pre>
<p>The related graphs are shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-797 image-border" src="Images/9f2ad3cf-df57-4201-9eb1-c31a6222d1fd.png" style="width:27.08em;height:20.67em;" width="563" height="430"/></div>
<p>The methods used are single and complete in terms of estimating the distance between two points. The top middle graph shows the clustering with two groups, while the top right one shows the clustering for five groups.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to Python packages – contrastive</h1>
                </header>
            
            <article>
                
<p>To install the Python package called <kbd>contrastive</kbd>, we issue the following command after launching the Anaconda prompt:</p>
<pre><strong>pip install contrastive</strong> </pre>
<p>The following screenshot shows the top part of the installation:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-798 image-border" src="Images/9e28a64d-0e6b-4b2c-a4c6-45d85f13561c.png" style="width:37.83em;height:5.58em;" width="642" height="94"/></div>
<p>Using the <kbd>dir()</kbd> and <kbd>print()</kbd> functions, we could find more information about a function embedded in the Python package:</p>
<pre>from contrastive import cluster  
x=dir(cluster) 
print(x) </pre>
<p>The output is shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-799 image-border" src="Images/31d9c78d-cc2d-4f0c-bc13-3f0164441a45.png" style="width:30.58em;height:13.92em;" width="627" height="286"/></div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to Python packages – sklearn (scikit-learn)</h1>
                </header>
            
            <article>
                
<p>First, let's look at the functions contained in the Python package called <kbd>sklearn</kbd>. The code has just three lines:</p>
<pre>import sklearn as sk 
x=dir(sk) 
print(x) </pre>
<p>The related output is shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-800 image-border" src="Images/ee25739c-0211-4257-89ed-1b4dbaf9dad6.png" style="width:30.92em;height:12.92em;" width="629" height="262"/></div>
<p>For one specific submodule, it is called <kbd>sklearn.cluster</kbd>, as shown:</p>
<pre>from sklearn import cluster 
x=dir(cluster) 
print(x) </pre>
<p>The output is shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-801 image-border" src="Images/e9ceacea-0503-459c-9ee5-8d8f72436b0e.png" style="width:29.42em;height:12.92em;" width="636" height="278"/></div>
<p>In addition, we can show many embedded datasets by using the following three lines of Python code:</p>
<pre>import sklearn.datasets as datasets 
x=dir(datasets) 
print(x) </pre>
<p>The output is shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-802 image-border" src="Images/e85dd7e0-2720-41f0-a4df-bc9c9f8d09b6.png" style="width:32.00em;height:17.33em;" width="701" height="380"/></div>
<p>For example, if we want to use a dataset called <kbd>iris</kbd>, we can apply the <kbd>load_iris()</kbd> function, as shown:</p>
<pre>from sklearn import datasets 
data= datasets.load_iris() 
print(data) </pre>
<p>The first few lines are shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-803 image-border" src="Images/78d793f6-bbc2-4122-ba7e-561f86b2151c.png" style="width:22.33em;height:11.75em;" width="497" height="261"/></div>
<p>The following code is one example of using the module:</p>
<pre>from sklearn import cluster 
from sklearn import datasets 
x=datasets.load_iris() 
iris = datasets.load_iris() 
x = iris.data 
k_means = cluster.KMeans(n_clusters=3) 
k_means.fit(x)  
print(k_means.labels_[::10]) 
<strong>[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]</strong> </pre>
<p>The last line of the previous code shows the potential groups they belong to. The next example is by <em>Michel, Gramfort and Varoquaux</em> (2010), <a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py">http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py</a>. The output is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-804 image-border" src="Images/749edf45-95c9-4f24-b49a-45c034ac5492.png" style="width:39.00em;height:16.33em;" width="526" height="220"/></div>
<p>From the previous graphs, we visually view groups by their colors. For example, the left graph shows us there are six groups. The next example is about Agglomerative Clustering on a 2D embedding of digits. Since the program is too long, we would not include it here. You can find the program at <a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py">http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py</a>. The related graph output is shown in the following screenshot. The graph shows several groups based on a few different definitions of distance (linkage):</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-805 image-border" src="Images/82def4b1-5195-49ee-9f01-9fc1953a61e2.png" style="width:54.25em;height:18.33em;" width="739" height="249"/></div>
<p>Based on the definitions of distance of <strong>ward</strong>, <strong>average</strong>, or <strong>complete</strong>, we have three ways to generate clusters. For each method, various colors represent different clusters. The last example is looking at the <kbd>iris</kbd> dataset, which is associated with <strong>Principal Component Analysis</strong> (<strong>PCA</strong>). PCA uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called <strong>principal components</strong> (<a href="http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html">http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html</a>)<a href="http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html">.</a> The related 3D picture is shown here. The graph shows the groups for these three types:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-806 image-border" src="Images/83b77261-d427-415f-a5fa-3c8e72f649db.png" style="width:20.25em;height:16.33em;" width="307" height="249"/></div>
<p>One useful link is <a href="http://scikit-learn.org/stable/modules/clustering.html">http://scikit-learn.org/stable/modules/clustering.html</a>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to R packages – rattle</h1>
                </header>
            
            <article>
                
<p>Before discussing one or two examples of using <kbd>rattle</kbd>, it might be a good idea to discuss an R package called <kbd>rattle.data</kbd>. As its name suggests, we could guess that it contains data used by <kbd>rattle</kbd>. It is a good idea to use a small dataset to generate a dendrogram. For the next case, we use the first <kbd>20</kbd> observations from a dataset called <kbd>wine</kbd>:</p>
<pre>library(rattle.data) 
data(wine)  
x&lt;-head(wine,20) </pre>
<p>To launch <kbd>rattle</kbd>, we have the following two lines:</p>
<pre>library(rattle) 
rattle() </pre>
<p>For data, we choose <span class="packt_screen">R Dataset</span>, then choose <kbd>x</kbd>, as shown in the following screenshot. To save space, only the top part is presented here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-807 image-border" src="Images/ede71328-7e7c-42df-9206-5cd13e04c788.png" style="width:45.17em;height:15.50em;" width="690" height="236"/></div>
<p>The following screenshot shows our choice:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-808 image-border" src="Images/17d38ba4-ee8c-4f14-95ae-cf360dc70211.png" style="width:33.42em;height:16.50em;" width="622" height="308"/></div>
<p>From the previous screenshot, we see <span class="packt_screen">14</span> observations. Click <span class="packt_screen">Clusters</span>, with a default of <kbd>10</kbd> clusters, and <span class="packt_screen">Dendrogram.</span> See the result in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-809 image-border" src="Images/b713d931-ff37-4e00-babf-cef265c52817.png" style="width:30.00em;height:30.83em;" width="643" height="661"/></div>
<p>The previous dendrogram is a tree diagram that is used to show the relationship for a clustering analysis. Based on the definition of distance or relationship, we have a link between the closed associated one pair. Then, we move one step up to link this pair with another adjacent point or observation. The procedure continues until we reach the final step of just one group.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to R packages – randomUniformForest</h1>
                </header>
            
            <article>
                
<p>The R package, <kbd>randomUniformForest</kbd> is for classification, regression, and unsupervised learning. The basic block for a random forest is the decision tree model. To make our classification, researchers add some random effect. Because of this, the random forest method performs better than the decision tree model. We can use the following code to install, load, and get help about this specific R package:</p>
<pre>&gt; install.packages("randomUniformForest") 
&gt; library(randomUniformForest) 
&gt; help(package=randomUniformForest) </pre>
<p>First, let's look at a relatively simple program that tries to classify different plants into four groups by using the random forest method:</p>
<pre>library(randomUniformForest) 
data(iris) 
shortName&lt;-unsupervised.randomUniformForest 
x =shortName(iris[,-5],mtry=1,nodesize = 2) 
plot(x) </pre>
<p>It is obvious that the function name of <kbd>unsupervised.randomUniformForest</kbd> is quite long. To make our program more readable, we generate a short name called <kbd>shortName</kbd> instead. The related output is shown in the following screenshot, where it illustrates four groups by color:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-810 image-border" src="Images/fa4f83d4-327a-4bb3-81c9-026a929f7e18.png" style="width:26.75em;height:26.00em;" width="546" height="529"/></div>
<p>The second example presents several groups based on the wine quality, such as alcohol concentration, color, and dilution:</p>
<pre>library(randomUniformForest) 
data(wineQualityRed) 
x = wineQualityRed[, -ncol(wineQualityRed)] 
# run unsupervised analysis on the first half of dataset  
data1 = 1:floor(nrow(x)/2) 
sf&lt;-unsupervised.randomUniformForest 
model1 =sf(x,subset =data1,depth = 5)  
plot(model1) </pre>
<p>The graph is shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-811 image-border" src="Images/09a517ab-ae5d-4c5a-9842-8628aa80eec5.png" style="width:26.67em;height:23.42em;" width="557" height="488"/></div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to R packages – Rmixmod</h1>
                </header>
            
            <article>
                
<p>This R package is for supervised, unsupervised, and semi-supervised classification with <strong>MIXture MODelling</strong> (interface of MIXMOD software). First, let's look at a dataset called <kbd>birds</kbd>:</p>
<pre>&gt; library(Rmixmod) 
&gt; data(birds)<br/>&gt; head(birds) 
<strong>  gender         eyebrow collar sub-caudal border 
1   male poor pronounced dotted      white    few 
2 female            none dotted      black   none 
3 female      pronounced   none      white   none 
4   male      pronounced dotted      white   none 
5   male      pronounced dotted      white   none 
6   male      pronounced dotted      white   none</strong> 
&gt; dim(birds) 
<strong>[1] 69  5</strong> </pre>
<p>From the previous output, we know that there are <kbd>69</kbd> observations with <kbd>5</kbd> characteristics. The following example code is designed to plot bars based on eyebrow and collar:</p>
<pre>x &lt;- mixmodCluster(birds,2) 
bb&lt;-barplotCluster 
bb(x["bestResult"], birds) 
bb(x["bestResult"], birds, variables=c(2,3,4)) 
bb(x["bestResult"], birds, variables=c("eyebrow","collar")) </pre>
<p>The graphs are shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-812 image-border" src="Images/a2327af5-5f00-40f4-ba15-e1f8b139cda2.png" style="width:38.25em;height:29.00em;" width="788" height="598"/></div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementation using Julia</h1>
                </header>
            
            <article>
                
<p>For Julia, we use the package called <kbd>Clustering</kbd>. The next example is borrowed from <em>Lin, Regier, and Arslan</em> (2016) with a minor modification (<a href="https://github.com/JuliaStats/Clustering.jl/blob/master/test/affprop.jl">https://github.com/JuliaStats/Clustering.jl/blob/master/test/affprop.jl</a>). First, we generate a set of random numbers. Then, replace the values along the main diagonal line with the median values. Then, the program classifies them into different groups:</p>
<pre>using Base.Test 
using Distances 
using Clustering 
# 
srand(12345) 
d = 10 
n = 500 
x = rand(d, n) 
S = -pairwise(Euclidean(), x, x) 
# set diagonal value to median value 
S = S - diagm(diag(S)) + median(S)*eye(size(S,1))  
R = affinityprop(S) </pre>
<p>The output is shown here:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-813 image-border" src="Images/80add43a-f72b-43ec-8d58-112f57ead85c.png" style="width:38.92em;height:4.33em;" width="643" height="71"/></div>
<p>Based on the output, we see a few groups and which numbers belong to which group.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Task view for Cluster Analysis</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, we have discussed various kinds of task views. A task view is a set of R packages organized by one or a few experts around a topic. For the <strong>Cluster Analysis</strong>, <a href="https://CRAN.R-project.org/view=Cluster">https://CRAN.R-project.org/view=Cluster</a>, the top part is shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-814 image-border" src="Images/68937f40-a789-46c7-959e-22890acc7fa9.png" style="width:31.42em;height:26.50em;" width="601" height="507"/></div>
<p>As discussed in <em><a href="c812a40e-eb24-4bb8-8af5-1cfe1834ec77.xhtml">Chapter 6</a>, Managing Packages</em>, we can use just three lines of R code to install these cluster-related packages:</p>
<pre>&gt;install.packages("ctv") 
&gt;library("ctv") 
&gt;install.views("Cluster") </pre>
<p>In total, there are about 154 R packages included in the previous task view called <strong>Cluster,</strong> as of March 21, 2018.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have discussed unsupervised learning. In particular, we have explained hierarchical clustering and k-means clustering. As for R and Python, we have explained several related packages:</p>
<ul>
<li>R:<span> </span><kbd>rattle</kbd>,<span> </span><kbd>Rmixmod</kbd>, and<span> </span><kbd>randomUniformForest</kbd></li>
<li>Python:<span> </span><kbd>scipy.cluster</kbd>,<span> </span><kbd>contrastive</kbd>, and<span> </span><kbd>sklearn</kbd></li>
</ul>
<p>Several real-world examples have also been used to illustrate the applications of these packages in detail.</p>
<p>For the next chapter, we will discuss supervised learning, such as classification, the k-nearest neighbors algorithm, Bayes' classifiers, reinforcement learning, and specific R and Python-related modules, such as<span> </span><kbd>RTextTools</kbd><span> </span>and<span> </span><kbd>sklearn</kbd>. In addition, we will discuss implementation via R, Python, Julia, and Octave.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Review questions and exercises</h1>
                </header>
            
            <article>
                
<ol>
<li>What does <em>unsupervised learning</em> mean?</li>
<li>What is the major difference between unsupervised learning and supervised learning?</li>
<li>How do we install the Python package <kbd>sklearn</kbd>?</li>
<li>Discuss the relationship between distance and clustering classification.</li>
<li>How do we define the distance between two objects?</li>
<li>For non-numeric values, how do we define a distance between two members?</li>
<li>For R, we could find a set of related packages related to unsupervised learning called <kbd>cluster</kbd>. Is there any task view, or similar super package, for Python?</li>
<li>First, generate the following set of random numbers:</li>
</ol>
<pre style="padding-left: 90px">&gt;set.seed(12345) 
&gt;n=30 
&gt;nGroup=4 
&gt;x &lt;- matrix(rnorm(n*nGroup),nrow =nGroup) </pre>
<p style="padding-left: 60px">Then, based on the various definitions of distance, estimate the distances between those four groups.</p>
<ol start="9">
<li>For the following set of data, estimate the minimum, maximum, and average distances:</li>
</ol>
<pre style="padding-left: 90px">&gt; data &lt;- rbind(c(180,20), c(160,5), c(60, 150), c(160,60), c(80,120)) </pre>
<ol start="10">
<li>What is the usage of a dendrogram?</li>
<li>Draw a dendrogram by using all wine data.</li>
</ol>
<p> </p>
<ol start="12">
<li>Generate 20 random numbers with a mean of 1.2 and standard deviation of 2 from a normal distribution. Then draw a dendrogram.</li>
<li>Using a five-year monthly historical price data for 30 stocks, estimate their annualized standard deviations and means. Classify them into different groups. The source of data is Yahoo!Finance (<a href="http://finance.yahoo.com">http://finance.yahoo.com</a>). Note that the following formulae are used to calculate an annualized standard deviation:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/9f277317-4d64-44b3-a79d-1796a5aeacc9.png" style="width:14.00em;height:3.17em;" width="2610" height="590"/></div>
<p style="padding-left: 60px">Where <em>σ<sub>annual</sub></em> is the annualized standard deviation, <em>σ<sub>monthly</sub></em> is the standard deviation based on monthly returns, <img src="Images/55c15b72-5583-4adc-b383-60f548c17314.png" width="42" height="16"/> is the annualized mean return, and <img src="Images/6e626250-a85a-4787-a4ed-f19fb52240f3.png" width="49" height="17"/> is the monthly mean return.</p>
<ol start="14">
<li>For the R package called <kbd>cluster</kbd>, what is the meaning of the <kbd>votes.repub</kbd> dataset? Using that dataset, conduct a Cluster Analysis and draw a dendogram tree.</li>
<li>Find more information about the <kbd>linkage_tree()</kbd> function contained in the <kbd>sklearn.cluster</kbd> submodule. (Python)</li>
<li>For the <kbd>rattle</kbd> package, how do we save an R script?</li>
</ol>


            </article>

            
        </section>
    </div></body></html>