<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch09" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 9. Time Series</h1></div></div></div><p class="calibre11">The capacity to demonstrate and perform decision modeling and examination is a crucial component of some real-world applications ranging from emergency medical treatment in intensive care units to military commands and control frameworks. Existing methodologies and techniques for deduction have not been progressively viable with applications where exchange offs between decision quality and computational tractability are essential. A successful way to deal with time-critical element decision modeling should give express backing to the demonstration of transient procedures and for managing time-critical circumstances.</p><p class="calibre11">In this chapter, we will cover:</p><div><ul class="itemizedlist"><li class="listitem">What is Forecasting?</li><li class="listitem">Decision-making processes</li><li class="listitem">What is Time Series?</li><li class="listitem">Types of models</li><li class="listitem">Trend analysis</li><li class="listitem">Analysis of seasonality</li><li class="listitem">ARIMA</li><li class="listitem">Smoothing</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch09lvl1sec71" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>What is forecasting?</h1></div></div></div><p class="calibre11">Let's take the example of an organization that needs to find out the demand for its inventory in the near future, to maximize the return on investment.</p><p class="calibre11">For instance, numerous stock frameworks apply for indeterminate demand. The stock parameters in these frameworks require evaluations of the demand and forecast error distributions.</p><p class="calibre11">The two phases of these frameworks, forecasting and stock control, are frequently analyzed autonomously. It is essential to comprehend the cooperation between demand estimating and stock control since this impacts the execution of the stock framework.</p><p class="calibre11">Forecasting requirements include:</p><div><ul class="itemizedlist"><li class="listitem">Each decision gets to be operational sooner or later, so it ought to be based on figures of future conditions.</li><li class="listitem">Figures are required all through an organization and they should absolutely not be created by a disconnected gathering of forecasters.</li><li class="listitem">Forecasting is never "wrapped up". Forecasts are required constantly, and as time proceeds onward, the effect of the forecasts on real execution is measured, original forecasts are overhauled, and decisions are adjusted, and this goes on in a loop.</li></ul></div><p class="calibre11">The decision maker makes use of forecasting models to carry out the decisions. They are regularly utilized to demonstrate the procedure to research the effect of various strategies reflectively.</p><p class="calibre11">It is helpful to break the components of decision making into three groups:</p><div><ul class="itemizedlist"><li class="listitem">Uncontrollable</li><li class="listitem">Controllable</li><li class="listitem">Resources (that define the problem situation)</li></ul></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec117" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Decision-making process</h2></div></div></div><p class="calibre11">What is a system? Frameworks are shaped with parts set up together in a specific way, keeping in mind the end goal to meet a specific a target. The relationship between the parts figures out what the framework does and its overall capacities. Along these lines, the connections in a framework are regularly more critical than the individual parts. When all is said and done, the frameworks that are building blocks for different frameworks are called subsystems.</p><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec65" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The dynamics of a system</h3></div></div></div><p class="calibre11">A framework that does not change is a static framework. A hefty portion of business frameworks are rapid frameworks, which mean that their states change after some time. We allude to the way a framework changes after some time as the framework's conduct. What's more, when the framework's improvement takes a typical pattern, we say the framework has a behavior pattern. Whether a framework is static or dynamic relies upon how it changes over time.</p><p class="calibre11">The decision-making process has the following components:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Performance measure (or indicator)</strong>: The development of powerful measures is seen as important in every organization. Performance measures give the desirable levels of results, that is, the target of your choice. The goal is essential in recognizing the anticipating action:<div><ul class="itemizedlist1"><li class="listitem"><strong class="calibre19">Strategic</strong>: Return on Investment, growth, and innovations</li><li class="listitem"><strong class="calibre19">Tactical</strong>: Cost, quantity, and customer satisfaction</li><li class="listitem"><strong class="calibre19">Operational</strong>: Target setting and conformance with standard</li></ul></div><p class="calibre44">
</p></li><li class="listitem"><strong class="calibre19">Resources</strong>: Resources are the consistent components that don't change amid the time range of the forecast. Resources are the variables that characterize the decision issue. Strategic decisions ordinarily have longer time horizons than both the tactical and the operational choices.</li><li class="listitem"><strong class="calibre19">Forecasts</strong>: Forecast information originates from the environment of the decision maker. Uncontrollable inputs must be determined or predicted.</li><li class="listitem"><strong class="calibre19">Decisions</strong>: Decision inputs are the collection of all conceivable approaches that are possible.</li><li class="listitem"><strong class="calibre19">Interaction</strong>: Associations among the preceding decision parts are the logical, scientific functions representing the circumstances and end result connections among inputs, resources, forecasts, and the result. At the point when the result of a decision relies upon the strategy, we transform one or more parts of the risky circumstance with the aim of realizing an attractive change in some other part of it. We succeed in the event that we know about the connection among the parts of the issue.</li><li class="listitem"><strong class="calibre19">Actions</strong>: Decision making includes the choice of a strategy that is chosen by the decision maker. The way that our strategy influences the result of a choice relies upon how the forecasts and different inputs are interrelated and how they identify with the result.</li></ul></div></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch09lvl1sec72" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>What is TimeSeries?</h1></div></div></div><p class="calibre11">A time series is an arrangement of insights, typically gathered at standard intervals. Time series information normally happens in numerous applications:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Economics</strong>: For example, monthly data for unemployment, hospital admissions, and so on</li><li class="listitem"><strong class="calibre19">Finance</strong>: For example, daily exchange rate, a share price, and so on</li><li class="listitem"><strong class="calibre19">Environmental</strong>: For example, daily rainfall, air quality readings, and so on</li><li class="listitem"><strong class="calibre19">Medicine</strong>: For example, ECG brain wave activity every 2 to 8 seconds</li></ul></div><p class="calibre11">The techniques for time series investigation predate those for general stochastic procedures and Markov chains. The goals of time series analysis are to portray and outline time series data, fit low-dimensional models, and to make desirable forecasts.</p><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec118" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Trends, seasonality, cycles, and residuals</h2></div></div></div><p class="calibre11">One straightforward strategy for depicting a series is that of classical disintegration. The idea is that the arrangement can be segmented into four components:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Trend (Tt)</strong>: Long-term movements in the mean</li><li class="listitem"><strong class="calibre19">Seasonal effects (It)</strong>: Cyclical fluctuations related to the calendar</li><li class="listitem"><strong class="calibre19">Cycles (Ct)</strong>: Other cyclical fluctuations (such as a business cycle)</li><li class="listitem"><strong class="calibre19">Residuals (Et)</strong>: Other random or systematic fluctuations</li></ul></div><p class="calibre11">The idea is to create separate models for these four elements and then combine them:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Additively</strong>: <em class="calibre23">Xt = Tt + It + Ct + Et</em></li><li class="listitem"><strong class="calibre19">Multiplicatively</strong>:  <em class="calibre23">Xt = Tt It Ct Et</em></li></ul></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec66" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Difference from standard linear regression</h3></div></div></div><p class="calibre11">The information is not inexorably independent and is not, as a matter of course, indistinguishably distributed. One characteristic for time series is that it is a rundown of observations where the ordering matters. Sequence is essential on the grounds that there is reliance and changing the sequence could change the importance of the information.</p></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec67" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Basic objectives of the analysis</h3></div></div></div><p class="calibre11">The basic objective usually is to determine a model that describes the pattern of the time series. Uses for such a model are:</p><div><ul class="itemizedlist"><li class="listitem">Describing the important features of the time series pattern</li><li class="listitem">Explaining how the past affects the future or how two time series can "interact"</li><li class="listitem">Forecasting future values of the series</li><li class="listitem">Serving as a control standard for a variable that measures the quality of products in some manufacturing situations</li></ul></div></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec68" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Types of models</h3></div></div></div><p class="calibre11">There are two basic types of "time domain" model:</p><div><ul class="itemizedlist"><li class="listitem">Ordinary regression models that use time indices as x-variables:<div><ul class="itemizedlist1"><li class="listitem">Helpful for an initial description of the data and form the basis of several simple forecasting methods</li></ul></div><p class="calibre44">
</p></li><li class="listitem">ARIMA models (for Autoregressive Integrated Moving Average):<div><ul class="itemizedlist1"><li class="listitem">Models that relate the present value of a series to past values and past prediction errors</li></ul></div><p class="calibre44">
</p></li></ul></div></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec69" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Important characteristics to consider first</h3></div></div></div><p class="calibre11">Some important questions to first consider when looking at a time series are:</p><div><ul class="itemizedlist"><li class="listitem">Is there a trend?<div><ul class="itemizedlist1"><li class="listitem">The pattern in which the measurements tend to increase or decrease over time.</li></ul></div><p class="calibre44">
</p></li><li class="listitem">The effect of seasonality?<div><ul class="itemizedlist1"><li class="listitem">Is there a regularly repeating pattern of highs and lows related to calendar time such as seasons, quarters, months, days of the week, and so on?</li></ul></div><p class="calibre44">
</p></li><li class="listitem">Are there any outliers?<div><ul class="itemizedlist1"><li class="listitem">In regression, outliers are at a distance from the trend line. With time series data, the outliers are at a distance from other data.</li></ul></div><p class="calibre44">
</p></li><li class="listitem">Is there a period unrelated to seasonality factors?</li><li class="listitem">Is there a constant variance over a definite period of time?</li><li class="listitem">Are there any abrupt changes to either side?</li></ul></div><p class="calibre11">The following plot is an example of random numbers over time. By a time series plot, we simply mean that the variable is plotted against time. Similar plots can be made for heartbeats over time, market fluctuations, seismic graphs, and so on.</p><p class="calibre11">
</p><div><img src="img/image_09_001.jpg" alt="Important characteristics to consider first" class="calibre211"/></div><p class="calibre11">
</p><p class="calibre11">Some features of the plot include:</p><div><ul class="itemizedlist"><li class="listitem">There is no consistent trend (upward or downward) over the entire time span. The series appears to slowly wander up and down.</li><li class="listitem">There are some obvious outliers.</li><li class="listitem">It's difficult to judge whether the variance is constant or not.</li></ul></div></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec70" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Systematic pattern and random noise</h3></div></div></div><p class="calibre11">As in most different analyses, in time series analysis it is accepted that the information comprises of a systematic pattern (as a set arrangement of identifiable segments) and random noise (error), which typically makes the pattern hard to recognize. Most of the time, series analysis systems include some type of filtering through noise, keeping in mind the end goal to make the pattern more identifiable.</p></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec71" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Two general aspects of time series patterns</h3></div></div></div><p class="calibre11">Most of the time, series patterns can be portrayed regarding two fundamental classes of segments:</p><div><ul class="itemizedlist"><li class="listitem">Pattern</li><li class="listitem">Regularity</li></ul></div><p class="calibre11">A pattern is a generally straight or (regularly) nonlinear part that progresses after some time and does not rehash (probably) inside the time range caught by our data.</p><p class="calibre11">Regularity may have a formally similar nature; in any case, it rehashes itself in systematic intervals after some time. These two general classes of time series components may coexist in actual real-life data.</p><p class="calibre11">For instance, offers of an organization can quickly grow over the years, yet they still follow predictable seasonal patterns (for example, as much as 30% of yearly deals every year are made in October, while just 10% are made in March).</p></div></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec119" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Trend analysis</h2></div></div></div><p class="calibre11">There are no demonstrated "programmed" systems to distinguish pattern segments in the time series data. In any case, the length of the pattern is repetitive (increasing or decreasing in a consistent manner) and some portion of data analysis is ordinarily not extremely difficult. In the event that the time series information contains considerable errors, then the initial phase during the time spent pattern distinguishing proof is smoothing.</p><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec72" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Smoothing</h3></div></div></div><p class="calibre11">Smoothing dependably includes some type of neighborhood averaging of data such that the non-systematic parts of individual perceptions offset each other. The most widely recognized method is moving normal smoothing. This replaces every component of the series by either the simple or weighted normal of n encompassing components, where n is the width of the smoothing "window".</p><p class="calibre11">Medians can be utilized rather than means. Here are some advantages of medians:</p><div><ul class="itemizedlist"><li class="listitem">Within the smoothing window, its results are less biased by outliers</li><li class="listitem">If there are outliers in the data, middle smoothing regularly delivers smoother or more "reliable" curves than moving normal, taking into account the same window width</li></ul></div><p class="calibre11">The fundamental weakness of middle smoothing is that without clear outliers it might deliver more jagged curves than moving normal and it doesn't take into account weighting.</p></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec73" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Fitting a function</h3></div></div></div><p class="calibre11">Numerous monotonous time series information can be satisfactorily approximated by a linear function. If there is a reasonable monotonous nonlinear part, the data initially should be changed to evacuate the nonlinearity. Normally a logarithmic, exponential, or (less frequently) polynomial function can be utilized.</p></div></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec120" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Analysis of seasonality</h2></div></div></div><p class="calibre11">Seasonal dependency (seasonality) is another general part of the time series design. For example, if we see a time series graph of buying trends, we can see that there is a huge spike during the end of October and December every year. This pattern repeats every year.</p><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec74" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Autocorrelation correlogram</h3></div></div></div><p class="calibre11">Seasonal patterns of time series can be analyzed by means of correlograms. The correlogram graphically and numerically shows the <strong class="calibre19">autocorrelation function (ACF),</strong> which are serial relationship coefficients (and their standard errors) for sequential lags in a predetermined range of lags.</p><p class="calibre11">Ranges of two standard errors for every lag are generally set apart in correlograms, yet commonly the size of autocorrelation is of more interest than its dependability.</p><p class="calibre11">The following is the correlogram for the <code class="literal">mtcars</code> dataset:</p><p class="calibre11">
</p><div><img src="img/image_09_002.jpg" alt="Autocorrelation correlogram" class="calibre212"/></div><p class="calibre11">
</p><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec5" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Examining correlograms</h4></div></div></div><p class="calibre11">We must remember while examining correlograms that autocorrelations for successive lags are formally dependent. For example, if the primary component is firmly related to the second, and the second to the third, then the main component should likewise be to some degree related to the third one, and so on.</p></div></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec75" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Partial autocorrelations</h3></div></div></div><p class="calibre11">Another helpful strategy to inspect serial dependencies is to look at the <strong class="calibre19">partial autocorrelation capacity</strong> (<strong class="calibre19">PACF</strong>), an expansion of autocorrelation, where the dependence on the intermediate components (those inside the lag) is eliminated.</p></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec76" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Removing serial dependency</h3></div></div></div><p class="calibre11">Serial dependency for a specific lag of k can be evacuated by differencing the series, that is, changing over every <em class="calibre23">i</em>th component of the series into its difference from the <em class="calibre23">(i-k)</em>th component.</p><p class="calibre11">There are two explanations behind such changes:</p><div><ul class="itemizedlist"><li class="listitem">Seasonal dependencies of a hidden nature can be recognized in the series</li><li class="listitem">ARIMA and other procedures require that we make the series stationary, which itself requires removing the seasonal dependencies</li></ul></div></div></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec121" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>ARIMA</h2></div></div></div><p class="calibre11">We have discussed the numerical modeling of the procedure of time series analysis. In real life, patterns are not so clear and the observations generally have a considerable amount of errors.</p><p class="calibre11">The requirements are to:</p><div><ul class="itemizedlist"><li class="listitem">Find hidden patterns</li><li class="listitem">Generate forecasts</li></ul></div><p class="calibre11">Let's now understand ARIMA and how it can help us with getting these.</p><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec77" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Common processes</h3></div></div></div><div><ul class="itemizedlist"><li class="listitem">Autoregressive procedure:<div><ul class="itemizedlist1"><li class="listitem">Most time series comprise of components that are serially dependent as in you can evaluate a coefficient or an arrangement of coefficients that depict back-to-back components of the series from particular, time-lagged (past) components.</li><li class="listitem">Stationary prerequisite. The autoregressive procedure is stable only within a certain range of parameters that the parameters fall into. Previous effects can pile up and affect the consecutive points and the series may not be stationary.</li></ul></div><p class="calibre44">
</p></li><li class="listitem">Moving normal procedure. Autonomous from the autoregressive procedure, every component in the series can likewise be influenced by the past error (or arbitrary shock) that can't be represented by the autoregressive component.</li><li class="listitem">Invertibility necessity. There is a "duality" between the moving normal procedure and the autoregressive procedure:<div><ul class="itemizedlist1"><li class="listitem">The moving normal equation can be transformed into an autoregressive structure. In any case, undifferentiated from the stationary condition depicted over, this must be done if the moving normal parameters take after specific conditions, that is, if the model is invertible. Also, the series won't be stationary.</li></ul></div><p class="calibre44">
</p></li></ul></div></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec78" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>ARIMA methodology</h3></div></div></div><p class="calibre11">Autoregressive moving average model.</p><p class="calibre11">The general model presented by Box and Jenkins (1976) incorporates autoregressive, and in addition, moving normal parameters and unequivocally incorporates differencing in the detailing of the model.</p><p class="calibre11">In particular, the three sorts of parameters in the model are:</p><div><ul class="itemizedlist"><li class="listitem">The autoregressive parameters (p)</li><li class="listitem">The quantity of differencing passes (d)</li><li class="listitem">Moving normal parameters (q)</li></ul></div><p class="calibre11">In the documentation presented by Box and Jenkins, models are abridged as ARIMA (p, d, q).</p><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec6" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Identification</h4></div></div></div><p class="calibre11">The input series for ARIMA should be stationary. It must have a steady mean, difference, and autocorrelation through time. Subsequently, for the most part, the series first should be differenced until it is stationary (this, likewise, regularly requires log changing the data to make the variance stable).</p><p class="calibre11">The quantity of times the series should be differenced to accomplish stationarity is reflected in the "d" parameter. So as to decide the fundamental level of differencing, we need to look at the plot of the data and auto-correlogram.</p><p class="calibre11">Noteworthy changes in level (solid upward or descending changes) ordinarily require first order nonseasonal (lag=1) differencing:</p><div><ul class="itemizedlist"><li class="listitem"> Solid changes of incline generally require second order nonseasonal differencing</li></ul></div></div><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec7" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Estimation and forecasting</h4></div></div></div><p class="calibre11">The next step is estimation. Here the parameters are assessed (utilizing function minimization systems) so that the sum of squared residuals is minimized. The assessments of the parameters are utilized as a part of the last stage (forecasting) to compute new estimations of the series (past those incorporated into the input dataset) and confidence intervals for those predicted values.</p><p class="calibre11">The estimation procedure is performed on changed (differenced) information before the forecasted figures are produced. It is required that the series should be integrated so that the forecasts are communicated in values compatible with the input data.</p></div><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec8" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The constant in ARIMA models</h4></div></div></div><p class="calibre11">With the standard autoregressive and moving normal parameters, ARIMA models may likewise incorporate a constant. The representation of this constant is dependent on the model that is fit:</p><div><ul class="itemizedlist"><li class="listitem">If in the model there are no autoregressive parameters present, then the mean of the series is the expected value of the constant</li><li class="listitem">If in the model there are autoregressive parameters present, then the intercept is represented by the constant</li></ul></div></div><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec9" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Identification phase</h4></div></div></div><p class="calibre11">Before the estimation can start, we have to settle on (distinguish) the particular number and kind of ARIMA parameters to be evaluated. The significant instruments utilized as a part of the ID stage are:</p><div><ul class="itemizedlist"><li class="listitem">Plots of the arrangement</li><li class="listitem">Correlograms of autocorrelation (ACF)</li><li class="listitem">Partial autocorrelation (PACF)</li></ul></div><p class="calibre11">The choice is not direct and in less typical cases requires experience as well as a decent arrangement of experimentation with option models (and also the specialized parameters of ARIMA).</p><p class="calibre11">In any case, a good amount of experimental time series patterns can be adequately approximated utilizing one of the five fundamental models. These models are based on the shape of the <strong class="calibre19">autocorrelogram</strong> (<strong class="calibre19">ACF</strong>) and <strong class="calibre19">partial auto correlogram</strong> (<strong class="calibre19">PACF</strong>):</p><div><ul class="itemizedlist"><li class="listitem">One autoregressive parameter (p):<div><ul class="itemizedlist1"><li class="listitem">ACF: Exponential decay</li><li class="listitem">PACF: Spike at lag 1</li><li class="listitem">No correlation for other lags</li></ul></div><p class="calibre44">
</p></li><li class="listitem">Two autoregressive parameters (p):<div><ul class="itemizedlist1"><li class="listitem">ACF: A sine-wave shape pattern or a set of exponential decays</li><li class="listitem">PACF: Spikes at lags 1 and 2</li><li class="listitem">No correlation for other lags</li></ul></div><p class="calibre44">
</p></li><li class="listitem">One moving average parameter (q):<div><ul class="itemizedlist1"><li class="listitem">ACF: Spike at lag 1</li><li class="listitem">No correlation for other lags</li><li class="listitem">PACF: Damps out exponentially</li></ul></div><p class="calibre44">
</p></li><li class="listitem">Two moving average parameters (q):<div><ul class="itemizedlist1"><li class="listitem">ACF: Spikes at lags 1 and 2</li><li class="listitem">No correlation for other lags</li><li class="listitem">PACF: A sine-wave shape pattern or a set of exponential decays</li></ul></div><p class="calibre44">
</p></li><li class="listitem">One autoregressive (p) and one moving average (q) parameter:<div><ul class="itemizedlist1"><li class="listitem">ACF: Exponential decay starting at lag 1</li><li class="listitem">PACF: Exponential decay starting at lag 1</li></ul></div><p class="calibre44">
</p></li></ul></div></div><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec10" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Seasonal models</h4></div></div></div><p class="calibre11">A series in which a pattern repeats seasonally over time requires special models.</p><p class="calibre11">This is similar to the simple ARIMA parameters in seasonal models:</p><div><ul class="itemizedlist"><li class="listitem">Seasonal autoregressive (ps)</li><li class="listitem">Seasonal differencing (ds)</li><li class="listitem">Seasonal moving normal parameters (qs)</li></ul></div><p class="calibre11">For instance, let's take the model (0,1,2)(0,1,1).</p><p class="calibre11">This depicts a model that incorporates:</p><div><ul class="itemizedlist"><li class="listitem">No autoregressive parameters</li><li class="listitem">Two general moving normal parameters</li><li class="listitem">One regular moving normal parameter</li></ul></div><p class="calibre11">The seasonal lag utilized for the seasonal parameters is normally decided amid the identification proof stage and should be expressly indicated.</p><p class="calibre11">The general suggestions concerning the choice of parameters to be assessed (taking into account ACF and PACF) likewise apply to seasonal models. The principle distinction is that in seasonal series, ACF and PACF will indicate sizable coefficients at products of the seasonal lag.</p></div></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec79" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Parameter estimation</h3></div></div></div><p class="calibre11">There are a few distinct techniques for assessing the parameters. Every one of them should fundamentally deliver the same estimates, yet might be pretty much proficient for any given model. Generally, amid the parameter estimation stage, a function minimization calculation is utilized to maximize the probability (likelihood) of the watched series given the parameter values.</p><p class="calibre11">This requires the computing of the (conditional) aggregates of squares (SS) of the residuals, given the separate parameters.</p><p class="calibre11">In practice, this requires the calculation of the (conditional) sums of squares (SS) of the residuals, given the respective parameters.</p><p class="calibre11">Different methods have been proposed to compute the SS for the residuals:</p><div><ul class="itemizedlist"><li class="listitem">The approximate maximum likelihood method according to McLeod and Sales (1983)</li><li class="listitem">The approximate maximum likelihood method with backcasting</li><li class="listitem">The exact maximum likelihood method according to Melard (1984)</li></ul></div></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec80" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Evaluation of the model</h3></div></div></div><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Parameter estimates</strong>:<div><ul class="itemizedlist1"><li class="listitem">Report surmised <em class="calibre23">t</em> values, figured from the parameter standard errors</li><li class="listitem">If there is no significance, then the separate parameter can most of the time be dropped from the model without considerably influencing the overall fit of the model</li></ul></div><p class="calibre44">
</p></li><li class="listitem"><strong class="calibre19">Other quality criteria</strong>: Another clear and normal measure of the quality of the model is the exactness of its forecasts created taking into account partial data so that the forecasts can be contrasted and known (unique) observations</li></ul></div></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec81" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Interrupted time series ARIMA</h3></div></div></div><p class="calibre11">We might want to assess the effect of one or more discrete occasions on the qualities in the time series. These kinds of interruptions on time series analysis are portrayed in subtle elements in McDowall, McCleary, Meidinger, and Hay (1980). McDowall, et al., recognize three noteworthy sorts of effects that are conceivable:</p><div><ul class="itemizedlist"><li class="listitem">Permanent abrupt</li><li class="listitem">Permanent gradual</li><li class="listitem">Abrupt temporary</li></ul></div></div></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec122" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Exponential smoothing</h2></div></div></div><p class="calibre11">Exponential smoothing has turned out to be exceptionally well known as a forecasting strategy for various types of time series data. The strategy was freely created by Brown and Holt. Brown worked for the US Navy during World War II, where his task was to design a tracking system for fire-control information to compute the location of submarines. Later, he connected this strategy to the forecasting of interest for spare parts (an inventory control issue).</p><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec82" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Simple exponential smoothing</h3></div></div></div><p class="calibre11">A simple model for a time series t would be to consider every observation as comprising of a constant (<em class="calibre23">b</em>) and an error component (epsilon), that is: <em class="calibre23">Xt = b + t</em>.</p><p class="calibre11">The constant b is generally steady in every fragment of the series; however, it may change gradually after some time. On the off chance of fitting, then one approach to seclude the genuine estimation of b, and therefore the orderly or unsurprising part of the series, is to figure a sort of moving normal, where the current and quickly new observations are doled out more prominent weight than the particular older observations.</p><p class="calibre11">Exponential smoothing fulfills precisely such weighting where exponentially smaller weights are doled out to older observations. The specific formula for simple exponential smoothing is as follows:</p><p class="calibre11">
<em class="calibre23">St = a*Xt + (1-a)*St-1</em>
</p><p class="calibre11">At the point when connected recursively to each progressive observation in the series, each new smoothed value (forecast) is figured as the weighted normal of the present observation and the past smoothed observation.</p><p class="calibre11">The past smoothed observation was processed from the past observed value and the smoothed quality before the past perception, and so on. Subsequently, each smoothed value is the weighted normal of the past perceptions where the weights decrease exponentially relying upon the estimation of the parameter (alpha).</p><p class="calibre11">On the off chance that it is equivalent to 1 (one), then the past perceptions are overlooked altogether; if it is equivalent to 0 (zero), then the present perception is disregarded totally, and the smoothed worth comprises completely of the past smoothed quality (which is processed from the smoothed perception before it, and so on; along these lines every single smoothed quality will be equivalent to the underlying smoothed worth S0). Estimations in the middle will deliver transitional results:</p><div><ul class="itemizedlist"><li class="listitem">If it is equal to 1, then the past observations are ignored completely</li><li class="listitem">If it is equal to 0, then the current observation is ignored completely:<div><ul class="itemizedlist1"><li class="listitem">The smoothed value consists completely of the past smoothed value (which, in turn, is computed from the smoothed observation before it, and so on; thus all smoothed values will be equal to the initial smoothed value, S0). Estimations in the middle will deliver transitional results.</li></ul></div><p class="calibre44">
</p></li></ul></div><p class="calibre11">The hypothetical model for the process hidden in the observed time series, simple exponential smoothing, will frequently create precise forecasts.</p></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec83" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Indices of lack of fit (error)</h3></div></div></div><p class="calibre11">The most direct method for assessing the exactness of the forecasts in light of a specific value is to just plot the observed values and the one-stage ahead of forecasts. This plot can likewise incorporate the residuals (scaled against the right <em class="calibre23">Y</em> axis), so that locales of better or most noticeably bad fit can likewise effectively be distinguished.</p><p class="calibre11">This visual check of the precision of forecasts is frequently the most intense technique for figuring out if or not the present exponential smoothing model fits the data:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Mean error</strong>: The <strong class="calibre19">mean error</strong> (<strong class="calibre19">ME</strong>) quality is essentially processed as the normal error value (normal of observed minus one-stage ahead forecast):<div><ul class="itemizedlist1"><li class="listitem">Clearly, a downside of this measure is that positive and negative error values can counterbalance each other, so this measure is not a decent marker of general fit.</li></ul></div><p class="calibre44">
</p></li><li class="listitem"><strong class="calibre19">Mean absolute error:</strong> The <strong class="calibre19">mean absolute error</strong> (<strong class="calibre19">MAE</strong>) value is processed as the normal absolute error value:<div><ul class="itemizedlist1"><li class="listitem">If the value is 0 (zero) then the fit (forecast) is considered perfect.</li><li class="listitem">Compared with the mean squared error value, this measure of fit will neglect anomalies, therefore, one of a kind or uncommon large error values will influence the MAE less than the MSE value.</li></ul></div><p class="calibre44">
</p></li><li class="listitem">Sum<strong class="calibre19"> of squared error (SSE) and mean squared error</strong>: These values are calculated as the aggregate (or normal) of the squared error values. This is the most normally utilized absence of-fit indicator as a part of statistical fitting strategies.</li><li class="listitem"><strong class="calibre19">Percentage error (PE)</strong>: All the preceding measures depend on the actual error value. It might appear to be sensible to rather express the absence of fit as far as the relative deviation of the one-stage ahead forecasts from the observed values, which is with respect to the magnitude of the observed values.<div><ul class="itemizedlist1"><li class="listitem">For instance, when attempting to foresee month-to-month deals that may fluctuate generally from month to month, we might be fulfilled if our expectation "hits the objective" with about ±10% precision. At the end of the day, the outright errors might be of less interest, but rather more are the relative errors in the forecasts:</li></ul></div><p class="calibre44">
</p></li></ul></div><p class="calibre11">
<em class="calibre23">PEt = 100*(Xt - Ft )/Xt</em>
</p><p class="calibre11">Here <em class="calibre23">Xt</em> is the observed value at time <em class="calibre23">t</em>, and <em class="calibre23">Ft</em> is the forecasts (smoothed values).</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Mean percentage error (MPE)</strong>: This value is computed as the average of the PE values.</li><li class="listitem"><strong class="calibre19">Mean absolute percentage error (MAPE)</strong>: As is the situation with the mean error value, a mean percentage error close to 0 (zero) can be created by substantial positive and negative rate percentage errors that offset each other. Consequently, a superior measure of relative general fit is the mean absolute percentage error. Additionally, this measure is generally more significant than the mean squared mistake:<div><ul class="itemizedlist1"><li class="listitem">For instance, realizing that the normal forecast is "off" by ±5% is a helpful result all by itself, though a mean squared error of 30.8 is not quickly interpretable.</li></ul></div><p class="calibre44">
</p></li><li class="listitem"><strong class="calibre19">Automatic search for best parameter</strong>: A quasi-Newton function minimization procedure (the same as in ARIMA) is utilized to minimize either the mean squared error, mean absolute error, or mean total rate error.</li><li class="listitem"><strong class="calibre19">The initially smoothed value S0</strong>: We require a S0 value keeping in mind the end goal to process the smoothed quality (forecast) for the main observation in the series. Depending upon the decision of the parameter (that is, when it is near zero), the underlying value for the smoothing procedure can influence the nature of the forecasts for some observations.</li></ul></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch09lvl1sec73" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Implementation in Julia</h1></div></div></div><p class="calibre11">TimeSeries is a registered package. So like other packages, we can add it to your Julia packages:</p><pre class="programlisting">
<strong class="calibre19">Pkg.update() 
Pkg.add("TimeSeries")</strong>
</pre><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec123" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The TimeArray time series type</h2></div></div></div><pre class="programlisting">immutable TimeArray{T, N, D&lt;:TimeType, A&lt;:AbstractArray} &lt;: AbstractTimeSeries

 timestamp::Vector{D}
 values::A
 colnames::Vector{UTF8String}
 meta::Any

 function TimeArray(timestamp::Vector{D},
 values::AbstractArray{T,N},
 colnames::Vector{UTF8String},
 meta::Any)
 nrow, ncol = size(values, 1), size(values, 2)
 nrow != size(timestamp, 1) ? error("values must match length of
 timestamp"):
 ncol != size(colnames,1) ? error("column names must match width of
 array"):
 timestamp != unique(timestamp) ? error("there are duplicate dates"):
 ~(flipdim(timestamp, 1) == sort(timestamp) || timestamp ==
 sort(timestamp)) ? error("dates are mangled"):
 flipdim(timestamp, 1) == sort(timestamp) ? 
 new(flipdim(timestamp, 1), flipdim(values, 1), colnames, meta):
 new(timestamp, values, colnames, meta)
 end
end</pre><p class="calibre11">There are four fields for the type:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">timestamp</code>: The timestamp field consists of a vector of values of a child type of <code class="literal">TimeType</code>, in practice, either <code class="literal">Date</code> or <code class="literal">DateTime</code>. The <code class="literal">DateTime</code> type is similar to the Date type except it represents time frames smaller than a day. For the construction of a TimeArray to work, this vector needs to be sorted. If the vector includes dates that are not sequential, the construction of the object will error out. The vector also needs to be ordered from the oldest to latest date, but this can be handled by the constructor and will not prohibit an object from being created.</li><li class="listitem"><code class="literal">values</code>: The values field holds the data from the time series and its row count must match the length of the timestamp array. If these do not match, the constructor will fail. All the values inside the values array must be of the same type.</li><li class="listitem"><code class="literal">colnames</code>: The <code class="literal">colnames</code> field is a vector of type UTF8 String and contains the names of the columns for each column in the values field. The length of this vector must match the column count of the values array, or the constructor will fail.</li><li class="listitem"><code class="literal">meta</code>:<strong class="calibre19"> </strong>The meta field defaults to holding nothing, which is represented by type Void. This default is designed to allow programmers to ignore this field. For those who wish to utilize this field, <code class="literal">meta</code> can hold common types such as String or more elaborate user-defined types. One might want to assign a name to an object that is immutable versus relying on variable bindings outside of the object's type fields.</li></ul></div><p class="calibre11">We'll be using historical financial datasets available in the <code class="literal">MarketData</code> package:</p><pre class="programlisting">
<strong class="calibre19">Pkg.add("MarketData") 
using TimeSeries 
using MarketData 
</strong>
</pre><p class="calibre11">Now let's go through the data:</p><pre class="programlisting">ohlc[1] 
</pre><p class="calibre11">This produces the following output:</p><pre class="programlisting">1x4 TimeSeries.TimeArray{Float64,2,Date,Array{Float64,2}} 2000-01-03 to 2000-01-03 
 
             Open      High      Low       Close      
2000-01-03 | 104.88    112.5     101.69    111.94 
</pre><p class="calibre11">Let's go through some more records and statistics:</p><pre class="programlisting">ohlc[[1:3;9]] 
</pre><p class="calibre11">This produces the following output:</p><pre class="programlisting">4x4 TimeSeries.TimeArray{Float64,2,Date,Array{Float64,2}} 2000-01-03 to 2000-01-13 
 
             Open      High      Low       Close      
2000-01-03 | 104.88    112.5     101.69    111.94     
2000-01-04 | 108.25    110.62    101.19    102.5      
2000-01-05 | 103.75    110.56    103.0     104.0      
2000-01-13 | 94.48     98.75     92.5      96.75   
</pre><p class="calibre11">We can also go through them using the column names:</p><pre class="programlisting">500x2 TimeSeries.TimeArray{Float64,2,Date,Array{Float64,2}} 2000-01-03 to 2001-12-31 
 
             Open      Close      
2000-01-03 | 104.88    111.94     
2000-01-04 | 108.25    102.5      
2000-01-05 | 103.75    104.0      
2000-01-06 | 106.12    95.0       
 
2001-12-26 | 21.35     21.49      
2001-12-27 | 21.58     22.07      
2001-12-28 | 21.97     22.43      
2001-12-31 | 22.51     21.9  
</pre><p class="calibre11">To access the records using Date, it can be done as follows:</p><pre class="programlisting">ohlc[[Date(2000,1,3),Date(2000,2,4)]] 
</pre><p class="calibre11">It will give the following output:</p><pre class="programlisting">2x4 TimeSeries.TimeArray{Float64,2,Date,Array{Float64,2}} 2000-01-03 to 2000-02-04 
 
             Open      High      Low       Close      
2000-01-03 | 104.88    112.5     101.69    111.94     
2000-02-04 | 103.94    110.0     103.62    108.0 
</pre><p class="calibre11">We can also list the records over the range of the dates:</p><pre class="programlisting">ohlc[Date(2000,1,10):Date(2000,2,10)] 
</pre><p class="calibre11">It produces the following output:</p><pre class="programlisting">23x4 TimeSeries.TimeArray{Float64,2,Date,Array{Float64,2}} 2000-01-10 to 2000-02-10 
 
             Open      High      Low       Close      
2000-01-10 | 102.0     102.25    94.75     97.75      
2000-01-11 | 95.94     99.38     90.5      92.75      
2000-01-12 | 95.0      95.5      86.5      87.19      
2000-01-13 | 94.48     98.75     92.5      96.75      
 
2000-02-07 | 108.0     114.25    105.94    114.06     
2000-02-08 | 114.0     116.12    111.25    114.88     
2000-02-09 | 114.12    117.12    112.44    112.62     
2000-02-10 | 112.88    113.88    110.0     113.5 
</pre><p class="calibre11">We can also use two different columns:</p><pre class="programlisting">ohlc["Open"][Date(2000,1,10)] 
</pre><p class="calibre11">It produces the following output:</p><pre class="programlisting">1x1 TimeSeries.TimeArray{Float64,1,Date,Array{Float64,1}} 2000-01-10 to 2000-01-10 
 
             Open       
2000-01-10 | 102 
</pre></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec124" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Using time constraints</h2></div></div></div><p class="calibre11">There are some specific methods that can segment on time ranges if the condition is met.</p><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec84" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>when</h3></div></div></div><p class="calibre11">The when method allows aggregating elements from a <code class="literal">TimeArray</code> into specific time periods.</p><p class="calibre11">For example: <code class="literal">dayofweek</code> or <code class="literal">month</code>. Here are some dates methods with examples:</p><pre class="programlisting">day   Jan 3, 2000 = 3 
dayname  Jan 3, 2000 = "Monday" 
week  Jan 3, 2000 = 1 
month Jan 3, 2000 = 1 
monthname   Jan 3, 2000 = "January" 
year  Jan 3, 2000 = 2000 
dayofweek   Monday = 1 
dayofweekofmonth  Fourth Monday in Jan = 4 
dayofyear   Dec 31, 2000 = 366 
quarterofyear  Dec 31, 2000 = 4 
dayofquarter   Dec 31, 2000 = 93 
</pre></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec85" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>from</h3></div></div></div><pre class="programlisting">from(cl, Date(2001, 10, 24)) 
</pre><p class="calibre11">This will give the following output:</p><pre class="programlisting">47x1 TimeSeries.TimeArray{Float64,1,Date,Array{Float64,1}} 2001-10-24 to 2001-12-31 
 
             Close     
2001-10-24 | 18.95     
2001-10-25 | 19.19     
2001-10-26 | 18.67     
2001-10-29 | 17.63     
 
2001-12-26 | 21.49     
2001-12-27 | 22.07     
2001-12-28 | 22.43     
2001-12-31 | 21.9  
</pre></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec86" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>to</h3></div></div></div><pre class="programlisting">to(cl, Date(2000, 10, 24))</pre><p class="calibre11">This code will generate the following output:</p><pre class="programlisting">206x1 TimeSeries.TimeArray{Float64,1,Date,Array{Float64,1}} 2000-01-03 to 2000-10-24 
 
             Close      
2000-01-03 | 111.94     
2000-01-04 | 102.5      
2000-01-05 | 104.0      
2000-01-06 | 95.0       
 
2000-10-19 | 18.94      
2000-10-20 | 19.5       
2000-10-23 | 20.38      
2000-10-24 | 18.88 
</pre></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec87" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>findwhen</h3></div></div></div><p class="calibre11">This is probably one of the most used and efficient methods. It tests a condition and returns the <code class="literal">Date</code> or <code class="literal">DateTime</code> vector:</p><pre class="programlisting">red = findwhen(ohlc["Close"] .&lt; ohlc["Open"]); 
</pre><p class="calibre11">This will generate the following output:</p><pre class="programlisting">252x4 TimeSeries.TimeArray{Float64,2,Date,Array{Float64,2}} 2000-01-04 to 2001-12-31 
 
             Open      High      Low       Close      
2000-01-04 | 108.25    110.62    101.19    102.5      
2000-01-06 | 106.12    107.0     95.0      95.0       
2000-01-10 | 102.0     102.25    94.75     97.75      
2000-01-11 | 95.94     99.38     90.5      92.75      
 
2001-12-14 | 20.73     20.83     20.09     20.39      
2001-12-20 | 21.4      21.47     20.62     20.67      
2001-12-21 | 21.01     21.54     20.8      21.0       
2001-12-31 | 22.51     22.66     21.83     21.9  
</pre></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec88" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>find</h3></div></div></div><p class="calibre11">The <code class="literal">find</code> method is similar to <code class="literal">findwhen</code>. It tests a condition and returns a vector of Int, representing the row in the array where the condition is true:</p><pre class="programlisting">green = find(ohlc["Close"] .&gt; ohlc["Open"]); 
</pre><p class="calibre11">This will generate the following output:</p><pre class="programlisting">244x4 TimeSeries.TimeArray{Float64,2,Date,Array{Float64,2}} 2000-01-03 to 2001-12-28 
 
             Open      High      Low       Close      
2000-01-03 | 104.88    112.5     101.69    111.94     
2000-01-05 | 103.75    110.56    103.0     104.0      
2000-01-07 | 96.5      101.0     95.5      99.5       
2000-01-13 | 94.48     98.75     92.5      96.75      
 
2001-12-24 | 20.9      21.45     20.9      21.36      
2001-12-26 | 21.35     22.3      21.14     21.49      
2001-12-27 | 21.58     22.25     21.58     22.07      
2001-12-28 | 21.97     23.0      21.96     22.43 
</pre></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec89" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Mathematical, comparison, and logical operators</h3></div></div></div><p class="calibre11">These methods are also supported by the TimeSeries package.</p><p class="calibre11">To use mathematical operators:</p><div><ul class="itemizedlist"><li class="listitem">+ or .+: Mathematical element-wise addition</li><li class="listitem">- or .-: Mathematical element-wise subtraction</li><li class="listitem">* or .*: Mathematical element-wise multiplication</li><li class="listitem">./: Mathematical element-wise division</li><li class="listitem">.^: Mathematical element-wise exponentiation</li><li class="listitem">% or .%: Mathematical element-wise remainder</li></ul></div><p class="calibre11">To use comparison operators:</p><div><ul class="itemizedlist"><li class="listitem">.&gt; element-wise greater-than comparison</li><li class="listitem">.&lt; element-wise less-than comparison</li><li class="listitem">.== element-wise equivalent comparison</li><li class="listitem">.&gt;= element-wise greater-than or equal comparison</li><li class="listitem">.&lt;= element-wise less-than or equal comparison</li><li class="listitem">.!= element-wise not-equivalent comparison</li></ul></div><p class="calibre11">To use logical operators:</p><div><ul class="itemizedlist"><li class="listitem">&amp; element-wise logical AND</li><li class="listitem">| element-wise logical OR</li><li class="listitem">!, ~ element-wise logical NOT</li><li class="listitem">$ element-wise logical XOR</li></ul></div></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec90" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Applying methods to TimeSeries</h3></div></div></div><p class="calibre11">Common transformation of time series data involves:</p><div><ul class="itemizedlist"><li class="listitem">Lagging</li><li class="listitem">Leading</li><li class="listitem">Calculating change</li><li class="listitem">Windowing operations and aggregation operations</li></ul></div><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec11" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Lag</h4></div></div></div><p class="calibre11">The <code class="literal">lag</code> method is putting yesterday's value in today's timestamp:</p><pre class="programlisting">cl[1:4] 
 
#Output 
4x1 TimeSeries.TimeArray{Float64,1,Date,Array{Float64,1}} 2000-01-03 to 2000-01-06 
 
             Close      
2000-01-03 | 111.94     
2000-01-04 | 102.5      
2000-01-05 | 104.0      
2000-01-06 | 95.0 
</pre><p class="calibre11">It is applying lag on this:</p><pre class="programlisting">lag(cl[1:4]) 
</pre><p class="calibre11">It generates the following output:</p><pre class="programlisting">3x1 TimeSeries.TimeArray{Float64,1,Date,Array{Float64,1}} 2000-01-04 to 2000-01-06 
 
             Close      
2000-01-04 | 111.94     
2000-01-05 | 102.5      
2000-01-06 | 104.0 
</pre></div><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec12" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Lead</h4></div></div></div><p class="calibre11">Lead is opposite to the lag:</p><pre class="programlisting">lead(cl[1:4]) 
</pre><p class="calibre11">The output generated is as follows:</p><pre class="programlisting">3x1 TimeSeries.TimeArray{Float64,1,Date,Array{Float64,1}} 2000-01-03 to 2000-01-05 
 
             Close      
2000-01-03 | 102.5      
2000-01-04 | 104.0      
2000-01-05 | 95.0 
</pre><p class="calibre11">As the cl is 500 rows long we can lead till that. For now, we will lead by 400:</p><pre class="programlisting">lead(cl, 400) 
</pre><p class="calibre11">The output generated is as follows:</p><pre class="programlisting">100x1 TimeSeries.TimeArray{Float64,1,Date,Array{Float64,1}} 2000-01-03 to 2000-05-24 
 
             Close     
2000-01-03 | 19.5      
2000-01-04 | 19.13     
2000-01-05 | 19.25     
2000-01-06 | 18.9      
 
2000-05-19 | 21.49     
2000-05-22 | 22.07     
2000-05-23 | 22.43     
2000-05-24 | 21.9 
</pre></div><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec13" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Percentage</h4></div></div></div><p class="calibre11">One of the most common time series operations is to calculate the change in percentage:</p><pre class="programlisting">percentchange(cl) 
</pre><p class="calibre11">The output generated is as follows:</p><pre class="programlisting">499x1 TimeSeries.TimeArray{Float64,1,Date,Array{Float64,1}} 2000-01-04 to 2001-12-31 
 
             Close    
2000-01-04 | -0.0843  
2000-01-05 | 0.0146   
2000-01-06 | -0.0865  
2000-01-07 | 0.0474   
 
2001-12-26 | 0.0061   
2001-12-27 | 0.027    
2001-12-28 | 0.0163   
2001-12-31 | -0.0236 
</pre><p class="calibre11">This shows the percentage change from the previous record.</p></div></div><div><div><div><div><h3 class="title5"><a id="ch09lvl3sec91" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Combining methods in TimeSeries</h3></div></div></div><p class="calibre11">Two <code class="literal">TimeArrays</code> can be merged to generate a meaningful array.</p><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec14" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Merge</h4></div></div></div><p class="calibre11">Merge joins two TimeArrays. By default, it joins using an inner join:</p><pre class="programlisting">merge(op[1:4], cl[2:6], :left) 
</pre><p class="calibre11">The output generated is as follows:</p><pre class="programlisting">4x2 TimeSeries.TimeArray{Float64,2,Date,Array{Float64,2}} 2000-01-03 to 2000-01-06 
 
             Open      Close      
2000-01-03 | 104.88    NaN        
2000-01-04 | 108.25    102.5      
2000-01-05 | 103.75    104.0      
2000-01-06 | 106.12    95.0 
</pre><p class="calibre11">In the previous example, we provided the type of join we want to perform. We can also do right or outer joins.</p></div><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec15" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Collapse</h4></div></div></div><p class="calibre11">The <code class="literal">collapse</code> method is used to compress data into a larger time frame.</p></div><div><div><div><div><h4 class="title8"><a id="ch09lvl4sec16" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Map</h4></div></div></div><p class="calibre11">This is used for transformations in the timeseries data. The first argument of this method is a binary function (the time stamp and the values). This method returns two values, respectively the new time stamp and the new vector of values:</p><pre class="programlisting">a = TimeArray([Date(2015, 10, 24), Date(2015, 11, 04)], [15, 16], ["Number"]) 
</pre><p class="calibre11">The output generated is as follows:</p><pre class="programlisting"> 2x1 TimeSeries.TimeArray{Int64,1,Date,Array{Int64,1}} 2015-10-24 to 2015-11-04 
 
             Number    
2015-10-24 | 15        
2015-11-04 | 16 
</pre><p class="calibre11">You apply the map method as follows:</p><pre class="programlisting">map((timestamp, values) -&gt; (timestamp + Dates.Year(1), values), a) 
</pre><p class="calibre11">This transforms the records for the particular time provided:</p><pre class="programlisting">2x1 TimeSeries.TimeArray{Int64,1,Date,Array{Int64,1}} 2016-10-24 to 2016-11-04 
 
             Number    
2016-10-24 | 15        
2016-11-04 | 16 
</pre></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch09lvl1sec74" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we learned about what is forecasting and why it is needed in a business. Forecasting helps to identify the demand and take necessary steps, as well as in other domains it helps to predict weather, and so on. The decision-making process is highly affected by the results of forecasting. Time Series is the arrangement of insights, typically gathered at standard intervals. It has been used in various domains such as medical, weather, finance markets, and so on.</p><p class="calibre11">We also learned about the different types of models and how to analyze trends in Time Series. We also took into consideration seasonality effects on the Time series analysis. We discussed ARIMA in detail and also explored the Time Series library of Julia.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch09lvl1sec75" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://timeseriesjl.readthedocs.io/en/latest/">http://timeseriesjl.readthedocs.io/en/latest/</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://documents.software.dell.com/statistics/textbook/time-series-analysis">https://documents.software.dell.com/statistics/textbook/time-series-analysis</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://home.ubalt.edu/ntsbarsh/stat-data/forecast.htm">http://home.ubalt.edu/ntsbarsh/stat-data/forecast.htm</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://userwww.sfsu.edu/efc/classes/biol710/timeseries/timeseries1.htm">http://userwww.sfsu.edu/efc/classes/biol710/timeseries/timeseries1.htm</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://onlinecourses.science.psu.edu/stat510/node/47">https://onlinecourses.science.psu.edu/stat510/node/47</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm">http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm</a></li></ul></div></div></div>



  </body></html>