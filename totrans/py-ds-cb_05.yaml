- en: Chapter 5. Data Mining – Needle in a Haystack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with distance measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning and using kernel methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering data using the k-means method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning vector quantization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding outliers in univariate data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering outliers using the local outlier factor method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus mostly on unsupervised data mining algorithms.
    We will start with a recipe covering various distance measures. Understanding
    distance measures and various spaces is critical when building data science applications.
    Any dataset is usually a set of points that are objects belonging to a particular
    space. We can define space as a universal set of points from which the points
    in our dataset are drawn. The most often encountered space is Euclidean. In Euclidean
    space, the points are vectors real number. The length of the vector denotes the
    number of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: We then have a recipe introducing kernel methods. Kernel methods are a very
    important topic in machine learning. They help us solve nonlinear data problems
    using linear methods. We will introduce the concept of the kernel trick.
  prefs: []
  type: TYPE_NORMAL
- en: We will follow it with some clustering algorithm recipes. Clustering is the
    process of partitioning a set of points into logical groups. For example, in a
    supermarket scenario, items are grouped into categories qualitatively. However,
    we will look at quantitative approaches. Specifically, we will focus our attention
    on the k-means algorithm and discuss its limitations and advantages.
  prefs: []
  type: TYPE_NORMAL
- en: Our next recipe is an unsupervised technique called learning vector quantization.
    It can be used both for clustering and classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will look at the outlier detection methods. Outliers are those observations
    in a dataset that differ significantly from the other observations in that dataset.
    It is very important to study these outliers as they might be indicative of unusual
    phenomena or errors in the underlying process that is generating the data. When
    machine learning models are fitted over data, it is important to understand how
    to handle outliers before passing the data to algorithms. We will concentrate
    on a few empirical outlier detection techniques in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will rely heavily on the Python libraries, NumPy, SciPy, matplotlib, and
    scikit-learn for most of our recipes. We will also change our coding style from
    scripting to writing procedures and classes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Working with distance measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distance and similarity measures are key to various data mining tasks. In this
    recipe, we will see some distance measures in action. Our next recipe will cover
    similarity measures. Let's define a distance measure before we look at the various
    distance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'As data scientists, we are always presented with points or vectors of different
    dimensions. Mathematically, a set of points is defined as a space. A distance
    measure in this space is defined as a function d(x,y), which takes two points
    x and y as arguments in this space and gives a real number as the output. The
    distance function, that is, the real number output, should satisfy the following
    axioms:'
  prefs: []
  type: TYPE_NORMAL
- en: The distance function output should be non-negative, d(x,y) >= 0
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of the distance function should be zero only when x = y
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The distance should be symmetric, that is, d(x,y) = d(y,x)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The distance should obey the triangle inequality, that is, d(x,y) <= d(x,z)
    + d(z,y)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A careful look at the fourth axiom reveals that distance is the length of the
    shortest path between two points.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the following link for more information on the axioms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://en.wikipedia.org/wiki/Metric_%28mathematics%29](http://en.wikipedia.org/wiki/Metric_%28mathematics%29)'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will look at distance measures in Euclidean and non-Euclidean spaces. We
    will start with Euclidean distance and then define Lr–norm distance. Lr-norm is
    a family of distance measures of which Euclidean is a member. We will then follow
    it with the cosine distance. In non-Euclidean spaces, we will look at Jaccard's
    distance and Hamming distance.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start by defining the functions to calculate the various distance measures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write a main routine in order to invoke these various distance
    measure functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at the main function. We created a sample dataset and two vectors
    of three dimensions and invoked the `euclidean_distance` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the most common distance measure used is Euclidean distance. It belongs
    to a family of the Lr-Norm distance. A space is defined as a Euclidean space if
    the points in this space are vectors composed of real numbers. It''s also called
    the L2-norm distance. The formula for Euclidean distance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, Euclidean distance is derived by finding the distance in each
    dimension (subtracting the corresponding dimensions), squaring the distance, and
    finally taking a square root.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our code, we leverage NumPy square root and power function in order to implement
    the preceding formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Euclidean distance is strictly positive. When x is equal to y, the distance
    is zero. This should become clear from how we invoked Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we defined two NumPy arrays, `x` and `y`. We have kept them
    the same. Now, when we invoke the `euclidean_distance` function with these parameters,
    our output is zero.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now invoke the L2-norm function, `lrNorm_distance`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Lr-Norm distance metric is from a family of distance metrics of which Euclidean
    distance is a member. This should become clear as we see its formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see that we now have a parameter, `r`. Let''s substitute `r` with 2\.
    This will turn the preceding equation to a Euclidean equation. Hence, Euclidean
    is called the L2-norm distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to two vectors, we will also pass a third parameter called `power`.
    This is the `r` defined in the formula. Invoking it with a power value set to
    two will yield the Euclidean distance. You can check it by running the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will yield zero as a result, which is similar to the Euclidean distance
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Let's define two sample vectors, `x` and `y`, and invoke the `cosine_distance`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the spaces where the points are considered as directions, the cosine distance
    yields a cosine of the angle between the given input vectors as a distance value.
    Both the Euclidean space also the spaces where the points are vectors of integers
    or Boolean values, are candidate spaces where the cosine distance function can
    be applied. The cosine of the angle between the input vectors is the ratio of
    a dot product of the input vectors to the product of an L2-norm of individual
    input vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the numerator where the dot product between the input vector
    is calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the NumPy dot function to get the dot product value. The dot product
    for the two vectors, `x` and `y`, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the denominator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We again use the dot function to find the L2-norm of our input vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we can calculate the cosine of the angle between the two input vectors.
  prefs: []
  type: TYPE_NORMAL
- en: We will move on to Jaccard's distance. Similar to the previous invocations,
    we will define the sample vectors and invoke the `jaccard_distance` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'From vectors of real values, let''s move on to sets. Commonly called Jaccard''s
    coefficient, it is the ratio of the sizes of the intersection and the union of
    the given input vectors. One minus this value gives the Jaccard''s distance. As
    you can see, in the implementation, we first converted the input lists to sets.
    This will allows us to leverage the union and intersection operations provided
    by the Python set datatype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the distance is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We must use the intersection and union functionalities that are available in
    the `set` datatype in order to calculate the distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our last distance metric is the Hamming distance. With two bit vectors, the
    Hamming distance calculates how many bits have differed in these two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we used the `zip` functionality to check each of the bits and
    maintain a counter on how many bits have differed. The Hamming distance is used
    with a categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that by subtracting one from our distance values, we can arrive at
    a similarity value.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another distance that we didn't go into in detail, but is used prevalently,
    is the Manhattan or city block distance. It's an L1-norm distance. By passing
    an r value as 1 to the Lr-norm distance function, we will get the Manhattan distance.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the underlying space in which the data is placed, an appropriate
    distance measure needs to be selected. When using these distances in algorithms,
    we need to be mindful about the underlying space. For example, in the k-means
    algorithm, at every step cluster center is calculated as an average of all the
    points that are close to each other. A nice property of Euclidean is that the
    average of the points exists and as a point in the same space. Note that our input
    for the Jaccard's distance was sets. An average of the sets does not make any
    sense.
  prefs: []
  type: TYPE_NORMAL
- en: While using the cosine distance, we need to check whether the underlying space
    is Euclidean or not. If the elements of the vectors are real numbers, then the
    space is Euclidean, if they are integers, then the space is non-Euclidean. The
    cosine distance is most commonly used in text mining. In text mining, the words
    are considered as the axes, and a document is a vector in this space. The cosine
    of the angle between two document vectors denotes how similar the two documents
    are.
  prefs: []
  type: TYPE_NORMAL
- en: 'SciPy has an implementation of all these distance measures listed and much
    more at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.scipy.org/doc/scipy/reference/spatial.distance.html](http://docs.scipy.org/doc/scipy/reference/spatial.distance.html).'
  prefs: []
  type: TYPE_NORMAL
- en: The above URL lists all the distance measures supported by SciPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, the scikit-learn `pairwise` submodule provides you with a method
    called `pairwise_distance`, which can be used to find out the distance matrix
    from input records. This can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikitlearn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html](http://scikitlearn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html).'
  prefs: []
  type: TYPE_NORMAL
- en: We had mentioned that the Hamming distance is used with a categorical variable.
    A point worth mentioning here is the one-hot encoding that is used typically for
    categorical variables. After the one-hot encoding, the Hamming distance can be
    used as a similarity/distance measure between the input vectors.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Reducing data dimension with Random Projections* recipe in [Chapter 4](ch04.xhtml
    "Chapter 4. Data Analysis – Deep Dive"), *Analyzing Data - Deep Dive*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning and using kernel methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use kernel methods for data processing.
    Having the knowledge of kernels in your arsenal of methods will help you in dealing
    with nonlinear problems. This recipe is an introduction to kernel methods.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, linear models—models that can separate the data using a straight
    line or hyper plane—are easy to interpret and understand. Nonlinearity in the
    data stops us from using linear models effectively. If the data can be transformed
    into a space where the relationship becomes linear, we can use linear models.
    However, mathematical computation in the transformed space can turn into a costly
    operation. This is where the kernel functions come to our rescue.
  prefs: []
  type: TYPE_NORMAL
- en: Kernels are similarity functions. It takes two input parameters, and the similarity
    between the two inputs is the output of the kernel function. In this recipe, we
    will look at how kernel achieves this similarity. We will also discuss what is
    called a kernel trick.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally defining a kernel K is a similarity function: K(x1,x2) > 0 denotes
    the similarity of x1 and x2.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s define it mathematically before looking at the various kernels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04041_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, `xi` and, `xj` are the input vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04041_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The above mapping function is used to transform the input vectors into a new
    space. For example, if the input vector is in an n-dimensional space, the transformation
    function transforms it into a new space of dimension, m, where m >> n:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04041_05_09.jpg)![Getting ready](img/B04041_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The above image denotes the dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04041_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The above image is the dot product, `xi` and `xj` are now transformed into a
    new space by the mapping function.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see a simple kernel in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our mapping function will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04041_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When the original data is supplied to this mapping function, it transforms the
    input into the new space.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s create two input vectors and define the mapping function as described
    in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the main routine to invoke the kernel transformation. In
    the main function, we will define a kernel function and pass the input variable
    to the function, and print the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/B04041_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's follow this program from our main function. We created two input vectors,
    `x` and `y`. Both the vectors are of three dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: We then defined a mapping function. The mapping function uses the input vector
    values and transforms the input vector into a new space with an increased dimension.
    In this case, the number of the dimension is increased to nine from three.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now apply a mapping function on these vectors in order to increase their
    dimension to nine.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we print `tranf_x`, we will get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we transformed our input, x, from three dimensions to a nine-dimensional
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take the dot product in the transformed space and print its output.
  prefs: []
  type: TYPE_NORMAL
- en: The output is 313600, a scalar value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now recap: we first transformed our two input vectors into a higher
    dimensional space and then calculated the dot product in order to derive a scalar
    output.'
  prefs: []
  type: TYPE_NORMAL
- en: What we did was a very costly operation of transforming our original three-dimensional
    vector to a nine-dimensional vector and then performing the dot product operation
    on it.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we can choose a kernel function, which can arrive at the same scalar
    output without explicitly transforming the original space into a new space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our new kernel is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With two inputs, `x` and `y`, this kernel computes the dot product of the vectors,
    and squares them.
  prefs: []
  type: TYPE_NORMAL
- en: After printing the output from the kernel, we get 313600.
  prefs: []
  type: TYPE_NORMAL
- en: We never did the transformation but still were able to get the same result as
    the dot product output in the transformed space. This is called the kernel trick.
  prefs: []
  type: TYPE_NORMAL
- en: 'There was no magic in choosing this kernel. By expanding the kernel, we can
    arrive at our mapping function. Refer to the following reference for the expansion
    details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://en.wikipedia.org/wiki/Polynomial_kernel](http://en.wikipedia.org/wiki/Polynomial_kernel).'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several types of kernels. Based on our data characteristics and algorithm
    needs, we need to choose the right kernel. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear kernel: This is the simplest kind of kernel function. For two given
    inputs, it returns the dot product of the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/B04041_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Polynomial kernel: This is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/B04041_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, `x` and `y` are the input vectors, `d` is the degree of the polynomial,
    and `c` is a constant. In our recipe, we used a polynomial kernel of degree 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the scikit implementation of the linear and polynomial kernels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Using Kernel PCA* recipe in [Chapter 4](ch04.xhtml "Chapter 4. Data Analysis
    – Deep Dive"), *Analyzing Data - Deep Dive*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reducing data dimension with Random Projections* recipe in [Chapter 4](ch04.xhtml
    "Chapter 4. Data Analysis – Deep Dive"), *Analyzing Data - Deep Dive*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering data using the k-means method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will look at the k-means algorithm. K-means is a center-seeking
    unsupervised algorithm. It is an iterative non-deterministic method. What we mean
    by iterative is that the algorithm steps are repeated till the convergence of
    a specified number of steps. Non-deterministic means that a different starting
    value may lead to a different final cluster assignment. The algorithm requires
    the number of clusters, `k`, as input. There is no good way to select the value
    of `k`, it has to be determined by running the algorithm multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: For any clustering algorithm, the quality of its output is determined by inter-cluster
    cohesiveness and intra-cluster separation. Points in the same cluster should be
    close to each other; points in different clusters should be far away from each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we jump into how to write the k-means algorithm in Python, there are
    two key concepts that we need to cover that will help us understand better the
    quality of the output produced by our algorithm. First is a definition with respect
    to the quality of the clusters formed, and second is a metric that is used to
    find the quality of the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every cluster detected by k-means can be evaluated using the following measures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster location**: This is the coordinates of the cluster center. K-means
    starts with some random points as the cluster center and iteratively finds a new
    center around which points that are similar are grouped.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cluster radius**: This is the average deviation of all the points from the
    cluster center.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mass of the cluster**: This is the number of points in a cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Density of the cluster**: This is the ratio of mass of the cluster to its
    radius.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we will measure the quality of our output clusters. As mentioned previously,
    this is an unsupervised problem and we don't have labels against which to check
    our output in order to get measures such as precision, recall, accuracy, F1-score,
    or other similar metrics. The metric that we will use for our k-means algorithm
    is called a silhouette coefficient. It takes values in the range of -1 to 1\.
    Negative values indicate that the cluster radius is greater than the distance
    between the clusters so that the clusters overlap. This suggests poor clustering.
    Large values, that is, values close to 1, indicate good clustering.
  prefs: []
  type: TYPE_NORMAL
- en: A silhouette coefficient is defined for each point in the cluster. With a cluster,
    C, and a point, `i`, in this cluster, let `xi` be the average distance of this
    point from all the other points in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, calculate the average distance that the point `i` has from all the points
    in another cluster, D. Pick the smallest of these values and call it `yi`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04041_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For every cluster, the average of the silhouette coefficient of all the points
    can serve as a good measure of the cluster quality. An average of the silhouette
    coefficient of all the data points can serve as an overall quality metric for
    the clusters formed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and generate some random data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We sampled two sets of data from a normal distribution. The first set was picked
    up with a mean of `0.2` and standard deviation of `0.2`. For the second set, our
    mean value was `0.9` and standard deviation was `0.1`. Each dataset was a matrix
    of size 100 * 100—we have `100` instances and `100` dimensions. Finally, we merged
    both of them using the row stacking function from NumPy. Our final dataset was
    of size 200 * 100.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do a scatter plot of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04041_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Though we plotted only the first and second dimension, you can still clearly
    see that we have two clusters. Let's now jump into writing our k-means clustering
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's define a function that can perform the k-means clustering for the given
    data and a parameter, `k`. The function fits the clustering on the given data
    and returns an overall silhouette coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s invoke the preceding function for the different values of `k` and store
    the returned silhouette coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Finally, let's plot the silhouette coefficient for the different values of `k`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned previously, k-means is an iterative algorithm. Roughly, the steps
    of k-means are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize `k` random points from the dataset as initial center points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Do the following till the convergence of the specified number of times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the points to the closest cluster center. Typically, Euclidean distance
    is used to find the distance between a point and the cluster center.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recalculate the new cluster centers based on the assignment in this iteration.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exit the loop if a cluster assignment of the points remains the same as the
    previous iteration. The algorithm has converged to an optimal solution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will leverage the k-means implementation from the scikit-learn library.
    Our cluster function takes the k value and dataset as a parameter and runs the
    k-means algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `no_clusters` is the parameter that we will pass to the function. Using
    the init parameter, we set the initial center points as random. When init is set
    to random, scikit-learn estimates the mean and variance from the data and then
    samples k centers from a Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we must call the fit method to run k-means on our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We get the labels, that is, the cluster assignment for each point and find out
    the silhouette coefficient for all the points in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In real-world scenarios, when we start with the k-means algorithm on a dataset,
    we don''t know the number of clusters present in the data; in other words, we
    don''t know the ideal value for k. However, in our example, we know that k=2 as
    we generated the data in such a manner that it fits in two clusters. Hence, we
    need to run k-means for the different values of k:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For each run, that is, each value of k, we store the silhouette coefficient.
    A plot of k versus the silhouette coefficient reveals the ideal k value for the
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![How it works…](img/B04041_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, our silhouette coefficient is very high for k=2.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A couple of points to be noted about k-means. The k-means algorithm cannot be
    used for categorical data, k-medoids is used. Instead of averaging all the points
    in a cluster in order to find the cluster center, k-medoids selects a point that
    has the smallest average distance to all the other points in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Care needs to be taken while assigning the initial cluster. If the data is very
    dense with very widely separated clusters, and if the initial random centers are
    chosen in the same cluster, k-means may not perform very well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, k-means works if the data has star convex clusters. Refer to the
    following link for more information on star convex-shaped data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://mathworld.wolfram.com/StarConvex.html](http://mathworld.wolfram.com/StarConvex.html)'
  prefs: []
  type: TYPE_NORMAL
- en: The presence of nested or other complicated clusters will result in a junk output
    from k-means.
  prefs: []
  type: TYPE_NORMAL
- en: The presence of outliers in the data may yield poor results. A good practice
    is to do a thorough data exploration in order to identify the data characteristics
    before running k-means.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative method to initialize the centers during the beginning of the
    algorithm is the k-means++ method. So, instead of setting the init parameter to
    random, we can set it using k-means++. Refer to the following paper for k-means++:'
  prefs: []
  type: TYPE_NORMAL
- en: '*k-means++: the advantages of careful seeding*. *ACM-SIAM symposium* on *Discrete
    algorithms. 2007*'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Working with Distance Measures* recipe in [Chapter 5](ch05.xhtml "Chapter 5. Data
    Mining – Needle in a Haystack"), *Data Mining - Finding a needle in a haystack*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering outliers using the local outlier factor method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Local Outlier Factor (LOF) is an outlier detection algorithm that detects
    the outliers based on comparing the local density of the data instance with its
    neighbors. It does this in order to decide if the data instance belongs to a region
    of similar density. It can detect an outlier in a dataset, in such circumstances
    where the number of clusters are unknown and the clusters are of different density
    and sizes. It's inspired by the KNN (K-Nearest Neighbors) algorithm and is widely
    used.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous recipe, we looked at univariate data. In this one, we will use
    multivariate data and try to find outliers. Let's use a very small dataset to
    understand the LOF algorithm for outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a 5 X 2 matrix, and looking at the data, we know that the last
    tuple is an outlier. Let''s also plot it as a scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04041_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: LOF works by calculating the local density of each point. Based on the distance
    of k-nearest neighbors of a point, the local density of the point is estimated.
    By comparing the local density of the point with the densities of its neighbors,
    outliers are detected. Outliers have a low density compared with their neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to go through some term definitions in order to understand LOF:'
  prefs: []
  type: TYPE_NORMAL
- en: The k-distance of object P is the distance between the object P and its kth
    nearest neighbor. K is a parameter of the algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The k-distance neighborhood of P is the list of all the objects, Q, whose distance
    from P is either less than or equal to the distance between P and its kth nearest
    object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reachability distance from P to Q is defined as the maximum of the distance
    between P and its kth nearest neighbor, and the distance between P and Q. The
    following notation may help clarify this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The Local Reachability Density of P (LRD(P)) is the ratio of the k-distance
    neighborhood of P and the sum of the reachability distance of k and its neighborhood.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Local Outlier Factor of P (LOF(P)) is the average of the ratio of the local
    reachability of P and those of P's k-nearest neighbors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s get the `pairwise` distance between the points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s calculate the k-distance. We will use `heapq` and get the k-nearest
    neighbors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the k-distance neighborhood:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, calculate the reachability distance and LRD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate LOF:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In step 1, we select our distance metric to be Manhattan and our k value as
    two. We are looking at the second nearest neighbor for our data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'We must then proceed to calculate the pairwise distance between our tuples.
    The pairwise similarity is stored in the dist matrix. As you can see, the shape
    of dist is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: It is a 5 X 5 matrix, where the rows and columns are individual tuples and the
    cell value indicates the distance between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 2, we then import `heapq`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`heapq` is a data structure that is also known as a priority queue. It is similar
    to a regular queue except that each element is associated with a priority, and
    an element with a high priority is served before an element with a low priority.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the Wikipedia link for more information on priority queues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://en.wikipedia.org/wiki/Priority_queue](http://en.wikipedia.org/wiki/Priority_queue).'
  prefs: []
  type: TYPE_NORMAL
- en: The Python heapq documentation can be found at [https://docs.python.org/2/library/heapq.html](https://docs.python.org/2/library/heapq.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Next, we define a dictionary where the key is the tuple ID and the value is
    the distance of the tuple to its kth nearest neighbor. In our case, it should
    be the second nearest neighbor.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then enter a for loop in order to find the kth nearest neighbor''s distance
    for each of the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'From our distance matrix, we extract the ith row. As you can see, the ith row
    captures the distance between the object `i` and all the other objects. Remember
    that the cell value (`i`,`i`) holds the distance to itself. We need to ignore
    this in the next step. We must convert the array to a list for our convenience.
    Let''s try to understand this with an example. The distance matrix looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Let's assume that we are in the first iteration of our for loop and hence, our
    `i` =`0`. (remember that the Python indexing starts with `0`).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now our distances list will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: From this, we need the kth nearest neighbor, that is, the second nearest neighbor,
    as we have set K = `2` at the beginning of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at it, we can see that both index 1 and index 3 can be our the kth nearest
    neighbor as both have a value of `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we use the `heapq.nsmallest` function. Remember that we had mentioned
    that `heapq` is a normal queue but with a priority associated with each element.
    The value of the element is the priority in this case. When we say that give me
    the n smallest, `heapq` will return the smallest elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at what the `heapq.nsmallest` function does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: It returns the n smallest elements from the given dataset. In our case, we need
    the second nearest neighbor. Additionally, we need to avoid (`i`,`i`) as mentioned
    previously. So we must pass n = 3 to `heapq.nsmallest`. This ensures that it returns
    the three smallest elements. We then subset the list to exclude the first element
    (see [1:] after nsmallest function call) and finally retrieve the second nearest
    neighbor (see `[k-1]` after `[1:]`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We must also get the index of the second nearest neighbor of `i` and store
    it in our dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s print our dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Our tuples have two elements: the distance, and the index of the elements in
    the distances array. So, for instance `0`, the second nearest neighbor is the
    element in index `1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Having calculated the k-distance for all our data points, we then move on to
    find the k-distance neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 3, we find the k-distance neighborhood for each of our data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to our previous step, we import the heapq module and declare a dictionary
    that is going to hold our k-distance neighborhood details. Let''s recap what the
    k-distance neighborhood is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-distance neighborhood of P is the list of all the objects, Q, whose distance
    from P is either less than or equal to the distance between P and its kth nearest
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The first two lines should be familiar to you. We did this in our previous step.
    Look at the second line. Here, we invoked n smallest again with `n=3` in our case
    (K+1), but we selected all the elements in the output list except the first one.
    (Guess why? The answer is in the previous step.)
  prefs: []
  type: TYPE_NORMAL
- en: Let's see it in action by printing the values. As usual, in the loop, we assume
    that we are seeing the first data point or tuple where i=0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our distances list is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `heapq.nsmallest` function returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'These are 1 to k-nearest neighbor''s distances. We need to find their indices,
    a simple list.index function will only return the first match, so we will write
    the `all_indices` function in order to retrieve all the indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'With a value and list, `all_indices` will return all the indices where the
    value occurs in the list. We must convert our k smallest to a set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'So, [1.0,1.0] becomes a set ([1.0]). Now, using a for loop, we can find all
    the indices of the elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We get two indices for 1.0; they are 1 and 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The next for loop is to convert a list of the lists to a list. The `all_indices`
    function returns a list, and we then append this list to the `ksmallest_idx` list.
    Hence, we flatten it using the next for loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we add the k smallest neighborhood to our dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add tuples where the first item in the tuple is the distance and the
    second item is the index of the nearest neighbor. Let''s print the k-distance
    neighborhood dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In step 4, we calculate the LRD. The LRD is calculated using the reachability
    distance. Let''s recap both the definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The reachability distance from P to Q is defined as the maximum of the distance
    between P and its kth nearest neighbor, and the distance between P and Q. The
    following notation may help clarify this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Local Reachability density of P (LRD(P)) is the ratio of the k-distance
    neighborhood of P and the sum of the reachability distance of k and its neighborhood:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will first declare a dictionary in order to store the LRD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: For every point, we will first find the k-distance neighborhood of that point.
    For example, for i = 0, the numerator would be len (`k_distance_neig[0]`), 2.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in the inner for loop, we calculate the denominator. We then calculate
    the reachability distance for each k-distance neighborhood point. The ratio is
    stored in the `local_reach_density` dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in step 5, we calculate the LOF for each point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: For each data point, we calculate the LRD sum of its neighbor and the reachability
    distance sum with its neighbor, and multiply them to get the LOF.
  prefs: []
  type: TYPE_NORMAL
- en: 'The point with a very high LOF is considered an outlier. Let''s print `lof_list`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the last point has a very high LOF compared with the others
    and hence, it's an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can refer to the following paper in order to understand more about LOF:'
  prefs: []
  type: TYPE_NORMAL
- en: '*LOF: Identifying Density-Based Local Outliers*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, Jörg Sander*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proc. ACM SIGMOD 2000 Int. Conf. On Management of Data, Dalles, TX, 2000*'
  prefs: []
  type: TYPE_NORMAL
- en: Learning vector quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will see a model-free method for clustering the data points
    called Learning Vector Quantization, LVQ for short. LVQ can be used in classification
    tasks. Not much of an inference can be made between the target variables and prediction
    variables using this technique. Unlike the other methods, it is tough to make
    out what relationships exist between the response variable, Y, and predictor,
    X. They serve very well as a black box approach in many real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LVQ is an online learning algorithm where the data points are processed one
    at a time. It makes a very simple intuition. Assume that we have prototype vectors
    identified for the different classes present in our dataset. The training points
    will be attracted towards the prototypes of similar classes and will repel the
    other prototypes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The major steps in LVQ are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Select k initial prototype vectors for each class in the dataset. If it's a
    two-class problem and we decide to have two prototype vectors for each class,
    we will end up with four initial prototype vectors. The initial prototype vectors
    are selected randomly from the input dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We will start our iteration. Our iteration will end when our epsilon value has
    reached either zero or a predefined threshold. We will decide an epsilon value
    and decrement the epsilon value with every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each iteration, we will sample an input point (with replacement) and find
    the closest prototype vector to this point. We will use Euclidean distance to
    find the closest point. We will update the prototype vector of the closest point,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If the class label of the prototype vector is the same as the input data point,
    we will increment the prototype vector with the difference between the prototype
    vector and data point.
  prefs: []
  type: TYPE_NORMAL
- en: If the class label is different, we will decrement the prototype vector with
    the difference between the prototype vector and data point.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the Iris dataset to demonstrate how LVQ works. As in some of our
    previous recipe, we will use the convenient data loading function from scikit-learn
    in order to load the Iris dataset. Iris is a well known classification dataset.
    However our purpose of using it here is to only demonstrate LVQ's capability.
    Datasets without class lablels can also be used or processed by LVQ. As we are
    going to use Euclidean distance, we will scale the data using minmax scaling.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s first declare the parameters for LVQ:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a class to hold the prototype vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the function to find the closest prototype vector for a given vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A convenient function to find the class ID of the closest prototype vector
    is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Choose the initial K * number of classes of prototype vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform iteration to adjust the prototype vector in order to classify/cluster
    any new incoming points using the existing data points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is a small test to verify the correctness of our method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In step 1, we initialize the parameters for the algorithm. We have chosen our
    R value as two, that is, we have two prototype vectors per class label. The Iris
    dataset is a three-class problem, so we have six prototype vectors in total. We
    must choose our epsilon value and epsilon decrement factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then define a data structure to hold the details of our prototype vector
    in step 2\. Our class stores the following for each point in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The class id to which the prototype vector belongs is the vector itself and
    the epsilon value. It also has a function update that is used to change the prototype
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'In step 3, we define the following function, which takes any given vector as
    the input and a list of all the prototype vectors. Out of all the prototype vectors,
    this function returns the closest prototype vector to the given vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it loops through all the prototype vectors to find the closest
    one. It uses Euclidean distance to measure the similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 is a small function that can return the class ID of the closest prototype
    vector to the given vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have finished all the required preprocessing for the LVQ algorithm,
    we can move on to the actual algorithm in step 5\. For each class, we must select
    the initial prototype vectors. We then select R random points from each class.
    The outer loop goes through each class, and for each class, we select R random
    samples and create our prototype object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: In step 6, we increment or decrement the prototype vectors iteratively. We loop
    continuously till our epsilon value falls below a threshold of 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then randomly sample a point from our dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The point and its corresponding class ID have been retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then find the closed prototype vector to this point, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'If the current point''s class ID matches the prototype''s class ID, we call
    the `update` method, with the increment set to `True`, or else we will call the
    `update` with the increment set to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we update the epsilon value for the closest prototype vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print the prototype vectors in order to look at them manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'In step 7, we put our prototype vectors into action to do some predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: We can get the predicted class ID using the `find_class_id` function. We pass
    a point and all the learned prototype vectors to it to get the class ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we give our predicted output in order to generate a classification
    report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The classification report function is a convenient function provided by the
    scikit-learn library to view the classification accuracy scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see that we have done pretty well with our classification. Keep in mind
    that we did not keep a separate test set. Never measure the accuracy of your model
    based on the training data. Always use a test set that is unseen by the training
    routines. We did it only for illustration purposes.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keep in mind that this technique does not involve any optimization criteria
    as in the other classification methods. Hence, it is very difficult to judge how
    good the prototype vectors have been generated.
  prefs: []
  type: TYPE_NORMAL
- en: In our recipe, we initialized the prototype vectors as random values. You can
    use the k-means algorithm to initialize the prototype vectors.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Clustering of data using K-Means* recipe in [Chapter 5](ch05.xhtml "Chapter 5. Data
    Mining – Needle in a Haystack"), *Data Mining - Finding a needle in a haystack*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding outliers in univariate data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outliers are data points that are far away from the other data points in your
    data. They have to be handled carefully in data science applications. Including
    them in some of your algorithms unknowingly may lead to wrong results or conclusions.
    It is very important to account for them properly and have the right algorithms
    in order to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *"Outlier detection is an extremely important problem with a direct application
    in a wide variety of application domains, including fraud detection (Bolton, 2002),
    identifying computer network intrusions and bottlenecks (Lane, 1999), criminal
    activities in e-commerce and detecting suspicious activities (Chiu, 2003)."* |
      |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*- Jayakumar and Thomas, A New Procedure of Clustering Based on Multivariate
    Outlier Detection (Journal of Data Science 11(2013), 69-84)* |'
  prefs: []
  type: TYPE_TB
- en: We will look at the detection of outliers in univariate data in this recipe
    and then move on to look at outliers in multivariate and text data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will look at the following three methods for outlier detection
    in univariate data:'
  prefs: []
  type: TYPE_NORMAL
- en: Median absolute deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean plus or minus three standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see how we can leverage these methods to spot outliers in univariate
    data. Before we jump into the next section, let''s create a dataset with outliers
    so that we can evaluate our method empirically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create 100 data points, and 10 percent of them will be outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `randn` function in the `random` module of NumPy to generate
    our inliers. This will be a sample from a distribution with a mean of zero and
    a standard deviation of one. Let''s verify the mean and standard deviation of
    our sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We will calculate the mean and standard deviation with the functions from NumPy
    and print the output. Let''s inspect the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the mean is close to zero and the standard deviation is close
    to one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create the outliers. This will be 10 percent of the whole dataset,
    that is, 10 points, given that our sample size is 100\. As you can see, we sampled
    our outliers from a uniform distribution between -9 and 9\. Any points between
    this range will have an equal chance of being selected. We will concatenate our
    inlier and outlier data. It will be good to see the data with a scatter plot before
    we run our outlier detection program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the graph that is generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B04041_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Our *y* axis is the actual values that we generated and our *x* axis is a running
    count. It will be a good exercise to mark the points that you feel are outliers.
    We can later compare our program output with your manual selections.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start with the median absolute deviation. Then we will plot our values,
    with the outliers marked in red:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Moving on to the mean plus or minus three standard deviation, we will plot
    our values, with the outliers colored in red:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In step 1, we use the median absolute deviation to detect the outliers in the
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: We first calculate the median value of our dataset using the median function
    from NumPy. Next, we declare a variable with a value of 1.4826\. This is a constant
    to be multiplied with the absolute deviation from the median. Finally, we calculate
    the median of absolute deviations of each entry from the median value and multiply
    it with the constant, b.
  prefs: []
  type: TYPE_NORMAL
- en: 'Any point that is more than or less than three times the median absolute deviation
    is deemed as an outlier for our method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We then calculate the lower and upper limits of the median absolute deviation,
    as shown previously, and classify every point as either an outlier or inlier,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we have all our outlier points stored in a list by the name of outliers.
    We must also store the index of the outliers in a separate list called outlier_index.
    This is done for the ease of plotting, as you will see in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then plot the original points and outliers. The plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The points marked in red are classified as outliers by the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 3, we code up the second algorithm, mean plus or minus three standard
    deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'We then calculate the standard deviation and mean of our dataset. Here, you
    can see that we have set our `b = 3`. As the name of our algorithm suggests, we
    will need a standard deviation of three, and this b is used for the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: We can calculate the lower and upper limits as the mean minus three times the
    standard deviation. Using these values, we can then classify every point as either
    an outlier or inlier in the for loop. We then add all the outliers and their indices
    to the two lists, outliers and outlier_index, to plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we plot the outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As per the definition of outliers, outliers in a given dataset are those points
    that are far away from the other points in the data source. The estimates of the
    center of the dataset and the spread of the dataset can be used to detect the
    outliers. In the methods that we outlined in this recipe, we used the mean and
    median as the estimates for the center of the data and standard deviation, and
    the median absolute deviation as the estimates for the spread. Spread is also
    called scale.
  prefs: []
  type: TYPE_NORMAL
- en: Let's do a little bit of rationalization about why our methods work in the detection
    of the outliers. Let's start with the method of using standard deviation. For
    Gaussian data, we know that 68.27 percent of the data lies with in one standard
    deviation, 95.45 percent in two, and 99.73 percent lies in three. Thus, according
    to our rule that any point that is more than three standard deviations from the
    mean is classified as an outlier. However, this method is not robust. Let's look
    at a small example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's sample eight data points from a normal distribution, with the mean as
    zero and the standard deviation as one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the convenient function from `NumPy .random` to generate our numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Let's add two outliers to it manually, for example, 45 and 69, to this list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our dataset now looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: The mean of the preceding dataset is 11.211 and the standard deviation is `23.523`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the upper rule, mean + 3 * std. This is 11.211 + 3 * 23.523 =
    81.78.
  prefs: []
  type: TYPE_NORMAL
- en: Now, according to this upper bound rule, both the points, 45 and 69, are not
    outliers! Both the mean and the standard deviation are non-robust estimators of
    the center and scale of the dataset, as they are extremely sensitive to outliers.
    If we replace one of the points with an extreme point in a dataset with n observations,
    it will completely change the estimate of the mean and the standard deviation.
    This property of the estimators is called the finite sample breakdown point.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The finite sample breakdown point is defined as the proportion of the observations
    in a sample that can be replaced before the estimator fails to describe the data
    accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for the mean and standard deviation, the finite sample breakdown point
    is 0 percent because in a large sample, replacing even a single point would change
    the estimators drastically.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the median is a more robust estimate. The median is the middle
    observation in a finite set of observations that is sorted in an ascending order.
    For the median to change drastically, we have to replace half of the observations
    in the data that are far away from the median. This gives you a 50 percent finite
    sample breakdown point for the median.
  prefs: []
  type: TYPE_NORMAL
- en: 'The median absolute deviation method is attributed to the following paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Leys, C., et al., Detecting outliers: Do not use standard deviation around
    the mean, use absolute deviation around the median, Journal of Experimental Social
    Psychology (2013)*, [http://dx.doi.org/10.1016/j.jesp.2013.03.013](http://dx.doi.org/10.1016/j.jesp.2013.03.013).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Performing summary statistics and plots* recipe in [Chapter 1](ch01.xhtml
    "Chapter 1. Python for Data Science"), *Using Python for Data Science*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
