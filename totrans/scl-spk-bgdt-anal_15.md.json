["```py\n*outputDF = transfomer.*transform*(inputDF)*\n\n```", "```py\n*transformer = estimator.*fit*(inputDF)* *outputDF = transformer.*transform*(inputDF)*\n\n```", "```py\nval lines = Seq(\n | (1, \"Hello there, how do you like the book so far?\"),\n | (2, \"I am new to Machine Learning\"),\n | (3, \"Maybe i should get some coffee before starting\"),\n | (4, \"Coffee is best when you drink it hot\"),\n | (5, \"Book stores have coffee too so i should go to a book store\")\n | )\nlines: Seq[(Int, String)] = List((1,Hello there, how do you like the book so far?), (2,I am new to Machine Learning), (3,Maybe i should get some coffee before starting), (4,Coffee is best when you drink it hot), (5,Book stores have coffee too so i should go to a book store))\n\n```", "```py\nscala> val sentenceDF = spark.createDataFrame(lines).toDF(\"id\", \"sentence\")\nsentenceDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string]\n\n```", "```py\nscala> sentenceDF.show(false)\n|id|sentence |\n|1 |Hello there, how do you like the book so far? |\n|2 |I am new to Machine Learning |\n|3 |Maybe i should get some coffee before starting |\n|4 |Coffee is best when you drink it hot |\n|5 |Book stores have coffee too so i should go to a book store|\n\n```", "```py\nimport org.apache.spark.ml.feature.Tokenizer\nimport org.apache.spark.ml.feature.RegexTokenizer\n\n```", "```py\nscala> val tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_942c8332b9d8\n\n```", "```py\nscala> val wordsDF = tokenizer.transform(sentenceDF)\nwordsDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 1 more field]\n\n```", "```py\nscala> wordsDF.show(false)\n|id|sentence |words |\n|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |\n|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |\n|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |\n|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |\n|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|\n\n```", "```py\nscala> val regexTokenizer = new RegexTokenizer().setInputCol(\"sentence\").setOutputCol(\"regexWords\").setPattern(\"\\\\W\")\nregexTokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_15045df8ce41\n\n```", "```py\nscala> val regexWordsDF = regexTokenizer.transform(sentenceDF)\nregexWordsDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 1 more field]\n\n```", "```py\nscala> regexWordsDF.show(false)\n|id|sentence |regexWords |\n|1 |Hello there, how do you like the book so far? |[hello, there, how, do, you, like, the, book, so, far] |\n|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |\n|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |\n|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |\n|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|\n\n```", "```py\nimport org.apache.spark.ml.feature.StopWordsRemover\n\n```", "```py\nscala> val remover = new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filteredWords\")\nremover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_48d2cecd3011\n\n```", "```py\nscala> val noStopWordsDF = remover.transform(wordsDF)\nnoStopWordsDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 2 more fields]\n\n```", "```py\nscala> noStopWordsDF.show(false)\n|id|sentence |words |filteredWords |\n|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |\n|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |\n|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |\n|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |\n|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|\n\n```", "```py\n\nscala> noStopWordsDF.select(\"sentence\", \"filteredWords\").show(5,false)\n|sentence |filteredWords |\n|Hello there, how do you like the book so far? |[hello, there,, like, book, far?] |\n|I am new to Machine Learning |[new, machine, learning] |\n|Maybe i should get some coffee before starting |[maybe, get, coffee, starting] |\n|Coffee is best when you drink it hot |[coffee, best, drink, hot] |\n|Book stores have coffee too so i should go to a book store|[book, stores, coffee, go, book, store]|\n\n```", "```py\nscala> val noHello = Array(\"hello\") ++ remover.getStopWords\nnoHello: Array[String] = Array(hello, i, me, my, myself, we, our, ours, ourselves, you, your, yours, yourself, yourselves, he, him, his, himself, she, her, hers, herself, it, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, these, those, am, is, are, was, were ...\nscala>\n\n//create new transfomer using the amended Stop Words list\nscala> val removerCustom = new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filteredWords\").setStopWords(noHello)\nremoverCustom: org.apache.spark.ml.feature.StopWordsRemover = stopWords_908b488ac87f\n\n//invoke transform function\nscala> val noStopWordsDFCustom = removerCustom.transform(wordsDF)\nnoStopWordsDFCustom: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 2 more fields]\n\n//output dataset showing only sentence and filtered words - now will not show hello\nscala> noStopWordsDFCustom.select(\"sentence\", \"filteredWords\").show(5,false)\n+----------------------------------------------------------+---------------------------------------+\n|sentence |filteredWords |\n+----------------------------------------------------------+---------------------------------------+\n|Hello there, how do you like the book so far? |[there,, like, book, far?] |\n|I am new to Machine Learning |[new, machine, learning] |\n|Maybe i should get some coffee before starting |[maybe, get, coffee, starting] |\n|Coffee is best when you drink it hot |[coffee, best, drink, hot] |\n|Book stores have coffee too so i should go to a book store|[book, stores, coffee, go, book, store]|\n+----------------------------------------------------------+---------------------------------------+\n\n```", "```py\nimport org.apache.spark.ml.feature.NGram\n\n```", "```py\nscala> val ngram = new NGram().setN(2).setInputCol(\"filteredWords\").setOutputCol(\"ngrams\")\nngram: org.apache.spark.ml.feature.NGram = ngram_e7a3d3ab6115\n\n```", "```py\nscala> val nGramDF = ngram.transform(noStopWordsDF)\nnGramDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 3 more fields]\n\n```", "```py\nscala> nGramDF.show(false)\n|id|sentence |words |filteredWords |ngrams |\n|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |[hello there,, there, like, like book, book far?] |\n|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |[new machine, machine learning] |\n|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |[maybe get, get coffee, coffee starting] |\n|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |[coffee best, best drink, drink hot] |\n|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|[book stores, stores coffee, coffee go, go book, book store]|\n\n```", "```py\nscala> nGramDF.select(\"sentence\", \"ngrams\").show(5,false)\n|sentence |ngrams |\n|Hello there, how do you like the book so far? |[hello there,, there, like, like book, book far?] |\n|I am new to Machine Learning |[new machine, machine learning] |\n|Maybe i should get some coffee before starting |[maybe get, get coffee, coffee starting] |\n|Coffee is best when you drink it hot |[coffee best, best drink, drink hot] |\n|Book stores have coffee too so i should go to a book store|[book stores, stores coffee, coffee go, go book, book store]|\n\n```", "```py\nimport org.apache.spark.ml.feature.HashingTF\n\n```", "```py\nscala> val hashingTF = new HashingTF().setInputCol(\"filteredWords\").setOutputCol(\"rawFeatures\").setNumFeatures(100)\nhashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_b05954cb9375\n\n```", "```py\nscala> val rawFeaturesDF = hashingTF.transform(noStopWordsDF)\nrawFeaturesDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 3 more fields]\n\n```", "```py\nscala> rawFeaturesDF.show(false)\n|id |sentence |words |filteredWords |rawFeatures |\n|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |(100,[30,48,70,93],[2.0,1.0,1.0,1.0]) |\n|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |(100,[25,52,72],[1.0,1.0,1.0]) |\n|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |(100,[16,51,59,99],[1.0,1.0,1.0,1.0]) |\n|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |(100,[31,51,63,72],[1.0,1.0,1.0,1.0]) |\n|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|(100,[43,48,51,77,93],[1.0,1.0,1.0,1.0,2.0])|\n\n```", "```py\nimport org.apache.spark.ml.feature.IDF\n\n```", "```py\nscala> val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\nidf: org.apache.spark.ml.feature.IDF = idf_d8f9ab7e398e\n\n```", "```py\nscala> val idfModel = idf.fit(rawFeaturesDF)\nidfModel: org.apache.spark.ml.feature.IDFModel = idf_d8f9ab7e398e\n\n```", "```py\nscala> val featuresDF = idfModel.transform(rawFeaturesDF)\nfeaturesDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 4 more fields]\n\n```", "```py\nscala> featuresDF.select(\"id\", \"features\").show(5, false)\n|id|features |\n|1 |(20,[8,10,13],[0.6931471805599453,3.295836866004329,0.6931471805599453]) |\n|2 |(20,[5,12],[1.0986122886681098,1.3862943611198906]) |\n|3 |(20,[11,16,19],[0.4054651081081644,1.0986122886681098,2.1972245773362196]) |\n|4 |(20,[3,11,12],[0.6931471805599453,0.8109302162163288,0.6931471805599453]) |\n|5 |(20,[3,8,11,13,17],[0.6931471805599453,0.6931471805599453,0.4054651081081644,1.3862943611198906,1.0986122886681098])|\n\n```", "```py\n\nscala> featuresDF.show(false)\n|id|sentence |words |filteredWords |rawFeatures |features |\n|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |(20,[8,10,13],[1.0,3.0,1.0]) |(20,[8,10,13],[0.6931471805599453,3.295836866004329,0.6931471805599453]) |\n|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |(20,[5,12],[1.0,2.0]) |(20,[5,12],[1.0986122886681098,1.3862943611198906]) |\n|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |(20,[11,16,19],[1.0,1.0,2.0]) |(20,[11,16,19],[0.4054651081081644,1.0986122886681098,2.1972245773362196]) |\n|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |(20,[3,11,12],[1.0,2.0,1.0]) |(20,[3,11,12],[0.6931471805599453,0.8109302162163288,0.6931471805599453]) |\n|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|(20,[3,8,11,13,17],[1.0,1.0,1.0,2.0,1.0])|(20,[3,8,11,13,17],[0.6931471805599453,0.6931471805599453,0.4054651081081644,1.3862943611198906,1.0986122886681098])|\n\n```", "```py\nimport org.apache.spark.ml.feature.Word2Vec\n\n```", "```py\nscala> val word2Vec = new Word2Vec().setInputCol(\"words\").setOutputCol(\"wordvector\").setVectorSize(3).setMinCount(0)\nword2Vec: org.apache.spark.ml.feature.Word2Vec = w2v_fe9d488fdb69\n\n```", "```py\nscala> val word2VecModel = word2Vec.fit(noStopWordsDF)\nword2VecModel: org.apache.spark.ml.feature.Word2VecModel = w2v_fe9d488fdb69\n\n```", "```py\nscala> val word2VecDF = word2VecModel.transform(noStopWordsDF)\nword2VecDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 3 more fields]\n\n```", "```py\nscala> word2VecDF.show(false)\n|id|sentence |words |filteredWords |wordvector |\n|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |[0.006875938177108765,-0.00819675214588642,0.0040686681866645815]|\n|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |[0.026012470324834187,0.023195965060343344,-0.10863214979569116] |\n|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |[-0.004304863978177309,-0.004591284319758415,0.02117823390290141]|\n|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |[0.054064739029854536,-0.003801364451646805,0.06522738828789443] |\n|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|[-0.05887459063281615,-0.07891856770341595,0.07510609552264214] |\n\n```", "```py\nimport org.apache.spark.ml.feature.CountVectorizer\n\n```", "```py\nscala> val countVectorizer = new CountVectorizer().setInputCol(\"filteredWords\").setOutputCol(\"features\")\ncountVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_555716178088\n\n```", "```py\nscala> val countVectorizerModel = countVectorizer.fit(noStopWordsDF)\ncountVectorizerModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_555716178088\n\n```", "```py\nscala> val countVectorizerDF = countVectorizerModel.transform(noStopWordsDF)\ncountVectorizerDF: org.apache.spark.sql.DataFrame = [id: int, sentence: string ... 3 more fields]\n\n```", "```py\nscala> countVectorizerDF.show(false)\n|id |sentence |words |filteredWords |features |\n|1 |Hello there, how do you like the book so far? |[hello, there,, how, do, you, like, the, book, so, far?] |[hello, there,, like, book, far?] |(18,[1,4,5,13,15],[1.0,1.0,1.0,1.0,1.0])|\n|2 |I am new to Machine Learning |[i, am, new, to, machine, learning] |[new, machine, learning] |(18,[6,7,16],[1.0,1.0,1.0]) |\n|3 |Maybe i should get some coffee before starting |[maybe, i, should, get, some, coffee, before, starting] |[maybe, get, coffee, starting] |(18,[0,8,9,14],[1.0,1.0,1.0,1.0]) |\n|4 |Coffee is best when you drink it hot |[coffee, is, best, when, you, drink, it, hot] |[coffee, best, drink, hot] |(18,[0,3,10,12],[1.0,1.0,1.0,1.0]) |\n|5 |Book stores have coffee too so i should go to a book store|[book, stores, have, coffee, too, so, i, should, go, to, a, book, store]|[book, stores, coffee, go, book, store]|(18,[0,1,2,11,17],[1.0,2.0,1.0,1.0,1.0])|\n\n```", "```py\nimport org.apache.spark.ml.clustering.LDA\n\n```", "```py\nscala> val lda = new LDA().setK(10).setMaxIter(10)\nlda: org.apache.spark.ml.clustering.LDA = lda_18f248b08480\n\n```", "```py\nscala> val ldaModel = lda.fit(countVectorizerDF)\nldaModel: org.apache.spark.ml.clustering.LDAModel = lda_18f248b08480\n\n```", "```py\nscala> val ll = ldaModel.logLikelihood(countVectorizerDF)\nll: Double = -275.3298948279124\n\n```", "```py\nscala> val lp = ldaModel.logPerplexity(countVectorizerDF)\nlp: Double = 12.512670220189033\n\n```", "```py\nscala> val topics = ldaModel.describeTopics(10)\ntopics: org.apache.spark.sql.DataFrame = [topic: int, termIndices: array<int> ... 1 more field]\n\n```", "```py\nscala> topics.show(10, false)\n|topic|termIndices |termWeights |\n|0 |[2, 5, 7, 12, 17, 9, 13, 16, 4, 11] |[0.06403877783050851, 0.0638177222807826, 0.06296749987731722, 0.06129482302538905, 0.05906095287220612, 0.0583855194291998, 0.05794181263149175, 0.057342702589298085, 0.05638654243412251, 0.05601913313272188] |\n|1 |[15, 5, 13, 8, 1, 6, 9, 16, 2, 14] |[0.06889315890755099, 0.06415969116685549, 0.058990446579892136, 0.05840283223031986, 0.05676844625413551, 0.0566842803396241, 0.05633554021408156, 0.05580861561950114, 0.055116582320533423, 0.05471754535803045] |\n|2 |[17, 14, 1, 5, 12, 2, 4, 8, 11, 16] |[0.06230542516700517, 0.06207673834677118, 0.06089143673912089, 0.060721809302399316, 0.06020894045877178, 0.05953822260375286, 0.05897033457363252, 0.057504989644756616, 0.05586725037894327, 0.05562088924566989] |\n|3 |[15, 2, 11, 16, 1, 7, 17, 8, 10, 3] |[0.06995373276880751, 0.06249041124300946, 0.061960612781077645, 0.05879695651399876, 0.05816564815895558, 0.05798721645705949, 0.05724374708387087, 0.056034215734402475, 0.05474217418082123, 0.05443850583761207] |\n|4 |[16, 9, 5, 7, 1, 12, 14, 10, 13, 4] |[0.06739359010780331, 0.06716438619386095, 0.06391509491709904, 0.062049068666162915, 0.06050715515506004, 0.05925113958472128, 0.057946856127790804, 0.05594837087703049, 0.055000929117413805, 0.053537418286233956]|\n|5 |[5, 15, 6, 17, 7, 8, 16, 11, 10, 2] |[0.061611492476326836, 0.06131944264846151, 0.06092975441932787, 0.059812552365763404, 0.05959889552537741, 0.05929123338151455, 0.05899808901872648, 0.05892061664356089, 0.05706951425713708, 0.05636134431063274] |\n|6 |[15, 0, 4, 14, 2, 10, 13, 7, 6, 8] |[0.06669864676186414, 0.0613859230159798, 0.05902091745149218, 0.058507882633921676, 0.058373998449322555, 0.05740944364508325, 0.057039150886628136, 0.057021822698594314, 0.05677330199892444, 0.056741558062814376]|\n|7 |[12, 9, 8, 15, 16, 4, 7, 13, 17, 10]|[0.06770789917351365, 0.06320078344027158, 0.06225712567900613, 0.058773135159638154, 0.05832535181576588, 0.057727684814461444, 0.056683575112703555, 0.05651178333610803, 0.056202395617563274, 0.05538103218174723]|\n|8 |[14, 11, 10, 7, 12, 9, 13, 16, 5, 1]|[0.06757347958335463, 0.06362319365053591, 0.063359294927315, 0.06319462709331332, 0.05969320243218982, 0.058380063437908046, 0.057412693576813126, 0.056710451222381435, 0.056254581639201336, 0.054737785085167814] |\n|9 |[3, 16, 5, 7, 0, 2, 10, 15, 1, 13] |[0.06603941595604573, 0.06312775362528278, 0.06248795574460503, 0.06240547032037694, 0.0613859713404773, 0.06017781222489122, 0.05945655694365531, 0.05910351349013983, 0.05751269894725456, 0.05605239791764803] |\n\n```", "```py\nscala> val inputText = sc.textFile(\"Sentiment_Analysis_Dataset10k.csv\")\ninputText: org.apache.spark.rdd.RDD[String] = Sentiment_Analysis_Dataset10k.csv MapPartitionsRDD[1722] at textFile at <console>:77\n\n```", "```py\nscala> val sentenceDF = inputText.map(x => (x.split(\",\")(0), x.split(\",\")(1), x.split(\",\")(2))).toDF(\"id\", \"label\", \"sentence\")\nsentenceDF: org.apache.spark.sql.DataFrame = [id: string, label: string ... 1 more field]\n\n```", "```py\nscala> import org.apache.spark.ml.feature.Tokenizer\nimport org.apache.spark.ml.feature.Tokenizer\n\nscala> val tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_ebd4c89f166e\n\nscala> val wordsDF = tokenizer.transform(sentenceDF)\nwordsDF: org.apache.spark.sql.DataFrame = [id: string, label: string ... 2 more fields]\n\nscala> wordsDF.show(5, true)\n| id|label| sentence| words|\n| 1| 0|is so sad for my ...|[is, so, sad, for...|\n| 2| 0|I missed the New ...|[i, missed, the, ...|\n| 3| 1| omg its already ...|[, omg, its, alre...|\n| 4| 0| .. Omgaga. Im s...|[, , .., omgaga.,...|\n| 5| 0|i think mi bf is ...|[i, think, mi, bf...|\n\n```", "```py\nscala> import org.apache.spark.ml.feature.StopWordsRemover\nimport org.apache.spark.ml.feature.StopWordsRemover\n\nscala> val remover = new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filteredWords\")\nremover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_d8dd48c9cdd0\n\nscala> val noStopWordsDF = remover.transform(wordsDF)\nnoStopWordsDF: org.apache.spark.sql.DataFrame = [id: string, label: string ... 3 more fields]\n\nscala> noStopWordsDF.show(5, true)\n| id|label| sentence| words| filteredWords|\n| 1| 0|is so sad for my ...|[is, so, sad, for...|[sad, apl, friend...|\n| 2| 0|I missed the New ...|[i, missed, the, ...|[missed, new, moo...|\n| 3| 1| omg its already ...|[, omg, its, alre...|[, omg, already, ...|\n| 4| 0| .. Omgaga. Im s...|[, , .., omgaga.,...|[, , .., omgaga.,...|\n| 5| 0|i think mi bf is ...|[i, think, mi, bf...|[think, mi, bf, c...|\n\n```", "```py\nscala> import org.apache.spark.ml.feature.CountVectorizer\nimport org.apache.spark.ml.feature.CountVectorizer\n\nscala> val countVectorizer = new CountVectorizer().setInputCol(\"filteredWords\").setOutputCol(\"features\")\ncountVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_fdf1512dfcbd\n\nscala> val countVectorizerModel = countVectorizer.fit(noStopWordsDF)\ncountVectorizerModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_fdf1512dfcbd\n\nscala> val countVectorizerDF = countVectorizerModel.transform(noStopWordsDF)\ncountVectorizerDF: org.apache.spark.sql.DataFrame = [id: string, label: string ... 4 more fields]\n\nscala> countVectorizerDF.show(5,true)\n| id|label| sentence| words| filteredWords| features|\n| 1| 0|is so sad for my ...|[is, so, sad, for...|[sad, apl, friend...|(23481,[35,9315,2...|\n| 2| 0|I missed the New ...|[i, missed, the, ...|[missed, new, moo...|(23481,[23,175,97...|\n| 3| 1| omg its already ...|[, omg, its, alre...|[, omg, already, ...|(23481,[0,143,686...|\n| 4| 0| .. Omgaga. Im s...|[, , .., omgaga.,...|[, , .., omgaga.,...|(23481,[0,4,13,27...|\n| 5| 0|i think mi bf is ...|[i, think, mi, bf...|[think, mi, bf, c...|(23481,[0,33,731,...|\n\n```", "```py\n\nscala> val inputData=countVectorizerDF.select(\"label\", \"features\").withColumn(\"label\", col(\"label\").cast(\"double\"))\ninputData: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\n```", "```py\nscala> val Array(trainingData, testData) = inputData.randomSplit(Array(0.8, 0.2))\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n\n```", "```py\nscala> import org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.classification.LogisticRegression\n\nscala> val lr = new LogisticRegression()\nlr: org.apache.spark.ml.classification.LogisticRegression = logreg_a56accef5728\n\n```", "```py\nscala> var lrModel = lr.fit(trainingData)\nlrModel: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_a56accef5728\n\nscala> lrModel.coefficients\nres160: org.apache.spark.ml.linalg.Vector = [7.499178040193577,8.794520490564185,4.837543313917086,-5.995818019393418,1.1754740390468577,3.2104594489397584,1.7840290776286476,-1.8391923375331787,1.3427471762591,6.963032309971087,-6.92725055841986,-10.781468845891563,3.9752.836891070557657,3.8758544006087523,-11.760894935576934,-6.252988307540...\n\nscala> lrModel.intercept\nres161: Double = -5.397920610780994\n\n```", "```py\nscala> import org.apache.spark.ml.classification.BinaryLogisticRegressionSummary\nimport org.apache.spark.ml.classification.BinaryLogisticRegressionSummary\n\nscala> val summary = lrModel.summary\nsummary: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary@1dce712c\n\nscala> val bSummary = summary.asInstanceOf[BinaryLogisticRegressionSummary]\nbSummary: org.apache.spark.ml.classification.BinaryLogisticRegressionSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary@1dce712c\n\nscala> bSummary.areaUnderROC\nres166: Double = 0.9999231930196596\n\nscala> bSummary.roc\nres167: org.apache.spark.sql.DataFrame = [FPR: double, TPR: double]\n\nscala> bSummary.pr.show()\n| recall|precision|\n| 0.0| 1.0|\n| 0.2306543172990738| 1.0|\n| 0.2596354944726621| 1.0|\n| 0.2832387212429041| 1.0|\n|0.30504929787869733| 1.0|\n| 0.3304451747833881| 1.0|\n|0.35255452644158947| 1.0|\n| 0.3740663280549746| 1.0|\n| 0.3952793546459516| 1.0|\n\n```", "```py\nscala> val training = lrModel.transform(trainingData)\ntraining: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\n\nscala> val test = lrModel.transform(testData)\ntest: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\n\n```", "```py\nscala> training.filter(\"label == prediction\").count\nres162: Long = 8029\n\nscala> training.filter(\"label != prediction\").count\nres163: Long = 19\n\nscala> test.filter(\"label == prediction\").count\nres164: Long = 1334\n\nscala> test.filter(\"label != prediction\").count\nres165: Long = 617\n\n```"]