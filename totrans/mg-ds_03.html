<html><head></head><body><div><h1 class="header-title">Testing Your Models</h1>
                
            
            
                
<p>Coming up with a perfect machine learning model is not simple if you do not use a good testing methodology. This seemingly perfect model will fail the moment you deploy it. Testing the model's performance is not an easy task, but it is an essential part of every data science project. Without proper testing, you can't be sure whether your models will work as expected, and you can't choose the best approach to solve the task at hand.</p>
<p class="mce-root"/>
<p>This chapter will explore various approaches for model testing and look at different types of metrics, using mathematical functions that evaluate the quality of predictions. We will also go through a set of methods for testing classifier models.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Offline model testing</li>
<li>Online model testing</li>
</ul>


            

            
        
    </div>
<div><h1 class="header-title">Offline model testing</h1>
                
            
            
                
<p>Offline model testing encompasses all the model-evaluation processes that are performed before the model is deployed. Before discussing online testing in detail, we must first define model errors and ways to calculate them.</p>


            

            
        
    </div>
<div><h1 class="header-title">Understanding model errors</h1>
                
            
            
                
<p>Every model can make mistakes because collected data, and the model itself, introduces implications about the nature of your problem. The best example of a good working model is inside your brain. You use modeling in real time—the brain renders everything you see by interpreting electromagnetic impulses recorded by your eyes. While this picture of the world is imperfect, it is useful as we receive over 90% of information through the visual channel. The last 10% comes from hearing, touch, and our other senses. Thus, each model, <strong>M</strong>, tries to predict real value, <strong>Y</strong>, by making a guess, <sub><img class="fm-editor-equation" src="img/c76546bd-25e3-4ecf-912a-d99afa7820eb.png" style="width:0.92em;height:1.25em;" width="140" height="200"/></sub>.</p>
<p>The difference between the real value and model's approximation makes up the model error:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/91dce14c-f063-4766-9b5c-6f57446e1e70.png" style="width:7.08em;height:1.25em;" width="1190" height="210"/></p>
<p>For regression problems, we can measure the error in quantities that the model predicts. For example, if we predict house prices using a machine learning model and get a prediction of $300,000 for a house with a real price of $350,000, we can say that the error is $350,000 - $300,000 = $50,000.</p>
<p>For classification problems in the simplest setting, we can measure the error as 0 for a guess, and 1 for a wrong answer. For example, for a cat/dog recognizer, we give an error of 1 if the model predicts that there is a cat in a dog photo, and 0 if it gives a correct answer.</p>


            

            
        
    </div>
<div><h1 class="header-title">Decomposing errors</h1>
                
            
            
                
<p>You won't find a machine learning model that perfectly solves your problems without making even a single mistake, no matter how small. Since every model makes mistakes, it is critical to understand their nature. Suppose that our model makes a prediction and we know the real value. If this prediction is incorrect, then there is some difference between the prediction and the true value:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/d24ad2f9-c167-4252-a586-8eab3b33ffc4.png" style="width:15.42em;height:1.17em;" width="2510" height="190"/></p>
<p>The other part of this error will come from imperfections in our data, and some from imperfections in our model. No matter how complex our model is, it can only reduce the modeling error. Irreducible error is out of our control, hence its name.</p>
<p>Let's look at it in the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/a9aa3c92-9622-4e74-a604-6389a73e0725.png" style="width:22.75em;height:1.08em;" width="3570" height="170"/></p>
<p>Not all reducible errors are the same. We can decompose reducible errors further. For example, look at the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-364 image-border" src="img/737bbad2-ca32-44e0-918e-4821c09f8c73.png" style="width:21.33em;height:21.33em;" width="743" height="742"/></p>
<p>The red center of each target represents our goal (real value), and the blue shots represent the model predictions. In the target, the model's aim is off—all predictions are close together, but they are far away from the target. This kind of error is called <strong>bias</strong>. The simpler our model is, the more bias it will have. For a simple model, the bias component can become prevailing:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-366 image-border" src="img/d5bf457f-5afb-4cf4-8109-4be8f3003775.png" style="width:23.00em;height:23.00em;" width="276" height="276"/></p>
<p>In the preceding plot, we try to model a complex relationship between variables with a simple line. This kind of model has a high bias.</p>
<p>The second component of the model error is <strong>variance</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-365 image-border" src="img/3226d01c-6805-4da1-b48b-d87d7faa061c.png" style="width:18.25em;height:18.25em;" width="742" height="743"/></p>
<p class="CDPAlignLeft CDPAlign">All predictions appear to be clustered around the true target, but the spread is too high. The source of this error comes from the model's sensitivity to fluctuations in data. If the model has high variance, randomness in measurements can lead to very different predictions.</p>
<p>So far, we have decomposed model error into the three following numbers:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/d46b3109-8245-44da-95f4-62f6017375bc.png" style="width:20.25em;height:1.00em;" width="3440" height="170"/></p>
<p>It is not a coincidence that bias and variance sit close together in the same formula. There is a relationship between them. Predictive models show a property called the <strong>bias-variance</strong> <strong>tradeoff</strong>—the more biased a model, the lower the variance component of the error. And in reverse, the more variance it has, the lower its bias will be. This important fact will be a game-changer for building ensemble models, which we will explore in <a href="eb2995e4-1a9a-43d9-b162-557a4664069b.xhtml">Chapter 3</a>, <em>Understanding AI</em>.</p>
<p>Typically, models that impose some kind of structure in the data have a high bias (they assume certain laws that the data conforms to). Biased models will work well, as long as the data does not contradict the underlying logic of the model. To give you an example of such a model, think of a simple line. For example, we will predict a housing price as a linear function of its size in square feet: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/c9e0c82d-5039-401e-a129-8dcf13e4c6d5.png" style="width:24.25em;height:1.33em;" width="3820" height="210"/></p>
<p class="mce-root"/>
<p>Notice that if we change the square footage by a little, say 0.1, then the prediction won't change by much. Thus, this model has low variance. When the model is sensitive to changes in its input, its variance will outgrow the bias. The variance component will grow with your model increases in complexity and the total number of parameters grows. In the following plot, you can see how two different models fit the same dataset. The first simple model has low variance, and the second complex model has high variance:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-367 image-border" src="img/84bb9ab9-a0d4-4e34-a62c-4ef303df7ff7.png" style="width:40.17em;height:16.50em;" width="724" height="297"/></p>
<p>In the preceding plot, slight changes in <strong>X</strong> can lead to large fluctuations of <strong>Y</strong>. Models with high variance are robust and imply that the data is much less structured.</p>


            

            
        
    </div>
<div><h1 class="header-title">Understanding overfitting</h1>
                
            
            
                
<p>The bias-variance trade-off goes hand in hand with a very important problem in machine learning called <strong>overfitting</strong>. If your model is too simple, it will cause large errors. If it is too complex, it will memorize the data too well. An overfitted model remembers data too well and acts like a database. Suppose that our housing dataset contains some lucky deals where previous houses had a low price because of circumstances not captured in the data. An overfit model will memorize those examples too closely and predict incorrect price values on unseen data.</p>
<p>Now, having understood the error decomposition, can we use it as a stepping stone to design a model-testing pipeline?</p>
<p class="mce-root"/>
<p>We need to determine how to measure model error in such a way that it will correspond to the real model performance on unseen data. The answer comes from the question itself. We will split all the available data into two sets: a training set and a test set, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-368 image-border" src="img/a6fd01ba-178a-4d6f-aace-c1717aa27156.png" style="width:27.67em;height:5.33em;" width="421" height="81"/></p>
<p>We will use data in the training set to train our model. The test set acts as unseen data and you should not use the test set in the training process. When the model's training is finished, you can feed the test data into your model. Now you can calculate errors for all predictions. The model did not use the test data during training, so the test set error represents the model error on unseen data. The drawback to this approach is that you take a significant amount of data, usually up to 30%, to use for testing. This means less training data and lower model quality. There is also a caveat – if you use your test set too much, error metrics will start to lie. For example, suppose that you did the following:</p>
<ol>
<li>Trained a model</li>
<li>Measured the error on the test data</li>
<li>Changed your model to improve the metrics</li>
<li>Repeated steps 1-3 ten times</li>
<li>Deployed the model to production</li>
</ol>
<p>It is likely that the quality of your model will be much lower than expected. Why did this happen? Let's look more closely <em>step 3</em>. You looked at a score, and changed your model or data processing code several consecutive times. In fact, you did several learning iterations by hand. By repeatedly improving the test score, you indirectly disclosed information about the test data to your model. When the metric values measured on a test set deviate from the metrics measured on the real data, we say that the test data has leaked into our model. Data leaks are notoriously hard to detect before they cause damage. To avoid them, you should always be mindful of the possibility of a leak, think critically, and follow best practices.</p>
<p>We can use a separate piece of data to fight test set leakage. Data scientists use validation sets to tune model parameters and compare different models before choosing the best one. Then, the test data is used only as a final check that informs you about model quality on unseen data. After you have measured the test metric scores, the only decision left is to make is whether the model will proceed to testing in a real-world scenario.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the following screenshot, you can see an example of a train/validation/test split of the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-369 image-border" src="img/b0e5b92e-1785-463b-bb97-2e720e6c2af0.png" style="width:29.33em;height:5.58em;" width="426" height="81"/></p>
<p>Unfortunately, the following two problems persist when we use this approach:</p>
<ul>
<li>The information about our test set might still leak into our solution after many iterations. Test-set leakage does not disappear completely when you use the validation set, it just becomes slower. To overcome this, change your test data from time to time. Ideally, make a new test set for every model-deployment cycle.</li>
<li>You might overfit your validation data quickly, because of the train-measure-change feedback cycle for tuning your models.</li>
</ul>
<p>To prevent overfitting, you can randomly select train and validation sets from your data for each experiment. Randomly shuffle all available data, then select random train and validation datasets by splitting the data into three parts according to proportions you have chosen.</p>
<p>There is no general rule for how much training, validation, and testing data you should use. Often, more training data means a more accurate model, but it means that you will have less data to assess the model's performance. The typical split for medium-sized datasets (up to 100,000 data points) is to use 60-80% of the data to train the model and use the rest for validation.</p>
<p>The situation changes for large datasets. If you have a dataset with 10,000,000 rows, using 30% for testing would comprise 3,000,000 rows. It is likely that this amount would be overkill. Increasing test and validation test sizes will yield diminishing returns. For some problems, you will get good results with 100,000 examples for testing, which would amount for a 1% test size. The more data you have, the lower the proportion you should use for testing.</p>
<p class="mce-root"/>
<p>Often, there is too little data. In those situations, taking from 30%-40% data for testing and validation might severely decrease the model's accuracy. You can apply a technique called cross-validation in data-scarce situations. With cross-validation, there's no need to create a separate validation or test set. Cross-validation proceeds in the following way:</p>
<ol>
<li>You choose some fixed number of iterations—three, for example.</li>
<li>Split the dataset into three parts.</li>
<li>For each iteration, cross-validation uses 2/3 of the dataset as a training data and 1/3 as validation data.</li>
<li>Train model for each of the three train-validation set pairs.</li>
<li>Calculate the metric values using each validation set.</li>
<li>Aggregate the metrics into a single number by averaging all metric values.</li>
</ol>
<p>The following screenshot explains cross-validation visually:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-370 image-border" src="img/029fc396-016c-4c6d-b0aa-0cc072c3b1ed.png" style="width:26.08em;height:22.33em;" width="991" height="849"/></p>
<p class="CDPAlignLeft CDPAlign">Cross-validation has one main drawback: it requires significantly more computational resources to assess model quality. In our example, to make a single assessment we needed to fit three models. With a regular train/test split, we would train only one model. In addition, cross-validation accuracy will grow with the number of iterations you use (also called folds). So cross-validation allows you to use more data for training, while requiring more computational resources. How do we choose between cross-validation and train-validation-test splits for projects?</p>
<p>In cross-validation, <sub><img class="fm-editor-equation" src="img/50985a49-a4a0-4825-850d-cde48295079d.png" style="width:0.58em;height:1.00em;"/></sub> is a variable parameter that is set up by a data scientist. The lowest possible value is 1, which is equivalent to a simple train/test split. The largest extreme is <sub><img class="fm-editor-equation" src="img/14ec3d61-d29f-4a58-8f3c-cdf28e242428.png" style="width:0.58em;height:1.00em;" width="100" height="170"/></sub> equal to the number of data points in the dataset. This means that if we have <sub><img class="fm-editor-equation" src="img/94f8ab10-6399-4c76-a3ce-7f21586c2418.png" style="width:1.00em;height:0.92em;" width="170" height="160"/></sub> points in the dataset, the model will be trained and tested <sub><img class="fm-editor-equation" src="img/f3838149-cf5b-433b-8eea-3ee7c78648a5.png" style="width:2.92em;height:1.00em;" width="490" height="170"/></sub> times. This special case of cross-validation is called <strong>leave-one-out cross-validation</strong>. In theory, a larger number of folds means that the cross-validation will return more accurate metric values. While leave-one-out cross-validation is the most theoretically accurate method, it is seldom used in practice because of the large computational requirements. In practice, the values of <sub><img class="fm-editor-equation" src="img/50985a49-a4a0-4825-850d-cde48295079d.png" style="width:0.58em;height:1.00em;" width="100" height="170"/></sub> range from 3 to 15 folds, depending on the dataset size. Your project may need to use more, so take this as advice and not as a rule.</p>
<p>The following table sums up a general way of thinking:</p>
<table style="border-collapse: collapse;width: 100%;border-color: #000000;height: 141px" border="1">
<tbody>
<tr>
<td style="width: 125px" class="td1 CDPAlignCenter CDPAlign"/>
<td style="width: 371px" class="td1 CDPAlignCenter CDPAlign">
<p class="p2"><strong>Model training requires low to moderate computational resources and time</strong></p>
</td>
<td style="width: 235px" class="td2 CDPAlignCenter CDPAlign">
<p class="p2"><strong>Model training requires large computational resources and takes a long time</strong></p>
</td>
</tr>
<tr>
<td style="width: 125px" class="td3">
<p class="p2 CDPAlignCenter CDPAlign"><strong>Small to medium dataset</strong></p>
</td>
<td style="width: 371px" class="td3">
<p class="CDPAlignCenter CDPAlign">Cross-validation</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Either</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Large dataset</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Either</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Train/validation/test split</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Another important aspect related to model testing is how to split the data. A slight error in your splitting logic can mean all your testing efforts were in vain. Splitting is easy, if all observations in your dataset are independent. Then you can use random data splits. But what if we are solving the stock-price prediction problem? When our data rows are tied to time, we can't look at them as independent values. Today's stock prices depend on their past values. If this wasn't true, the prices would randomly jump from $0 to $1,000. In this situation, suppose we have two years' worth of stock data, from January 2017 to December 2018. If we use random splits, it is possible that our model will train in September 2018 and test on February 2017. This makes no sense. We must always think about causal relationships and dependencies between your observations and be sure to check whether your validation procedure is correct.</p>
<p>Next, we will learn about metrics, which are formulas we can use to summarize validation and test errors. Metrics will allow us to compare different models and choose the best candidates for production use.</p>
<p class="mce-root"/>


            

            
        
    </div>
<div><h1 class="header-title">Using technical metrics</h1>
                
            
            
                
<p>Each model, no matter how complex and accurate, makes mistakes. It is natural to expect that some models will be better than others when solving a specific problem. Currently, we can measure errors by comparing individual model predictions with the ground truth. It would be useful to summarize them into a single number for measuring the model's performance. We can use a metric to do this. There are many kinds of metrics that are suitable for different machine learning problems.</p>
<p>In particular, for regression problems the most common metric is the <strong>root mean square error</strong>, or <strong>RMSE</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/04394a71-3018-4d3e-82d0-2bb8f07996be.png" style="width:19.42em;height:3.42em;" width="3410" height="600"/></p>
<p>Let's examine the elements of this formula:</p>
<ul>
<li><em>N</em> is the total number of data points.</li>
<li><em>predicted - actual</em> measures the error between ground truth and model prediction.</li>
<li>The Sigma sign at the start of the formula means sum.</li>
</ul>
<p>Another popular way to measure regression errors is <strong>mean absolute error</strong> (<strong>MAE</strong>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/6f81be96-3130-428c-9cac-d5bab44c92ff.png" style="width:17.33em;height:3.33em;" width="3010" height="580"/></p>
<p>Note that MAE is very similar to RMSE. Compared to MAE, RMSE has a square root instead of absolute value and it squares errors. While MAE and RMSE may seem identical, there are some technical differences between them. Data scientists can choose best metrics for a problem, knowing their trade-offs and shortcomings. You don't need to learn them all, but I would like to highlight one difference to give you a general feel of the thought process. RMSE penalizes large errors more than MAE. This property comes from the fact that RMSE uses squared errors, while MAE uses absolute values. To illustrate, an error of 4 would be 4 in MAE, but in RMSE it will turn into 16 because of the square.</p>
<p class="mce-root"/>
<p>For classification problems, the metric-calculation process is more involved. Let's imagine that we are building a binary classifier that estimates the probability of a person having pneumonia. To calculate how accurate the model is, we may just divide the total of correct answers by the number of rows in the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/6aabef97-73f4-434d-8301-331290a05a0b.png" style="width:10.08em;height:2.25em;" width="1610" height="360"/></p>
<p>Here, <sub><img class="fm-editor-equation" src="img/a63ff15c-921b-49d1-a89b-55c00b1d2ac7.png" style="width:3.50em;height:1.00em;" width="540" height="150"/> </sub>is the amount of correct predictions, and <sub><img class="fm-editor-equation" src="img/4b8b3e08-085a-41d7-8772-0d9defe1ec0a.png" style="width:1.00em;height:0.92em;" width="170" height="160"/></sub> is the total number of predictions. Accuracy is simple to understand and calculate, but it has a major flaw. Let's assume the average probability of having pneumonia is 0.001%. That is, one person out of 100,000 has the illness. If you had collected data on 200,000 people, it is feasible that your dataset would contain only two positive cases. Imagine you have asked a data scientist to build a machine learning model that estimates pneumonia probability based on a patient's data. You have said that you would only accept an accuracy of no less than 99.9%. Suppose that someone created a dummy algorithm that always outputs zeros.</p>
<p>This model has no real value, but its accuracy on our data will be high as it will make only two errors:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/300e6318-20d0-4d3a-814e-6cb59bd9947f.png" style="width:14.00em;height:2.25em;" width="2550" height="410"/></p>
<p>The problem is that accuracy considers only global fraction of answers. When one class outnumbers the others, accuracy outputs misleading values.</p>
<p>Let's look at model predictions in more detail by constructing a confusion table:</p>
<table style="border-collapse: collapse;width: 100%;border-color: #000000;height: 141px" border="1">
<tbody>
<tr>
<td style="width: 281px"/>
<td style="width: 257px">
<p class="CDPAlignCenter CDPAlign"><strong>Model prediction:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Has pneumonia</strong></p>
</td>
<td style="width: 162px">
<p class="CDPAlignCenter CDPAlign"><strong>Model prediction:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Does not have pneumonia</strong></p>
</td>
</tr>
<tr>
<td style="width: 281px">
<p class="CDPAlignCenter CDPAlign"><strong>Real outcome:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Has pneumonia</strong></p>
</td>
<td style="width: 257px">
<p class="CDPAlignCenter CDPAlign">0</p>
</td>
<td style="width: 162px">
<p class="CDPAlignCenter CDPAlign">2</p>
</td>
</tr>
<tr>
<td style="width: 281px">
<p class="CDPAlignCenter CDPAlign"><strong>Real outcome:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Does not have pneumonia</strong></p>
</td>
<td style="width: 257px">
<p class="CDPAlignCenter CDPAlign">0</p>
</td>
<td style="width: 162px">
<p class="CDPAlignCenter CDPAlign">199,998</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>After looking at this table, we can see that the dummy model won't be helpful to anyone. It didn't identify two people with the condition as positive. We call those errors <strong>False Negatives</strong> (<strong>FN</strong>). The model also correctly identified all patients with no pneumonia, or <strong>True Negatives</strong> (<strong>TN</strong>), but it has failed to diagnose ill patients correctly.</p>
<p class="mce-root"/>
<p>Now, suppose that your team has built a real model and got the following results: </p>
<table style="border-collapse: collapse;width: 100%;border-color: #000000;height: 141px" border="1">
<tbody>
<tr>
<td style="width: 281px"/>
<td style="width: 257px">
<p class="CDPAlignCenter CDPAlign"><strong>Model prediction:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Has pneumonia</strong></p>
</td>
<td style="width: 172px">
<p class="CDPAlignCenter CDPAlign"><strong>Model prediction:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Does not have pneumonia</strong></p>
</td>
</tr>
<tr>
<td style="width: 281px">
<p class="CDPAlignCenter CDPAlign"><strong>Real outcome:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Has pneumonia</strong></p>
</td>
<td style="width: 257px">
<p class="CDPAlignCenter CDPAlign">2</p>
</td>
<td style="width: 172px">
<p class="CDPAlignCenter CDPAlign">0</p>
</td>
</tr>
<tr>
<td style="width: 281px">
<p class="CDPAlignCenter CDPAlign"><strong>Real outcome:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Does not have pneumonia</strong></p>
</td>
<td style="width: 257px">
<p class="CDPAlignCenter CDPAlign">30</p>
</td>
<td style="width: 172px">
<p class="CDPAlignCenter CDPAlign">199,968</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>This model correctly identified two cases, making two <strong>True Positive</strong> (<strong>TP</strong>) predictions. This is a clear improvement over the previous iteration. However, the model also identified 30 people as having pneumonia, while they were not ill in reality. We call such an error a <strong>False Positive</strong> (<strong>FP</strong>) prediction. Is having 30 false positives a significant disadvantage? That depends on how physicians will use the model. If all subjects will be automatically prescribed with heavy medication with side-effects, false positives can be critical.</p>
<p>It may be less severe if we consider a positive model only as a possibility of having a disease. If a positive model answer only signals that the patient must go through a specific set of diagnostic procedures, then we can see a benefit: to achieve the same level of pneumonia identification, therapists will diagnose only 32 patients, where previously they had to investigate 200,000 cases. If we had not used the confusion table, we might have missed dangerous model behavior that would negatively affect people's health.</p>
<p>Next, your team has done another experiment and created a new model:</p>
<table style="border-collapse: collapse;width: 100%;border-color: #000000;height: 141px" border="1">
<tbody>
<tr>
<td style="width: 281px"/>
<td style="width: 257px">
<p class="CDPAlignCenter CDPAlign"><strong>Model prediction:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Has pneumonia</strong></p>
</td>
<td style="width: 169px">
<p class="CDPAlignCenter CDPAlign"><strong>Model prediction:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Does not have pneumonia</strong></p>
</td>
</tr>
<tr>
<td style="width: 281px">
<p class="CDPAlignCenter CDPAlign"><strong>Real outcome:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Has pneumonia</strong></p>
</td>
<td style="width: 257px">
<p class="CDPAlignCenter CDPAlign">0</p>
</td>
<td style="width: 169px">
<p class="CDPAlignCenter CDPAlign">2</p>
</td>
</tr>
<tr>
<td style="width: 281px">
<p class="CDPAlignCenter CDPAlign"><strong>Real outcome:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>Does not have pneumonia</strong></p>
</td>
<td style="width: 257px">
<p class="CDPAlignCenter CDPAlign">100,000</p>
</td>
<td style="width: 169px">
<p class="CDPAlignCenter CDPAlign">99,998</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Does this model perform better? The model would have missed one patient that needed therapy and assigned 100,000 healthy people to a treatment group, making physicians do unnecessary work. In truth, you can make the final decision only after presenting results to the people who will use the model. They may have a different opinion on what is best. It would be best to define this at the first stages of the project by creating a model-testing methodology document by collaborating with experts in the field.</p>
<p>You will face binary classification problems everywhere, thus having a good understanding of terminology is important. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>You can see all new concepts summed up in the following table:</p>
<table style="border-collapse: collapse;width: 100%;border-color: #000000;height: 141px" border="1">
<tbody>
<tr>
<td style="width: 271px"/>
<td style="width: 275px">
<p class="CDPAlignCenter CDPAlign"><strong>Model prediction:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>1 (positive case)</strong></p>
</td>
<td style="width: 166px">
<p class="CDPAlignCenter CDPAlign"><strong>Model prediction:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>0 (negative case)</strong></p>
</td>
</tr>
<tr>
<td style="width: 271px">
<p class="CDPAlignCenter CDPAlign"><strong>Real outcome:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>1 (positive case)</strong></p>
</td>
<td style="width: 275px">
<p class="CDPAlignCenter CDPAlign">TP</p>
</td>
<td style="width: 166px">
<p class="CDPAlignCenter CDPAlign">FN</p>
</td>
</tr>
<tr>
<td style="width: 271px">
<p class="CDPAlignCenter CDPAlign"><strong>Real outcome:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>0 (negative case)</strong></p>
</td>
<td style="width: 275px">
<p class="CDPAlignCenter CDPAlign">FP</p>
</td>
<td style="width: 166px">
<p class="CDPAlignCenter CDPAlign">TN</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>It is crucial to note that you can control the amount of false positive and false negative responses for a single model. Classifiers output a probability of a data point belonging to a class. That is, the model prediction is a number between 0 and 1. You can decide whether a prediction belongs to a positive or negative class by comparing it with a threshold. For example, if the threshold is 0.5, then any model prediction greater than 0.5 will belong to class 1 and to 0 otherwise.<br/>
<br/>
By changing the threshold, you can change the proportions between the cells in the confusion table. By choosing a large threshold, like 0.9, the volume of false positive responses will decrease, but false negative responses will increase. Threshold selection is essential for binary classification problems. Some environments, such as digital advertising, will be more forgiving to false positives, while in others, such as healthcare or insurance, you may find them unacceptable.</p>
<p>Confusion tables provide deep insights into classification problems but require your attention and time. This can be limiting when you want to do numerous experiments and compare many models. To simplify the process, statisticians and data scientists have designed many metrics that sum up classifier performance without suffering from problems like accuracy metrics do. First, let's examine some ways to summarize confusion table rows and columns. From there, we will explore how to condense it into a single statistic.</p>
<p>In the following table, you can see two new metrics for summarizing different kinds of errors, precision and recall:</p>
<table style="border-collapse: collapse;width: 100%;border-color: #000000;height: 141px" border="1">
<tbody>
<tr>
<td style="width: 115px"/>
<td style="width: 409px">
<p class="CDPAlignCenter CDPAlign"><strong>Model prediction:</strong></p>
<p class="CDPAlignCenter CDPAlign"><strong>1</strong> (<strong>positive case</strong>)</p>
</td>
<td style="width: 117px" class="CDPAlignCenter CDPAlign">
<p><strong>Model prediction:</strong></p>
<p><strong>0</strong> (<strong>negative case</strong>)</p>
</td>
<td style="width: 131px" class="CDPAlignCenter CDPAlign"><strong>Combined metric</strong></td>
</tr>
<tr>
<td style="width: 115px">
<p><strong>Real outcome:</strong></p>
<p><strong>1</strong> (<strong>positive case</strong>)</p>
</td>
<td style="width: 409px">
<p>True Positive</p>
</td>
<td style="width: 117px">
<p>False Negative</p>
</td>
<td style="width: 131px">
<p><img src="img/7714af33-14b7-42ca-a21b-8ed301f8b76c.png" style="width:9.92em;height:3.58em;" width="204" height="73"/></p>
</td>
</tr>
<tr>
<td style="width: 115px">
<p><strong>Real outcome:</strong></p>
<p><strong>0</strong> (<strong>negative case</strong>)</p>
</td>
<td style="width: 409px">
<p>False Positive</p>
</td>
<td style="width: 117px">
<p>True Negative</p>
</td>
<td style="width: 131px"/>
</tr>
<tr>
<td style="width: 115px"><strong>Combined metric</strong></td>
<td style="width: 409px">
<p><img src="img/9e75cead-a17d-444b-804a-6e25c5aa57f9.png" style="width:9.33em;height:3.50em;" width="183" height="69"/>, also called <strong>True Positive Rate</strong> (<strong>TPR</strong>)</p>
</td>
<td style="width: 117px"/>
<td style="width: 131px"/>
</tr>
</tbody>
</table>
<p> </p>
<p>Precision measures a proportion of positive (relevant) cases that your model has identified. If your model predicted 10 positive cases and 2 positive predictions turned out to be negative in reality, then its precision would be 0.8. Recall represents a probability of correctly predicting a positive case. If out of 10 positive cases, the model had predicted all 10 correctly (10 true positives) and marked 5 negative cases as positive (5 false positives), then its recall would be 0.67. A recall of 0.67 means that if our model predicts a positive case, it will be correct 67 times out of 100.</p>
<p>For binary classification, precision and recall diminish the amount of metrics we must work with to two. This is better, but not ideal. We can sum up everything into a single number by using a metric called <strong>F1-score</strong>. You can calculate F1 using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/692dbd05-b1da-4d50-b59d-f082413b7b83.png" style="width:12.42em;height:2.50em;" width="2180" height="440"/></p>
<p>F1 is 1 for a perfect classifier and 0 for the worst classifier. Because it considers both precision and recall, it does not suffer from the same problem as accuracy and is a better default metric for classification problems.</p>


            

            
        
    </div>
<div><h1 class="header-title">More about imbalanced classes</h1>
                
            
            
                
<p>In the preceding examples, you might have noticed that many prediction problems suffer from a phenomenon where one class occurs much more frequently than the others. Identifying diseases such as cancer, estimating probabilities of credit default, or detecting fraud in financial transactions are all examples of imbalanced problems – positive cases are much less frequent than the negative ones. In such situations, estimating classifier performance becomes tricky. Metrics such as accuracy start to show an overly optimistic picture, so you need to resort to more advanced technical metrics. The F1 score gives much more realistic values in this setting. However, the F1 score is calculated from class assignments (0 or 1 in the case of binary classification) rather than class probabilities (0.2 and 0.95 in the case of binary classification).</p>
<p class="mce-root"/>
<p>Most machine learning models output a probability of an example belonging to a certain class, rather than direct class assignment. In particular, a cancer-detection model could output a 0.32 (32%) disease probability based on the incoming data. Then we must decide whether the patient will be labeled as having cancer or not. To do this, we can use a threshold: all values lower than or equal to this threshold will be labeled as 0 (does not have cancer), and all values greater than this threshold will be considered as 1 (has cancer). The threshold can greatly affect the resulting model's quality, especially for imbalanced datasets. For example, lower threshold values of will likely result in more 0 labels, however, the relationship is not linear.</p>
<p>To illustrate this, let's take a trained model and generate predictions for the test dataset. If we calculate class assignments by taking lots of different thresholds, and then calculate the precision, recall, and F1 score for each of those assignments, we could depict each precision and recall value in a single plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c11524ea-c914-4a1d-97b1-75b79b2d55e8.png" style="width:54.08em;height:37.17em;" width="640" height="440"/></p>
<p>The preceding plot was made using the <kbd>yellowbrick</kbd> library, which contains many useful visualization tools for model selection and interpretation. You can see the capabilities of this library here: <a href="https://www.scikit-yb.org/en/latest/index.html">https://www.scikit-yb.org/en/latest/index.html</a>.</p>
<p>In the preceding plot, you can see the precision (blue), recall (green), and F1 (red) values for each threshold between 0 and 1. Based on this plot, we can see that 0.5, which is the default in many machine learning libraries, might not be the best choice and that something like 0.45 would yield more optimal metric values.</p>
<p>Another useful concept shown in the plot is the queue rate (depicted in magenta), which shows the proportion of instances in the test dataset labeled as positive. For the 0.45 threshold (identified in the plot as a dashed line), you can see that the queue rate is 0.4. This means that approximately 40% of all cases will be labeled as fraudulent. Depending on the business process in which the model will be used, positive cases might need to be further investigated by humans. In some cases, manual checking takes a lot of time or resources, but it is OK to misclassify a few positive instances for a much lower queue rate. In such cases, you might want to choose models with lower queue rates even if their performance is lower.</p>
<p>All information about precision, recall, and thresholds can be further summarized into a single number called the <strong>area under precision-recall curve</strong> (<strong>PR AUC</strong>). This metric can be used to make quick judgments over a large number of different models without making manual evaluations of model quality on different thresholds. Another metric that is frequently used for binary classifier evaluations is called the <strong>area under the</strong> <strong>receiver operating characteristic curve</strong> (<strong>ROC AUC</strong>). In general, you will want to use PR AUC for imbalanced datasets and ROC AUC for balanced datasets.<br/>
<br/>
The difference arises from the ways in which those metrics are calculated, but we will omit the technical details here for the sake of brevity. Calculating AUC metrics is a bit more involved than the other metrics presented in this chapter. For more information, check out <a href="https://www.chioka.in/differences-between-roc-auc-and-pr-auc/">https://www.chioka.in/differences-between-roc-auc-and-pr-auc/</a> and <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">https://en.wikipedia.org/wiki/Receiver_operating_characteristic</a>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>There is no single rule for choosing the right balance for the precision, recall, F1, and queue rate. Those values should be thoughtfully investigated with respect to the business process. Relying solely on technical metrics for model selection can result in a disaster, as models that are the best for your customers are not always the most accurate models. In some cases, high precision might be more important than recall, while for others, the queue rate will be most important. At this point, we need to introduce another kind of metric that will act as a bridge between technical metrics and business requirements: business metrics.</p>


            

            
        
    </div>
<div><h1 class="header-title">Applying business metrics</h1>
                
            
            
                
<p>While technical metrics may be essential in the model-development process, they do not speak the language of business. A bunch of confusion tables with the F1 score will rarely impress your customers or stakeholders. They are more concerned with the problem that the model will solve than with its internals. They won't be interested in the false positive rate, but they will listen when you will talk about the money that the model would save them in the next quarter. Therefore, designing a business metric is important. Your project will need a quality measure that is crystal clear for all key stakeholders, with or without experience in data science. If you are in the business environment, a good start would be to look at the <strong>key performance indicators</strong> (<strong>KPI</strong>) of business processes you are trying to improve using machine learning. It is likely that you will find a ready-to-use business metric.</p>
<p>At this point, we conclude an introduction to technical metrics. There are many more ways to test classification and regression models, all with pros and cons. Enumerating and describing them all would take a book in itself and would be unnecessary because we have already achieved our goal. Armed with new concepts from this chapter, you now understand the general flow of how to evaluate a machine learning model before testing it in real-world conditions. Now you can use offline model testing to check the model's quality before deploying the model. Next, we will explore online testing to complete your understanding of a model's quality assessment.</p>
<p class="mce-root"/>


            

            
        
    </div>
<div><h1 class="header-title">Online model testing</h1>
                
            
            
                
<p>Even a great offline model testing pipeline won't guarantee that the model will perform exactly the same in production. There are always risks that can affect your model performance, such as the following:</p>
<ul>
<li><strong>Humans</strong>: We can make mistakes and leave bugs in the code.</li>
<li><strong>Data collection</strong>: Selection bias and incorrect data-collection procedures may disrupt true metric values.</li>
<li><strong>Changes</strong>: Real-world data may change and deviate from your training dataset, leading to unexpected model behavior.</li>
</ul>
<p>The only way to be certain about model performance in the near future is to perform a live test. Depending on the environment, such test may introduce big risks. For example, models that assess airplane engine quality or patient health would be unsuitable for real-world testing before we become confident in their performance. </p>
<p>When the time for a live test comes, you will want to minimize risks while making statistically valid conclusions. Thankfully, there is a statistical framework for that purpose known as hypothesis testing. When performing a hypothesis test, you check the validity of some idea (hypothesis) by collecting data and executing a statistical test. Imagine you need to check whether your new advertising model increases revenues from the ad service. To do this, you randomly split all your clients into two groups: one group uses the old advertising algorithm, while the others see ads recommended by a new algorithm. After you have collected a sufficient volume of data, you compare two groups and measure differences between them. Why do we need to bother with statistics, you may ask?</p>
<p>Because we can answer the following questions only with the help of stats:</p>
<ul>
<li>How should I sort (sample) individuals into each group? Can my sampling process distort results of the test?</li>
<li>What is the minimum number of clients in each group? Can random fluctuations in the data affect my measurements?</li>
<li>How long should I run the test for to get a confident answer?</li>
<li>What formula should I use to compare results in each group?</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The experiment setup for a hypothesis test splits test targets into two groups on purpose. We can try to use a single group instead. For instance, we can take one set of measurements with the old model. After the first part of the experiment is finished, we can deploy the new algorithm and measure its effect. Then, we compare two measurements made one after another. What could go wrong? In fact, the results we get wouldn't mean anything. Many things could have changed in between our measurements, such as the following:</p>
<ul>
<li>User preferences</li>
<li>General user mood</li>
<li>Popularity of our service</li>
<li>Average user profile</li>
<li>Any other attribute of users or businesses</li>
</ul>
<p>All these hidden effects could affect our measurements in unpredictable ways, which is why we need two groups: test and control. We must select these groups in such a way that the only difference between them is our hypothesis. It should be present in the test group and missing from the control group. To illustrate, in medical trials, control groups are the ones who get the placebo. Suppose we want to test the positive effect of a new painkiller. Here are some examples of bad test setups:</p>
<ul>
<li>The control group consists only of women.</li>
<li>The test and control groups are in different geographical locations.</li>
<li>You use biased interviews to preselect people for an experiment.</li>
</ul>
<p>The easiest way to create groups is random selection. Truly random selection may be hard to do in the real world, but is easy if you deal with internet services. There, you may just randomly decide which version of your algorithm to use for each active user. Be sure to always design experiment setups with an experienced statistician or data scientist, as correct tests are notoriously hard to execute, especially in offline settings.</p>
<p>Statistical tests check the validity of a null hypothesis, that is, that the results you got are by chance. The opposite result is called an alternative hypothesis. For instance, here is the hypothesis set for our ad model test:</p>
<ul>
<li><strong>Null hypothesis</strong>: The new model does not affect the ad service revenue.</li>
<li><strong>Alternative hypothesis</strong>: The new model affects the ad service revenue.</li>
</ul>
<p class="mce-root"/>
<p>Typically, a statistical test measures the probability of a null hypothesis being true. If the chances are low, then the alternative hypothesis is true. Otherwise, we accept the null hypothesis. If, according to a statistical test, the probability that the new model does not affect service revenue would be 5%, we would say that we accept the alternative hypothesis at a 95% confidence level. This means the model affects the ad service revenue with a 95% probability. The significance level for rejecting the null hypothesis depends on the level of risk you want to take. For an ad model, a 95% significance may be enough, while no less than a 99% significance is satisfactory for a model that tests patient health conditions.</p>
<p>The most typical hypothesis test is comparing two means. If we use this test in our ad model example, we would measure average revenues with and without the new ranking algorithm. We may accept or reject the null hypothesis using a test statistic when the experiment is finished.</p>
<p>The amount of data you need to collect for conducting a hypothesis test depends on several factors:</p>
<ul>
<li><strong>Confidence level</strong>: The more statistical confidence you need, the more data is required to support the evidence.</li>
<li><strong>Statistical power</strong>: This measures the probability of detecting a significant difference, if one exists. The more statistical power your test has, the lower the chance of false negative responses.</li>
<li><strong>Hypothesized difference and population variance</strong>: If your data has large variance, you need to collect more data to detect a significant difference. If the difference between the two means is smaller than population variance, you would need even more data.</li>
</ul>
<p>You can see how different test parameters determine their data hunger in the following table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Confidence level</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Statistical power</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Hypothesized difference<br/>
<br/></strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Population variance</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Recommended sample size</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">95%</td>
<td class="CDPAlignCenter CDPAlign">90%</td>
<td class="CDPAlignCenter CDPAlign">$10</td>
<td class="CDPAlignCenter CDPAlign">$100</td>
<td class="CDPAlignCenter CDPAlign">22 ad demonstrations to clients</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">99%</td>
<td class="CDPAlignCenter CDPAlign">90%</td>
<td class="CDPAlignCenter CDPAlign">$10</td>
<td class="CDPAlignCenter CDPAlign">$100</td>
<td class="CDPAlignCenter CDPAlign">30 ad demonstrations to clients</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">99%</td>
<td class="CDPAlignCenter CDPAlign">90%</td>
<td class="CDPAlignCenter CDPAlign">$1</td>
<td class="CDPAlignCenter CDPAlign">$100</td>
<td class="CDPAlignCenter CDPAlign">2,976 ad demonstrations to clients</td>
</tr>
</tbody>
</table>
<p> </p>
<p>While powerful, hypothesis tests have limitations: you need to wait until the experiment ends before you can apply its results. If your model is bad, you won't be able to reduce damage without compromising the test procedure. Another limitation is that you can test only one model at a time with a single hypothesis test. </p>
<p class="mce-root"/>
<p>In situations where you can trade off statistical rigor for speed and risk-aversion, there is an alternative approach called <strong>Multi-Armed Bandits</strong> (<strong>MABs</strong>). To understand how MABs work, imagine yourself inside a casino with lots of slot machines. You know that some of those machines yield better returns than others. Your task is to find the best slot machine with a minimal number of trials. Thus, you try different (multi) arms of slot machines (bandits) to maximize your reward. You can extend this situation to testing multiple ad models: for each user, you must find a model that is most likely to increase your ad revenue.</p>
<p>The most popular MAB algorithm is called an epsilon-greedy bandit. Despite the name, the inner workings of the method are simple:</p>
<ol>
<li>Select a small number called <strong>epsilon</strong>. Suppose we have chosen 0.01.</li>
<li>Choose a random number between 0 and 1. This number will determine whether MAB will explore or exploit a possible set of choices.</li>
<li>If the number is lower or equal to epsilon, make a choice at random and record a reward after making an action tied to your choice. We call this process exploration – MAB tries different actions at random with a low probability to find out their mean reward.</li>
<li>If your number is greater than epsilon, make the best choice according to the data you have collected. We call this process exploitation – MAB exploits knowledge it has collected to execute an action that has the best expected reward. MAB selects the best action by averaging all recorded rewards for each choice and selecting a choice with the greatest reward expectation.</li>
</ol>
<p>Frequently, we start with large values of epsilon and decrease it to smaller values. In this way, MAB explores lots of random choices at the start and exploits the most profitable actions toward the end. The exploration frequency is gradually diminishing, becoming closer to zero.</p>
<p>When you first launch MAB, it collects rewards from random actions. As time passes, you will see that average rewards for all choices converge to their true values. The major benefit of MABs is that they change their behavior in real time. While someone is waiting for a hypothesis test results, MAB gives you a changing picture while covering to the best choice. Bandits are one of the most basic reinforcement learning algorithms. Despite their simplicity, they can provide good results.</p>
<p class="mce-root"/>
<p>We now have two new testing approaches to use. How do we choose between them? Unfortunately, there is no simple answer. Hypothesis tests and MABs pose different constraints on data, sampling processes, and experiment conditions. It is better to consult an experienced statistician or a data scientist before deciding. Mathematical constraints are not the only things that affect the choice; the environment is also important. MABs are easy to apply in situations where you can test different choices on random individuals from the whole population. This may be very convenient when testing models for a large online retailer, but is impossible for clinical trials, where you are better to apply hypothesis testing. Let's see a rule of thumb for choosing between MABs and hypothesis tests:<br/></p>
<ul>
<li>MABs are better suited to environments where you need to test many alternatives with a limited set of resources. You trade off statistical rigor for efficiency when using MABs. MABs can take a lot of time to converge, gradually improving over time.</li>
<li>You should apply hypothesis tests if you have only one alternative, if your trial involves great risks, or if you need a statistically-rigorous answer. Hypothesis tests take a fixed amount of time and resources to complete, but impose larger risks than MABs.</li>
</ul>
<p>While testing models in an online setting is extremely important for making sure that your offline test results stay true after the deployment stage, there is still a danger zone that we have not covered. Abrupt and unexpected changes in data can severely affect or even break the deployed model, so it is also important to monitor incoming data quality.</p>


            

            
        
    </div>
<div><h1 class="header-title">Online data testing</h1>
                
            
            
                
<p>Even after performing successful online tests, you are not fully guarded against unexpected issues with model operation. Machine learning models are sensitive to incoming data. Good models have a certain degree of generalization, but significant changes in data or underlying processes that generate data can lead the model predictions astray. If online data significantly diverges from test data, you can't be certain about model performance before performing online tests. If the test data differs from the training data, then your model won't work as expected.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To overcome this, your system needs to monitor all incoming data and check its quality on the fly. Here are some typical checks:</p>
<ul>
<li>Missing values in mandatory data fields</li>
<li>Minimum and maximum values</li>
<li>Acceptable values of categorical data fields</li>
<li>String data formats (dates, addresses)</li>
<li>Target variable statistics (distribution checks, averages)</li>
</ul>


            

            
        
    </div>
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we answered a very important question: what does it mean for a model to work correctly? We explored the nature of errors and studied metrics that can quantify and measure model errors. We drew a line between offline and online model testing and defined testing procedures for both types. We can perform offline model testing using train/validation/test data splits and cross-validation. For online testing, we can choose between hypothesis tests and MABs.</p>
<p>In the next chapter, we will look into the inner workings of data science. We will dive into the main concepts behind machine learning and deep learning, giving an intuitive understanding of how machines learn. </p>


            

            
        
    </div></body></html>