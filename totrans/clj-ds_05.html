<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch05" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 5. Big Data</h1></div></div></div><div><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote1" summary="Block quote"><tr class="calibre17"><td class="calibre18"> </td><td class="calibre18"><p class="calibre19"><em class="calibre13">"More is different."</em></p></td><td class="calibre18"> </td></tr><tr class="calibre17"><td class="calibre18"> </td><td colspan="2" class="calibre20">--<em class="calibre13">Philip Warren Anderson</em></td></tr></table></div><p class="calibre11">In the previous chapters, we've used regression techniques to fit models to the data. In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <em class="calibre13">Correlation</em>, for example, we built a linear model that used ordinary least squares and the normal equation to fit a straight line through the athletes' heights and log weights. In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Classification">Chapter 4</a>, <em class="calibre13">Classification</em>, we used Incanter's optimize namespace to minimize the logistic cost function and build a classifier of Titanic's passengers. In this chapter, we'll apply similar analysis in a way that's suitable for much larger quantities of data.</p><p class="calibre11">We'll be working with a relatively modest dataset of only 100,000 records. This isn't big data (at 100 MB, it will fit comfortably in the memory of one machine), but it's large enough to demonstrate the common techniques of large-scale data processing. Using Hadoop (the popular framework for distributed computation) as its case study, this chapter will focus on how to scale algorithms to very large volumes of data through parallelism. We'll cover two libraries that Clojure offers to work with Hadoop—<strong class="calibre12">Tesser</strong> and <strong class="calibre12">Parkour</strong>.</p><p class="calibre11">Before we get to Hadoop and distributed data processing though, we'll see how some of the same principles that enable Hadoop to be effective at a very large scale can also be applied to data processing on a single machine, by taking advantage of the parallel capacity available in all modern computers.</p><div><div><div><div><h1 class="title2"><a id="ch05lvl1sec95" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Downloading the code and data</h1></div></div></div><p class="calibre11">This chapter <a id="id556" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>makes use of data on individual income by the zip code provided by the U.S. Internal Revenue Service (IRS). The data contains selected income and tax items classified by state, zip code, and income classes.</p><p class="calibre11">It's 100 MB in size and can be downloaded from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.irs.gov/pub/irs-soi/12zpallagi.csv">http://www.irs.gov/pub/irs-soi/12zpallagi.csv</a> to <a id="id557" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the example code's data directory. Since the file contains the IRS Statistics of Income (SoI), we've renamed the file to <code class="literal">soi.csv</code> for the examples.</p><div><div><h3 class="title4"><a id="note46" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The example code for this <a id="id558" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chapter is available from the Packt Publishing's website or <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clojuredatascience/ch5-big-data">https://github.com/clojuredatascience/ch5-big-data</a>.</p></div></div><p class="calibre11">As usual, a script has been provided to download and rename the data for you. It can be run on the <a id="id559" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>command line from within the project directory with:</p><div><pre class="programlisting">
<strong class="calibre12">script/download-data.sh</strong>
</pre></div><p class="calibre11">If you run this, the file will be downloaded and renamed automatically.</p><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec95" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Inspecting the data</h2></div></div></div><p class="calibre11">Once you've downloaded <a id="id560" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the data, take a look at the column headings in the first line of the file. One way to access the first line of the file is to load the file into memory, split on newline characters, and take the first result. The Clojure core library's function <code class="literal">slurp</code> will return the whole file as a string:</p><div><pre class="programlisting">(defn ex-5-1 []
  (-&gt; (slurp "data/soi.csv")
      (str/split #"\n")
      (first)))</pre></div><p class="calibre11">The file is around 100 MB in size on disk. When loaded into memory and converted into object representations, the data will occupy more space in memory. This is particularly wasteful when we're only interested in the first row.</p><p class="calibre11">Fortunately, we don't have to load the whole file into memory if we take advantage of Clojure's lazy sequences. Instead of returning a string representation of the contents of the whole file, we could return a reference to the file and then step through it one line at a time:</p><div><pre class="programlisting">(defn ex-5-2 []
  (-&gt; (io/reader "data/soi.csv")
      (line-seq)
      (first)))</pre></div><p class="calibre11">In the preceding code, we're using <code class="literal">clojure.java.io/reader</code> to return a reference to the file. Also, we're using the <code class="literal">clojure.core</code> function <code class="literal">line-seq</code> to return a lazy sequence of lines from the file. In this way, we can read files even larger than the available memory.</p><p class="calibre11">The result of the previous function is as follows:</p><div><pre class="programlisting">"STATEFIPS,STATE,zipcode,AGI_STUB,N1,MARS1,MARS2,MARS4,PREP,N2,NUMDEP,A00100,N00200,A00200,N00300,A00300,N00600,A00600,N00650,A00650,N00900,A00900,SCHF,N01000,A01000,N01400,A01400,N01700,A01700,N02300,A02300,N02500,A02500,N03300,A03300,N00101,A00101,N04470,A04470,N18425,A18425,N18450,A18450,N18500,A18500,N18300,A18300,N19300,A19300,N19700,A19700,N04800,A04800,N07100,A07100,N07220,A07220,N07180,A07180,N07260,A07260,N59660,A59660,N59720,A59720,N11070,A11070,N09600,A09600,N06500,A06500,N10300,A10300,N11901,A11901,N11902,A11902"</pre></div><p class="calibre11">There are 77 fields in <a id="id561" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the file, so we won't identify them all. The first four fields are:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">STATEFIPS</code>: This is the Federal Information Processing System (FIPS) code.</li><li class="listitem"><code class="literal">STATE</code>: This is the two-letter code for the State.</li><li class="listitem"><code class="literal">zipcode</code>: This is the 5-digit zip code.</li><li class="listitem"><code class="literal">AGI_STUB</code>: This is the side of the adjusted gross income, binned in the following way:<div><ol class="orderedlist"><li class="listitem1">$1 under $25,000</li><li class="listitem1">$25,000 under $50,000</li><li class="listitem1">$50,000 under $75,000</li><li class="listitem1">$75,000 under $100,000</li><li class="listitem1">$100,000 under $200,000</li><li class="listitem1">$200,000 or more</li></ol></div></li></ul></div><p class="calibre11">The other fields that we're interested in are as follows:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">N1</code>: The number of returns submitted</li><li class="listitem"><code class="literal">MARS2</code>: The number of joint returns submitted</li><li class="listitem"><code class="literal">NUMDEP</code>: The number of dependents</li><li class="listitem"><code class="literal">N00200</code>: The number of returns with salaries and wages</li><li class="listitem"><code class="literal">N02300</code>: The number of returns with unemployment compensation</li></ul></div><p class="calibre11">If you're curious, the <a id="id562" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>full list of column definitions is available in the IRS data definition document at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.irs.gov/pub/irs-soi/12zpdoc.doc">http://www.irs.gov/pub/irs-soi/12zpdoc.doc</a>.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec96" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Counting the records</h2></div></div></div><p class="calibre11">Our file is certainly <a id="id563" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>wide, but is it tall? We'd like to determine the total number of rows in the file. Having created a lazy sequence, this is simply a matter of counting the length of the sequence:</p><div><pre class="programlisting">(defn ex-5-3 []
  (-&gt; (io/reader "data/soi.csv")
      (line-seq)
      (count)))</pre></div><p class="calibre11">The preceding example returns 166,905, including the header row, so we know there are actually 166,904 rows in the file.</p><p class="calibre11">The <code class="literal">count</code> function is the simplest way to count the number of elements in a sequence. For vectors (and other types implementing the counted interface), this is also the most efficient one, since the collection already knows how many elements it contains and therefore it doesn't need to recalculate it. For a lazy sequence however, the only way to determine how many elements are contained in the sequence is to step through it from the beginning to the end.</p><p class="calibre11">Clojure's <a id="id564" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>implementation of <code class="literal">count</code> is written in Java, but the Clojure equivalent would be a reduce over the sequence like this:</p><div><pre class="programlisting">(defn ex-5-4 []
  (-&gt;&gt; (io/reader "data/soi.csv")
       (line-seq)
       (reduce (fn [i x]
                 (inc i)) 0)))</pre></div><p class="calibre11">The preceding function we pass to <code class="literal">reduce</code> accepts a counter <code class="literal">i</code> and the next element from the sequence <code class="literal">x</code>. For each <code class="literal">x</code>, we simply increment the counter <code class="literal">i</code>. The reduce function accepts an initial value of zero, which represents the concept of nothing. If there are no lines to reduce over, zero will be returned.</p><p class="calibre11">As of version 1.5, Clojure <a id="id565" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>offers the reducers library (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://clojure.org/reducers">http://clojure.org/reducers</a>), which provides an alternative way to perform reductions that trades memory efficiency for speed.</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch05lvl1sec96" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The reducers library</h1></div></div></div><p class="calibre11">The <code class="literal">count</code> <a id="id566" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>operation we implemented previously is a sequential algorithm. Each line is processed one at a time until the sequence is exhausted. But there is nothing about the operation that demands that it must be done in this way.</p><p class="calibre11">We could split the number of lines into two sequences (ideally of roughly equal length) and reduce over each sequence independently. When we're done, we would just add together the total number of lines from each sequence to get the total number of lines in the file:</p><div><img src="img/7180OS_05_100.jpg" alt="The reducers library" class="calibre233"/></div><p class="calibre11">If each <strong class="calibre12">Reduce</strong> ran on its own processing unit, then the two count operations would run in parallel. All the other things being equal, the algorithm would run twice as fast. This is one of the aims <a id="id567" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the <code class="literal">clojure.core.reducers</code> library—to bring the benefit of parallelism to algorithms implemented on a single machine by taking advantage of multiple cores.</p><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec97" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Parallel folds with reducers</h2></div></div></div><p class="calibre11">The parallel <a id="id568" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>implementation of reduce implemented by the reducers library is called <a id="id569" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><strong class="calibre12">fold</strong>. To make use of a fold, we have to supply a combiner function that will take the results of our reduced sequences (the partial row counts) and return the final result. Since our row counts are numbers, the combiner function is simply <code class="literal">+</code>.</p><div><div><h3 class="title6"><a id="note47" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Reducers are a part of Clojure's standard library, they do not need to be added as an external dependency.</p></div></div><p class="calibre11">The adjusted example, using <code class="literal">clojure.core.reducers</code> as <code class="literal">r</code>, looks like this:</p><div><pre class="programlisting">(defn ex-5-5 []
  (-&gt;&gt; (io/reader "data/soi.csv")
       (line-seq)
       (r/fold + (fn [i x]
                   (inc i)))))</pre></div><p class="calibre11">The combiner function, <code class="literal">+</code>, has been included as the first argument to fold and our unchanged reduce function is supplied as the second argument. We no longer need to pass the initial value of zero—<code class="literal">fold</code> will get the initial value by calling the combiner function with no arguments. Our preceding example works because <code class="literal">+</code>, called with no arguments, already returns zero:</p><div><pre class="programlisting">(defn ex-5-6 []
  (+))

;; 0</pre></div><p class="calibre11">To participate in folding then, it's important that the combiner function have two implementations: one with zero arguments that returns the identity value and another with two arguments that <em class="calibre13">combines</em> the arguments. Different folds will, of course, require different combiner functions and identity values. For example, the identity value for multiplication is <code class="literal">1</code>.</p><p class="calibre11">We can visualize the <a id="id570" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>process of seeding the computation with an identity value, iteratively reducing over the sequence of <code class="literal">xs</code> and combining the reductions into an output value as a tree:</p><div><img src="img/7180OS_05_110.jpg" alt="Parallel folds with reducers" class="calibre47"/></div><p class="calibre11">There may be more than two reductions to combine, of course. The default implementation of <code class="literal">fold</code> will split the input collection into chunks of 512 elements. Our 166,000-element sequence will therefore generate 325 reductions to be combined. We're going to run out of page real estate quite quickly with a tree representation diagram, so let's visualize the process more schematically instead—as a two-step reduce and combine process.</p><p class="calibre11">The first step performs a parallel reduce across all the chunks in the collection. The second step performs a serial reduce over the intermediate results to arrive at the final result:</p><div><img src="img/7180OS_05_120.jpg" alt="Parallel folds with reducers" class="calibre234"/></div><p class="calibre11">The preceding <a id="id571" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>representation shows reduce over several sequences of <code class="literal">xs</code>, represented here as circles, into a series of outputs, represented here as squares. The squares are combined serially to produce the final result, represented by a star.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec98" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Loading large files with iota</h2></div></div></div><p class="calibre11">Calling <code class="literal">fold</code> on a <a id="id572" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>lazy sequence requires Clojure to realize the sequence into memory and then chunk the sequence into groups for parallel execution. For situations where the calculation performed on each row is small, the overhead involved in coordination outweighs the benefit of parallelism. We can <a id="id573" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>improve the situation slightly by using a library called <code class="literal">iota</code> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/thebusby/iota">https://github.com/thebusby/iota</a>).</p><div><div><h3 class="title6"><a id="note48" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The <code class="literal">iota</code> library loads files directly into the data structures suitable for folding over with <a id="id574" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>reducers that can handle files larger than available memory by making use of memory-mapped files.</p></div></div><p class="calibre11">With <code class="literal">iota</code> in the place of our <code class="literal">line-seq</code> function, our line count simply becomes:</p><div><pre class="programlisting">(defn ex-5-7 []
  (-&gt;&gt; (iota/seq "data/soi.csv")
       (r/fold + (fn [i x]
                   (inc i)))))</pre></div><p class="calibre11">So far, we've just been working with the sequences of unformatted lines, but if we're going to do anything more than counting the rows, we'll want to parse them into a more useful data <a id="id575" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>structure. This is another area in which <a id="id576" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Clojure's reducers can help make our code more efficient.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec99" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Creating a reducers processing pipeline</h2></div></div></div><p class="calibre11">We already <a id="id577" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>know that the file is comma-separated, so let's first create a function to turn each row into a vector of fields. All fields except the first two contain numeric data, so let's parse them into doubles while we're at it:</p><div><pre class="programlisting">(defn parse-double [x]
  (Double/parseDouble x))

(defn parse-line [line]
  (let [[text-fields double-fields] (-&gt;&gt; (str/split line #",")
                                         (split-at 2))]
    (concat text-fields
            (map parse-double double-fields))))</pre></div><p class="calibre11">We're using the reducers version of <code class="literal">map</code> to apply our <code class="literal">parse-line</code> function to each of the lines from the file in turn:</p><div><pre class="programlisting">(defn ex-5-8 []
   (-&gt;&gt; (iota/seq "data/soi.csv")
        (r/drop 1)
        (r/map parse-line)
        (r/take 1)
        (into [])))

;; [("01" "AL" 0.0 1.0 889920.0 490850.0 ...)]</pre></div><p class="calibre11">The final <code class="literal">into</code> function call converts the reducers' internal representation (a reducible collection) into a Clojure vector. The previous example should return a sequence of 77 fields, representing the first row of the file after the header.</p><p class="calibre11">We're just dropping the column names at the moment, but it would be great if we could make use of these to return a map representation of each record, associating the column name with the field value. The keys of the map would be the column headings and the values would be the parsed fields. The <code class="literal">clojure.core</code> function <code class="literal">zipmap</code> will create a map out of two sequences—one for the keys and one for the values:</p><div><pre class="programlisting">(defn parse-columns [line]
  (-&gt;&gt; (str/split line #",")
       (map keyword)))

(defn ex-5-9 []
  (let [data (iota/seq "data/soi.csv")
        column-names (parse-columns (first data))]
    (-&gt;&gt; (r/drop 1 data)
         (r/map parse-line)
         (r/map (fn [fields]
                  (zipmap column-names fields)))
         (r/take 1)
         (into []))))</pre></div><p class="calibre11">This function returns a map representation of each row, a much more user-friendly data structure:</p><div><pre class="programlisting">[{:N2 1505430.0, :A19300 181519.0, :MARS4 256900.0 ...}]</pre></div><p class="calibre11">A great thing about Clojure's reducers is that in the preceding computation, calls to <code class="literal">r/map</code>, <code class="literal">r/drop</code> and <code class="literal">r/take</code> are composed into a reduction that will be performed in a single pass <a id="id578" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>over the data. This becomes particularly valuable as the number of operations increases.</p><p class="calibre11">Let's assume that we'd like to filter out zero ZIP codes. We could extend the reducers pipeline like this:</p><div><pre class="programlisting">(defn ex-5-10 []
  (let [data (iota/seq "data/soi.csv")
        column-names (parse-columns (first data))]
    (-&gt;&gt; (r/drop 1 data)
         (r/map parse-line)
         (r/map (fn [fields]
                  (zipmap column-names fields)))
         (r/remove (fn [record]
                     (zero? (:zipcode record))))
         (r/take 1)
         (into []))))</pre></div><p class="calibre11">The <code class="literal">r/remove</code> step is now also being run together with the <code class="literal">r/map</code>, <code class="literal">r/drop</code> and <code class="literal">r/take</code> calls. As the size of the data increases, it becomes increasingly important to avoid making multiple iterations over the data unnecessarily. Using Clojure's reducers ensures that our calculations are compiled into a single pass.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec100" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Curried reductions with reducers</h2></div></div></div><p class="calibre11">To make the <a id="id579" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>process clearer, we can create a <strong class="calibre12">curried</strong> version of each of our previous steps. To parse the lines, create a record from the fields and filter zero ZIP codes. The curried version of the function is a reduction waiting for a collection:</p><div><pre class="programlisting">(def line-formatter
  (r/map parse-line))

(defn record-formatter [column-names]
  (r/map (fn [fields]
           (zipmap column-names fields))))

(def remove-zero-zip
  (r/remove (fn [record]
              (zero? (:zipcode record)))))</pre></div><p class="calibre11">In each case, we're calling one of reducers' functions, but without providing a collection. The response is a curried version of the function that can be applied to the collection at a later time. The curried functions can be composed together into a single <code class="literal">parse-file</code> function using <code class="literal">comp</code>:</p><div><pre class="programlisting">(defn load-data [file]
  (let [data (iota/seq file)
        col-names  (parse-columns (first data))
        parse-file (comp remove-zero-zip
                         (record-formatter col-names)
                         line-formatter)]
    (parse-file (rest data))))</pre></div><p class="calibre11">It's only when the <code class="literal">parse-file</code> function is called with a sequence that the pipeline is actually executed.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec101" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Statistical folds with reducers</h2></div></div></div><p class="calibre11">With the <a id="id580" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data parsed, it's time to perform some descriptive statistics. Let's assume that we'd like to know the mean <em class="calibre13">number of returns</em> (column <code class="literal">N1</code>) submitted to the IRS by ZIP code. One way of doing this—the way we've done several times throughout the book—is by adding up the values and dividing it by the count. Our first attempt might look like this:</p><div><pre class="programlisting">(defn ex-5-11 []
  (let [data (load-data "data/soi.csv")
        xs (into [] (r/map :N1 data))]
    (/ (reduce + xs)
       (count xs))))

;; 853.37</pre></div><p class="calibre11">While this works, it's comparatively slow. We iterate over the data once to create <code class="literal">xs</code>, a second time to calculate the sum, and a third time to calculate the count. The bigger our dataset gets, the larger the time penalty we'll pay. Ideally, we would be able to calculate the mean value in a single pass over the data, just like our <code class="literal">parse-file</code> function previously. It <a id="id581" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>would be even better if we can perform it in parallel too. </p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec102" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Associativity</h2></div></div></div><p class="calibre11">Before we <a id="id582" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>proceed, it's useful to take a moment to reflect on why the following code wouldn't do what we want:</p><div><pre class="programlisting">(defn mean
  ([] 0)
  ([x y] (/ (+ x y) 2)))</pre></div><p class="calibre11">Our <code class="literal">mean</code> function is a function of two arities. Without arguments, it returns zero, the identity for the <code class="literal">mean</code> computation. With two arguments, it returns their mean:</p><div><pre class="programlisting">(defn ex-5-12 []
  (-&gt;&gt; (load-data "data/soi.csv")
       (r/map :N1)
       (r/fold mean)))

;; 930.54</pre></div><p class="calibre11">The preceding example folds over the <code class="literal">N1</code> data with our <code class="literal">mean</code> function and produces a different result from the one we obtained previously. If we could expand out the computation for the first three <code class="literal">xs</code>, we might see something like the following code:</p><div><pre class="programlisting">(mean (mean (mean 0 a) b) c)</pre></div><p class="calibre11">This is a bad idea, because the <code class="literal">mean</code> function is not associative. For an associative function, the following holds true:</p><div><img src="img/7180OS_05_01.jpg" alt="Associativity" class="calibre235"/></div><p class="calibre11">Addition is associative, but multiplication and division are not. So the <code class="literal">mean</code> function is not associative either. Contrast the <code class="literal">mean</code> function with the following simple addition:</p><div><pre class="programlisting">
<code class="literal">(+ 1 (+ 2 3))</code>
</pre></div><p class="calibre11">This yields an identical result to:</p><div><pre class="programlisting">
<code class="literal">(+ (+ 1 2) 3)</code>
</pre></div><p class="calibre11">It doesn't matter how the arguments to <code class="literal">+</code> are partitioned. Associativity is an important property of functions used to reduce over a set of data because, by definition, the results of a previous calculation are treated as inputs to the next.</p><p class="calibre11">The easiest way of converting the <code class="literal">mean</code> function into an associative function is to calculate the sum <a id="id583" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and the count separately. Since the sum and the count are associative, they can be calculated in parallel over the data. The <code class="literal">mean</code> function can be calculated simply by dividing one by the other.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec103" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating the mean using fold</h2></div></div></div><p class="calibre11">We'll create a <a id="id584" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>fold using two custom functions, <code class="literal">mean-combiner</code> and <code class="literal">mean-reducer</code>. This requires defining three entities:</p><div><ul class="itemizedlist"><li class="listitem">The identity value for the fold</li><li class="listitem">The reduce function</li><li class="listitem">The combine function</li></ul></div><p class="calibre11">We discovered the <a id="id585" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>benefits of associativity in the previous section, and so we'll want to update our intermediate <code class="literal">mean</code> by using associative operations only and calculating the sum and count separately. One way of representing the two values is a map of two keys, <code class="literal">:count</code> and <code class="literal">:sum</code>. The value that represents zero for our mean would be a sum of zero and a count of zero, or a map such as the following: <code class="literal">{:count 0 :sum 0}</code>.</p><p class="calibre11">The combine function, <code class="literal">mean-combiner</code>, provides the seed value when it's called without arguments. The two-argument combiner needs to add together the <code class="literal">:count</code> and the <code class="literal">:sum</code> for each of the two arguments. We can achieve this by merging the maps with <code class="literal">+</code>:</p><div><pre class="programlisting">(defn mean-combiner
  ([] {:count 0 :sum 0})
  ([a b] (merge-with + a b)))</pre></div><p class="calibre11">The <code class="literal">mean-reducer</code> function needs to accept an accumulated value (either an identity value or the results of a previous reduction) and incorporate the new <code class="literal">x</code>. We do this simply by incrementing the <code class="literal">:count</code> and adding <code class="literal">x</code> to the accumulated <code class="literal">:sum</code>:</p><div><pre class="programlisting">(defn mean-reducer [acc x]
  (-&gt; acc
      (update-in [:count] inc)
      (update-in [:sum] + x)))</pre></div><p class="calibre11">The preceding two functions are enough to completely specify our <code class="literal">mean</code> fold:</p><div><pre class="programlisting">(defn ex-5-13 []
  (-&gt;&gt; (load-data "data/soi.csv")
       (r/map :N1)
       (r/fold mean-combiner
               mean-reducer)))

;; {:count 166598, :sum 1.4216975E8}</pre></div><p class="calibre11">The result gives us all we need to calculate the mean of <code class="literal">N1</code>, which is calculated in only one pass over the <a id="id586" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data. The final step of the <a id="id587" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculation can be performed with the following <code class="literal">mean-post-combiner</code> function:</p><div><pre class="programlisting">(defn mean-post-combiner [{:keys [count sum]}]
  (if (zero? count) 0 (/ sum count)))

(defn ex-5-14 []
  (-&gt;&gt; (load-data "data/soi.csv")
       (r/map :N1)
       (r/fold mean-combiner
               mean-reducer)
       (mean-post-combiner)))

;; 853.37</pre></div><p class="calibre11">Happily, the values agree with the mean we calculated previously.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec104" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating the variance using fold</h2></div></div></div><p class="calibre11">Next, we'd <a id="id588" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>like to calculate the <a id="id589" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>variance of the <code class="literal">N1</code> values. Remember that the variance is the mean squared difference from the mean:</p><div><img src="img/7180OS_05_02.jpg" alt="Calculating the variance using fold" class="calibre41"/></div><p class="calibre11">To implement this as a fold, we might write something as follows:</p><div><pre class="programlisting">(defn ex-5-15 []
   (let [data (-&gt;&gt; (load-data "data/soi.csv")
                   (r/map :N1))
         mean-x (-&gt;&gt; data
                     (r/fold mean-combiner
                             mean-reducer)
                     (mean-post-combine))
         sq-diff (fn [x] (i/pow (- x mean-x) 2))]
     (-&gt;&gt; data
          (r/map sq-diff)
          (r/fold mean-combiner
                  mean-reducer)
          (mean-post-combine))))

;; 3144836.86</pre></div><p class="calibre11">First, we calculate the <code class="literal">mean</code> value of the series using the fold we constructed just now. Then, we define a function of <code class="literal">x</code> and <code class="literal">sq-diff</code>, which calculates the squared difference of <code class="literal">x</code> from the <code class="literal">mean</code> value. We map it over the squared differences and call our <code class="literal">mean</code> fold a second time to arrive at the final variance result.</p><p class="calibre11">Thus, we make <a id="id590" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>two complete passes <a id="id591" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>over the data, firstly to calculate the mean, and secondly to calculate the difference of each <code class="literal">x</code> from the <code class="literal">mean</code> value. It might seem that calculating the variance is necessarily a sequential algorithm: it may not seem possible to reduce the number of steps further and calculate the variance in only a single fold over the data.</p><p class="calibre11">In fact, it is possible to express the variance calculation as a single fold. To do so, we need to keep track of three things: the count, the (current) mean, and the sum of squared differences:</p><div><pre class="programlisting">(defn variance-combiner
  ([] {:count 0 :mean 0 :sum-of-squares 0})
  ([a b]
   (let [count (+ (:count a) (:count b))]
     {:count count
      :mean (/ (+ (* (:count a) (:mean a))
                  (* (:count b) (:mean b)))
               count)
      :sum-of-squares (+ (:sum-of-squares a)
                         (:sum-of-squares b)
                         (/ (* (- (:mean b)
                                  (:mean a))
                               (- (:mean b)
                                  (:mean a))
                               (:count a)
                               (:count b))
                            count))})))</pre></div><p class="calibre11">Our combiner function is shown in the preceding code. The identity value is a map with all three values set to zero. The zero-arity combiner returns this value.</p><p class="calibre11">The two-arity combiner needs to combine the counts, means, and sums-of-squares for both of the <a id="id592" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>supplied values. Combining <a id="id593" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the counts is easy—we simply add them together. The means is only marginally trickier: we need to calculate the weighted mean of the two means. If one mean is based on fewer records, then it should count for less in the combined mean:</p><div><img src="img/7180OS_05_03.jpg" alt="Calculating the variance using fold" class="calibre41"/></div><p class="calibre11">Combining the sums of squares is the most complicated calculation. While adding the sums of squares, we also need to add a factor to account for the fact that the sum of squares from <code class="literal">a</code> and <code class="literal">b</code> were likely calculated from differing means:</p><div><pre class="programlisting">(defn variance-reducer [{:keys [count mean sum-of-squares]} x]
  (let [count' (inc count)
        mean'  (+ mean (/ (- x mean) count'))]
    {:count count'
     :mean mean'
     :sum-of-squares (+ sum-of-squares
                        (* (- x mean') (- x mean)))}))</pre></div><p class="calibre11">The reducer is much simpler and contains the explanation on how the variance fold is able to calculate the variance in one pass over the data. For each new record, the <code class="literal">:mean</code> value is recalculated from the previous <code class="literal">mean</code> and current <code class="literal">count</code>. We then add to the sum of squares the product of the difference between the means before and after taking account of this new record.</p><p class="calibre11">The final result is a map containing the <code class="literal">count</code>, <code class="literal">mean</code> and total <code class="literal">sum-of-squares</code>. Since the variance is just the <code class="literal">sum-of-squares</code> divided by the <code class="literal">count</code>, our <code class="literal">variance-post-combiner</code> function is a relatively simple one:</p><div><pre class="programlisting">(defn variance-post-combiner [{:keys [count mean sum-of-squares]}]
   (if (zero? count) 0 (/ sum-of-squares count)))</pre></div><p class="calibre11">Putting the three functions together yields the following:</p><div><pre class="programlisting">(defn ex-5-16 []
  (-&gt;&gt; (load-data "data/soi.csv")
       (r/map :N1)
       (r/fold variance-combiner
               variance-reducer)
       (variance-post-combiner)))

;; 3144836.86</pre></div><p class="calibre11">Since the standard <a id="id594" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>deviation is simply <a id="id595" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the square root of the variance, we only need a slightly modified <code class="literal">variance-post-combiner</code> function to calculate it as well.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch05lvl1sec97" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Mathematical folds with Tesser</h1></div></div></div><p class="calibre11">We should now <a id="id596" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>understand how to use folds to calculate parallel implementations of simple algorithms. Hopefully, we should also have some appreciation for the ingenuity required to find efficient solutions that will perform the minimum number of iterations over the data.</p><p class="calibre11">Fortunately, the Clojure library Tesser (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/aphyr/tesser">https://github.com/aphyr/tesser</a>) includes implementations for common mathematical folds, including the mean, standard deviation, and covariance. To see <a id="id597" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>how to use Tesser, let's consider the covariance of two fields from the IRS dataset: the salaries and wages, <code class="literal">A00200</code>, the unemployment compensation, <code class="literal">A02300</code>.</p><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec105" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating covariance with Tesser</h2></div></div></div><p class="calibre11">We encountered <a id="id598" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>covariance in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <em class="calibre13">Correlation</em>, as a measure of how two sequences of data vary together. The formula is reproduced as <a id="id599" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>follows:</p><div><img src="img/7180OS_05_04.jpg" alt="Calculating covariance with Tesser" class="calibre236"/></div><p class="calibre11">A covariance fold is included in <code class="literal">tesser.math</code>. In the following code, we'll include <code class="literal">tesser.math</code> as <code class="literal">m</code> and <code class="literal">tesser.core</code> as <code class="literal">t</code>:</p><div><pre class="programlisting">(defn ex-5-17 []
  (let [data (into [] (load-data "data/soi.csv"))]
    (-&gt;&gt; (m/covariance :A02300 :A00200)
         (t/tesser (t/chunk 512 data )))))

;; 3.496E7</pre></div><p class="calibre11">The <code class="literal">m/covariance</code> function expects to receive two arguments: a function to return the <code class="literal">x</code> value and another to return the <code class="literal">y</code> value. Since keywords act as functions to extract their corresponding values from a map, we simply pass the keywords as arguments.</p><p class="calibre11">Tesser works in a similar way to Clojure's reducers, but with some minor differences. Clojure's <code class="literal">fold</code> takes <a id="id600" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>care of splitting our data into subsequences for parallel execution. With Tesser however, we must divide our data into chunks explicitly. Since this is something we're going to do repeatedly, let's create a little helper <a id="id601" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function called <code class="literal">chunks</code>:</p><div><pre class="programlisting">(defn chunks [coll]
  (-&gt;&gt; (into [] coll)
       (t/chunk 1024)))</pre></div><p class="calibre11">For the most of the rest of this chapter, we'll be using the <code class="literal">chunks</code> function to split our input data into groups of <code class="literal">1024</code> records.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec106" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Commutativity</h2></div></div></div><p class="calibre11">Another difference <a id="id602" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>between Clojure's reducers and Tesser's folds is that Tesser doesn't guarantee that the input order will be preserved. Along with being associative, as we discussed previously, Tesser's functions must be commutative. A commutative function is the one whose result is the same if its arguments are provided in a different order:</p><div><img src="img/7180OS_05_05.jpg" alt="Commutativity" class="calibre237"/></div><p class="calibre11">Addition and multiplication are commutative, but subtraction and division are not. Commutativity is a useful property of functions intended for distributed data processing, because it lowers the amount of coordination required between subtasks. When Tesser executes a combine function, it's free to do so on whichever reducer functions return their values first. If the order doesn't matter, it doesn't need to wait for the first to complete.</p><p class="calibre11">Let's rewrite our <code class="literal">load-data</code> function into a <code class="literal">prepare-data</code> function that will return a commutative Tesser fold. It performs the same steps (parsing a line of the text file, formatting the record as a map and removing zero ZIP codes) that our previous reducers-based function did, but it no longer assumes that the column headers will be the first row in the file—<em class="calibre13">first</em> is a concept that explicitly requires ordered data:</p><div><pre class="programlisting">(def column-names
  [:STATEFIPS :STATE :zipcode :AGI_STUB :N1 :MARS1 :MARS2 ...])

(defn prepare-data []
  (-&gt;&gt; (t/remove #(.startsWith % "STATEFIPS"))
       (t/map parse-line)
       (t/map (partial format-record column-names))
       (t/remove  #(zero? (:zipcode %)))))</pre></div><p class="calibre11">Now that all the <a id="id603" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>preparation is being done in Tesser, we can pass the result of <code class="literal">iota/seq</code> directly as input. This will be particularly useful when we come to run our code distributed on Hadoop later in the chapter:</p><div><pre class="programlisting">(defn ex-5-18 []
  (let [data (iota/seq "data/soi.csv")]
    (-&gt;&gt; (prepare-data)
         (m/covariance :A02300 :A00200)
         (t/tesser (chunks data)))))

;; 3.496E7</pre></div><p class="calibre11">In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <em class="calibre13">Correlation</em>, we saw how in the case of simple linear regression with one feature and one response variable, the correlation coefficient is the covariance over the product of standard deviations:</p><div><img src="img/7180OS_05_06.jpg" alt="Commutativity" class="calibre238"/></div><p class="calibre11">Tesser includes functions to calculate the correlation of a pair of attributes as a fold too:</p><div><pre class="programlisting">(defn ex-5-19 []
  (let [data (iota/seq "data/soi.csv")]
    (-&gt;&gt; (prepare-data)
         (m/correlation :A02300 :A00200)
         (t/tesser (chunks data)))))

;; 0.353</pre></div><p class="calibre11">There's a modest, positive correlation between these two variables. Let's build a linear model that predicts the value of unemployment compensation, <code class="literal">A02300</code>, using salaries and wages, <code class="literal">A00200</code>.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec107" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Simple linear regression with Tesser</h2></div></div></div><p class="calibre11">Tesser doesn't <a id="id604" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>currently provide a linear regression fold, but it does give us the tools we need to implement one. We saw in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <em class="calibre13">Correlation</em>, how the coefficients for a simple linear regression model, the slope and the intercept, can be calculated as a simple function of the variance, covariance, and means of the two inputs:</p><div><img src="img/7180OS_05_07.jpg" alt="Simple linear regression with Tesser" class="calibre239"/></div><div><img src="img/7180OS_05_08.jpg" alt="Simple linear regression with Tesser" class="calibre70"/></div><p class="calibre11">The slope <em class="calibre13">b</em> is the covariance divided by the variance in <em class="calibre13">X</em>. The intercept is the value that ensures the regression line passes through the means of both the series. Ideally, therefore, we'd be able to calculate each of these four variables in a single fold over the data. Tesser provides two fold combinators, <code class="literal">t/fuse</code> and <code class="literal">t/facet</code>, to build more sophisticated folds out of more basic folds.</p><p class="calibre11">In cases where we <a id="id605" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have one input record and multiple calculations to be run in parallel, we should use <code class="literal">t/fuse</code>. For example, in the following example, we're fusing the mean and the standard deviation folds into a single fold that will calculate both values at once:</p><div><pre class="programlisting">(defn ex-5-20 []
  (let [data (iota/seq "data/soi.csv")]
    (-&gt;&gt; (prepare-data)
         (t/map :A00200)
         (t/fuse {:A00200-mean (m/mean)
                  :A00200-sd   (m/standard-deviation)})
         (t/tesser (chunks data)))))

;; {:A00200-sd 89965.99846545042, :A00200-mean 37290.58880658831}</pre></div><p class="calibre11">Here, we have the same calculation to run on all the fields in the map; therefore, we should use <code class="literal">t/facet</code>:</p><div><pre class="programlisting">(defn ex-5-21 []
  (let [data (iota/seq "data/soi.csv")]
    (-&gt;&gt; (prepare-data)
         (t/map #(select-keys % [:A00200 :A02300]))
         (t/facet)
         (m/mean)
         (t/tesser (chunks data)))))

;; {:A02300 419.67862159209596, :A00200 37290.58880658831}</pre></div><p class="calibre11">In the preceding code, we selected only two values from the record (<code class="literal">A00200</code> and <code class="literal">A02300</code>) and calculated the <code class="literal">mean</code> value for both of them simultaneously. Returning to the challenge of performing simple linear regression—we have four numbers to calculate, so let's <code class="literal">fuse</code> them together:</p><div><pre class="programlisting">(defn calculate-coefficients [{:keys [covariance variance-x
                                      mean-x mean-y]}]
  (let [slope     (/ covariance variance-x)
        intercept (- mean-y (* mean-x slope))]
    [intercept slope]))

(defn ex-5-22 []
  (let [data (iota/seq "data/soi.csv")
        fx :A00200
        fy :A02300]
    (-&gt;&gt; (prepare-data)
         (t/fuse {:covariance (m/covariance fx fy)
                  :variance-x (m/variance (t/map fx))
                  :mean-x (m/mean (t/map fx))
                  :mean-y (m/mean (t/map fx))})
         (t/post-combine calculate-coefficients)
         (t/tesser (chunks data)))))

;; [37129.529236553506 0.0043190406799462925]</pre></div><p class="calibre11">
<code class="literal">fuse</code> very succinctly binds together the calculations we want to perform. In addition, it allows us to specify a <code class="literal">post-combine</code> step to be included as part of the fuse. Rather than handing the result <a id="id606" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>off to another function to finalize the output, we can specify it directly as an integral part of the fold. The <code class="literal">post-combine</code> step receives the four results and calculates the slope and intercept from them, returning the two coefficients as a vector.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec108" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating a correlation matrix</h2></div></div></div><p class="calibre11">We've only <a id="id607" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>compared two features to see how they are correlated, but Tesser makes it very simple to look at the inter-correlation of a large number of target features. We supply the target features as a map of the feature name to some function of the input record that returns the desired feature. In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <em class="calibre13">Correlation</em>, for example, we would have taken the logarithm of the height. Here, we will simply extract each of the features as it is and provide human-readable names for each of them:</p><div><pre class="programlisting">(defn ex-5-23 []
  (let [data (iota/seq "data/soi.csv")
        attributes {:unemployment-compensation :A02300
                    :salary-amount             :A00200
                    :gross-income              :AGI_STUB
                    :joint-submissions         :MARS2
                    :dependents                :NUMDEP}]
    (-&gt;&gt; (prepare-data)
         (m/correlation-matrix attributes)
         (t/tesser (chunks data)))))</pre></div><p class="calibre11">Tesser will calculate the correlation between each pair of features and return the results in a map. The map is keyed by tuples (vectors of two elements) containing the names of each pair of features, and the associated value is the correlation between them.</p><p class="calibre11">If you run the preceding example now, you'll find that there are a high correlations between some of the variables. For example, the correlation between <code class="literal">:dependents</code> and <code class="literal">:unemployment-compensation</code> is <code class="literal">0.821</code>. Let's build a linear regression model that uses all of these <a id="id608" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>variables as inputs.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch05lvl1sec98" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Multiple regression with gradient descent</h1></div></div></div><p class="calibre11">When we ran <a id="id609" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>multiple linear regression in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <em class="calibre13">Correlation</em>, we used the normal equation and matrices to quickly arrive at the coefficients for a multiple linear regression model. The normal equation is repeated as follows:</p><div><img src="img/7180OS_05_09.jpg" alt="Multiple regression with gradient descent" class="calibre138"/></div><p class="calibre11">The normal equation uses matrix algebra to very quickly and efficiently arrive at the least squares estimates. Where all data fits in memory, this is a very convenient and concise equation. Where the data exceeds the memory available to a single machine however, the calculation becomes unwieldy. The reason for this is matrix inversion. The calculation of <img src="img/7180OS_05_10.jpg" alt="Multiple regression with gradient descent" class="calibre240"/> is not something that can be accomplished on a fold over the data—each cell in the output matrix depends on many others in the input matrix. These complex relationships require that the matrix be processed in a nonsequential way.</p><p class="calibre11">An alternative approach to <a id="id610" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>solve linear regression problems, and many other related machine learning problems, is a technique called <strong class="calibre12">gradient descent</strong>. Gradient descent reframes the problem as the solution to an iterative algorithm—one that does not calculate the answer in one very computationally intensive step, but rather converges towards the correct answer over a series of much smaller steps.</p><p class="calibre11">We encountered gradient descent in the previous chapter, when we used Incanter's <code class="literal">minimize</code> function to calculate the parameters that produced the lowest cost for our logistic regression classifier. As the volume of data increases, Incanter no longer remains a viable solution to <a id="id611" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>run gradient descent. In the next section, we'll see how we can run gradient descent for ourselves using Tesser.</p><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec109" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The gradient descent update rule</h2></div></div></div><p class="calibre11">Gradient <a id="id612" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>descent works by the iterative application of a function that moves the parameters in the direction of their optimum values. To apply this function, we need to know the gradient of the cost function with the current parameters.</p><p class="calibre11">Calculating the formula for the gradient involves calculus that's beyond the scope of this book. Fortunately, the resulting formula isn't terribly difficult to interpret:</p><div><img src="img/7180OS_05_11.jpg" alt="The gradient descent update rule" class="calibre241"/></div><p class="calibre11"><img src="img/7180OS_05_12.jpg" alt="The gradient descent update rule" class="calibre242"/> is the partial derivative, or the gradient, of our cost function <em class="calibre13">J(β)</em> for the parameter at index <em class="calibre13">j</em>. Therefore, we can see that the gradient of the cost function with respect to the parameter at index <em class="calibre13">j</em> is equal to the difference between our prediction and the true value of <em class="calibre13">y</em> multiplied by the value of <em class="calibre13">x</em> at index <em class="calibre13">j</em>.</p><p class="calibre11">Since we're seeking to descend the gradient, we want to subtract some proportion of the gradient from the current parameter values. Thus, at each step of gradient descent, we perform the following update:</p><div><img src="img/7180OS_05_13.jpg" alt="The gradient descent update rule" class="calibre243"/></div><p class="calibre11">Here, <code class="literal">:=</code> is the assigment <a id="id613" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>operator and <em class="calibre13">α</em> is a factor called the <strong class="calibre12">learning rate</strong>. The learning rate controls how large an adjustment we wish make to the parameters at each iteration as a fraction of the gradient. If our prediction <em class="calibre13">ŷ</em> nearly matches the actual value of <em class="calibre13">y</em>, then there would be little need to change the parameters. In contrast, a <a id="id614" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>larger error will result in a larger adjustment to the parameters. This rule is called the <strong class="calibre12">Widrow-Hoff learning rule</strong> or the <a id="id615" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><strong class="calibre12">Delta rule</strong>.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec110" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The gradient descent learning rate</h2></div></div></div><p class="calibre11">As we've seen, gradient descent is an iterative algorithm. The learning rate, usually represented by <em class="calibre13">α</em>, dictates the speed at which the gradient descent converges to the final answer. If the <a id="id616" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>learning rate is too small, convergence will happen very slowly. If it is too large, gradient descent will not find values close to the optimum and may even diverge from the correct answer:</p><div><img src="img/7180OS_05_130.jpg" alt="The gradient descent learning rate" class="calibre244"/></div><p class="calibre11">In the preceding chart, a small learning rate leads to a show convergence over many iterations of the algorithm. While the algorithm does reach the minimum, it does so over many more steps than is ideal and, therefore, may take considerable time. By contrast, in following diagram, we can see the effect of a learning rate that is too large. The parameter estimates are changed so significantly between iterations that they actually overshoot the optimum values and diverge from the minimum value:</p><div><img src="img/7180OS_05_140.jpg" alt="The gradient descent learning rate" class="calibre244"/></div><p class="calibre11">The gradient descent algorithm requires us to iterate repeatedly over our dataset. With the correct version of <a id="id617" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>alpha, each iteration should successively yield better approximations of the ideal parameters. We can choose to terminate the algorithm when either the change between iterations is very small or after a predetermined number of iterations.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec111" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Feature scaling</h2></div></div></div><p class="calibre11">As more features <a id="id618" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>are added to the linear model, it is important to scale features appropriately. Gradient descent will not perform very well if the features have radically different scales, since it won't be possible to pick a learning rate to suit them all.</p><p class="calibre11">A simple scaling we can perform is to subtract the mean value from each of the values and divide it by the standard-deviation. This will tend to produce values with zero mean that generally vary between <code class="literal">-3</code> and <code class="literal">3</code>:</p><div><pre class="programlisting">(defn feature-scales [features]
  (-&gt;&gt; (prepare-data)
       (t/map #(select-keys % features))
       (t/facet)
       (t/fuse {:mean (m/mean)
                :sd   (m/standard-deviation)})))</pre></div><p class="calibre11">The feature-factors function in the preceding code uses <code class="literal">t/facet</code> to calculate the <code class="literal">mean</code> value and standard deviation of all the input features:</p><div><pre class="programlisting">(defn ex-5-24 []
  (let [data (iota/seq "data/soi.csv")
        features [:A02300 :A00200 :AGI_STUB :NUMDEP :MARS2]]
    (-&gt;&gt; (feature-scales features)
         (t/tesser (chunks data)))))

;; {:MARS2 {:sd 533.4496892658647, :mean 317.0412009748016}...}</pre></div><p class="calibre11">If you run the preceding example, you'll see the different means and standard deviations returned by the <code class="literal">feature-scales</code> function. Since our feature scales and input records are represented as maps, we can perform the scale across all the features at once using Clojure's <code class="literal">merge-with</code> function:</p><div><pre class="programlisting">(defn scale-features [factors]
  (let [f (fn [x {:keys [mean sd]}]
            (/ (- x mean) sd))]
    (fn [x]
      (merge-with f x factors))))</pre></div><p class="calibre11">Likewise, we can perform the all-important reversal with <code class="literal">unscale-features</code>:</p><div><pre class="programlisting">(defn unscale-features [factors]
  (let [f (fn [x {:keys [mean sd]}]
            (+ (* x sd) mean))]
    (fn [x]
      (merge-with f x factors))))</pre></div><p class="calibre11">Let's scale our <a id="id619" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>features and take a look at the very first feature. Tesser won't allow us to execute a fold without a reduce, so we'll temporarily revert to using Clojure's reducers:</p><div><pre class="programlisting">(defn ex-5-25 []
  (let [data     (iota/seq "data/soi.csv")
        features [:A02300 :A00200 :AGI_STUB :NUMDEP :MARS2]
        factors (-&gt;&gt; (feature-scales features)
                     (t/tesser (chunks data)))]
    (-&gt;&gt; (load-data "data/soi.csv")
         (r/map #(select-keys % features ))
         (r/map (scale-features factors))
         (into [])
         (first))))

;; {:MARS2 -0.14837567114357617, :NUMDEP 0.30617757526890155,
;;  :AGI_STUB -0.714280814223704, :A00200 -0.5894942801950217,
;;  :A02300 0.031741856083514465}</pre></div><p class="calibre11">This simple step will help gradient descent perform optimally on our data.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec112" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Feature extraction</h2></div></div></div><p class="calibre11">Although we've <a id="id620" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>used maps to represent our input data in this chapter, it's going to be more convenient when running gradient descent to represent our features as a matrix. Let's write a function to transform our input data into a map of <code class="literal">xs</code> and <code class="literal">y</code>. The <code class="literal">y</code> axis will be a scalar response value and <code class="literal">xs</code> will be a matrix of scaled feature values.</p><p class="calibre11">As in the previous chapters, we're adding a bias term to the returned matrix of features:</p><div><pre class="programlisting">(defn feature-matrix [record features]
  (let [xs (map #(% record) features)]
    (i/matrix (cons 1 xs))))

(defn extract-features [fy features]
  (fn [record]
    {:y  (fy record)
     :xs (feature-matrix record features)}))</pre></div><p class="calibre11">Our <code class="literal">feature-matrix</code> function simply accepts an input of a record and the features to convert into a matrix. We call this from within <code class="literal">extract-features</code>, which returns a function that we can <a id="id621" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>call on each input record:</p><div><pre class="programlisting">(defn ex-5-26 []
  (let [data     (iota/seq "data/soi.csv")
        features [:A02300 :A00200 :AGI_STUB :NUMDEP :MARS2]
        factors (-&gt;&gt; (feature-scales features)
                     (t/tesser (chunks data)))]
    (-&gt;&gt; (load-data "data/soi.csv")
         (r/map (scale-features factors))
         (r/map (extract-features :A02300 features))
         (into [])
         (first))))

;; {:y 433.0, :xs  A 5x1 matrix
;;  -------------
;;  1.00e+00
;; -5.89e-01
;; -7.14e-01
;;  3.06e-01
;; -1.48e-01
;; }</pre></div><p class="calibre11">The preceding example shows the data converted into a format suitable to perform gradient descent: a map containing the <code class="literal">y</code> response variable and a matrix of values, including the bias term.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec113" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Creating a custom Tesser fold</h2></div></div></div><p class="calibre11">Each iteration of <a id="id622" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>gradient descent adjusts the coefficients by an amount determined by the cost function. The cost function is calculated by summing over the errors for each parameter in the dataset, so it will be useful to have a fold that sums the values of the matrices element-wise.</p><p class="calibre11">Whereas Clojure represents a fold with a reducer, a combiner, and an identity value obtained from the combiner, Tesser folds are expressed as six collaborative functions. The implementation of Tesser's <code class="literal">m/mean</code> fold is as follows:</p><div><pre class="programlisting">{:reducer-identity  (constantly [0 0])
 :reducer           (fn reducer [[s c] x]
                     [(+ s x) (inc c)])
 :post-reducer      identity
 :combiner-identity (constantly [0 0])
 :combiner          (fn combiner [x y] (map + x y))
 :post-combiner     (fn post-combiner [x]
                      (double (/ (first x)
                                 (max 1 (last x)))))}</pre></div><p class="calibre11">Tesser chooses to represent the <code class="literal">reducer</code> identity separately from the <code class="literal">combiner</code> function, and includes <a id="id623" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>three other functions as well; the <code class="literal">combiner-identity</code>, <code class="literal">post-reducer</code>, and <code class="literal">post-combiner</code> functions. Tesser's <code class="literal">mean</code> fold represents the pair of numbers (the count and the accumulated sum) as a vector of two numbers but, in other respects, it's similar to our own.</p><div><img src="img/7180OS_05_150.jpg" alt="Creating a custom Tesser fold" class="calibre245"/></div><p class="calibre11">We've already seen how to make use of a <code class="literal">post-combiner</code> function with our <code class="literal">mean-post-combiner</code> and <code class="literal">variance-post-combiner</code> functions earlier in the chapter.</p><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec03" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Creating a matrix-sum fold</h3></div></div></div><p class="calibre11">To create a custom <code class="literal">matrix-sum</code> fold, we'll need an identity value. We encountered the identity matrix in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <em class="calibre13">Correlation</em>, but this is the identity for matrix multiplication not addition. If the identity value for <code class="literal">+</code> is zero (because adding zero to a number doesn't change it), it <a id="id624" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>follows that the identity matrix for matrix addition is simply a matrix of zeros.</p><p class="calibre11">We have to make sure that the matrix is the same size as the matrices we want to add. So, let's parameterize our <code class="literal">matrix-sum</code> fold with the number rows and columns for the matrix. We can't know in advance how large this needs to be, because the identity function is called before anything else in the fold:</p><div><pre class="programlisting">(defn matrix-sum [nrows ncols]
  (let [zeros-matrix (i/matrix 0 nrows ncols)]
    {:reducer-identity (constantly zeros-matrix)
     :reducer i/plus
     :combiner-identity (constantly zeros-matrix)
     :combiner i/plus}))</pre></div><p class="calibre11">The preceding example is the completed <code class="literal">matrix-sum</code> fold definition. We don't provide the <code class="literal">post-combiner</code> and <code class="literal">post-reducer</code> functions; since, if omitted, these are assumed to be the identity function, which is what we want. We can use our new fold to calculate a sum of all the <a id="id625" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>features in our input like this:</p><div><pre class="programlisting">(defn ex-5-27 []
   (let [columns [:A02300 :A00200 :AGI_STUB :NUMDEP :MARS2]
         data    (iota/seq "data/soi.csv")]
     (-&gt;&gt; (prepare-data)
          (t/map (extract-features :A02300 columns))
          (t/map :xs)
          (t/fold (matrix-sum (inc (count columns)) 1))
          (t/tesser (chunks data)))))

;; A 6x1 matrix
;; -------------
;; 1.67e+05
;; 6.99e+07
;; 6.21e+09
;; ...
;; 5.83e+05
;; 9.69e+07
;; 5.28e+07</pre></div><p class="calibre11">Calculating the sum of a matrix gets us closer to being able to perform gradient descent. Let's use our new fold to calculate the total model error, given some initial coefficients.</p></div></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec114" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating the total model error</h2></div></div></div><p class="calibre11">Let's take a <a id="id626" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>look again at the delta rule for gradient descent:</p><div><img src="img/7180OS_05_14.jpg" alt="Calculating the total model error" class="calibre246"/></div><p class="calibre11">For each parameter <em class="calibre13">j</em>, we adjust the parameter by some proportion of the overall prediction error, <em class="calibre13">ŷ - y</em>, multiplied by the feature. Larger features, therefore, get a larger share of the cost than smaller features and are adjusted by a correspondingly larger amount. To implement this in the code, we need to calculate:</p><div><img src="img/7180OS_05_15.jpg" alt="Calculating the total model error" class="calibre247"/></div><p class="calibre11">This is the sum of the prediction error multiplied by the feature across all the input records. As we did earlier, our predicted value of <em class="calibre13">y</em> will be calculated using the following formula for each input record <em class="calibre13">x</em>:</p><div><img src="img/7180OS_05_16.jpg" alt="Calculating the total model error" class="calibre148"/></div><p class="calibre11">The coefficients <em class="calibre13">β</em> will be the same across all our input records, so let's create a <code class="literal">calculate-error</code> function. Given the transposed coefficients <em class="calibre13">β</em><sup class="calibre42">T</sup>, we return a function that will calculate <img src="img/7180OS_05_17.jpg" alt="Calculating the total model error" class="calibre248"/>. Since <em class="calibre13">x</em> is a matrix and <em class="calibre13">ŷ - y</em> is a scalar, the result will be a matrix:</p><div><pre class="programlisting">(defn calculate-error [coefs-t]
  (fn [{:keys [y xs]}]
    (let [y-hat (first (i/mmult coefs-t xs))
          error (- y-hat y)]
      (i/mult xs error))))</pre></div><p class="calibre11">To calculate the sum of the error for the entire dataset, we can simply chain our previously <a id="id627" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>defined <code class="literal">matrix-sum</code> function after the <code class="literal">calculate-error</code> step:</p><div><pre class="programlisting">(defn ex-5-28 []
  (let [columns [:A02300 :A00200 :AGI_STUB :NUMDEP :MARS2]
        fcount  (inc (count columns))
        coefs   (vec (replicate fcount 0))
        data    (iota/seq "data/soi.csv")]
    (-&gt;&gt; (prepare-data)
         (t/map (extract-features :A02300 columns))
         (t/map (calculate-error (i/trans coefs)))
         (t/fold (matrix-sum fcount 1))
         (t/tesser (chunks data)))))

;; A 6x1 matrix
;;  -------------
;; -6.99e+07
;; -2.30e+11
;; -8.43e+12
;;  ...
;; -1.59e+08
;; -2.37e+11
;; -8.10e+10</pre></div><p class="calibre11">Notice how the gradient is negative for all the features. This means that in order to descend the gradient and produce better estimates of the model coefficients, parameters must be increased.</p><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec04" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Creating a matrix-mean fold</h3></div></div></div><p class="calibre11">The update rule <a id="id628" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>defined in the previous code actually calls for the mean of the cost to be assigned to each of the features. This means that we need both <code class="literal">sum</code> and <code class="literal">count</code> to be calculated. We don't want to perform two separate passes over the data. So, as we did previously, we <code class="literal">fuse</code> the two folds into one:</p><div><pre class="programlisting">(defn ex-5-29 []
  (let [columns [:A02300 :A00200 :AGI_STUB :NUMDEP :MARS2]
        fcount  (inc (count columns))
        coefs   (vec (replicate fcount 0))
        data    (iota/seq "data/soi.csv")]
    (-&gt;&gt; (prepare-data)
         (t/map (extract-features :A02300 columns))
         (t/map (calculate-error (i/trans coefs)))
         (t/fuse {:sum   (t/fold (matrix-sum fcount 1))
                  :count (t/count)})
         (t/post-combine (fn [{:keys [sum count]}]
                           (i/div sum count)))
         (t/tesser (chunks data)))))</pre></div><p class="calibre11">The <code class="literal">fuse</code> function will return a map of <code class="literal">:sum</code> and <code class="literal">:count</code>, so we'll call <code class="literal">post-combine</code> on the result. The <code class="literal">post-combine</code> function specifies a function to be run at the end of our fold which simply divides the sum by the count.</p><p class="calibre11">Alternatively, we could create another custom fold to return the mean instead of the sum of a <a id="id629" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sequence of matrices. It has a lot in common with the <code class="literal">matrix-sum</code> fold defined previously but, like the <code class="literal">mean</code> fold we calculated earlier in the chapter, we will also keep track of the count of records processed:</p><div><pre class="programlisting">(defn matrix-mean [nrows ncols]
  (let [zeros-matrix (i/matrix 0 nrows ncols)]
    {:reducer-identity  (constantly [zeros-matrix 0])
     :reducer           (fn [[sum counter] x]
                          [(i/plus sum x) (inc counter)])
     :combiner-identity (constantly [zeros-matrix 0])
     :combiner          (fn [[sum-a count-a] [sum-b count-b]]
                          [(i/plus sum-a sum-b)
                           (+ count-a count-b)])
     :post-combiner     (fn [[sum count]]
                          (i/div sum count))}))</pre></div><p class="calibre11">The reducer identity is a vector containing <code class="literal">[zeros-matrix 0]</code>. Each reduction adds to the matrix total and increments the counter by one. Each combine step sums the two matrices—and both the counts—to yield a total sum and count over all the partitions. Finally, in the <code class="literal">post-combiner</code> step, the mean is calculated as the ratio of <code class="literal">sum</code> and <code class="literal">count</code>.</p><p class="calibre11">Although the code for the custom fold is more lengthy than our fused <code class="literal">sum</code> and <code class="literal">count</code> solution, we now have a general way of computing the means of matrices. It leads to more concise and readable examples and we can use it in our error-calculating code like this:</p><div><pre class="programlisting">(defn ex-5-30 []
  (let [features [:A02300 :A00200 :AGI_STUB :NUMDEP :MARS2]
        fcount   (inc (count features))
        coefs    (vec (replicate fcount 0))
        data     (iota/seq "data/soi.csv")]
    (-&gt;&gt; (prepare-data)
         (t/map (extract-features :A02300 features))
         (t/map (calculate-error (i/trans coefs)))
         (t/fold (matrix-mean fcount 1))
         (t/tesser (chunks data)))))

;;  A 5x1 matrix
;;  -------------
;;  4.20e+01
;;  3.89e+01
;;  -3.02e+01
;;  9.02e+01
;;  6.62e+01</pre></div><p class="calibre11">The small extra <a id="id630" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>effort of creating a custom fold has made the intention of the calling code a little easier to follow.</p></div></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec115" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Applying a single step of gradient descent</h2></div></div></div><p class="calibre11">The objective of <a id="id631" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculating the cost is to determine the amount by which to adjust each of the coefficients. Once we've calculated the average cost, as we did previously, we need to update the estimate of our coefficients <em class="calibre13">β</em>. Together, these steps represent a single iteration of gradient descent:</p><div><img src="img/7180OS_05_14.jpg" alt="Applying a single step of gradient descent" class="calibre246"/></div><p class="calibre11">We can return the updated coefficients in a <code class="literal">post-combiner</code> step that makes use of the average cost, the value of alpha, and the previous coefficients. Let's create a utility function <code class="literal">update-coefficients</code>, which will receive the coefficients and alpha and return a function that will calculate the new coefficients, given a total model cost:</p><div><pre class="programlisting">(defn update-coefficients [coefs alpha]
  (fn [cost]
    (-&gt;&gt; (i/mult cost alpha)
         (i/minus coefs))))</pre></div><p class="calibre11">With the preceding function in place, we have everything we need to package up a batch gradient descent update rule:</p><div><pre class="programlisting">(defn gradient-descent-fold [{:keys [fy features factors
                                     coefs alpha]}]
  (let [zeros-matrix (i/matrix 0 (count features) 1)]
    (-&gt;&gt; (prepare-data)
         (t/map (scale-features factors))
         (t/map (extract-features fy features))
         (t/map (calculate-error (i/trans coefs)))
         (t/fold (matrix-mean (inc (count features)) 1))
         (t/post-combine (update-coefficients coefs alpha)))))

(defn ex-5-31 []
  (let [features [:A00200 :AGI_STUB :NUMDEP :MARS2]
        fcount   (inc (count features))
        coefs    (vec (replicate fcount 0))
        data     (chunks (iota/seq "data/soi.csv"))
        factors  (-&gt;&gt; (feature-scales features)
                      (t/tesser data))
        options {:fy :A02300 :features features
                 :factors factors :coefs coefs :alpha 0.1}]
    (-&gt;&gt; (gradient-descent-fold options)
         (t/tesser data))))

;; A 6x1 matrix
;; -------------
;; -4.20e+02
;; -1.38e+06
;; -5.06e+07
;; -9.53e+02
;; -1.42e+06
;; -4.86e+05</pre></div><p class="calibre11">The resulting matrix <a id="id632" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>represents the values of the coefficients after the first iteration of gradient descent.</p></div><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec116" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Running iterative gradient descent</h2></div></div></div><p class="calibre11">Gradient descent is <a id="id633" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>an iterative algorithm, and we will usually need to run it many times to convergence. With a large dataset, this can be very time-consuming.</p><p class="calibre11">To save time, we've included a random sample of <code class="literal">soi.csv</code> in the data directory called <code class="literal">soi-sample.csv</code>. The smaller size allows us to run iterative gradient descent in a reasonable timescale. The following code runs gradient descent for 100 iterations, plotting the values of the parameters between each iteration on an <code class="literal">xy-plot</code>:</p><div><pre class="programlisting">(defn descend [options data]
  (fn [coefs]
    (-&gt;&gt; (gradient-descent-fold (assoc options :coefs coefs))
         (t/tesser data))))

(defn ex-5-32 []
  (let [features [:A00200 :AGI_STUB :NUMDEP :MARS2]
        fcount   (inc (count features))
        coefs    (vec (replicate fcount 0))
        data     (chunks (iota/seq "data/soi-sample.csv"))
        factors  (-&gt;&gt; (feature-scales features)
                      (t/tesser data))
        options  {:fy :A02300 :features features
                  :factors factors :coefs coefs :alpha 0.1}
        iterations 100
        xs (range iterations)
        ys (-&gt;&gt; (iterate (descend options data) coefs)
                (take iterations))]
    (-&gt; (c/xy-plot xs (map first ys)
                   :x-label "Iterations"
                   :y-label "Coefficient")
        (c/add-lines xs (map second ys))
        (c/add-lines xs (map #(nth % 2) ys))
        (c/add-lines xs (map #(nth % 3) ys))
        (c/add-lines xs (map #(nth % 4) ys))
        (i/view))))</pre></div><p class="calibre11">If you run the <a id="id634" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>example, you should see a chart similar to the following:</p><div><img src="img/7180OS_05_160.jpg" alt="Running iterative gradient descent" class="calibre45"/></div><p class="calibre11">In the preceding chart, you can see how the parameters converge to relatively stable the values over the course of 100 iterations.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch05lvl1sec99" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Scaling gradient descent with Hadoop</h1></div></div></div><p class="calibre11">The length of <a id="id635" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>time each iteration of batch gradient descent takes to run is determined by the size of your data and by how many processors your computer has. Although several chunks of data are processed in parallel, the dataset is large and the processors are finite. We've achieved a speed gain by performing calculations in parallel, but if we double the size of the dataset, the runtime will double as well.</p><p class="calibre11">Hadoop is one of several systems that has emerged in the last decade which aims to parallelize work that exceeds the capabilities of a single machine. Rather than running code across multiple processors, Hadoop takes care of running a calculation across many servers. In fact, Hadoop clusters can, and some do, consist of many thousands of servers.</p><p class="calibre11">Hadoop consists of two primary subsystems— the <strong class="calibre12">Hadoop Distributed File System</strong> (<strong class="calibre12">HDFS</strong>)—and the job processing system, <strong class="calibre12">MapReduce</strong>. HDFS stores files in chunks. A given file may be composed of many chunks and chunks are often replicated across many servers. In this way, Hadoop can store quantities of data much too large for any single server and, through replication, ensure that the data is stored reliably in the event of hardware failure too. As the name implies, the MapReduce programming model is built around the concept of map and reduce steps. Each job is composed of at least one map step and may optionally specify a reduce step. An entire job may consist of several map and reduce steps chained together.</p><div><img src="img/7180OS_05_170.jpg" alt="Scaling gradient descent with Hadoop" class="calibre249"/></div><p class="calibre11">In the respect that reduce steps are optional, Hadoop has a slightly more flexible approach to distributed calculation than Tesser. Later in this chapter and in the future chapters, we'll explore <a id="id636" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>more of the capabilities that Hadoop has to offer. Tesser does enable us to convert our folds into Hadoop jobs, so let's do this next.</p><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec117" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Gradient descent on Hadoop with Tesser and Parkour</h2></div></div></div><p class="calibre11">Tesser's <a id="id637" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Hadoop capabilities are available in the <code class="literal">tesser.hadoop</code> namespace, which we're including as <code class="literal">h</code>. The primary public API function in the Hadoop namespace is <code class="literal">h/fold</code>.</p><p class="calibre11">The <code class="literal">fold</code> function expects to receive at least four arguments, representing the configuration of the Hadoop job, the input file we want to process, a working directory for Hadoop to store its intermediate files, and the fold we want to run, referenced as a Clojure var. Any additional arguments supplied will be passed as arguments to the fold when it is executed.</p><p class="calibre11">The reason for using a var to represent our fold is that the function call initiating the fold may happen on a completely different computer than the one that actually executes it. In a distributed setting, the var and arguments must entirely specify the behavior of the function. We can't, in general, rely on other mutable local state (for example, the value of an atom, or the value of variables closing over the function) to provide any additional context.</p><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec05" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Parkour distributed sources and sinks</h3></div></div></div><p class="calibre11">The <a id="id638" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data which we want our Hadoop job to process may exist on multiple machines too, stored distributed in chunks on HDFS. Tesser makes use of a library called <strong class="calibre12">Parkour</strong> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/damballa/parkour/">https://github.com/damballa/parkour/</a>) to handle accessing potentially distributed <a id="id639" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data sources. We'll study Parkour in more detail later this and the next chapter but, for now, we'll just be using the <code class="literal">parkour.io.text</code> namespace to reference input and output text files.</p><p class="calibre11">Although Hadoop is designed to be run and distributed across many servers, it can also run in <em class="calibre13">local mode</em>. Local mode is suitable for testing and enables us to interact with the local filesystem as if it were HDFS. Another namespace we'll be using from Parkour is the <code class="literal">parkour.conf</code> namespace. This will allow us to create a default Hadoop configuration and operate it in local mode:</p><div><pre class="programlisting">(defn ex-5-33 []
  (-&gt;&gt; (text/dseq "data/soi.csv")
       (r/take 2)
       (into [])))</pre></div><p class="calibre11">In the preceding example, we use Parkour's <code class="literal">text/dseq</code> function to create a representation of the IRS input data. The return value implements Clojure's reducers protocol, so <a id="id640" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we can use <code class="literal">r/take</code> on the result.</p></div><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec06" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Running a feature scale fold with Hadoop</h3></div></div></div><p class="calibre11">Hadoop <a id="id641" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>needs a location to write its temporary files while working on a task, and will complain if we try to overwrite an existing directory. Since we'll be executing several jobs over the course of the next few examples, let's create a little utility function that returns a new file with a randomly-generated name.</p><div><pre class="programlisting">(defn rand-file [path]
  (io/file path (str (long (rand 0x100000000)))))

(defn ex-5-34 []
  (let [conf     (conf/ig)
        input    (text/dseq "data/soi.csv")
        workdir  (rand-file "tmp")
        features [:A00200 :AGI_STUB :NUMDEP :MARS2]]
    (h/fold conf input workdir #'feature-scales features)))</pre></div><p class="calibre11">Parkour provides a default Hadoop configuration object with the shorthand (<code class="literal">conf/ig</code>). This will return an empty configuration. The default value is enough, we don't need to supply any custom configuration.</p><div><div><h3 class="title8"><a id="note49" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">All of our Hadoop jobs will write their temporary files to a random directory inside the project's <code class="literal">tmp</code> directory. Remember to delete this folder later, if you're concerned about preserving disk space.</p></div></div><p class="calibre11">If you run the preceding example now, you should get an output similar to the following:</p><div><pre class="programlisting">;; {:MARS2 317.0412009748016, :NUMDEP 581.8504423822615,
;; :AGI_STUB 3.499939975269811, :A00200 37290.58880658831}</pre></div><p class="calibre11">Although the return value is identical to the values we got previously, we're now making use of Hadoop behind the scenes to process our data. In spite of this, notice that Tesser will return the response from our fold as a single Clojure data structure.</p></div><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec07" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Running gradient descent with Hadoop</h3></div></div></div><p class="calibre11">Since <code class="literal">tesser.hadoop</code> folds return Clojure data structures just like <code class="literal">tesser.core</code> folds, defining a <a id="id642" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>gradient descent function that makes use of our scaled features is very simple:</p><div><pre class="programlisting">(defn hadoop-gradient-descent [conf input-file workdir]
  (let [features [:A00200 :AGI_STUB :NUMDEP :MARS2]
        fcount  (inc (count features))
        coefs   (vec (replicate fcount 0))
        input   (text/dseq input-file)
        options {:column-names column-names
                 :features features
                 :coefs coefs
                 :fy :A02300
                 :alpha 1e-3}
        factors (h/fold conf input (rand-file workdir)
                        #'feature-scales
                        features)
        descend (fn [coefs]
                  (h/fold conf input (rand-file workdir)
                          #'gradient-descent-fold
                          (merge options {:coefs coefs
                                          :factors factors})))]
    (take 5 (iterate descend coefs))))</pre></div><p class="calibre11">The preceding code defines a <code class="literal">hadoop-gradient-descent</code> function that iterates a <code class="literal">descend</code> function <code class="literal">5</code> times. Each iteration of descend calculates the improved coefficients based on the <code class="literal">gradient-descent-fold</code> function. The final return value is a vector of coefficients after <code class="literal">5</code> iterations of a gradient descent.</p><p class="calibre11">We run the job on the full IRS data in the following example:</p><div><pre class="programlisting">(defn ex-5-35 []
  (let [workdir  "tmp"
        out-file (rand-file workdir)]
    (hadoop-gradient-descent (conf/ig) "data/soi.csv" workdir)))</pre></div><p class="calibre11">After several iterations, you should see an output similar to the following:</p><div><pre class="programlisting">;; ([0 0 0 0 0]
;; (20.9839310796048 46.87214911003046 -7.363493937722712
;;  101.46736841329326 55.67860863427868)
;; (40.918665605227744 56.55169901254631 -13.771345753228694
;;  162.1908841131747 81.23969785586247)
;; (59.85666340457121 50.559130068258995 -19.463888245285332
;;  202.32407094149158 92.77424653758085)
;; (77.8477613139478 38.67088624825574 -24.585818946408523
;;  231.42399118694212 97.75201693843269))</pre></div><p class="calibre11">We've seen <a id="id643" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>how we're able to calculate gradient descent using distributed techniques locally. Now, let's see how we can run this on a cluster of our own.</p></div><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec08" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Preparing our code for a Hadoop cluster</h3></div></div></div><p class="calibre11">Hadoop's Java <a id="id644" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>API defines <code class="literal">Tool</code> and the associated <code class="literal">ToolRunner</code> classes that are intended to help execute jobs on a Hadoop cluster. A <code class="literal">Tool</code> class is Hadoop's name for a generic command-line application that interacts with the Hadoop framework. By creating our own tool, we create a command-line application that can be submitted to a Hadoop cluster.</p><p class="calibre11">Since it's a Java framework, Hadoop expects to interact with class representations of our code. So, the namespace defining our tool needs to contain the <code class="literal">:gen-class</code> declaration, which instructs the Clojure compiler to create a class from our namespace:</p><div><pre class="programlisting">(ns cljds.ch5.hadoop
  (:gen-class)
  ...)</pre></div><p class="calibre11">By default, <code class="literal">:gen-class</code> will expect the namespace to define a main function called <code class="literal">-main</code>. This will be the function that Hadoop will call with our arguments, so we can simply delegate the call to a function that will actually execute our job:</p><div><pre class="programlisting">(defn -main [&amp; args]
  (tool/run hadoop-gradient-descent args))</pre></div><p class="calibre11">Parkour provides a Clojure interface to many of Hadoop's classes. In this case, <code class="literal">parkour.tool/run</code> contains all we need to run our distributed gradient descent function on Hadoop. With the preceding example in place, we need to instruct the Clojure compiler to ahead-of-time (AOT) compile our namespace and specify the class we'd like our project's main class to be. We can achieve it by adding the <code class="literal">:aot</code> and <code class="literal">:main</code> declarations to the <code class="literal">project.clj</code> function like this:</p><div><pre class="programlisting">{:main cljds.ch5.hadoop
 :aot [cljds.ch5.hadoop]}</pre></div><p class="calibre11">In the example code, we have specified these as a part of the <code class="literal">:uberjar</code> profile, since our last step, before <a id="id645" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sending the job to the cluster, would be to package it up as an uberjar file.</p></div><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec09" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Building an uberjar</h3></div></div></div><p class="calibre11">A JAR contains <a id="id646" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>executable java code. An uberjar contains executable java code, plus all the dependencies required to run it. An uberjar provides a convenient way to package up code to be run in a distributed environment, because the job can be sent from machine to machine while <a id="id647" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>carrying its dependencies with it. Although it makes for large job payloads, it avoids the need to ensure that job-specific dependencies are preinstalled on all the machines in the cluster. To create an uberjar file with <strong class="calibre12">Leiningen</strong>, execute the following command line within the project directory:</p><div><pre class="programlisting">
<strong class="calibre12">lein uberjar</strong>
</pre></div><p class="calibre11">Once you do this, two files will be created in the target directory. One file, <code class="literal">ch5-0.1.0.jar</code>, contains the project's compiled code. This is the same file as the one that would be generated with <code class="literal">lein jar</code>. In addition, uberjar generates the <code class="literal">ch5-0.1.0-standalone.jar</code> file. This contains the AOT-compiled project code in addition to the project's dependencies. The resulting file is large, but it contains everything the Hadoop job will need in order to run.</p></div><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec10" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Submitting the uberjar to Hadoop</h3></div></div></div><p class="calibre11">Once we've <a id="id648" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>created an uberjar file, we're ready to submit it to Hadoop. Having a working local <a id="id649" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Hadoop installation is not a prerequisite to follow along with the examples in this chapter, and we won't describe the steps required to install it here.</p><div><div><h3 class="title8"><a id="note50" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Links to Hadoop <a id="id650" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>installation guides are provided on this book's wiki at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://wiki.clojuredatascience.com">http://wiki.clojuredatascience.com</a>.</p></div></div><p class="calibre11">However, if you already have Hadoop installed and configured in local mode, you can run the example job on the command line now. Since the tool specified by the main class also accepts two arguments—the work directory and the input file—these will need to be provided too:</p><div><pre class="programlisting">
<strong class="calibre12">hadoop jar target/ch5-0.1.0-standalone.jar data/soi.csv tmp</strong>
</pre></div><p class="calibre11">If the command runs successfully, you may see logging messages as an output by the Hadoop process. After some time, you should see the final coefficients output by the job.</p><p class="calibre11">Although it takes more time to execute at the moment, our Hadoop <a id="id651" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>job has the advantage that it can be distributed on a cluster that can scale <a id="id652" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>indefinitely with the size of the data we have.</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch05lvl1sec100" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Stochastic gradient descent</h1></div></div></div><p class="calibre11">The method <a id="id653" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we've just seen of calculating gradient descent is often called <strong class="calibre12">batch gradient descent</strong>, because each update to the coefficients happens <a id="id654" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>inside an iteration over all the data in a <em class="calibre13">single batch</em>. With very large amounts of data, each iteration can be time-consuming and waiting for convergence could take a very long time.</p><p class="calibre11">An alternative method of gradient descent is called <strong class="calibre12">stochastic gradient descent</strong> or <strong class="calibre12">SGD</strong>. In this method, the <a id="id655" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>estimates of the coefficients are continually updated as the input data is processed. The update method for stochastic gradient descent looks like this:</p><div><img src="img/7180OS_05_14.jpg" alt="Stochastic gradient descent" class="calibre246"/></div><p class="calibre11">In fact, this is identical to batch gradient descent. The difference in application is purely that expression <img src="img/7180OS_05_17.jpg" alt="Stochastic gradient descent" class="calibre248"/> is calculated over a <em class="calibre13">mini-batch</em>—a random smaller subset of the overall data. The <a id="id656" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>mini-batch size should be large enough to represent a fair sample of the input records—for our data, a reasonable mini-batch size might be about 250.</p><p class="calibre11">Stochastic gradient descent arrives at the best estimates by splitting the entire dataset into mini-batches and processing each of them in turn. Since the output of each mini-batch is the coefficient we would like to use for the next mini-batch (in order to incrementally improve the estimates), the algorithm is inherently sequential.</p><p class="calibre11">The advantage stochastic gradient descent offers over batch gradient descent is that it can arrive at good estimates in just one iteration over the dataset. For very large datasets, it may not even be necessary to process all the mini-batches before good convergence has been achieved.</p><div><img src="img/7180OS_05_180.jpg" alt="Stochastic gradient descent" class="calibre250"/></div><p class="calibre11">We could implement SGD with Tesser by taking advantage of the fact that the combiner is applied serially, and treat each chunk as a mini-batch from which the coefficients could be calculated. This would mean that our reduce step was the identity function—we have no reduction to perform.</p><p class="calibre11">Instead, let's use this as an opportunity to learn more on how to construct a Hadoop job in Parkour. Before delving more into Parkour, let's see how stochastic gradient descent could be implemented <a id="id657" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>using what we already know:</p><div><pre class="programlisting">(defn stochastic-gradient-descent [options data]
  (let [batches (-&gt;&gt; (into [] data)
                     (shuffle)
                     (partition 250))
        descend (fn [coefs batch]
                  (-&gt;&gt; (gradient-descent-fold
                        (assoc options :coefs coefs))
                       (t/tesser (chunks batch))))]
    (reductions descend (:coefs options) batches)))</pre></div><p class="calibre11">The preceding code groups the input collection into smaller groups of 250 elements. Gradient descent is run on each of these mini-batches and the coefficients are updated. The next iteration of gradient descent will use the new coefficients on the next batch and, for an appropriate value of alpha, produce improved recommendations.</p><p class="calibre11">The following code will chart the output over many hundreds of batches:</p><div><pre class="programlisting">(defn ex-5-36 []
  (let [features [:A00200 :AGI_STUB :NUMDEP :MARS2]
        fcount   (inc (count features))
        coefs    (vec (replicate fcount 0))
        data     (chunks (iota/seq "data/soi.csv"))
        factors  (-&gt;&gt; (feature-scales features)
                      (t/tesser data))
        options  {:fy :A02300 :features features
                  :factors factors :coefs coefs :alpha 1e-3}
        ys       (stochastic-gradient-descent options data)
        xs       (range (count ys))]
    (-&gt; (c/xy-plot xs (map first ys)
                   :x-label "Iterations"
                   :y-label "Coefficient")
        (c/add-lines xs (map #(nth % 1) ys))
        (c/add-lines xs (map #(nth % 2) ys))
        (c/add-lines xs (map #(nth % 3) ys))
        (c/add-lines xs (map #(nth % 4) ys))
        (i/view))))</pre></div><p class="calibre11">We're supplying a learning rate over 100 times smaller than the value for batch gradient descent. This will help ensure that mini-batches containing outliers don't pull the parameters too far away from their optimal values. Because of the variance inherent in each of the mini-batches, the output of stochastic gradient descent will not converge exactly to the most <a id="id658" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>optimal parameters, but will instead oscillate around the minimum.</p><div><img src="img/7180OS_05_190.jpg" alt="Stochastic gradient descent" class="calibre45"/></div><p class="calibre11">The preceding image shows the more random effect of stochastic gradient descent; in particular, the effect of variance among the mini-batches on the parameter estimates. In spite of the much <a id="id659" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>lower learning rate, we can see spikes corresponding to the batches with the data containing outliers.</p><div><div><div><div><h2 class="title5"><a id="ch05lvl2sec118" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Stochastic gradient descent with Parkour</h2></div></div></div><p class="calibre11">For the rest of this <a id="id660" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chapter, we're going to build a Hadoop job directly with Parkour. Parkour exposes more of Hadoop's underlying capabilities than Tesser does, and this is a mixed blessing. While Tesser makes it very easy to write folds and apply them to large datasets in Hadoop, Parkour will require us to understand more about Hadoop's computation model.</p><p class="calibre11">Although Hadoop's approach to MapReduce embodies many of the principles we've encountered so far this chapter, it differs from Tesser's abstractions in several critical ways:</p><div><ul class="itemizedlist"><li class="listitem">Hadoop assumes that the data to be processed are key/value pairs</li><li class="listitem">Hadoop does not require a reduce stage following a map</li><li class="listitem">Tesser folds over the whole sequence of inputs, Hadoop reduces over groups</li><li class="listitem">Hadoop's groups of values are defined by a partitioner</li><li class="listitem">Tesser's combine phase happens <em class="calibre13">after</em> reduce, Hadoop's combine stage happens <em class="calibre13">before</em> reduce</li></ul></div><p class="calibre11">The last of these is particularly unfortunate. The terminology we've learned for Clojure reducers and Tesser is reversed for Hadoop: for Hadoop, the combiners aggregate the output from the mappers before the data is sent to the reducers.</p><p class="calibre11">We can see the broad flow represented in the following diagram with the output of the mappers combined into intermediate representations and sorted before being sent to the reducers. Each reducer reduces over a subset of the entire data. The combine step is optional and, in fact, we won't need one for our stochastic gradient descent jobs:</p><div><img src="img/7180OS_05_200.jpg" alt="Stochastic gradient descent with Parkour" class="calibre251"/></div><p class="calibre11">With or without a combining step, the data is sorted into groups before being sent to the reducers and the grouping strategy is defined by a partitioner. The default partitioning scheme is to partition by the key of your key/value pair (different keys are represented by different shades of gray in the preceding diagram). In fact, any custom partitioning scheme can be used.</p><p class="calibre11">As you can see, Parkour and Hadoop do not assume that the output is a single result. Since the groups that Hadoop reduces over are by default defined by the grouping key, the result of a reduce can <a id="id661" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>be a dataset of many records. In the preceding diagram, we illustrated the case for three different results, one for each of the keys in our data.</p><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec11" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Defining a mapper</h3></div></div></div><p class="calibre11">The first component <a id="id662" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the Hadoop job we'll define is the <strong class="calibre12">mapper</strong>. The mapper's role is usually to take a chunk of input records and transform them in some way. It's possible to specify a Hadoop job with no reducers; in this case, the output of the mappers is also the output of the whole job.</p><p class="calibre11">Parkour allows us to define the action of a mapper as a Clojure function. The only requirement of the function is that it accepts the input data (either from a source file or the output of a previous MapReduce step) as the final argument. Additional arguments can be provided if necessary, so long as the input is the final argument:</p><div><pre class="programlisting">(defn parse-m
  {::mr/source-as :vals
   ::mr/sink-as   :vals}
  [fy features factors lines]
  (-&gt;&gt; (skip-header lines)
       (r/map parse-line)
       (r/map (partial format-record column-names))
       (r/map (scale-features factors))
       (r/map (extract-features fy features))
       (into [])
       (shuffle)
       (partition 250)))</pre></div><p class="calibre11">The <code class="literal">map</code> function in the preceding example, <code class="literal">parse-m</code> (by convention, Parkour mappers have the suffix <code class="literal">-m</code>), is responsible for taking a single line of the input and converting it into a feature representation. We're reusing many of the functions we defined earlier in the chapter: <code class="literal">parse-line</code>, <code class="literal">format-record</code>, <code class="literal">scale-features</code>, and <code class="literal">extract-features</code>. Parkour <a id="id663" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will provide input to the mapper function as a reducible collection, so we will chain the functions together with <code class="literal">r/map</code>.</p><p class="calibre11">Stochastic gradient descent expects to process data in mini-batches, so our mapper is responsible for partitioning the data into groups of 250 rows. We <code class="literal">shuffle</code> before calling <code class="literal">partition</code> to ensure that the ordering of the data is random.</p></div><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec12" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Parkour shaping functions</h3></div></div></div><p class="calibre11">We're also supplying <a id="id664" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>metadata to the <code class="literal">parse-m</code> function in the form of the <code class="literal">{::mr/source-as :vals ::mr/sink-as :vals}</code> map. These are two namespaced keywords referencing <code class="literal">parkour.mapreduce/source-as</code> and <code class="literal">parkour.mapreduce/sink-as</code>, and are instructions to Parkour on how the data should be shaped before providing it to the function and what shape of data it can expect in return.</p><div><img src="img/7180OS_05_210.jpg" alt="Parkour shaping functions" class="calibre252"/></div><p class="calibre11">Valid options for a Parkour mapper are <code class="literal">:keyvals</code>, <code class="literal">:keys</code>, and <code class="literal">:vals</code>. The preceding diagram shows the effect <a id="id665" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>for a short sequence of three key/value pairs. By requesting to source our data as <code class="literal">:vals</code>, we get a sequence containing only the value portion of the key/value pair.</p></div><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec13" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Defining a reducer</h3></div></div></div><p class="calibre11">Defining a reducer in <a id="id666" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Parkour is the same as defining a mapper. Again, the last argument must be the input (now, the input from a prior map step), but additional arguments can be provided. Our Parkour reducer for stochastic gradient descent looks like this:</p><div><pre class="programlisting">(defn sum-r
  {::mr/source-as :vals
   ::mr/sink-as   :vals}
  [fcount alpha batches]
  (let [initial-coefs (vec (replicate fcount 0))
        descend-batch (fn [coefs batch]
                        (-&gt;&gt; (t/map (calculate-error
                                     (i/trans coefs)))
                             (t/fold (matrix-mean fcount 1))
                             (t/post-combine
                              (update-coefficients coefs alpha))
                             (t/tesser (chunks batch))))]
    (r/reduce descend-batch initial-coefs batches)))</pre></div><p class="calibre11">Our input is provided as a reducible collection like before, so we use the Clojure's reducers library to iterate over it. We're using <code class="literal">r/reduce</code> rather than <code class="literal">r/fold</code>, because we don't want to perform our reduction in parallel over the data. In fact, the reason for using Hadoop is that we can control the parallelism of each of the map and reduce phases independently. Now that we have our map and reduce steps defined, we can combine them into a single job by using the functions in the <code class="literal">parkour.graph</code> namespace.</p></div><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec14" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Specifying Hadoop jobs with Parkour graph</h3></div></div></div><p class="calibre11">The <a id="id667" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><code class="literal">graph</code> namespace is Parkour's main API to define Hadoop jobs. Each job must have at a minimum an input, a mapper, and an output, and we can chain these specifications with Clojure's <code class="literal">-&gt;</code> macro. Let's first define a very simple job, which takes the output from our mappers and writes them immediately to disk:</p><div><pre class="programlisting">(defn hadoop-extract-features [conf workdir input output]
  (let [fy       :A02300
        features [:A00200 :AGI_STUB :NUMDEP :MARS2]
        fcount   (inc (count features))
        input   (text/dseq input)
        factors (h/fold conf input (rand-file workdir)
                        #'feature-scales
                        features)
        conf (conf/ig)]
    (-&gt; (pg/input input)
        (pg/map #'parse-m fy features factors)
        (pg/output (text/dsink output))
        (pg/execute conf "extract-features-job"))))

(defn ex-5-37 []
  (let [workdir  "tmp"
        out-file (rand-file workdir)]
    (hadoop-extract-features (conf/ig) "tmp"
                             "data/soi.csv" out-file)
    (str out-file)))

;; "tmp/1935333306"</pre></div><p class="calibre11">The response from the preceding example should be a directory within the project's <code class="literal">tmp</code> directory, where Hadoop will have written its files. If you navigate to the directory, you should see several files. On my computer, I see four files—<code class="literal">_SUCCESS</code>, <code class="literal">part-m-00000</code>, <code class="literal">part-m-00001</code>, and <code class="literal">part-m-00002</code>. The presence of the <code class="literal">_SUCCESS</code> file indicates that our job is <a id="id668" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>completed successfully. The <code class="literal">part-m-xxxxx</code> files are chunks of our input file.</p><p class="calibre11">The fact that there are three files indicates that Hadoop created three mappers to process our input data. If we were running in distributed mode, these could have been created in parallel. If you open one of the files, you should see a long sequence of <code class="literal">clojure.lang.LazySeq@657d118e</code>. Since we wrote to a text file, it is a text representation of the output of our mapper data.</p></div><div><div><div><div><h3 class="title7"><a id="ch05lvl3sec15" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chaining mappers and reducers with Parkour graph</h3></div></div></div><p class="calibre11">What <a id="id669" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we really <a id="id670" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>want to do is to its chain our map and reduce steps to happen one after the other. For this, we will have to insert an intermediate step, the <strong class="calibre12">partitioner</strong>, and tell the partitioner how to serialize our <code class="literal">clojure.lang.LazySeqs</code>.</p><p class="calibre11">The latter can be accomplished by borrowing from Tesser, which implements the serialization and deserialization of arbitrary Clojure data structures using <strong class="calibre12">Fressian</strong>. In the next chapter, we'll look closer, at the support Parkour provides to create well-defined schemas for our partitioned data but, for now, it's simply enough for the partitioner to pass the encoded data through.</p><div><div><h3 class="title8"><a id="note51" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Fressian is an <a id="id671" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>extensible binary data format. You can learn more about it from the documentation at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clojure/data.fressian">https://github.com/clojure/data.fressian</a>.</p></div></div><p class="calibre11">Our keys will be encoded as <code class="literal">FressianWritable</code>, while our keys are not specified at all (we sink our map data just as <code class="literal">vals</code>). Hadoop's representation of nil is a <code class="literal">NullWritable</code> type. We import both in our namespace with:</p><div><pre class="programlisting">(:import [org.apache.hadoop.io NullWritable]
         [tesser.hadoop_support FressianWritable])</pre></div><p class="calibre11">With the <a id="id672" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>import in <a id="id673" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>place, we can specify our job in its entirety:</p><div><pre class="programlisting">(defn hadoop-sgd [conf workdir input-file output]
  (let [kv-classes [NullWritable FressianWritable]
        fy       :A02300
        features [:A00200 :AGI_STUB :NUMDEP :MARS2]
        fcount   (inc (count features))
        input   (text/dseq input-file)
        factors (h/fold conf input (rand-file workdir)
                        #'feature-scales
                        features)
        conf (conf/assoc! conf "mapred.reduce.tasks" 1)]
    (-&gt; (pg/input input)
        (pg/map #'parse-m fy features factors)
        (pg/partition kv-classes)
        (pg/reduce #'sum-r fcount 1e-8)
        (pg/output (text/dsink output))
        (pg/execute conf "sgd-job"))))</pre></div><p class="calibre11">We need to ensure that we have only one reducer processing our mini-batches (although there are variations of SGD that would permit us to average the results of several stochastic gradient descent runs, we want to arrive at a single set of near-optimal coefficients). We will use Parkour's <code class="literal">conf</code> namespace to <code class="literal">assoc! mapred.reduce.tasks</code> to <code class="literal">1</code>.</p><p class="calibre11">Between the map and reduce steps, we specify the partitioner and pass the <code class="literal">kv-classes</code> function defined at the top of the function. The final example simply runs this job:</p><div><pre class="programlisting">(defn ex-5-38 []
  (let [workdir  "tmp"
        out-file (rand-file workdir)]
    (hadoop-sgd (conf/ig) "tmp" "data/soi.csv" out-file)
    (str out-file)))

;; "tmp/4046267961"</pre></div><p class="calibre11">If you navigate to the directory returned by the job, you should now see a directory containing just two files—<code class="literal">_SUCCESS</code> and <code class="literal">part-r-00000</code>. One file is the output per reducer, so <a id="id674" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with one <a id="id675" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>reducer, we ended up with one <code class="literal">part-r-xxxxx</code> file. Inside this file will be the coefficients of the linear model calculated with stochastic gradient descent.</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title2"><a id="ch05lvl1sec101" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we learned some of the fundamental techniques of distributed data processing and saw how the functions used locally for data processing, map and reduce, are powerful ways of processing even very large quantities of data. We learned how Hadoop can scale unbounded by the capabilities of any single server by running functions on smaller subsets of the data whose outputs are themselves combined to finally produce a result. Once you understand the tradeoffs, this "divide and conquer" approach toward processing data is a simple and very general way of analyzing data on a large scale.</p><p class="calibre11">We saw both the power and limitations of simple folds to process data using both Clojure's reducers and Tesser. We've also begun exploring how Parkour exposes more of Hadoop's underlying capabilities.</p><p class="calibre11">In the next chapter, we'll see how to use Hadoop and Parkour to address a particular machine learning challenge—clustering a large volume of text documents.</p></div></div>



  </body></html>