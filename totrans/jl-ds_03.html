<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch03" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 3. Data Exploration</h1></div></div></div><p class="calibre11">When we first receive a dataset, most of the times we only know what it is related to—an overview that is not enough to start applying algorithms or create models on it. Data exploration is of paramount importance in data science. It is the necessary process prior to creating a model because it gives a highlight of the dataset and definitely makes clear the path to achieving our objectives. Data exploration familiarizes the data scientist with the data and helps to know what general hypothesis we can infer from the dataset. So, we can say it is a process of extracting some information from the dataset, not knowing beforehand what to look for.</p><p class="calibre11">In this chapter, we will study:</p><div><ul class="itemizedlist"><li class="listitem">Sampling, population, and weight vectors</li><li class="listitem">Inferring column types</li><li class="listitem">Summary of a dataset</li><li class="listitem">Scalar statistics</li><li class="listitem">Measures of variation</li><li class="listitem">Data exploration using visualizations</li></ul></div><p class="calibre11">Data exploration involves descriptive statistics. Descriptive statistics is a field of data analysis that finds out patterns by meaningfully summarizing data. This may not lead to the exact results or the model that we intend to build, but it definitely helps to understand the data. Suppose there are 10 million people in New Delhi and if we calculate the mean of the heights of 1,000 people taken at random living there, it wouldn't be the average height of the people of New Delhi, but it would definitely give an idea.</p><p class="calibre11">Julia can effectively be used for data exploration. Julia provides a package called <code class="literal">StatsBase.jl</code>, which contains the necessary functions for statistics. We would presume throughout the chapter that you have added the package:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Pkg.update() 
julia&gt; Pkg.add("StatsBase")  
</strong>
</pre><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec21" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sampling</h1></div></div></div><p class="calibre11">In the previous example, we spoke about calculating the mean height of 1,000 people out of the 10 million people living in New Delhi. While gathering the data of these 10 million people, let's say we started from a particular age or community, or in any sequential manner. Now, if we take 1,000 people who are consecutive in the dataset, there is a high probability that they would have similarities among them. This similarity would not give us the actual highlight of the dataset that we are trying to achieve. So, taking a small chunk of consecutive data points from the dataset wouldn't give us the insight that we want to gain. To overcome this, we use sampling.</p><p class="calibre11">Sampling is a technique to randomly select data from the given dataset such that they are not related to each other, and therefore we can generalize the results that we generate on this selected data over the complete dataset. Sampling is done over a population.</p><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec31" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Population</h2></div></div></div><p class="calibre11">A population in statistics refers to the set of all the data points that are there in the dataset and which have at least one common property. In the previous example, the people have the common property of being from the same geographic region.</p><p class="calibre11">Let's take the example of the iris dataset. Although it has just 150 records, it would give us an idea on how to take a sample from the dataset:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; using RDatasets 
 
julia&gt; iris_dataframe = dataset("datasets", "iris") 
</strong>
</pre><p class="calibre11">We will use the RDatasets package, which contains the iris dataset, and will load it into a DataFrame. So, this DataFrame contains the "population" and we want to take a sample from it:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; sample(iris_dataframe[:SepalLength]) 
6.6 
 
julia&gt; sample(iris_dataframe[:SepalLength], 5) 
5-element Array{Float64,1}: 
 4.8 
 5.4 
 5.0 
 5.2 
 4.3  
</strong>
</pre><p class="calibre11">The <code class="literal">sample()</code> function can be used to return a random value from the dataset or an array of randomly chosen values:</p><pre class="programlisting">
<strong class="calibre19">Julia&gt; sample(x, num_of_elements[; replace=true, ordered=false]) 
</strong>
</pre><p class="calibre11">The <code class="literal">replace</code> and <code class="literal">ordered</code> arguments are used in specific cases:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">replace</code>: This is used if the replacement is done when a same value is returned (<code class="literal">default=true</code>)</li><li class="listitem"><code class="literal">ordered</code>: This is used when the values returned are in ascending order (<code class="literal">default=false</code>)</li></ul></div><p class="calibre11">Ideally, the sample taken from the given dataset should represent the population. But mostly, it under—or over—represents many groups present in the dataset. Let's take the earlier example, what if we were unable to gather the complete data for ages between 50-70 and from community X? Therefore, our dataset doesn't represent the exact population. Something has to be done to correct the observed dataset.</p><p class="calibre11">Weighting adjustment is one of the very common correction techniques. In this technique, an adjustment weight is assigned to each record. The record or group that we think is under-represented gets a weight larger than 1 and the record or group that we think is over-represented gets a weight smaller than 1.</p></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec32" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Weight vectors</h2></div></div></div><p class="calibre11">Julia has a type, WeightVec, to represent weight vectors to facilitate the assignment of weights to samples. The need for a specialized data type for weight vectors was:</p><div><ul class="itemizedlist"><li class="listitem">To explicitly distinguish the role of this particular vector from other data vectors</li><li class="listitem">To save computation cycles by storing the sum of the weights and avoiding recomputing the sum of the weights again</li></ul></div><p class="calibre11">Weight vectors can be constructed like this:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; wv = WeightVec([1., 2., 3.], 6.) 
StatsBase.WeightVec{Float64,Array{Float64,1}}([1.0,2.0,3.0],6.0) 
</strong>
</pre><p class="calibre11">We have provided the sum of the weights as the second argument. It is optional and is done to save computation time.</p><p class="calibre11">For simplicity, <code class="literal">WeightVec</code> supports a few general methods. Let <code class="literal">wv</code> be of the type <code class="literal">WeightVec</code>:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; eltype(wv) 
Float64 
</strong>
</pre><p class="calibre11">
<code class="literal">eltype</code> is used to get the type of the values in <code class="literal">WeightVec</code>:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; length(wv) 
3 
 
julia&gt; isempty(wv) 
false 
 
julia&gt; values(wv) 
3-element Array{Float64,1}: 
 1.0 
 2.0 
 3.0 
 
julia&gt; sum(wv) 
6.0 
 
# Applying eltypes to iris_dataframe 
# this method is of DataFrames.jl 
julia&gt; eltypes(iris_dataframe)   
5-element Array{Type{T},1}: 
 Float64                       
 Float64                       
 Float64                       
 Float64                       
 Union{ASCIIString,UTF8String} 
</strong>
</pre><p class="calibre11">Other methods are self-explanatory. As the sum is already stored by <code class="literal">WeightVec</code>, it is returned instantaneously without any computation.</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec22" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Inferring column types</h1></div></div></div><p class="calibre11">To understand the dataset and move any further, we need to first understand what type of data we have. As our data is stored in columns, we should know their type before performing any operations. This is also called creating a data dictionary:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; typeof(iris_dataframe[1,:SepalLength]) 
Float64 
 
julia&gt; typeof(iris_dataframe[1,:Species]) 
ASCIIString 
</strong>
</pre><p class="calibre11">We have used the classic dataset of iris here. We already know the type of the data in these columns. We can apply the same function to any similar dataset. Suppose we were only given columns without labels; then it would have been hard to determine the type of data these columns contain. Sometimes, the dataset looks as if it contains numeric digits but their data type is <code class="literal">ASCIIString</code>. These can lead to errors in further steps. These errors are avoidable.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec23" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Basic statistical summaries</h1></div></div></div><p class="calibre11">Although, we are currently using RDatasets, about which we have sufficient details and documentation, these methods and techniques can be extended to other datasets.</p><p class="calibre11">Let's use a different dataset:</p><p class="calibre11">
</p><div><img src="img/B05321_03_8.jpg" alt="Basic statistical summaries" class="calibre73"/></div><p class="calibre11">
</p><p class="calibre11">We are using another dataset from the RDatasets package. These are exam scores from Inner London. To get some information about the dataset, we will use the <code class="literal">describe()</code> function, which we have already discussed in previous chapters:</p><p class="calibre11">
</p><div><img src="img/B05321_03_9.jpg" alt="Basic statistical summaries" class="calibre74"/></div><p class="calibre11">
</p><p class="calibre11">The columns are described as follows:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">Length</code> refers to the number of records (rows).</li><li class="listitem"><code class="literal">Type</code> refers to the data type of the column. Therefore, <code class="literal">School</code> is of the <code class="literal">Pooled ASCIIString</code> data type.</li><li class="listitem"><code class="literal">NA</code> and <code class="literal">NA%</code> refer to the number and percentage of the <code class="literal">NA</code> values present in the column. This is really helpful as you don't need to manually check for missing records now.</li><li class="listitem"><code class="literal">Unique</code> refers to the number of unique records present in the column.</li><li class="listitem"><code class="literal">Min</code> and <code class="literal">Max</code> are the minimum and maximum values present in the column (this does not apply to columns having <code class="literal">ASCIIStrings</code>). These are the values at the 0% and 100% of the data points. <code class="literal">Min</code> and <code class="literal">Max</code> define the range of data.</li><li class="listitem">1st quantile and 3rd quantile refer to the value at 25% and 75% of the data points respectively. Similarly, median refers to the value at the 50% of the data points.</li></ul></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec33" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Calculating the mean of the array or dataframe</h2></div></div></div><p class="calibre11">Julia provides different kinds of mean functions. Each has its own use case:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">geomean(arr)</code>: This computes the geometric mean of <code class="literal">arr</code>:</li></ul></div><p class="calibre11">
</p><div><img src="img/B05321_03_10.jpg" alt="Calculating the mean of the array or dataframe" class="calibre75"/></div><p class="calibre11">
</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">harmmean(arr)</code>: This computes the harmonic mean of <code class="literal">arr</code>:</li></ul></div><p class="calibre11">
</p><div><img src="img/B05321_03_11.jpg" alt="Calculating the mean of the array or dataframe" class="calibre76"/></div><p class="calibre11">
</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">trimmean(arr, fraction)</code>: This is used to compute the mean of the trimmed dataset. The second argument is used to provide the fraction over which the dataset will be trimmed. For example, if the value provided in <code class="literal">fraction</code> is 0.3, the mean will be computed by neglecting the top 30% and bottom 30% values. It's generally used to remove outliers:</li></ul></div><p class="calibre11">
</p><div><img src="img/B05321_03_12.jpg" alt="Calculating the mean of the array or dataframe" class="calibre77"/></div><p class="calibre11">
</p><p class="calibre11">The mean function is also extended. It can take a weighted vector as an argument to compute the weighted mean:</p><p class="calibre11">
</p><div><img src="img/B05321_03_13.jpg" alt="Calculating the mean of the array or dataframe" class="calibre78"/></div><p class="calibre11">
</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec24" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Scalar statistics</h1></div></div></div><p class="calibre11">Various functions are provided by Julia's package to compute various statistics. These functions are used to describe data in different ways as required.</p><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec34" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Standard deviations and variances</h2></div></div></div><p class="calibre11">The mean and median we earlier computed (in the <code class="literal">describe()</code> function) are measures of central tendency. Mean refers to the center computed after applying weights to all the values and median refers to the center of the list.</p><p class="calibre11">This is only one piece of information and we would like to know more about the dataset. It would be good to have knowledge about the spread of data points across the dataset. We cannot use just the min and max functions as we can have outliers in the dataset. Therefore, these min and max functions will lead to incorrect results.</p><p class="calibre11">Variance is a measurement of the spread between data points in a dataset. It is computed by calculating the distance of numbers from the mean. Variance measures how far each number in the set is from the mean.</p><p class="calibre11">The following is the formula for variance:</p><p class="calibre11">
</p><div><img src="img/B05321_03_14.jpg" alt="Standard deviations and variances" class="calibre79"/></div><p class="calibre11">
</p><p class="calibre11">
</p><div><img src="img/B05321_03_15.jpg" alt="Standard deviations and variances" class="calibre80"/></div><p class="calibre11">
</p><p class="calibre11">We can also have a variance along a specific dimension, which is useful for DataFrames:</p><p class="calibre11">
</p><div><img src="img/B05321_03_16.jpg" alt="Standard deviations and variances" class="calibre81"/></div><p class="calibre11">
</p><p class="calibre11">Here, the second argument is the dimension along which we want to compute the variance.</p><p class="calibre11">Standard deviation is the measurement of the spread or dispersion of the values in the dataset. It is square root of the variance. If it is close to 0, that means the dataset has very little dispersion from the mean. And greater values define high dispersion of the values from the mean. Standard deviation is different from variance as it has the same units as the mean:</p><p class="calibre11">
</p><div><img src="img/B05321_03_17.jpg" alt="Standard deviations and variances" class="calibre82"/></div><p class="calibre11">
</p><p class="calibre11">We can also calculate the standard deviation along a dimension such as variance.</p><p class="calibre11">Julia provides a function to calculate mean and variance, and also mean and standard deviation together:</p><p class="calibre11">
</p><div><img src="img/B05321_03_18.jpg" alt="Standard deviations and variances" class="calibre83"/></div><p class="calibre11">
</p><p class="calibre11">Statistical analysis involves characterization of data based on skewness and kurtosis. Skewness is the measure of the lack of symmetry from the center point of the dataset or the distribution. So, a distribution can be skewed to the left or it can be skewed to the right.</p><p class="calibre11">Kurtosis is the measure of the flatness of the distribution or the dataset as compared to a normal distribution. So, a distribution with a high peak at the center (mean) and a sharp decline to both the sides is said to have high kurtosis, and a distribution with a flatter peak at the mean is said to have low kurtosis:</p><p class="calibre11">
</p><div><img src="img/B05321_03_19.jpg" alt="Standard deviations and variances" class="calibre84"/></div><p class="calibre11">
</p><p class="calibre11">A moment in statistics is:</p><div><ul class="itemizedlist"><li class="listitem">0th moment is the  total probability</li><li class="listitem">1st moment is the mean</li><li class="listitem">2nd central moment is the variance</li><li class="listitem">3rd moment is the skewness</li><li class="listitem">4th moment is the kurtosis (with shift and normalization)</li></ul></div><p class="calibre11">
</p><div><img src="img/B05321_03_20.jpg" alt="Standard deviations and variances" class="calibre85"/></div><p class="calibre11">
</p><p class="calibre11">Here we are calculating the k-th order central moment. It is defined as:</p><pre class="programlisting">(a - mean(a)).^k 
</pre></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec25" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Measures of variation</h1></div></div></div><p class="calibre11">It is good to have knowledge of the variation of values in the dataset. Various statistical functions facilitate:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">span(arr)</code>: span is used to calculate the total spread of the dataset, which is <code class="literal">maximum(arr)</code> to <code class="literal">minimum(arr)</code>:</li></ul></div><p class="calibre11">
</p><div><img src="img/B05321_03_21.jpg" alt="Measures of variation" class="calibre86"/></div><p class="calibre11">
</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">variation(arr)</code>: Also called the coefficient of variance. It is the ratio of the standard deviation to the mean of the dataset. In relation to the mean of the population, CV denotes the extent of variability. Its advantage is that it is a dimensionless number and can be used to compare different datasets.</li></ul></div><p class="calibre11">
</p><div><img src="img/B05321_03_22.jpg" alt="Measures of variation" class="calibre87"/></div><p class="calibre11">
</p><p class="calibre11">Standard error of mean: We work on different samples drawn from the population. We compute the means of these samples and call them sample means. For different samples, we wouldn't be having the same sample mean but a distribution of sample means. The standard deviation of the distribution of these sample means is called standard error of mean.</p><p class="calibre11">In Julia, we can compute standard error of mean using <code class="literal">sem(arr)</code>.</p><p class="calibre11">Mean absolute deviation is a robust measure of central tendency. Robustness refers to not being affected by outliers.</p><p class="calibre11">
</p><div><img src="img/B05321_03_23.jpg" alt="Measures of variation" class="calibre88"/></div><p class="calibre11">
</p><p class="calibre11">We can provide the center as a second argument.</p><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec35" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Z-scores</h2></div></div></div><p class="calibre11">A z-score refers to the relationship with the mean of the scores. It is calculated by how many standard deviations an element is above or below the mean. A 0 z-score means it is the same as the mean.</p><p class="calibre11">It is given by the formula <em class="calibre23">z = (X - μ) / σ</em>:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; a = [12,23,45,68,99,72,61,39,21,71] 
</strong>
</pre><p class="calibre11">On this dataset, we can calculate the z-score like this:</p><p class="calibre11">
</p><div><img src="img/B05321_03_24.jpg" alt="Z-scores" class="calibre89"/></div><p class="calibre11">
</p><p class="calibre11">The mean and the standard deviation are calculated by themselves.</p></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec36" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Entropy</h2></div></div></div><p class="calibre11">Entropy is the measure of disorder in the dataset and provides the approximate measure of randomness in the system. It increases with randomness.</p><p class="calibre11">Let's create a probability vector:</p><p class="calibre11">
</p><div><img src="img/B05321_03_25.jpg" alt="Entropy" class="calibre90"/></div><p class="calibre11">
</p><p class="calibre11">We have created a rather small array:</p><p class="calibre11">
</p><div><img src="img/B05321_03_26.jpg" alt="Entropy" class="calibre91"/></div><p class="calibre11">
</p><p class="calibre11">The sum of the elements of the probability vector is 1. It is tending to 1 here. We now calculate the entropy:</p><p class="calibre11">
</p><div><img src="img/B05321_03_27.jpg" alt="Entropy" class="calibre92"/></div><p class="calibre11">
</p><p class="calibre11">The entropy computation is done using natural logarithms. We can also provide the base of the logarithm if needed.</p><p class="calibre11">
</p><div><img src="img/B05321_03_28.jpg" alt="Entropy" class="calibre93"/></div><p class="calibre11">
</p><p class="calibre11">The second argument that we have provided is for the base of the logarithm. We can also compute cross-entropy, which is considered an effective alternative to a squared error:</p><pre class="programlisting">
<strong class="calibre19">Julia&gt; crossentropy(ProbabilityVector1, ProbabilityVector2) 
</strong>
</pre></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec37" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Quantiles</h2></div></div></div><p class="calibre11">To understand the dataset better, we want to know the lowest and highest points in the dataset. We can use min and max functions for that. So, we can also say that the min and max data points are at 0% and at 100%. If we want to find out any data point at n% of the dataset we use the <code class="literal">quantile</code> function.</p><p class="calibre11">Quantiles can be very useful in scenarios where there are outliers. For example, for <code class="literal">a</code> we are analyzing the response times of various browsers for a website: 98% of the traffic comes from the desktop and it is able to load the page in less than a second; 2% of the remaining traffic is from mobile, where it takes 5 seconds to load the page. Here, we might want to ignore this 2% (if the use case allows us) to analyze the actual traffic of the website.</p><p class="calibre11">
</p><div><img src="img/B05321_03_29.jpg" alt="Quantiles" class="calibre94"/></div><p class="calibre11">
</p><p class="calibre11">Now, to compute the quantile:</p><p class="calibre11">
</p><div><img src="img/B05321_03_30.jpg" alt="Quantiles" class="calibre95"/></div><p class="calibre11">
</p><p class="calibre11">Here, we have received five values. These five values represent data points at 0%, 25%, 50%, 75%, and 100% of the dataset.</p><p class="calibre11">Interquartile range is the measure of the variability and is calculated by having the difference of the upper and lower quartiles, which is Q3-Q1. It is computed as:</p><p class="calibre11">
</p><div><img src="img/B05321_03_31.jpg" alt="Quantiles" class="calibre96"/></div><p class="calibre11">
</p><p class="calibre11">Percentile is a common term in statistics and is used to represent where the data point falls in the dataset. It can be calculated as:</p><p class="calibre11">
</p><div><img src="img/B05321_03_32.jpg" alt="Quantiles" class="calibre97"/></div><p class="calibre11">
</p><p class="calibre11">We have used the same dataset and calculated where 0.5 would lie in the dataset.</p><p class="calibre11">There is another important function, <code class="literal">nquantile</code>. It is used to create a vector of quantiles defined by us:</p><p class="calibre11">
</p><div><img src="img/B05321_03_33.jpg" alt="Quantiles" class="calibre98"/></div><p class="calibre11">
</p></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec38" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Modes</h2></div></div></div><p class="calibre11">While exploring the dataset, we would want to know which data is frequently repeated in the dataset. This is the value that has the maximum probability to come in the sample. Julia provides a function to calculate the mode:</p><p class="calibre11">
</p><div><img src="img/B05321_03_34.jpg" alt="Modes" class="calibre99"/></div><p class="calibre11">
</p><p class="calibre11">We calculated the mode on the same dataset that we used in the previous example. So, <code class="literal">0.2566</code> appears most often in the dataset.</p></div><div><div><div><div><h2 class="title3"><a id="ch03lvl2sec39" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary of datasets</h2></div></div></div><p class="calibre11">Earlier we discussed the <code class="literal">describe()</code> function, which prints the summary of the dataset. Julia also provides another function, <code class="literal">summarystats()</code>.</p><p class="calibre11">Using <code class="literal">summarystats(a)</code> on the same dataset of the previous example, we get the following result. So, we now don't have to calculate them individually and it gives an idea of what kind of dataset we have.</p><p class="calibre11">
</p><div><img src="img/B05321_03_35.jpg" alt="Summary of datasets" class="calibre100"/></div><p class="calibre11">
</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec26" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Scatter matrix and covariance</h1></div></div></div><p class="calibre11">Covariance is used very often by data scientists to find out how two ordered sets of data follow in the same direction. It can very easily define whether the variables are correlated or not. To best represent this behavior, we create a covariance matrix. The unnormalized version of the covariance matrix is the scatter matrix.</p><p class="calibre11">To create a scatter matrix, we use the <code class="literal">scattermat(arr)</code> function.</p><p class="calibre11">The default behavior is to treat each row as an observation and column as a variable. This can be changed by providing the keyword arguments <code class="literal">vardim</code> and <code class="literal">mean</code>:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">Vardim</code>: <code class="literal">vardim=1 (default)</code> means each column is a variable and each row is an observation. vardim=2 is the reverse.</li><li class="listitem"><code class="literal">mean</code>: The mean is computed by <code class="literal">scattermat</code>. We can use a predefined mean to save compute cycles.</li></ul></div><p class="calibre11">We can also create a weighted covariance matrix using the <code class="literal">cov</code> function. It also takes vardim and mean as optional arguments for the same purpose.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec27" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Computing deviations</h1></div></div></div><p class="calibre11">StatsBase.jl provides various functions to compute deviations between two datasets. This can be calculated using other functions, but to facilitate and for ease of use, StatsBase provides these efficiently implemented functions:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Mean absolute deviation</strong>: For two datasets, <code class="literal">a</code> and <code class="literal">b</code>, it is calculated as <code class="literal">meanad(x,y)</code> which is a wrapper over <code class="literal">mean(abs(x-y))</code>.</li><li class="listitem"><strong class="calibre19">Maximum absolute deviation</strong>: For two datasets, <code class="literal">a</code> and <code class="literal">b</code>, it is calculated as <code class="literal">maxad(x,y)</code>, which is a wrapper over <code class="literal">maximum(abs(x-y))</code>.</li><li class="listitem"><strong class="calibre19">Mean squared deviation</strong>: For two datasets, <code class="literal">a</code> and <code class="literal">b</code>, it is calculated as <code class="literal">msd(x,y)</code>, which is a wrapper over <code class="literal">mean(abs2(x-y))</code>.</li><li class="listitem"><strong class="calibre19">Root mean squared deviation</strong>: For two datasets, <code class="literal">a</code> and <code class="literal">b</code>, it is calculated as <code class="literal">rmsd(a,b)</code>, which is a wrapper over <code class="literal">sqrt(msd(a, b))</code>.</li></ul></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec28" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Rankings</h1></div></div></div><p class="calibre11">When a dataset is sorted in ascending order, a rank is assigned to each value. Ranking is a process where the dataset is transformed and values are replaced by their ranks. Julia provides functions for various types of rankings.</p><p class="calibre11">In ordinal ranking, all items in the dataset are assigned a distinct value. Items that have equal values are assigned a ranking arbitrarily. In Julia, this is done using the <code class="literal">ordinalrank</code> function.</p><p class="calibre11">
</p><div><img src="img/B05321_03_36.jpg" alt="Rankings" class="calibre101"/></div><p class="calibre11">
</p><p class="calibre11">Suppose this is our dataset and we want to do ordinal ranking:</p><p class="calibre11">
</p><div><img src="img/B05321_03_37.jpg" alt="Rankings" class="calibre102"/></div><p class="calibre11">
</p><p class="calibre11">Using the <code class="literal">ordinalrank(arr)</code> function, we've got the ordinal ranking. Similarly, StatsBase also provides functions to find other types of rankings, such as <code class="literal">competerank()</code>, <code class="literal">denserank()</code>, and <code class="literal">tiedrank()</code>.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec29" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Counting functions</h1></div></div></div><p class="calibre11">In data exploration, counting over a range is often done. It helps to find out the most/least occurring value. Julia provides the counts function to count over a range. Let's say we have an array of values. For our convenience, we will now use the random function to create an array:</p><p class="calibre11">
</p><div><img src="img/B05321_03_38.jpg" alt="Counting functions" class="calibre103"/></div><p class="calibre11">
</p><p class="calibre11">We have created an array of 30 values ranging from 1 to 5. Now we want to know how many times they occur in the dataset:</p><p class="calibre11">
</p><div><img src="img/B05321_03_39.jpg" alt="Counting functions" class="calibre104"/></div><p class="calibre11">
</p><p class="calibre11">Using the <code class="literal">count</code> function, we found that 1(<code class="literal">7</code>), 2(<code class="literal">1</code>), 3(<code class="literal">5</code>), 4(<code class="literal">11</code>), and 5(<code class="literal">6</code>). counts take different arguments to suit the use case.</p><p class="calibre11">The <code class="literal">proportions()</code> function is used to compute the proportions of the values in the dataset and  Julia provides the function:</p><p class="calibre11">
</p><div><img src="img/B05321_03_40.jpg" alt="Counting functions" class="calibre105"/></div><p class="calibre11">
</p><p class="calibre11">We calculated proportions on the same dataset that we used in the previous examples. It shows that the ratio of value 1 in the dataset is <code class="literal">0.23333</code>. This can also be seen as the probability of finding the value in the dataset.</p><p class="calibre11">Other count functions include:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">countmap(arr)</code>: This is a map function that maps the values to the number of occurrences (or total weights) in the dataset:</li></ul></div><p class="calibre11">
</p><div><img src="img/B05321_03_41.jpg" alt="Counting functions" class="calibre106"/></div><p class="calibre11">
</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">proportionmap(arr)</code>: This is a map function similar to <code class="literal">countmap(arr)</code> but maps values to their proportions:</li></ul></div><p class="calibre11">
</p><div><img src="img/B05321_03_42.jpg" alt="Counting functions" class="calibre107"/></div><p class="calibre11">
</p><p class="calibre11">Applying <code class="literal">countmap</code> and <code class="literal">proportionmap</code> to our dataset gave these values. Both these functions return a dictionary.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec30" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Histograms</h1></div></div></div><p class="calibre11">Data exploration after a basic understanding can also be done with the aid of visualizations. Plotting a histogram is one of the most common ways of data exploration through visualization. A histogram type is used to tabulate data over a real plane separated into regular intervals.</p><p class="calibre11">A histogram is created using the fit method:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; fit(Histogram, data[, weight][, edges])  
</strong>
</pre><p class="calibre11">
<code class="literal">fit</code> takes the following arguments:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">data</code>: Data is passed to the <code class="literal">fit</code> function in the form of a vector, which can either be one-dimensional or n-dimensional (tuple of vectors of equal length).</li><li class="listitem"><code class="literal">weight</code>: This is the optional argument. A <code class="literal">WeightVec</code> type can be passed as an argument if values have different weights. The default weight of values is 1.</li><li class="listitem"><code class="literal">edges</code>: This is a vector used to give the edges of the bins along each dimension.</li></ul></div><p class="calibre11">It also takes a keyword argument, <code class="literal">nbins</code>, which is used to define the number of bins that the histogram should use along each side:</p><p class="calibre11">
</p><div><img src="img/image_03_036.jpg" alt="Histograms" class="calibre108"/></div><p class="calibre11">
</p><p class="calibre11">In this example, we used two random value generators and <code class="literal">nbins</code> to define the number of bins. We created a histogram on the randomly generated data. Let's try this on a dataset from the <code class="literal">RDatasets</code> package. This package is explained here: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/sleep.html">https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/sleep.html</a>.</p><p class="calibre11">
</p><div><img src="img/image_03_037.jpg" alt="Histograms" class="calibre109"/></div><p class="calibre11">
</p><p class="calibre11">We are using a dataset called <code class="literal">sleepstudy</code> from the <code class="literal">RDatasets</code> package. It contains three columns: <code class="literal">Reaction (Float64)</code>, <code class="literal">Days (Integer)</code>, and <code class="literal">Subject (Integer)</code>. We will create a histogram on this data.</p><p class="calibre11">
</p><div><img src="img/image_03_038.jpg" alt="Histograms" class="calibre110"/></div><p class="calibre11">
</p><p class="calibre11">We can now realize that it is easier to understand the data through visualizations. Visualization is an important part of data exploration. To be actually able to visualize data, the necessary data munging and some understanding of variables are required. In this particular visualization, we can observe which areas are denser and the reaction times.</p><p class="calibre11">We discussed the scatter matrix earlier. We can create a scatter plot and will try to find out if it helps us.</p><p class="calibre11">
</p><div><img src="img/image_03_039.jpg" alt="Histograms" class="calibre111"/></div><p class="calibre11">
</p><p class="calibre11">We can very well observe that the reaction times of the subjects are increasing day by day. We were able to get to this conclusion very quickly; otherwise, it would have taken some significant amount of time.</p><p class="calibre11">Let's drill down more into this dataset. Suppose we want to know how each individual subject has performed. As all the subjects are not the same, some might have performed quite differently from others.</p><p class="calibre11">On a large dataset, we can do a grouping or clustering; but here, as have a small dataset, we can individually analyze subjects.</p><p class="calibre11">
</p><div><img src="img/image_03_040.jpg" alt="Histograms" class="calibre112"/></div><p class="calibre11">
</p><p class="calibre11">It is evident that subject <code class="literal">309</code>, even after being deprived of sleep for many days, had a very low reaction time. These are small insights that we sometimes miss through analyzing a dataset that is exposed through visualizations.</p><p class="calibre11">We will discuss visualizations in detail in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch05.xhtml" title="Chapter 5. Making Sense of Data Using Visualization">Chapter 5</a>, <em class="calibre23">Making Sense of Data Using Visualization</em>. We will explore various packages available with Julia for visualization and also how can we call R and Python's packages if needed for visualizations. We will also go through some basic D3.js examples.</p><p class="calibre11">It is easy to create basic plots in Julia, for example:</p><p class="calibre11">
</p><div><img src="img/image_03_041.jpg" alt="Histograms" class="calibre113"/></div><p class="calibre11">
</p><p class="calibre11">Let's now try some visualizations on the iris dataset:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; x=:SepalLength, y=:SepalWidth, color=:Species) 
</strong>
</pre><p class="calibre11">Although it is not completely visible now, we can see there are visible clusters. Maybe, we can differentiate between various species using these clusters. Therefore, visualizations can be very helpful in finding these kinds of insights.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec31" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Correlation analysis</h1></div></div></div><p class="calibre11">Julia provides some functions to facilitate correlation analysis. Correlation and dependence are two common terms in statistics. Dependence refers to one variable having a statistical relationship with another variable, whereas correlation is one variable having a much wider class of relationship with the other variable, which may also include dependence.</p><p class="calibre11">The <code class="literal">autocov(x)</code> function is used to compute auto-covariance of <code class="literal">x</code>. It returns a vector of the same size as <code class="literal">x</code>.</p><p class="calibre11">
</p><div><img src="img/B05321_03_50.jpg" alt="Correlation analysis" class="calibre114"/></div><p class="calibre11">
</p><p class="calibre11">This is a dataset we generated. We can apply <code class="literal">autocov</code> on this dataset:</p><p class="calibre11">
</p><div><img src="img/B05321_03_51.jpg" alt="Correlation analysis" class="calibre115"/></div><p class="calibre11">
</p><p class="calibre11">To compute auto-correlation, we use the <code class="literal">autocor</code> function:</p><p class="calibre11">
</p><div><img src="img/B05321_03_52.jpg" alt="Correlation analysis" class="calibre116"/></div><p class="calibre11">
</p><p class="calibre11">Similarly, we can also compute cross-covariance and cross-correlation. For that, we will generate another random array of the same size:</p><p class="calibre11">
</p><div><img src="img/B05321_03_53.jpg" alt="Correlation analysis" class="calibre117"/></div><p class="calibre11">
</p><p class="calibre11">Cross-covariance and cross-correlation of 2 arrays of length=6 results in arrays of lengths=11.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec32" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we discussed why data exploration is important and how can we perform exploratory analysis on datasets.</p><p class="calibre11">These are the various important techniques and concepts that we discussed:</p><div><ul class="itemizedlist"><li class="listitem">Sampling is a technique to randomly select unrelated data from the given dataset so that we can generalize the results that we generate on this selected data over the complete dataset.</li><li class="listitem">Weight vectors are important when the dataset that we have or gather doesn't represent the actual data.</li><li class="listitem">Why it is necessary to know the column types and how summary functions can be really helpful in getting the gist of the dataset.</li><li class="listitem">Mean, median, mode, standard deviation, variance, and scalar statistics, and how they are implemented in Julia.</li><li class="listitem">Measuring the variations in a dataset is really important and z-scores and entropy can be really useful.</li><li class="listitem">After some basic data cleaning and some understanding, visualization can be very beneficial and insightful.</li></ul></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch03lvl1sec33" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://julia.readthedocs.io/en/latest/manual/">http://julia.readthedocs.io/en/latest/manual/</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://dataframesjl.readthedocs.io/en/latest/">https://dataframesjl.readthedocs.io/en/latest/</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/JuliaStats/StatsBase.jl">https://github.com/JuliaStats/StatsBase.jl</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://dcjones.github.io/Gadfly.jl/">http://dcjones.github.io/Gadfly.jl/</a></li></ul></div></div></div>



  </body></html>