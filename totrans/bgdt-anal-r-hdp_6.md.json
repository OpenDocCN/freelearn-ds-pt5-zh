["```py\nModel <-lm(target ~ ex_var1, data=train_dataset)\n```", "```py\n# Defining data variables\nX = matrix(rnorm(2000), ncol = 10)\ny = as.matrix(rnorm(200))\n\n# Bundling data variables into dataframe\ntrain_data <- data.frame(X,y)\n\n# Training model for generating prediction\nlmodel<- lm(y~ train_data $X1 + train_data $X2 + train_data $X3 + train_data $X4 + train_data $X5 + train_data $X6 + train_data $X7 + train_data $X8 + train_data $X9 + train_data $X10,data= train_data)\n\nsummary(lmodel)\n```", "```py\n# Defining the datasets with Big Data matrix X\nX = matrix(rnorm(20000), ncol = 10)\nX.index = to.dfs(cbind(1:nrow(X), X))\ny = as.matrix(rnorm(2000))\n```", "```py\n# Function defined to be used as reducers \nSum = \n  function(., YY) \n    keyval(1, list(Reduce('+', YY)))\n```", "```py\n    # XtX = \n      values(\n\n    # For loading hdfs data in to R \n        from.dfs(\n\n    # MapReduce Job to produce XT*X\n          mapreduce(\n            input = X.index,\n\n    # Mapper – To calculate and emitting XT*X\n            map = \n              function(., Xi) {\n                yi = y[Xi[,1],]\n                Xi = Xi[,-1]\n                keyval(1, list(t(Xi) %*% Xi))},\n\n    # Reducer – To reduce the Mapper output by performing sum operation over them\n            reduce = Sum,\n            combine = TRUE)))[[1]]\n    ```", "```py\n    X = matrix(rnorm(2000), ncol = 10)\n    ```", "```py\nXty = values(\n\n# For loading hdfs data\nfrom.dfs(\n\n# MapReduce job to produce XT * y\n      mapreduce(\n       input = X.index,\n\n# Mapper – To calculate and emitting XT*y\n        map = function(., Xi) {\n          yi = y[Xi[,1],]\n          Xi = Xi[,-1]\n          keyval(1, list(t(Xi) %*% yi))},\n\n# Reducer – To reducer the Mapper output by performing # sum operation over them\n        reduce = Sum,\n        combine = TRUE)))[[1]]\n```", "```py\n    solve(XtX, Xty)\n    ```", "```py\n#loading iris dataset\ndata(iris)\n\n# Setting up target variable\ntarget <- data.frame(isSetosa=(iris$Species == 'setosa'))\n\n# Adding target to iris and creating new dataset\ninputdata <- cbind(target,iris)\n\n# Defining the logistic regression formula\nformula <- isSetosa ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width\n\n# running Logistic model via glm()\nlogisticModel <- glm(formula, data=inputdata, family=\"binomial\")\n```", "```py\n    # Mapper – computes the contribution of a subset of points to the gradient.\n\n    lr.map = \n        function(., M) {\n          Y = M[,1] \n          X = M[,-1]\n          keyval(\n            1,\n            Y * X * \n              g(-Y * as.numeric(X %*% t(plane))))}\n    ```", "```py\n    # Reducer – Perform sum operation over Mapper output.\n\n    lr.reduce =\n        function(k, Z) \n          keyval(k, t(as.matrix(apply(Z,2,sum))))\n    ```", "```py\n# MapReduce job – Defining MapReduce function for executing logistic regression\n\nlogistic.regression = \n  function(input, iterations, dims, alpha){\n  plane = t(rep(0, dims))\n  g = function(z) 1/(1 + exp(-z))\n  for (i in 1:iterations) {\n    gradient = \n      values(\n        from.dfs(\n          mapreduce(\n            input,\n            map = lr.map,\n            reduce = lr.reduce,\n            combine = T)))\n    plane = plane + alpha * gradient }\n  plane }\n```", "```py\n# Loading dataset\ndata(foodstamp)\n\n# Storing data to hdfs \ntestdata <-  to.dfs(as.matrix(foodstamp))\n\n# Running logistic regression with R and Hadoop\nprint(logistic.regression(testdata,10,3,0.05))\n```", "```py\n# Loading iris flower dataset\ndata(\"iris\")\n# generating clusters for iris dataset\nkmeans <- kmeans(iris[, -5], 3, iter.max = 1000)\n\n# comparing iris Species with generated cluster points\nComp <- table(iris[, 5], kmeans$cluster)\n```", "```py\n    # distance calculation function\n    dist.fun = \n          function(C, P) {\n            apply(\n              C,\n              1, \n              function(x) \n                colSums((t(P) - x)^2))}\n    ```", "```py\n    # k-Means Mapper\n      kmeans.map = \n          function(., P) {\n            nearest = {\n\n    # First interations- Assign random cluster centers \n              if(is.null(C)) \n                sample(\n                  1:num.clusters, \n                  nrow(P), \n                  replace = T)\n\n    # Rest of the iterations, where the clusters are assigned # based on the minimum distance from points\n              else {\n                D = dist.fun(C, P)\n                nearest = max.col(-D)}}\n\n           if(!(combine || in.memory.combine))\n              keyval(nearest, P) \n            else \n              keyval(nearest, cbind(1, P))}\n    ```", "```py\n    # k-Means Reducer\n    kmeans.reduce = {\n\n    # calculating the column average for both of the \n    # conditions\n\n          if (!(combine || in.memory.combine) ) \n            function(., P) \n              t(as.matrix(apply(P, 2, mean)))\n          else \n            function(k, P) \n              keyval(\n                k, \n                t(as.matrix(apply(P, 2, sum))))}\n    ```", "```py\n        # k-Means MapReduce – for \n        kmeans.mr = \n          function(\n            P, \n            num.clusters, \n            num.iter, \n            combine, \n            in.memory.combine) {\n            C = NULL\n            for(i in 1:num.iter ) {\n              C = \n                values(\n\n        # Loading hdfs dataset\n                  from.dfs(\n\n        # MapReduce job, with specification of input dataset,\n        # Mapper and Reducer\n                    mapreduce(\n                      P,\n                      map = kmeans.map,\n                      reduce = kmeans.reduce)))\n              if(combine || in.memory.combine)\n                C = C[, -1]/C[, 1]\n              if(nrow(C) < num.clusters) {\n                C = \n                  rbind(\n                    C,\n                    matrix(\n                      rnorm(\n                        (num.clusters - \n                           nrow(C)) * nrow(C)), \n                      ncol = nrow(C)) %*% C) }}\n                C}\n        ```", "```py\n    # Input data points\n    P = do.call(\n          rbind, \n          rep(\n\n            list(\n\n    # Generating Matrix of\n              matrix(\n    # Generate random normalized data with sd = 10\n                rnorm(10, sd = 10), \n                ncol=2)), \n            20)) + \n        matrix(rnorm(200), ncol =2)\n    ```", "```py\n    # Running kmeans.mr Hadoop MapReduce algorithms with providing the required input parameters\n\n    kmeans.mr(\n          to.dfs(P),\n          num.clusters = 12, \n          num.iter = 5,\n          combine = FALSE,\n          in.memory.combine = FALSE)\n    ```", "```py\n# user ID, item ID, item's rating\n1,         101,     5.0\n1,         102,     3.0\n1,         103,     2.5\n2,         101,     2.0\n2,         102,     2.5\n2,         103,     5.0\n2,         104,     2.0\n3,         101,     2.0\n3,         104,     4.0\n3,         105,     4.5\n3,         107,     5.0\n4,         101,     5.0\n4,         103,     3.0\n4,         104,     4.5\n4,         106,     4.0\n5,         101,     4.0\n5,         102,     3.0\n5,         103,     2.0\n5,         104,     4.0\n5,         105,     3.5\n5,         106,     4.0\n```", "```py\nCo-occurrence matrix * scoring matrix = Recommended Results\n```", "```py\n    # Quote plyr package\n    library (plyr)\n\n    # Read dataset\n    train <-read.csv (file = \"small.csv\", header = FALSE)\n    names (train) <-c (\"user\", \"item\", \"pref\") \n\n    # Calculated User Lists\n    usersUnique <-function () {\n      users <-unique (train $ user)\n      users [order (users)]\n    }\n\n    # Calculation Method Product List\n    itemsUnique <-function () {\n      items <-unique (train $ item)\n      items [order (items)]\n    }\n\n    # Derive unique User Lists\n    users <-usersUnique () \n\n    # Product List\n    items <-itemsUnique () \n\n    # Establish Product List Index\n    index <-function (x) which (items %in% x)\n    data<-ddply(train,.(user,item,pref),summarize,idx=index(item)) \n\n    # Co-occurrence matrix\n    Co-occurrence <-function (data) {\n      n <-length (items)\n      co <-matrix (rep (0, n * n), nrow = n)\n      for (u in users) {\n        idx <-index (data $ item [which(data$user == u)])\n        m <-merge (idx, idx)\n        for (i in 1: nrow (m)) {\n          co [m$x[i], m$y[i]] = co[m$x[i], m$y[i]]+1\n        }\n      }\n      return (co)\n    }\n\n    # Generate co-occurrence matrix\n    co <-co-occurrence (data) \n    ```", "```py\n    # Recommendation algorithm\n    recommend <-function (udata = udata, co = coMatrix, num = 0) {\n      n <- length(items)\n\n      # All of pref\n      pref <- rep (0, n)\n      pref[udata$idx] <-udata$pref\n\n      # User Rating Matrix\n      userx <- matrix(pref, nrow = n)\n\n      # Scoring matrix co-occurrence matrix *\n      r <- co %*% userx\n\n      # Recommended Sort\n      r[udata$idx] <-0\n      idx <-order(r, decreasing = TRUE)\n      topn <-data.frame (user = rep(udata$user[1], length(idx)), item = items[idx], val = r[idx])\n\n      # Recommended results take months before the num\n      if (num> 0) {\n        topn <-head (topn, num)\n      }\n\n      # Recommended results take months before the num\n      if (num> 0) {\n        topn <-head (topn, num)\n      }\n\n      # Back to results \n      return (topn)\n    }\n    ```", "```py\n    # initializing dataframe for recommendations storage\n    recommendation<-data.frame()\n\n    # Generating recommendations for all of the users\n    for(i in 1:length(users)){\n      udata<-data[which(data$user==users[i]),]\n      recommendation<-rbind(recommendation,recommend(udata,co,0)) \n    }\n    ```", "```py\n    # Load rmr2 package\n    library (rmr2)\n\n    # Input Data File\n    train <-read.csv (file = \"small.csv\", header = FALSE)\n    names (train) <-c (\"user\", \"item\", \"pref\")\n\n    # Use the hadoop rmr format, hadoop is the default setting.\n    rmr.options (backend = 'hadoop')\n\n    # The data set into HDFS\n    train.hdfs = to.dfs (keyval (train$user, train))\n\n    # see the data from hdfs\n    from.dfs (train.hdfs)\n    ```", "```py\n    # MapReduce job 1 for co-occurrence matrix items\n    train.mr <-mapreduce (\n      train.hdfs, \n      map = function (k, v) {\n        keyval (k, v$item)\n      }\n\n    # for identification of co-occurrence items\n      , Reduce = function (k, v) {\n        m <-merge (v, v)\n        keyval (m$x, m$y)\n      }\n    )\n    ```", "```py\n    # MapReduce function for calculating the frequency of the combinations of the items.\n    step2.mr <-mapreduce (\n      train.mr,\n\n      map = function (k, v) {\n        d <-data.frame (k, v)\n        d2 <-ddply (d,. (k, v), count)\n\n        key <- d2$k\n        val <- d2\n        keyval(key, val)\n      }\n    )\n\n    # loading data from HDFS\n    from.dfs(step2.mr)\n    ```", "```py\n    # MapReduce job for establish user scoring matrix to articles\n\n    train2.mr <-mapreduce (\n      train.hdfs, \n      map = function(k, v) {\n          df <- v\n\n    # key as item\n        key <-df $ item\n\n    # value as [item, user pref]\n        val <-data.frame (item = df$item, user = df$user, pref = df$pref)\n\n    # emitting (key, value)pairs\n        keyval(key, val)\n      }\n    )\n\n    # loading data from HDFS\n    from.dfs(train2.mr)\n    ```", "```py\n    # Running equi joining two data – step2.mr and train2.mr\n    eq.hdfs <-equijoin (\n      left.input = step2.mr, \n      right.input = train2.mr,\n      map.left = function (k, v) {\n        keyval (k, v)\n      },\n      map.right = function (k, v) {\n        keyval (k, v)\n      },\n      outer = c (\"left\")\n    )\n\n    # loading data from HDFS\n    from.dfs (eq.hdfs)\n    ```", "```py\n    # MapReduce job to obtain recommended list of result from equijoined data\n    cal.mr <-mapreduce (\n      input = eq.hdfs,\n\n      map = function (k, v) {\n        val <-v\n        na <-is.na (v$user.r)\n        if (length (which(na))> 0) val <-v [-which (is.na (v $ user.r)),]\n        keyval (val$kl, val)\n      }\n      , Reduce = function (k, v) {\n        val <-ddply (v,. (kl, vl, user.r), summarize, v = freq.l * pref.r)\n        keyval (val $ kl, val)\n      }\n    )\n\n    # loading data from HDFS\n    from.dfs (cal.mr)\n    ```", "```py\n    # MapReduce job for sorting the recommendation output\n    result.mr <-mapreduce (\n      input = cal.mr,\n      map = function (k, v) {\n        keyval (v $ user.r, v)\n      }\n      , Reduce = function (k, v) {\n        val <-ddply (v,. (user.r, vl), summarize, v = sum (v))\n        val2 <-val [order (val$v, decreasing = TRUE),]\n        names (val2) <-c (\"user\", \"item\", \"pref\")\n        keyval (val2$user, val2)\n      }\n    )\n    # loading data from HDFS\n    from.dfs (result.mr)\n    ```"]