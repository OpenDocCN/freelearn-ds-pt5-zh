["```py\npackage com.chapter16.SparkTesting\nobject SimpleScalaTest {\n  def main(args: Array[String]):Unit= {\n    val a = 5\n    val b = 5\n    assert(a == b)\n      println(\"Assertion success\")       \n  }\n}\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._\nobject SimpleScalaTest {\n  def main(args: Array[String]):Unit= {\n    val a = 5\n    val b = 4\n    assert(a == b)\n      println(\"Assertion success\")       \n  }\n}\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._\nobject AssertResult {\n  def main(args: Array[String]):Unit= {\n    val x = 10\n    val y = 6\n    assertResult(3) {\n      x - y\n    }\n  }\n}\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._\nobject ExpectedException {\n  def main(args: Array[String]):Unit= {\n    val s = \"Hello world!\"\n    try {\n      s.charAt(0)\n      fail()\n    } catch {\n      case _: IndexOutOfBoundsException => // Expected, so continue\n    }\n  }\n}\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._\nobject ExpectedException {\n  def main(args: Array[String]):Unit= {\n    val s = \"Hello world!\"\n    try {\n      s.charAt(-1)\n      fail()\n    } catch {\n      case _: IndexOutOfBoundsException => // Expected, so continue\n    }\n  }\n}\n\n```", "```py\nassertDoesNotCompile(\"val a: String = 1\")\n\n```", "```py\nassertTypeError(\"val a: String = 1\")\n\n```", "```py\nassertCompiles(\"val a: Int = 1\")\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._ \nobject CompileOrNot {\n  def main(args: Array[String]):Unit= {\n    assertDoesNotCompile(\"val a: String = 1\")\n    println(\"assertDoesNotCompile True\")\n\n    assertTypeError(\"val a: String = 1\")\n    println(\"assertTypeError True\")\n\n    assertCompiles(\"val a: Int = 1\")\n    println(\"assertCompiles True\")\n\n    assertDoesNotCompile(\"val a: Int = 1\")\n    println(\"assertDoesNotCompile True\")\n  }\n}\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.apache.spark._\nimport org.apache.spark.sql.SparkSession\nclass wordCounterTestDemo {\n  val spark = SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n    .appName(s\"OneVsRestExample\")\n    .getOrCreate()\n  def myWordCounter(fileName: String): Long = {\n    val input = spark.sparkContext.textFile(fileName)\n    val counts = input.flatMap(_.split(\" \")).distinct()\n    val counter = counts.count()\n    counter\n  }\n}\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.apache.spark._\nimport org.apache.spark.sql.SparkSession\nobject wordCounter {\n  val spark = SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n    .appName(\"Testing\")\n    .getOrCreate()    \n  val fileName = \"data/words.txt\";\n  def myWordCounter(fileName: String): Long = {\n    val input = spark.sparkContext.textFile(fileName)\n    val counts = input.flatMap(_.split(\" \")).distinct()\n    val counter = counts.count()\n    counter\n  }\n  def main(args: Array[String]): Unit = {\n    val counter = myWordCounter(fileName)\n    println(\"Number of words: \" + counter)\n  }\n}\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._\nimport org.junit.Test\nimport org.apache.spark.sql.SparkSession\nclass wordCountTest {\n  val spark = SparkSession\n    .builder\n    .master(\"local[*]\")\n    .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n    .appName(s\"OneVsRestExample\")\n    .getOrCreate()   \n    @Test def test() {\n      val fileName = \"data/words.txt\"\n      val obj = new wordCounterTestDemo()\n      assert(obj.myWordCounter(fileName) == 214)\n           }\n    spark.stop()\n}\n\n```", "```py\nassert(obj.myWordCounter(fileName) == 210)\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.SparkSession\nclass wordCountRDD {\n  def prepareWordCountRDD(file: String, spark: SparkSession): RDD[(String, Int)] = {\n    val lines = spark.sparkContext.textFile(file)\n    lines.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_ + _)\n  }\n}\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.scalatest.{ BeforeAndAfterAll, FunSuite }\nimport org.scalatest.Assertions._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.rdd.RDD\nclass wordCountTest2 extends FunSuite with BeforeAndAfterAll {\n  var spark: SparkSession = null\n  def tokenize(line: RDD[String]) = {\n    line.map(x => x.split(' ')).collect()\n  }\n  override def beforeAll() {\n    spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n      .appName(s\"OneVsRestExample\")\n      .getOrCreate()\n  }  \n  test(\"Test if two RDDs are equal\") {\n    val input = List(\"To be,\", \"or not to be:\", \"that is the question-\", \"William Shakespeare\")\n    val expected = Array(Array(\"To\", \"be,\"), Array(\"or\", \"not\", \"to\", \"be:\"), Array(\"that\", \"is\", \"the\", \"question-\"), Array(\"William\", \"Shakespeare\"))\n    val transformed = tokenize(spark.sparkContext.parallelize(input))\n    assert(transformed === expected)\n  }  \n  test(\"Test for word count RDD\") {\n    val fileName = \"C:/Users/rezkar/Downloads/words.txt\"\n    val obj = new wordCountRDD\n    val result = obj.prepareWordCountRDD(fileName, spark)    \n    assert(result.count() === 214)\n  }\n  override def afterAll() {\n    spark.stop()\n  }\n}\n\n```", "```py\ntest(\"Test for word count RDD\") { \n  val fileName = \"data/words.txt\"\n  val obj = new wordCountRDD\n  val result = obj.prepareWordCountRDD(fileName, spark)    \n  assert(result.count() === 210)\n}\ntest(\"Test if two RDDs are equal\") {\n  val input = List(\"To be\", \"or not to be:\", \"that is the question-\", \"William Shakespeare\")\n  val expected = Array(Array(\"To\", \"be,\"), Array(\"or\", \"not\", \"to\", \"be:\"), Array(\"that\", \"is\", \"the\", \"question-\"), Array(\"William\", \"Shakespeare\"))\n  val transformed = tokenize(spark.sparkContext.parallelize(input))\n  assert(transformed === expected)\n}\n\n```", "```py\n<dependency>\n  <groupId>com.holdenkarau</groupId>\n  <artifactId>spark-testing-base_2.10</artifactId>\n  <version>2.0.0_0.6.0</version>\n</dependency>\n\n```", "```py\n\"com.holdenkarau\" %% \"spark-testing-base\" % \"2.0.0_0.6.0\"\n\n```", "```py\njavaOptions ++= Seq(\"-Xms512M\", \"-Xmx2048M\", \"-XX:MaxPermSize=2048M\", \"-XX:+CMSClassUnloadingEnabled\")\n\n```", "```py\n<argLine>-Xmx2048m -XX:MaxPermSize=2048m</argLine>\n\n```", "```py\nparallelExecution in Test := false\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._\nimport org.apache.spark.rdd.RDD\nimport com.holdenkarau.spark.testing.SharedSparkContext\nimport org.scalatest.FunSuite\nclass TransformationTestWithSparkTestingBase extends FunSuite with SharedSparkContext {\n  def tokenize(line: RDD[String]) = {\n    line.map(x => x.split(' ')).collect()\n  }\n  test(\"works, obviously!\") {\n    assert(1 == 1)\n  }\n  test(\"Words counting\") {\n    assert(sc.parallelize(\"Hello world My name is Reza\".split(\"\\\\W\")).map(_ + 1).count == 6)\n  }\n  test(\"Testing RDD transformations using a shared Spark Context\") {\n    val input = List(\"Testing\", \"RDD transformations\", \"using a shared\", \"Spark Context\")\n    val expected = Array(Array(\"Testing\"), Array(\"RDD\", \"transformations\"), Array(\"using\", \"a\", \"shared\"), Array(\"Spark\", \"Context\"))\n    val transformed = tokenize(sc.parallelize(input))\n    assert(transformed === expected)\n  }\n}\n\n```", "```py\n17/02/26 13:22:00 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n\n```", "```py\nspark.executor.extraJavaOptions=-Dlog4j.configuration=file:/usr/local/spark-2.1.1/conf/log4j.properties spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/usr/local/spark-2.1.1/conf/log4j.properties\n\n```", "```py\npackage com.chapter14.Serilazition\nimport org.apache.log4j.LogManager\nimport org.apache.log4j.Level\nimport org.apache.spark.sql.SparkSession\nobject myCustomLog {\n  def main(args: Array[String]): Unit = {   \n    val log = LogManager.getRootLogger    \n    //Everything is printed as INFO once the log level is set to INFO untill you set the level to new level for example WARN. \n    log.setLevel(Level.INFO)\n    log.info(\"Let's get started!\")    \n    // Setting logger level as WARN: after that nothing prints other than WARN\n    log.setLevel(Level.WARN)    \n    // Creating Spark Session\n    val spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n      .appName(\"Logging\")\n      .getOrCreate()\n    // These will note be printed!\n    log.info(\"Get prepared!\")\n    log.trace(\"Show if there is any ERROR!\")\n    //Started the computation and printing the logging information\n    log.warn(\"Started\")\n    spark.sparkContext.parallelize(1 to 20).foreach(println)\n    log.warn(\"Finished\")\n  }\n}\n\n```", "```py\n17/05/13 16:39:14 INFO root: Let's get started!\n17/05/13 16:39:15 WARN root: Started\n4 \n1 \n2 \n5 \n3 \n17/05/13 16:39:16 WARN root: Finished\n\n```", "```py\nlog4j.logger.org=OFF\n\n```", "```py\nobject myCustomLogger {\n  def main(args: Array[String]):Unit= {\n    // Setting logger level as WARN\n    val log = LogManager.getRootLogger\n    log.setLevel(Level.WARN)\n    // Creating Spark Context\n    val conf = new SparkConf().setAppName(\"My App\").setMaster(\"local[*]\")\n    val sc = new SparkContext(conf)\n    //Started the computation and printing the logging information\n    //log.warn(\"Started\")\n    val i = 0\n    val data = sc.parallelize(i to 100000)\n    data.map{number =>\n      log.info(“My number”+ i)\n      number.toString\n    }\n    //log.warn(\"Finished\")\n  }\n}\n\n```", "```py\norg.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: ...\nException in thread \"main\" org.apache.spark.SparkException: Task not serializable \nCaused by: java.io.NotSerializableException: org.apache.log4j.spi.RootLogger\nSerialization stack: object not serializable\n\n```", "```py\nclass MyMapper(n: Int) extends Serializable {\n  @transient lazy val log = org.apache.log4j.LogManager.getLogger(\"myLogger\")\n  def logMapper(rdd: RDD[Int]): RDD[String] =\n    rdd.map { i =>\n      log.warn(\"mapping: \" + i)\n      (i + n).toString\n    }\n  }\n\n```", "```py\n//Companion object \nobject MyMapper {\n  def apply(n: Int): MyMapper = new MyMapper(n)\n}\n\n```", "```py\n//Main object\nobject myCustomLogwithClosureSerializable {\n  def main(args: Array[String]) {\n    val log = LogManager.getRootLogger\n    log.setLevel(Level.WARN)\n    val spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n      .appName(\"Testing\")\n      .getOrCreate()\n    log.warn(\"Started\")\n    val data = spark.sparkContext.parallelize(1 to 100000)\n    val mapper = MyMapper(1)\n    val other = mapper.logMapper(data)\n    other.collect()\n    log.warn(\"Finished\")\n  }\n\n```", "```py\nclass MultiplicaitonOfTwoNumber {\n  def multiply(a: Int, b: Int): Int = {\n    val product = a * b\n    product\n  }\n}\n\n```", "```py\nval myRDD = spark.sparkContext.parallelize(0 to 1000)\n    myRDD.foreachPartition(s => {\n      val notSerializable = new MultiplicaitonOfTwoNumber\n      println(notSerializable.multiply(s.next(), s.next()))\n    })\n\n```", "```py\npackage com.chapter16.SparkTesting\nimport org.apache.spark.sql.SparkSession\nclass MultiplicaitonOfTwoNumber {\n  def multiply(a: Int, b: Int): Int = {\n    val product = a * b\n    product\n  }\n}\nobject MakingTaskSerilazible {\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n      .appName(\"MakingTaskSerilazible\")\n      .getOrCreate()\n val myRDD = spark.sparkContext.parallelize(0 to 1000)\n    myRDD.foreachPartition(s => {\n      val notSerializable = new MultiplicaitonOfTwoNumber\n      println(notSerializable.multiply(s.next(), s.next()))\n    })\n  }\n}\n\n```", "```py\n0\n5700\n1406\n156\n4032\n7832\n2550\n650\n\n```", "```py\n--driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000\n\n```", "```py\n--num-executors 1\\\n--executor-cores 1 \\\n--conf \"spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=n,address=localhost:4000,suspend=n\"\n\n```", "```py\n$ SPARK_HOME/bin/spark-submit \\\n--class \"com.chapter13.Clustering.KMeansDemo\" \\\n--master spark://ubuntu:7077 \\\n--num-executors 1\\\n--executor-cores 1 \\\n--conf \"spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=n,address= host_name_to_your_computer.org:5005,suspend=n\" \\\n--driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000 \\\n KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar \\\nSaratoga_NY_Homes.txt\n\n```", "```py\nListening for transport dt_socket at address: 4000 \n\n```", "```py\n$ export SPARK_JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,address=4000,suspend=y,onuncaught=n\n\n```", "```py\nYARN_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4000 $YARN_OPTS\"\n\n```", "```py\n$ export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000\n\n```", "```py\n$ SPARK_HOME/bin/spark-submit \\\n--class \"com.chapter13.Clustering.KMeansDemo\" \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 16g \\\n--executor-memory 4g \\\n--executor-cores 4 \\\n--queue the_queue \\\n--num-executors 1\\\n--executor-cores 1 \\\n--conf \"spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=n,address= host_name_to_your_computer.org:4000,suspend=n\" \\\n--driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000 \\\n KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar \\\nSaratoga_NY_Homes.txt\n\n```", "```py\n$ export SPARK_WORKER_OPTS=\"-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n\"\n$ export SPARK_MASTER_OPTS=\"-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n\"\n\n```", "```py\n$ SPARKH_HOME/sbin/start-master.sh\n\n```", "```py\nobject DebugTestSBT {\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"C:/Exp/\")\n      .appName(\"Logging\")\n      .getOrCreate()      \n    spark.sparkContext.setCheckpointDir(\"C:/Exp/\")\n    println(\"-------------Attach debugger now!--------------\")\n    Thread.sleep(8000)\n    // code goes here, with breakpoints set on the lines you want to pause\n  }\n}\n\n```", "```py\n$ sbt assembly\n\n```", "```py\n$ export SPARK_JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\n\n```", "```py\n$ SPARK_HOME/bin/spark-submit --class Test --master local[*] --driver-memory 4G --executor-memory 4G /path/project-assembly-0.0.1.jar\n\n```"]