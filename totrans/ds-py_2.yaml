- en: '*Chapter 3*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Machine Learning via Scikit-Learn
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Prepare data for different types of supervised learning models.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune model hyperparameters using a grid search.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract feature importance from a tuned model.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate performance of classification and regression models.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will be covering the important concepts of handling data
    and making the data ready for analysis.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**scikit-learn** is a free, open source library built for Python that contains
    an assortment of supervised and unsupervised machine learning algorithms. Additionally,
    scikit-learn provides functions for data preprocessing, hyperparameter tuning,
    and model evaluation, which we will be covering in the upcoming chapters. It streamlines
    the model-building process and is easy to install on a wide variety of platforms.
    scikit-learn started in 2007 as a Google Summer of Code project by David Corneapeau,
    and after a series of developments and releases, scikit-learn has evolved into
    one of the premier tools used by academics and professionals for machine learning.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn to build a variety of widely used modeling algorithms,
    namely, linear and logistic regression, support vector machines (SVMs), decision
    trees, and random forests. First, we will cover linear and logistic regression.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Linear and Logistic Regression
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In regression, a single dependent, or outcome variable is predicted using one
    or more independent variables. Use cases for regression are included, but are
    not limited to predicting:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The win percentage of a team, given a variety of team statistics
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The risk of heart disease, given family history and a number of physical and
    psychological characteristics
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The likelihood of snowfall, given several climate measurements
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear and logistic regression are popular choices for predicting such outcomes
    due to the ease and transparency of interpretability, as well as the ability to
    extrapolate to values not seen in the training data. The end goal of linear regression
    is to draw a straight line through the observations that minimizes the absolute
    distance between the line and observations (that is, the line of best fit). Therefore,
    in linear regression, it is assumed that the relationship between the feature(s)
    and the continuous dependent variable follows a straight line. Lines are defined
    in slope-intercept form (that is, *y = a + bx*), where *a* is the intercept (that
    is, the value of *y* when *x* is 0), *b* is the slope, and *x* is the independent
    variable. There are two types of linear regression that will be covered in this
    chapter: simple linear regression and multiple linear regression.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Simple Linear Regression
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Simple linear regression models define the relationship between one feature
    and the continuous outcome variable using *y =* *α* *+* *β**x*. This equation
    is like the slope-intercept form, where *y* denotes the predicted value of the
    dependent variable, *α* denotes the intercept, *β* (beta) represents the slope,
    and *x* is the value of the independent variable. Given *x*, regression models
    compute the values for *α* and *β* that minimize the absolute difference between
    predicted *y* values (that is, *ŷ*) and actual *y* values.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we are predicting the weight of an individual in kilograms (kg)
    using height in meters (m) as the lone predictor variable, and the simple linear
    regression model computes 1.5 as the value for *α* and 50 as the coefficient for
    *β*, this model can be interpreted as for every 1 m increase in height, weight
    increases by 50 kg. Thus, we can predict that the weight of an individual who
    is 1.8 m is 91.5 kg using y = 1.5 + (50 x 1.8). In the following exercises, we
    will demonstrate simple linear regression using scikit-learn.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 21: Preparing Data for a Linear Regression Model'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To prepare our data for a simple linear regression model, we will use a random
    subset of the Weather in Szeged 2006-2016 dataset, which consists of hourly weather
    measurements from April 1, 2006, to September 9, 2016, in Szeged, Hungary. The
    adapted data is provided as a `.csv` file (https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter02/weather.csv)
    and consists of 10,000 observations of 8 variables:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '`Temperature_c`: The temperature in Celsius'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Humidity`: The proportion of humidity'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Wind_Speed_kmh`: The wind speed in kilometers per hour'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Wind_Bearing_Degrees`: The wind direction in degrees clockwise from due north'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Visibility_km`: The visibility in kilometers'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pressure_millibars`: The atmospheric pressure as measured in millibars'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Rain`: rain = 1, snow = 0'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Description`: Warm, normal, or cold'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Import the `weather.csv` dataset using the following code:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Explore the data using `df.info()`:![Figure 3.1: Information describing df'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13322_03_01.jpg)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.1: Information describing df'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `Description` column is the lone categorical variable in `df`. Check the
    number of levels in `Description` as follows:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The number of levels is shown in the following screenshot:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.2: Number of levels in the ''Description'' column](img/C13322_03_02.jpg)'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.2: Number of levels in the ''Description'' column'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi-class, categorical variables must be converted into dummy variables via
    a process termed "dummy coding." Dummy coding a multi-class, categorical variable
    creates n-1 new binary features, which correspond to the levels within the categorical
    variable. For example, a multi-class, categorical variable with three levels will
    create two binary features. After the multi-class, categorical feature has been
    dummy coded, the original feature must be dropped.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To dummy code all multi-class, categorical variables, refer to the following
    code:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The original DataFrame, `df`, consisted of eight columns, one of which (that
    is, Description) was a multi-class, categorical variable with three levels.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In step 4, we transformed this feature into n-1 (that is, 2), separated dummy
    variables, and dropped the original feature, `Description`. Thus, `df_dummies`
    should now contain one more column than df (that is, 9 columns). Check this out
    using the following code:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Figure 3.3: Number of columns after dummy coding'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_03.jpg)'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.3: Number of columns after dummy coding'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To remove any possible order effects in the data, it is good practice to first
    shuffle the rows of the data prior to splitting the data into features (`X`) and
    outcome (`y`). To shuffle the rows in `df_dummies`, refer to the code here:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that the data has been shuffled, we will split the rows in our data into
    features (`X`) and the dependent variable (`y`).
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Linear regression is used for predicting a continuous outcome. Thus, in this
    exercise, we will pretend that the continuous variable `Temperature_c` (the temperature
    in Celsius) is the dependent variable, and that we are preparing data to fit a
    linear regression model.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split `df_shuffled` into `X` and `y` as follows:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Split `X` and `y` into testing and training data using the code here:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that the data has been dummy coded, shuffled, split into `X` and `y`, and
    further divided into testing and training datasets, it is ready to be used in
    a linear or logistic regression model.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The screenshot here shows the first five rows of `X_train`:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.4: The first five rows of X_train'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_04.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.4: The first five rows of X_train'
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 22: Fitting a Simple Linear Regression Model and Determining the Intercept
    and Coefficient'
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will continue using the data we prepared in Exercise 21
    to fit a simple linear regression model to predict the temperature in Celsius
    from the humidity.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 21, perform the following steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'To instantiate a linear regression model, refer to the code here:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Fit the model to the `Humidity` column in the training data using this code:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure 3.5: The output from fitting the simple linear regression model'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_05.jpg)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.5: The output from fitting the simple linear regression model'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Extract the value for the intercept using the following code:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Extract the value for the `coefficient` as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we can print a message with the formula for predicting temperature in
    Celsius using the code here:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Figure 3.6: A formula to predict temperature in Celsius from humidity using
    simple linear regression'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_06.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.6: A formula to predict temperature in Celsius from humidity using
    simple linear regression'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Great work! According to this simple linear regression model, a day with a 0.78
    humidity value has a predicted a temperature of 10.56 degrees Celsius. Now that
    we are familiar with extracting the intercept and coefficients of our simple linear
    regression model, it is time to generate predictions and subsequently evaluate
    how the model performs on unseen, test data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！根据这个简单线性回归模型，一个湿度值为0.78的日子预测温度为10.56摄氏度。现在我们已经熟悉了提取简单线性回归模型的截距和系数，是时候生成预测并评估模型在未见过的测试数据上的表现了。
- en: Teaching tip
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 教学提示
- en: Practice calculating temperature at various levels of humidity.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 练习在不同湿度水平下计算温度。
- en: 'Exercise 23: Generating Predictions and Evaluating the Performance of a Simple
    Linear Regression Model'
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习23：生成预测并评估简单线性回归模型的表现
- en: The very purpose of supervised learning is to use existing, labeled data to
    generate predictions. Thus, this exercise will demonstrate how to generate predictions
    on the test feature and generate model performance metrics by comparing the predictions
    to the actual values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的核心目的是使用现有的标记数据生成预测。因此，本练习将演示如何在测试特征上生成预测，并通过将预测与实际值进行比较来生成模型性能指标。
- en: 'Continuing from *Exercise 22*, perform the following steps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从*练习22*继续，执行以下步骤：
- en: 'Generate predictions on the test data using the following:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码在测试数据上生成预测：
- en: '[PRE12]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: A common way to evaluate model performance is to examine the correlation between
    the predicted and actual values using a scatterplot. The scatterplot displays
    the relationship between the actual and predicted values. A perfect regression
    model will display a straight, diagonal line between predicted and actual values.
    The relationship between the predicted and actual values can be quantified using
    the Pearson r correlation coefficient. In the following step, we will create a
    scatterplot of the predicted and actual values.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估模型性能的一种常见方法是使用散点图检查预测值与实际值之间的相关性。散点图展示了实际值与预测值之间的关系。一个完美的回归模型将在预测值和实际值之间显示一条直线。预测值与实际值之间的关系可以通过皮尔逊r相关系数来量化。在接下来的步骤中，我们将创建一个预测值与实际值的散点图。
- en: 'It is helpful if the correlation coefficient is displayed in the plot''s title.
    The following code will show us how to do this:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果相关系数显示在图表标题中会更有帮助。以下代码将演示如何做到这一点：
- en: '[PRE13]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the resultant output:'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是生成的输出结果：
- en: '![Figure 3.7: Predicted versus actual values from a simple linear regression
    model'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.7：简单线性回归模型的预测值与实际值对比'
- en: '](img/C13322_03_07.jpg)'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_07.jpg)'
- en: 'Figure 3.7: Predicted versus actual values from a simple linear regression
    model'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.7：简单线性回归模型的预测值与实际值对比
- en: Note
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: With a Pearson r value of 0.62, there is a moderate, positive, linear correlation
    between the predicted and actual values. A perfect model would have all points
    on the plot in a straight line and an r value of 1.0.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 皮尔逊r值为0.62，表明预测值与实际值之间存在中等程度的正向线性相关性。一个完美的模型会使散点图中的所有点都在一条直线上，并且r值为1.0。
- en: 'A model that fits the data very well will have normally distributed residuals.
    To create a density plot of the residuals, refer to the following code:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个与数据拟合得非常好的模型，其残差应该呈正态分布。要创建残差的密度图，请参照以下代码：
- en: '[PRE14]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Refer to the resultant output here:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参阅此处的输出结果：
- en: '![Figure 3.8: A histogram of residuals from a simple linear regression model'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.8：简单线性回归模型的残差直方图'
- en: '](img/C13322_03_08.jpg)'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13322_03_08.jpg)'
- en: 'Figure 3.8: A histogram of residuals from a simple linear regression model'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.8：简单线性回归模型的残差直方图
- en: Note
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The histogram shows us that the residuals are negatively skewed and the value
    of the Shapiro W p-value in the title tells us that the distribution is not normal.
    This gives us further evidence that our model has room for improvement.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直方图显示残差呈负偏态，标题中的Shapiro W p值告诉我们该分布不是正态分布。这进一步证明我们的模型还有改进空间。
- en: 'Lastly, we will compute metrics for mean absolute error, mean squared error,
    root mean squared error, and R-squared, and put them into a DataFrame using the
    code here:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将计算平均绝对误差、均方误差、均方根误差和R平方值，并使用以下代码将它们放入一个数据框中：
- en: '[PRE15]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Please refer to the resultant output:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参阅生成的输出结果：
- en: '![Figure 3.9: Model evaluation metrics from a simple linear regression model'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.9：简单线性回归模型的模型评估指标'
- en: '](img/C13322_03_09.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13322_03_09.jpg)'
- en: 'Figure 3.9: Model evaluation metrics from a simple linear regression model'
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.9：简单线性回归模型的模型评估指标
- en: '**Mean absolute error** (**MAE**) is the average absolute difference between
    the predicted values and the actual values. **Mean squared error** (**MSE**) is
    the average of the squared differences between the predicted and actual values.
    **Root mean squared error** (**RMSE**) is the square root of the MSE. R-squared
    tells us the proportion of variance in the dependent variable that can be explained
    by the model. Thus, in this simple linear regression model, humidity explained
    only 38.9% of the variance in temperature. Additionally, our predictions were
    within ± 6.052 degrees Celsius.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）是预测值与实际值之间的平均绝对差。**均方误差**（**MSE**）是预测值与实际值之间差的平方的平均值。**均方根误差**（**RMSE**）是MSE的平方根。R方告诉我们可以由模型解释的因变量方差的比例。因此，在这个简单线性回归模型中，湿度仅能解释温度方差的38.9%。此外，我们的预测值在±6.052摄氏度范围内。'
- en: Here, we have successfully used scikit-learn to fit and evaluate a simple linear
    regression model. This is the first step in a very exciting journey to becoming
    a machine learning guru. Next, we will continue expanding our knowledge of regression
    and improving this model by exploring multiple linear regression.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们成功地使用 scikit-learn 拟合和评估了一个简单线性回归模型。这是成为机器学习专家之旅中的第一步。接下来，我们将继续扩展我们对回归的知识，并通过探索多元线性回归来改进这个模型。
- en: Multiple Linear Regression
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: Multiple linear regression models define the relationship between two or more
    features and the continuous outcome variable using *y =* *α* *+* *β**1**x**i1*
    *+* *β**2**x**i2* *+ … +* *β**p-1**x**i,p-1*. Again, *α* represents the intercept
    and *β* denotes the slope for each feature (*x*) in the model. Thus, if we are
    predicting the weight of an individual in kg using height in *m*, total cholesterol
    in milligrams per deciliter (*mg/dL)*, and minutes of cardiovascular exercise
    per day, and the multiple linear regression model computes 1.5 as the value for
    *α*, 50 as the coefficient for *β**1*, 0.1 as the coefficient for *β**2*, and
    -0.4 as the coefficient for *β**3*, this model can be interpreted as for every
    1 *m* increase in height, weight increases by 50 kg, controlling for all other
    features in the model. Additionally, for every 1 mg/dL increase in total cholesterol,
    weight increases by 0.1 kg, controlling for all other features in the model. Lastly,
    for every minute of cardiovascular exercise per day, weight decreases by 0.4 kg,
    controlling for all other features in the model. Thus, we can predict the weight
    of an individual who is 1.8 m tall, with total cholesterol of 200 mg/dL, and completes
    30 minutes of cardiovascular exercise per day as 99.5 kg using *y = 1.5 + (0.1
    x 50) + (200 x 0.5) + (30 x -0.4)*. In the following exercise, we will demonstrate
    conducting multiple linear regression using scikit-learn.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 多元线性回归模型通过 *y =* *α* *+* *β**1**x**i1* *+* *β**2**x**i2* *+ … +* *β**p-1**x**i,p-1*
    定义了两个或更多特征与连续结果变量之间的关系。再次说明，*α* 表示截距，*β* 表示模型中每个特征（*x*）的斜率。因此，如果我们使用身高（*m*）、总胆固醇（*mg/dL*）和每日心血管运动分钟数来预测个体的体重（kg），且多元线性回归模型计算出
    *α* 为1.5，*β**1* 为50，*β**2* 为0.1，*β**3* 为-0.4，这个模型可以解释为：在控制模型中的所有其他特征的情况下，每增加1
    *m* 的身高，体重增加50 kg。此外，每增加1 mg/dL 的总胆固醇，体重增加0.1 kg。最后，每天每增加1分钟的心血管运动，体重减少0.4 kg。因此，我们可以预测一个身高为1.8
    m、总胆固醇为200 mg/dL，并每天完成30分钟心血管运动的个体的体重为99.5 kg，使用 *y = 1.5 + (0.1 x 50) + (200
    x 0.5) + (30 x -0.4)*。在下一项练习中，我们将演示如何使用 scikit-learn 进行多元线性回归。
- en: 'Exercise 24: Fitting a Multiple Linear Regression Model and Determining the
    Intercept and Coefficients'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 24：拟合多元线性回归模型并确定截距和系数
- en: In this exercise, we will continue using the data we prepared in *Exercise 21*,
    *Preparing Data for a Linear Regression Model*, to fit a multiple linear regression
    model to predict the temperature in Celsius from all the features in the data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将继续使用我们在*练习21*中准备的数据，拟合一个多元线性回归模型，以预测数据中所有特征对应的温度（摄氏度）。
- en: 'Continuing from Exercise 23, perform the following steps:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 继续自练习 23，执行以下步骤：
- en: 'To instantiate a linear regression model, refer to the code here:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要实例化一个线性回归模型，请参考以下代码：
- en: '[PRE16]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Fit the model to the training data using this code:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将模型拟合到训练数据中：
- en: '[PRE17]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure 3.10: The output from fitting the multiple linear regression model'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_10.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.10: The output from fitting the multiple linear regression model'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Extract the value for the intercept using the following code:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Extract the value for the coefficients as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we can print a message with the formula for predicting temperature in
    Celsius using the code here:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Our output should look like this:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11: A formula to predict temperature in Celsius from humidity using
    multiple linear regression'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_11.jpg)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.11: A formula to predict temperature in Celsius from humidity using
    multiple linear regression'
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Nice job! According to this multiple regression model, a day with 0.78 humidity,
    5.0 km/h wind speed, wind direction at 81 degrees clockwise from due north, 3
    km of visibility, 1000 millibars of pressure, no rain, and is described as normal,
    has a predicted temperature in Celsius of 5.72 degrees. Now that we are familiar
    with extracting the intercept and coefficients of our multiple linear regression
    model, we can generate predictions and evaluate how the model performs on the
    test data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5: Generating Predictions and Evaluating the Performance of a Multiple
    Linear Regression Model'
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Exercise 23*, *Generating Predictions and Evaluating the Performance of
    a Simple Linear Regression Model*, we learned how to generate predictions and
    evaluate the performance of a simple linear regression model using a variety of
    methods. To reduce the code redundancy, we will evaluate the performance of our
    multiple linear regression model using the metrics in *step 4* of *Exercise 23*,
    and we will determine if the multiple linear regression model performed better
    or worse in relation to the simple linear regression model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 24, perform the following steps:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Generate predictions on the test data using all the features.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot predictions versus actual using a scatterplot.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the distribution of the residuals.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the metrics for mean absolute error, mean squared error, root mean
    squared error, and R-squared and put them into a DataFrame.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine if the multiple linear regression model performed better or worse
    in relation to the simple linear regression model.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 343.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You should find that the multiple linear regression model performed better on
    every metric relative to the simple linear regression model. Most notably, in
    the simple linear regression model, only 38.9% of the variance in temperature
    was described by the model. However, in the multiple linear regression model,
    86.6% of the variance in temperature was explained by the combination of features.
    Additionally, our simple linear regression model predicted temperatures, on average,
    within ± 6.052 degrees, while our multiple linear regression model predicted temperatures,
    on average, within ± 2.861 degrees.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The transparent nature of the intercept and beta coefficients make linear regression
    models very easy to interpret. In business, it is commonly requested that data
    scientists explain the effect of a certain feature on an outcome. Thus, linear
    regression provides metrics allowing a reasonable response to the business inquiry
    earlier.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: However, much of the time, a problem requires the data scientist to predict
    an outcome measure that is not continuous, but categorical. For example, in insurance,
    given certain features of a customer, what is the probability that this customer
    will not renew their policy? In this case, there is not a linear relationship
    between the features in the data and the outcome variable, so linear regression
    will falter. A viable option for conducting regression analysis on a categorical
    dependent variable is logistic regression.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression uses categorical and continuous variables to predict a categorical
    outcome. When the dependent variable of choice has two categorical outcomes, the
    analysis is termed binary logistic regression. However, if the outcome variable
    consists of more than two levels, the analysis is referred to as multinomial logistic
    regression. For the purposes of this chapter, we will focus our learning on the
    former.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: When predicting a binary outcome, we do not have a linear relationship between
    the features and the outcome variable; an assumption of linear regression. Thus,
    to express a nonlinear relationship in a linear way, we must transform the data
    using logarithmic transformation. As a result, logistic regression allows us to
    predict the probability of the binary outcome occurring given the feature(s) in
    the model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'For logistic regression with 1 predictor, the logistic regression equation
    is shown here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12: Logistic regression formula with 1 predictor'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_12.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.12: Logistic regression formula with 1 predictor'
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the preceding figure, *P(Y)* is the probability of the outcome occurring,
    *e* is the base of natural logarithms, *α* is the intercept, *β* is the beta coefficient,
    and *x* is the value of the predictor. This equation can be extended to multiple
    predictors using the formula here:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: Logistic regression formula with more than one predictor'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_13.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Logistic regression formula with more than one predictor'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thus, using logistic regression to model the probability of an event occurring
    is the same as fitting a linear regression model, except the continuous outcome
    variable has been replaced by the log odds (an alternate way of expressing probabilities)
    of success for a binary outcome variable. In linear regression, we assumed a linear
    relationship between the predictor variable(s) and the outcome variable. Logistic
    regression, on the other hand, assumes a linear relationship between the predictor
    variable(s) and the natural log of *p/(1-p)*, where *p* is the probability of
    the event occurring.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will use the `weather.csv` dataset to demonstrate
    building a logistic regression model to predict the probability of rain using
    all the features in our data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 25: Fitting a Logistic Regression Model and Determining the Intercept
    and Coefficients'
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To model the probability of rain (as opposed to snow) using all the features
    in our data, we will use the `weather.csv` file and store the dichotomous variable
    `Rain` as the outcome measure.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Import data using the following code:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Dummy code the `Description` variable as follows:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Shuffle `df_dummies` using the code here:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Split the features and outcome into `X` and `y`, respectively, as follows:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Split the features and outcome into training and testing data using the code
    here:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Instantiate a logistic regression model using this code:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Fit the logistic regression model to the training data using `model.fit(X_train,
    y_train`). We should get the following output:![Figure 3.14: The output from fitting
    a logistic regression model'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13322_03_14.jpg)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.14: The output from fitting a logistic regression model'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Get the intercept using the following:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Extract the coefficients using the following:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Place the coefficients into a list as follows:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Match features to their coefficients, place them in a DataFrame, and print
    the DataFrame to the console as follows:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Refer to the resultant output here:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.15: Features and their coefficients from the logistic regression
    model'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_15.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.15: Features and their coefficients from the logistic regression model'
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The coefficient for temperature can be interpreted as for every 1-degree increase
    in temperature, the log odds of rain increase by 5.69, controlling for all other
    features in the model. To generate predictions, we could convert the log odds
    to odds and the odds to probability. However, scikit-learn has functionality to
    generate predicted probability, as well as predicted classes.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 26: Generating Predictions and Evaluating the Performance of a Logistic
    Regression Model'
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Exercise 25*, we learned how to fit a logistic regression model and extract
    the elements necessary to generate predictions. However, scikit-learn makes our
    lives much easier by providing us with functions to predict the probability of
    an outcome, as well as the classes of an outcome. In this exercise, we will learn
    to generate predicted probabilities and classes, as well as evaluating a model
    performance using a confusion matrix and a classification report.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 25, perform the following steps:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate predicted probabilities using the following code:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Generate predicted classes using the following:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Evaluate a performance using a confusion matrix as follows:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Refer to the resultant output here:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.16: The confusion matrix from our logistic regression model'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_16.jpg)'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.16: The confusion matrix from our logistic regression model'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: From the confusion matrix, we can see that, of the 383 observations that were
    not classified as rainy, 377 of them were correctly classified, and of the 2917
    observations that were classified as rainy, 2907 of them were correctly classified.
    To further inspect our model's performance using metrics such as precision, recall,
    and f1-score, we will generate a classification report.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate a classification report using the following code:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Refer to the resultant output:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.17: The classification report generated from our logistic regression
    model'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_17.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.17: The classification report generated from our logistic regression
    model'
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see from our confusion matrix and classification report, our model
    is performing very well and may be difficult to improve upon. However, machine
    learning models including logistic regression consist of numerous hyperparameters
    that can be adjusted to further improve model performance. In the next exercise,
    we will learn to find the optimal combination of hyperparameters to maximize model
    performance.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 27: Tuning the Hyperparameters of a Multiple Logistic Regression Model'
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *step 7* of *Exercise 25*, we fit a logistic regression model and the subsequent
    output from that model is displayed in Figure 3.14\. Each of those arguments inside
    the `LogisticRegression()` function is set to a default hyperparameter. To tune
    the model, we will use scikit-learn's grid search function, which fits a model
    for every combination of possible hyperparameter values and determines the value
    for each hyperparameter resulting in the best model. In this exercise, we will
    learn how to use grid search to tune models.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 26*:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'The data has already been prepared for us (see Exercise 26); thus, we can jump
    right into instantiating a grid of possible hyperparameter values as follows:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Instantiate a grid search model to find the model with the greatest `f1` score
    (that is, the harmonic average of precision and recall) as follows:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Fit the model on the training using `model.fit(X_train, y_train)` (keep in
    mind, this may take a while) and find the resultant output here:![Figure 3.18:
    The output from our logistic regression grid search model'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13322_03_18.jpg)'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.18: The output from our logistic regression grid search model'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can return the optimal combination of hyperparameters as a dictionary as
    follows:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Refer to the resultant output here:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.19: The tuned hyperparameters from our logistic regression grid
    search model'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_19.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.19: The tuned hyperparameters from our logistic regression grid search
    model'
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We have found the combination of hyperparameters that maximizes the `f1` score.
    Remember, simply using the default hyperparameters in *Exercise 25* resulted in
    a model that performed very well on the test data. Thus, in the following activity,
    we will evaluate how the model with tuned hyperparameters performed on the test
    data.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6: Generating Predictions and Evaluating Performance of a Tuned Logistic
    Regression Model'
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the best combination of hyperparameters has been converged upon, we need
    to evaluate model performance much like we did in *Exercise 25*.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 27:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Generate the predicted probabilities of rain.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the predicted class of rain.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate performance with a confusion matrix and store it as a DataFrame.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print a classification report.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 346.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By tuning the hyperparameters of the logistic regression model, we were able
    to improve upon a logistic regression model that was already performing very well.
    We will continue to expand upon tuning different types of models in the following
    exercises and activities.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Max Margin Classification Using SVMs
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SVM is an algorithm for supervised learning that solves both classification
    and regression problems. However, SVM is most commonly used in classification
    problems, so, for the purposes of this chapter, we will focus on SVM as a binary
    classifier. The goal of SVM is to determine the best location of a hyperplane
    that create a class boundary between data points plotted on a multidimensional
    space. To help clarify this concept, refer to Figure 3.20.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20: Hyperplane (blue) separating the circles from the squares in
    three dimensions'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_20.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.20: Hyperplane (blue) separating the circles from the squares in three
    dimensions'
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Figure 3.20, the squares and circles are observations in the same DataFrame
    that represent different classes. In this figure, the hyperplane is depicted by
    a semi-transparent blue boundary lying between the circles and squares that separate
    the observations into two distinct classes. In this example, the observations
    are said to be linearly separable.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'The location of the hyperplane is determined by finding the position that creates
    the maximum separation (that is, margin) between the two classes. Thus, this is
    referred to as the **Maximum Margin Hyperplane** (MMH) and improves the likelihood
    that the points will remain on the correct side of the hyperplane boundary. It
    is possible to express the MMH using the points from each class that are closest
    to the MMH. These points are termed support vectors and each class has at least
    1\. Figure 3.21 visually depicts the support vectors in relation to the MMH in
    2 dimensions:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21: Support vectors in relation to the MMH'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_21.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.21: Support vectors in relation to the MMH'
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In reality, most data is not linearly separable. In this case, SVM makes use
    of a slack variable, which creates a soft margin (as opposed to a maximum margin),
    allowing some observations to fall on the incorrect side of the line. See the
    following plot:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22: 2 observations (as denoted with grey shading and the Greek letter
    Χi) fall on the incorrect side of the soft margin line'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_22.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.22: 2 observations (as denoted with grey shading and the Greek letter
    Χi) fall on the incorrect side of the soft margin line'
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A cost value is applied to the misclassified data points and, instead of finding
    the maximum margin, the algorithm minimizes the total cost. As the cost parameter
    increases, a harder SVM optimization will go for 100% separation and may overfit
    the training data. Conversely, lower cost parameters emphasize a wider margin
    and may underfit the training data. Thus, to create SVM models that perform well
    on the test data, it is important to determine a cost parameter that balances
    overfitting and underfitting.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, data that is not linearly separable can be transformed into a
    higher-dimension space using the kernel trick. After this mapping to a higher-dimensional
    space, a nonlinear relationship can appear linear. By transforming the original
    data, SVM can discover associations not explicitly apparent in the original features.
    scikit-learn uses the Gaussian RBF kernel by default, but comes equipped with
    common kernels such as linear, polynomial, and sigmoid as well. In order to maximize
    the performance of an SVM classifier model, the optimal combination of the kernel
    and cost function must be determined. Luckily, this can be easily achieved using
    grid search hyperparameter tuning, as introduced in Exercise 27\. In the following
    exercises and activities, we will learn how this feat is accomplished.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 28: Preparing Data for the Support Vector Classifier (SVC) Model'
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before fitting an SVM classifier model to predict a binary outcome variable;
    in this case, rain or snow, we must prepare our data. Since SVM is a black box,
    meaning the processes between input and output are not explicit, we do not need
    to worry about interpretability. Thus, we will transform the features in our data
    into z-scores prior to fitting the model. The following steps will show how to
    do this:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `weather.csv` using the following code:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Dummy code the categorical feature, `Description`, as follows:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Shuffle `df_dummies` to remove any ordering effects using the code here:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Split `df_shuffled` into `X` and `y` using the following code:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Split `X` and `y` into testing and training data using the code here:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To prevent any data leakage, scale `X_train` and `X_test` by fitting a scaler
    model to `X_train` and transforming them to z-scores separately, as follows:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now that our data has been properly divided into features and outcome variables,
    split into testing and training data, and scaled separately, we can tune the hyperparameters
    of our SVC model using a grid search.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 29: Tuning the SVC Model Using Grid Search'
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previously, we discussed the importance of determining the optimal cost function
    and kernel for SVM classifier models. In Exercise 27, we learned how to find the
    optimal combination of hyperparameters using scikit-learn's grid search function.
    In this exercise, we will demonstrate using grid search to find the best combination
    of the cost function and kernel.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 28*:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the grid for which to search using the following code:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Instantiate the `GridSearchCV` model with the `gamma` hyperparameter set to
    `auto` to avoid warnings, and set probability to `True` so we can extract probability
    of rain as follows:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Fit the grid search model using `model.fit(X_train_scaled, y_train)`:![Figure
    3.23: The output from fitting the SVC grid search model'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13322_03_23.jpg)'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.23: The output from fitting the SVC grid search model'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the best parameters using the following code:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'See the resultant output below:'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.24: Tuned hyperparameters for our SVC grid search model'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_24.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.24: Tuned hyperparameters for our SVC grid search model'
  id: totrans-293
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once the optimal combination of hyperparameters has been determined, it is time
    to generate predictions and subsequently evaluate how our model performed on the
    unseen test data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7: Generating Predictions and Evaluating the Performance of the SVC
    Grid Search Model'
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In previous exercises/activities, we learned to generate predictions and evaluate
    classifier model performance. In this activity we will, again, evaluate the performance
    of our model by generating predictions, creating a confusion matrix, and printing
    a classification report.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 29:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Extract the predicted classes.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create and print a confusion matrix.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate and print a classification report.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 348.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we demonstrated how to tune the hyperparameters of an SVC model using
    grid search. After tuning the SVC model, it did not perform as well as the tuned
    logistic regression model in predicting rain/snow. Additionally, SVC models are
    a **black box** in that they do not provide insight into the contribution of features
    on the outcome measure. In the upcoming *Decision Trees* section, we will introduce
    a different algorithm known as a decision tree, which uses a "*divide and conquer*"
    approach to generate predictions and offers a feature importance attribute for
    determining the importance of each feature on the outcome.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine we are considering changing jobs. We are weighing the pros and cons
    of prospective job opportunities and, after a few years of being in our current
    position, we start to realize the things that are important to us. However, not
    all aspects of a career are of equal importance. In fact, after being in the job
    for a few years, we decide that the most important aspect of a position is our
    interest in the projects we will be doing, followed by compensation, then work-related
    stress, trailed by commute time, and, lastly, benefits. We have just created the
    scaffolding of a cognitive decision tree. We can go into further detail by saying
    that we want a job where we are very interested in the allocated projects, paying
    at least $55k/year, with low work-related stress, a commute of under 30 minutes,
    and good dental insurance. Creating mental decision trees is a decision-making
    process we all utilize by nature and is one of the reasons why decision trees
    are one of the most widely used machine learning algorithms today.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, decision trees use either *gini* impurity or *entropy*
    information gain as the criterion to measure the quality of a split. First, the
    decision tree algorithm determines the feature that maximizes the value indicating
    quality of a split. This becomes referred to as the root node, as it is the most
    important feature in the data. In the job offer mentioned earlier, being very
    interested in the prospective projects would be considered the root node. Taking
    into consideration the root node, the job opportunities are divided into those
    with very interesting projects and those without very interesting projects.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Next, each of these two categories are divided into the next most important
    feature, given the previous feature(s), and so on and so forth, until the potential
    jobs are identified as being of interest or not.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is termed recursive partitioning, or "*divide and conquer*",
    because it continues the process of splitting and subsetting the data until the
    algorithm determines the subsets in the data as sufficiently homogenous, or:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Nearly all the observations at the corresponding node have the same class (that
    is, purity).
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no further features in the data for which to split.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tree has reached the size limit decided upon a priori.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if purity is determined by entropy, we must understand that entropy
    is a measure of randomness within a set of values. Decision trees operate by choosing
    the splits that minimize entropy (randomness) and, in turn, maximize information
    gain. Information gain is calculated as the difference in entropy between the
    split and all other following splits. The total entropy is then computed by taking
    the sum of the entropy in each partition, weighted by the proportion of observations
    in the partition. Luckily, scikit-learn provides us with a function that does
    all of this for us. In the following exercises and activities, we will implement
    the decision tree classifier model to predict whether it is raining or snowing,
    using the familiar `weather.csv` dataset.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 8: Preparing Data for a Decision Tree Classifier'
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will prepare our data for a decision tree classifier model.
    Perform the following steps to complete the activity:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Import `weather.csv` and store it as a DataFrame
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dummy code the multi-level, categorical feature `Summary`
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shuffle the data to remove any possible order effects
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into features and outcome
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further divide the features and outcome into testing and training data
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scale `X_train` and `X_test` using the following code:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Note
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 349
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following exercise, we will learn to tune and fit a decision tree classifier
    model..
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 30: Tuning a Decision Tree Classifier Using Grid Search'
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the current exercise, we will instantiate a hyperparameter space and tune
    the hyperparameters of a decision tree classifier using a grid search.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Activity 8*, perform the following steps:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the hyperparameter space as follows:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Instantiate a grid search model using the code here:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Fit to the training set using the following:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'See the resultant output displayed here:'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.25: The output from fitting our decision tree classifier grid search
    model'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_25.jpg)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.25: The output from fitting our decision tree classifier grid search
    model'
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the tuned parameters:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'See the resultant output below:'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13322_03_26.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.26: The tuned hyperparameters for our decision tree classifier grid
    search model'
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see from Figure 3.26 that it used **gini** impurity as the criterion
    to measure the quality of a split. Further explanations of the hyperparameters
    are outside the scope of this chapter but can be found in the decision tree classifier
    scikit-learn documentation.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Remember, in practice, it is common for decision makers to ask how various features
    are affecting the predictions. In linear and logistic regression, the intercept
    and coefficient(s) make model predictions very transparent.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Decision trees can also be very easy to interpret, as we can see where the decisions
    were made, but this requires an installation and proper configuration of Graphviz,
    as well as unscaled features.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Instead of plotting the tree in the following exercise, we will explore an attribute
    found in scitkit-learn's tree-based model algorithms, '`feature_importances_`',
    which returns an array containing values of relative feature importance for each
    feature. It is important to note that this attribute is unavailable from a grid
    search model. As a result, in the next exercise, we will learn to programmatically
    extract values from the `best_parameters` dictionary and re-fit the tuned decision
    tree model, allowing us to access the attributes provided by the decision tree
    classifier function.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 31: Programmatically Extracting Tuned Hyperparameters from a Decision
    Tree Classifier Grid Search Model'
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous exercise, we saved the tuned hyperparameters as key value pairs
    in the `best_parameters` dictionary. This allows us to programmatically access
    the values and assign them to the appropriate hyperparameters of a decision tree
    classifier model. By fitting the tuned decision tree model, we will be able to
    access the attributes made available from the scikit-learn decision tree classifier
    function.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 30*, perform the following steps:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'Prove that we can access the value for ''`Tree_criterion`'' using:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'See the resultant output here:'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.27: The value assigned to the ‘Tree_criterion’ key in the best_parameters
    dictionary'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_27.jpg)'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.27: The value assigned to the ''Tree_criterion'' key in the best_parameters
    dictionary'
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Instantiate decision tree classifier model and assign the values to the corresponding
    hyperparameters as follows:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Fit the grid search model to the scaled training data using the following:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![Figure 3.28: The output from fitting the decision tree classifier model with
    tuned hyperparameters'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_28.jpg)'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.28: The output from fitting the decision tree classifier model with
    tuned hyperparameters'
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Extract `feature_importances` attribute using:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![Figure 3.29: An array of feature importance from our tuned decision tree
    classifier model'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_29.jpg)'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.29: An array of feature importance from our tuned decision tree classifier
    model'
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: From the array in Figure 3.29, we can see that the first feature completely
    dominated the other variables in terms of feature importance.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize this using the following code:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'See the resultant output here:'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.30: Feature importance from a tuned decision tree classifier model'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_30.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.30: Feature importance from a tuned decision tree classifier model'
  id: totrans-375
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It looks like temperature in Celsius was the sole driver in this classification
    problem. With the outcome measure being `rain ('Rain'=1)` or `snow ('Rain'=0)`
    and the way in which decision trees make split decisions via "*divide and conquer*,"
    it makes sense that the algorithm used temperature to determine if there was rainfall
    or snowfall at the time of measurement. In the upcoming activity, we will evaluate
    how the model performed.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9: Generating Predictions and Evaluating the Performance of a Decision
    Tree Classifier Model'
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have generated predictions and evaluated the model performance in previous
    exercises and activities. We will be taking the same approach in this activity
    to evaluate the performance of our tuned decision tree classifier model.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 31, perform the following steps:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Generate the predicted probabilities of rain.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the predicted classes of rain.
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate and print a confusion matrix.
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print a classification report.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 350.
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You should find that there was only one misclassified observation. Thus, by
    tuning a decision tree classifier model on our `weather.csv` dataset, we were
    able to predict rain (or snow) with great accuracy. We can see that the sole driving
    feature was temperature in Celsius. This makes sense due to the way in which decision
    trees use recursive partitioning to make predictions.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, after evaluation, a single model is a weak learner and does not perform
    well. However, by combining weak learners, we create a stronger learner. The approach
    of combining numerous weak learners to create a stronger learner is termed ensemble.
    Random forest models combine numerous decision tree models to create a stronger
    ensemble model. Random forests can be used for classification or regression problems.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As briefly mentioned earlier, random forests are ensembles of decision trees
    that can be used to solve classification or regression problems. Random forests
    use a small portion of the data to fit each tree, so they can handle very large
    datasets, and they are less prone to the "*curse of dimensionality*" relative
    to other algorithms. The curse of dimensionality is a situation in which an abundance
    of features in the data diminishes the performance of the model. Predictions of
    the random forest are then determined by combining the predictions of each tree.
    Like SVM, random forests are a **black box** with inputs and outputs which cannot
    be interpreted.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming exercises and activities, we will tune and fit a random forest
    regressor using grid search to predict the temperature in Celsius. Then, we will
    evaluate the performance of the model.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 32: Preparing Data for a Random Forest Regressor'
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will prepare the data for the random forest regressor with ''`Temperature_c`''
    as the dependent variable, just as we did in Exercise 21:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'Import ''`weather.csv`'' and save it as `df` using the following code:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Dummy code the multi-class, categorical variable, Description, as follows:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Remove any possible ordering effects by shuffling `df_dummies` using the following
    code:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Split `df_shuffled` into `X` and `y` using the following code:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Split `X` and `y` into testing and training data as follows:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Scale `X_train` and `X_test` using the code here:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now that we have imported, shuffled, separated our data into features (`X`)
    and dependent variable (`y`), split `X` and `y` into testing and training data,
    and scaled `X_train` and `X_test`, we will tune a random forest regressor model
    using grid search.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10: Tuning a Random Forest Regressor'
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data has been prepared for inclusion in a random forest regressor. Now,
    we must set up the hyperparameter space and find the optimal combination of hyperparameters
    using a grid search.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 32, perform the following steps:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Specify the hyperparameter space.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate the `GridSearchCV` model optimizing the explained variance.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the grid search model to the training set.
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the tuned parameters.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 351.
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After performing a grid search of our random forest regressor hyperparameters,
    we need to fit a random forest regressor model with the tuned hyperparameters.
    We will programmatically extract the values in the `best_parameters` dictionary
    and assign them to the corresponding hyperparameters in the random forest regressor
    function, so we can access the attributes from the random forest regressor function.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 33: Programmatically Extracting Tuned Hyperparameters and Determining
    Feature Importance from a Random Forest Regressor Grid Search Model'
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By extracting the value from the key-value pairs in the `best_parameters` dictionary,
    we eliminate the possibility of manual errors, as well as make our code more automated.
    In this exercise, we will replicate the steps from *Exercise 31*, but will adapt
    our code for the random forest regressor model.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Activity 10*, perform the following steps:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a random forest regressor model with the values for each key from
    the `best_parameters` dictionary assigned to the corresponding hyperparameter:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Fit the model on the training data using the following:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Find the resultant output here:'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.31: The output from fitting the random forest regressor model with
    tuned hyperparameters'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_31.jpg)'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.31: The output from fitting the random forest regressor model with
    tuned hyperparameters'
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot feature importance in descending order using the following code:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'See the resultant output here:'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.32: Feature importance from a random forest regressor model with
    tuned hyperparameters'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_32.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.32: Feature importance from a random forest regressor model with tuned
    hyperparameters'
  id: totrans-432
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From Figure 3.32, we can see that the '`Description_Warm`' dummy variable and
    '`Humidity`' are the main drivers of temperature in Celsius. Meanwhile, '`Visibility_km`'
    and '`Wind_Bearing_degrees`' have a small effect on the temperature. Let's now
    check to see how our model performs on the test data.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 11: Generating Predictions and Evaluating the Performance of a Tuned
    Random Forest Regressor Model'
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Exercise 23* and *Activity 5*, we learned to generate predictions and evaluate
    the performance of regression models that predict a continuous outcome. In this
    activity, we will be taking the same approach to evaluate the performance of our
    random forest regressor model to predict temperature in Celsius.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 33*, perform the following steps:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Generate predictions on the test data.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the correlation of predicted and actual values.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the distribution of residuals.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute metrics, then place them in a DataFrame and print it.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 352.
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The random forest regressor model seems to underperform compared to the multiple
    linear regression, as evidenced by greater MAE, MSE, and RMSE values, as well
    as less explained variance. Additionally, there was a weaker correlation between
    the predicted and actual values, and the residuals were further from being normally
    distributed. Nevertheless, by leveraging ensemble methods using a random forest
    regressor, we constructed a model that explains 75.8% of the variance in -temperature
    and predicts temperature in Celsius ± 3.781 degrees.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to the open source machine learning library
    for Python, scikit-learn. You learned to preprocess data, as well as how to tune
    and fit a few different regression and classification algorithms. Lastly, you
    learned how to quickly and effectively evaluate the performance of classification
    and regression models. This was a very comprehensive introduction to the scikit-learn
    library, and the strategies employed here can be applied to building numerous
    additional algorithms provided by scikit-learn.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about dimensionality reduction and unsupervised
    learning.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
