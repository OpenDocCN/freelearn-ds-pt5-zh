<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 3. Correlation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch03" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 3. Correlation</h1></div></div></div><div class="calibre2"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote1" summary="Block quote"><tr class="calibre17"><td class="calibre18"> </td><td class="calibre18"><p class="calibre19"><span class="strong1"><em class="calibre13">"The more I learn about people, the better I like my dog."</em></span></p></td><td class="calibre18"> </td></tr><tr class="calibre17"><td class="calibre18"> </td><td colspan="2" class="calibre20">--<span class="strong1"><span class="strong1"><em class="calibre13">Mark Twain</em></span></span></td></tr></table></div><p class="calibre11">In previous chapters, we've considered how to describe samples in terms of summary statistics and how population parameters can be inferred from them. Such analysis tells us something about a population in general and a sample in particular, but it doesn't allow us to make very precise statements about individual elements. This is because so much information has been lost by reducing the data to just two statistics: the mean and standard deviation.</p><p class="calibre11">We often want to go further and establish a relationship between two or more variables or to predict one variable given another. This takes us into the study of correlation and regression. Correlation concerns the strength and direction of the relationship between two or more variables. Regression determines the nature of this relationship and enables us to make predictions from it.</p><p class="calibre11">Linear regression is our first machine learning algorithm. Given a sample of data, our model will learn a linear equation that allows it to make predictions about new, unseen data. To do this, we'll return to Incanter and study the relationship between height and weight for Olympic athletes. We'll introduce the concept of matrices and show how Incanter can be used to manipulate them.</p><div class="calibre2" title="About the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec59" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>About the data</h1></div></div></div><p class="calibre11">This chapter will <a id="id247" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>make use of data on athletes in the London 2012 Olympic Games, courtesy of Guardian News and Media Ltd. The data was originally sourced <a id="id248" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>from the Guardian's excellent data blog at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.theguardian.com/data">http://www.theguardian.com/data</a>.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note28" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Download the example code for this chapter from the publisher's website or from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clojuredatascience/ch3-correlation">https://github.com/clojuredatascience/ch3-correlation</a>.</p></div></div><p class="calibre11">Consult the <code class="literal">Readme</code> <a id="id249" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>file in this chapter's sample code or the book's wiki at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://wiki.clojuredatascience.com">http://wiki.clojuredatascience.com</a> for more information on the data.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Inspecting the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec60" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Inspecting the data</h1></div></div></div><p class="calibre11">The first task when <a id="id250" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>confronted with a new dataset is to study it to ensure that we understand what it contains.</p><p class="calibre11">The <code class="literal">all-london-2012-athletes.xlsx</code> file is small enough that it's been provided with the sample code for this chapter. We can inspect the data with Incanter, as we did in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Statistics">Chapter 1</a>, <span class="strong1"><em class="calibre13">Statistics</em></span> using the <code class="literal">incanter.excel/read-xls</code> and <code class="literal">incanter.core/view</code> functions:</p><div class="calibre2"><pre class="programlisting">(ns cljds.ch3.examples
  (:require [incanter.charts :as c]
            [incanter.core :as i]
            [incanter.excel :as xls]
            [incanter.stats :as s]))

(defn athlete-data []
  (-&gt; (io/resource "all-london-2012-athletes.xlsx")
      (str)
      (xls/read-xls)))

(defn ex-3-1 []
  (i/view (athlete-data)))</pre></div><p class="calibre11">If you run this code (either in the REPL or on the command line with <code class="literal">lein run –e 3.1</code>), you should see the following output:</p><div class="mediaobject"><img src="Images/7180OS_03_100.jpg" alt="Inspecting the data" class="calibre80"/></div><p class="calibre11">We're fortunate that the <a id="id251" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data is clearly labeled in the columns and contains the following information:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Name of the athlete</li><li class="listitem">Country for which they are competing</li><li class="listitem">Age in years</li><li class="listitem">Height in centimeters</li><li class="listitem">Weight in kilograms</li><li class="listitem">Sex as the string "M" or "F"</li><li class="listitem">Date of birth as a string</li><li class="listitem">Place of birth as a string (with country)</li><li class="listitem">Gold medals won</li><li class="listitem">Silver medals won</li><li class="listitem">Bronze medals won</li><li class="listitem">Total gold, silver, and bronze medals won</li><li class="listitem">Sport in which they competed</li><li class="listitem">Event as a comma-separated list</li></ul></div><p class="calibre11">Even though the data is clearly labeled, gaps are evident in the data for height, weight, and place of birth. We'll have to be careful to make sure these don't trip us up.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Visualizing the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec61" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Visualizing the data</h1></div></div></div><p class="calibre11">First, we'll consider the <a id="id252" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>spread of the heights of the London 2012 athletes. Let's plot our height values as a histogram to see how the data is distributed, remembering to filter the nil values first:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-2 []
  (-&gt; (remove nil? (i/$ "Height, cm" (athlete-data)))
      (c/histogram :nbins 20
                   :x-label "Height, cm"
                   :y-label "Frequency")
      (i/view)))</pre></div><p class="calibre11">This code generates the following histogram:</p><div class="mediaobject"><img src="Images/7180OS_03_110.jpg" alt="Visualizing the data" class="calibre45"/></div><p class="calibre11">The data is approximately normally distributed, as we have come to expect. The mean height of our athletes is around 177 cm. Let's take a look at the distribution of weights of swimmers from the 2012 <a id="id253" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Olympics:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-3 []
  (-&gt; (remove nil? (i/$ "Weight" (athlete-data)))
      (c/histogram :nbins 20
                   :x-label "Weight"
                   :y-label "Frequency")
      (i/view)))</pre></div><p class="calibre11">This code generates the following histogram:</p><div class="mediaobject"><img src="Images/7180OS_03_120.jpg" alt="Visualizing the data" class="calibre45"/></div><p class="calibre11">This data shows a pronounced skew. The tail is much longer to the right of the peak than to the left, so we say the skew is positive. We can quantify the skewness of the data with Incanter's <code class="literal">incanter.stats/skewness</code> function:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-4 []
  (-&gt;&gt; (swimmer-data)
       (i/$ "Weight")
       (remove nil?) 
       (s/skewness)))
;; 0.238</pre></div><p class="calibre11">Fortunately, this skew can <a id="id254" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>be effectively mitigated by taking the logarithm of the weight using Incanter's <code class="literal">incanter.core/log</code> function:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-5 []
  (-&gt; (remove nil? (i/$ "Weight" (athlete-data)))
      (i/log)
      (c/histogram :nbins 20
                   :x-label "log(Weight)"
                   :y-label "Frequency")
      (i/view)))</pre></div><p class="calibre11">This code results in the following histogram:</p><div class="mediaobject"><img src="Images/7180OS_03_130.jpg" alt="Visualizing the data" class="calibre45"/></div><p class="calibre11">This is much closer to the <a id="id255" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>normal distribution. This suggests that weight is distributed according to a <span class="strong1"><strong class="calibre12">log-normal distribution</strong></span>.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The log-normal distribution"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec62" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The log-normal distribution</h1></div></div></div><p class="calibre11">The log-normal <a id="id256" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>distribution is simply the distribution of a set of values whose logarithm is normally distributed. The base of the logarithm can be any positive number except for one. Like the normal distribution, the log-normal distribution is important in the description of many naturally occurring phenomena.</p><p class="calibre11">A logarithm represents the power to which a fixed number (the base) must be raised to produce a given number. By plotting the logarithms as a histogram, we've shown that these powers are approximately normally distributed. Logarithms are usually taken to base 10 or base <span class="strong1"><em class="calibre13">e</em></span>: the <a id="id257" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>transcendental number that's equal to approximately 2.718. Incanter's <code class="literal">log</code> function and its inverse <code class="literal">exp</code> both use base <span class="strong1"><em class="calibre13">e</em></span>. <span class="strong1"><em class="calibre13">log<sub class="calibre25">e</sub></em></span> is also called the <span class="strong1"><strong class="calibre12">natural logarithm</strong></span> or <span class="strong1"><em class="calibre13">ln</em></span>, because of the properties that make it particularly suitable in calculus.</p><p class="calibre11">The log-normal distribution tends to occur in processes of growth where the growth rate is independent of size. This is known as <span class="strong1"><em class="calibre13">Gibrat's law</em></span> and was formally defined in 1931 by Robert Gibrat, who<a id="id258" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> noticed that it applied to the growth of firms. Since the growth rate is a proportion of the size, larger firms tend to grow more quickly than smaller firms.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note29" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The normal distribution occurs in situations where many small variations have an additive effect, whereas the log-normal distribution occurs in situations where many small variations have a multiplicative effect.</p></div></div><p class="calibre11">Gibrat's law has since been found to be applicable to lots of situations, including the sizes of cities and, according to Wolfram MathWorld, the numbers of words in sentences by George Bernard Shaw.</p><p class="calibre11">For the rest of this chapter, we'll be using the natural logarithm of the weight data so that our data is approximately normally distributed. We'll choose a population of athletes with roughly similar body types, say Olympic swimmers.</p><div class="calibre2" title="Visualizing correlation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec36" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Visualizing correlation</h2></div></div></div><p class="calibre11">One of the quickest <a id="id259" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and simplest ways of determining if two variables are correlated is to view them on a scatter plot. We'll filter our data to select only swimmers and then plot the heights against the weights:</p><div class="calibre2"><pre class="programlisting">(defn swimmer-data []
  (-&gt;&gt; (athlete-data)
       (i/$where {"Height, cm" {:$ne nil} "Weight" {:$ne nil}
                  "Sport" {:$eq "Swimming"}})))
(defn ex-3-6 []
  (let [data (swimmer-data)
        heights (i/$ "Height, cm" data)
        weights (i/log (i/$ "Weight" data))]
    (-&gt; (c/scatter-plot heights weights
                        :x-label "Height, cm"
                        :y-label "Weight")
        (i/view))))</pre></div><p class="calibre11">This code yields the following plot:</p><div class="mediaobject"><img src="Images/7180OS_03_140.jpg" alt="Visualizing correlation" class="calibre45"/></div><p class="calibre11">The output clearly <a id="id260" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>shows a relationship between the two variables. The chart has the characteristically skewed elliptical shape of two correlated, normally distributed variables centered on the means. The following diagram compares the scatter plot against probability distributions of the height and log weight:</p><div class="mediaobject"><img src="Images/7180OS_03_150.jpg" alt="Visualizing correlation" class="calibre81"/></div><p class="calibre11">Points close to the tail of one distribution also tend to be close to the same tail of the other distribution, and vice versa. Thus, there is a relationship between the two distributions that we'll show how to quantify over the next several sections. If we look closely at the previous scatter plot though, we'll see that the points are packed into columns and rows due to the measurements being rounded (to centimeters and kilograms for height and weight, respectively). Where this occurs, it is sometimes preferable to <span class="strong1"><em class="calibre13">jitter</em></span> the data to make the strength of the relationship <a id="id261" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>clearer. Without jittering, it could be that what appears to be one point is actually many points that share exactly the same pair of values. Introducing some random noise makes this possibility less likely.</p></div><div class="calibre2" title="Jittering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec37" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Jittering</h2></div></div></div><p class="calibre11">Since each value is <a id="id262" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>rounded to the nearest centimeter, a value captured as 180 could actually have been anywhere between 179.5 cm and 180.5 cm. To unwind this effect, we can add random noise in the -0.5 to 0.5 range to each of the height data points.</p><p class="calibre11">The weight data point was captured to the nearest kilogram, so a value of 80 could actually have been anywhere between 79.5 kg and 80.5 kg. We can add random noise in the same range to unwind this effect (though clearly, this must be done before we take the logarithm):</p><div class="calibre2"><pre class="programlisting">(defn jitter [limit]
  (fn [x]
    (let [amount (- (rand (* 2 limit)) limit)]
      (+ x amount))))

(defn ex-3-7 []
  (let [data (swimmer-data)
        heights (-&gt;&gt; (i/$ "Height, cm" data)
                     (map (jitter 0.5)))
        weights (-&gt;&gt; (i/$ "Weight" data)
                     (map (jitter 0.5))
                     (i/log))]
    (-&gt; (c/scatter-plot heights weights
                        :x-label "Height, cm"
                        :y-label "Weight")
        (i/view))))</pre></div><p class="calibre11">The jittered <a id="id263" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>graph appears as follows:</p><div class="mediaobject"><img src="Images/7180OS_03_160.jpg" alt="Jittering" class="calibre45"/></div><p class="calibre11">As with introducing <a id="id264" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>transparency to the scatter plot in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Statistics">Chapter 1</a>, <span class="strong1"><em class="calibre13">Statistics</em></span>, jittering is a mechanism to ensure that we don't let incidental factors—such as data volume or rounding artifacts—obscure our ability to see patterns in the data.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Covariance"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec63" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Covariance</h1></div></div></div><p class="calibre11">One way of quantifying <a id="id265" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the strength of the relationship between two variables is their covariance. This measures the tendency of two variables to change together.</p><p class="calibre11">If we have two series, <span class="strong1"><em class="calibre13">X</em></span> and <span class="strong1"><em class="calibre13">Y</em></span>, their deviations from the mean are:</p><div class="mediaobject"><img src="Images/7180OS_03_01.jpg" alt="Covariance" class="calibre82"/></div><div class="mediaobject"><img src="Images/7180OS_03_02.jpg" alt="Covariance" class="calibre83"/></div><p class="calibre11">Where <span class="strong1"><em class="calibre13">x<sub class="calibre25">i</sub></em></span> is the value of <span class="strong1"><em class="calibre13">X</em></span> at index <span class="strong1"><em class="calibre13">i</em></span>, <span class="strong1"><em class="calibre13">y<sub class="calibre25">i</sub></em></span> is the value of <span class="strong1"><em class="calibre13">Y</em></span> at index <span class="strong1"><em class="calibre13">i</em></span>, <span class="inlinemediaobject"><img src="Images/7180OS_03_03.jpg" alt="Covariance" class="calibre24"/></span> is the mean of <span class="strong1"><em class="calibre13">X</em></span>, and <span class="inlinemediaobject"><img src="Images/7180OS_03_04.jpg" alt="Covariance" class="calibre84"/></span> is the mean of <span class="strong1"><em class="calibre13">Y</em></span>. If <span class="strong1"><em class="calibre13">X</em></span> and <span class="strong1"><em class="calibre13">Y</em></span> tend to vary together, their deviations from the mean tend to have the same sign: negative if they're less than the mean, positive if they're greater. If we multiply them together, the product is positive when they have the same sign and negative when they have <a id="id266" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>different signs. Adding up the products gives a measure of the tendency of the two variables to deviate from the mean in the same direction for each given sample.</p><p class="calibre11">Covariance is defined as the mean of these products:</p><div class="mediaobject"><img src="Images/7180OS_03_05.jpg" alt="Covariance" class="calibre85"/></div><p class="calibre11">Covariance can be calculated in Clojure using the following code:</p><div class="calibre2"><pre class="programlisting">(defn covariance [xs ys]
  (let [x-bar (s/mean xs)
        y-bar (s/mean xs)
        dx (map (fn [x] (- x x-bar)) xs)
        dy (map (fn [y] (- y y-bar)) ys)]
    (s/mean (map * dx dy))))</pre></div><p class="calibre11">Alternatively, we could use the <code class="literal">incanter.stats/covariance</code> function. The covariance of height and log-weight for our Olympic swimmers is <code class="literal">1.354</code>, but this is a hard number to interpret. The units are the product of the units of the inputs.</p><p class="calibre11">Because of this, covariance is rarely reported as a summary statistic on its own. A solution to make the number more comprehensible is to divide the deviations by the product of the standard deviations. This transforms the units to standard scores and constrains the output to a <a id="id267" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number between <code class="literal">-1</code> and <code class="literal">+1</code>. The result is called <span class="strong1"><strong class="calibre12">Pearson's correlation</strong></span>.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Pearson's correlation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec64" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Pearson's correlation</h1></div></div></div><p class="calibre11">Pearson's <a id="id268" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>correlation is often given the variable name <span class="strong1"><em class="calibre13">r</em></span> and is calculated in the following way, where <span class="strong1"><em class="calibre13">dx<sub class="calibre25">i</sub></em></span> and <span class="strong1"><em class="calibre13">dy<sub class="calibre25">i</sub></em></span> are calculated as before:</p><div class="mediaobject"><img src="Images/7180OS_03_06.jpg" alt="Pearson's correlation" class="calibre86"/></div><p class="calibre11">Since the standard deviations are constant values for the variables <span class="strong1"><em class="calibre13">X</em></span> and <span class="strong1"><em class="calibre13">Y</em></span> the equation can be simplified to the following, where <span class="strong1"><em class="calibre13">σ<sub class="calibre25">x</sub></em></span> and <span class="strong1"><em class="calibre13">σ<sub class="calibre25">y</sub></em></span> are the standard deviations of <span class="strong1"><em class="calibre13">X</em></span> and <span class="strong1"><em class="calibre13">Y</em></span> respectively:</p><div class="mediaobject"><img src="Images/7180OS_03_07.jpg" alt="Pearson's correlation" class="calibre87"/></div><p class="calibre11">This is sometimes referred to as Pearson's product-moment correlation coefficient or simply just the <span class="strong1"><em class="calibre13">correlation coefficient</em></span> and is usually denoted by the letter <span class="strong1"><em class="calibre13">r</em></span>.</p><p class="calibre11">We have previously written functions to calculate the standard deviation. Combining with our function to calculate covariance yields the following implementation of Pearson's correlation:</p><div class="calibre2"><pre class="programlisting">(defn correlation [x y]
  (/ (covariance x y)
     (* (standard-deviation x)
        (standard-deviation y))))</pre></div><p class="calibre11">Alternately, we can make use of the <code class="literal">incanter.stats/correlation</code> function.</p><p class="calibre11">Because standard scores are dimensionless, so is <span class="strong1"><em class="calibre13">r</em></span>. If <span class="strong1"><em class="calibre13">r</em></span> is -1.0 or 1.0, the variables are perfectly negatively or perfectly positively correlated.</p><p class="calibre11">If <span class="strong1"><em class="calibre13">r</em></span> is zero though, it doesn't necessarily follow that the variables are uncorrelated. Pearson's correlation only measures linear relationships. There could still be some nonlinear relationship between variables that isn't captured by <span class="strong1"><em class="calibre13">r</em></span>, as demonstrated by the following plots:</p><div class="mediaobject"><img src="Images/7180OS_03_170.jpg" alt="Pearson's correlation" class="calibre88"/></div><p class="calibre11">Note that the correlation of the central example is undefined because the standard deviation of <span class="strong1"><em class="calibre13">y</em></span> is zero. Since our equation for <span class="strong1"><em class="calibre13">r</em></span> would involve dividing the covariance  by zero, the result is meaningless. In this case, there can't be any correlation between the variables; the value for <span class="strong1"><em class="calibre13">y</em></span> is always <a id="id269" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the mean. A simple inspection of standard deviations would confirm this.</p><p class="calibre11">The correlation coefficient can be calculated for the height and log-weight data for our swimmers:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-8 []
  (let [data (swimmer-data)
        heights (i/$ "Height, cm" data)
        weights (i/log (i/$ "Weight" data))]
    (correlation heights weights)))</pre></div><p class="calibre11">This yields the answer <code class="literal">0.867</code>, which quantifies the strong, positive correlation we already observed on the scatter plot.</p><div class="calibre2" title="Sample r and population rho"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec38" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Sample r and population rho</h2></div></div></div><p class="calibre11">Like the mean or <a id="id270" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>standard deviation, the correlation coefficient is a statistic. It describes a sample; in this case, a sample of paired values: height and weight. While our known sample correlation coefficient is given the letter <span class="strong1"><em class="calibre13">r</em></span>, the unknown population correlation coefficient is given the Greek letter rho: <span class="inlinemediaobject"><img src="Images/7180OS_03_08.jpg" alt="Sample r and population rho" class="calibre89"/></span>.</p><p class="calibre11">As we discovered in the last chapter, we should not assume that what we measured in our sample applies to the population as a whole. In this case, our population might be all swimmers from all recent Olympic Games. It would not be appropriate to generalize, for example, to other Olympic sports such as weightlifting or to noncompetitive swimmers.</p><p class="calibre11">Even within an <a id="id271" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>appropriate population—such as swimmers from the recent Olympic Games—our sample is just one of many potential samples of different correlation coefficients. How far we can trust our <span class="strong1"><em class="calibre13">r</em></span> as an estimate of <span class="inlinemediaobject"><img src="Images/7180OS_03_08.jpg" alt="Sample r and population rho" class="calibre89"/></span> will depend on two factors:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The size of the sample</li><li class="listitem">The magnitude of <span class="strong1"><em class="calibre13">r</em></span></li></ul></div><p class="calibre11">Clearly, for a fair sample, the larger it is the more we can trust it to be a representative of the population as a whole. It may not be intuitively obvious to you that the magnitude of <span class="strong1"><em class="calibre13">r</em></span> also affects how confident we can be of it representing <span class="inlinemediaobject"><img src="Images/7180OS_03_08.jpg" alt="Sample r and population rho" class="calibre89"/></span>. The reason is that large coefficients are less likely to have arisen by chance or by random sampling error.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Hypothesis testing"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec65" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Hypothesis testing</h1></div></div></div><p class="calibre11">In the previous chapter, we <a id="id272" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>introduced hypothesis testing as a means to quantify the probability that a given hypothesis (such as that the two samples were from a single population) is true. We will use the same process to quantify the probability that a correlation exists in the wider population based on our sample.</p><p class="calibre11">First, we must formulate two hypotheses, a null hypothesis and an alternate hypothesis:</p><div class="mediaobject"><img src="Images/7180OS_03_09.jpg" alt="Hypothesis testing" class="calibre90"/></div><div class="mediaobject"><img src="Images/7180OS_03_10.jpg" alt="Hypothesis testing" class="calibre91"/></div><p class="calibre11">
<span class="strong1"><em class="calibre13">H<sub class="calibre25">0</sub></em></span> is the hypothesis that the population correlation is zero. In other words, our conservative view is that the measured correlation is purely due to chance sampling error.</p><p class="calibre11">
<span class="strong1"><em class="calibre13">H<sub class="calibre25">1</sub></em></span> is the alternative possibility that the population correlation is not zero. Notice that we don't specify the direction of the correlation, only that there is one. This means we are performing a two-tailed test.</p><p class="calibre11">The standard error of the sample <span class="strong1"><em class="calibre13">r</em></span> is given by:</p><div class="mediaobject"><img src="Images/7180OS_03_11.jpg" alt="Hypothesis testing" class="calibre92"/></div><p class="calibre11">This formula is only <a id="id273" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>accurate when <span class="inlinemediaobject"><img src="Images/7180OS_03_08.jpg" alt="Hypothesis testing" class="calibre89"/></span> is close to zero (recall that the magnitude of <span class="strong1"><em class="calibre13">r</em></span> influences our confidence), but fortunately, this is exactly what we're assuming under our null hypothesis.</p><p class="calibre11">Once again, we can make use of the <span class="strong1"><em class="calibre13">t</em></span>-distribution and calculate our <span class="strong1"><em class="calibre13">t</em></span>-statistic:</p><div class="mediaobject"><img src="Images/7180OS_03_12.jpg" alt="Hypothesis testing" class="calibre93"/></div><p class="calibre11">The term <span class="strong1"><em class="calibre13">df</em></span> is the degree of freedom of our data. For correlation testing, the degree of freedom is <span class="strong1"><em class="calibre13">n - 2</em></span> where <span class="strong1"><em class="calibre13">n</em></span> is the size of the sample. Putting this value into the formula, we obtain:</p><div class="mediaobject"><img src="Images/7180OS_03_13.jpg" alt="Hypothesis testing" class="calibre94"/></div><p class="calibre11">This gives us a <span class="strong1"><em class="calibre13">t</em></span>-value of <code class="literal">102.21</code>. To convert this into a <span class="strong1"><em class="calibre13">p</em></span> value, we need to refer to the <span class="strong1"><em class="calibre13">t</em></span>-distribution. Incanter provides the <span class="strong1"><strong class="calibre12">cumulative distribution function </strong></span>(<span class="strong1"><strong class="calibre12">CDF</strong></span>) for the <span class="strong1"><em class="calibre13">t</em></span>-distribution with <a id="id274" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the <code class="literal">incanter.stats/cdf-t</code> function. The value of the CDF corresponds to the <span class="strong1"><em class="calibre13">p</em></span>-value for a one-tailed test. We multiply the value by two because we're performing a two-tailed test:</p><div class="calibre2"><pre class="programlisting">(defn t-statistic [x y]
  (let [r (correlation x y)
        r-square (* r r)
        df (- (count x) 2)]
    (/ (* r df)
       (i/sqrt (- 1 r-square)))))

(defn ex-3-9 []
  (let [data (swimmer-data)
        heights (i/$ "Height, cm" data)
        weights (i/log (i/$ "Weight" data))
        t-value (t-statistic heights weights)
        df (- (count heights) 2)
        p  (* 2 (s/cdf-t t-value :df df :lower-tail? false))]
    (println "t-value" t-value)
    (println "p value " p)))</pre></div><p class="calibre11">The <span class="strong1"><em class="calibre13">p</em></span>-value is so small as to be essentially zero, meaning that the chances of the null hypothesis being true <a id="id275" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>is essentially non-existent. We are forced to accept the alternate hypothesis.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Confidence intervals"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec66" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Confidence intervals</h1></div></div></div><p class="calibre11">Having established <a id="id276" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that there certainly is a correlation in the wider population, we might want to quantify the range of values we expect <span class="inlinemediaobject"><img src="Images/7180OS_03_08.jpg" alt="Confidence intervals" class="calibre89"/></span> to lie within by calculating a confidence interval. As in the previous chapter with the mean, the confidence interval of <span class="strong1"><em class="calibre13">r</em></span> expresses the probability (expressed as a percentage) that the population parameter <span class="inlinemediaobject"><img src="Images/7180OS_03_08.jpg" alt="Confidence intervals" class="calibre89"/></span> lies between two specific values.</p><p class="calibre11">However, a complication arises when trying to calculate the standard error of the correlation coefficient that didn't exist for the mean. Because the absolute value of <span class="strong1"><em class="calibre13">r</em></span> cannot exceed <span class="strong1"><strong class="calibre12">1</strong></span>, the distribution of possible samples of <span class="strong1"><em class="calibre13">r</em></span> is skewed as <span class="strong1"><em class="calibre13">r</em></span> approaches the limit of its range.</p><div class="mediaobject"><img src="Images/7180OS_03_180.jpg" alt="Confidence intervals" class="calibre95"/></div><p class="calibre11">The previous graph shows the negatively skewed distribution of <span class="strong1"><em class="calibre13">r</em></span> samples for a <span class="inlinemediaobject"><img src="Images/7180OS_03_08.jpg" alt="Confidence intervals" class="calibre89"/></span> of 0.6.</p><p class="calibre11">Fortunately, a transformation <a id="id277" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>called the <span class="strong1"><strong class="calibre12">Fisher z-transformation</strong></span> will stabilize the variance of <span class="strong1"><em class="calibre13">r</em></span> throughout its range. This is analogous to how our <a id="id278" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>weight data became normally distributed when we took the logarithm.</p><p class="calibre11">The equation for the <span class="strong1"><em class="calibre13">z</em></span>-transformation is:</p><div class="mediaobject"><img src="Images/7180OS_03_14.jpg" alt="Confidence intervals" class="calibre96"/></div><p class="calibre11">The standard error of <span class="strong1"><em class="calibre13">z</em></span> is:</p><div class="mediaobject"><img src="Images/7180OS_03_15.jpg" alt="Confidence intervals" class="calibre97"/></div><p class="calibre11">Thus, the process to calculate confidence intervals is to convert <span class="strong1"><em class="calibre13">r</em></span> to <span class="strong1"><em class="calibre13">z</em></span> using the <span class="strong1"><em class="calibre13">z</em></span>-transformation, compute a confidence interval in terms of <span class="strong1"><em class="calibre13">SE<sub class="calibre25">z</sub></em></span>, and then convert the confidence interval back to <span class="strong1"><em class="calibre13">r</em></span>.</p><p class="calibre11">To calculate a confidence interval in terms of <span class="strong1"><em class="calibre13">SE<sub class="calibre25">z</sub></em></span>, we can take the number of standard deviations away from the mean that gives us the desired confidence. 1.96 is a common number to use, because it is <a id="id279" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the number of standard deviations away from the mean that contains 95 percent of the area. In other words, 1.96 standard errors from the mean of the sample <span class="strong1"><em class="calibre13">r</em></span> contains the true population correlation <span class="strong1"><em class="calibre13">ρ</em></span> with 95 percent certainty.</p><div class="mediaobject"><img src="Images/7180OS_03_190.jpg" alt="Confidence intervals" class="calibre98"/></div><p class="calibre11">We can verify this using Incanter's <code class="literal">incanter.stats/quantile-normal</code> function. This will return the standard score associated with a given cumulative probability, assuming a one-tailed test.</p><p class="calibre11">However, as shown in the previous diagram, we'd like to subtract the same amount— 2.5 percent—from each tail, so that the 95 percent confidence interval is centered on zero. A simple translation is to halve the difference to 100 percent while performing a two-tailed test. So, a desired confidence of 95 percent means we look up the critical value of 97.5 percent:</p><div class="calibre2"><pre class="programlisting">(defn critical-value [confidence ntails]
  (let [lookup (- 1 (/ (- 1 confidence) ntails))]
    (s/quantile-normal lookup)))

(critical-value 0.95 2)
=&gt; 1.96</pre></div><p class="calibre11">So, our 95 percent confidence interval in <span class="strong1"><em class="calibre13">z</em></span>-space for <span class="inlinemediaobject"><img src="Images/7180OS_03_08.jpg" alt="Confidence intervals" class="calibre89"/></span> is given by:</p><div class="mediaobject"><img src="Images/7180OS_03_16.jpg" alt="Confidence intervals" class="calibre99"/></div><p class="calibre11">Substituting our formulae for <span class="strong1"><em class="calibre13">z<sub class="calibre25">r</sub></em></span> and <span class="strong1"><em class="calibre13">SE<sub class="calibre25">z</sub></em></span> gives:</p><div class="mediaobject"><img src="Images/7180OS_03_17.jpg" alt="Confidence intervals" class="calibre100"/></div><p class="calibre11">For <code class="literal">r = 0.867</code> and <code class="literal">n = 859</code>, this gives a lower and upper bound of <code class="literal">1.137</code> and <code class="literal">1.722</code>, respectively. To convert <a id="id280" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>these from <span class="strong1"><em class="calibre13">z</em></span>-scores back to <span class="strong1"><em class="calibre13">r</em></span>-values, we use the following equation, the inverse of the <span class="strong1"><em class="calibre13">z</em></span>-transformation:</p><div class="mediaobject"><img src="Images/7180OS_03_18.jpg" alt="Confidence intervals" class="calibre101"/></div><p class="calibre11">The transformations and confidence interval can be calculated with the following code:</p><div class="calibre2"><pre class="programlisting">(defn z-&gt;r [z]
  (/ (- (i/exp (* 2 z)) 1)
     (+ (i/exp (* 2 z)) 1)))

(defn r-confidence-interval [crit x y]
  (let [r   (correlation x y)
        n   (count x)
        zr  (* 0.5 (i/log (/ (+ 1 r)
                             (- 1 r))))
        sez (/ 1 (i/sqrt (- n 3)))]
    [(z-&gt;r (- zr (* crit sez)))
     (z-&gt;r (+ zr (* crit sez)))]))

(defn ex-3-10 []
  (let [data (swimmer-data)
        heights  (i/$ "Height, cm" data)
        weights  (i/log (i/$ "Weight" data))
        interval (r-confidence-interval 1.96 heights weights)]
    (println "Confidence Interval (95%): " interval)))</pre></div><p class="calibre11">This gives a 95 percent confidence interval for <span class="inlinemediaobject"><img src="Images/7180OS_03_08.jpg" alt="Confidence intervals" class="calibre89"/></span> being between <code class="literal">0.850</code> and <code class="literal">0.883</code>. We can be very confident that <a id="id281" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>there is a strong positive correlation between the height and weight in the wider population of Olympic-class swimmers.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Regression"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec67" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Regression</h1></div></div></div><p class="calibre11">While it may be useful <a id="id282" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to know that two variables are correlated, we can't use this information alone to predict the weights of Olympic swimmers given their height or vice versa. In establishing a correlation, we have measured the strength and sign of a relationship, but not the slope. Knowing the expected rate of change for one variable given a unit change in the other is required in order to make predictions.</p><p class="calibre11">What we'd like to determine is an equation that relates the specific value of one variable, called the <span class="strong1"><strong class="calibre12">independent </strong></span><a id="id283" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">variable</strong></span>, to the expected value of the other, the <span class="strong1"><strong class="calibre12">dependent </strong></span><a id="id284" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">variable</strong></span>. For example, if our linear equation predicts the weight given the height, then the height is our independent variable and the weight is our dependent variable.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note30" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The lines described <a id="id285" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>by these equations are called <span class="strong1"><strong class="calibre12">regression lines</strong></span>. The term was introduced by the 19th century British polymath Sir Francis Galton. He and his student Karl Pearson (who defined the correlation coefficient) developed a variety of methods to study linear relationships in the 19th century and these collectively became known as regression techniques.</p></div></div><p class="calibre11">Remember that correlation does not imply causation and there is no implied causation by the terms dependent and independent—they're just the names for mathematical inputs and outputs. A classic example is the highly positive correlation between the number of fire engines sent to a fire and the damage done by the fire. Clearly, sending fire engines to a fire does not itself cause damage. No one would recommend reducing the number of engines sent to a fire as a way of reducing damage. In situations like these, we should look for an additional variable, which is causally connected with the other variables, and explains the correlation between them. In the previous example, this might be the <span class="strong1"><em class="calibre13">size of fire</em></span>. Such hidden causes are called <span class="strong1"><strong class="calibre12">confounding </strong></span><a id="id286" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">variables</strong></span>, because they confound our <a id="id287" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>ability to determine the relationship between their dependent variables.</p><div class="calibre2" title="Linear equations"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec39" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Linear equations</h2></div></div></div><p class="calibre11">Two variables, which <a id="id288" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we can signify as <span class="strong1"><em class="calibre13">x</em></span> and <span class="strong1"><em class="calibre13">y</em></span>, may be related to each other exactly or inexactly. The simplest relationship between an independent variable labeled <span class="strong1"><em class="calibre13">x</em></span> and a dependent variable labeled <span class="strong1"><em class="calibre13">y</em></span> is a straight line expressed in the formula:</p><div class="mediaobject"><img src="Images/7180OS_03_19.jpg" alt="Linear equations" class="calibre102"/></div><p class="calibre11">Here, the values of the parameters <span class="strong1"><em class="calibre13">a</em></span> and <span class="strong1"><em class="calibre13">b</em></span> determine respectively the precise height and steepness of the line. The parameter <span class="strong1"><em class="calibre13">a</em></span> is referred to as the intercept or constant and <span class="strong1"><em class="calibre13">b</em></span> as the gradient or slope. For example, in the mapping between Celsius and Fahrenheit temperature scales, <span class="strong1"><em class="calibre13">a = 32 </em></span>and <span class="strong1"><em class="calibre13">b = 1.8</em></span>. Substituting these values of <span class="strong1"><em class="calibre13">a</em></span> and <span class="strong1"><em class="calibre13">b</em></span> into our equation yields:</p><div class="mediaobject"><img src="Images/7180OS_03_20.jpg" alt="Linear equations" class="calibre103"/></div><p class="calibre11">To calculate 10 degrees Celsius in Fahrenheit, we substitute 10 for <span class="strong1"><em class="calibre13">x</em></span>:</p><div class="mediaobject"><img src="Images/7180OS_03_21.jpg" alt="Linear equations" class="calibre104"/></div><p class="calibre11">Thus, our equation tells us that 10 degrees Celsius is 50 degrees Fahrenheit, which is indeed the case. Using Incanter, we can easily write a function that maps Celsius to Fahrenheit and plot it as a graph using <code class="literal">incanter.charts/function-plot</code>:</p><div class="calibre2"><pre class="programlisting">(defn celsius-&gt;fahrenheit [x]
  (+ 32 (* 1.8 x)))

(defn ex-3-11 []
  (-&gt; (c/function-plot celsius-&gt;fahrenheit -10 40
                       :x-label "Celsius"
                       :y-label "Fahrenheit")
      (i/view)))</pre></div><p class="calibre11">This code yields <a id="id289" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the following line graph:</p><div class="mediaobject"><img src="Images/7180OS_03_200.jpg" alt="Linear equations" class="calibre45"/></div><p class="calibre11">Notice how the red line crosses zero on the Celsius scale at 32 on the Fahrenheit scale. The intercept <span class="strong1"><em class="calibre13">a</em></span> is the value of <span class="strong1"><em class="calibre13">y</em></span>, where <span class="strong1"><em class="calibre13">x</em></span> is zero.</p><p class="calibre11">The slope of the line is determined by <span class="strong1"><em class="calibre13">b</em></span>; it is close to 2 for this equation. See how the range of the Fahrenheit scale is almost double the range of the Celsius scale. In other words, the line sweeps almost twice as fast vertically as it does horizontally.</p></div><div class="calibre2" title="Residuals"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec40" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Residuals</h2></div></div></div><p class="calibre11">Unfortunately few <a id="id290" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>relationships we will study are as tidy as the mapping between Celsius and Fahrenheit. The straight-line equation rarely allows us to specify <span class="strong1"><em class="calibre13">y</em></span> exactly in terms of <span class="strong1"><em class="calibre13">x</em></span>. There will ordinarily be an error, thus:</p><div class="mediaobject"><img src="Images/7180OS_03_22.jpg" alt="Residuals" class="calibre105"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">ε</em></span> is an error term standing for the difference between the value calculated by the parameters <span class="strong1"><em class="calibre13">a</em></span> and <span class="strong1"><em class="calibre13">b</em></span> for a given value of <span class="strong1"><em class="calibre13">x</em></span> and the actual value of <span class="strong1"><em class="calibre13">y</em></span>. If our predicted value of <span class="strong1"><em class="calibre13">y</em></span> is <span class="inlinemediaobject"><img src="Images/7180OS_03_23.jpg" alt="Residuals" class="calibre106"/></span> (pronounced "y-hat"), then the error is the difference between the two:</p><div class="mediaobject"><img src="Images/7180OS_03_24.jpg" alt="Residuals" class="calibre107"/></div><p class="calibre11">This error is referred to as the residual. The residual might be due to random factors like measurement error or non-random factors that are unknown. For example, if we are trying to predict weight as a function of height, unknown factors might include diet, level of fitness, and body type (or simply the effect of rounding to the nearest kilogram).</p><p class="calibre11">If we select parameters for <span class="strong1"><em class="calibre13">a</em></span> and <span class="strong1"><em class="calibre13">b</em></span> that are not ideal, then the residual for each <span class="strong1"><em class="calibre13">x</em></span> will be larger than it needs to be. Therefore, it follows that the parameters we'd like to find are the ones that minimize the residuals across all values of <span class="strong1"><em class="calibre13">x</em></span> and <span class="strong1"><em class="calibre13">y</em></span>.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Ordinary least squares"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec68" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Ordinary least squares</h1></div></div></div><p class="calibre11">In order to optimize <a id="id291" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the parameters of our linear model, we'd like to devise a cost function, also called a <span class="strong1"><strong class="calibre12">loss function</strong></span>, that quantifies how <a id="id292" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>closely our predictions fit the data. We cannot simply sum up the residuals, positive and negative, because even large residuals will cancel each other out if their signs are in opposite directions.</p><p class="calibre11">We could square the values before calculating the sum so that positive and negative residuals both count towards the cost. This also has the effect of penalizing large errors more than smaller errors, but not so much that the largest residual always dominates.</p><p class="calibre11">Expressed as an optimization problem, we seek to identify the coefficients that minimize the sum of the <a id="id293" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>residual squares. This is called <span class="strong1"><strong class="calibre12">Ordinary Least Squares</strong></span> (<span class="strong1"><strong class="calibre12">OLS</strong></span>), and the formula to calculate the slope of the regression line using OLS is:</p><div class="mediaobject"><img src="Images/7180OS_03_25.jpg" alt="Ordinary least squares" class="calibre108"/></div><p class="calibre11">Although this looks more complicated than the previous equations, it's really just the sum of squared residuals divided by the sum of squared differences from the mean. This shares a number of terms from the equations we have already looked at and can be simplified to:</p><div class="mediaobject"><img src="Images/7180OS_03_26.jpg" alt="Ordinary least squares" class="calibre109"/></div><p class="calibre11">The intercept is the term that allows a line of this slope to pass through the mean of both <span class="strong1"><em class="calibre13">X</em></span> and <span class="strong1"><em class="calibre13">Y</em></span>:</p><div class="mediaobject"><img src="Images/7180OS_03_27.jpg" alt="Ordinary least squares" class="calibre70"/></div><p class="calibre11">These values of <span class="strong1"><em class="calibre13">a</em></span> and <span class="strong1"><em class="calibre13">b</em></span> are the coefficients of our least squares estimates.</p><div class="calibre2" title="Slope and intercept"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec41" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Slope and intercept</h2></div></div></div><p class="calibre11">We've already <a id="id294" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>written the <code class="literal">covariance</code>, <code class="literal">variance</code>, and <code class="literal">mean</code> <a id="id295" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>functions we need to calculate the slope and intercept for the swimming height and weight data. Therefore, the slope and intercept calculations are trivial:</p><div class="calibre2"><pre class="programlisting">(defn slope [x y]
  (/ (covariance x y)
     (variance x)))

(defn intercept [x y]
  (- (s/mean y)
     (* (s/mean x)
        (slope x y))))

(defn ex-3-12 []
  (let [data (swimmer-data)
        heights (i/$ "Height, cm" data)
        weights (i/log (i/$ "Weight" data))
        a (intercept heights weights)
        b (slope heights weights)]
    (println "Intercept: " a)
    (println "Slope: " b)))</pre></div><p class="calibre11">The output gives <a id="id296" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a slope of approximately <code class="literal">0.0143</code> and an <a id="id297" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>intercept of approximately <code class="literal">1.6910</code>.</p></div><div class="calibre2" title="Interpretation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec42" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Interpretation</h2></div></div></div><p class="calibre11">The <span class="strong1"><strong class="calibre12">intercept value</strong></span> is <a id="id298" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the value of the dependent variable (log weight) when the independent variable (<code class="literal">height</code>) is zero. To find out what this value equates to in kilograms, we can use the <code class="literal">incanter.core/exp</code> function, which performs the inverse of the <code class="literal">incanter.core/log</code> function. Our model seems to suggest that the best guess for the weight of an Olympic swimmer of zero height is 5.42 kg. This is meaningless, and it is unwise to extrapolate beyond the bounds of your training data.</p><p class="calibre11">The slope value shows how much <span class="strong1"><em class="calibre13">y</em></span> changes for each unit change in <span class="strong1"><em class="calibre13">x</em></span>. Our model suggests that each additional centimeter of height adds on an average of 1.014 kg to the weight of our Olympic swimmers. Since our model is based on all Olympic swimmers, this is the average effect of a unit increase in height without taking into account any other factor, such as age, gender, or body type.</p></div><div class="calibre2" title="Visualization"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec43" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Visualization</h2></div></div></div><p class="calibre11">We can visualize the <a id="id299" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>output of our linear equation with <code class="literal">incanter.charts/function-plot</code> and a simple function of <span class="strong1"><em class="calibre13">x</em></span> that calculates <span class="inlinemediaobject"><img src="Images/7180OS_03_23.jpg" alt="Visualization" class="calibre106"/></span> based on the coefficients <span class="strong1"><em class="calibre13">a</em></span> and <span class="strong1"><em class="calibre13">b</em></span>.</p><div class="calibre2"><pre class="programlisting">(defn regression-line [a b]
  (fn [x] 
    (+ a (* b x))))

(defn ex-3-13 []
  (let [data (swimmer-data)
        heights (-&gt;&gt; (i/$ "Height, cm" data)
                     (map (jitter 0.5)))
        weights (i/log (i/$ "Weight" data))
        a (intercept heights weights)
        b (slope heights weights)]
    (-&gt; (c/scatter-plot heights weights
                        :x-label "Height, cm"
                        :y-label "log(Weight)")
        (c/add-function (regression-line a b) 150 210)
        (i/view))))</pre></div><p class="calibre11">The <code class="literal">regression-line</code> <a id="id300" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function returns a function of <span class="strong1"><em class="calibre13">x</em></span> that calculates <span class="inlinemediaobject"><img src="Images/7180OS_03_28.jpg" alt="Visualization" class="calibre110"/></span>.</p><div class="mediaobject"><img src="Images/7180OS_03_210.jpg" alt="Visualization" class="calibre45"/></div><p class="calibre11">We can also use the <code class="literal">regression-line</code> function to calculate each residual, showing how far our estimate <span class="inlinemediaobject"><img src="Images/7180OS_03_23.jpg" alt="Visualization" class="calibre106"/></span> deviates <a id="id301" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>from each measured <span class="strong1"><em class="calibre13">y</em></span>.</p><div class="calibre2"><pre class="programlisting">(defn residuals [a b x y]
  (let [estimate (regression-line a b)
        residual (fn [x y]
                   (- y (estimate x)))]
    (map residual x y)))

(defn ex-3-14 []
  (let [data (swimmer-data)
        heights (-&gt;&gt; (i/$ "Height, cm" data)
                     (map (jitter 0.5)))
        weights (i/log (i/$ "Weight" data))
        a (intercept heights weights)
        b (slope heights weights)]
    (-&gt; (c/scatter-plot heights (residuals a b heights weights)
                        :x-label "Height, cm"
                        :y-label "Residuals")
        (c/add-function (constantly 0) 150 210)
        (i/view))))</pre></div><p class="calibre11">A <span class="strong1"><strong class="calibre12">residual plot</strong></span> is a <a id="id302" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>graph that shows the residuals on the <span class="strong1"><em class="calibre13">y</em></span>-axis and the independent variable on the <span class="strong1"><em class="calibre13">x</em></span>-axis. If the points in the residual plot are randomly dispersed around the horizontal axis, a linear model is a good fit for the data:</p><div class="mediaobject"><img src="Images/7180OS_03_220.jpg" alt="Visualization" class="calibre45"/></div><p class="calibre11">With the exception of some outliers on the left side of the chart, the residual plot appears to indicate that a linear <a id="id303" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>model is a good fit for the data. Plotting the residuals is important to verify that the linear model is appropriate. There are certain assumptions that a linear model makes about your data that will, if violated, invalidate models you build.</p></div><div class="calibre2" title="Assumptions"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec44" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Assumptions</h2></div></div></div><p class="calibre11">Obviously, the <a id="id304" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>primary assumption of linear regression is that there is a linear relationship between the dependent and independent variable. In addition, the residuals must not be correlated with each other or with the independent variable. In other words, we expect the errors to have a zero mean and constant variance versus the dependent and independent variable. A residual plot allows us to quickly determine if this is the case.</p><p class="calibre11">The left side of our residual plot has greater residuals than the right side. This corresponds to greater variance of weight amongst shorter athletes. The variables are said to be <span class="strong1"><strong class="calibre12">heteroscedastic</strong></span> when the variance of one variable changes with respect to another. This is a concern in regression analysis, because it invalidates the assumption that modeling errors are uncorrelated and normally distributed and that their variances do not vary with the <a id="id305" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>effects being modeled.</p><p class="calibre11">The heteroscedasticity of our residuals are fairly small and should not influence the quality of our model very much. If the variance on the left side of the graph were more pronounced, it would cause the least squares estimate of variance to be incorrect, which in turn would affect inferences we make based on the standard error.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Goodness-of-fit and R-square"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec69" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Goodness-of-fit and R-square</h1></div></div></div><p class="calibre11">Although we can see from the residual plot that a linear model is a good fit for our data, it would be desirable to <a id="id306" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>quantify just how good it is. Also called the <span class="strong1"><strong class="calibre12">coefficient of determination</strong></span>, <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> varies between zero and one and indicates the explanatory power of the linear regression model. It calculates the proportion of variation in the dependent variable explained, or accounted for, by the independent variable.</p><p class="calibre11">Generally, the closer <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> is to 1, the better the regression line fits the points and the more the variation in <span class="strong1"><em class="calibre13">Y</em></span> is explained by <span class="strong1"><em class="calibre13">X</em></span>. <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> can be calculated using the following formula:</p><div class="mediaobject"><img src="Images/7180OS_03_29.jpg" alt="Goodness-of-fit and R-square" class="calibre111"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">var(ε)</em></span> is the variance of the residuals and <span class="strong1"><em class="calibre13">var(Y)</em></span> is the variance in <span class="strong1"><em class="calibre13">Y</em></span>. To understand what this means, let's suppose you're trying to guess someone's weight. If you don't know anything else about them, your best strategy would be to guess the mean of the weights within the population in general. This way, the mean squared error of your guess compared to their true weight would be <span class="strong1"><em class="calibre13">var(Y)</em></span> or the variance of the weights in the population.</p><p class="calibre11">But if I told you their height, you would guess <span class="inlinemediaobject"><img src="Images/7180OS_03_28.jpg" alt="Goodness-of-fit and R-square" class="calibre110"/></span> as per the regression model. In this case, your mean squared error would be <span class="strong1"><em class="calibre13">var(ε)</em></span> or the variance of the residuals of the model.</p><p class="calibre11">The term <span class="strong1"><em class="calibre13">var(ε)/ var(Y)</em></span> is the ratio of mean squared error with and without the explanatory variable, which is the fraction of variability left unexplained by the model. The complement <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> is the fraction of variability explained by the model.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note31" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">As with <span class="strong1"><em class="calibre13">r</em></span>, a low <span class="strong1"><em class="calibre13">R<sup class="calibre112">2</sup></em></span> does not mean that the two variables are uncorrelated. It might simply be that their relationship is not linear.</p></div></div><p class="calibre11">The <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> value describes how well the line fits the data. The line of <span class="strong1"><em class="calibre13">best fit</em></span> is the line that minimizes the value of <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span>. As the coefficients increase or decrease away from their optimum values, <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> will always increase.</p><div class="mediaobject"><img src="Images/7180OS_03_240.jpg" alt="Goodness-of-fit and R-square" class="calibre113"/></div><p class="calibre11">The left graph shows the variance for a model that always guesses the mean of <span class="strong1"><em class="calibre13">y</em></span> and the right one shows smaller squares associated with the residuals left unexplained by the model <span class="strong1"><em class="calibre13">f</em></span>. In purely geometric terms, you can see the how the model has explained most of the variance in <span class="strong1"><em class="calibre13">y</em></span>. The following code calculates <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> by dividing the variance of the residuals with the variance of the <span class="strong1"><em class="calibre13">y</em></span> values:</p><div class="calibre2"><pre class="programlisting">(defn r-squared [a b x y]
  (let [r-var (variance (residuals a b x y))
        y-var (variance y)]
    (- 1 (/ r-var y-var))))

(defn ex-3-15 []
  (let [data (swimmer-data)
        heights (i/$ "Height, cm" data)
        weights (i/log (i/$ "Weight" data))
        a (intercept heights weights)
        b (slope heights weights)]
    (r-squared a b heights weights)))</pre></div><p class="calibre11">This gives a value of <code class="literal">0.753</code>. In other words, over 75 percent of the variance of the weight of 2012 Olympic swimmers can be explained by the height.</p><p class="calibre11">In the case of a simple regression model (with a single independent variable), the relationship between the coefficient of determination <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> and the correlation coefficient <span class="strong1"><em class="calibre13">r</em></span> is a straightforward one:</p><div class="mediaobject"><img src="Images/7180OS_03_30.jpg" alt="Goodness-of-fit and R-square" class="calibre114"/></div><p class="calibre11">A correlation coefficient of 0.5 might suggest that half the variability in <span class="strong1"><em class="calibre13">Y</em></span> is explained by <span class="strong1"><em class="calibre13">X</em></span>, but actually, <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> would be 0.5<sup class="calibre42">2</sup> or 0.25.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Multiple linear regression"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec70" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Multiple linear regression</h1></div></div></div><p class="calibre11">We've seen so far in this chapter how to build a regression line with one independent variable. However, it is often desirable to build a model with several independent variables. This is <a id="id307" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>called <span class="strong1"><strong class="calibre12">multiple linear regression</strong></span>.</p><p class="calibre11">Each independent variable is going to need its own coefficient. Rather than working our way through the alphabet to represent each one, let's designate a new variable <span class="strong1"><em class="calibre13">β</em></span>, pronounced "beta", to hold all of our coefficients:</p><div class="mediaobject"><img src="Images/7180OS_03_31.jpg" alt="Multiple linear regression" class="calibre115"/></div><p class="calibre11">This model is equivalent <a id="id308" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to our <span class="strong1"><strong class="calibre12">bivariate linear regression</strong></span> model, where <span class="inlinemediaobject"><img src="Images/7180OS_03_32.jpg" alt="Multiple linear regression" class="calibre116"/></span> and <span class="inlinemediaobject"><img src="Images/7180OS_03_33.jpg" alt="Multiple linear regression" class="calibre117"/></span> so long as we ensure that <span class="strong1"><em class="calibre13">x<sub class="calibre25">1</sub></em></span> is always equal to one. This ensures that <span class="strong1"><em class="calibre13">β<sub class="calibre25">1</sub></em></span> is always a constant factor representing our intercept. <span class="strong1"><em class="calibre13">x<sub class="calibre25">1</sub></em></span> is called the <span class="strong1"><strong class="calibre12">bias </strong></span><a id="id309" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">term</strong></span>.</p><p class="calibre11">Having generalized the linear equation in terms of beta, easy to extend to as many coefficients as we'd like:</p><div class="mediaobject"><img src="Images/7180OS_03_34.jpg" alt="Multiple linear regression" class="calibre118"/></div><p class="calibre11">Each of the values of <span class="strong1"><em class="calibre13">x<sub class="calibre25">1</sub></em></span> up to <span class="strong1"><em class="calibre13">x<sub class="calibre25">n</sub></em></span> correspond to an independent variable that might help explain the value of <span class="strong1"><em class="calibre13">y</em></span>. Each of the values of <span class="strong1"><em class="calibre13">β<sub class="calibre25">1</sub></em></span> up to <span class="strong1"><em class="calibre13">β<sub class="calibre25">n</sub></em></span> correspond to a coefficient that determines the relative contribution of this independent variable.</p><p class="calibre11">Our simple linear regression aimed to explain weight only in terms of height, but many other factors help to explain someone's weight: their age, gender, diet, and body type. We know the ages of our <a id="id310" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Olympic swimmers, so we could build a model that incorporates this additional data too.</p><p class="calibre11">We've been providing the independent variable as a single sequence of values, but with multiple parameters, we'll need to provide several values for each <span class="strong1"><em class="calibre13">x</em></span>. We can use Incanter's <code class="literal">i/$</code> function to select multiple columns and manipulate each <span class="strong1"><em class="calibre13">x</em></span> as a Clojure vector, but there is a better way: matrices.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Matrices"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec71" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Matrices</h1></div></div></div><p class="calibre11">A matrix is a <a id="id311" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>two-dimensional grid of numbers. The dimensions are expressed as the number of rows and columns in the matrix.</p><p class="calibre11">For example, <span class="strong1"><em class="calibre13">A</em></span> is a matrix with four rows and two columns:</p><div class="mediaobject"><img src="Images/7180OS_03_35.jpg" alt="Matrices" class="calibre119"/></div><p class="calibre11">In mathematical notation, a matrix will usually be assigned to a variable with an upper-case letter to distinguish it from other variables in an equation.</p><p class="calibre11">We can construct a matrix from our dataset using Incanter's <code class="literal">incanter.core/to-matrix</code> function:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-16 []
  (-&gt;&gt; (swimmer-data)
       (i/$ ["Height, cm" "Weight"])
       (i/to-matrix)))</pre></div><p class="calibre11">Incanter also defines the <code class="literal">incanter.core/matrix</code> function that will take a sequence of scalar values or a sequence of sequences and convert them into a matrix if it can:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-17 []
  (-&gt;&gt; (swimmer-data)
       (i/$ "Height, cm")
       (i/matrix)))</pre></div><p class="calibre11">If you run this in the <a id="id312" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>REPL, the output will be a summary of the contents of the matrix:</p><div class="calibre2"><pre class="programlisting">  A 859x1 matrix
 ---------------
 1.66e+02
 1.92e+02
 1.73e+02
 ...
 1.88e+02
 1.87e+02
 1.83e+02</pre></div><p class="calibre11">Incanter returns a representation exactly as shown in the preceding example, presenting only the top and bottom three rows of the matrix. Matrices can often become very large and Incanter takes care not to inundate the REPL with information.</p><div class="calibre2" title="Dimensions"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec45" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Dimensions</h2></div></div></div><p class="calibre11">The element <a id="id313" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>in the <span class="strong1"><em class="calibre13">i<sup class="calibre42">th</sup></em></span> row <span class="strong1"><em class="calibre13">j<sup class="calibre42">th</sup></em></span> column is referred to as <span class="strong1"><em class="calibre13">A<sub class="calibre25">ij</sub></em></span>. Therefore, in our <a id="id314" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>earlier example:</p><div class="mediaobject"><img src="Images/7180OS_03_36.jpg" alt="Dimensions" class="calibre120"/></div><p class="calibre11">One of the most fundamental attributes of a matrix is its size. Incanter provides the <code class="literal">incanter.core/dim</code>, <code class="literal">ncol</code>, and <code class="literal">nrow</code> functions to query matrices dimensions.</p></div><div class="calibre2" title="Vectors"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec46" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Vectors</h2></div></div></div><p class="calibre11">A vector is a <a id="id315" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>special case of matrix with only one column. The number of rows in the <a id="id316" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>vector are referred to as its dimension:</p><div class="mediaobject"><img src="Images/7180OS_03_37.jpg" alt="Vectors" class="calibre121"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">y</em></span> is a four-dimensional vector. The <span class="strong1"><em class="calibre13">i<sup class="calibre42">th</sup></em></span> element is referred to as <span class="strong1"><em class="calibre13">y<sub class="calibre25">i</sub></em></span>.</p><p class="calibre11">Vectors in <a id="id317" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>mathematical literature are one-indexed unless otherwise specified. So, <span class="strong1"><em class="calibre13">y<sub class="calibre25">1</sub></em></span> refers to the first element, not the second. Vectors are generally assigned to lowercase <a id="id318" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>variables in equations. Incanter's API doesn't distinguish between vectors and single column matrices and we can create a vector by passing a single sequence to the <code class="literal">incanter.core/matrix</code> function.</p></div><div class="calibre2" title="Construction"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec47" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Construction</h2></div></div></div><p class="calibre11">As we've seen, it's <a id="id319" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>possible to build matrices out of Clojure sequences and Incanter <a id="id320" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>datasets. It's also possible to build matrices out of smaller building blocks, provided the dimensions are compatible. Incanter provides the <code class="literal">incanter.core/bind-columns</code> and <code class="literal">incanter.core/bind-rows</code> functions to stack matrices above one another or side by side.</p><p class="calibre11">For example, we could add a column of 1s to the front of another matrix in the following way:</p><div class="calibre2"><pre class="programlisting">(defn add-bias [x]
  (i/bind-columns (repeat (i/nrow x) 1) x))</pre></div><p class="calibre11">In fact, we'll want to do this for our bias term. Recall that <span class="strong1"><em class="calibre13">β<sub class="calibre25">1</sub></em></span> will represent a constant value, so we must ensure that our corresponding <span class="strong1"><em class="calibre13">x<sub class="calibre25">1</sub></em></span> is constant too. Without the bias term, <span class="strong1"><em class="calibre13">y</em></span> would have to be zero when the values of <span class="strong1"><em class="calibre13">x</em></span> are zero.</p></div><div class="calibre2" title="Addition and scalar multiplication"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec48" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Addition and scalar multiplication</h2></div></div></div><p class="calibre11">A scalar is a <a id="id321" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>name for a simple number. When we add a scalar to a <a id="id322" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix, it's as if we added the number to each element of the <a id="id323" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix, individually. Incanter provides the <code class="literal">incanter.core/plus</code> function to add scalars and matrices together.</p><p class="calibre11">Matrix-matrix addition <a id="id324" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>works by adding the elements in each corresponding position. Only matrices of the same dimensions can be added together. If the matrices are of the same dimensions, they are said to be compatible.</p><div class="mediaobject"><img src="Images/7180OS_03_38.jpg" alt="Addition and scalar multiplication" class="calibre122"/></div><p class="calibre11">The <code class="literal">plus</code> function will also add compatible matrices. The <code class="literal">minus</code> function will subtract scalars or <a id="id325" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>compatible matrices. Multiplying a matrix by a scalar results in <a id="id326" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>each of the elements in the matrix being multiplied by the scalar.</p><div class="mediaobject"><img src="Images/7180OS_03_39.jpg" alt="Addition and scalar multiplication" class="calibre123"/></div><p class="calibre11">The <code class="literal">incanter.core/mult</code> performs matrix-scalar multiplication, while <code class="literal">incanter.core/div</code> <a id="id327" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>performs the inverse.</p><p class="calibre11">We can also use <code class="literal">mult</code> and <code class="literal">div</code> on compatible matrices, but this element-wise method of multiplying and dividing <a id="id328" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>is not what we normally intend to do when we speak of matrix multiplication.</p></div><div class="calibre2" title="Matrix-vector multiplication"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec49" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Matrix-vector multiplication</h2></div></div></div><p class="calibre11">The standard way <a id="id329" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to multiply matrices is handled by the <code class="literal">incanter.core/mmult</code> function, which applies the complex matrix multiplication <a id="id330" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>algorithm. For example, the result of multiplying a 3 x 2 matrix with a 2 x 1 matrix is a 3 x 1 matrix. The number of columns on the left has to match the number of rows on the right of the multiplication:</p><div class="mediaobject"><img src="Images/7180OS_03_40.jpg" alt="Matrix-vector multiplication" class="calibre124"/></div><div class="mediaobject"><img src="Images/7180OS_03_41.jpg" alt="Matrix-vector multiplication" class="calibre125"/></div><div class="mediaobject"><img src="Images/7180OS_03_42.jpg" alt="Matrix-vector multiplication" class="calibre126"/></div><p class="calibre11">To get <span class="strong1"><em class="calibre13">Ax</em></span>, multiply each row of <span class="strong1"><em class="calibre13">A</em></span> element-by-element with the corresponding element of <span class="strong1"><em class="calibre13">x</em></span> and sum the results. For example, the first row of matrix <span class="strong1"><em class="calibre13">A</em></span> contains the elements <span class="strong1"><em class="calibre13">1</em></span> and <span class="strong1"><em class="calibre13">3</em></span>. These are multiplied <a id="id331" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>pairwise by the elements in vector <span class="strong1"><em class="calibre13">x</em></span>: <span class="strong1"><em class="calibre13">1</em></span> and <span class="strong1"><em class="calibre13">5</em></span>. Then, the products are added together to produce <span class="strong1"><em class="calibre13">16</em></span>. This is called the <span class="strong1"><strong class="calibre12">dot product</strong></span> and <a id="id332" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>is what is commonly intended by <a id="id333" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix multiplication.</p></div><div class="calibre2" title="Matrix-matrix multiplication"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec50" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Matrix-matrix multiplication</h2></div></div></div><p class="calibre11">Matrix-matrix <a id="id334" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>multiplication proceeds very similarly to <a id="id335" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix-vector multiplication. The sum of the products is taken pairwise, row by row and column by column, from the corresponding elements of matrices <span class="strong1"><em class="calibre13">A</em></span> and <span class="strong1"><em class="calibre13">B</em></span>.</p><div class="mediaobject"><img src="Images/7180OS_03_40.jpg" alt="Matrix-matrix multiplication" class="calibre124"/></div><div class="mediaobject"><img src="Images/7180OS_03_43.jpg" alt="Matrix-matrix multiplication" class="calibre127"/></div><div class="mediaobject"><img src="Images/7180OS_03_44.jpg" alt="Matrix-matrix multiplication" class="calibre128"/></div><p class="calibre11">As before, we can only multiply matrices together when the number of columns in the first matrix is equal to the number of rows in the second matrix. If the first matrix <span class="strong1"><em class="calibre13">A</em></span> is of dimensions <span class="inlinemediaobject"><img src="Images/7180OS_03_45.jpg" alt="Matrix-matrix multiplication" class="calibre129"/></span> and the second matrix <span class="strong1"><em class="calibre13">B</em></span> is of dimensions <span class="inlinemediaobject"><img src="Images/7180OS_03_46.jpg" alt="Matrix-matrix multiplication" class="calibre129"/></span>, <span class="strong1"><em class="calibre13">n<sub class="calibre25">a</sub></em></span> and <span class="strong1"><em class="calibre13">m<sub class="calibre25">B</sub></em></span> must be equal if the matrices are to be multiplied.</p><div class="mediaobject"><img src="Images/7180OS_03_250.jpg" alt="Matrix-matrix multiplication" class="calibre130"/></div><p class="calibre11">In the previous <a id="id336" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>visual example:</p><div class="mediaobject"><img src="Images/7180OS_03_47.jpg" alt="Matrix-matrix multiplication" class="calibre131"/></div><div class="mediaobject"><img src="Images/7180OS_03_48.jpg" alt="Matrix-matrix multiplication" class="calibre132"/></div><p class="calibre11">Luckily, we don't <a id="id337" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have to remember the process ourselves. Incanter uses very efficient algorithms to perform matrix algebra for us.</p></div><div class="calibre2" title="Transposition"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec51" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Transposition</h2></div></div></div><p class="calibre11">Transposing a <a id="id338" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix means flipping the matrix over the main diagonal running from the top-left to the bottom-right corner. The transpose of matrix <span class="strong1"><em class="calibre13">A</em></span> is represented as <span class="strong1"><em class="calibre13">A<sup class="calibre42">T</sup></em></span>:</p><div class="mediaobject"><img src="Images/7180OS_03_49.jpg" alt="Transposition" class="calibre133"/></div><p class="calibre11">The columns and rows have been changed such that:</p><div class="mediaobject"><img src="Images/7180OS_03_50.jpg" alt="Transposition" class="calibre134"/></div><p class="calibre11">Therefore, if:</p><div class="mediaobject"><img src="Images/7180OS_03_36.jpg" alt="Transposition" class="calibre120"/></div><p class="calibre11">Then:</p><div class="mediaobject"><img src="Images/7180OS_03_51.jpg" alt="Transposition" class="calibre135"/></div><p class="calibre11">Incanter provides the <code class="literal">incanter.core/trans</code> function to transpose a matrix.</p></div><div class="calibre2" title="The identity matrix"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec52" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The identity matrix</h2></div></div></div><p class="calibre11">Certain matrices <a id="id339" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have special properties and are used regularly in matrix <a id="id340" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>algebra. One of the most important of these is the identity matrix. It's a square matrix with ones along the main diagonal and zeros everywhere else:</p><div class="mediaobject"><img src="Images/7180OS_03_52.jpg" alt="The identity matrix" class="calibre136"/></div><p class="calibre11">The identity matrix is the identity for matrix multiplication. As with a scalar multiplication by the number one, a matrix multiplication by the identity matrix has no effect.</p><p class="calibre11">Incanter provides the <code class="literal">incanter.core/identity-matrix</code> function to construct identity matrices. Since they're <a id="id341" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>always square, we only provide a single argument <a id="id342" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>corresponding to both, the width and height.</p></div><div class="calibre2" title="Inversion"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec53" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Inversion</h2></div></div></div><p class="calibre11">If we have a square <a id="id343" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix <span class="strong1"><em class="calibre13">A</em></span>, the inverse of <span class="strong1"><em class="calibre13">A</em></span> is denoted as <span class="strong1"><em class="calibre13">A<sup class="calibre42">-1</sup></em></span> and it will have the following properties, where <span class="strong1"><em class="calibre13">I</em></span> is the identity matrix:</p><div class="mediaobject"><img src="Images/7180OS_03_53.jpg" alt="Inversion" class="calibre137"/></div><p class="calibre11">The identity <a id="id344" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix is its own inverse. Not all matrices are invertible and noninvertible matrices are also called <span class="strong1"><strong class="calibre12">singular</strong></span> or <span class="strong1"><strong class="calibre12">degenerate</strong></span> matrices. We can calculate the <a id="id345" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>inverse of a matrix with the <code class="literal">incanter.core/solve</code> <a id="id346" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function. <code class="literal">solve</code> will raise an exception if passed a singular matrix.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The normal equation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec72" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The normal equation</h1></div></div></div><p class="calibre11">Now that we've <a id="id347" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>covered the basics of matrix and vector manipulation we're in a position to study the <span class="strong1"><strong class="calibre12">normal equation</strong></span>. This is an equation that uses matrix algebra to calculate the coefficients of our OLS linear regression model:</p><div class="mediaobject"><img src="Images/7180OS_03_54.jpg" alt="The normal equation" class="calibre138"/></div><p class="calibre11">We read "to find <span class="strong1"><em class="calibre13">β</em></span>, multiply the inverse of <span class="strong1"><em class="calibre13">X</em></span> transpose <span class="strong1"><em class="calibre13">X</em></span>, by <span class="strong1"><em class="calibre13">X</em></span> transpose <span class="strong1"><em class="calibre13">y</em></span>" where <span class="strong1"><em class="calibre13">X</em></span> is the matrix of independent variables (including the intercept term) for our sample and <span class="strong1"><em class="calibre13">y</em></span> is a vector containing the dependent variables for our sample. The result <span class="strong1"><em class="calibre13">β</em></span> contains the calculated coefficients. This normal equation is relatively easy to derive from the equation of multiple regression, applying the rules of matrix multiplication, but the mathematics is beyond the scope of this book.</p><p class="calibre11">We can implement the normal equation with Incanter using only the functions we have just encountered:</p><div class="calibre2"><pre class="programlisting">(defn normal-equation [x y]
  (let [xtx  (i/mmult (i/trans x) x)
        xtxi (i/solve xtx)
        xty  (i/mmult (i/trans x) y)]
    (i/mmult xtxi xty)))</pre></div><p class="calibre11">This normal equation <a id="id348" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>expresses the mathematics of least squares linear regression in a very succinct way. We can use it as follows (remembering to add the bias term):</p><div class="calibre2"><pre class="programlisting">(defn ex-3-18 []
  (let [data (swimmer-data)
        x (i/matrix (i/$ "Height, cm" data))
        y (i/matrix (i/log (i/$ "Weight" data)))]
    (normal-equation (add-bias x) y)))</pre></div><p class="calibre11">This yields the following matrix:</p><div class="calibre2"><pre class="programlisting"> A 2x1 matrix
 -------------
 1.69e+00
 1.43e-02</pre></div><p class="calibre11">These are the values of <span class="strong1"><em class="calibre13">β<sub class="calibre25">1</sub></em></span> and <span class="strong1"><em class="calibre13">β<sub class="calibre25">2</sub></em></span> corresponding to the intercept and slope parameters. Happily, they agree with the values we calculated previously.</p><div class="calibre2" title="More features"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec54" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>More features</h2></div></div></div><p class="calibre11">Part of the strength <a id="id349" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the normal equation is that we've now implemented everything we need in order to support multiple linear regression. Let's write a function to convert the features of interest to a matrix:</p><div class="calibre2"><pre class="programlisting">(defn feature-matrix [col-names dataset]
  (-&gt; (i/$ col-names dataset)
      (i/to-matrix)))</pre></div><p class="calibre11">This function will allow us to select specific columns as a matrix in one step.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note32" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">A feature is a synonym for an independent variable and is popularly used in machine learning. Other synonyms are predictor, regressor, and explanatory variable, or simply input variable.</p></div></div><p class="calibre11">To start with, let's select height and age as our two features:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-19 []
  (feature-matrix ["Height, cm" "Age"] (swimmer-data)))</pre></div><p class="calibre11">This returns the following matrix of two columns:</p><div class="calibre2"><pre class="programlisting">A 859x2 matrix
 ---------------
 1.66e+02  2.30e+01
 1.92e+02  2.20e+01
 1.73e+02  2.00e+01
 ...
 1.88e+02  2.40e+01
 1.87e+02  1.90e+01
 1.83e+02  2.20e+01</pre></div><p class="calibre11">Our normal equation <a id="id350" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function will accept this new matrix without any further change:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-20 []
  (let [data (swimmer-data)
        x (-&gt;&gt; data
                (feature-matrix ["Height, cm" "Age"])
                (add-bias))
        y (-&gt;&gt; (i/$ "Weight" data)
                (i/log)
                (i/matrix))]
    (normal-equation x y)))</pre></div><p class="calibre11">It will return the following coefficients:</p><div class="calibre2"><pre class="programlisting"> A 3x1 matrix
 -------------
 1.69e+00
 1.40e-02
 2.80e-03</pre></div><p class="calibre11">These three numbers correspond to the intercept, the slope for height, and the slope for age, respectively. To determine whether our model has significantly improved by this new data, we could calculate the <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> value of our new model and compare it to the earlier one.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Multiple R-squared"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec73" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Multiple R-squared</h1></div></div></div><p class="calibre11">While calculating <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> <a id="id351" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>previously, we saw how it was the amount of variance explained by the model:</p><div class="mediaobject"><img src="Images/7180OS_03_55.jpg" alt="Multiple R-squared" class="calibre139"/></div><p class="calibre11">Since the variance is the mean squared error, we can multiply both the <span class="strong1"><em class="calibre13">var(ε)</em></span> and <span class="strong1"><em class="calibre13">var(y)</em></span> terms by the sample size and arrive at the following alternative equation for R<sup class="calibre42">2</sup>:</p><div class="mediaobject"><img src="Images/7180OS_03_56.jpg" alt="Multiple R-squared" class="calibre140"/></div><p class="calibre11">This is simply the sum of squared residuals over the sum of squared differences from the mean. Incanter contains <a id="id352" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the <code class="literal">incanter.core/sum-of-squares</code> function that makes this very simple to express:</p><div class="calibre2"><pre class="programlisting">(defn r-squared [coefs x y]
   (let [fitted      (i/mmult x coefs)
         residuals   (i/minus y fitted)
         differences (i/minus y (s/mean y))
         rss         (i/sum-of-squares residuals)
         ess         (i/sum-of-squares differences)]
     (- 1 (/ rss ess))))</pre></div><p class="calibre11">We use the variable names <code class="literal">rss</code> for <span class="strong1"><strong class="calibre12">residual sum of squares</strong></span> and <code class="literal">ess</code> for <span class="strong1"><strong class="calibre12">explained sum of squares</strong></span>. We can calculate the matrix <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> for our new model as follows:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-21 []
  (let [data (swimmer-data)
        x (-&gt;&gt; (feature-matrix ["Height, cm" "Age"] data)
               (add-bias))
        y (-&gt;&gt; (i/$ "Weight" data)
               (i/log)
               (i/matrix))
        beta (normal-equation x y)]
    (r-squared beta x y)))</pre></div><p class="calibre11">This yields the value <code class="literal">0.757</code>. Our <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> value has increased by a small amount by including the age value. Because we have used multiple independent variables, <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> is now called the <span class="strong1"><strong class="calibre12">coefficient of </strong></span><a id="id353" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">multiple determination</strong></span>.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Adjusted R-squared"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec74" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Adjusted R-squared</h1></div></div></div><p class="calibre11">As we add more <a id="id354" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>independent variables to our regression, we might be encouraged by the fact that our <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> value always increases. Adding a new independent variable isn't going to make it harder to predict the dependent variable—if the new variable has no explanatory power, then its coefficient will simply be zero and the R<sup class="calibre42">2</sup> will remain the same as it was without the independent variable.</p><p class="calibre11">However, this doesn't tell us whether a model has been improved by the addition of a new variable. If we want to know whether our new variable is really helping it to generate a better fit, we can use the adjusted <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span>, often written as <span class="inlinemediaobject"><img src="Images/7180OS_03_57.jpg" alt="Adjusted R-squared" class="calibre141"/></span> and pronounced as "R-bar squared." Unlike <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span>, <span class="inlinemediaobject"><img src="Images/7180OS_03_57.jpg" alt="Adjusted R-squared" class="calibre141"/></span> will only increase if the new independent variable increases <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> more than would be expected due to chance:</p><div class="calibre2"><pre class="programlisting">(defn matrix-adj-r-squared [coefs x y]
  (let [r-squared (matrix-r-squared coefs x y)
        n (count y)
        p (count coefs)]
    (- 1
       (* (- 1 r-squared)
          (/ (dec n)
             (dec (- n p)))))))</pre></div><p class="calibre11">The adjusted <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> depends on two additional parameters, <span class="strong1"><em class="calibre13">n</em></span> and <span class="strong1"><em class="calibre13">p</em></span>, corresponding to the sample size and <a id="id355" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of model parameters, respectively:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-22 []
  (let [data (swimmer-data)
        x (-&gt;&gt; (feature-matrix ["Height, cm" "Age"] data)
               (add-bias))
        y (-&gt;&gt; (i/$ "Weight" data)
               (i/log)
               (i/matrix))
        beta (normal-equation x y)]
    (adj-r-squared beta x y)))</pre></div><p class="calibre11">This example returns a value of <code class="literal">0.756</code>. This is still greater than the original model, so age certainly carries some explanatory power.</p><div class="calibre2" title="Incanter's linear model"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec55" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Incanter's linear model</h2></div></div></div><p class="calibre11">While implementing <a id="id356" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>our own version of the normal equation and <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> provides a valuable opportunity to introduce matrix algebra, it's important to note that Incanter provides the <code class="literal">incanter.stats/linear-model</code> function that does everything we've covered and more.</p><p class="calibre11">The function expects to be called with <span class="strong1"><em class="calibre13">y</em></span> and <span class="strong1"><em class="calibre13">x</em></span> (as either sequences or, in the case of multiple regression, matrices). We can also pass in an optional keyword argument—<code class="literal">intercept</code> with a Boolean value—indicating whether we'd like Incanter to add the intercept term for us. The function will return a map containing the coefficients of the linear model—<code class="literal">:coefs</code> and the fitted data—<code class="literal">:fitted</code>, as well as <code class="literal">:residuals</code>, <code class="literal">:r-square</code>, and <code class="literal">:adj-r-square</code>, amongst others.</p><p class="calibre11">It will also return significance tests and 95 percent confidence intervals for the coefficients as the <code class="literal">:t-probs</code> and <code class="literal">:coefs-ci</code> keys, respectively, as well as the <code class="literal">:f-prob</code> keys, corresponding to a significance test on the regression model as a whole.</p><div class="calibre2" title="The F-test of model significance"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch03lvl3sec01" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The F-test of model significance</h3></div></div></div><p class="calibre11">The <code class="literal">:f-prob</code> key <a id="id357" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>returned by <code class="literal">linear-model</code> is a significance test of the entire model using an <span class="strong1"><em class="calibre13">F</em></span>-test. As we discovered in the previous chapter, an <span class="strong1"><em class="calibre13">F</em></span>-test is <a id="id358" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>appropriate when performing multiple significance tests at once. In the case of multiple linear regression, we are testing whether any of the coefficients of the model, except for the intercept term, are statistically indistinguishable from zero.</p><p class="calibre11">Our null and alternate hypotheses are therefore:</p><div class="mediaobject"><img src="Images/7180OS_03_58.jpg" alt="The F-test of model significance" class="calibre142"/></div><div class="mediaobject"><img src="Images/7180OS_03_59.jpg" alt="The F-test of model significance" class="calibre143"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">j</em></span> is some index in the parameter's vector excluding the intercept. The <span class="strong1"><em class="calibre13">F</em></span>-statistic we calculate is the ratio of explained variance over the unexplained (residual) variance. This can be <a id="id359" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>expressed as the <span class="strong1"><strong class="calibre12">mean square model</strong></span> (<span class="strong1"><strong class="calibre12">MSM</strong></span>) over the <span class="strong1"><strong class="calibre12">mean </strong></span><a id="id360" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">square error</strong></span> (<span class="strong1"><strong class="calibre12">MSE</strong></span>):</p><div class="mediaobject"><img src="Images/7180OS_03_60.jpg" alt="The F-test of model significance" class="calibre144"/></div><p class="calibre11">The MSM is equal <a id="id361" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to the <span class="strong1"><strong class="calibre12">explained sum of squares</strong></span> (<span class="strong1"><strong class="calibre12">ESS</strong></span>) divided by the model degree of freedom, where the model degree of freedom is the number of parameters in the model excluding the intercept term. The MSE is equal to the <span class="strong1"><strong class="calibre12">sum of </strong></span><a id="id362" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">residual squares</strong></span> (<span class="strong1"><strong class="calibre12">RSS</strong></span>) divided by the residual degree of freedom, where the residual degree of freedom is the size of the sample minus the <a id="id363" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of model parameters.</p><p class="calibre11">Once we've calculated the <span class="strong1"><em class="calibre13">F</em></span>-statistic, we look it up in an <span class="strong1"><em class="calibre13">F</em></span>-distribution parameterized by the same two degrees of freedom:</p><div class="calibre2"><pre class="programlisting">(defn f-test [y x]
  (let [coefs       (normal-equation x y)
        fitted      (i/mmult x coefs)
        difference  (i/minus fitted (s/mean y))
        residuals   (i/minus y fitted)
        ess         (i/sum-of-squares difference)
        rss         (i/sum-of-squares residuals)
        p           (i/ncol x)
        n           (i/nrow y)
        df1         (- p 1)
        df2         (- n p)
        msm         (/ ess df1)
        mse         (/ rss df2)
        f-stat      (/ msm mse)]
    (s/cdf-f f-stat :df1 df1 :df2 df2 :lower-tail? false)))

(defn ex-3-23 []
  (let [data (swimmer-data)
        x (-&gt;&gt; (feature-matrix ["Height, cm" "Age"] data)
               (add-bias))
        y (-&gt;&gt; (i/$ "Weight" data)
               (i/log))
        beta (:coefs (s/linear-model y x :intercept false))]
    (f-test beta x y)))</pre></div><p class="calibre11">The test returns a result of <code class="literal">1.11x10e-16</code>. This is a tiny number; as a result, we can be certain that the model is significant.</p><p class="calibre11">Note that with smaller samples of data, the <span class="strong1"><em class="calibre13">F</em></span>-test quantifies increasing uncertainty that a linear model is appropriate. With a random sample of five, for example, the data sometimes shows barely any linear relationship at all and the <span class="strong1"><em class="calibre13">F</em></span>-test judges the data insignificant at even a 50 percent confidence interval.</p></div></div><div class="calibre2" title="Categorical and dummy variables"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec56" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Categorical and dummy variables</h2></div></div></div><p class="calibre11">We might attempt <a id="id364" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>at this point to include <code class="literal">"Sex"</code> as a feature in our regression analysis, but we'll encounter a problem. The input is expressed as <code class="literal">"M"</code> or <code class="literal">"F"</code> rather than a number. This is an example of a categorical variable: a variable that can take one of a finite set of values that are unordered and (usually) not numeric. Other examples of categorical variables are the sport that the athlete participates in or the particular event in which they are most proficient.</p><p class="calibre11">Ordinary least squares relies on a numerical value of residual distance to minimize. What could the numeric distance between swimming and athletics be? This might imply that it is impossible to include categorical variables in our regression equation.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note33" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Categorical or nominal variables are distinct from continuous variables, because they don't sit on the number line. Sometimes categories are represented by numbers like for ZIP codes, but we shouldn't assume that numeric categories are necessarily ordered or that the interval between categories are equal.</p></div></div><p class="calibre11">Fortunately, many categorical variables can be considered dichotomies and, in fact, our sample data contains two categories for <code class="literal">sex</code>. These can be included in our regression model provided we transform them into two numbers, for example, zero and one.</p><p class="calibre11">When a category such as sport takes on more than two values, we could include an independent variable for each type of sport. We would create a variable for swimming and another for weightlifting, and so on. The value of swimming would be one for swimmers and zero otherwise.</p><p class="calibre11">Since <code class="literal">sex</code> might be a useful explanatory variable for our regression model, let's convert female to <code class="literal">0</code> and male to <code class="literal">1</code>. We can add a derived column containing our dummy variable using Incanter's <code class="literal">incanter.core/add-derived-column</code> function.</p><p class="calibre11">Let's calculate our <span class="inlinemediaobject"><img src="Images/7180OS_03_57.jpg" alt="Categorical and dummy variables" class="calibre141"/></span> value to see if it has improved:</p><div class="calibre2"><pre class="programlisting">(defn dummy-mf [sex]
  (if (= sex "F")
    0.0 1.0))

(defn ex-3-25 []
  (let [data (-&gt;&gt; (swimmer-data)
                  (i/add-derived-column "Dummy MF"
                                        ["Sex"]
                                        dummy-mf))
        x (-&gt;&gt; data
               (feature-matrix ["Height, cm"
                                "Age"
                                "Dummy MF"])
               (add-bias))
        y (-&gt;&gt; (i/$ "Weight" data)
               (i/log)
               (i/matrix))
        beta (normal-equation x y)]
    (adj-r-squared beta x y)))</pre></div><p class="calibre11">The code yields the <a id="id365" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>value <code class="literal">0.809</code>. Using the height, age, and gender features, we <a id="id366" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have successfully explained over 80 percent of the variance in weight of our Olympic swimmers.</p></div><div class="calibre2" title="Relative power"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec57" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Relative power</h2></div></div></div><p class="calibre11">At this point, it might <a id="id367" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>be useful to ask what is the most important feature to explain the observed weight: is it age, gender, or height? We could make use of our adjusted <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> and see how much the value changes, but this would require us to re-run the regression for each variable we want to test.</p><p class="calibre11">We can't look at the magnitude of the coefficients, because the ranges of the data they apply to are vastly different: height in centimeters, age in years, and gender measured as a dummy variable in the range zero to one.</p><p class="calibre11">In order to compare the relative contributions of the coefficients, we can calculate the standardized regression coefficient, or beta weight.</p><div class="mediaobject"><img src="Images/7180OS_03_62.jpg" alt="Relative power" class="calibre145"/></div><p class="calibre11">To calculate the beta weight we multiply each coefficient by the ratio of the standard deviations for the associated independent variable and the model's dependent variable. This can be accomplished with the following Clojure code:</p><div class="calibre2"><pre class="programlisting">(defn beta-weight [coefs x y]
  (let [sdx (map s/sd (i/trans x))
        sdy (s/sd y)]
    (map #(/ (* %1 %2) sdy) sdx coefs)))

(defn ex-3-26 []
  (let [data (-&gt;&gt; (swimmer-data)
                  (i/add-derived-column "Dummy MF"
                                        ["Sex"]
                                        dummy-mf))
        x (-&gt;&gt; data
               (feature-matrix ["Height, cm"
                                "Age"
                                "Dummy MF"])
               (add-bias))
        y (-&gt;&gt; (i/$ "Weight" data)
               (i/log)
               (i/matrix))
        beta (normal-equation x y)]
    (beta-weight beta x y)))</pre></div><p class="calibre11">This outputs (rounded to three decimal places):</p><div class="calibre2"><pre class="programlisting">(0.0 0.650 0.058 0.304)</pre></div><p class="calibre11">This indicates that height is the most important explanatory variable, followed by gender and then age. Transforming <a id="id368" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>it into standardized coefficients tells us that with an increase of one standard deviation in height, the mean weight increases by <code class="literal">0.65</code> standard deviations.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Collinearity"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec75" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Collinearity</h1></div></div></div><p class="calibre11">We might try <a id="id369" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>at this point to keep adding features to our model in an attempt to increase its explanatory power.</p><p class="calibre11">For example, we also have a <code class="literal">"Date of birth"</code> column and we may be tempted to try and include this too. It is a date, but we could easily convert it into a number suitable for use in regression. We could do this simply by extracting the year from their birth date using the <code class="literal">clj-time</code> library:</p><div class="calibre2"><pre class="programlisting">(defn to-year [str]
  (-&gt; (coerce/from-date str)
      (time/year)))

(defn ex-3-27 []
  (let [data (-&gt;&gt; (swimmer-data)
                  (i/add-derived-column "Dummy MF"
                                        ["Sex"]
                                        dummy-mf)
                  (i/add-derived-column "Year of birth"
                                        ["Date of birth"]
                                        to-year))
        x (-&gt;&gt; data
               (feature-matrix ["Height, cm"
                                "Age"
                                "Dummy MF"
                                "Year of birth"])
               (add-bias))
        y (-&gt;&gt; (i/$ "Weight" data)
               (i/log)
               (i/matrix))
        beta (normal-equation x y)]
    (beta-weight beta x y)))

;; (-0.0 0.650 0.096 0.304 0.038)</pre></div><p class="calibre11">The new "Year of Birth" feature has a beta weight of only <code class="literal">0.038</code>, less than the weight of the age feature we calculated earlier. However, the age weight of the age feature is now showing a value of <code class="literal">0.096</code>. Its relative importance has increased by over 65 percent since we added <code class="literal">"Year of birth"</code> as a feature. The fact that the addition of a new feature has altered the importance of an existing feature indicates that we have a problem.</p><p class="calibre11">By including the <a id="id370" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>additional <code class="literal">"Year of birth"</code> parameter, we have inadvertently broken a rule of the regression estimator. Let's see why:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-28 []
  (let [data (-&gt;&gt; (swimmer-data)
                  (i/add-derived-column "Year of birth"
                                        ["Date of birth"]
                                        to-year))
        x (-&gt;&gt; (i/$ "Age" data)
               (map (jitter 0.5)))
        y (i/$ "Year of birth" data)]
    (-&gt; (c/scatter-plot x y
                        :x-label "Age"
                        :y-label "Year of birth")
        (i/view))))</pre></div><p class="calibre11">The following scatter plot shows the age of swimmers (with jittering) plotted against their year of birth. As you would expect, the two variables are very closely correlated:</p><div class="mediaobject"><img src="Images/7180OS_03_260.jpg" alt="Collinearity" class="calibre45"/></div><p class="calibre11">The two features are so highly correlated that the algorithm is unable to determine which of them best explains the observed changes in <span class="strong1"><em class="calibre13">y</em></span><a id="id371" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>. This is an undesirable issue when we deal with multivariate linear regression called <span class="strong1"><strong class="calibre12">collinearity</strong></span>.</p><div class="calibre2" title="Multicollinearity"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec58" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Multicollinearity</h2></div></div></div><p class="calibre11">For multiple regression to produce the best coefficient estimates, the underlying data must conform to the <a id="id372" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>same assumptions as simple regression plus one additional assumption— the absence of perfect <span class="strong1"><strong class="calibre12">multicollinearity</strong></span>. This means that the independent variables should not be exactly linearly correlated with each other.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note34" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">In practice, independent variables are often collinear in some way. Consider, for example, that age and height or gender and height are themselves correlated with each other. It's only when this condition becomes extreme that serious coefficient errors can arise.</p></div></div><p class="calibre11">If the independent variables are, in fact, not independent, then linear regression can't determine the relative <a id="id373" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>contribution of each independent variable. If two features are so strongly correlated that they always vary together, how can the algorithm distinguish their relative importance? As a result, there may be high variance in the coefficient estimates and a high standard error.</p><p class="calibre11">We've already seen one symptom of high multicollinearity: regression coefficients that change significantly when independent variables are added or removed from the equation. Another symptom is when there is an insignificant coefficient in a multiple regression for a particular independent variable, but a substantial <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> for the simple regression model using the same independent variable.</p><p class="calibre11">While these offer clues of multicollinearity, to confirm, we must look directly at the intercorrelation of the independent variables. One way to determine the intercorrelation is to examine the correlation between each of the independent variables, looking for coefficients of 0.8 or more. While this simple approach often works, it may fail to take into account situations where an independent variable has a linear relationship with the other variables taken together.</p><p class="calibre11">The surest method to assess multicollinearity is to regress each independent variable on all the other independent variables. When any of the <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> from these equations is near 1.0, there is high-multicollinearity. In fact, the largest of these <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> serves as an indicator of the degree of multicollinearity that exists.</p><p class="calibre11">Once identified, there are several ways to address multicollinearity:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Increase the sample size. More data can produce more precise parameter estimates with smaller standard errors.</li><li class="listitem">Combine the features into one. If you have several features that measure essentially the same attribute, find a way to unify them into a single feature.</li><li class="listitem">Discard the offending variable(s).</li><li class="listitem">Limit the equation of prediction. Collinearity affects the coefficients of the model, but the result may still be a good fit for the data.</li></ul></div><p class="calibre11">Since age and year of birth carry essentially the same information, we may as well discard one. We can easily see which of the two contains more explanatory power by calculating the bivariate regression for each feature and the dependent variable.</p><p class="calibre11">"Age" <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> = 0.1049, whereas "Year of birth" <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> = 0.1050.</p><p class="calibre11">As expected, there is virtually no difference between the two features, both explaining around 10 percent of <a id="id374" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the variance in weight. Since the year of birth marginally explains marginally more of the variance, we'll keep it and discard the age feature.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Prediction"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec76" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Prediction</h1></div></div></div><p class="calibre11">Finally, we arrive at <a id="id375" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>one of the most important uses of linear regression: prediction. We've trained a model capable of predicting the weight of Olympic swimmers given the data about their height, gender, and year of birth.</p><p class="calibre11">Mark Spitz is a nine-time Olympic swimming champion, and he won seven gold medals at the 1972 Olympics. He was born in 1950 and, according to his Wikipedia page, is 183cm tall and weighs 73kg. Let's see what our model predicts as his weight.</p><p class="calibre11">Our multiple regression model requires these values to be presented as a matrix form. Each of the parameters needs to be provided in the order in which the model learned the features so that the correct coefficient is applied. After the bias term, our feature vector needs to contain height, gender, and year of birth in the same units as our model was trained:</p><div class="mediaobject"><img src="Images/7180OS_03_63.jpg" alt="Prediction" class="calibre146"/></div><p class="calibre11">Our <span class="strong1"><em class="calibre13">β</em></span> matrix contains the coefficients for each of these features:</p><div class="mediaobject"><img src="Images/7180OS_03_64.jpg" alt="Prediction" class="calibre147"/></div><p class="calibre11">The prediction of our model will be the sum of the products of the <span class="strong1"><em class="calibre13">β</em></span> coefficients and features <span class="strong1"><em class="calibre13">x</em></span> for each row:</p><div class="mediaobject"><img src="Images/7180OS_03_65.jpg" alt="Prediction" class="calibre23"/></div><p class="calibre11">Since matrix multiplication produces each element by adding up the products of the rows and columns of each matrix respectively, producing our result is as simple as multiplying the <a id="id376" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>transpose of <span class="strong1"><em class="calibre13">β</em></span> with the <span class="strong1"><em class="calibre13">x<sub class="calibre25">spitz</sub></em></span> vector.</p><p class="calibre11">Recall that the dimensions of the resulting matrix will be the number of rows from the first matrix and the number of columns from the second matrix:</p><div class="mediaobject"><img src="Images/7180OS_03_66.jpg" alt="Prediction" class="calibre148"/></div><p class="calibre11"><span class="inlinemediaobject"><img src="Images/7180OS_03_67.jpg" alt="Prediction" class="calibre149"/></span> is a product of a <span class="inlinemediaobject"><img src="Images/7180OS_03_68.jpg" alt="Prediction" class="calibre150"/></span> matrix and an <span class="inlinemediaobject"><img src="Images/7180OS_03_69.jpg" alt="Prediction" class="calibre150"/></span> matrix. The result is a <span class="inlinemediaobject"><img src="Images/7180OS_03_70.jpg" alt="Prediction" class="calibre151"/></span> matrix:</p><div class="mediaobject"><img src="Images/7180OS_03_270.jpg" alt="Prediction" class="calibre152"/></div><p class="calibre11">Calculating this in <a id="id377" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>code is very simple:</p><div class="calibre2"><pre class="programlisting">(defn predict [coefs x]
  (-&gt; (i/trans coefs)
      (i/mmult x)
      (first)))</pre></div><p class="calibre11">We call <code class="literal">first</code> to return the first (and only) element from the matrix rather than the matrix itself:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-29 []
  (let [data (-&gt;&gt; (swimmer-data)
                  (i/add-derived-column "Dummy MF"
                                        ["Sex"]
                                        dummy-mf)
                  (i/add-derived-column "Year of birth"
                                        ["Date of birth"]
                                        to-year))
        x (-&gt;&gt; data
               (feature-matrix ["Height, cm"
                                "Dummy MF"
                                "Year of birth"])
               (add-bias))
        y (-&gt;&gt; (i/$ "Weight" data)
               (i/log)
               (i/matrix))
        beta (normal-equation x y)
        xspitz (i/matrix [1.0 183 1 1950])]
    (i/exp (predict beta xspitz))))</pre></div><p class="calibre11">This returns <code class="literal">84.21</code>, corresponding to a expected weight of 84.21 kg. This is much heavier than Mark Spitz's reported weight of 73 kg. Our model doesn't appear to have performed very well.</p><div class="calibre2" title="The confidence interval of a prediction"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec59" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The confidence interval of a prediction</h2></div></div></div><p class="calibre11">We previously <a id="id378" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculated confidence intervals for population parameters. It's also possible to construct confidence intervals for a specific prediction called <a id="id379" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">prediction interval</strong></span>. The prediction interval quantifies the amount of uncertainty in the prediction by providing a minimum and a maximum value between which the true value is expected to fall with a certain probability.</p><p class="calibre11">The prediction interval for <span class="inlinemediaobject"><img src="Images/7180OS_03_23.jpg" alt="The confidence interval of a prediction" class="calibre106"/></span> is wider than the confidence interval for a population parameter such as <span class="strong1"><em class="calibre13">µ</em></span>, the mean. This is because the confidence interval simply needs to account for our uncertainty in estimating the mean, while the prediction interval must also take into account the variance of <span class="strong1"><em class="calibre13">y</em></span> from the mean.</p><div class="mediaobject"><img src="Images/7180OS_03_280.jpg" alt="The confidence interval of a prediction" class="calibre153"/></div><p class="calibre11">The previous image <a id="id380" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>shows the relationship between the outer prediction interval and the inner confidence interval. We can calculate the prediction interval using the following formula:</p><div class="mediaobject"><img src="Images/7180OS_03_71.jpg" alt="The confidence interval of a prediction" class="calibre154"/></div><p class="calibre11">Here, <span class="inlinemediaobject"><img src="Images/7180OS_03_72.jpg" alt="The confidence interval of a prediction" class="calibre73"/></span> is the prediction, plus or minus the interval. We're making use of the <span class="strong1"><em class="calibre13">t</em></span>-distribution, where the degree of freedom is <span class="inlinemediaobject"><img src="Images/7180OS_03_73.jpg" alt="The confidence interval of a prediction" class="calibre155"/></span>, the sample size minus the number of parameters. This is the same as we calculated for the <span class="strong1"><em class="calibre13">F</em></span>-test previously. While the formula may look intimidating, it's relatively straightforward to translate into the code shown in the following example, which calculates the 95 percent prediction interval:</p><div class="calibre2"><pre class="programlisting"> (defn prediction-interval [x y a]
  (let [xtx    (i/mmult (i/trans x) x)
        xtxi   (i/solve xtx)
        xty    (i/mmult (i/trans x) y)
        coefs  (i/mmult xtxi xty)
        fitted (i/mmult x coefs)
        resid  (i/minus y fitted)
        rss    (i/sum-of-squares resid)
        n      (i/nrow y)
        p      (i/ncol x)
        dfe    (- n p)
        mse    (/ ssr dfe)
        se-y   (first (i/mmult (i/trans a) xtxi a))
        t-stat (i/sqrt (* mse (+ 1 se-y)))]
    (* (s/quantile-t 0.975 :df dfe) t-stat)))</pre></div><p class="calibre11">Since the <span class="strong1"><em class="calibre13">t</em></span>-statistic is parameterized by the degree of freedom of the error, it takes into account the uncertainty present in the model.</p><p class="calibre11">If we'd like to <a id="id381" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculate the confidence interval for the mean instead of the prediction interval, we can simply omit the addition of one to <code class="literal">se-y</code> while calculating <code class="literal">t-stat</code>.</p><p class="calibre11">The preceding code can be used to generate the following chart, showing how the prediction interval varies with the value of the independent variable:</p><div class="mediaobject"><img src="Images/7180OS_03_290.jpg" alt="The confidence interval of a prediction" class="calibre45"/></div><p class="calibre11">In the preceding graph, a model trained on a sample size of five shows how the 95 percent prediction interval <a id="id382" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>increases as we move further from the mean height. Applying the previous formula to Mark Spitz yields the following:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-30 []
  (let [data (-&gt;&gt; (swimmer-data)
                  (i/add-derived-column "Dummy MF"
                                        ["Sex"]
                                        dummy-mf)
                  (i/add-derived-column "Year of birth"
                                        ["Date of birth"]
                                        to-year))
        x (-&gt;&gt; data
               (feature-matrix ["Height, cm"
                                "Dummy MF"
                                "Year of birth"])
               (add-bias))
        y (-&gt;&gt; (i/$ "Weight" data)
               (i/log)
               (i/matrix))
        xspitz (i/matrix [1.0 183 1 1950])]
    (i/exp (prediction-interval x y xspitz))))</pre></div><p class="calibre11">This returns the range from 72.7 kg to 97.4 kg. This range just includes Mark's weight of 73 kg, so our <a id="id383" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>prediction is within the 95 percent prediction interval. It's uncomfortably close to the bounds though.</p></div><div class="calibre2" title="Model scope"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec60" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Model scope</h2></div></div></div><p class="calibre11">Mark Spitz was born in <a id="id384" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>1950, decades before even the oldest swimmer in the 2012 Olympic Games. By trying to predict Mark's weight using his year of birth, we're guilty of trying to extrapolate too far beyond our training data. We have exceeded the scope of our model.</p><p class="calibre11">There is a second way in which this is problematic. Our data was based entirely on swimmers currently competing at international standard, whereas Mark has not competed for many years. In other words, Mark is now not a part of the population we have trained our model on. To fix both of these problems, we need to look up Mark's details from 1979, when he was a competition swimmer.</p><p class="calibre11">According to <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.topendsports.com/athletes/swimming/spitz-mark.htm">http://www.topendsports.com/athletes/swimming/spitz-mark.htm</a>, in 1972, 22-year-old Mark Spitz was 185 cm tall and he weighed 79 kg.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note35" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Selecting the right features is one of the most important prerequisites to get good results from any predictive algorithm.</p></div></div><p class="calibre11">You should strive to select features not only on the basis of their predictive power, but also on their relevance to the domain being modeled.</p></div><div class="calibre2" title="The final model"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec61" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The final model</h2></div></div></div><p class="calibre11">Although it has a <a id="id385" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>slightly lower <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span>, let's retrain our model with age in place of year of birth as a feature. This will allow us to easily predict weights for past and future unseen data, as it models more closely the variable we suspect of having a causal relationship with weight.</p><p class="calibre11">This yields <span class="strong1"><em class="calibre13">β</em></span> of approximately:</p><div class="mediaobject"><img src="Images/7180OS_03_74.jpg" alt="The final model" class="calibre147"/></div><p class="calibre11">Our features for <a id="id386" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Mark in the 1972 games are:</p><div class="mediaobject"><img src="Images/7180OS_03_75.jpg" alt="The final model" class="calibre156"/></div><p class="calibre11">We can use them to predict his competitive weight with the following code:</p><div class="calibre2"><pre class="programlisting">(defn ex-3-32 []
  (let [data (-&gt;&gt; (swimmer-data)
                  (i/add-derived-column "Dummy MF"
                                        ["Sex"]
                                        dummy-mf))
        x (-&gt;&gt; data
               (feature-matrix ["Height, cm"
                                "Dummy MF"
                                "Age"])
               (add-bias))
        y (-&gt;&gt; (i/$ "Weight" data)
               (i/log)
               (i/matrix))
        beta (normal-equation x y)
        xspitz (i/matrix [1.0 185 1 22])]
    (i/exp (predict beta xspitz))))</pre></div><p class="calibre11">This returns <code class="literal">78.47</code>, corresponding to a prediction of 78.47 kg. This is now very close to Mark's true competition weight of 79 kg.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec77" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we've learned about how to determine whether two or more variables share a linear relationship. We've seen how to express the strength of their correlation with <span class="strong1"><em class="calibre13">r</em></span> and how well a linear model explains the variance with <span class="strong1"><em class="calibre13">R<sup class="calibre42">2</sup></em></span> and <span class="inlinemediaobject"><img src="Images/7180OS_03_57.jpg" alt="Summary" class="calibre141"/></span>. We've also performed hypothesis tests and calculated confidence intervals to infer the range of the true population parameter for correlation, <span class="inlinemediaobject"><img src="Images/7180OS_03_08.jpg" alt="Summary" class="calibre89"/></span>.</p><p class="calibre11">Having established a correlation between variables, we were able to build a predictive model using ordinary least squares regression and simple Clojure functions. We then generalized our approach using Incanter's matrix functionality and the normal equation. This simple model demonstrated the principles of machine learning by determining the model parameters <span class="strong1"><em class="calibre13">β</em></span>, inferred from our sample data, that could be used to make predictions. Our model was able to predict an expected weight for a new athlete that fell well within the prediction interval of the true value.</p><p class="calibre11">In the next chapter, we'll see how similar techniques can be used to classify data into discrete classes. We'll demonstrate a variety of different approaches particular to classification as well as introduce a very general technique for parameter optimization that works for a variety of machine learning models, including linear regression.</p></div></div>



  </body></html>