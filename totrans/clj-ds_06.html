<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 6. Clustering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch06" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 6. Clustering</h1></div></div></div><div class="calibre2"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote1" summary="Block quote"><tr class="calibre17"><td class="calibre18"> </td><td class="calibre18"><p class="calibre19"><span class="strong1"><em class="calibre13">Things that have a common quality ever quickly seek their kind.</em></span></p></td><td class="calibre18"> </td></tr><tr class="calibre17"><td class="calibre18"> </td><td colspan="2" class="calibre20">--<span class="strong1"><span class="strong1"><em class="calibre13">Marcus Aurelius</em></span></span></td></tr></table></div><p class="calibre11">In previous chapters, we covered multiple learning algorithms: linear and logistic regression, C4.5, naive Bayes, and random forests. In each case we were required to train the algorithm by providing features and a desired output. In linear regression, for example, the desired output was the weight of an Olympic swimmer, whereas for the other algorithms we provided a class: whether the passenger survived or perished. These are examples of <a id="id676" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">supervised learning algorithms</strong></span>: we tell our algorithm the desired output and it will attempt to learn a model that reproduces it.</p><p class="calibre11">There is another class <a id="id677" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of learning algorithm referred to as <span class="strong1"><strong class="calibre12">unsupervised learning</strong></span>. Unsupervised algorithms are able to operate on the data without a set of reference answers. We may not even know ourselves what structure lies within the data; the algorithm will attempt to determine the structure for itself.</p><p class="calibre11">Clustering is an example of an unsupervised learning algorithm. The results of cluster analysis are groupings of input data that are more similar to each other in some way. The technique is general: any set entities that have a conceptual similarity or distance from each other can be clustered. For example, we could cluster groups of social media accounts by similarity in terms of shared followers, or we could cluster the results of market research by measuring the similarity of respondents' answers to a questionnaire.</p><p class="calibre11">One common application of clustering is to identify documents that share similar subject matter. This provides us with an ideal opportunity to talk about text processing, and this chapter will introduce a variety of techniques specific to dealing with text.</p><div class="calibre2" title="Downloading the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec102" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Downloading the data</h1></div></div></div><p class="calibre11">This chapter <a id="id678" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>makes use of the <span class="strong1"><strong class="calibre12">Reuters-21578</strong></span> dataset: a venerable collection of articles that were published on the Reuters newswire in 1987. It is one of the most widely used for testing the categorization and classification of text. The copyright for the text of articles and annotations in the Reuters-21578 collection resides with Reuters Ltd. Reuters Ltd. and Carnegie Group, Inc. have agreed to allow the free distribution of this data for research purposes only.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note52" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">You can download the example code for this chapter from the Packt Publishing's website or from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clojuredatascience/ch6-clustering">https://github.com/clojuredatascience/ch6-clustering</a>.</p></div></div><p class="calibre11">As usual, within the <a id="id679" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sample code is a script to download and unzip the files to the data directory. You can run it from within the project directory with the following command:</p><div class="calibre2"><pre class="programlisting">
<span class="strong1"><strong class="calibre12">script/download-data.sh</strong></span>
</pre></div><p class="calibre11">Alternatively, at the <a id="id680" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>time of writing, the Reuters dataset can be downloaded from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz">http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz</a>. The rest of this chapter will assume that the files have been downloaded and installed to the project's data directory.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Extracting the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec103" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Extracting the data</h1></div></div></div><p class="calibre11">After you run the <a id="id681" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>preceding script, the articles will be unzipped to the directory <code class="literal">data/reuters-sgml</code>. Each <code class="literal">.sgm</code> file in the extract contains around 1,000 short articles that have been wrapped in XML-style tags <a id="id682" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>using Standard Generalized Markup Language (SGML). Rather than write our own parser for the format, we can make use of the one already written in the Lucene text indexer.</p><div class="calibre2"><pre class="programlisting">(:import [org.apache.lucene.benchmark.utils ExtractReuters])

(defn sgml-&gt;txt [in-path out-path]
  (let [in-file  (clojure.java.io/file in-path)
        out-file (clojure.java.io/file out-path)]
    (.extract (ExtractReuters. in-file out-file))))</pre></div><p class="calibre11">Here we're making use of Clojure's Java interop to simply call the extract method on Lucene's <code class="literal">ExtractReuters</code> class. Each article is extracted as its own text file.</p><p class="calibre11">This code can be run by executing:</p><div class="calibre2"><pre class="programlisting">
<span class="strong1"><strong class="calibre12">lein extract-reuters</strong></span>
</pre></div><p class="calibre11">on the command line within the project directory. The output will be a new directory, <code class="literal">data/reuters-text</code>, containing over 20,000 individual text files. Each file contains a single Reuters newswire article.</p><p class="calibre11">If you're short on disk space you can delete the <code class="literal">reuters-sgml</code> and <code class="literal">reuters21578.tar.gz</code> files now: the contents of the <code class="literal">reuters-text</code> directory are the only files we will be <a id="id683" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>using in this chapter. Let's look at a few now.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Inspecting the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec104" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Inspecting the data</h1></div></div></div><p class="calibre11">The year 1987 <a id="id684" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>was the year of "Black Monday". On 19th October stock markets around the world crashed and the Dow Jones Industrial Average declined 508 points to 1738.74. Articles such as the one contained in <code class="literal">reut2-020.sgm-962.txt</code> describe the event:</p><div class="calibre2"><pre class="programlisting">19-OCT-1987 16:14:37.57

WALL STREET SUFFERS WORST EVER SELLOFF

Wall Street tumbled to its worst point loss ever and the worst percentage decline since the First World War as a frenzy of stock selling stunned even the most bearish market participants. "Everyone is in awe and the word 'crash' is on everyone's mind," one trader said.     The Dow Jones industrial average fell 508 points to 1738, a level it has not been at since the Autumn of 1986.     Volume soared to 603 mln shares, almost doubling the previous record of 338 mln traded just last Friday. Reuter &amp;#3;</pre></div><p class="calibre11">The structure of this article is representative of the majority of articles in the corpus. The first line is the timestamp indicating when the article was published, followed by a blank line. The article has a headline which is often—but not always—in upper case, and then another blank line. Finally comes the article body text. As is often the case when working with semi-structured text such as this, there are multiple spaces, odd characters, and abbreviations.</p><p class="calibre11">Other articles are simply headlines, for example in <code class="literal">reut2-020.sgm-761.txt</code>:</p><div class="calibre2"><pre class="programlisting">20-OCT-1987 17:09:34.49
REAGAN SAYS HE SEES NO RECESSION</pre></div><p class="calibre11">These are the files on which we will be performing our cluster analysis.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Clustering text"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec105" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Clustering text</h1></div></div></div><p class="calibre11">Clustering is the <a id="id685" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>process of finding groups of objects that are similar to each other. The goal is that objects within a cluster should be more similar to each other than to objects in other clusters. Like classification, it is not a specific algorithm so much as a general class of algorithms that solve a general problem.</p><p class="calibre11">Although there are a variety of clustering algorithms, all rely to some extent on a distance measure. For an algorithm to determine whether two objects belong in the same or different clusters it must be able to determine a quantitative measure of the distance (or, if you prefer, the similarity) between them. This calls for a numeric measure of distance: the smaller the distance, the greater the similarity between two objects.</p><p class="calibre11">Since clustering is a <a id="id686" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>general technique that can be applied to diverse data types, there are a large number of possible distance measures. Nonetheless, most data can be represented by one of a handful of common abstractions: a set, a point in space, or a vector. For each of these there exists a commonly-used measure.</p><div class="calibre2" title="Set-of-words and the Jaccard index"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec119" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Set-of-words and the Jaccard index</h2></div></div></div><p class="calibre11">If your data can be <a id="id687" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>represented as a set of things the Jaccard index, also <a id="id688" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>known as the <span class="strong1"><strong class="calibre12">Jaccard similarity</strong></span>, can be used. It's <a id="id689" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>one of the simplest measures conceptually: it is the set intersection divided by the set union, or the number of shared elements in common out of the total unique elements in the sets:</p><div class="mediaobject"><img src="Images/7180OS_06_01.jpg" alt="Set-of-words and the Jaccard index" class="calibre253"/></div><p class="calibre11">Many things can be represented as sets. Accounts on social networks can be represented as sets of friends or followers, and customers can be represented as sets of products purchased or viewed. For our text documents, a set representation could simply be the set of unique words used.</p><div class="mediaobject"><img src="Images/7180OS_06_100.jpg" alt="Set-of-words and the Jaccard index" class="calibre254"/></div><p class="calibre11">The Jaccard index is <a id="id690" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>very simple to calculate in Clojure:</p><div class="calibre2"><pre class="programlisting">(:require [clojure.set :as set])

(defn jaccard-similarity [a b]
  (let [a (set a)
        b (set b)]
    (/ (count (set/intersection a b))
       (count (set/union a b)))))

(defn ex-6-1 []
  (let [a [1 2 3]
        b [2 3 4]]
    (jaccard a b)))

;; =&gt; 1/2</pre></div><p class="calibre11">It has the advantage that the sets don't have to be of the same cardinality for the distance measure to make sense. In <a id="id691" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the preceding diagram, <span class="strong1"><strong class="calibre12">A</strong></span> is "larger" than <span class="strong1"><strong class="calibre12">B</strong></span>, yet <a id="id692" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the intersection divided by the union is still a fair reflection of their similarity. To <a id="id693" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>apply the Jaccard index to text documents, we need to translate them into sets of words. This is the process of <span class="strong1"><strong class="calibre12">tokenization</strong></span>.</p></div><div class="calibre2" title="Tokenizing the Reuters files"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec120" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tokenizing the Reuters files</h2></div></div></div><p class="calibre11">Tokenization is the <a id="id694" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>name for the technique of taking a string of text and splitting it into smaller units for the purpose of analysis. A common <a id="id695" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>approach is to split a text string into individual words. An obvious separator would be whitespace so that <code class="literal">"tokens like these"</code> become <code class="literal">["tokens" "like" "these"]</code>.</p><div class="calibre2"><pre class="programlisting">(defn tokenize [s]
  (str/split s #"\W+"))</pre></div><p class="calibre11">This is convenient and simple, but unfortunately, language is subtle and few simple rules can be applied universally. For example, our tokenizer treats apostrophes as whitespace:</p><div class="calibre2"><pre class="programlisting">(tokenize "doesn't handle apostrophes")
;; ["doesn" "t" "handle" "apostrophes"]</pre></div><p class="calibre11">Hyphens are treated as whitespace too:</p><div class="calibre2"><pre class="programlisting">(tokenize "good-looking user-generated content")
;; ["good" "looking" "user" "generated" "content"]</pre></div><p class="calibre11">and removing them rather changes the meaning of the sentence. However, not all hyphens should be preserved:</p><div class="calibre2"><pre class="programlisting">(tokenize "New York-based")
;; ["New" "York" "based"]</pre></div><p class="calibre11">The terms <code class="literal">"New"</code>, <code class="literal">"York"</code>, and <code class="literal">"based"</code> correctly represent the subject of the phrase, but it would be preferable to group <code class="literal">"New York"</code> into a single term, since it represents a specific name and really ought to be preserved intact. <code class="literal">York-based</code>, on the other hand, would be a meaningless token on its own.</p><p class="calibre11">In short, text is <a id="id696" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>messy, and parsing meaning reliably from free <a id="id697" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>text is an extremely rich and active area of research. In particular, for extracting names (e.g. <code class="literal">"New York"</code>) from text, we need to consider the context in which the terms are used. Techniques that label tokens within a <a id="id698" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sentence by their grammatical function are called <span class="strong1"><strong class="calibre12">parts-of-speech taggers</strong></span>.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note53" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">For more information on advanced tokenization and parts-of-speech tagging, see the <code class="literal">clojure-opennlp</code> <a id="id699" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>library at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/dakrone/clojure-opennlp">https://github.com/dakrone/clojure-opennlp</a>.</p></div></div><p class="calibre11">In this chapter we have the luxury of a large quantity of documents and so we'll continue to use our simple tokenizer. We'll find that—in spite of its deficiencies—it will perform well enough to extract meaning from the documents.</p><p class="calibre11">Let's write a function to return the tokens for a document from its file name:</p><div class="calibre2"><pre class="programlisting">(defn tokenize-reuters [content]
  (-&gt; (str/replace content  #"^.*\n\n" "")
      (str/lower-case)
      (tokenize)))

(defn reuters-terms [file]
  (-&gt; (io/resource file)
      (slurp)
      (tokenize-reuters)))</pre></div><p class="calibre11">We're removing the timestamp from the top of the file and making the text lower-case before tokenizing. In the next section, we'll see how to measure the similarity between tokenized documents.</p><div class="calibre2" title="Applying the Jaccard index to documents"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch06lvl3sec16" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Applying the Jaccard index to documents</h3></div></div></div><p class="calibre11">Having <a id="id700" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>tokenized our input documents, we can simply pass the resulting sequence of tokens to our <code class="literal">jaccard-similarity</code> function defined previously. Let's compare the similarity of a couple of <a id="id701" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>documents from the Reuters corpus:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-2 []
  (let [a (set (reuters-terms "reut2-020.sgm-761.txt"))
        b (set (reuters-terms "reut2-007.sgm-750.txt"))
        s (jaccard a b)]
    (println "A:" a)
    (println "B:" b)
    (println "Similarity:" s)))

A: #{recession says reagan sees no he}
B: #{bill transit says highway reagan and will veto he}
Similarity: 1/4</pre></div><p class="calibre11">The Jaccard index outputs a number between zero and one, so it has judged these documents to be 25 percent similar based on the words in their headlines. Notice how we've lost the order of the words in the headline. Without further tricks that we'll come to shortly, the Jaccard index looks only at the items in common between two sets. Another aspect we've lost is the number of times a term occurs in the document. A document that repeats the same word many times may in some sense regard that word as more important. For example, <code class="literal">reut2-020.sgm-932.txt</code> has a headline like this:</p><div class="calibre2"><pre class="programlisting">19-OCT-1987 16:41:40.58
NYSE CHAIRMAN JOHN PHELAN SAYS NYSE WILL OPEN TOMORROW ON TIME</pre></div><p class="calibre11">NYSE appears twice in the headline. We could infer that this headline is especially about the New York Stock Exchange, perhaps more so than a headline that mentioned NYSE only once.</p></div><div class="calibre2" title="The bag-of-words and Euclidean distance"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch06lvl3sec17" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The bag-of-words and Euclidean distance</h3></div></div></div><p class="calibre11">A possible <a id="id702" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>improvement over the set-of-words <a id="id703" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>approach is the <span class="strong1"><strong class="calibre12">bag-of-words approach</strong></span>. This preserves the word count of the terms within the document. The term count can <a id="id704" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>be incorporated by distance measures for a potentially more accurate result.</p><p class="calibre11">One of the most common conceptions of distance is the Euclidean distance measure. In geometry, the Euclidean <a id="id705" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>measure is how we calculate the distance between two points in space. In two dimensions, the Euclidean distance is given by the <span class="strong1"><strong class="calibre12">Pythagoras formula</strong></span>:</p><div class="mediaobject"><img src="Images/7180OS_06_02.jpg" alt="The bag-of-words and Euclidean distance" class="calibre255"/></div><p class="calibre11">This represents the difference between two points as the length of the straight-line distance between them.</p><div class="mediaobject"><img src="Images/7180OS_06_110.jpg" alt="The bag-of-words and Euclidean distance" class="calibre256"/></div><p class="calibre11">This can be <a id="id706" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>extended to three dimensions:</p><div class="mediaobject"><img src="Images/7180OS_06_03.jpg" alt="The bag-of-words and Euclidean distance" class="calibre257"/></div><p class="calibre11">and generalized <a id="id707" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to <span class="strong1"><em class="calibre13">n</em></span> dimensions:</p><div class="mediaobject"><img src="Images/7180OS_06_04.jpg" alt="The bag-of-words and Euclidean distance" class="calibre258"/></div><p class="calibre11">where <span class="strong1"><em class="calibre13">A</em></span><sub class="calibre25">i</sub> and <span class="strong1"><em class="calibre13">B</em></span><sub class="calibre25">i</sub> are the values of <span class="strong1"><em class="calibre13">A</em></span> or <span class="strong1"><em class="calibre13">B</em></span> at dimension <span class="strong1"><em class="calibre13">i</em></span>. The distance measure is thus the overall similarity between two documents, having taken into account how many times each word occurs.</p><div class="calibre2"><pre class="programlisting">(defn euclidean-distance [a b]
  (-&gt;&gt; (map (comp i/sq -) a b)
       (apply +)
       (i/sqrt)))</pre></div><p class="calibre11">Since each word <a id="id708" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>now represents a dimension in space, we <a id="id709" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>need to make sure that when we calculate the Euclidean distance measure we are comparing the magnitude in the same dimension of each document. Otherwise, we may literally be comparing "apples" with "oranges".</p></div></div><div class="calibre2" title="Representing text as vectors"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec121" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Representing text as vectors</h2></div></div></div><p class="calibre11">Unlike the Jaccard <a id="id710" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>index, the Euclidean distance relies on a consistent ordering of words into dimensions. The word count, or term frequency, represents the position of that document in a large multi-dimensional space, and we need to ensure that when we compare values we do so in the correct dimension. Let's <a id="id711" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>represent our documents as term <a id="id712" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">frequency vectors</strong></span>.</p><p class="calibre11">Imagine all the words that could appear in a document being given a unique number. For example, the word "apple" could be assigned the number 53, the word "orange" could be assigned the number 21,597. If all numbers are unique, they could correspond to the index that a word appears in a term vector.</p><p class="calibre11">The dimension of these vectors can be very large. The maximum number of dimensions possible is the cardinality of the vector. The value of the element at the index corresponding to a word is usually the number of occurrences of the word in the document. This is known <a id="id713" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>as the <span class="strong1"><strong class="calibre12">term frequency</strong></span> (<span class="strong1"><strong class="calibre12">tf</strong></span>), weighting.</p><p class="calibre11">In order to be able to compare text vectors it's important that the same word always appears at the same index in the vector. This means that we must use the same word/index mapping for each vector that we create. This word/index mapping is our dictionary.</p></div><div class="calibre2" title="Creating a dictionary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec122" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Creating a dictionary</h2></div></div></div><p class="calibre11">To create a valid <a id="id714" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>dictionary, we need to make sure that the indexes for <a id="id715" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>two words don't clash. One way to do this is to have a monotonically increasing counter which is incremented for each word added to the dictionary. The count at the point the word is added becomes the index of the word. To both add a word to the dictionary and increment a counter in a thread-safe way, we can use an atom:</p><div class="calibre2"><pre class="programlisting">(def dictionary
  (atom {:count 0
         :words {}}))

(defn add-term-to-dict [dict word]
  (if (contains? (:terms dict) word)
    dict
    (-&gt; dict
        (update-in [:terms] assoc word (get dict :count))
        (update-in [:count] inc))))

(defn add-term-to-dict! [dict term]
  (doto dict
    (swap! add-term-to-dict term)))</pre></div><p class="calibre11">To perform an update to an atom, we have to execute our code in a <code class="literal">swap!</code> function.</p><div class="calibre2"><pre class="programlisting">(add-term-to-dict! dictionary "love")

;; #&lt;Atom@261d1f0a: {:count 1, :terms {"love" 0}}&gt;</pre></div><p class="calibre11">Adding another <a id="id716" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>word will cause the count to increase:</p><div class="calibre2"><pre class="programlisting">(add-term-to-dict! dictionary "music")

;; #&lt;Atom@261d1f0a: {:count 2, :terms {"music" 1, "love" 0}}&gt;</pre></div><p class="calibre11">And adding the same word twice will have no effect:</p><div class="calibre2"><pre class="programlisting">(add-term-to-dict! dictionary "love")

;; #&lt;Atom@261d1f0a: {:count 2, :terms {"music" 1, "love" 0}}&gt;</pre></div><p class="calibre11">Performing this update <a id="id717" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>inside an atom ensures that each word gets its own index even when the dictionary is being simultaneously updated by multiple threads.</p><div class="calibre2"><pre class="programlisting">(defn build-dictionary! [dict terms]
  (reduce add-term-to-dict! dict terms))</pre></div><p class="calibre11">Building a whole dictionary is as simple as reducing our <code class="literal">add-term-to-dict!</code> function over a supplied dictionary atom with a collection of terms.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Creating term frequency vectors"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec106" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Creating term frequency vectors</h1></div></div></div><p class="calibre11">To calculate the <a id="id718" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Euclidean distance, let's first create a vector from our dictionary and document. This will allow us to easily compare the term frequencies between documents because they will occupy the same index of the vector.</p><div class="calibre2"><pre class="programlisting">(defn term-id [dict term]
  (get-in @dict [:terms term]))

(defn term-frequencies [dict terms]
  (-&gt;&gt; (map #(term-id dict %) terms)
       (remove nil?)
       (frequencies)))

(defn map-&gt;vector [dictionary id-counts]
  (let [zeros (vec (replicate (:count @dictionary) 0))]
    (-&gt; (reduce #(apply assoc! %1 %2) (transient zeros) id-counts)
        (persistent!))))

(defn tf-vector [dict document]
  (map-&gt;vector dict (term-frequencies dict document)))</pre></div><p class="calibre11">The <code class="literal">term-frequencies</code> function creates a map of term ID to frequency count for each term in the document. The <code class="literal">map-&gt;vector</code> function simply takes this map and associates the frequency count at the index of the vector given by the term ID. Since there may be many terms, and the vector may be very long, we're using Clojure's <code class="literal">transient!</code> and <code class="literal">persistent!</code> functions to temporarily create a mutable vector for efficiency.</p><p class="calibre11">Let's print the document, dictionary, and resulting vector for <code class="literal">reut2-020.sgm-742.txt</code>:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-3 []
  (let [doc  (reuters-terms "reut2-020.sgm-742.txt")
        dict (build-dictionary! dictionary doc)]
    (println "Document:" doc)
    (println "Dictionary:" dict)
    (println "Vector:" (tf-vector dict doc))))</pre></div><p class="calibre11">The output is shown as follows (the formatting has been adjusted for legibility):</p><div class="calibre2"><pre class="programlisting">;; Document: [nyse s phelan says nyse will continue program
;;            trading curb until volume slows]
;; Dictionary: #&lt;Atom@bb156ec: {:count 12, :terms {s 1, curb 8,
;;             phelan 2, says 3, trading 7, nyse 0, until 9,
;;             continue 5, volume 10, will 4, slows 11,
;;             program 6}}&gt;
;; Vector: [2 1 1 1 1 1 1 1 1 1 1 1]</pre></div><p class="calibre11">With 12 terms in the input, there are 12 terms in the dictionary and a vector of 12 elements returned.</p><div class="calibre2"><pre class="programlisting">(defn print-distance [doc-a doc-b measure]
  (let [a-terms (reuters-terms doc-a)
        b-terms (reuters-terms doc-b)
        dict (-&gt; dictionary
                 (build-dictionary! a-terms)
                 (build-dictionary! b-terms))
        a (tf-vector dict a-terms)
        b (tf-vector dict b-terms)]
    (println "A:" a)
    (println "B:" b)
    (println "Distance:" (measure a b))))

(defn ex-6-4 []
  (print-distance "reut2-020.sgm-742.txt"
                  "reut2-020.sgm-932.txt"
                  euclidean-distance))

;; A: [2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]
;; B: [2 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1]
;; Distance: 3.7416573867739413</pre></div><p class="calibre11">Like the Jaccard index, the <a id="id719" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Euclidean distance cannot decrease below zero. Unlike the Jaccard index, though, the value can grow indefinitely.</p><div class="calibre2" title="The vector space model and cosine distance"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec123" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The vector space model and cosine distance</h2></div></div></div><p class="calibre11">The vector <a id="id720" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>space model can be considered a generalization of the set-of-words and bag-of-words models. Like the bag-of-words model, the <a id="id721" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>vector space model represents each document as a vector, each element of which represents a term. The value at each index is a measure of importance of the word, which may or may not be the term frequency.</p><p class="calibre11">If your data conceptually represents a vector (that is to say, a magnitude in a particular direction), then the cosine distance may be the most appropriate choice. The cosine distance measure determines the similarity of two elements as the cosine of the angle between their vector representations.</p><div class="mediaobject"><img src="Images/7180OS_06_120.jpg" alt="The vector space model and cosine distance" class="calibre196"/></div><p class="calibre11">If both vectors point <a id="id722" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>in the same direction, then the <a id="id723" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>angle between them will be zero and the cosine of zero is one. The cosine similarity can be defined in the following way:</p><div class="mediaobject"><img src="Images/7180OS_06_05.jpg" alt="The vector space model and cosine distance" class="calibre259"/></div><p class="calibre11">This is a more complicated equation than the ones we've covered previously. It relies on calculating the dot product of the two vectors and the magnitude of each.</p><div class="calibre2"><pre class="programlisting">(defn cosine-similarity [a b]
  (let [dot-product (-&gt;&gt; (map * a b)
                         (apply +))
        magnitude (fn [d]
                    (-&gt;&gt; (map i/sq d)
                         (apply +)
                         (i/sqrt)))]
    (/ dot-product (* (magnitude a) (magnitude b)))))</pre></div><p class="calibre11">Examples of the cosine similarity are shown as follows:</p><div class="mediaobject"><img src="Images/7180OS_06_130.jpg" alt="The vector space model and cosine distance" class="calibre260"/></div><p class="calibre11">The cosine similarity is often used as a similarity measure in high-dimensional spaces where each vector contains a lot of zeros because it can be very efficient to evaluate: only the non-zero dimensions need to be considered. Since most text documents use only a small fraction of all words (and therefore are zero for a large proportion of dimensions), the cosine measure is often used for clustering text.</p><p class="calibre11">In the vector space <a id="id724" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>model, we need a consistent strategy for measuring the importance of each term. In the set-of-words model, all terms are counted <a id="id725" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>equally. This is equivalent to setting the value of the vector at that point to one. In the bag-of-words model, the term frequencies were counted. We'll continue to use the term frequency for now, but we'll see shortly how to use a more sophisticated measure of importance, called <span class="strong1"><strong class="calibre12">term frequency-inverse document frequency</strong></span> (<span class="strong1"><strong class="calibre12">TF-IDF</strong></span>).</p><div class="calibre2"><pre class="programlisting">(defn ex-6-5 []
  (print-distance "reut2-020.sgm-742.txt"
                  "reut2-020.sgm-932.txt"
                  cosine-similarity))

;; A: [2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]
;; B: [2 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1]
;; Distance: 0.5012804118276031</pre></div><p class="calibre11">The closer the cosine value is to <code class="literal">1</code>, the more similar the two entities are. To convert <code class="literal">cosine-similarity</code> to a distance measure, we can simply subtract the <code class="literal">cosine-similarity</code> from <code class="literal">1</code>.</p><p class="calibre11">Although all the measures mentioned earlier produce different measures for the same input, they all satisfy the constraint that the distance between <span class="strong1"><em class="calibre13">A</em></span> and <span class="strong1"><em class="calibre13">B</em></span> should be the same as the difference between <span class="strong1"><em class="calibre13">B</em></span> and <span class="strong1"><em class="calibre13">A</em></span>. Often the same underlying data can be transformed to represent a set (Jaccard), a point in space (Euclidean), or a vector (Cosine). Sometimes the only way to know which is right is to try it and see how good the results are.</p><p class="calibre11">The number of unique <a id="id726" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>words that appear in one <a id="id727" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>document is typically small compared to the number of unique words that appear in any document in a collection being processed. As a result, these high-dimensional document vectors are quite sparse.</p></div><div class="calibre2" title="Removing stop words"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec124" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Removing stop words</h2></div></div></div><p class="calibre11">Much of the similarity <a id="id728" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>between the headlines has been generated by often-occurring words that don't add a great deal of meaning to the content. Examples are "a", "says", and "and". We should filter these out in order to avoid generating spurious similarities.</p><p class="calibre11">Consider the following two idioms:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><code class="literal">"Music is the food of love"</code></li><li class="listitem"><code class="literal">"War is the locomotive of history"</code></li></ul></div><p class="calibre11">We could calculate the cosine similarity between them using the following Clojure code:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-6 []
  (let [a (tokenize "music is the food of love")
        b (tokenize "war is the locomotive of history")]
    (add-documents-to-dictionary! dictionary [a b])
    (cosine-similarity (tf-vector dictionary a)
                       (tf-vector dictionary b))))

;; 0.5</pre></div><p class="calibre11">The two documents are showing a similarity of <code class="literal">0.5</code> in spite of the fact that the only words they share in common are <code class="literal">is</code>, <code class="literal">the</code>, and <code class="literal">of</code>. Ideally we'll want to remove these.</p></div><div class="calibre2" title="Stemming"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec125" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Stemming</h2></div></div></div><p class="calibre11">Now let's <a id="id729" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>consider an alternative phrase:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><code class="literal">"Music is the food of love"</code></li><li class="listitem"><code class="literal">"It's lovely that you're musical"</code></li></ul></div><p class="calibre11">Let's compare their cosine similarity as well:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-7 []
  (let [a (tokenize "music is the food of love")
        b (tokenize "it's lovely that you're musical")]
    (add-documents-to-dictionary! dictionary [a b])
    (cosine-similarity (tf-vector dictionary a)
                       (tf-vector dictionary b))))

;; 0.0</pre></div><p class="calibre11">In spite of the fact that the two sentences refer to music and positive feelings, the two phrases have a cosine similarity of zero: there are no words in common between the two phrases. This makes sense but does not express the behavior we usually want, which is to capture the similarity between "concepts", rather than the precise words that were used.</p><p class="calibre11">One way of tackling this problem is to <span class="strong1"><strong class="calibre12">stem</strong></span> words, which reduces them to their roots. Words which share a common meaning are more likely to stem to the same root. The Clojure library <a id="id730" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>stemmers (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/mattdw/stemmers">https://github.com/mattdw/stemmers</a>) will do this for us, and fortunately they will also remove stop words too.</p><div class="calibre2"><pre class="programlisting">(defn ex-6-8 []
  (let [a (stemmer/stems "music is the food of love")
        b (stemmer/stems "it's lovely that you're musical")]
    (add-documents-to-dictionary! dictionary [a b])
    (cosine-similarity (tf-vector dictionary a)
                       (tf-vector dictionary b))))

;; 0.8164965809277259</pre></div><p class="calibre11">Much better. After stemming and stop word removal, the similarity between the phrases has dropped from 0.0 to 0.82. This is a good outcome since, although the sentences used different words, the sentiments they expressed were related.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Clustering with k-means and Incanter"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec107" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Clustering with k-means and Incanter</h1></div></div></div><p class="calibre11">Finally, having tokenized, stemmed, and vectorized our input documents—and with a selection of distance measures to choose from—we're in a position to run clustering on our data. The first clustering <a id="id731" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>algorithm we'll look at is called <span class="strong1"><em class="calibre13">k-means clustering</em></span>.</p><p class="calibre11">
<span class="strong1"><em class="calibre13">k</em></span>-means is an iterative algorithm that proceeds as follows:</p><div class="calibre2"><ol class="orderedlist1"><li class="listitem2">Randomly pick <span class="strong1"><em class="calibre13">k</em></span> cluster centroids.</li><li class="listitem2">Assign each of the data points to the cluster with the closest centroid.</li><li class="listitem2">Adjust each cluster centroid to the mean of its assigned data points.</li><li class="listitem2">Repeat until convergence or the maximum number of iterations reached.</li></ol></div><p class="calibre11">The process is visualized in the following diagram for <span class="strong1"><em class="calibre13">k=3</em></span> clusters:</p><div class="mediaobject"><img src="Images/7180OS_06_140.jpg" alt="Clustering with k-means and Incanter" class="calibre261"/></div><p class="calibre11">In the preceding figure, we can see that the initial cluster centroids at iteration 1 don't represent the structure of the data well. Although the points are clearly arranged in three groups, the initial centroids (represented by crosses) are all distributed around the top area of the graph. The points are colored according to their closest centroid. As the iterations proceed, we can <a id="id732" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see how the cluster centroids are moved closer to their "natural" positions in the center of each of the groups of points. </p><p class="calibre11">Before we define the main <span class="strong1"><em class="calibre13">k</em></span>-means function, it's useful to define a couple of utility functions first: a function to calculate the centroid for a cluster, and a function to group the data into their respective clusters.</p><div class="calibre2"><pre class="programlisting">(defn centroid [xs]
  (let [m (i/trans (i/matrix xs))]
    (if (&gt; (i/ncol m) 1)
      (i/matrix (map s/mean m))
     m)))

(defn ex-6-9 []
  (let [m (i/matrix [[1 2 3]
                     [2 2 5]])]
    (centroid m)))

;; A 3x1 matrix
;;  -------------
;; 1.50e+00
;; 2.00e+00
;; 4.00e+00</pre></div><p class="calibre11">The <code class="literal">centroid</code> function <a id="id733" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>simply calculates the mean of each column of the input matrix.</p><div class="calibre2"><pre class="programlisting">(defn clusters [cluster-ids data]
  (-&gt;&gt; (map vector cluster-ids data)
       (conj-into {})
       (vals)
       (map i/matrix))) 

(defn ex-6-10 []
  (let [m (i/matrix [[1 2 3]
                     [4 5 6]
                     [7 8 9]])]
    (clusters [0 1 0] m)))

;; A 1x3 matrix
;; -------------
;; 4.00e+00  5.00e+00  6.00e+00 
;;  A 2x3 matrix
;; -------------
;; 7.00e+00  8.00e+00  9.00e+00 
;; 1.00e+00  2.00e+00  3.00e+00</pre></div><p class="calibre11">The <code class="literal">clusters</code> function splits a larger matrix up into a sequence of smaller matrices based on the supplied cluster IDs. The cluster IDs are provided as a sequence of elements the same length as the clustered points, listing the cluster ID of the point at that index in the sequence. Items that share a common cluster ID will be grouped together. With these two functions in place, here's the finished <code class="literal">k-means</code> function:</p><div class="calibre2"><pre class="programlisting">(defn k-means [data k]
  (loop [centroids (s/sample data :size k)
         previous-cluster-ids nil]
    (let [cluster-id (fn [x]
                       (let [distance  #(s/euclidean-distance x %)
                             distances (map distance centroids)]
                         (-&gt;&gt; (apply min distances)
                              (index-of distances))))
          cluster-ids (map cluster-id data)]
      (if (not= cluster-ids previous-cluster-ids)
        (recur (map centroid (clusters cluster-ids data))
               cluster-ids)
        clusters))))</pre></div><p class="calibre11">We start by picking <code class="literal">k</code> random cluster centroids by sampling the input data. Then, we use loop/recur to continuously <a id="id734" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>update the cluster centroids until <code class="literal">previous-cluster-ids</code> are the same as <code class="literal">cluster-ids</code>. At this point, no documents have moved cluster, so the clustering has converged.</p><div class="calibre2" title="Clustering the Reuters documents"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec126" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Clustering the Reuters documents</h2></div></div></div><p class="calibre11">Let's use our <code class="literal">k-means</code> function to cluster the Reuters documents now. Let's go easy on our algorithm to start <a id="id735" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with, and pick a small sample of larger documents. Larger documents will make it more likely that the algorithm will be able to determine meaningful similarities between them. Let's set the minimum threshold at 500 characters. This means that at the very least our input documents will have a headline and a couple of sentences of body text to work with.</p><div class="calibre2"><pre class="programlisting">(defn ex-6-11 []
  (let [documents (fs/glob "data/reuters-text/*.txt")
        doc-count 100
        k 5
        tokenized (-&gt;&gt; (map slurp documents)
                       (remove too-short?)
                       (take doc-count)
                       (map stem-reuters))]
    (add-documents-to-dictionary! dictionary tokenized)
    (-&gt; (map #(tf-vector dictionary %) tokenized)
        (k-means k))))</pre></div><p class="calibre11">We're using the <code class="literal">fs</code> library (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/Raynes/fs">https://github.com/Raynes/fs</a>) to create a list of files to perform our clustering on by calling <code class="literal">fs/glob</code> with a pattern that matches all the text files. We remove those <a id="id736" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>which are too short, tokenize the first 100, and add them to the dictionary. We create <code class="literal">tf</code> vectors for our inputs and then call <code class="literal">k-means</code> on them.</p><p class="calibre11">If you run the preceding example, you'll receive a list of clustered document vectors, which isn't very useful. Let's create a <code class="literal">summary</code> function that uses the dictionary to report the most common terms in each of the clusters.</p><div class="calibre2"><pre class="programlisting">(defn cluster-summary [dict clusters top-term-count]
  (for [cluster clusters]
    (let [sum-terms (if (= (i/nrow cluster) 1)
                      cluster
                      (-&gt;&gt; (i/trans cluster)
                           (map i/sum)
                           (i/trans)))
          popular-term-ids (-&gt;&gt; (map-indexed vector sum-terms)
                                (sort-by second &gt;)
                                (take top-term-count)
                                (map first))
          top-terms (map #(id-&gt;term dict %) popular-term-ids)]
      (println "N:" (i/nrow cluster))
      (println "Terms:" top-terms))))

(defn ex-6-12 []
  (cluster-summary dictionary (ex-6-11) 5))</pre></div><p class="calibre11">
<span class="strong1"><em class="calibre13">k</em></span>-means is by its nature a <a id="id737" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>stochastic algorithm, and is sensitive to the starting position for the centroids. I get the following output, but yours will almost certainly differ:</p><div class="calibre2"><pre class="programlisting">;; N: 2
;; Terms: (rocket launch delta satellit first off weather space)
;;  N: 4
;; Terms: (said will for system 000 bank debt from bond farm)
;; N: 12
;; Terms: (said reuter for iranian it iraq had new on major)
;; N: 62
;; Terms: (said pct dlr for year mln from reuter with will)
;; N: 20
;; Terms: (said for year it with but dlr mln bank week)</pre></div><p class="calibre11">Unfortunately, we <a id="id738" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>don't seem to be getting very good results. The first cluster contains two articles about rockets and space, and the third seems to consist of articles about Iran. The most popular word in most of the articles is "said".</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Better clustering with TF-IDF"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec108" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Better clustering with TF-IDF</h1></div></div></div><p class="calibre11">
<span class="strong1"><strong class="calibre12">Term </strong></span><a id="id739" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">Frequency-Inverse Document Frequency</strong></span> (<span class="strong1"><strong class="calibre12">TF-IDF</strong></span>) is a general approach to weighting terms within a document vector so that terms that are popular across the whole dataset are not weighted as highly as terms that are less usual. This captures the intuitive conviction—and what we observed earlier—that words such as "said" are not a strong basis for building clusters.</p><div class="calibre2" title="Zipf's law"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec127" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Zipf's law</h2></div></div></div><p class="calibre11">Zipf's law <a id="id740" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>states that the <a id="id741" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>frequency of any word is inversely proportional to its rank in the frequency table. Thus, the most frequent word will occur approximately twice as often as the second most frequent word and three times as often as the next most frequent word, and so on. Let's see if this applies across our Reuters corpus:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-13 []
  (let [documents (fs/glob "data/reuters-text/*.txt")
        doc-count 1000
        top-terms 25
        term-frequencies (-&gt;&gt; (map slurp documents)
                              (remove too-short?)
                              (take doc-count)
                              (mapcat tokenize-reuters)
                              (frequencies)
                              (vals)
                              (sort &gt;)
                              (take top-terms))]
    (-&gt; (c/xy-plot (range (inc top-terms)) term-frequencies
                   :x-label "Terms"
                   :y-label "Term Frequency")
        (i/view))))</pre></div><p class="calibre11">Using the preceding code we can calculate the frequency graph of the top 25 most popular terms in the first 1,000 Reuters documents.</p><div class="mediaobject"><img src="Images/7180OS_06_150.jpg" alt="Zipf's law" class="calibre45"/></div><p class="calibre11">In the first <a id="id742" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>1,000 documents, the most popular term appears almost 10,000 times. The 25<sup class="calibre42">th</sup> most popular term appears around 1,000 times overall. In fact, the data is showing that words are appearing more <a id="id743" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>commonly in the Reuters corpus than their placement in the frequency table would suggest. This is most likely due to the bulletin nature of the Reuters corpus, which tends to re-use the same short words repeatedly.</p></div><div class="calibre2" title="Calculating the TF-IDF weight"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec128" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating the TF-IDF weight</h2></div></div></div><p class="calibre11">Calculating <a id="id744" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>TF-IDF only requires two modifications to the code we've created already. Firstly, we must keep track of how many documents a given term appears in. Secondly, we must weight the term appropriately when building the document vector.</p><p class="calibre11">Since we've already created a dictionary of terms, we may as well store the document frequencies for each term there.</p><div class="calibre2"><pre class="programlisting">(defn inc-df! [dictionary term-id]
  (doto dictionary
    (swap! update-in [:df term-id] (fnil inc 0))))

(defn build-df-dictionary! [dictionary document]
  (let [terms    (distinct document)
        dict     (build-dictionary! dictionary document)
        term-ids (map #(term-id dictionary %) document)]
    (doseq [term-id term-ids]
      (inc-df! dictionary term-id))
    dict))</pre></div><p class="calibre11">The <code class="literal">build-df-dictionary</code> function earlier accepts a dictionary and a sequence of terms. We build the dictionary from the distinct terms and look up the <code class="literal">term-id</code> for each one. Finally, <a id="id745" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we iterate over the term IDs and increment the <code class="literal">:df</code> for each one.</p><p class="calibre11">If a document has words <span class="strong1"><em class="calibre13">w</em></span><sub class="calibre25">1</sub>, …, <span class="strong1"><em class="calibre13">w</em></span><sub class="calibre25">n</sub>, then the inverse document frequency for word <span class="strong1"><em class="calibre13">w</em></span><sub class="calibre25">i</sub> is defined as:</p><div class="mediaobject"><img src="Images/7180OS_06_06.jpg" alt="Calculating the TF-IDF weight" class="calibre262"/></div><p class="calibre11">That is, the reciprocal of the number of documents it appears in. If a word occurs commonly across a collection of documents, its <span class="strong1"><em class="calibre13">DF</em></span> value is large and its <span class="strong1"><em class="calibre13">IDF</em></span> value is small. With a large number of documents, it's common to normalize the <span class="strong1"><em class="calibre13">IDF</em></span> value by multiplying it by a constant number, usually the document count <span class="strong1"><em class="calibre13">N</em></span>, so the <span class="strong1"><em class="calibre13">IDF</em></span> equation looks like this:</p><div class="mediaobject"><img src="Images/7180OS_06_07.jpg" alt="Calculating the TF-IDF weight" class="calibre262"/></div><p class="calibre11">The TF-IDF weight <span class="strong1"><em class="calibre13">W</em></span><sub class="calibre25">i</sub> of word <span class="strong1"><em class="calibre13">w</em></span><sub class="calibre25">i</sub> is given by the product of the term frequency and the inverse document frequency:</p><div class="mediaobject"><img src="Images/7180OS_06_08.jpg" alt="Calculating the TF-IDF weight" class="calibre263"/></div><p class="calibre11">However, the <span class="strong1"><em class="calibre13">IDF</em></span> value in the preceding equation is still not ideal since for large corpora the range of the <span class="strong1"><em class="calibre13">IDF</em></span> term is usually much greater than the <span class="strong1"><em class="calibre13">TF</em></span> and can overwhelm its effect. To reduce this problem, and balance the weight of the <span class="strong1"><em class="calibre13">TF</em></span> and the <span class="strong1"><em class="calibre13">IDF</em></span> terms, the usual practice is to use the logarithm of the <span class="strong1"><em class="calibre13">IDF</em></span> value instead:</p><div class="mediaobject"><img src="Images/7180OS_06_09.jpg" alt="Calculating the TF-IDF weight" class="calibre264"/></div><p class="calibre11">Thus, the <a id="id746" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>TF-IDF weight <span class="strong1"><em class="calibre13">w</em></span><sub class="calibre25">i</sub> for a word <span class="strong1"><em class="calibre13">w</em></span><sub class="calibre25">i</sub> becomes:</p><div class="mediaobject"><img src="Images/7180OS_06_10.jpg" alt="Calculating the TF-IDF weight" class="calibre265"/></div><p class="calibre11">This is a classic TF-IDF weighting: common words are given a small weight and terms that occur infrequently get a large weight. The important words for determining the topic of a document usually have a high <span class="strong1"><em class="calibre13">TF</em></span> and a moderately large <span class="strong1"><em class="calibre13">IDF</em></span>, so the product of the two becomes a large value, thereby giving more importance to these words in the resulting vector.</p><div class="calibre2"><pre class="programlisting">(defn document-frequencies [dict terms]
  (-&gt;&gt; (map (partial term-id dict) terms)
       (select-keys (:df @dict))))

(defn tfidf-vector [dict doc-count terms]
  (let [tf (term-frequencies dict terms)
        df (document-frequencies dict (distinct terms))
        idf   (fn [df] (i/log (/ doc-count df)))
        tfidf (fn [tf df] (* tf (idf df)))]
    (map-&gt;vector dict (merge-with tfidf tf df))))</pre></div><p class="calibre11">The preceding code calculates the TF-IDF from <code class="literal">term-frequencies</code> defined previously and <a id="id747" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><code class="literal">document-frequencies</code> extracted from our dictionary.</p></div><div class="calibre2" title="k-means clustering with TF-IDF"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec129" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>k-means clustering with TF-IDF</h2></div></div></div><p class="calibre11">With <a id="id748" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the preceding adjustments in place, we're in a position to calculate the TF-IDF vectors <a id="id749" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>for the Reuters documents. The following example is a modification of <code class="literal">ex-6-12</code> using the new <code class="literal">tfidf-vector</code> function:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-14 []
  (let [documents (fs/glob "data/reuters-text/*.txt")
        doc-count 100
        k 5
        tokenized (-&gt;&gt; (map slurp documents)
                       (remove too-short?)
                       (take doc-count)
                       (map stem-reuters))]
    (reduce build-df-dictionary! dictionary tokenized)
    (-&gt; (map #(tfidf-vector dictionary doc-count %) tokenized)
        (k-means k)
        (cluster-summary dictionary 10))))</pre></div><p class="calibre11">The preceding code is very similar to the previous example, but we have substituted our new <code class="literal">build-df-dictionary</code> and <code class="literal">tfidf-vector</code> functions. If you run the example, you should see output that looks a little better than before:</p><div class="calibre2"><pre class="programlisting">N: 5
Terms: (unquot unadjust year-on-year novemb grew sundai labour m-3 ahead 120)
N: 15
Terms: (rumor venezuela azpurua pai fca keat ongpin boren gdp moder)
N: 16
Terms: (regan drug lng soviet bureau deleg gao dean fdic algerian)
N: 46
Terms: (form complet huski nrc rocket north underwrit card oat circuit)
N: 18
Terms: (freez cocoa dec brown bean sept seixa telex argentin brown-forman)</pre></div><p class="calibre11">Although the top words may be hard to interpret because they have been stemmed, these represent the most unusually common words within each of the clusters. Notice that "said" is <a id="id750" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>no longer the most highly rated word across all clusters.</p></div><div class="calibre2" title="Better clustering with n-grams"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec130" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Better clustering with n-grams</h2></div></div></div><p class="calibre11">It should <a id="id751" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>be clear from looking at the earlier lists of words how much has been sacrificed by reducing our documents to unordered sequences of terms. Without the context of a sentence, it's very hard to get more than a vague sense of what each cluster might be about.</p><p class="calibre11">There is, however, nothing inherent in the vector space model that precludes maintaining the order of our input tokens. We can simply create a new term to represent a combination of words. The <a id="id752" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>combined term, representing perhaps several input words in sequence, is called an <span class="strong1"><strong class="calibre12"><span class="strong1"><em class="calibre13">n</em></span>-gram</strong></span>.</p><p class="calibre11">An example of an <span class="strong1"><em class="calibre13">n</em></span>-gram might be "new york", or "stock market". In fact, because they contain two terms, these are called <a id="id753" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">bigrams</strong></span>. <span class="strong1"><em class="calibre13">n</em></span>-grams can be of arbitrary length. The longer an <span class="strong1"><em class="calibre13">n</em></span>-gram, the more context it carries, but also the rarer it is.</p><p class="calibre11">
<span class="strong1"><em class="calibre13">n</em></span>-grams are closely related to the concept of <span class="strong1"><strong class="calibre12">shingling</strong></span>. When we shingle our <span class="strong1"><em class="calibre13">n</em></span>-grams, we're creating overlapping sequences of terms. The term shingling comes from the way the terms overlap like roof shingles.</p><div class="calibre2"><pre class="programlisting">(defn n-grams [n words]
  (-&gt;&gt; (partition n 1 words)
       (map (partial str/join " ")))) 

(defn ex-6-15 []
  (let [terms (reuters-terms "reut2-020.sgm-761.txt")]
    (n-grams 2 terms)))

;; ("reagan says" "says he" "he sees" "sees no" "no recession")</pre></div><p class="calibre11">Already, using 2-grams would allow us (for example) to distinguish between the following uses of the word "coconut" in the dataset: "coconut oil", "coconut planters", "coconut plantations", "coconut farmers", "coconut association", "coconut authority", "coconut products", "coconut exports", "coconut industry", and the rather pleasing "coconut chief". Each of these pairs of words defines a different concept—sometimes subtly different—that we can capture and compare across documents.</p><p class="calibre11">We can get the best of both worlds with <span class="strong1"><em class="calibre13">n</em></span>-grams and shingling by combining the results of multiple lengths of <span class="strong1"><em class="calibre13">n</em></span>-gram:</p><div class="calibre2"><pre class="programlisting">(defn multi-grams [n words]
  (-&gt;&gt; (range 1 (inc n))
       (mapcat #(n-grams % words))))

(defn ex-6-16 []
  (let [terms (reuters-terms "reut2-020.sgm-761.txt")]
    (multi-grams 4 terms)))

;; ("reagan" "says" "he" "sees" "no" "recession" "reagan says" 
;; "says he" "he sees" "sees no" "no recession" "reagan says he" 
;; "says he sees" "he sees no" "sees no recession" "reagan says he 
;; sees" "says he sees no" "he sees no recession")</pre></div><p class="calibre11">While stemming and stop word removal had the effect of reducing the size of our dictionary, and using TF-IDF had the effect of improving the utility of the weight for each term in a document, producing <span class="strong1"><em class="calibre13">n</em></span>-grams has the effect of massively expanding the number of terms we need to accommodate.</p><p class="calibre11">This explosion of <a id="id754" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>features is going to immediately overwhelm our implementation of <span class="strong1"><em class="calibre13">k</em></span>-means in Incanter. Fortunately, there's a machine learning library called <span class="strong1"><strong class="calibre12">Mahout</strong></span> that's specifically designed to run algorithms such as <span class="strong1"><em class="calibre13">k</em></span>-means on very large quantities of data.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Large-scale clustering with Mahout"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec109" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Large-scale clustering with Mahout</h1></div></div></div><p class="calibre11">Mahout (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://mahout.apache.org/">http://mahout.apache.org/</a>) is a machine learning library intended for use in distributed <a id="id755" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>computing environments. Version 0.9 of <a id="id756" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the library targets Hadoop and is the version we'll be using here.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note54" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">At the time of writing, Mahout 0.10 has just been released and also targets Spark. Spark is an alternative distributed computing framework that we'll be introducing in the next chapter.</p></div></div><p class="calibre11">We saw in the previous chapter that one of Hadoop's abstractions is the sequence file: a binary representation of Java keys and values. Many of Mahout's algorithms expect to operate on sequence files, and we'll need to create one as input to Mahout's <span class="strong1"><em class="calibre13">k</em></span>-means algorithm. Mahout's <span class="strong1"><em class="calibre13">k</em></span>-means algorithm also expects to receive its input as a vector, represented by one of Mahout's vector types.</p><p class="calibre11">Although Mahout contains classes and utility programs that will extract vectors from text, we'll use this as an opportunity to demonstrate how to use Parkour and Mahout together. Not only will we have finer-grained control over the vectors that are created, but it will allow us to demonstrate more of the capabilities of Parkour for specifying Hadoop jobs.</p><div class="calibre2" title="Converting text documents to a sequence file"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec131" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Converting text documents to a sequence file</h2></div></div></div><p class="calibre11">We won't <a id="id757" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>define a custom job to convert our text documents into a sequence file representation, though: Mahout already defines a useful <code class="literal">SequenceFilesFromDirectory</code> class to convert a directory of text files. We'll use this to create a single file representing the entire contents of the <code class="literal">reuters-txt</code> directory.</p><p class="calibre11">Though the sequence file may be physically stored in separate chunks (on HDFS, for example), it is logically one file, representing all the input documents as key/value pairs. The key is the name of the file, and the value is the file's text contents.</p><div class="mediaobject"><img src="Images/7180OS_06_160.jpg" alt="Converting text documents to a sequence file" class="calibre266"/></div><p class="calibre11">The following code will handle the conversion:</p><div class="calibre2"><pre class="programlisting">(:import [org.apache.mahout.text
           SequenceFilesFromDirectory])

(defn text-&gt;sequencefile [in-path out-path]
  (SequenceFilesFromDirectory/main
   (into-array String (vector "-i" in-path
                              "-o" out-path
                              "-xm" "sequential"
                              "-ow"))))

(defn ex-6-17 []
  (text-&gt;sequencefile "data/reuters-text"
                      "data/reuters-sequencefile"))</pre></div><p class="calibre11">
<code class="literal">SequenceFilesFromDirectory</code> is a Mahout utility class, part of a suite of classes designed to be called on the command line.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="tip07" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre22">Since running the preceding example is a prerequisite for subsequent examples, it's also available on the command line:</p><div class="calibre2"><pre class="programlisting1">
<span class="strong1"><strong class="calibre12">lein create-sequencefile</strong></span>
</pre></div></div></div><p class="calibre11">We're calling <a id="id758" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the <code class="literal">main</code> function directly, passing the arguments we would otherwise pass on the command line as a string array.</p></div><div class="calibre2" title="Using Parkour to create Mahout vectors"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec132" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using Parkour to create Mahout vectors</h2></div></div></div><p class="calibre11">Now that we <a id="id759" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have a sequence file representation of the Reuters corpus, we need to transform each document (now represented as a single key/value pair) into a vector. We saw how to do this earlier using a shared dictionary modeled as a Clojure atom. The atom ensures that each distinct term gets its own ID even in a multi-threaded environment.</p><p class="calibre11">We will be <a id="id760" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>using Parkour and Hadoop to generate our vectors, but this presents a challenge. How can we assign a unique ID to each word <a id="id761" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>when the nature of MapReduce programming is that mappers operate in parallel and share no state? Hadoop doesn't provide the equivalent of a Clojure atom for sharing mutable state across nodes in a cluster, and in fact minimizing shared state is key to scaling distributed applications.</p><p class="calibre11">Creating a shared set of unique IDs therefore presents an interesting challenge for our Parkour job: let's see how we can produce unique IDs for our dictionary in a distributed way.</p></div><div class="calibre2" title="Creating distributed unique IDs"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec133" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Creating distributed unique IDs</h2></div></div></div><p class="calibre11">Before we <a id="id762" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>look at Hadoop-specific solutions, though, it's worth noting that one easy way of creating a <a id="id763" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cluster-wide unique identifier is to create a universally unique identifier, or UUID.</p><div class="calibre2"><pre class="programlisting">(defn uuid []
  (str (java.util.UUID/randomUUID)))</pre></div><p class="calibre11">This creates a long string of bytes in the form: <code class="literal">3a65c7db-6f41-4087-a2ec-8fe763b5f185</code> that is virtually guaranteed not to clash with any other UUID generated anywhere else in the world.</p><p class="calibre11">While this works for generating unique IDs, the number of possible IDs is astronomically large, and Mahout's sparse vector representation needs to be initialized with the cardinality of the vector expressed as an integer. IDs generated with <code class="literal">uuid</code> are simply too big. Besides, it doesn't help us coordinate the creation of IDs: every machine in the cluster will generate different UUIDs to represent the same terms.</p><p class="calibre11">One way of <a id="id764" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>getting around this is to use the term itself to generate a unique ID. If we used a consistent hashing function to create an integer from each input term, all machines in the cluster would generate the same ID. Since a good hashing function is likely to produce a unique output for <a id="id765" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>unique input terms, this technique is likely to work well. There will be some hash collisions (where two words hash to the same ID) but this should be a small percentage of the overall.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note55" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The method of hashing the features themselves to create a unique ID is often referred to as the "hashing trick". Although it's commonly used for text vectorization, it can be applied to any problem that involves large numbers of features.</p></div></div><p class="calibre11">However, the challenge of producing distinct IDs that are unique across the whole cluster gives us the opportunity to talk about a useful feature of Hadoop that Parkour exposes: the distributed cache.</p></div><div class="calibre2" title="Distributed unique IDs with Hadoop"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec134" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Distributed unique IDs with Hadoop</h2></div></div></div><p class="calibre11">Let's <a id="id766" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>consider what our Parkour mapper and reducer might look like if we were to calculate unique, cluster-wide IDs. The mapper is easy: we'll want to calculate the document frequency for each <a id="id767" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>term we encounter, so the following mapper simply returns a vector for each unique term: the first element of the vector (the key) is the term itself, and the second element (the value) is <code class="literal">1</code>.</p><div class="calibre2"><pre class="programlisting">(defn document-count-m
  {::mr/source-as :vals}
  [documents]
  (-&gt;&gt; documents
       (r/mapcat (comp distinct stemmer/stems))
       (r/map #(vector % 1))))</pre></div><p class="calibre11">The reducer's job will be to take these key/value pairs of terms to document count and reduce them such that each unique term has a unique ID. A trivial way of doing this would be to ensure that there is only one reducer on the cluster. Since all the terms would all be passed to this single process, the reducer could simply keep an internal counter and assign each term an ID in a similar way to what we did with the Clojure atom earlier. This isn't taking advantage of Hadoop's distributed capabilities, though.</p><p class="calibre11">One feature <a id="id768" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of Parkour that we haven't introduced yet is the runtime context that's accessible from within every <a id="id769" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>mapper and reducer. Parkour binds the <code class="literal">parkour.mapreduce/*context*</code> dynamic variable to the Hadoop task context of the task within which our mappers and reducers run. The task context contains, amongst other things, the following properties:</p><div class="informaltable"><table border="1" class="calibre27"><colgroup class="calibre28"><col class="calibre29"/><col class="calibre29"/><col class="calibre29"/></colgroup><thead class="calibre30"><tr class="calibre31"><th valign="bottom" class="calibre32">
<p class="calibre19">Property</p>
</th><th valign="bottom" class="calibre32">
<p class="calibre19">Type</p>
</th><th valign="bottom" class="calibre32">
<p class="calibre19">Description</p>
</th></tr></thead><tbody class="calibre33"><tr class="calibre34"><td class="calibre35">
<p class="calibre19">
<code class="literal">mapred.job.id</code>
</p>
</td><td class="calibre35">
<p class="calibre19">String</p>
</td><td class="calibre35">
<p class="calibre19">The job's ID</p>
</td></tr><tr class="calibre36"><td class="calibre35">
<p class="calibre19">
<code class="literal">mapred.task.id</code>
</p>
</td><td class="calibre35">
<p class="calibre19">int</p>
</td><td class="calibre35">
<p class="calibre19">The task attempt ID</p>
</td></tr><tr class="calibre38"><td class="calibre35">
<p class="calibre19">
<code class="literal">mapred.task.partition</code>
</p>
</td><td class="calibre35">
<p class="calibre19">int</p>
</td><td class="calibre35">
<p class="calibre19">The ID of the task within the job</p>
</td></tr></tbody></table></div><p class="calibre11">The last of these, the <code class="literal">mapred.task.partition</code> property, is the number of the task assigned by Hadoop, guaranteed to be a monotonically increasing integer unique across the cluster. This number is our task's global offset. Within each task we can also keep a local offset and output both with each word processed. The two offsets together—global and local—provide a unique identifier for the term across the cluster.</p><p class="calibre11">The following diagram visualizes the process for eight terms processed on three separate mappers:</p><div class="mediaobject"><img src="Images/7180OS_06_170.jpg" alt="Distributed unique IDs with Hadoop" class="calibre267"/></div><p class="calibre11">Each mapper is only aware of its own partition number and the term's local offset. However, these two numbers are all that's required to calculate a unique, global ID. The preceding <span class="strong1"><strong class="calibre12">Calculate Offsets</strong></span> box determines what the global offset should be for each task partition. Partition <span class="strong1"><strong class="calibre12">1</strong></span> has a global offset of <span class="strong1"><strong class="calibre12">0</strong></span>. Partition <span class="strong1"><strong class="calibre12">2</strong></span> has a global offset of <span class="strong1"><strong class="calibre12">3</strong></span>, because partition <span class="strong1"><strong class="calibre12">1</strong></span> processed <span class="strong1"><strong class="calibre12">3</strong></span> words. Partition <span class="strong1"><strong class="calibre12">3</strong></span> has an offset of <span class="strong1"><strong class="calibre12">5</strong></span>, because partitions <span class="strong1"><strong class="calibre12">1</strong></span> and <span class="strong1"><strong class="calibre12">2</strong></span> processed <span class="strong1"><strong class="calibre12">5</strong></span> words between them, and so on.</p><p class="calibre11">For the preceding <a id="id770" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>approach to work, we need to know three things: the global offset of the mapper, the local offset of the term, and the total number of terms processed by each mapper. These three <a id="id771" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>numbers can be used to define a unique, cluster-wide ID for each term. The reducer that creates these three numbers is defined as follows. It introduces a couple of new concepts that we'll discuss shortly.</p><div class="calibre2"><pre class="programlisting">(defn unique-index-r
  {::mr/source-as :keyvalgroups,
   ::mr/sink-as dux/named-keyvals}
  [coll]
  (let [global-offset (conf/get-long mr/*context*
                                     "mapred.task.partition" -1)]
    (tr/mapcat-state
     (fn [local-offset [word doc-counts]]
       [(inc local-offset)
        (if (identical? ::finished word)
          [[:counts [global-offset local-offset]]]
          [[:data [word [[global-offset local-offset]
                         (apply + doc-counts)]]]])])
     0 (r/mapcat identity [coll [[::finished nil]]]))))</pre></div><p class="calibre11">The first step the reducer performs is to fetch the <code class="literal">global-offset</code>, the task partition for this particular reducer. We're using <code class="literal">mapcat-state</code>, a function defined in the transduce library (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/brandonbloom/transduce">https://github.com/brandonbloom/transduce</a>) to build up a sequence of tuples in the <a id="id772" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>format <code class="literal">[[:data ["apple" [1 4]] [:data ["orange" [1 5]] ...]</code> where the vector of numbers <code class="literal">[1 4]</code> represents the global and local offsets respectively. Finally, when we've reached the end of this reduce task, we append a tuple to the sequence in the format <code class="literal">[:counts [1 5]]</code>. This represents the final local count, <code class="literal">5</code>, for this particular reducer partition, <code class="literal">1</code>. Thus, a single reducer is calculating all three of the elements we require to calculate all the term IDs.</p><p class="calibre11">The keyword provided to <code class="literal">::mr/source-as</code> is not one we've encountered previously. In the previous chapter, we saw how the shaping options <code class="literal">:keyvals</code>, <code class="literal">:keys</code>, and <code class="literal">:vals</code> let Parkour know how we wanted our data provided, and the structure of the data we'd be providing in return. For reducers, Parkour describes a more comprehensive set of shaping functions that account for the fact that inputs may be grouped. The following diagram <a id="id773" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>illustrates the <a id="id774" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>available options:</p><div class="mediaobject"><img src="Images/7180OS_06_175.jpg" alt="Distributed unique IDs with Hadoop" class="calibre196"/></div><p class="calibre11">The option provided to <code class="literal">::mr/sink-as</code> is not one we've encountered before either. The <code class="literal">parkour.io.dux</code> namespace provides options for de-multiplexing outputs. In practice this means that, by sinking as <code class="literal">dux/named-keyvals</code>, a single reducer can write to several different outputs. In other words, we've introduced a fork into our data pipeline: some data is written to one branch, the rest to another.</p><p class="calibre11">Having set a sink specification of <code class="literal">dux/named-keyvals</code>, the first element of our tuple will be interpreted as the destination to write to; the second element of our tuple will be treated as the key/value pair to be written. As a result, we can write out the <code class="literal">:data</code> (the local and global offset) to one destination and the <code class="literal">:counts</code> (number of terms processed by each mapper) to another.</p><p class="calibre11">The job that makes use of the mapper and reducer that we've defined is presented next. As with the Parkour job we <a id="id775" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>specified in the previous chapter, we chain together an input, map, partition, reduce, and output step.</p><div class="calibre2"><pre class="programlisting">(defn df-j [dseq]
  (-&gt; (pg/input dseq)
      (pg/map #'document-count-m)
      (pg/partition (mra/shuffle [:string :long]))
      (pg/reduce #'unique-index-r)
      (pg/output :data (mra/dsink [:string index-value])
                 :counts (mra/dsink [:long :long]))))</pre></div><p class="calibre11">There are two primary differences between the preceding code and the job specification we've seen previously. Firstly, our output specifies two named sinks: one for each of the outputs of our reducer. Secondly, we're using the <code class="literal">parkour.io.avro</code> namespace as <code class="literal">mra</code> to specify a schema for our data with <code class="literal">(mra/dsink [:string long-pair])</code>.</p><p class="calibre11">In the previous chapter we made use of Tesser's <code class="literal">FressianWritable</code> to serialize arbitrary Clojure data structures to disk. This worked because the contents of the <code class="literal">FressianWritable</code> did not need to be interpreted by Hadoop: the value was completely opaque. With Parkour, we have the option to define custom key/value pair types. Since the key and value do need to be interpreted as separate entities by Hadoop (for the purpose of reading, partitioning, and writing sequence files), Parkour allows us to define a "tuple schema" using the <code class="literal">parkour.io.avro</code> namespace, which explicitly defines the type of the key and the value. <code class="literal">long-pair</code> is a custom schema used to store both the local and global offset in a single tuple:</p><div class="calibre2"><pre class="programlisting">(def long-pair (avro/tuple-schema [:long :long]))</pre></div><p class="calibre11">And, since <a id="id776" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>schemas are composable, we can refer to the <code class="literal">long-pair</code> schema when defining our output schema: <code class="literal">(mra/dsink [:string long-pair])</code>.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note56" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Parkour uses the <a id="id777" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>library <code class="literal">Acbracad</code> to serialize Clojure data structures using Avro. For more information about serialization options consult the documentation for Abracad at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/damballa/abracad">https://github.com/damballa/abracad</a>.</p></div></div><p class="calibre11">Let's look at another feature of Hadoop that Parkour exposes which allows our term ID job to be more efficient <a id="id778" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>than it would otherwise be: the distributed cache.</p></div><div class="calibre2" title="Sharing data with the distributed cache"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec135" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Sharing data with the distributed cache</h2></div></div></div><p class="calibre11">As we <a id="id779" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>discussed in the previous section, if we know the local offset of each word for a particular mapper, and we know how many records each mapper processed overall, then we're in a position to calculate a unique, contiguous ID for each word.</p><p class="calibre11">The diagram that <a id="id780" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>showed the process a few pages ago contained two central boxes each labeled <span class="strong1"><strong class="calibre12">Calculate Offsets</strong></span> and a <span class="strong1"><strong class="calibre12">Global ID</strong></span>. Those boxes map directly to the functions that we present next:</p><div class="calibre2"><pre class="programlisting">(defn global-id [offsets [global-offset local-offset]]
  (+ local-offset (get offsets global-offset)))

(defn calculate-offsets [dseq]
  (-&gt;&gt; (into [] dseq)
       (sort-by first)
       (reductions (fn [[_ t] [i n]]
                     [(inc i) (+ t n)])
                   [0 0])
       (into {})))</pre></div><p class="calibre11">Once we've calculated the map of offsets to use for generating unique IDs, we'd really like them to be available to all of our map and reduce tasks as a shared resource. Having generated the offsets in a distributed fashion, we'd like to consume it in a distributed fashion too.</p><p class="calibre11">The distributed <a id="id781" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cache is Hadoop's way of allowing tasks to access common data. This is a much more efficient way of sharing small quantities of data (data that's small enough to reside in memory) than through potentially costly data joins.</p><div class="mediaobject"><img src="Images/7180OS_06_180.jpg" alt="Sharing data with the distributed cache" class="calibre268"/></div><p class="calibre11">Before reading from the distributed cache, we have to write something to it. This can be achieved with <a id="id782" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Parkour's <code class="literal">parkour.io.dval</code> namespace:</p><div class="calibre2"><pre class="programlisting">(defn unique-word-ids [conf df-data df-counts]
  (let [offsets-dval (-&gt; (calculate-offsets df-counts)
                         (dval/edn-dval))]
    (-&gt; (pg/input df-data)
        (pg/map #'word-id-m offsets-dval)
        (pg/output (mra/dsink [word-id]))
        (pg/fexecute conf `word-id)
        (-&gt;&gt; (r/map parse-idf)
             (into {}))
        (dval/edn-dval))))</pre></div><p class="calibre11">Here, we're writing two sets of data to the distributed cache with the <code class="literal">dval/edn-dval</code> function. The first is the result of the <code class="literal">calculate-offsets</code> function just defined which is passed to the <code class="literal">word-id-m</code> mappers for their use. The second set of data written to the distributed <a id="id783" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cache is their output. We'll see how this is generated in the <code class="literal">word-id-m</code> function, as follows:</p><div class="calibre2"><pre class="programlisting">(defn word-id-m
  {::mr/sink-as :keys}
  [offsets-dval coll]
  (let [offsets @offsets-dval]
    (r/map
     (fn [[word word-offset]]
       [word (global-id offsets word-offset)])
     coll)))</pre></div><p class="calibre11">The value returned by <code class="literal">dval/edn-dval</code> implements the <code class="literal">IDRef</code> interface. This means that we can use Clojure's <code class="literal">deref</code> function (or the deref macro character <code class="literal">@</code>) to retrieve the value that it wraps, just as we do with Clojure's atoms. Dereferencing the distributed value the first time causes the <a id="id784" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data to be downloaded from the distributed cache to a local mapper cache. Once the data is available locally, Parkour takes care of reconstructing the Clojure data structure (the map of offsets) that we wrote to it in EDN format.</p></div><div class="calibre2" title="Building Mahout vectors from input documents"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec136" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Building Mahout vectors from input documents</h2></div></div></div><p class="calibre11">In the <a id="id785" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>previous sections, we took a detour to introduce several new Parkour and Hadoop concepts, but we're finally in a position to build text vectors for Mahout using unique IDs for every term. Some further code is omitted for brevity but the whole job is available to <a id="id786" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>view in the <code class="literal">cljds.ch6.vectorizer</code> example code namespace. </p><p class="calibre11">As mentioned previously, Mahout's implementation of <span class="strong1"><em class="calibre13">k</em></span>-means expects us to provide a vector representation of our input using one of its vector classes. Since our dictionary is large, and most documents use few of these terms, we'll be using a sparse vector representation. The following code makes use of a <code class="literal">dictionary</code> distributed value to create a <code class="literal">org.apache.mahout.math.RandomAccessSparseVector</code> for every input document:</p><div class="calibre2"><pre class="programlisting">(defn create-sparse-tfidf-vector [dictionary [id doc]]
  (let [vector (RandomAccessSparseVector. (count dictionary))]
    (doseq [[term tf] (-&gt; doc stemmer/stems frequencies)]
      (let [term-info (get dictionary term)
            id  (:id term-info)
            idf (:idf term-info)]
        (.setQuick vector id (* tf idf))))
    [id vector]))

(defn create-tfidf-vectors-m [dictionary coll]
  (let [dictionary @dictionary]
    (r/map #(create-sparse-tfidf-vector dictionary %) coll)))</pre></div><p class="calibre11">Finally, we make <a id="id787" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>use of the <code class="literal">create-tfidf-vectors-m</code> function, which brings everything we've covered together into a single Hadoop job:</p><div class="calibre2"><pre class="programlisting">(defn tfidf [conf dseq dictionary-path vector-path]
  (let [doc-count (-&gt;&gt; dseq (into []) count)
        [df-data df-counts] (pg/execute (df-j dseq) conf df)
        dictionary-dval (make-dictionary conf df-data
                                         df-counts doc-count)]
    (write-dictionary dictionary-path dictionary-dval)
    (-&gt; (pg/input dseq)
        (pg/map #'create-tfidf-vectors-m dictionary-dval)
        (pg/output (seqf/dsink [Text VectorWritable] vector-path))
        (pg/fexecute conf `vectorize))))</pre></div><p class="calibre11">This task handles the creation of the dictionary, writing the dictionary to the distributed cache, and then using the dictionary—with the mapper we just defined—to convert each input document to a Mahout vector. To ensure sequence file compatibility with Mahout, we set the key/value classes of our final output to be <code class="literal">Text</code> and <code class="literal">VectorWritable</code>, where the key is the original filename of the document and the value is the Mahout vector representation of the contents.</p><p class="calibre11">We can call this job by running:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-18 []
  (let [input-path  "data/reuters-sequencefile" 
        output-path "data/reuters-vectors"]
    (vectorizer/tfidf-job (conf/ig) input-path output-path)))</pre></div><p class="calibre11">The job will <a id="id788" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>write the dictionary out to the <code class="literal">dictionary-path</code> (we'll be needing it again), and the vectors out to the <code class="literal">vector-path</code>.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="tip08" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre22">Since running the preceding example is a prerequisite for subsequent examples, it's also available on the command line:</p><div class="calibre2"><pre class="programlisting1">
<span class="strong1"><strong class="calibre12">lein create-vectors</strong></span>
</pre></div></div></div><p class="calibre11">Next, we'll discover how to use these vectors to actually perform clustering with Mahout.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Running k-means clustering with Mahout"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec110" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Running k-means clustering with Mahout</h1></div></div></div><p class="calibre11">Now that we <a id="id789" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have a sequence file of vectors suitable for consumption by Mahout, it's time to actually run <span class="strong1"><em class="calibre13">k</em></span>-means clustering on the whole dataset. Unlike our local Incanter version, Mahout won't have any trouble dealing with the full Reuters corpus.</p><p class="calibre11">As with the <code class="literal">SequenceFilesFromDirectory</code> class, we've created a wrapper around another of Mahout's command-line programs, <code class="literal">KMeansDriver</code>. The Clojure variable names make it easier to see what each command-line argument is for.</p><div class="calibre2"><pre class="programlisting">(defn run-kmeans [in-path clusters-path out-path k]
  (let [distance-measure  "org.apache.mahout.common.distance.CosineDistanceMeasure"
        max-iterations    100
        convergence-delta 0.001]
    (KMeansDriver/main
     (-&gt;&gt; (vector "-i"  in-path
                  "-c"  clusters-path
                  "-o"  out-path
                  "-dm" distance-measure
                  "-x"  max-iterations
                  "-k"  k
                  "-cd" convergence-delta
                  "-ow"
                  "-cl")
          (map str)
          (into-array String)))))</pre></div><p class="calibre11">We're providing the string <code class="literal">org.apache.mahout.common.distance.CosineDistanceMeasure</code> to indicate to the driver that we'd like to use Mahout's implementation of the cosine distance measure. Mahout also includes a <code class="literal">EuclideanDistanceMeasure</code> and a <code class="literal">TanimotoDistanceMeasure</code> (similar to the Jaccard distance, the complement of the Jaccard index, but one that will operate on vectors rather than sets). Several other distance measures are also defined; consult the Mahout documentation for all the available options.</p><p class="calibre11">With the preceding <code class="literal">run-kmeans</code> function in place, we simply need to let Mahout know where to access our files. As in the previous chapter, we assume Hadoop is running in local mode and all file paths are relative to the project root:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-19 []
  (run-kmeans "data/reuters-vectors/vectors"
              "data/kmeans-clusters/clusters"
              "data/kmeans-clusters"
              10))</pre></div><p class="calibre11">This example may run for a little while as Mahout iterates over our large dataset.</p><div class="calibre2" title="Viewing k-means clustering results"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec137" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Viewing k-means clustering results</h2></div></div></div><p class="calibre11">Once it's finished, we'll want to see a cluster summary for each cluster as we did with our Incanter <a id="id790" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>implementation. Fortunately, Mahout defines a <code class="literal">ClusterDumper</code> class that will do exactly this for us. We need to provide the location of our clusters, of course, but we'll provide the location of our dictionary, too. Providing the dictionary means that the output can return the top terms for each cluster.</p><div class="calibre2"><pre class="programlisting">(defn run-cluster-dump [in-path dict-path points-dir out-path]
  (let [distance-measure
        "org.apache.mahout.common.distance.CosineDistanceMeasure"]
    (ClusterDumper/main
     (-&gt;&gt; (vector "-i" in-path
                  "-o" out-path
                  "-d" dict-path
                  "--pointsDir" points-dir
                  "-dm" distance-measure
                  "-dt" "sequencefile"
                  "-b" "100"
                  "-n" "20"
                  "-sp" "0"
                  "--evaluate")
          (map str)
          (into-array String)))))</pre></div><p class="calibre11">Next, we define the code that will actually call the <code class="literal">run-cluster-dump</code> function:</p><div class="calibre2"><pre class="programlisting">(defn path-for [path]
  (-&gt; (fs/glob path)
      (first)
      (.getAbsolutePath)))

(defn ex-6-20 []
  (run-cluster-dump
   (path-for "data/kmeans-clusters/clusters-*-final")
   "data/reuters-vectors/dictionary/part-r-00000"
   "data/kmeans-clusters/clusteredPoints"
   "data/kmeans-clusterdump"))</pre></div><p class="calibre11">We're making use of the <code class="literal">me.raynes.fs</code> library once again to determine which directory the final clusters are contained by. Mahout will append <code class="literal">-final</code> to the directory containing the final clusters, but we don't know ahead of time which directory this will be. The <code class="literal">fs/glob</code> function will find a directory that matches the pattern <code class="literal">clusters-*-final</code>, and replace <code class="literal">*</code> <a id="id791" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with whichever iteration number the true directory name contains.</p></div><div class="calibre2" title="Interpreting the clustered output"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec138" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Interpreting the clustered output</h2></div></div></div><p class="calibre11">If you open the <a id="id792" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>file created by the previous example, <code class="literal">data/kmeans-clusterdump</code>, in any text editor, you'll see output representing the top terms of the Mahout clusters. The file will be large, but an excerpt is provided next:</p><div class="calibre2"><pre class="programlisting">:VL-11417{n=312 c=[0.01:0.039, 0.02:0.030, 0.07:0.047, 0.1:0.037, 0.10:0.078, 0.11:0.152, 0.12:0.069,
  Top Terms:
    tonnes              =&gt;   2.357810452962533
    department          =&gt;   1.873890568048526
    wheat               =&gt;  1.7797807546762319
    87                  =&gt;  1.6685682321206117
    u.s                 =&gt;   1.634764205186795
    mln                 =&gt;  1.5050923755535712
    agriculture         =&gt;  1.4595903158187866
    ccc                 =&gt;  1.4314624499051998
    usda                =&gt;  1.4069041441648433
    dlrs                =&gt;  1.2770121846443567</pre></div><p class="calibre11">The first line contains information about the cluster: the ID (in this case <code class="literal">VL-11417</code>) followed by curly braces containing the size of the cluster and the location of the cluster centroid. Since the text has been converted to weights and numeric IDs, the centroid is impossible to interpret on its own. The top terms beneath the centroid description hint at the contents of the cluster, though; they're the terms around which the cluster has coalesced.</p><div class="calibre2"><pre class="programlisting">VL-12535{n=514 c=[0:0.292, 0.25:0.015, 0.5:0.012, 00:0.137, 00.46:0.018, 00.50:0.036, 00.91:0.018, 0
  Top Terms:
    president           =&gt;   3.330068911559851
    reagan              =&gt;   2.485271333256584
    chief               =&gt;  2.1148699971952327
    senate              =&gt;   1.876725117983985
    officer             =&gt;  1.8531712558019022
    executive           =&gt;  1.7373591731030653
    bill                =&gt;  1.6326750159727461
    chairman            =&gt;  1.6280977206471365
    said                =&gt;  1.6279512813119108
    house               =&gt;  1.5771017798189988</pre></div><p class="calibre11">The two clusters earlier hint at two clear topics present in the data set, although your clusters may be different due to the stochastic nature of the <span class="strong1"><em class="calibre13">k</em></span>-means algorithm.</p><p class="calibre11">Depending on your initial centroids and how many iterations you let the algorithm run, you may see clusters that appear "better" or "worse" in some respect. This will be based on an instinctive response to how well the clustered terms go together. But often it's not clear simply by looking at the top terms how well clustering has performed. In any case, instinct is not a very reliable way of judging the quality of an unsupervised learning algorithm. What we'd <a id="id793" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>ideally like is some quantitative measure for how well the clustering has performed.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Cluster evaluation measures"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec111" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Cluster evaluation measures</h1></div></div></div><p class="calibre11">At the bottom of the <a id="id794" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>file we looked at in the previous section, you'll see some statistics that suggest how well the data has been clustered:</p><div class="calibre2"><pre class="programlisting">Inter-Cluster Density: 0.6135607681542804
Intra-Cluster Density: 0.6957348405534836</pre></div><p class="calibre11">These two numbers can be considered as the equivalent to the variance within and the variance between measures we have seen in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch02.xhtml" title="Chapter 2. Inference">Chapter 2</a>, <span class="strong1"><em class="calibre13">Inference</em></span> and <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <span class="strong1"><em class="calibre13">Correlation</em></span>. Ideally, we are seeking a lower variance (or a higher density) within clusters compared to the density between clusters.</p><div class="calibre2" title="Inter-cluster density"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec139" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Inter-cluster density</h2></div></div></div><p class="calibre11">Inter-cluster <a id="id795" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>density is the average distance between cluster centroids. Good clusters probably don't have centers that are too close to <a id="id796" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>each other. If they did, it would indicate the clustering is creating groups with similar features, and perhaps drawing distinctions between cluster members that are hard to support.</p><div class="mediaobject"><img src="Images/7180OS_06_190.jpg" alt="Inter-cluster density" class="calibre269"/></div><p class="calibre11">Thus, ideally our <a id="id797" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>clustering will produce clusters <a id="id798" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with a <span class="strong1"><strong class="calibre12">large inter-cluster distance</strong></span>.</p></div><div class="calibre2" title="Intra-cluster density"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec140" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Intra-cluster density</h2></div></div></div><p class="calibre11">By contrast, the <a id="id799" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>intra-cluster density is a measure of <a id="id800" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>how compact the clusters are. Ideally, clustering will identify groups of items that are similar to each other. Compact clusters indicate that all of the items within a cluster are strongly alike.</p><div class="mediaobject"><img src="Images/7180OS_06_200.jpg" alt="Intra-cluster density" class="calibre270"/></div><p class="calibre11">The best clustering outcomes therefore produce compact, distinct clusters with a <span class="strong1"><strong class="calibre12">high intra-cluster density</strong></span> and a <span class="strong1"><strong class="calibre12">low inter-cluster density</strong></span>.</p><p class="calibre11">It is not always clear how many clusters are justified by the data, though. Consider the following that shows the same dataset grouped into varying numbers of clusters. It's hard to say with any degree of <a id="id801" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>confidence what <a id="id802" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the ideal number of clusters is.</p><div class="mediaobject"><img src="Images/7180OS_06_210.jpg" alt="Intra-cluster density" class="calibre271"/></div><p class="calibre11">Although the preceding illustration is contrived, it illustrates a general issue with clustering data. There is often no one, clear "best" number of clusters. The most effective clustering will depend to a large degree on the ultimate use of the data.</p><p class="calibre11">We can however infer which might be better values of <span class="strong1"><em class="calibre13">k</em></span> by determining how the value of some quality score varies with the number of clusters. The quality score could be a statistic such as the inter- or intra-cluster density. As the number of clusters approaches its ideal, we would expect the value of this quality score to improve. Conversely, as the number of clusters diverges from its ideal we would expect the quality to decrease. To get a reasonable idea of how many clusters are justified in the dataset, therefore, we should run the algorithm many times for different values of <span class="strong1"><em class="calibre13">k</em></span>.</p></div><div class="calibre2" title="Calculating the root mean square error with Parkour"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec141" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating the root mean square error with Parkour</h2></div></div></div><p class="calibre11">One of the <a id="id803" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>most common measures of cluster quality is the <span class="strong1"><strong class="calibre12">sum of squared errors</strong></span> (<span class="strong1"><strong class="calibre12">SSE</strong></span>). For each point, the <a id="id804" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>error is the measured distance to the nearest <a id="id805" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cluster centroid. The total clustering SSE is therefore the sum over all clusters for a clustered point to its corresponding centroid:</p><div class="mediaobject"><img src="Images/7180OS_06_11.jpg" alt="Calculating the root mean square error with Parkour" class="calibre272"/></div><p class="calibre11">where <span class="strong1"><em class="calibre13">µ</em></span><sub class="calibre25">i</sub> is the centroid of points in cluster <span class="strong1"><em class="calibre13">S</em></span><sub class="calibre25">i</sub>, <span class="strong1"><em class="calibre13">k</em></span> is the total number of clusters and <span class="strong1"><em class="calibre13">n</em></span> is the total number of points.</p><p class="calibre11">To calculate the <span class="strong1"><em class="calibre13">RMSE</em></span> in Clojure we therefore need to be able to relate each point in the cluster to its corresponding cluster centroid. Mahout saves cluster centroids and clustered points in two separate files, so in the next section we'll combine them.</p><div class="calibre2" title="Loading clustered points and centroids"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch06lvl3sec18" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Loading clustered points and centroids</h3></div></div></div><p class="calibre11">Given a <a id="id806" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>parent directory (e.g. <code class="literal">data/reuters-kmeans/kmeans-10</code>), the following function will load points into vectors stored in a map indexed by cluster ID using Parkour's <code class="literal">seqf/dseq</code> function to load key/value pairs from a sequence file. In this case, the key is the cluster ID (as an integer) and the value is the TF-IDF vector.</p><div class="calibre2"><pre class="programlisting">(defn load-cluster-points [dir]
  (-&gt;&gt; (points-path dir)
       (seqf/dseq)
       (r/reduce
        (fn [accum [k v]]
          (update-in accum [k] conj v)) {})))</pre></div><p class="calibre11">The output of the preceding function is a map keyed by cluster ID whose values are sequences of clustered points. Likewise, the following function will convert each cluster into a map, keyed by cluster ID, whose values are maps containing the keys <code class="literal">:id</code> and <code class="literal">:centroid</code>.</p><div class="calibre2"><pre class="programlisting">(defn load-cluster-centroids [dir]
  (let [to-tuple (fn [^Cluster kluster]
                   (let [id (.getId kluster)]
                     [id  {:id id
                           :centroid (.getCenter kluster)}]))]
    (-&gt;&gt; (centroids-path dir)
         (seqf/dseq)
         (r/map (comp to-tuple last))
         (into {}))))</pre></div><p class="calibre11">Having two maps keyed by cluster ID means that combining the clustered points and cluster centroids is a simple matter of calling <code class="literal">merge-with</code> on the maps supplying a custom merging function. In the following code, we merge the clustered points into the map containing <a id="id807" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the cluster <code class="literal">:id</code> and <code class="literal">:centroid</code>.</p><div class="calibre2"><pre class="programlisting">(defn assoc-points [cluster points]
  (assoc cluster :points points))

(defn load-clusters [dir]
  (-&gt;&gt; (load-cluster-points dir)
       (merge-with assoc-points
                   (load-cluster-centroids dir))
       (vals)))</pre></div><p class="calibre11">The final output is a single map, keyed by cluster ID, with each value as a map of <code class="literal">:id</code>, <code class="literal">:centroid</code> and <code class="literal">:points</code>. We'll use this map in the next section to calculate the clustering RMSE.</p></div></div><div class="calibre2" title="Calculating the cluster RMSE"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec142" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating the cluster RMSE</h2></div></div></div><p class="calibre11">To <a id="id808" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculate the RMSE, we need to be able to <a id="id809" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>establish the distance between every point and its associated cluster centroid. Since we used Mahout's <code class="literal">CosineDistanceMeasure</code> to perform the initial clustering, we should use the cosine distance to evaluate the clustering as well. In fact, we can simply make use of Mahout's implementation.</p><div class="calibre2"><pre class="programlisting">(def measure
  (CosineDistanceMeasure.))

(defn distance [^DistanceMeasure measure a b]
  (.distance measure a b))

(defn centroid-distances [cluster]
  (let [centroid (:centroid cluster)]
    (-&gt;&gt; (:points cluster)
         (map #(distance measure centroid %)))))

(defn squared-errors [cluster]
  (-&gt;&gt; (centroid-distances cluster)
       (map i/sq)))

(defn root-mean-square-error [clusters]
  (-&gt;&gt; (mapcat squared-errors clusters)
       (s/mean)
       (i/sqrt)))</pre></div><p class="calibre11">If the RMSE is plotted against the number of clusters, you'll find that it declines as the number of clusters increases. A single cluster will have the highest RMSE error (the variance of the original dataset from the mean), whereas the lowest RMSE will be the degenerate case <a id="id810" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>when every point is in its own cluster (an RMSE of zero). Clearly either of these extremes will provide a poor <a id="id811" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>explanation for the structure of the data. However, the RMSE doesn't decline in a straight line. It declines sharply as the number of clusters is increased from 1, but will fall more slowly once the "natural" number of clusters has been exceeded.</p><p class="calibre11">Therefore, one way of judging the ideal number of clusters is to plot how the RMSE changes with respect to the number of clusters. This is called the <span class="strong1"><strong class="calibre12">elbow method</strong></span>.</p></div><div class="calibre2" title="Determining optimal k with the elbow method"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec143" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Determining optimal k with the elbow method</h2></div></div></div><p class="calibre11">In order <a id="id812" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to determine the <a id="id813" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>value of <span class="strong1"><em class="calibre13">k</em></span> using the elbow <a id="id814" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>method, we're going to have to re-run <span class="strong1"><em class="calibre13">k</em></span>-means a number of times. The following code accomplishes this for all <span class="strong1"><em class="calibre13">k</em></span> between <code class="literal">2</code> and <code class="literal">21</code>.</p><div class="calibre2"><pre class="programlisting">(defn ex-6-21 []
  (doseq [k (range 2 21)
          :let [dir (str "data/kmeans-clusters-" k)]]
    (println dir)
    (run-kmeans "data/reuters-vectors/vectors"
                (str dir "/clusters")
                dir k)))</pre></div><p class="calibre11">This will take a little while to run, so it might be time to go and make a hot drink: the <code class="literal">println</code> statement will log each clustering run to let you know how much progress has been made. On my laptop the whole process takes about 15 minutes.</p><p class="calibre11">Once it's complete, you should be able to run the example to generate a scatter plot of the RMSE for each of the clustered values:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-22 []
  (let [ks (range 2 21)
        ys (for [k ks
                 :let [dir (str "data/kmeans-clusters-" k)
                       clusters (load-clusters dir)]]
             (root-mean-square-error clusters))]
    (-&gt; (c/scatter-plot ks ys
                        :x-label "k"
                        :y-label "RMSE")
        (i/view))))</pre></div><p class="calibre11">This should <a id="id815" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>return <a id="id816" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a plot similar to the following:</p><div class="mediaobject"><img src="Images/7180OS_06_220.jpg" alt="Determining optimal k with the elbow method" class="calibre45"/></div><p class="calibre11">The preceding scatter plot shows the RMSE plotted against the number of clusters. It should be clear how the rate of RMSE change slows as <span class="strong1"><em class="calibre13">k</em></span> exceeds around 13 clusters and increasing the number of clusters further yields diminishing returns. Therefore, the preceding chart suggests for our Reuters data that around 13 clusters is a good choice.</p><p class="calibre11">The elbow method provides an intuitive means to determine the ideal number of clusters, but it is sometimes hard to apply in practice. This is because we must interpret the shape of the curve defined by the RMSE for each <span class="strong1"><em class="calibre13">k</em></span>. If <span class="strong1"><em class="calibre13">k</em></span> is small, or the RMSE contains a lot of noise, it may not <a id="id817" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>be <a id="id818" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>apparent where the elbow falls, or if there is an elbow at all.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note57" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Since clustering is an unsupervised learning algorithm, we assume here that the internal structure of the clusters is the only means of validating the quality of clustering. If true cluster labels are known then it's possible to use external validation measures (such as entropy) of the kind that we encountered in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Classification">Chapter 4</a>, <span class="strong1"><em class="calibre13">Classification</em></span> to validate the success of the model.</p></div></div><p class="calibre11">Other clustering <a id="id819" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>evaluation schemes aim to provide a clearer means of determining the precise number of clusters. The two we'll cover are the Dunn index and the Davies-Bouldin index. Both are internal evaluation schemes, meaning that they only look at the structure of the clustered data. Each aims to identify the clustering that has produced the most compact, well-separated clusters, in different ways.</p></div><div class="calibre2" title="Determining optimal k with the Dunn index"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec144" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Determining optimal k with the Dunn index</h2></div></div></div><p class="calibre11">The <a id="id820" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Dunn index offers an alternative way to choose the optimal number of <span class="strong1"><em class="calibre13">k</em></span>. Rather than considering the <a id="id821" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>average error remaining in the clustered <a id="id822" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data, the Dunn index instead considers the ratio of two "worst-case" situations: the minimum distance between two cluster centroids, divided by the maximum cluster diameter. A higher index therefore indicates better clustering since in general we would like large inter-cluster distances and small intra-cluster distances.</p><p class="calibre11">For <span class="strong1"><em class="calibre13">k</em></span> clusters we can express the Dunn index in the following way:</p><div class="mediaobject"><img src="Images/7180OS_06_12.jpg" alt="Determining optimal k with the Dunn index" class="calibre273"/></div><p class="calibre11">where <span class="strong1"><em class="calibre13">δ(C<sub class="calibre25">i</sub>,C<sub class="calibre25">j</sub>)</em></span> distance between the two clusters <span class="strong1"><em class="calibre13">C</em></span><sub class="calibre25">i</sub> and <span class="strong1"><em class="calibre13">C</em></span><sub class="calibre25">j</sub> and <span class="inlinemediaobject"><img src="Images/7180OS_06_13.jpg" alt="Determining optimal k with the Dunn index" class="calibre274"/></span> represents the size (or scatter) of the largest cluster.</p><p class="calibre11">There are several possible ways to calculate the scatter of a cluster. We could take the distance between the furthest two points inside a cluster, or the mean of all the pairwise distances between data points inside the cluster, or the mean of each data point from the cluster <a id="id823" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>centroid itself. In the following code, <a id="id824" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we calculate the size by taking <a id="id825" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the median distance from the cluster centroid.</p><div class="calibre2"><pre class="programlisting">(defn cluster-size [cluster]
  (-&gt; cluster
      centroid-distances
      s/median))

(defn dunn-index [clusters]
  (let [min-separation (-&gt;&gt; (combinations clusters 2)
                            (map #(apply separation %))
                            (apply min))
        max-cluster-size (-&gt;&gt; (map cluster-size clusters)
                              (apply max))]
    (/ min-separation max-cluster-size)))</pre></div><p class="calibre11">The preceding code <a id="id826" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>makes use of the <code class="literal">combinations</code> function from <code class="literal">clojure.math.combinatorics</code> (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clojure/math.combinatorics/">https://github.com/clojure/math.combinatorics/</a>) to produce a lazy sequence of all pairwise combinations of clusters.</p><div class="calibre2"><pre class="programlisting">(defn ex-6-23 []
  (let [ks (range 2 21)
        ys (for [k ks
                 :let [dir (str "data/kmeans-clusters-" k)
                       clusters (load-clusters dir)]]
             (dunn-index clusters))]
    (-&gt; (c/scatter-plot ks ys
                        :x-label "k"
                        :y-label "Dunn Index")
        (i/view))))</pre></div><p class="calibre11">We use the <code class="literal">dunn-index</code> function in the preceding code to generate a scatter plot for the clusters from <span class="strong1"><em class="calibre13">k=2</em></span> to <span class="strong1"><em class="calibre13">k=20</em></span>:</p><div class="mediaobject"><img src="Images/7180OS_06_230.jpg" alt="Determining optimal k with the Dunn index" class="calibre45"/></div><p class="calibre11">A higher Dunn index indicates a better clustering. Thus, it appears that the best clustering is for <span class="strong1"><em class="calibre13">k=2</em></span>, followed by <span class="strong1"><em class="calibre13">k=6</em></span>, with <span class="strong1"><em class="calibre13">k=12</em></span> and <span class="strong1"><em class="calibre13">k=13</em></span> following closely behind. Let's try an alternative <a id="id827" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cluster <a id="id828" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>evaluation scheme and compare the <a id="id829" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>results.</p></div><div class="calibre2" title="Determining optimal k with the Davies-Bouldin index"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec145" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Determining optimal k with the Davies-Bouldin index</h2></div></div></div><p class="calibre11">The <a id="id830" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Davies-Bouldin index is an alternative evaluation scheme that measures the mean ratio of size <a id="id831" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and separation for all values in the cluster. For each cluster, an alternative cluster is <a id="id832" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>found that maximizes the ratio of the sum of cluster sizes divided by the inter-cluster distance. The Davies-Bouldin index is defined as the mean of this value for all clusters in the data:</p><div class="mediaobject"><img src="Images/7180OS_06_14.jpg" alt="Determining optimal k with the Davies-Bouldin index" class="calibre275"/></div><div class="mediaobject"><img src="Images/7180OS_06_15.jpg" alt="Determining optimal k with the Davies-Bouldin index" class="calibre276"/></div><p class="calibre11">where <span class="strong1"><em class="calibre13">δ(C<sub class="calibre25">i</sub>,C<sub class="calibre25">j</sub>)</em></span> is the distance between the two cluster centroids <span class="strong1"><em class="calibre13">C</em></span><sub class="calibre25">i</sub> and <span class="strong1"><em class="calibre13">C</em></span><sub class="calibre25">j</sub>, and <span class="strong1"><em class="calibre13">S</em></span><sub class="calibre25">i</sub> and <span class="strong1"><em class="calibre13">S</em></span><sub class="calibre25">j</sub> are the scatter. We <a id="id833" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can calculate the Davies-Bouldin index using the following code:</p><div class="calibre2"><pre class="programlisting">(defn scatter [cluster]
  (-&gt; (centroid-distances cluster)
      (s/mean)))

(defn assoc-scatter [cluster]
  (assoc cluster :scatter (scatter cluster)))

(defn separation [a b]
  (distance measure (:centroid a) (:centroid b)))

(defn davies-bouldin-ratio [a b]
  (/ (+ (:scatter a)
        (:scatter b))
     (separation a b)))

(defn max-davies-bouldin-ratio [[cluster &amp; clusters]]
  (-&gt;&gt; (map #(davies-bouldin-ratio cluster %) clusters)
       (apply max)))

(defn rotations [xs]
  (take (count xs)
        (partition (count xs) 1 (cycle xs))))

(defn davies-bouldin-index [clusters]
  (let [ds (-&gt;&gt; (map assoc-scatter clusters)
                (rotations)
                (map max-davies-bouldin-ratio))]
    (s/mean ds)))</pre></div><p class="calibre11">Let's now <a id="id834" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>plot the Davies-Bouldin on a scatter plot for clusters <span class="strong1"><em class="calibre13">k=2</em></span> to <span class="strong1"><em class="calibre13">k=20</em></span>:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-24 []
  (let [ks (range 2 21)
        ys (for [k ks
                 :let [dir (str "data/kmeans-clusters-" k)
                       clusters (load-clusters dir)]]
             (davies-bouldin-index clusters))]
    (-&gt; (c/scatter-plot ks ys
                        :x-label "k"
                        :y-label "Davies-Bouldin Index")
        (i/view))))</pre></div><p class="calibre11">This should <a id="id835" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>generate the following <a id="id836" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>plot:</p><div class="mediaobject"><img src="Images/7180OS_06_240.jpg" alt="Determining optimal k with the Davies-Bouldin index" class="calibre45"/></div><p class="calibre11">Unlike the Dunn index, the Davies-Bouldin index is minimized for good clustering schemes since in general we seek out clusters that are compact in size and have high inter-cluster <a id="id837" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>distances. The <a id="id838" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>preceding <a id="id839" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chart suggests that <span class="strong1"><em class="calibre13">k=2</em></span> is the ideal cluster size followed by <span class="strong1"><em class="calibre13">k=13</em></span>.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The drawbacks of k-means"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec112" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The drawbacks of k-means</h1></div></div></div><p class="calibre11">
<span class="strong1"><em class="calibre13">k</em></span>-means is one of the <a id="id840" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>most popular clustering algorithms due to its relative ease of implementation and the fact that it can be made to scale well to very large datasets. In spite of its popularity, there are several drawbacks.</p><p class="calibre11">
<span class="strong1"><em class="calibre13">k</em></span>-means is stochastic, and does not guarantee to find the global optimum solution for clustering. In fact, the algorithm can be very sensitive to outliers and noisy data: the quality of the final clustering can be highly dependent on the position of the initial cluster centroids. In other words, <span class="strong1"><em class="calibre13">k</em></span>-means will regularly discover a local rather than global minimum.</p><div class="mediaobject"><img src="Images/7180OS_06_250.jpg" alt="The drawbacks of k-means" class="calibre277"/></div><p class="calibre11">The preceding diagram illustrates how <span class="strong1"><em class="calibre13">k</em></span>-means may converge to a local minimum based on poor initial cluster centroids. Non-optimal clustering may even occur if the initial cluster centroids are well-placed, since <span class="strong1"><em class="calibre13">k</em></span>-means prefers clusters with similar sizes and densities. Where <a id="id841" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>clusters are not approximately equal in size and density, <span class="strong1"><em class="calibre13">k</em></span>-means may fail to converge to the most natural clustering:</p><div class="mediaobject"><img src="Images/7180OS_06_260.jpg" alt="The drawbacks of k-means" class="calibre278"/></div><p class="calibre11">Also, <span class="strong1"><em class="calibre13">k</em></span>-means strongly prefers clusters that are "globular" in shape. Clusters with more intricate shapes are not well-identified by the <span class="strong1"><em class="calibre13">k</em></span>-means algorithm.</p><p class="calibre11">In the next chapter, we'll see how a variety of dimensionality reduction techniques can help work around these problems. But before we get there, let's develop an intuition for an alternative way of <a id="id842" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>defining distance: as a measure of how far away from a "group" of things an element is.</p><div class="calibre2" title="The Mahalanobis distance measure"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch06lvl2sec146" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The Mahalanobis distance measure</h2></div></div></div><p class="calibre11">We saw at the <a id="id843" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>beginning of the chapter how some distance measures may be more appropriate than others, given your data, by showing how the <a id="id844" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Jaccard, Euclidean, and cosine measures relate to data representation. Another factor to consider when choosing a distance measure and clustering algorithm is the internal structure of your data. Consider the following scatter plot:</p><div class="mediaobject"><img src="Images/7180OS_06_270.jpg" alt="The Mahalanobis distance measure" class="calibre45"/></div><p class="calibre11">It's "obvious" that the point indicated by the arrow is distinct from the other points. We can clearly see <a id="id845" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that it's far from the distribution of the others and therefore represents an anomaly. Yet, if we calculate the Euclidean distance <a id="id846" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of all points from the mean (the "centroid"), the point will be lost amongst the others that are equivalently far, or even further, away:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-25 []
  (let [data (dataset-with-outlier)
        centroid  (i/matrix [[0 0]])
        distances (map #(s/euclidean-distance centroid %) data)]
    (-&gt; (c/bar-chart (range 202) distances
                     :x-label "Points"
                     :y-label "Euclidean Distance") 
        (i/view))))</pre></div><p class="calibre11">The preceding code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_06_280.jpg" alt="The Mahalanobis distance measure" class="calibre45"/></div><p class="calibre11">The Mahalanobis distance takes into account the covariance among the variables in calculating distances. In two dimensions, we can imagine the Euclidean distance as a circle growing out from the centroid: all points at the edge of the circle are equidistant from the centroid. The <a id="id847" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Mahalanobis distance stretches and <a id="id848" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>skews this circle to correct for the respective scales of the different variables, and to account for correlation amongst them. We can see the effect in the following example:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-26 []
  (let [data (dataset-with-outlier)
        distances    (map first (s/mahalanobis-distance data))]
    (-&gt; (c/bar-chart (range 202) distances
                     :x-label "Points"
                     :y-label "Mahalanobis Distance")
        (i/view))))</pre></div><p class="calibre11">The preceding code uses the function provided by <code class="literal">incanter.stats</code> to plot the Mahalanobis <a id="id849" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>distance between the same set of points. The <a id="id850" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>result is shown on the following chart:</p><div class="mediaobject"><img src="Images/7180OS_06_290.jpg" alt="The Mahalanobis distance measure" class="calibre45"/></div><p class="calibre11">This chart clearly identifies one point in particular as being much more distant than the other points. This matches our perception that this point in particular should be considered as being further away from the others.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The curse of dimensionality"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec113" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The curse of dimensionality</h1></div></div></div><p class="calibre11">There is one <a id="id851" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>fact that the Mahalanobis distance measure is unable to overcome, though, and this is known as the curse of dimensionality. As the number of dimensions in a dataset rises, every point tends to become equally far from every other point. We can demonstrate this quite simply with the following code:</p><div class="calibre2"><pre class="programlisting">(defn ex-6-27 []
  (let [distances (for [d (range 2 100)
                        :let [data (-&gt;&gt; (dataset-of-dimension d)
                                        (s/mahalanobis-distance)
                                        (map first))]]
                    [(apply min data) (apply max data)])]
    (-&gt; (c/xy-plot (range 2 101) (map first distances)
                   :x-label "Number of Dimensions"
                   :y-label "Distance Between Points"
                   :series-label "Minimum Distance"
                   :legend true)
        (c/add-lines (range 2 101) (map second distances)
                     :series-label "Maximum Distance")
        (i/view))))</pre></div><p class="calibre11">The preceding code <a id="id852" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>finds both the minimum and the maximum distance between any two pairs of points in a synthetic generated dataset of 100 points. As the number of dimensions approaches the number of elements in the set, we can see how the minimum and the maximum distance between each pair of elements approach one another:</p><div class="mediaobject"><img src="Images/7180OS_06_300.jpg" alt="The curse of dimensionality" class="calibre45"/></div><p class="calibre11">The effect is striking: as the number of dimensions increases, the distance between the closest two points rises too. The distance between the furthest two points rises as well, but at a slower rate. Finally, with 100 dimensions and 100 data points, every point appears to be equally <a id="id853" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>far from every other.</p><p class="calibre11">Of course, this is synthetic, randomly generated data. If we're attempting to cluster our data, we implicitly hope that it will have a discernible internal structure that we can tease out. Nonetheless, this structure will become more and more difficult to identify as the number of dimensions rises.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch06lvl1sec114" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we've learned about the process of clustering and covered the popular <span class="strong1"><em class="calibre13">k</em></span>-means clustering algorithm to cluster large numbers of text documents. </p><p class="calibre11">This provided an opportunity to cover the specific challenges presented by text processing where data is often messy, ambiguous, and high-dimensional. We saw how both stop words and stemming can help to reduce the number of dimensions and how TF-IDF can help identify the most important dimensions. We also saw how <span class="strong1"><em class="calibre13">n</em></span>-grams and shingling can help to tease out context for each word at the cost of a vast proliferation of terms.</p><p class="calibre11">We've explored Parkour in greater detail and seen how it can be used to write sophisticated, scalable, Hadoop jobs. In particular, we've seen how to make use of the distributed cache and custom tuple schemas to write Hadoop job process data represented as Clojure data structures. We used both of these to implement a method for generating unique, cluster-wide term IDs.</p><p class="calibre11">Finally, we witnessed the challenge presented by very high-dimensional spaces: the so-called "curse of dimensionality". In the next chapter, we'll cover this topic in more detail and describe a variety of techniques to combat it. We'll continue to explore the concepts of "similarity" and "difference" as we consider the problem of recommendation: how we can match users and items together.</p></div></div>



  </body></html>