["```py\n# Import numpy\nimport numpy as np\n# Import linear algebra module\nfrom scipy import linalg as la\n# Create dataset\ndata=np.array([[7., 4., 3.],\n[4., 1., 8.],\n[6., 3., 5.],\n[8., 6., 1.],\n[8., 5., 7.],\n[7., 2., 9.],\n[5., 3., 3.],\n[9., 5., 8.],\n[7., 4., 5.],\n[8., 2., 2.]])\n```", "```py\n# Calculate the covariance matrix\n# Center your data\ndata -= data.mean(axis=0)\ncov = np.cov(data, rowvar=False)\n```", "```py\n# Calculate eigenvalues and eigenvector of the covariance matrix\nevals, evecs = la.eig(cov)\n```", "```py\n# Multiply the original data matrix with Eigenvector matrix.\n\n# Sort the Eigen values and vector and select components\nnum_components=2\nsorted_key = np.argsort(evals)[::-1][:num_components]\nevals, evecs = evals[sorted_key], evecs[:, sorted_key]\n\nprint(\"Eigenvalues:\", evals)\nprint(\"Eigenvector:\", evecs)\nprint(\"Sorted and Selected Eigen Values:\", evals)\nprint(\"Sorted and Selected Eigen Vector:\", evecs)\n\n# Multiply original data and Eigen vector\nprincipal_components=np.dot(data,evecs)\nprint(\"Principal Components:\", principal_components)\n```", "```py\nEigenvalues: [0.74992815+0.j 3.67612927+0.j 8.27394258+0.j]\nEigenvector: [[-0.70172743 0.69903712 -0.1375708 ]\n[ 0.70745703 0.66088917 -0.25045969]\n[ 0.08416157 0.27307986 0.95830278]]\n\nSorted and Selected Eigen Values: [8.27394258+0.j 3.67612927+0.j]\n\nSorted and Selected Eigen Vector: [[-0.1375708 0.69903712]\n[-0.25045969 0.66088917]\n[ 0.95830278 0.27307986]]\n\nPrincipal Components: [[-2.15142276 -0.17311941]\n[ 3.80418259 -2.88749898]\n[ 0.15321328 -0.98688598]\n[-4.7065185 1.30153634]\n[ 1.29375788 2.27912632]\n[ 4.0993133 0.1435814 ]\n[-1.62582148 -2.23208282]\n[ 2.11448986 3.2512433 ]\n[-0.2348172 0.37304031]\n[-2.74637697 -1.06894049]]\n```", "```py\n# Import pandas and PCA\nimport pandas as pd\n\n# Import principal component analysis\nfrom sklearn.decomposition import PCA\n\n# Create dataset\ndata=np.array([[7., 4., 3.],\n[4., 1., 8.],\n[6., 3., 5.],\n[8., 6., 1.],\n[8., 5., 7.],\n[7., 2., 9.],\n[5., 3., 3.],\n[9., 5., 8.],\n[7., 4., 5.],\n[8., 2., 2.]])\n\n# Create and fit_transformed PCA Model\npca_model = PCA(n_components=2)\ncomponents = pca_model.fit_transform(data)\ncomponents_df = pd.DataFrame(data = components,\n\ncolumns = ['principal_component_1', 'principal_component_2'])\nprint(components_df)\n\n```", "```py\nprincipal_component_1 principal_component_2\n\n0 2.151423 -0.173119\n1 -3.804183 -2.887499\n2 -0.153213 -0.986886\n3 4.706518 1.301536\n4 -1.293758 2.279126\n5 -4.099313 0.143581\n6 1.625821 -2.232083\n7 -2.114490 3.251243\n8 0.234817 0.373040\n9 2.746377 -1.068940\n```", "```py\n# import pandas\nimport pandas as pd\n\n# import matplotlib\nimport matplotlib.pyplot as plt\n\n# import K-means\nfrom sklearn.cluster import KMeans\n\n# Create a DataFrame\ndata=pd.DataFrame({\"X\":[12,15,18,10,8,9,12,20],\n\"Y\":[6,16,17,8,7,6,9,18]})\nwcss_list = []\n\n# Run a loop for different value of number of cluster\nfor i in range(1, 6):\n    # Create and fit the KMeans model\n    kmeans_model = KMeans(n_clusters = i, random_state = 123)\n    kmeans_model.fit(data)\n\n    # Add the WCSS or inertia of the clusters to the score_list\n    wcss_list.append(kmeans_model.inertia_)\n\n# Plot the inertia(WCSS) and number of clusters\nplt.plot(range(1, 6), wcss_list, marker='*')\n\n# set title of the plot\nplt.title('Selecting Optimum Number of Clusters using Elbow Method')\n\n# Set x-axis label\nplt.xlabel('Number of Clusters K')\n\n# Set y-axis label\nplt.ylabel('Within-Cluster Sum of the Squares(Inertia)')\n\n# Display plot\nplt.show()\n```", "```py\n# import pandas\nimport pandas as pd\n\n# import matplotlib for data visualization\nimport matplotlib.pyplot as plt\n\n# import k-means for performing clustering\nfrom sklearn.cluster import KMeans\n\n# import silhouette score\nfrom sklearn.metrics import silhouette_score\n\n# Create a DataFrame\ndata=pd.DataFrame({\"X\":[12,15,18,10,8,9,12,20],\n\"Y\":[6,16,17,8,7,6,9,18]})\nscore_list = []\n\n# Run a loop for different value of number of cluster\nfor i in range(2, 6):\n    # Create and fit the KMeans model\n    kmeans_model = KMeans(n_clusters = i, random_state = 123)\n    kmeans_model.fit(data)\n\n    # Make predictions\n    pred=kmeans_model.predict(data)\n    # Calculate the Silhouette Score\n    score = silhouette_score (data, pred, metric='euclidean')\n\n    # Add the Silhouette score of the clusters to the score_list\n    score_list.append(score)\n\n# Plot the Silhouette score and number of cluster\nplt.bar(range(2, 6), score_list)\n\n# Set title of the plot\nplt.title('Silhouette Score Plot')\n\n# Set x-axis label\nplt.xlabel('Number of Clusters K')\n\n# Set y-axis label\nplt.ylabel('Silhouette Scores')\n\n# Display plot\nplt.show()\n\n```", "```py\n# import pandas\nimport pandas as pd\n\n# import matplotlib for data visualization\nimport matplotlib.pyplot as plt\n\n# Import K-means\nfrom sklearn.cluster import KMeans\n\n# Create a DataFrame\ndata=pd.DataFrame({\"X\":[12,15,18,10,8,9,12,20],\n\"Y\":[6,16,17,8,7,6,9,18]})\n\n# Define number of clusters\nnum_clusters = 2\n\n# Create and fit the KMeans model\nkm = KMeans(n_clusters=num_clusters)\nkm.fit(data)\n\n# Predict the target variable\npred=km.predict(data)\n\n# Plot the Clusters\nplt.scatter(data.X,data.Y,c=pred, marker=\"o\", cmap=\"bwr_r\")\n\n# Set title of the plot\nplt.title('K-Means Clustering')\n\n# Set x-axis label\nplt.xlabel('X-Axis Values')\n\n# Set y-axis label\nplt.ylabel('Y-Axis Values')\n\n# Display the plot\nplt.show()\n```", "```py\n# import pandas\nimport pandas as pd\n\n# import matplotlib for data visualization\nimport matplotlib.pyplot as plt\n\n# Import dendrogram\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import linkage\n\n# Create a DataFrame\ndata=pd.DataFrame({\"X\":[12,15,18,10,8,9,12,20],\n\"Y\":[6,16,17,8,7,6,9,18]})\n\n# create dendrogram using ward linkage\ndendrogram_plot = dendrogram(linkage(data, method = 'ward'))\n\n# Set title of the plot\nplt.title('Hierarchical Clustering: Dendrogram')\n\n# Set x-axis label\nplt.xlabel('Data Items')\n\n# Set y-axis label\nplt.ylabel('Distance')\n\n# Display the plot\nplt.show()\n```", "```py\n# import pandas\nimport pandas as pd\n\n# import matplotlib for data visualization\nimport matplotlib.pyplot as plt\n\n# Import Agglomerative Clustering\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Create a DataFrame\ndata=pd.DataFrame({\"X\":[12,15,18,10,8,9,12,20],\n\"Y\":[6,16,17,8,7,6,9,18]})\n\n# Specify number of clusters\nnum_clusters = 2\n\n# Create agglomerative clustering model\nac = AgglomerativeClustering(n_clusters = num_clusters, linkage='ward')\n\n# Fit the Agglomerative Clustering model\nac.fit(data)\n\n# Predict the target variable\npred=ac.labels_\n\n# Plot the Clusters\nplt.scatter(data.X,data.Y,c=pred, marker=\"o\")\n\n# Set title of the plot\nplt.title('Agglomerative Clustering')\n\n# Set x-axis label\nplt.xlabel('X-Axis Values')\n\n# Set y-axis label\nplt.ylabel('Y-Axis Values')\n\n# Display the plot\nplt.show()\n```", "```py\n# import pandas\nimport pandas as pd\n\n# import matplotlib for data visualization\nimport matplotlib.pyplot as plt\n\n# Import DBSCAN clustering model\nfrom sklearn.cluster import DBSCAN\n\n# import make_moons dataset\nfrom sklearn.datasets import make_moons\n\n# Generate some random moon data\nfeatures, label = make_moons(n_samples = 2000)\n\n# Create DBSCAN clustering model\ndb = DBSCAN()\n\n# Fit the Spectral Clustering model\ndb.fit(features)\n\n# Predict the target variable\npred_label=db.labels_\n\n# Plot the Clusters\nplt.scatter(features[:, 0], features[:, 1], c=pred_label, \nmarker=\"o\",cmap=\"bwr_r\")\n\n# Set title of the plot\nplt.title('DBSCAN Clustering')\n\n# Set x-axis label\nplt.xlabel('X-Axis Values')\n\n# Set y-axis label\nplt.ylabel('Y-Axis Values')\n\n# Display the plot\nplt.show()\n```", "```py\n# import pandas\nimport pandas as pd\n\n# import matplotlib for data visualization\nimport matplotlib.pyplot as plt\n\n# Import Spectral Clustering\nfrom sklearn.cluster import SpectralClustering\n\n# Create a DataFrame\ndata=pd.DataFrame({\"X\":[12,15,18,10,8,9,12,20],\n\"Y\":[6,16,17,8,7,6,9,18]})\n\n# Specify number of clusters\nnum_clusters = 2\n\n# Create Spectral Clustering model\nsc=SpectralClustering(num_clusters, affinity='rbf', n_init=100, assign_labels='discretize')\n\n# Fit the Spectral Clustering model\nsc.fit(data)\n\n# Predict the target variable\npred=sc.labels_\n\n# Plot the Clusters\nplt.scatter(data.X,data.Y,c=pred, marker=\"o\")\n\n# Set title of the plot\nplt.title('Spectral Clustering')\n\n# Set x-axis label\nplt.xlabel('X-Axis Values')\n\n# Set y-axis label\nplt.ylabel('Y-Axis Values')\n\n# Display the plot\nplt.show()\n```", "```py\n# Import libraries\nimport pandas as pd\n\n# read the dataset \ndiabetes = pd.read_csv(\"diabetes.csv\")\n\n# Show top 5-records\ndiabetes.head()\n```", "```py\n# split dataset in two parts: feature set and target label\nfeature_set = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n\nfeatures = diabetes[feature_set]\ntarget = diabetes.label\n```", "```py\n# partition data into training and testing set\nfrom sklearn.model_selection import train_test_split\n\nfeature_train, feature_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=1)\n```", "```py\n# Import K-means Clustering\nfrom sklearn.cluster import KMeans\n\n# Import metrics module for performance evaluation\nfrom sklearn.metrics import davies_bouldin_score\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import adjusted_rand_score\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import fowlkes_mallows_score\n\n# Specify the number of clusters\nnum_clusters = 2\n\n# Create and fit the KMeans model\nkm = KMeans(n_clusters=num_clusters)\nkm.fit(feature_train)\n\n# Predict the target variable\npredictions = km.predict(feature_test)\n\n# Calculate internal performance evaluation measures\nprint(\"Davies-Bouldin Index:\", davies_bouldin_score(feature_test, predictions))\nprint(\"Silhouette Coefficient:\", silhouette_score(feature_test, predictions))\n\n# Calculate External performance evaluation measures\nprint(\"Adjusted Rand Score:\", adjusted_rand_score(target_test, predictions))\nprint(\"Jaccard Score:\", jaccard_score(target_test, predictions))\nprint(\"F-Measure(F1-Score):\", f1_score(target_test, predictions))\nprint(\"Fowlkes Mallows Score:\", fowlkes_mallows_score(target_test, predictions))\n\n```", "```py\nDavies-Bouldin Index: 0.7916877512521091\nSilhouette Coefficient: 0.5365443098840619\nAdjusted Rand Score: 0.03789319261940484\nJaccard Score: 0.22321428571428573\nF-Measure(F1-Score): 0.36496350364963503\nFowlkes Mallows Score: 0.6041244457314743\n```"]