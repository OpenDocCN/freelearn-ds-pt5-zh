["```py\ndataSet<-\"UCIdatasets\" \npath<-\"http://canisius.edu/~yany/RData/\" \ncon<-paste(path,dataSet,\".RData\",sep='') \nload(url(con)) \ndim(.UCIdatasets) \nhead(.UCIdatasets) \n```", "```py\n>library(AppliedPredictiveModeling) \n>help(package=AppliedPredictiveModeling) \n```", "```py\n> library(AppliedPredictiveModeling) \n> data(abalone) \n> dim(abalone) \n> head(abalone)\n```", "```py\n> library(AppliedPredictiveModeling) \n> data(solubility) \n> ls(pattern=\"sol\") \n[1] \"solTestX\"       \"solTestXtrans\"  \"solTestY\"\n[4] \"solTrainX\"      \"solTrainXtrans\" \"solTrainY\"\n```", "```py\n> path<-\"http://canisius.edu/~yany/RData/\" \n> dataSet<-\"usGDPannual\" \n> con<-paste(path,dataSet,\".RData\",sep='') \n> load(url(con)) \n> head(.usGDPannual) \n YEAR  GDP \n1 1930 92.2 \n2 1931 77.4 \n3 1932 59.5 \n4 1933 57.2 \n5 1934 66.8 \n6 1935 74.3 \n> dataSet<-\"usGDPquarterly\" \n> con<-paste(path,dataSet,\".RData\",sep='') \n> load(url(con)) \n> head(.usGDPquarterly) \n DATE GDP_CURRENT GDP2009DOLLAR \n1 1947Q1       243.1        1934.5 \n2 1947Q2       246.3        1932.3 \n3 1947Q3       250.1        1930.3 \n4 1947Q4       260.3        1960.7 \n5 1948Q1       266.2        1989.5 \n6 1948Q2       272.9        2021.9 \n```", "```py\n> library(timeSeries)\n> data(MSFT)\n> x <- MSFT\n> by <- timeSequence(from = start(x), to = end(x), by = \"week\")\n> y<-aggregate(x,by,mean)\n```", "```py\n> head(x)\nGMT\n Open High Low Close Volume\n2000-09-27 63.4375 63.5625 59.8125 60.6250 53077800\n2000-09-28 60.8125 61.8750 60.6250 61.3125 26180200\n2000-09-29 61.0000 61.3125 58.6250 60.3125 37026800\n2000-10-02 60.5000 60.8125 58.2500 59.1250 29281200\n2000-10-03 59.5625 59.8125 56.5000 56.5625 42687000\n2000-10-04 56.3750 56.5625 54.5000 55.4375 68226700\n> head(y)\nGMT\n Open High Low Close Volume\n2000-09-27 63.4375 63.5625 59.8125 60.6250 53077800\n2000-10-04 59.6500 60.0750 57.7000 58.5500 40680380\n2000-10-11 54.9750 56.4500 54.1625 55.0875 36448900\n2000-10-18 53.0375 54.2500 50.8375 52.1375 50631280\n2000-10-25 61.7875 64.1875 60.0875 62.3875 86457340\n2000-11-01 66.1375 68.7875 65.8500 67.9375 53496000\n```", "```py\nmovingAverageFunction<- function(data,n=10){\n  out= data\n  for(i in n:length(data)){\n    out[i] = mean(data[(i-n+1):i])\n  }\n  return(out)\n}\n```", "```py\n> library(timeSeries)\n> data(MSFT)\n> p<-MSFT$Close\n> #\n> ma<-movingAverageFunction(p,3)\n> head(p)\n[1] 60.6250 61.3125 60.3125 59.1250 56.5625 55.4375\n> head(ma)\n[1] 60.62500 61.31250 60.75000 60.25000 58.66667 57.04167\n> mean(p[1:3])\n[1] 60.75\n> mean(p[2:4])\n[1] 60.25\n```", "```py\nlibrary(plyr)\nyear<-c(2000,2000,2001,2001,2004)\nvalues<-c(2, 3, 3, 5, 6)\ndf <- data.frame(DATE=year,B =values )\ndfsum <- ddply(df, c(\"DATE\"),summarize,B=sum(B))\n```", "```py\n> df\n DATE B\n1 2000 2\n2 2000 3\n3 2001 3\n4 2001 5\n5 2004 6\n> dfsum\n DATE B\n1 2000 5\n2 2001 8\n3 2004 6\n```", "```py\n> library(data.table)\n> path<-'http://canisius.edu/~yany/RData/'\n> dataSet<-'sp500monthly.RData'\n> link<-paste(path,dataSet,sep='')\n> load(url(link))\n> #head(.sp500monthly,2)\n> p<-.sp500monthly$ADJ.CLOSE\n> n<-length(p)\n> logRet<-log(p[2:n]/p[1:(n-1)])\n> years<-format(.sp500monthly$DATE[2:n],\"%Y\")\n> y<-data.frame(.sp500monthly$DATE[2:n],years,logRet)\n> colnames(y)<-c(\"DATE\",\"YEAR\",\"LOGRET\")\n> y2<- data.table(y)\n> z<-y2[,sum(LOGRET),by=YEAR]\n> z2<-na.omit(z)\n> annualRet<-data.frame(z2$YEAR,exp(z2[,2])-1)\n> n<-nrow(annualRet)\n> std<-sd(annualRet[,2])\n> stdErr<-std/sqrt(n)\n> ourMean<-mean(annualRet[,2])\n> min2<-ourMean-2*stdErr\n> max2<-ourMean+2*stdErr\n> cat(\"[min mean max ]\\n\")\n[min mean max ]\n> cat(min2,ourMean,max2,\"\\n\")\n0.05032956 0.09022369 0.1301178\n```", "```py\nlibrary(astsa)\npath<-\"http://canisius.edu/~yany/RData/\"\ndataSet<-\"usGDPquarterly\"\ncon<-paste(path,dataSet,\".RData\",sep='')\nload(url(con))\nx<-.usGDPquarterly$DATE\ny<-.usGDPquarterly$GDP_CURRENT\nplot(x,y)\ndiff4 = diff(y,4)\nacf2(diff4,24)\n```", "```py\n> path<-\"http://canisius.edu/~yany/RData/\"\n> dataSet<-\"usGDPannual\"\n> con<-paste(path,dataSet,\".RData\",sep='')\n> load(url(con))\n> title<-\"US GDP\"\n> xTitle<-\"Year\"\n> yTitle<-\"US annual GDP\"\n> x<-.usGDPannual$YEAR\n> y<-.usGDPannual$GDP\n> plot(x,y,main=title,xlab=xTitle,ylab=yTitle)\n```", "```py\n> yTitle<-\"Log US annual GDP\" \n> plot(x,log(y),main=title,xlab=xTitle,ylab=yTitle)\n```", "```py\nlibrary(LiblineaR)\ndata(iris)\nattach(iris)\nx=iris[,1:4]\ny=factor(iris[,5])\ntrain=sample(1:dim(iris)[1],100)\nxTrain=x[train,];xTest=x[-train,]\nyTrain=y[train]; yTest=y[-train]\ns=scale(xTrain,center=TRUE,scale=TRUE)\n#\ntryTypes=c(0:7)\ntryCosts=c(1000,1,0.001)\nbestCost=NA\nbestAcc=0\nbestType=NA\n#\nfor(ty in tryTypes){\n   for(co in tryCosts){\n     acc=LiblineaR(data=s,target=yTrain,type=ty,cost=co,bias=1,cross=5,verbose=FALSE)\n     cat(\"Results for C=\",co,\": \",acc,\" accuracy.\\n\",sep=\"\")\n     if(acc>bestAcc){\n         bestCost=co\n         bestAcc=acc\n         bestType=ty\n     }\n   }\n}\ncat(\"Best model type is:\",bestType,\"\\n\")\ncat(\"Best cost is:\",bestCost,\"\\n\")\ncat(\"Best accuracy is:\",bestAcc,\"\\n\")\n# Re-train best model with best cost value.\nm=LiblineaR(data=s,target=yTrain,type=bestType,cost=bestCost,bias=1,verbose=FALSE)\n# Scale the test data\ns2=scale(xTest,attr(s,\"scaled:center\"),attr(s,\"scaled:scale\"))\npr=FALSE; # Make prediction\nif(bestType==0 || bestType==7) pr=TRUE\np=predict(m,s2,proba=pr,decisionValues=TRUE)\nres=table(p$predictions,yTest) # Display confusion matrix\nprint(res)\n# Compute Balanced Classification Rate\nBCR=mean(c(res[1,1]/sum(res[,1]),res[2,2]/sum(res[,2]),res[3,3]/sum(res[,3])))\nprint(BCR)\n```", "```py\n> cat(\"Best model type is:\",bestType,\"\\n\")\nBest model type is: 4 \n> cat(\"Best cost is:\",bestCost,\"\\n\")\nBest cost is: 1 \n> cat(\"Best accuracy is:\",bestAcc,\"\\n\")\nBest accuracy is: 0.98 \n> print(res)\n            yTest\n setosa versicolor virginica\n setosa 16 0 0\n versicolor 0 17 0\n virginica 0 3 14\n> print(BCR)\n[1] 0.95\n```", "```py\n> library(datarobot)\n```", "```py\n> library(datarobot)\nDid not connect to DataRobot on package startup. Use `ConnectToDataRobot`.\nTo connect by default on startup, you can put a config file at: C:\\Users\\yany\\Documents/.config/datarobot/drconfig.yaml\n```", "```py\nloc<- \"YOUR-ENDPOINT-HERE\"\nmyToken<-\"YOUR-API_TOKEN-HERE\"\nConnectToDataRobot(endpoint=loc,token=myToken)\n```", "```py\n> help(package=datarobot)\n```", "```py\n> library(eclust)\n> data(\"simdata\")\n> dim(simdata)\n[1] 100 502\n> simdata[1:5, 1:6]\n Y E Gene1 Gene2 Gene3 Gene4\n[1,] -94.131497 0 -0.4821629 0.1298527 0.4228393 0.36643188\n[2,] 7.134990 0 -1.5216289 -0.3304428 -0.4384459 1.57602830\n[3,] 1.974194 0 0.7590055 -0.3600983 1.9006443 -1.47250061\n[4,] -44.855010 0 0.6833635 1.8051352 0.1527713 -0.06442029\n[5,] 23.547378 0 0.4587626 -0.3996984 -0.5727255 -1.75716775\n> table(simdata[,\"E\"])\n 0 1 \n50 50 \n>\n```", "```py\n> library(eclust)\n> data(\"simdata\")\n> X = simdata[,c(-1,-2)]\n> firstCorr<-cor(X[1:50,])\n> secondCorr<-cor(X[51:100,])\n> score<-u_fisherZ(n0=100,cor0=firstCorr,n1=100,cor1=secondCorr)\n> dim(score)\n[1] 500 500\n> score[1:5,1:5]\n Gene1 Gene2 Gene3 Gene4 Gene5\nGene1 1.000000 -8.062020 6.260050 -8.133437 -7.825391\nGene2 -8.062020 1.000000 9.162208 -7.431822 -7.814067\nGene3 6.260050 9.162208 1.000000 8.072412 6.529433\nGene4 -8.133437 -7.431822 8.072412 1.000000 -5.099261\nGene5 -7.825391 -7.814067 6.529433 -5.099261 1.000000\n>\n```", "```py\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score \n#\nnp.random.seed(123)\nn= 30 # number of samples \ndegrees = [1, 4, 15]\ndef true_fun(x):\n    return np.cos(1.5*np.pi*x)\nx = np.sort(np.random.rand(n))\ny = true_fun(x) + np.random.randn(n) * 0.1\nplt.figure(figsize=(14, 5))\ntitle=\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\"\nname1=\"polynomial_features\"\nname2=\"linear_regression\"\nname3=\"neg_mean_squared_error\"\n#\nfor i in range(len(degrees)):\n    ax=plt.subplot(1,len(degrees),i+1)\n    plt.setp(ax, xticks=(), yticks=())\n    pFeatures=PolynomialFeatures(degree=degrees[i],include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline=Pipeline([(name1,pFeatures),(name2,linear_regression)])\n    pipeline.fit(x[:,np.newaxis],y)\n    scores=cross_val_score(pipeline,x[:,np.newaxis],y,scoring=name3,cv=10)\n    xTest = np.linspace(0, 1, 100)\n    plt.plot(xTest,pipeline.predict(xTest[:,np.newaxis]),label=\"Model\")\n    plt.plot(xTest,true_fun(xTest),label=\"True function\")\n    plt.scatter(x,y,edgecolor='b',s=20,label=\"Samples\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.xlim((0,1))\n    plt.ylim((-2,2))\n    plt.legend(loc=\"best\")\n    plt.title(title.format(degrees[i],-scores.mean(),scores.std()))\nplt.show()\n```", "```py\nimport datetime\nimport pandas\nfrom sqlalchemy import create_engine\nfrom metta import metta_io as metta\nfrom catwalk.storage import FSModelStorageEngine, CSVMatrixStore\nfrom catwalk.model_trainers import ModelTrainer\nfrom catwalk.predictors import Predictor\nfrom catwalk.evaluation import ModelEvaluator\nfrom catwalk.utils import save_experiment_and_get_hash\nhelp(FSModelStorageEngine)\n```", "```py\n Help on class FSModelStorageEngine in module catwalk.storage:\n\nclass FSModelStorageEngine(ModelStorageEngine)\n | Method resolution order:\n | FSModelStorageEngine\n | ModelStorageEngine\n | builtins.object\n | \n | Methods defined here:\n | \n | __init__(self, *args, **kwargs)\n | Initialize self. See help(type(self)) for accurate signature.\n | \n | get_store(self, model_hash)\n | \n | ----------------------------------------------------------------------\n | Data descriptors inherited from ModelStorageEngine:\n | \n | __dict__\n | dictionary for instance variables (if defined)\n | \n | __weakref__\n | list of weak references to the object (if defined)\n```", "```py\nimport logging\nimport numpy as np\nfrom optparse import OptionParser\nimport sys\nfrom time import time\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_selection import SelectFromModel\n```", "```py\nusing QuantEcon\nP = [0.4 0.6; 0.2 0.8];\nmc = MarkovChain(P)\nx = simulate(mc, 100000);\nmean(x .== 1)\n#\nmc2 = MarkovChain(P, [\"employed\", \"unemployed\"])\nsimulate(mc2, 4)\n```", "```py\nusing QuantEcon\nP = [0.9 0.1 0.0; 0.4 0.4 0.2; 0.1 0.1 0.8];\nmc = MarkovChain(P)\nis_irreducible(mc) \n```", "```py\nusing QuantEcon\nP2 = [1.0 0.0 0.0; 0.1 0.8 0.1; 0.0 0.2 0.8];\nmc2 = MarkovChain(P2)\nis_irreducible(mc2)\n```", "```py\npkg load ltfat\nf = greasy;\nname1=\"sparsified coefficients\"\nname2=\"dual system coefficients\"\nF = frame('dgtreal','gauss',64,512);\nlambda = 0.1;\n% Solve the basis pursuit problem\n[c,~,~,frec,cd] = franalasso(F,f,lambda);\nfigure(1); % Plot sparse coefficients\nplotframe(F,c,’dynrange’,50);\nfigure(2); % Plot coefficients \nplotframe(F,cd,’dynrange’,50);\nnorm(f-frec)\nfigure(3);\nsemilogx([sort(abs(c),'descend')/max(abs(c)),...\nsort(abs(cd),’descend’)/max(abs(cd))]);\nlegend({name1,name2});\n```", "```py\n> library(lmtest)\n> data(ChickEgg)\n> dim(ChickEgg)\n[1] 54 2\n> ChickEgg[1:5,]\n chicken egg\n[1,] 468491 3581\n[2,] 449743 3532\n[3,] 436815 3327\n[4,] 444523 3255\n[5,] 433937 3156\n```", "```py\n> library(lmtest)\n> data(ChickEgg)\n> grangertest(chicken~egg, order = 3, data = ChickEgg)\nGranger causality test\nModel 1: chicken ~ Lags(chicken, 1:3) + Lags(egg, 1:3)\nModel 2: chicken ~ Lags(chicken, 1:3)\n Res.Df Df F Pr(>F) \n1 44 \n2 47 -3 5.405 0.002966 **\n---\nSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n```", "```py\n> grangertest(egg~chicken, order = 3, data = ChickEgg)\nGranger causality test\n\nModel 1: egg ~ Lags(egg, 1:3) + Lags(chicken, 1:3)\nModel 2: egg ~ Lags(egg, 1:3)\n Res.Df Df F Pr(>F)\n1 44 \n2 47 -3 0.5916 0.6238\n```", "```py\nret_f<-function(x,ticker=\"\"){\n     n<-nrow(x)\n     p<-x[,6]\n     ret<-p[2:n]/p[1:(n-1)]-1\n     output<-data.frame(x[2:n,1],ret)\n     name<-paste(\"RET_\",toupper(ticker),sep='')\n     colnames(output)<-c(\"DATE\",name)\n     return(output)\n}\n\n```", "```py\n> x<-read.csv(\"http://canisius.edu/~yany/data/ibmDaily.csv\",header=T)\n> ibmRet<-ret_f(x,\"ibm\")\n> x<-read.csv(\"http://canisius.edu/~yany/data/^gspcDaily.csv\",header=T)\n> mktRet<-ret_f(x,\"mkt\")\n> final<-merge(ibmRet,mktRet)\n> head(final)\n DATE RET_IBM RET_MKT\n1 1962-01-03 0.008742545 0.0023956877\n2 1962-01-04 -0.009965497 -0.0068887673\n3 1962-01-05 -0.019694350 -0.0138730891\n4 1962-01-08 -0.018750380 -0.0077519519\n5 1962-01-09 0.011829467 0.0004340133\n6 1962-01-10 0.001798526 -0.0027476933\n```", "```py\n> library(lmtest)\n> grangertest(RET_IBM ~ RET_MKT, order = 1, data =final)\nGranger causality test\nModel 1: RET_IBM ~ Lags(RET_IBM, 1:1) + Lags(RET_MKT, 1:1)\nModel 2: RET_IBM ~ Lags(RET_IBM, 1:1)\n Res.Df Df F Pr(>F) \n1 14149 \n2 14150 -1 24.002 9.729e-07 ***\n---\nSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n```", "```py\n> grangertest(RET_MKT ~ RET_IBM, order = 1, data =final)\nGranger causality test\nModel 1: RET_MKT ~ Lags(RET_MKT, 1:1) + Lags(RET_IBM, 1:1)\nModel 2: RET_MKT ~ Lags(RET_MKT, 1:1)\n Res.Df Df F Pr(>F) \n1 14149 \n2 14150 -1 7.5378 0.006049 **\n---\nSignif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n> \n```", "```py\nimport pandas as pd\nff=pd.read_pickle(\"c:/temp/ffMonthly.pkl\")\nprint(ff.head())\n```", "```py\nprint(ff.head())\n            MKT_RF SMB HML Rf\n1926-07-01 0.0296 -0.0230 -0.0287 0.0022\n1926-08-01 0.0264 -0.0140 0.0419 0.0025\n1926-09-01 0.0036 -0.0132 0.0001 0.0023\n1926-10-01 -0.0324 0.0004 0.0051 0.0032\n1926-11-01 0.0253 -0.0020 -0.0035 0.003\n```", "```py\n> path<-\"http://canisius.edu/~yany/RData/\"\n> dataSet<-\"businesscycle\"\n> link<-paste(path,dataSet,\".RData\",sep='')\n> load(url(link))\n> head(.businessCycle)\n      DATE CYCLE\n1 19261001 1.000\n2 19261101 0.846\n3 19261201 0.692\n4 19270101 0.538\n5 19270201 0.385\n6 19270301 0.231\n```"]