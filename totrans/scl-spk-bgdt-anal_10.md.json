["```py\ncase class Person(name: String) {\n val friends = scala.collection.mutable.ArrayBuffer[Person]() \n def numberOfFriends() = friends.length \n def isFriend(other: Person) = friends.find(_.name == other.name) \n def isConnectedWithin2Steps(other: Person) = {\n for {f <- friends} yield {f.name == other.name ||\n                              f.isFriend(other).isDefined}\n }.find(_ == true).isDefined\n }\n\nscala> val john = Person(\"John\")\njohn: Person = Person(John)\n\nscala> val ken = Person(\"Ken\")\nken: Person = Person(Ken)\n\nscala> val mary = Person(\"Mary\")\nmary: Person = Person(Mary)\n\nscala> val dan = Person(\"Dan\")\ndan: Person = Person(Dan)\n\nscala> john.numberOfFriends\nres33: Int = 0\n\nscala> john.friends += ken\nres34: john.friends.type = ArrayBuffer(Person(Ken))     //john -> ken\n\nscala> john.numberOfFriends\nres35: Int = 1\n\nscala> ken.friends += mary\nres36: ken.friends.type = ArrayBuffer(Person(Mary))    //john -> ken -> mary\n\nscala> ken.numberOfFriends\nres37: Int = 1\n\nscala> mary.friends += dan\nres38: mary.friends.type = ArrayBuffer(Person(Dan))   //john -> ken -> mary -> dan\n\nscala> mary.numberOfFriends\nres39: Int = 1\n\nscala> john.isFriend(ken)\nres40: Option[Person] = Some(Person(Ken))         //Yes, ken is a friend of john\n\nscala> john.isFriend(mary)\nres41: Option[Person] = None        //No, mary is a friend of ken not john\n\nscala> john.isFriend(dan)\nres42: Option[Person] = None      //No, dan is a friend of mary not john\n\nscala> john.isConnectedWithin2Steps(ken)\nres43: Boolean = true     //Yes, ken is a friend of john\n\nscala> john.isConnectedWithin2Steps(mary)\nres44: Boolean = true     //Yes, mary is a friend of ken who is a friend of john\n\nscala> john.isConnectedWithin2Steps(dan)\nres45: Boolean = false    //No, dan is a friend of mary who is a friend of ken who is a friend of john\n\n```", "```py\nGraph G = (V, E)\nV - set of Vertices\nE - set of Edges\n\n```", "```py\nimport org.apache.spark._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.graphx.GraphLoader\nimport org.apache.spark.graphx.GraphOps\n\n```", "```py\nclass Graph[VD: ClassTag, ED: ClassTag] \n\n```", "```py\nclass Graph[VD, ED] {\n  //A RDD containing the vertices and their associated attributes.\n  val vertices: VertexRDD[VD]\n\n  //A RDD containing the edges and their associated attributes. \n    The entries in the RDD contain just the source id and target id\n    along with the edge data.\n  val edges: EdgeRDD[ED]\n\n  //A RDD containing the edge triplets, which are edges along with the\n    vertex data associated with the adjacent vertices.\n  val triplets: RDD[EdgeTriplet[VD, ED]]\n}\n\n```", "```py\ntype VertexId = Long\n\n```", "```py\nclass VertexRDD[VD]() extends RDD[(VertexId, VD)]\n\n```", "```py\ncase class User(name: String, occupation: String)\n\n```", "```py\nscala> val users = sc.textFile(\"users.txt\").map{ line =>\n val fields = line.split(\",\")\n (fields(0).toLong, User(fields(1), fields(2)))\n}\nusers: org.apache.spark.rdd.RDD[(Long, User)] = MapPartitionsRDD[2645] at map at <console>:127\n\nscala> users.take(10)\nres103: Array[(Long, User)] = Array((1,User(John,Accountant)), (2,User(Mark,Doctor)), (3,User(Sam,Lawyer)), (4,User(Liz,Doctor)), (5,User(Eric,Accountant)), (6,User(Beth,Accountant)), (7,User(Larry,Engineer)), (8,User(Mary,Cashier)), (9,User(Dan,Doctor)), (10,User(Ken,Librarian)))\n\n```", "```py\nclass EdgeRDD[ED]() extends RDD[Edge[ED]]\n\n```", "```py\nscala> val friends = sc.textFile(\"friends.txt\").map{ line =>\n val fields = line.split(\",\")\n Edge(fields(0).toLong, fields(1).toLong, \"friend\")\n}\nfriends: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = MapPartitionsRDD[2648] at map at <console>:125\n\nscala> friends.take(10)\nres109: Array[org.apache.spark.graphx.Edge[String]] = Array(Edge(1,3,friend), Edge(3,1,friend), Edge(1,2,friend), Edge(2,1,friend), Edge(4,10,friend), Edge(10,4,friend), Edge(1,10,friend), Edge(10,1,friend), Edge(2,7,friend), Edge(7,2,friend))\n\n```", "```py\nscala> val graph = Graph(users, friends)\ngraph: org.apache.spark.graphx.Graph[User,String] = org.apache.spark.graphx.impl.GraphImpl@327b69c8\n\nscala> graph.vertices\nres113: org.apache.spark.graphx.VertexRDD[User] = VertexRDDImpl[2658] at RDD at VertexRDD.scala:57\n\nscala> graph.edges\nres114: org.apache.spark.graphx.EdgeRDD[String] = EdgeRDDImpl[2660] at RDD at EdgeRDD.scala:41\n\n```", "```py\nscala> graph.vertices.collect\nres111: Array[(org.apache.spark.graphx.VertexId, User)] = Array((4,User(Liz,Doctor)), (6,User(Beth,Accountant)), (8,User(Mary,Cashier)), (10,User(Ken,Librarian)), (2,User(Mark,Doctor)), (1,User(John,Accountant)), (3,User(Sam,Lawyer)), (7,User(Larry,Engineer)), (9,User(Dan,Doctor)), (5,User(Eric,Accountant)))\n\nscala> graph.edges.collect\nres112: Array[org.apache.spark.graphx.Edge[String]] = Array(Edge(1,2,friend), Edge(1,3,friend), Edge(1,10,friend), Edge(2,1,friend), Edge(2,3,friend), Edge(2,7,friend), Edge(3,1,friend), Edge(3,2,friend), Edge(3,10,friend), Edge(4,7,friend), Edge(4,10,friend), Edge(7,2,friend), Edge(7,4,friend), Edge(10,1,friend), Edge(10,4,friend), Edge(3,5,friend), Edge(5,3,friend), Edge(5,9,friend), Edge(6,8,friend), Edge(6,10,friend), Edge(8,6,friend), Edge(8,9,friend), Edge(8,10,friend), Edge(9,5,friend), Edge(9,8,friend), Edge(10,6,friend), Edge(10,8,friend))\n\n```", "```py\nscala> graph.vertices.collect\nres111: Array[(org.apache.spark.graphx.VertexId, User)] = Array((4,User(Liz,Doctor)), (6,User(Beth,Accountant)), (8,User(Mary,Cashier)), (10,User(Ken,Librarian)), (2,User(Mark,Doctor)), (1,User(John,Accountant)), (3,User(Sam,Lawyer)), (7,User(Larry,Engineer)), (9,User(Dan,Doctor)), (5,User(Eric,Accountant)))\n\nscala> graph.edges.collect\nres112: Array[org.apache.spark.graphx.Edge[String]] = Array(Edge(1,2,friend), Edge(1,3,friend), Edge(1,10,friend), Edge(2,1,friend), Edge(2,3,friend), Edge(2,7,friend), Edge(3,1,friend), Edge(3,2,friend), Edge(3,10,friend), Edge(4,7,friend), Edge(4,10,friend), Edge(7,2,friend), Edge(7,4,friend), Edge(10,1,friend), Edge(10,4,friend), Edge(3,5,friend), Edge(5,3,friend), Edge(5,9,friend), Edge(6,8,friend), Edge(6,10,friend), Edge(8,6,friend), Edge(8,9,friend), Edge(8,10,friend), Edge(9,5,friend), Edge(9,8,friend), Edge(10,6,friend), Edge(10,8,friend))\n\n```", "```py\ndef filter(pred: Tuple2[VertexId, VD] => Boolean): VertexRDD[VD] \n\n```", "```py\nscala> graph.vertices.filter(x => x._1 == 2).take(10)\nres118: Array[(org.apache.spark.graphx.VertexId, User)] = Array((2,User(Mark,Doctor)))\n\nscala> graph.vertices.filter(x => x._2.name == \"Mark\").take(10)\nres119: Array[(org.apache.spark.graphx.VertexId, User)] = Array((2,User(Mark,Doctor)))\n\nscala> graph.vertices.filter(x => x._2.occupation == \"Doctor\").take(10)\nres120: Array[(org.apache.spark.graphx.VertexId, User)] = Array((4,User(Liz,Doctor)), (2,User(Mark,Doctor)), (9,User(Dan,Doctor)))\n\n```", "```py\nscala> graph.edges.filter(x => x.srcId == 1)\nres123: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = MapPartitionsRDD[2672] at filter at <console>:134\n\nscala> graph.edges.filter(x => x.srcId == 1).take(10)\nres124: Array[org.apache.spark.graphx.Edge[String]] = Array(Edge(1,2,friend), Edge(1,3,friend), Edge(1,10,friend))\n\n```", "```py\ndef mapValues[VD2: ClassTag](f: VD => VD2): VertexRDD[VD2]\n//A variant of the mapValues() function accepts a vertexId in addition  \n  to the vertices.\ndef mapValues[VD2: ClassTag](f: (VertexId, VD) => VD2): VertexRDD[VD2]\n\n```", "```py\ndef mapValues[ED2: ClassTag](f: Edge[ED] => ED2): EdgeRDD[ED2]\n\n```", "```py\nscala> graph.vertices.mapValues{(id, u) => u.name}.take(10)\nres142: Array[(org.apache.spark.graphx.VertexId, String)] = Array((4,Liz), (6,Beth), (8,Mary), (10,Ken), (2,Mark), (1,John), (3,Sam), (7,Larry), (9,Dan), (5,Eric))\n\nscala> graph.edges.mapValues(x => s\"${x.srcId} -> ${x.dstId}\").take(10)\n7), Edge(3,1,3 -> 1), Edge(3,2,3 -> 2), Edge(3,10,3 -> 10), Edge(4,7,4 -> 7))\n\n```", "```py\ndef aggregateMessages[Msg: ClassTag](\n sendMsg: EdgeContext[VD, ED, Msg] => Unit,\n mergeMsg: (Msg, Msg) => Msg,\n tripletFields: TripletFields = TripletFields.All)\n : VertexRDD[Msg]\n\n```", "```py\nscala> graph.aggregateMessages[Int](_.sendToDst(1), _ + _).collect\nres207: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((4,2), (6,2), (8,3), (10,4), (2,3), (1,3), (3,3), (7,2), (9,2), (5,2))\n\n```", "```py\nscala> val triangleCounts = graph.triangleCount.vertices\ntriangleCounts: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[3365] at RDD at VertexRDD.scala:57\n\nscala> triangleCounts.take(10)\nres171: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((4,0), (6,1), (8,1), (10,1), (2,1), (1,1), (3,1), (7,0), (9,0), (5,0))\n\nscala> val triangleCountsPerUser = users.join(triangleCounts).map { case(id, (User(x,y), k)) => ((x,y), k) }\ntriangleCountsPerUser: org.apache.spark.rdd.RDD[((String, String), Int)] = MapPartitionsRDD[3371] at map at <console>:153\n\nscala> triangleCountsPerUser.collect.mkString(\"\\n\")\nres170: String =\n((Liz,Doctor),0)\n((Beth,Accountant),1)  *//1 count means this User is part of 1 triangle*\n((Mary,Cashier),1)  *//1 count means this User is part of 1 triangle*\n((Ken,Librarian),1)  *//1 count means this User is part of 1 triangle*\n((Mark,Doctor),1)  * //1 count means this User is part of 1 triangle*\n((John,Accountant),1)  *//1 count means this User is part of 1 triangle*\n((Sam,Lawyer),1)   *//1 count means this User is part of 1 triangle*\n((Larry,Engineer),0)\n((Dan,Doctor),0)\n((Eric,Accountant),0)\n\n```", "```py\ndef pregel[A]\n (initialMsg: A, // the initial message to all vertices\n maxIter: Int = Int.MaxValue, // number of iterations\n activeDir: EdgeDirection = EdgeDirection.Out) // incoming or outgoing edges\n (vprog: (VertexId, VD, A) => VD,\n sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)], //send message function\n mergeMsg: (A, A) => A) //merge strategy\n : Graph[VD, ED] \n\n```", "```py\nscala> graph.connectedComponents.vertices.collect res198: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId)] = Array((4,1), (6,1), (8,1), (10,1), (2,1), (1,1), (3,1), (7,1), (9,1), (5,1))\n scala> graph.connectedComponents.vertices.join(users).take(10)\nres197: Array[(org.apache.spark.graphx.VertexId, (org.apache.spark.graphx.VertexId, User))] = Array((4,(1,User(Liz,Doctor))), (6,(1,User(Beth,Accountant))), (8,(1,User(Mary,Cashier))), (10,(1,User(Ken,Librarian))), (2,(1,User(Mark,Doctor))), (1,(1,User(John,Accountant))), (3,(1,User(Sam,Lawyer))), (7,(1,User(Larry,Engineer))), (9,(1,User(Dan,Doctor))), (5,(1,User(Eric,Accountant))))\n\n```", "```py\nscala> lib.ShortestPaths.run(graph,Array(1)).vertices.join(users).take(10)\n\nres204: Array[(org.apache.spark.graphx.VertexId, (org.apache.spark.graphx.lib.ShortestPaths.SPMap, User))] = Array((4,(Map(1 -> 2),User(Liz,Doctor))), (6,(Map(1 -> 2),User(Beth,Accountant))), (8,(Map(1 -> 2),User(Mary,Cashier))), (10,(Map(1 -> 1),User(Ken,Librarian))), (2,(Map(1 -> 1),User(Mark,Doctor))), (1,(Map(1 -> 0),User(John,Accountant))), (3,(Map(1 -> 1),User(Sam,Lawyer))), (7,(Map(1 -> 2),User(Larry,Engineer))), (9,(Map(1 -> 3),User(Dan,Doctor))), (5,(Map(1 -> 2),User(Eric,Accountant))))\n\n```", "```py\nscala> val srcId = 1 //vertex ID 1 is the user John\nsrcId: Int = 1\n\nscala> val initGraph = graph.mapVertices((id, x) => if(id == srcId) 0.0 else Double.PositiveInfinity)\ninitGraph: org.apache.spark.graphx.Graph[Double,Long] = org.apache.spark.graphx.impl.GraphImpl@2b9b8608\n\nscala> val weightedShortestPath = initGraph.pregel(Double.PositiveInfinity, 5)(\n | (id, dist, newDist) => math.min(dist, newDist),\n | triplet => {\n | if (triplet.srcAttr + triplet.attr < triplet.dstAttr) {\n | Iterator((triplet.dstId, triplet.srcAttr + triplet.attr))\n | }\n | else {\n | Iterator.empty\n | }\n | },\n | (a, b) => math.min(a, b)\n | )\nweightedShortestPath: org.apache.spark.graphx.Graph[Double,Long] = org.apache.spark.graphx.impl.GraphImpl@1f87fdd3\n\nscala> weightedShortestPath.vertices.take(10).mkString(\"\\n\")\nres247: String =\n(4,10.0)\n(6,6.0)\n(8,6.0)\n(10,5.0)\n(2,1.0)\n(1,0.0)\n(3,3.0)\n(7,7.0)\n(9,5.0)\n(5,4.0)\n\n```", "```py\nscala> val prVertices = graph.pageRank(0.0001).vertices\nprVertices: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[8245] at RDD at VertexRDD.scala:57\n\nscala> prVertices.join(users).sortBy(_._2._1, false).take(10)\nres190: Array[(org.apache.spark.graphx.VertexId, (Double, User))] = Array((10,(1.4600029149839906,User(Ken,Librarian))), (8,(1.1424200609462447,User(Mary,Cashier))), (3,(1.1279748817993318,User(Sam,Lawyer))), (2,(1.1253662371576425,User(Mark,Doctor))), (1,(1.0986118723393328,User(John,Accountant))), (9,(0.8215535923013982,User(Dan,Doctor))), (5,(0.8186673059832846,User(Eric,Accountant))), (7,(0.8107902215195832,User(Larry,Engineer))), (4,(0.8047583729877394,User(Liz,Doctor))), (6,(0.783902117150218,User(Beth,Accountant))))\n\n```"]