<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch08" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 8. Ensemble Methods</h1></div></div></div><p class="calibre11">In this chapter, we will look at the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Understanding Ensemble – the bagging Method</li><li class="listitem">Understanding Ensemble – the boosting Method, AdaBoost</li><li class="listitem">Understanding Ensemble – the gradient Boosting</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec77" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Introduction</h1></div></div></div><p class="calibre11">In this chapter, we are going to look at recipes covering the ensemble methods. When we are faced with the challenge of uncertainty while making a decision in real life, we typically approach multiple friends for opinion. We make our decision based on the collective knowledge that we receive from those friends. Ensemble is a similar concept in machine learning. In the previous chapters, we built a single model for our datasets, and used the prediction of that model on unseen test data. What if we build a lot of models on that data and make our final prediction based on the prediction from all these individual models? This is the idea behind Ensemble. Using the Ensemble approach for a given problem, we proceed with building a lot of models, and use all of them to make our final prediction on unseen datasets. For a regression problem, the final output may be the average prediction value from all the models. In a classification context, a majority vote is taken to decide the output class.</p><p class="calibre11">The fundamental idea is to have a lot of models, each one of them producing slightly different results on the training dataset. Some models learn certain aspects of the data very well as compared to the others. The belief is that the final output from all these models should be better than the output produced by just any one of them.</p><p class="calibre11">As mentioned earlier, the idea of ensemble is to combine many models together. These models can be of the same or of different types. For example, we can combine a neural network model output with a Bayesian model. We will restrict our discussions to using an ensemble of the same type of models in this chapter. Combining the same kind of models is vastly used in the Data Science community through techniques like Bagging and Boosting.</p><p class="calibre11">Bootstrap aggregation, commonly known as Bagging, is an elegant technique for generating a lot of models and combining their output to make a final prediction. Every model in a Bagging<a id="id635" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> ensemble uses only a portion of the training data. The idea behind Bagging is to reduce the overfitting of data. As stated before, we want each of the model to be slightly different from the others. So we sample the data with replacement for training each of the models, and thus introduce variability. Another way to introduce variation in the model is to sample the attributes. We don't provide all the attributes to the model, but different models get a different set of attributes. Bagging can be easily parallelized. Based on the parallel processing framework available, models can be constructed in parallel with different samples of the training dataset. Bagging doesn't work with linear predictors like linear regression.</p><p class="calibre11">Boosting<a id="id636" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is an ensemble technique which produces a sequence of increasingly complex models. It works sequentially by training the newer models based on the errors in the previous models. Every model that is trained is associated with a weight, which is calculated based on how well the model has performed on the given data. When the final prediction is made, these weights decides the amount of influence that a particular model has over the final output. Boosting does not lend itself to parallelism as naturally as Bagging. Since the models are built in a sequence, it cannot be parallelized. Errors made by classifiers coming early in the sequence are considered as hard instances to classify. The framework is designed in such a way that models coming later in the sequence pick up those misclassified or erroneous predictions made by the previous predictor, and try to improve upon them. Typically, very weak classifiers are used in Boosting, for example a decision stump, which is a decision tree with a single splitting node and two leaves, is used inside the ensemble. A very famous success story about Boosting is the Viola Jone Face Detection algorithm where several weak classifiers (decision stumps) were used to find good features. You can read more about this success story at the following website:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework">https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework</a>
</p><p class="calibre11">In this chapter, we will study the Bagging and Boosting Methods in detail. We will extend our discussion to a special type of Boosting called Gradient Boosting in the final recipe. We will also take a look at both regression and classification problems, and see how they can be addressed by ensemble learning.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec78" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Understanding Ensemble – Bagging Method</h1></div></div></div><p class="calibre11">Ensemble methods<a id="id637" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> belong to the family of methods known as<a id="id638" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> committee-based learning. Instead of leaving the decision of classification or regression to a single model, a group of models is used to make decisions in an ensemble. Bagging is a famous and widely used ensemble method.</p><p class="calibre11">Bagging is<a id="id639" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> also known as bootstrap aggregation. Bagging can be<a id="id640" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> made effective only if we are able to introduce variability in the underlying models, that is, if we can successfully introduce variability in the underlying dataset, it will lead to models with slight variations.</p><p class="calibre11">We leverage Bootstrapping to fed to these models variability in our dataset. Bootstrapping is the process by <a id="id641" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>which we randomly sample the given dataset for a specified number <a id="id642" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of instances, with or without replacement. In bagging, we leverage bootstrapping to generate, say, <code class="literal">m</code> is the different datasets and construct a model for each of them. Finally, we average the output of all the models to produce the final prediction in case of regression problems.</p><p class="calibre11">Let us say we bootstrap the data m times, we would have <code class="literal">m</code> models, that is, <code class="literal">y m</code> values, and our final prediction would be as follows:</p><div><img src="img/B04041_08_01.jpg" alt="Understanding Ensemble – Bagging Method" class="calibre173"/></div><p class="calibre11">In case of classification problems, the final output is decided based on voting. Let us say we have one hundred models in our ensemble, and we have a two-class classification problem with class labels as {+1,-1}. If more than 50 models predict the output as +1, we declare the prediction as +1. </p><p class="calibre11">Randomization is<a id="id643" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> another technique by which variability can be introduced in the model building exercise. An example is to pick randomly a subset of attributes for each model in the ensemble. That way, different models will have different sets of attributes. This technique is called the random subspaces method. </p><p class="calibre11">With very stable models, Bagging may not achieve very great results. Bagging helps most if the underlying classifier is very sensitive to even small changes to the data. For example, Decision trees, which are very unstable. Unpruned decision trees are a good candidate for Bagging. But say a Nearest Neighbor Classifier, K, is a very stable model. However, we can leverage the random subspaces, and introduce some instability into the nearest neighbor methods.</p><p class="calibre11">In the following recipe, you will learn how to leverage Bagging and Random subspaces on a K-Nearest Neighbor algorithm. We will take up a classification problem, and the final prediction will be based on majority voting.</p><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec277" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready…</h2></div></div></div><p class="calibre11">We will leverage the Scikit learn classes' <code class="literal">KNeighborsClassifier</code> for classification and <code class="literal">BaggingClassifier</code> for applying the bagging principle. We will generate data for this recipe using the <code class="literal">make_classification</code> convenience function.</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec278" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it</h2></div></div></div><p class="calibre11">Let us<a id="id644" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> import the necessary libraries, and write a function <code class="literal">get_data()</code> to<a id="id645" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> provide us with a dataset to work through this recipe:</p><div><pre class="programlisting">from sklearn.datasets import make_classification
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import classification_report
from sklearn.cross_validation import train_test_split


def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    print no_features,redundant_features,informative_features,repeated_features
    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y</pre></div><p class="calibre11">Let us proceed to write three functions:</p><p class="calibre11">Function build_single_model to make a simple KNearest neighbor model with the given data.</p><p class="calibre11">Function build_bagging_model, a function which implements the Bagging routine.</p><p class="calibre11">The function view_model to inspect the model that we have built:</p><div><pre class="programlisting">def build_single_model(x,y):
    model = KNeighborsClassifier()
    model.fit(x,y)
    return model


def build_bagging_model(x,y):
	bagging = BaggingClassifier(KNeighborsClassifier(),n_estimators=100,random_state=9 \
             ,max_samples=1.0,max_features=0.7,bootstrap=True,bootstrap_features=True)
	bagging.fit(x,y)
	return bagging

def view_model(model):
    print "\n Sampled attributes in top 10 estimators\n"
    for i,feature_set in  enumerate(model.estimators_features_[0:10]):
        print "estimator %d"%(i+1),feature_set</pre></div><p class="calibre11">Finally, we <a id="id646" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will write our main function, which will call the other<a id="id647" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> functions:</p><div><pre class="programlisting">if __name__ == "__main__":
    x,y = get_data()    

    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
        
    # Build a single model    
    model = build_single_model(x_train,y_train)
    predicted_y = model.predict(x_train)
    print "\n Single Model Accuracy on training data\n"
    print classification_report(y_train,predicted_y)
    # Build a bag of models
    bagging = build_bagging_model(x_train,y_train)
    predicted_y = bagging.predict(x_train)
    print "\n Bagging Model Accuracy on training data\n"
    print classification_report(y_train,predicted_y)
	view_model(bagging)
    
    # Look at the dev set
    predicted_y = model.predict(x_dev)
    print "\n Single Model Accuracy on Dev data\n"
    print classification_report(y_dev,predicted_y)

    print "\n Bagging Model Accuracy on Dev data\n"
    predicted_y = bagging.predict(x_dev)
    print classification_report(y_dev,predicted_y)</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec279" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let us <a id="id648" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>start with the main method. We first call the <code class="literal">get_data</code> function <a id="id649" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to return the dataset as a matrix x of predictors and a vector y for the response variable. Let us look into the <code class="literal">get_data</code> function:</p><div><pre class="programlisting">    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
 x,y =make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)</pre></div><p class="calibre11">Take a look at the parameters passed to the <code class="literal">make_classification</code> method. The first parameter is the number of instances required; in this case, we say we need 500 instances. The second parameter is the number of, attributes that are required per instance. We say that we need <code class="literal">30</code> of them as defined by the variable <code class="literal">no_features</code>. The third parameter, <code class="literal">flip_y</code>, randomly interchanges 3 percent of the instances. This is done to introduce some noise in our data. The next parameter specifies the number of features out of those <code class="literal">30</code> that should be informative enough to be used in our classification. We have specified that 60 percent of our features, that is, 18 out of 30 should be informative. The next parameter is about redundant features. These are generated as a linear combination of the informative features in order to introduce a correlation among the features. Finally, repeated features are the duplicate features which are drawn randomly from both informative features and redundant features.</p><p class="calibre11">Let us split the data into a training and a testing set using <code class="literal">train_test_split</code>. We reserve 30 percent of our data for testing:</p><div><pre class="programlisting">    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)</pre></div><p class="calibre11">Once again we leverage train_test_split to split our test data into dev and test.</p><div><pre class="programlisting">    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)</pre></div><p class="calibre11">Having divided the data for building, evaluating, and testing the model, we proceed to build our models. We are going to initially build a single model using <code class="literal">KNeighborsClassifier</code> by invoking the following:</p><div><pre class="programlisting">model = build_single_model(x_train,y_train)</pre></div><p class="calibre11">Inside this<a id="id650" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> function, we create an object of type <code class="literal">KNeighborsClassifier</code> <a id="id651" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and fit our data, as follows:</p><div><pre class="programlisting">def build_single_model(x,y):
    model = KNeighborsClassifier()
    model.fit(x,y)
    return model</pre></div><p class="calibre11">As explained in the previous section, <code class="literal">KNearestNeighbor</code> is a very stable algorithm. Let us see how this model performs. We perform our predictions on the training data and look at our model metrics: </p><div><pre class="programlisting">    predicted_y = model.predict(x_train)
    print "\n Single Model Accuracy on training data\n"
    print classification_report(y_train,predicted_y)</pre></div><p class="calibre11">
<code class="literal">classification_report</code> is a convenient function under the module metric in Scikit learn. It gives a table for <code class="literal">precision</code>, <code class="literal">recall</code>, and <code class="literal">f1-score</code>:</p><div><img src="img/B04041_08_02.jpg" alt="How it works…" class="calibre174"/></div><p class="calibre11">Out of <code class="literal">350</code> instances, our precision is 87 percent. With this figure, let us proceed to build our bagging model:</p><div><pre class="programlisting">    bagging = build_bagging_model(x_train,y_train)</pre></div><p class="calibre11">We invoke the function <code class="literal">build_bagging_model</code> with our training data to build a bag of classifiers, as follows:</p><div><pre class="programlisting">def build_bagging_model(x,y):
bagging =             BaggingClassifier(KNeighborsClassifier(),n_estimators=100,random_state=9 \
           ,max_samples=1.0,max_features=0.7,bootstrap=True,bootstrap_features=True)
bagging.fit(x,y)
return bagging</pre></div><p class="calibre11">Inside the method, we invoke the <code class="literal">BaggingClassifier</code> class. Let us look at the arguments that we pass to this class to initialize it.</p><p class="calibre11">The <a id="id652" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>first argument is the underlying estimator or model. By<a id="id653" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> passing <code class="literal">KNeighborClassifier</code>, we are telling the bagging classifier that we want to build a bag of <code class="literal">KNearestNeighbor</code> classifiers. The next parameter specifies the number of estimators that we will build. In this case, we are saying we need <code class="literal">100</code> of them. The <code class="literal">random_state</code> argument is the seed to be used by the random number generator. In order to be consistent during different runs, we set this to an integer value.</p><p class="calibre11">Our next parameter is max_samples, where we specify the number of instances to be selected for one estimator when we bootstrap from our input dataset. In this case, we are asking the bagging routine to select all the instances.</p><p class="calibre11">Next, the parameter max_features specifies the number of attributes that are to be included while bootstrapping for an estimator. We say that we want to include only 70 percent of the attributes. So for each estimator/model inside the ensemble, it will be using a different subset of the attributes to build the model. This is the random space methodology that we introduced in the previous section. The function proceeds to fit the model and return the model to the calling function.</p><div><pre class="programlisting">    bagging = build_bagging_model(x_train,y_train)
    predicted_y = bagging.predict(x_train)
    print "\n Bagging Model Accuracy on training data\n"
    print classification_report(y_train,predicted_y)</pre></div><p class="calibre11">Let us look at the model accuracy:</p><div><img src="img/B04041_08_03.jpg" alt="How it works…" class="calibre175"/></div><p class="calibre11">You can see a big jump in the model metrics.</p><p class="calibre11">Before we test our models with our dev dataset, let us look at the attributes that were allocated to the different models, by invoking the view_model function:</p><div><pre class="programlisting">    view_model(bagging)</pre></div><p class="calibre11">We print the<a id="id654" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> attributes selected for the first ten models, as<a id="id655" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> follows:</p><div><pre class="programlisting">def view_model(model):
    print "\n Sampled attributes in top 10 estimators\n"
    for i,feature_set in  enumerate(model.estimators_features_[0:10]):
        print "estimator %d"%(i+1),feature_set</pre></div><div><img src="img/B04041_08_04.jpg" alt="How it works…" class="calibre176"/></div><p class="calibre11">As you can make out from the result, we have assigned attributes to every estimator pretty much randomly. In this way, we introduced variability into each of our estimator.</p><p class="calibre11">Let us proceed to check how our single classifier and bag of estimators have performed in our dev set:</p><div><pre class="programlisting">    # Look at the dev set
    predicted_y = model.predict(x_dev)
    print "\n Single Model Accuracy on Dev data\n"
    print classification_report(y_dev,predicted_y)

    print "\n Bagging Model Accuracy on Dev data\n"
    predicted_y = bagging.predict(x_dev)
    print classification_report(y_dev,predicted_y)</pre></div><div><img src="img/B04041_08_05.jpg" alt="How it works…" class="calibre18"/></div><p class="calibre11">As expected, our bag of estimators has performed better in our dev set as compared to our single classifier.</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec280" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">As we said<a id="id656" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> earlier, in the case of classification, the label with the <a id="id657" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>majority number of votes is considered as the final prediction. Instead of the voting scheme, we can ask the constituent models to output the prediction probabilities for the labels. An average of the probabilities can be finally taken to decide the final output label. In Scikit's case, the documentation of the API provides the details on how the final prediction is performed: </p><div><blockquote class="blockquote"><p class="calibre17"><em class="calibre15">'The predicted class of an input sample is computed as the class with the highest mean predicted probability. If base estimators do not implement a <em class="calibre15">predict phobia</em> method, then it resorts to voting.'</em></p></blockquote></div><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html</a>
</p><p class="calibre11">In the last chapter, we discussed cross validation. Although cross validation may look very similar to bagging, they have different uses in reality. In cross validation, we create K-Folds and, based on the model output from those folds, we may choose our parameters for the model, as we selected the alpha value for ridge regression. This is  done primarily to avoid exposing our test data in the model building exercise. Cross validation can be used in Bagging to determine the number of estimators we need to add to our bagging module.</p><p class="calibre11">However, a <a id="id658" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>drawback with Bagging is that we loose the interpretability<a id="id659" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of a model. Consider a Simple Decision tree derived after pruning. It is very easy to explain the decision tree model. But once we have a bag of 100 such models, it become a black box. For increased accuracy, we do a trading of interpretability.</p><p class="calibre11">Please refer to the following paper by Leo Breiman for more information about bagging:</p><div><blockquote class="blockquote"><p class="calibre17"><em class="calibre15">Leo Breiman. 1996. Bagging predictors.<em class="calibre15">Mach. Learn.</em>24, 2 (August 1996), 123-140. DOI=10.1023/A:1018054314350 http://dx.doi.org/10.1023/A:1018054314350</em></p></blockquote></div></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec281" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Using cross validation iterators</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>, <em class="calibre15">Machine Learning 2</em></li><li class="listitem"><em class="calibre15">Building Decision Trees to solve Multi-Class Problems</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6. Machine Learning 1">Chapter 6</a>, <em class="calibre15">Machine Learning 1</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec79" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Understanding Ensemble – Boosting Method</h1></div></div></div><p class="calibre11">Boosting is a powerful ensemble technique. It's pretty much used in most of the Data Science applications. In fact, it is one of the most essential tools in a Data Scientist tool kit. The<a id="id660" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> Boosting technique utilizes a bag of estimators similar to <a id="id661" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Bagging. But that is where the similarity ends. Let us quickly see how Boosting acts as a very effective ensemble technique before jumping into our recipe.</p><p class="calibre11">Let's take the familiar two-class classification problem, where the input is a set of predictors (<code class="literal">X</code>) and the output is a response variable (<code class="literal">Y</code>) which can either take <code class="literal">0</code> or <code class="literal">1</code> as value. The <a id="id662" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>input for the classification problem is represented as follows:</p><div><img src="img/B04041_08_06.jpg" alt="Understanding Ensemble – Boosting Method" class="calibre177"/></div><p class="calibre11">The job the classifier is to find a function which can approximate:</p><div><img src="img/B04041_08_07.jpg" alt="Understanding Ensemble – Boosting Method" class="calibre137"/></div><p class="calibre11">The misclassification rate of the classifier is defined as:</p><div><img src="img/B04041_08_08.jpg" alt="Understanding Ensemble – Boosting Method" class="calibre178"/></div><p class="calibre11">Let us say<a id="id663" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> we build a very weak classifier whose error rate is slightly better than random guessing. In Boosting, we build a sequence of weak classifiers on a slightly modified set of data. We modify the data slightly for every classifier, and <a id="id664" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>finally, we end up with M classifiers:</p><div><img src="img/B04041_08_09.jpg" alt="Understanding Ensemble – Boosting Method" class="calibre179"/></div><p class="calibre11">Finally, the predictions from all of them are combined through a weighted majority vote:</p><div><img src="img/B04041_08_10.jpg" alt="Understanding Ensemble – Boosting Method" class="calibre180"/></div><p class="calibre11">This method is called AdaBoost.</p><p class="calibre11">The weight alpha<a id="id665" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> and the sequential way of model building is where Boosting differs from Bagging. As mentioned earlier, Boosting builds a sequence of weak classifiers on a slightly modified data set for each classifier. Let us look at what that slight data modification refers to. It is from this modification that we derive our weight alpha.</p><p class="calibre11">Initially for the first classifier, m=1, we set the weight of each instance as 1/N, that is, if there are a hundred records, each record gets a weight of 0.001. Let us denote the weight by w—now we have a hundred such weights:</p><div><img src="img/B04041_08_11.jpg" alt="Understanding Ensemble – Boosting Method" class="calibre181"/></div><p class="calibre11">All the records now have an equal chance of being selected by a classifier. We build the classifier, and test it against our training data to get the misclassification rate. Refer to the misclassification rate formula given earlier in this section. We are going to change it slightly by<a id="id666" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> including the weights, as follows:</p><div><img src="img/B04041_08_12.jpg" alt="Understanding Ensemble – Boosting Method" class="calibre182"/></div><p class="calibre11">Where abs <a id="id667" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>stands for the absolute value of the results. With this error rate, we calculate our alpha (model weight) as follows:</p><div><img src="img/B04041_08_13.jpg" alt="Understanding Ensemble – Boosting Method" class="calibre183"/></div><p class="calibre11">Where epsilon is a very small value.</p><p class="calibre11">Let us say our model 1 has got an error rate of 0.3, that is, the model was able to classify 70 percent of the records correctly. Therefore, the weight for that model will be 0.8, approximately, which, is a good weight. Based on this, we go will back and set the weights of individual records, as follows:</p><div><img src="img/B04041_08_14.jpg" alt="Understanding Ensemble – Boosting Method" class="calibre184"/></div><p class="calibre11">As you can see, the weights of all the attributes which were misclassified will increase. This increases the chance of the misclassified record being selected by the next classifier. Thus, the classifier coming next in the sequence selects the instances with more weight and tries to fit it. In this way, all the future classifiers start concentrating on the records misclassified by the previous classifier.</p><p class="calibre11">This is the power of boosting. It is able to turn several weak classifiers into one powerful ensemble.</p><p class="calibre11">Let us see boosting in action. As we proceed with our code, we will also see a small variation to AdaBoost known as SAMME.</p><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec282" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting Started…</h2></div></div></div><p class="calibre11">We will leverage<a id="id668" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the scikit learn classes <code class="literal">DecisionTreeClassifier</code> for classification and the <code class="literal">AdaBoostClassifier</code> for applying the Boosting principle. We will generate data for this recipe using the <code class="literal">make_classification</code> convenience function.</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec283" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it</h2></div></div></div><p class="calibre11">Let us <a id="id669" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>import the necessary libraries, and write a function <code class="literal">get_data()</code> to provide us with a dataset to work through this recipe.</p><div><pre class="programlisting">from sklearn.datasets import make_classification
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report,zero_one_loss
from sklearn.cross_validation import train_test_split
from sklearn.tree import DecisionTreeClassifier
import numpy as np
import matplotlib.pyplot as plt
import itertools

def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    print no_features,redundant_features,informative_features,repeated_features
    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y
    
def build_single_model(x,y):
    model = DecisionTreeClassifier()
    model.fit(x,y)
    return model
    
def build_boosting_model(x,y,no_estimators=20):
    boosting = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,min_samples_leaf=1),random_state=9 \
    ,n_estimators=no_estimators,algorithm="SAMME")
    boosting.fit(x,y)
    return boosting

def view_model(model):
    print "\n Estimator Weights and Error\n"
    for i,weight in  enumerate(model.estimator_weights_):
        print "estimator %d weight = %0.4f error = %0.4f"%(i+1,weight,model.estimator_errors_[i])
        
    plt.figure(1)
    plt.title("Model weight vs error")
    plt.xlabel("Weight")
    plt.ylabel("Error")
    plt.plot(model.estimator_weights_,model.estimator_errors_)
    

def number_estimators_vs_err_rate(x,y,x_dev,y_dev):
    no_estimators = range(20,120,10)
    misclassy_rate = []
    misclassy_rate_dev = []
    
    for no_estimator in no_estimators:
        boosting = build_boosting_model(x,y,no_estimators=no_estimator)
        predicted_y = boosting.predict(x)
        predicted_y_dev = boosting.predict(x_dev)        
        misclassy_rate.append(zero_one_loss(y,predicted_y))
        misclassy_rate_dev.append(zero_one_loss(y_dev,predicted_y_dev))
    
    plt.figure(2)
    plt.title("No estimators vs Mis-classification rate")
    plt.xlabel("No of estimators")
    plt.ylabel("Mis-classification rate")
    plt.plot(no_estimators,misclassy_rate,label='Train')
    plt.plot(no_estimators,misclassy_rate_dev,label='Dev')
        
    plt.show() 

    
    
if __name__ == "__main__":
    x,y = get_data()    
    plot_data(x,y)

    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
        
    # Build a single model    
    model = build_single_model(x_train,y_train)
    predicted_y = model.predict(x_train)
    print "\n Single Model Accuracy on training data\n"
    print classification_report(y_train,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_train,predicted_y)*100),"%"


    # Build a bag of models
    boosting = build_boosting_model(x_train,y_train, no_estimators=85)
    predicted_y = boosting.predict(x_train)
    print "\n Boosting Model Accuracy on training data\n"
    print classification_report(y_train,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_train,predicted_y)*100),"%"
    
    view_model(boosting)
    
    # Look at the dev set
    predicted_y = model.predict(x_dev)
    print "\n Single Model Accuracy on Dev data\n"
    print classification_report(y_dev,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_dev,predicted_y)*100),"%"

    print "\n Boosting Model Accuracy on Dev data\n"
    predicted_y = boosting.predict(x_dev)
    print classification_report(y_dev,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_dev,predicted_y)*100),"%"

    number_estimators_vs_err_rate(x_train,y_train,x_dev,y_dev)</pre></div><p class="calibre11">Let us<a id="id670" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> proceed and write the following three functions:</p><p class="calibre11">The function build_single_model to make a simple Decision Tree model with the given data. </p><p class="calibre11">The<a id="id671" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> function build_boosting_model, a function which implements the Boosting routine.</p><p class="calibre11">The function view_model, to inspect the model that we have built.</p><div><pre class="programlisting">def build_single_model(x,y):
    model = DecisionTreeClassifier()
    model.fit(x,y)
    return model
    
def build_boosting_model(x,y,no_estimators=20):
    boosting = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,min_samples_leaf=1),random_state=9 \
    ,n_estimators=no_estimators,algorithm="SAMME")
    boosting.fit(x,y)
    return boosting

def view_model(model):
    print "\n Estimator Weights and Error\n"
    for i,weight in  enumerate(model.estimator_weights_):
        print "estimator %d weight = %0.4f error = %0.4f"%(i+1,weight,model.estimator_errors_[i])
        
    plt.figure(1)
    plt.title("Model weight vs error")
    plt.xlabel("Weight")
    plt.ylabel("Error")
    plt.plot(model.estimator_weights_,model.estimator_errors_)</pre></div><p class="calibre11">We then write a function called number_estimators_vs_err_rate. We use this function to see how our error rates change with respect to the number of models in our ensemble.</p><div><pre class="programlisting">def number_estimators_vs_err_rate(x,y,x_dev,y_dev):
    no_estimators = range(20,120,10)
    misclassy_rate = []
    misclassy_rate_dev = []
    
    for no_estimator in no_estimators:
        boosting = build_boosting_model(x,y,no_estimators=no_estimator)
        predicted_y = boosting.predict(x)
        predicted_y_dev = boosting.predict(x_dev)        
        misclassy_rate.append(zero_one_loss(y,predicted_y))
        misclassy_rate_dev.append(zero_one_loss(y_dev,predicted_y_dev))
    
    plt.figure(2)
    plt.title("No estimators vs Mis-classification rate")
    plt.xlabel("No of estimators")
    plt.ylabel("Mis-classification rate")
    plt.plot(no_estimators,misclassy_rate,label='Train')
    plt.plot(no_estimators,misclassy_rate_dev,label='Dev')
        
    plt.show()</pre></div><p class="calibre11">Finally, we<a id="id672" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> will write our main function, which will call the other<a id="id673" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> functions.</p><div><pre class="programlisting">if __name__ == "__main__":
    x,y = get_data()    
    plot_data(x,y)

    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
        
    # Build a single model    
    model = build_single_model(x_train,y_train)
    predicted_y = model.predict(x_train)
    print "\n Single Model Accuracy on training data\n"
    print classification_report(y_train,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_train,predicted_y)*100),"%"


    # Build a bag of models
    boosting = build_boosting_model(x_train,y_train, no_estimators=85)
    predicted_y = boosting.predict(x_train)
    print "\n Boosting Model Accuracy on training data\n"
    print classification_report(y_train,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_train,predicted_y)*100),"%"
    
    view_model(boosting)
    
    # Look at the dev set
    predicted_y = model.predict(x_dev)
    print "\n Single Model Accuracy on Dev data\n"
    print classification_report(y_dev,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_dev,predicted_y)*100),"%"

    print "\n Boosting Model Accuracy on Dev data\n"
    predicted_y = boosting.predict(x_dev)
    print classification_report(y_dev,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_dev,predicted_y)*100),"%"

    number_estimators_vs_err_rate(x_train,y_train,x_dev,y_dev)</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec284" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let us<a id="id674" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> start with the main method. We first call the <code class="literal">get_data</code> function to<a id="id675" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> return the dataset as a matrix x of predictors and a vector y for the response variable.  Let us look into the <code class="literal">get_data</code> function:</p><div><pre class="programlisting">    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
 x,y =make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)</pre></div><p class="calibre11">Take a look at the parameters passed to the <code class="literal">make_classification</code> method. The first parameter is the number of instances required; in this case, we say we need 500 instances. The second parameter gives the number of attributes that are required per instance. We say that we need 30 of them, as defined by the variable <code class="literal">no_features</code>. The third parameter <code class="literal">flip_y</code>, randomly interchanges 3 percent of the instances. This is done to introduce some noise in our data. The next parameter specifies the number of features out of those 30 which should be informative enough to be used in our classification. We have specified that 60 percent of our features, that is, 18 out of 30 should be informative. The next parameter is about redundant features. These are generated as a linear combination of the informative features for introducing a correlation among the features. Finally, repeated features are the duplicate features which are drawn randomly from both, informative features and redundant features.</p><p class="calibre11">Let us split the data into a training and a testing set using <code class="literal">train_test_split</code>. We reserve 30 percent of our data for testing.</p><div><pre class="programlisting">    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)</pre></div><p class="calibre11">Once again, we leverage train_test_split to split our test data into dev and test.</p><div><pre class="programlisting">    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)</pre></div><p class="calibre11">Having divided<a id="id676" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the data for building, evaluating, and testing the <a id="id677" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>model, we proceed to build our models.</p><p class="calibre11">Let us start by fitting a single decision tree, and look at the performance of the tree on training set:</p><div><pre class="programlisting">    # Build a single model    
    model = build_single_model(x_train,y_train)</pre></div><p class="calibre11">We build a model by invoking <code class="literal">build_single_model</code> function with the predictors and response variable. Inside this we fit a single decision tree and return the tree back to the calling function.</p><div><pre class="programlisting">def build_single_model(x,y):
    model = DecisionTreeClassifier()
    model.fit(x,y)
    return model</pre></div><p class="calibre11">Let us evaluate how good the model is using <code class="literal">classification_report</code>, a utility function from Scikit learn which displays a set of metrics including <code class="literal">precision</code>, <code class="literal">recall</code> and <code class="literal">f1-score</code>; we also display the misclassification rate.</p><div><pre class="programlisting">    predicted_y = model.predict(x_train)
    print "\n Single Model Accuracy on training data\n"
    print classification_report(y_train,predicted_y)
    print "Fraction of misclassfication =     
           %0.2f"%(zero_one_loss(y_train,predicted_y)*100),"%"</pre></div><div><img src="img/B04041_08_15.jpg" alt="How it works…" class="calibre185"/></div><p class="calibre11">As you can<a id="id678" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> see, our decision tree model has done a perfect job<a id="id679" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of fitting the data—our misclassification rate is 0. Before we test this model on our dev data, let us build our ensemble:</p><div><pre class="programlisting">    # Build a bag of models
    boosting = build_boosting_model(x_train,y_train, no_estimators=85)</pre></div><p class="calibre11">Using the method build_boosting_model, we build our ensemble as follows:</p><div><pre class="programlisting">    boosting = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,min_samples_leaf=1),random_state=9 \
    ,n_estimators=no_estimators,algorithm="SAMME")
    boosting.fit(x,y)</pre></div><p class="calibre11">We leverage <code class="literal">AdaBoostClassifier</code> from Scikit learn to build our Boosting ensemble. We instantiate the class with the following parameters:</p><p class="calibre11">An estimator—in our case, we say we want to build an ensemble of decision trees. Hence, we pass the <code class="literal">DecisionTreeClassifier</code> object. </p><p class="calibre11">
<code class="literal">max_depth</code>—We don't want to have fully grown trees in our ensemble. We need only stumps—trees with just two leaf nodes and one splitting node. Hence, we set the <code class="literal">max_depth</code> parameter to 1. </p><p class="calibre11">With the n_estimators parameters, we specify the number of trees that we want to grow; in this case, we will grow 86 trees.</p><p class="calibre11">Finally, we have a parameter called algorithm, which is set to <code class="literal">SAMME</code>. <code class="literal">SAMME</code> stands for Stage wise Additive Modeling using Multi-class Exponential loss function. <code class="literal">SAMME</code> is an improvement over the AdaBoosting algorithm. It tries to put more weights on the misclassified records. The model weight alpha is where <code class="literal">SAMME</code> differs from AdaBoost.</p><div><img src="img/B04041_08_16.jpg" alt="How it works…" class="calibre186"/></div><p class="calibre11">We have ignored the constant 0.5 in the preceding formula. Let us look at the new addition: log (K-1). If K = 2, then the preceding equation reduces to AdaBoost. Here, K is the number <a id="id680" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of classes in our response variable. For a two-class<a id="id681" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> problem, SAMME reduces to AdaBoost, as stated earlier.</p><p class="calibre11">Let us fit the model, and return it to our calling function. We run this model on our training dataset, and once again look at how the model has performed:</p><div><pre class="programlisting">    predicted_y = boosting.predict(x_train)
    print "\n Boosting Model Accuracy on training data\n"
    print classification_report(y_train,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_train,predicted_y)*100),"%"</pre></div><div><img src="img/B04041_08_17.jpg" alt="How it works…" class="calibre187"/></div><p class="calibre11">The result is not very different from our original model. We have correctly classified almost 98 percent of the records with this ensemble.</p><p class="calibre11">Before testing it on our dev set, let us proceed to look at the Boosting ensemble that we have built:</p><div><pre class="programlisting">    view_model(boosting)</pre></div><p class="calibre11">Inside view_model, we first print out the weights assigned to each classifier in our ensemble:</p><div><pre class="programlisting">    print "\n Estimator Weights and Error\n"
    for i,weight in  enumerate(model.estimator_weights_):
        print "estimator %d weight = %0.4f error = %0.4f"%(i+1,weight,model.estimator_errors_[i])</pre></div><div><img src="img/B04041_08_18.jpg" alt="How it works…" class="calibre188"/></div><p class="calibre11">Here we<a id="id682" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> have shown the weights of the first 20 ensembles. Based <a id="id683" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>on their misclassification rate, we have assigned different weights to these estimators.</p><p class="calibre11">Let us proceed to plot a graph showing the estimator weight versus the error thrown by each estimator:</p><div><pre class="programlisting">    plt.figure(1)
    plt.title("Model weight vs error")
    plt.xlabel("Weight")
    plt.ylabel("Error")
    plt.plot(model.estimator_weights_,model.estimator_errors_)</pre></div><div><img src="img/B04041_08_19.jpg" alt="How it works…" class="calibre189"/></div><p class="calibre11">As you can <a id="id684" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see, the models which are classifying properly are <a id="id685" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>assigned more weights than the ones with higher errors.</p><p class="calibre11">Let us now look at the way single tree and the bag of tree have performed against the dev data:</p><div><pre class="programlisting">    # Look at the dev set
    predicted_y = model.predict(x_dev)
    print "\n Single Model Accuracy on Dev data\n"
    print classification_report(y_dev,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_dev,predicted_y)*100),"%"

    print "\n Boosting Model Accuracy on Dev data\n"
    predicted_y = boosting.predict(x_dev)
    print classification_report(y_dev,predicted_y)
    print "Fraction of misclassfication = %0.2f"%(zero_one_loss(y_dev,predicted_y)*100),"%"</pre></div><p class="calibre11">Pretty much as we did with the training data, we print the classification report and the misclassification rate:</p><div><img src="img/B04041_08_20.jpg" alt="How it works…" class="calibre190"/></div><p class="calibre11">As you can<a id="id686" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> see, the single tree has performed poorly. Though<a id="id687" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> it displayed a 100 percent accuracy with the training data, with the dev data it has misclassified almost 40 percent of the records—a sign of overfitting. In contrast, the Boosting model is able to make a better fit of the dev data.</p><p class="calibre11">How do we go about improving the Boosting model? One way to do it is to test the error rate in the training set against the number of ensembles that we want to include in our bagging.</p><div><pre class="programlisting">    number_estimators_vs_err_rate(x_train,y_train,x_dev,y_dev)</pre></div><p class="calibre11">The following function proceeds to fit with the increasing number of ensembles and plot the error rate:</p><div><pre class="programlisting">def number_estimators_vs_err_rate(x,y,x_dev,y_dev):
    no_estimators = range(20,120,10)
    misclassy_rate = []
    misclassy_rate_dev = []
    
    for no_estimator in no_estimators:
        boosting = build_boosting_model(x,y,no_estimators=no_estimator)
        predicted_y = boosting.predict(x)
        predicted_y_dev = boosting.predict(x_dev)        
        misclassy_rate.append(zero_one_loss(y,predicted_y))
        misclassy_rate_dev.append(zero_one_loss(y_dev,predicted_y_dev))
    
    plt.figure(2)
    plt.title("No estimators vs Mis-classification rate")
    plt.xlabel("No of estimators")
    plt.ylabel("Mis-classification rate")
    plt.plot(no_estimators,misclassy_rate,label='Train')
    plt.plot(no_estimators,misclassy_rate_dev,label='Dev')
        
    plt.show()</pre></div><p class="calibre11">As you can see, we <a id="id688" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>declare a list, starting with 20 and ending <a id="id689" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with 120 in step size of 10s. Inside the <code class="literal">for</code> loop, we pass each element of this list as the number of estimator parameter to <code class="literal">build_boosting_model</code>, and then proceed to access the error rate of the model. We then check the error rates in the dev set. Now we have two lists—one which has all the error rates from the training data and another with the error rates from the dev data. We plot them both, where the <em class="calibre15">x</em> axis is the number of estimators and <em class="calibre15">y</em> axis is the misclassification rate in the dev and train sets.</p><div><img src="img/B04041_08_21.jpg" alt="How it works…" class="calibre191"/></div><p class="calibre11">The preceding plot gives a clue that in around 30 to 40 estimators, the error rate in dev is very low. We <a id="id690" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can further experiment with the tree model parameters to arrive at a good model.</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec285" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">Boosting was<a id="id691" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> introduced in the following seminal paper:</p><div><blockquote class="blockquote"><p class="calibre17"><em class="calibre15">Freund, Y. &amp; Schapire, R. (1997), 'A decision theoretic generalization of on-line learning and an application to boosting', Journal of Computer and System Sciences 55(1), 119–139.</em></p></blockquote></div><p class="calibre11">Initially, most of the Boosting methods reduced the multiclass problems into two-class problems and multiple two-class problems. The following paper extends AdaBoost to the multiclass problems:</p><div><blockquote class="blockquote"><p class="calibre17"><em class="calibre15">Multi-class AdaBoost Statistics and Its Interface, Vol. 2, No. 3. (2009), pp. 349-360,doi:10.4310/sii.2009.v2.n3.a8 by Trevor Hastie, Saharon Rosset, Ji Zhu, Hui Zou</em></p></blockquote></div><p class="calibre11">This paper also introduces SAMME, the method that we have used in our recipe.</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec286" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Building Decision Trees to solve Multi-Class Problems </em>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6. Machine Learning 1">Chapter 6</a>, <em class="calibre15">Machine Learning I</em></li><li class="listitem"><em class="calibre15">Using cross validation iterators </em>recipe in<em class="calibre15"> </em><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>, <em class="calibre15">Machine Learning II</em></li><li class="listitem"><em class="calibre15">Understanding Ensemble – Bagging Method </em>recipe in<em class="calibre15"> </em><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch08.xhtml" title="Chapter 8. Ensemble Methods">Chapter 8</a>, <em class="calibre15">Model Selection and Evaluation</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch08lvl1sec80" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Understanding Ensemble – Gradient Boosting</h1></div></div></div><p class="calibre11">Let us recall the Boosting algorithm explained in the previous recipe. In boosting, we fitted an additive <a id="id692" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>model in a forward, stage-wise manner. We built the classifiers sequentially. After building each classifier, we estimated the<a id="id693" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> weight/importance of the classifiers. Based on weights/importance, we adjusted the weights of the instances in our training set. Misclassified instances were weighted higher than the correctly classified ones. We would like the next model to pick those incorrectly classified instances and train on them. Instances from the dataset which didn't fit properly were identified using these weights. Another way of looking at it is that those records were the shortcomings of the previous model. The next model tries to overcome those shortcomings.</p><p class="calibre11">Gradient Boosting uses gradients instead of weights to identify those shortcomings. Let us quickly see how we can use gradients to improve models. </p><p class="calibre11">Let us take a <a id="id694" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>simple regression problem, where we are given the required predictor variable <em class="calibre15">X</em> and the response <em class="calibre15">Y</em>, which is a real number.</p><div><img src="img/B04041_08_22.jpg" alt="Understanding Ensemble – Gradient Boosting" class="calibre192"/></div><p class="calibre11">Gradient boosting proceeds as follows:</p><p class="calibre11">It starts with a very simple model, say, mean value.</p><div><img src="img/B04041_08_23.jpg" alt="Understanding Ensemble – Gradient Boosting" class="calibre193"/></div><p class="calibre11">The predicted value is simply the mean value of the response variables.</p><p class="calibre11">It then proceeds to<a id="id695" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> fit the residuals. Residual is the difference between the actual value y and the predicted value y hat.</p><div><img src="img/B04041_08_24.jpg" alt="Understanding Ensemble – Gradient Boosting" class="calibre194"/></div><p class="calibre11">The next classifier is trained on the data set as follows:</p><div><img src="img/B04041_08_25.jpg" alt="Understanding Ensemble – Gradient Boosting" class="calibre195"/></div><p class="calibre11">The subsequent model is trained on the residual of the previous model, and thus, the algorithm proceeds to build the required number of models inside the ensemble.</p><p class="calibre11">Let us try and understand why we train on residuals. By now it should be clear that Boosting makes additive models. Let us say we build two models <code class="literal">F1 (X)</code> and <code class="literal">F2(X)</code> to predict <code class="literal">Y1</code>. By the additive principle, we can combine these two models as follows:</p><div><img src="img/B04041_08_26.jpg" alt="Understanding Ensemble – Gradient Boosting" class="calibre196"/></div><p class="calibre11">That is, we combine the prediction from both the models to predict Y_1.</p><p class="calibre11">Equivalently, we can say that:</p><div><img src="img/B04041_08_27.jpg" alt="Understanding Ensemble – Gradient Boosting" class="calibre197"/></div><p class="calibre11">Residual is the <a id="id696" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>part where the model has not done well, or to put it simply, residuals are the short comings of the previous model. Hence, we use the residual improve our model, that is, improving the shortcomings of the previous model. Based on this discussion, you would wonder why the method is called Gradient Boosting instead of Residual Boosting.</p><p class="calibre11">Given a function which is differentiable, Gradient stands for the first-order derivatives of that function at certain values. In the case of regression, the objective function is:</p><div><img src="img/B04041_08_28.jpg" alt="Understanding Ensemble – Gradient Boosting" class="calibre198"/></div><p class="calibre11">where <code class="literal">F(xi)</code> is our regression model.</p><p class="calibre11">The linear regression problem is about minimizing this preceding function. We find the first-order derivative of that function at value <code class="literal">F(xi)</code>, and if we update our weights' coefficients with the negative of that derivative value, we will move towards a minimum solution in the search space. The first-order derivative of the preceding cost function with respect to <code class="literal">F(xi)</code> is  <code class="literal">F(xi ) – yi</code>. Please refer to the following link for the derivation:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://en.wikipedia.org/wiki/Gradient_descent">https://en.wikipedia.org/wiki/Gradient_descent</a>
</p><p class="calibre11">
<code class="literal">F(xi ) – yi</code>, the gradient, is the negative of our residual <code class="literal">yi – F(xi)</code>, and hence the name Gradient Boosting.</p><p class="calibre11">With this theory in mind, let us jump into our recipe for gradient boosting.</p><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec287" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting Started…</h2></div></div></div><p class="calibre11">We are going to<a id="id697" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> use the Boston dataset to demonstrate Gradient Boosting. The Boston data has 13 attributes and 506 instances. The target variable is a real number, the median value of houses in thousands. the Boston data can be downloaded from the UCI link:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names">https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names</a>
</p><p class="calibre11">We intend to generate Polynomial Features of degree 2, and consider only the interaction effects.</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec288" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it</h2></div></div></div><p class="calibre11">Let us import the <a id="id698" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>necessary libraries and write a function <code class="literal">get_data()</code> to provide us with a dataset to work through this recipe:</p><div><pre class="programlisting"># Load libraries
from sklearn.datasets import load_boston
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
import numpy as np
import matplotlib.pyplot as plt

def get_data():
    """
    Return boston dataset
    as x - predictor and
    y - response variable
    """
    data = load_boston()
    x    = data['data']
    y    = data['target']
    return x,y    

def build_model(x,y,n_estimators=500):
    """
    Build a Gradient Boost regression model
    """
    model = GradientBoostingRegressor(n_estimators=n_estimators,verbose=10,\
            subsample=0.7, learning_rate= 0.15,max_depth=3,random_state=77)
    model.fit(x,y)
    return model    

def view_model(model):
    """
    """
    print "\n Training scores"
    print "======================\n"
    for i,score in enumerate(model.train_score_):
        print "\tEstimator %d score %0.3f"%(i+1,score)

    plt.cla()
    plt.figure(1)
    plt.plot(range(1,model.estimators_.shape[0]+1),model.train_score_)
    plt.xlabel("Model Sequence")
    plt.ylabel("Model Score")
    plt.show()

    print "\n Feature Importance"
    print "======================\n"
    for i,score in enumerate(model.feature_importances_):
        print "\tFeature %d Importance %0.3f"%(i+1,score)
    
            

def model_worth(true_y,predicted_y):
    """
    Evaluate the model
    """
    print "\tMean squared error = %0.2f"%(mean_squared_error(true_y,predicted_y))



if __name__ == "__main__":
    
    x,y = get_data()
    
    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
    
    
    
    #Prepare some polynomial features
    poly_features = PolynomialFeatures(2,interaction_only=True)
    poly_features.fit(x_train)
    x_train_poly = poly_features.transform(x_train)
    x_dev_poly   = poly_features.transform(x_dev)
    
    # Build model with polynomial features
    model_poly = build_model(x_train_poly,y_train)
    predicted_y = model_poly.predict(x_train_poly)
    print "\n Model Performance in Training set (Polynomial features)\n"
    model_worth(y_train,predicted_y)  

    # View model details
    view_model(model_poly)
    
    # Apply the model on dev set
    predicted_y = model_poly.predict(x_dev_poly)
    print "\n Model Performance in Dev set  (Polynomial features)\n"
    model_worth(y_dev,predicted_y)  
    
    # Apply the model on Test set
    x_test_poly = poly_features.transform(x_test)
    predicted_y = model_poly.predict(x_test_poly)

    print "\n Model Performance in Test set  (Polynomial features)\n"
    model_worth(y_test,predicted_y)  </pre></div><p class="calibre11">Let us write the<a id="id699" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> following three functions.</p><p class="calibre11">The function build _model, which implements the Gradient Boosting routine.</p><p class="calibre11">The functions view_model and model_worth, which are used to inspect the model that we have built:</p><div><pre class="programlisting">def build_model(x,y,n_estimators=500):
    """
    Build a Gradient Boost regression model
    """
    model = GradientBoostingRegressor(n_estimators=n_estimators,verbose=10,\
            subsample=0.7, learning_rate= 0.15,max_depth=3,random_state=77)
    model.fit(x,y)
    return model    

def view_model(model):
    """
    """
    print "\n Training scores"
    print "======================\n"
    for i,score in enumerate(model.train_score_):
        print "\tEstimator %d score %0.3f"%(i+1,score)

    plt.cla()
    plt.figure(1)
    plt.plot(range(1,model.estimators_.shape[0]+1),model.train_score_)
    plt.xlabel("Model Sequence")
    plt.ylabel("Model Score")
    plt.show()

    print "\n Feature Importance"
    print "======================\n"
    for i,score in enumerate(model.feature_importances_):
        print "\tFeature %d Importance %0.3f"%(i+1,score)
    
            

def model_worth(true_y,predicted_y):
    """
    Evaluate the model
    """
    print "\tMean squared error = %0.2f"%(mean_squared_error(true_y,predicted_y))</pre></div><p class="calibre11">Finally, we <a id="id700" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>write our main function which will call the other functions:</p><div><pre class="programlisting">if __name__ == "__main__":
    
    x,y = get_data()
    
    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
    
    
    
    #Prepare some polynomial features
    poly_features = PolynomialFeatures(2,interaction_only=True)
    poly_features.fit(x_train)
    x_train_poly = poly_features.transform(x_train)
    x_dev_poly   = poly_features.transform(x_dev)
    
    # Build model with polynomial features
    model_poly = build_model(x_train_poly,y_train)
    predicted_y = model_poly.predict(x_train_poly)
    print "\n Model Performance in Training set (Polynomial features)\n"
    model_worth(y_train,predicted_y)  

    # View model details
    view_model(model_poly)
    
    # Apply the model on dev set
    predicted_y = model_poly.predict(x_dev_poly)
    print "\n Model Performance in Dev set  (Polynomial features)\n"
    model_worth(y_dev,predicted_y)  
    
    # Apply the model on Test set
    x_test_poly = poly_features.transform(x_test)
    predicted_y = model_poly.predict(x_test_poly)

    print "\n Model Performance in Test set  (Polynomial features)\n"
    model_worth(y_test,predicted_y)  </pre></div></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec289" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let us start <a id="id701" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with the main module and follow the code. We load the predictor x and response variable y using the get_data function:</p><div><pre class="programlisting">def get_data():
    """
    Return boston dataset
    as x - predictor and
    y - response variable
    """
    data = load_boston()
    x    = data['data']
    y    = data['target']
    return x,y    </pre></div><p class="calibre11">The function invokes the Scikit learn's convenience function <code class="literal">load_boston()</code> to retrieve the Boston house pricing dataset as numpy arrays.</p><p class="calibre11">We proceed to divide the data into the train and test sets using the train_test_split function from Scikit library. We reserve 30 percent of our dataset for testing.</p><div><pre class="programlisting">x_train,x_test_all,y_train,y_test_all = 
train_test_split(x,y,test_size = 0.3,random_state=9)</pre></div><p class="calibre11">Out of this 30 percent, we again extract the dev set in the next line:</p><div><pre class="programlisting">x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)</pre></div><p class="calibre11">We proceed to build the polynomial features as follows:</p><div><pre class="programlisting">poly_features = PolynomialFeatures(interaction_only=True)
poly_features.fit(x_train)</pre></div><p class="calibre11">As you can <a id="id702" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see, we have set interaction_only to True. By having interaction_only set to true, given say x1 and x2 attribute, only x1*x2 attribute is created. Square of x1 and square of x2 are not created, assuming that the degree is 2. The default degree is 2.</p><div><pre class="programlisting">x_train_poly = poly_features.transform(x_train)
x_dev_poly = poly_features.transform(x_dev)
x_test_poly = poly_features.transform(x_test)</pre></div><p class="calibre11">Using the transform function, we transform our train, dev, and test datasets to include polynomial features:</p><p class="calibre11">Let us proceed to build our model:</p><div><pre class="programlisting">    # Build model with polynomial features
    model_poly = build_model(x_train_poly,y_train)</pre></div><p class="calibre11">Inside the build_model function, we instantiate the GradientBoostingRegressor class as follows:</p><div><pre class="programlisting">    model = GradientBoostingRegressor(n_estimators=n_estimators,verbose=10,\
            subsample=0.7, learning_rate= 0.15,max_depth=3,random_state=77)</pre></div><p class="calibre11">Let us look at the parameters. The first parameter is the number of models in the ensemble. The second parameter is verbose—when this is set to a number greater than 1, it prints the progress as every model, trees in this case is built. The next parameter is subsample, which dictates the percentage of training data that will be used by the models. In this case, 0.7 indicates that we will use 70 percent of the training dataset. The next parameter is the learning rate. It's the shrinkage parameter to control the contribution of each tree. Max_depth, the next parameter, determines the size of the tree built. The random_state parameter is the seed to be used by the random number generator. In order to stay consistent during different runs, we set this to an integer value.</p><p class="calibre11">Since we have set our verbose parameter to more than 1, as we fit our model, we see the following results on the screen during each model iteration:</p><div><img src="img/B04041_08_29.jpg" alt="How it works…" class="calibre199"/></div><p class="calibre11">As you <a id="id703" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can see, the training loss reduces with each iteration. The fourth column is the out-of-bag improvement score. With the subsample, we had selected only 70 percent of the dataset; the OOB score is calculated with the rest 30 percent. There is an improvement in loss as compared to the previous model. For example, in iteration 2, we have an improvement of 10.32 when compared with the model built in iteration 1.</p><p class="calibre11">Let us proceed to check performance of the ensemble on the training data:</p><div><pre class="programlisting">    predicted_y = model_poly.predict(x_train_poly)
    print "\n Model Performance in Training set (Polynomial features)\n"
    model_worth(y_train,predicted_y)  </pre></div><div><img src="img/B04041_08_30.jpg" alt="How it works…" class="calibre200"/></div><p class="calibre11">As you can see, our boosting ensemble has fit the training data perfectly.</p><p class="calibre11">The model_worth function prints some more details of the model. They are as follows:</p><div><img src="img/B04041_08_31.jpg" alt="How it works…" class="calibre201"/></div><p class="calibre11">The score <a id="id704" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of each of the different models, which we saw in the verbose output is stored as an attribute in the model object, and is retrieved as follows:</p><div><pre class="programlisting">print "\n Training scores"
print "======================\n"
for i,score in enumerate(model.train_score_):
print "\tEstimator %d score %0.3f"%(i+1,score)</pre></div><p class="calibre11">Let us plot this in a graph:</p><div><pre class="programlisting">plt.cla()
plt.figure(1)
plt.plot(range(1,model.estimators_.shape[0]+1),model.train_score_)
plt.xlabel("Model Sequence")
plt.ylabel("Model Score")
plt.show()</pre></div><p class="calibre11">The <em class="calibre15">x</em> axis represents the model number and the y axis displays the training score. Remember that boosting is a sequential process, and every model is an improvement over the previous model.</p><div><img src="img/B04041_08_32.jpg" alt="How it works…" class="calibre202"/></div><p class="calibre11">As you can <a id="id705" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see in the graph, the mean square error, which is the model score decreases with every successive model.</p><p class="calibre11">Finally, we can also see the importance associated with each feature:</p><div><pre class="programlisting">    print "\n Feature Importance"
    print "======================\n"
    for i,score in enumerate(model.feature_importances_):
        print "\tFeature %d Importance %0.3f"%(i+1,score)</pre></div><p class="calibre11">Let us see how the features are stacked against each other.</p><div><img src="img/B04041_08_33.jpg" alt="How it works…" class="calibre203"/></div><p class="calibre11">Gradient Boosting <a id="id706" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>unifies feature selection and model building into a single operation. It can naturally discover the non-linear relationship between features. Please refer to the following paper on how Gradient boosting can be used for feature selection:</p><div><blockquote class="blockquote"><p class="calibre17"><em class="calibre15">Zhixiang Xu, Gao Huang, Kilian Q. Weinberger, and Alice X. Zheng. 2014. Gradient boosted feature selection. In <em class="calibre15">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</em>(KDD '14). ACM, New York, NY, USA, 522-531. </em></p></blockquote></div><p class="calibre11">Let us apply the dev data to the model and look at its performance:</p><div><pre class="programlisting">    # Apply the model on dev set
    predicted_y = model_poly.predict(x_dev_poly)
    print "\n Model Performance in Dev set  (Polynomial features)\n"
    model_worth(y_dev,predicted_y)  </pre></div><div><img src="img/B04041_08_34.jpg" alt="How it works…" class="calibre204"/></div><p class="calibre11">Finally, we look at the test set performance.</p><div><img src="img/B04041_08_35.jpg" alt="How it works…" class="calibre205"/></div><p class="calibre11">As you <a id="id707" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can see, our ensemble has performed extremely well in our test set as compared to the dev set.</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec290" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">For more<a id="id708" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> information about Gradient Boosting, please refer to the following paper:</p><div><blockquote class="blockquote"><p class="calibre17"><em class="calibre15">Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine. Annals of Statistics, pages 1189–1232.</em></p><p class="calibre17"><em class="calibre15">In this receipt we explained gradient boosting with a squared loss function. However Gradient Boosting should be viewed as a framework and not as a method.  Any differentiable loss function can be used in this framework. Any learning method and a differentiable loss function can be chosen by users and apply it into the Gradient Boosting framework.</em></p><p class="calibre17"><em class="calibre15">Scikit Learn also provides a Gradient Boosting method for classification, called GradientBosstingClassifier.</em></p></blockquote></div><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html</a>
</p></div><div><div><div><div><h2 class="title3"><a id="ch08lvl2sec291" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Understanding Ensemble, Bagging Method</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch08.xhtml" title="Chapter 8. Ensemble Methods">Chapter 8</a>,<em class="calibre15"> Model Selection and Evaluation</em></li><li class="listitem"><em class="calibre15">Understanding Ensemble</em>, <em class="calibre15">Boosting Method AdaBoost</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch08.xhtml" title="Chapter 8. Ensemble Methods">Chapter 8</a>, <em class="calibre15">Model Selection and Evaluation</em></li><li class="listitem"><em class="calibre15">Predicting real valued numbers using regression</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>,<em class="calibre15"> Machine Learning II</em></li><li class="listitem"><em class="calibre15">Variable Selection using LASSO Regression </em>recipe in<em class="calibre15"> </em><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>, <em class="calibre15"> Machine Learning II</em></li><li class="listitem"><em class="calibre15">Using cross validation iterators </em>recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chatper 7</a>, <em class="calibre15"> Machine Learning II</em> <em class="calibre15"> </em></li></ul></div></div></div></div>



  </body></html>