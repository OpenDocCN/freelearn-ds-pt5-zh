<html><head></head><body><div><div><h1 id="_idParaDest-13"><em class="italics"><a id="_idTextAnchor014"/>Chapter 1</em></h1>
		</div>
		<div><h1 id="_idParaDest-14"><a id="_idTextAnchor015"/>Introduction to Data Science and Data Pre-Processing</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Use various Python machine learning libraries</li>
				<li class="bullets">Handle missing data and deal with outliers</li>
				<li class="bullets">Perform data integration to bring together data from different sources</li>
				<li class="bullets">Perform data transformation to convert data into a machine-readable form</li>
				<li class="bullets">Scale data to avoid problems with values of different magnitudes</li>
				<li class="bullets">Split data into train and test datasets</li>
				<li class="bullets">Describe the different types of machine learning</li>
				<li class="bullets">Describe the different performance measures of a machine learning model</li>
			</ul>
			<p>This chapter introduces data science and covers the various processes included in the building of machine learning models, with a particular focus on pre-processing.</p>
		</div>
		<div><h2 id="_idParaDest-15"><a id="_idTextAnchor016"/>Introduction</h2>
			<p>We live in a world where we are constantly surrounded by data. As such, being able to understand and process data is an absolute necessity. </p>
			<p>Data Science is a field that deals with the description, analysis, and prediction of data. Consider an example from our daily lives: every day, we utilize multiple social media applications on our phones. These applications gather and process data in order to create a more personalized experience for each user – for example, showing us news articles that we may be interested in, or tailoring search results according to our location. This branch of data science is known as <strong class="bold">machine learning</strong>.</p>
			<p>Machine learning is the methodical learning of procedures and statistical representations that computers use to accomplish tasks without human intervention. In other words, it is the process of teaching a computer to perform tasks by itself without explicit instructions, relying only on patterns and inferences. Some common uses of machine learning algorithms are in email filtering, computer vision, and computational linguistics. </p>
			<p>This book will focus on machine learning and other aspects of data science using Python. Python is a popular language for data science, as it is versatile and relatively easy to use. It also has several ready-made libraries that are well equipped for processing data.</p>
			<h2 id="_idParaDest-16"><a id="_idTextAnchor017"/>Python Libraries</h2>
			<p>Throughout this book, we'll be using various Python libraries, including pandas, Matplotlib, Seaborn, and scikit-learn.</p>
			<p><strong class="bold">pandas</strong></p>
			<p>pandas is an open source package that has many functions for loading and processing data in order to prepare it for machine learning tasks. It also has tools that can be used to analyze and manipulate data. Data can be read from many formats using pandas. We will mainly be using CSV data throughout this book. To read CSV data, you can use the <code>read_csv()</code> function by passing <code>filename.csv</code> as an argument. An example of this is shown here:</p>
			<pre>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; pd.read_csv("data.csv")</pre>
			<p>In the preceding code, <code>pd</code> is an alias name given to pandas. It is not mandatory to give an alias. To visualize a pandas DataFrame, you can use the <code>head()</code> function to list the top five rows. This will be demonstrated in one of the following exercises.</p>
			<h4>Note</h4>
			<p class="callout">Please visit the following link to learn more about pandas: <a href="">https://pandas.pydata.org/pandas-docs/stable/</a>.</p>
			<p><strong class="bold">NumPy</strong></p>
			<p>NumPy is one of the main packages that Python has to offer. It is mainly used in practices related to scientific computing and when working on mathematical operations. It comprises of tools that enable us to work with arrays and array objects.</p>
			<p><strong class="bold">Matplotlib</strong></p>
			<p>Matplotlib is a data visualization package. It is useful for plotting data points in a 2D space with the help of NumPy. </p>
			<p><strong class="bold">Seaborn</strong></p>
			<p>Seaborn is also a data visualization library that is based on matplotlib. Visualizations created using Seaborn are far more attractive than ones created using matplotlib in terms of graphics.</p>
			<p><strong class="bold">scikit-learn</strong></p>
			<p>scikit-learn is a Python package used for machine learning. It is designed in such a way that it interoperates with other numeric and scientific libraries in Python to achieve the implementation of algorithms.</p>
			<p>These ready-to-use libraries have gained interest and attention from developers, especially in the data science space. Now that we have covered the various libraries in Python, in the next section we'll explore the roadmap for building machine learning models.</p>
			<h2 id="_idParaDest-17"><a id="_idTextAnchor018"/>Roadmap for Building Machine Learning Models</h2>
			<p>The roadmap for building machine learning models is straightforward and consists of five major steps, which are explained here:</p>
			<ul>
				<li><strong class="bold">Data Pre-processing</strong><p>This is the first step in building a machine learning model. Data pre-processing refers to the transformation of data before feeding it into the model. It deals with the techniques that are used to convert unusable raw data into clean reliable data.  </p><p>Since data collection is often not performed in a controlled manner, raw data often contains outliers (for example, age = 120), nonsensical data combinations (for example, model: bicycle, type: 4-wheeler), missing values, scale problems, and so on. Because of this, raw data cannot be fed into a machine learning model because it might compromise the quality of the results. As such, this is the most important step in the process of data science. </p></li>
				<li><strong class="bold">Model Learning</strong><p>After pre-processing the data and splitting it into train/test sets (more on this later), we move on to modeling. Models are nothing but sets of well-defined methods called algorithms that use pre-processed data to learn patterns, which can later be used to make predictions. There are different types of learning algorithms, including supervised, semi-supervised, unsupervised, and reinforcement learning. These will be discussed later.</p></li>
				<li><strong class="bold">Model Evaluation</strong><p>In this stage, the models are evaluated with the help of specific performance metrics. With these metrics, we can go on to tune the hyperparameters of a model in order to improve it. This process is called <strong class="bold">hyperparameter optimization</strong>. We will repeat this step until we are satisfied with the performance.</p></li>
				<li><strong class="bold">Prediction</strong><p>Once we are happy with the results from the evaluation step, we will then move on to predictions. Predictions are made by the trained model when it is exposed to a new dataset. In a business setting, these predictions can be shared with decision makers to make effective business choices.</p></li>
				<li><strong class="bold">Model Deployment </strong><p>The whole process of machine learning does not just stop with model building and prediction. It also involves making use of the model to build an application with the new data. Depending on the business requirements, the deployment may be a report, or it may be some repetitive data science steps that are to be executed. After deployment, a model needs proper management and maintenance at regular intervals to keep it up and running.</p></li>
			</ul>
			<p>This chapter will mainly focus on pre-processing. We will cover the different tasks involved in data pre-processing, such as data representation, data cleaning, and others.</p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor019"/>Data Representation</h2>
			<p>The main objective of machine learning is to build models that understand data and find underlying patterns. In order to do so, it is very important to feed the data in a way that is interpretable by the computer. To feed the data into a model, it must be represented as a table or a matrix of the required dimensions. Converting your data into the correct tabular form is one of the first steps before pre-processing can properly begin.</p>
			<p><strong class="bold">Data Represented in a Table</strong></p>
			<p>Data should be arranged in a two-dimensional space made up of rows and columns. This type of data structure makes it easy to understand the data and pinpoint any problems. An example of some raw data stored as a CSV (<strong class="bold">comma separated values</strong>) file is shown here:</p>
			<div><div><img src="img/C13322_01_01.jpg" alt="Figure 1.1: Raw data in CSV format" width="653" height="52"/>
				</div>
			</div>
			<h6>Figure 1.1: Raw data in CSV format</h6>
			<p>The representation of the same data in a table is as follows:</p>
			<div><div><img src="img/C13322_01_02.jpg" alt="Figure 1.2: CSV data in table format" width="655" height="133"/>
				</div>
			</div>
			<h6>Figure 1.2: CSV data in table format</h6>
			<p>If you compare the data in CSV and table formats, you will see that there are missing values in both. We will cover what to do with these later in the chapter. To load a CSV file and work on it as a table, we use the pandas library. The data here is loaded into tables called DataFrames.</p>
			<h4>Note</h4>
			<p class="callout">To learn more about pandas, visit the following link: <a href="">http://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html</a>.</p>
			<h3 id="_idParaDest-19"><a id="_idTextAnchor020"/>Independent and Target Variables</h3>
			<p>The DataFrame that we use contains variables or features that can be classified into two categories. These are independent variables (also called <strong class="keyword">predictor variables</strong>) and dependent variables (also called <strong class="keyword">target variables</strong>). Independent variables are used to predict the target variable. As the name suggests, independent variables should be independent of each other. If they are not, this will need to be addressed in the pre-processing (cleaning) stage.</p>
			<p><strong class="bold">Independent Variables</strong></p>
			<p>These are all the features in the DataFrame except the <strong class="bold">target variable</strong>. They are of size (m, n), where m is the number of observations and n is the number of features. These variables must be normally distributed and should NOT contain:</p>
			<ul>
				<li>Missing or NULL values</li>
				<li>Highly categorical data features or high cardinality (these terms will be covered in more detail later)</li>
				<li>Outliers</li>
				<li>Data on different scales</li>
				<li>Human error</li>
				<li>Multicollinearity (independent variables that are correlated)</li>
				<li>Very large independent feature sets (too many independent variables to be manageable)</li>
				<li>Sparse data</li>
				<li>Special characters</li>
			</ul>
			<p><strong class="bold">Feature Matrix and Target Vector</strong></p>
			<p>A single piece of data is called a scalar. A group of scalars is called a vector, and a group of vectors is called a matrix. A matrix is represented in rows and columns. Feature matrix data is made up of independent columns, and the target vector depends on the feature matrix columns. To get a better understanding of this, let's look at the following table:</p>
			<div><div><img src="img/C13322_01_03.jpg" alt="Figure 1.3: Table containing car details" width="629" height="113"/>
				</div>
			</div>
			<h6>Figure 1.3: Table containing car details</h6>
			<p>As you can see in the table, there are various columns: Car Model, Car Capacity, Car Brand, and Car Price. All columns except Car Price are independent variables and represent the feature matrix. Car Price is the dependent variable that depends on the other columns (Car Model, Car Capacity, and Car Brand). It is a target vector because it depends on the feature matrix data. In the next section, we'll go through an exercise based on features and a target matrix to get a thorough understanding.</p>
			<h4>Note </h4>
			<p class="callout">All exercises and activities will be primarily developed in Jupyter Notebook. It is recommended to keep a separate notebook for different assignments unless advised not to. Also, to load a sample dataset, the pandas library will be used, because it displays the data as a table. Other ways to load data will be explained in further sections.</p>
			<h3 id="_idParaDest-20"><a id="_idTextAnchor021"/>Exercise 1: Loading a Sample Dataset and Creating the Feature Matrix and Target Matrix</h3>
			<p>In this exercise, we will be loading the <code>House_price_prediction</code> dataset into the pandas DataFrame and creating feature and target matrices. The <code>House_price_prediction</code> dataset is taken from the UCI Machine Learning Repository. The data was collected from various suburbs of the USA and consists of 5,000 entries and 6 features related to houses. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>House_price_prediction</code> dataset can be found at this location: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/USA_Housing.csv</a>.</p>
			<ol>
				<li>Open a Jupyter notebook and add the following code to import pandas:<pre>import pandas as pd </pre></li>
				<li>Now we need to load the dataset into a pandas DataFrame. As the dataset is a CSV file, we'll be using the <code>read_csv()</code> function to read the data. Add the following code to do this:<pre>dataset = "https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/USA_Housing.csv"
df = pd.read_csv(dataset, header = 0)</pre><p>As you can see in the preceding code, the data is stored in a variable named <code>df</code>.</p></li>
				<li>To print all the column names of the DataFrame, we'll use the <code>df.columns</code> command. Write the following code in the notebook:<pre>df.columns</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_04.jpg" alt="Figure 1.4: List of columns present in the dataframe" width="1668" height="236"/></div><h6>Figure 1.4: List of columns present in the dataframe</h6></li>
				<li>The dataset contains n number of data points. We can find the total number of rows using the following command:<pre>df.index</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_05.jpg" alt="Figure 1.5: Total Index in the dataframe" width="1186" height="133"/></div><h6>Figure 1.5: Total Index in the dataframe</h6><p>As you can see in the preceding figure, our dataset contains 5000 rows, from index 0 to 5000.</p><h4> Note</h4><p class="callout">You can use the <code>set_index()</code> function in pandas to convert a column into an index of rows in a DataFrame. This is a bit like using the values in that column as your row labels.</p><p class="callout"><code>Dataframe.set_index('column name', inplace = True')'</code></p></li>
				<li>Let's set the <code>Address</code> column as an index and reset it back to the original DataFrame. The pandas library provides the <code>set_index()</code> method to convert a column into an index of rows in a DataFrame. Add the following code to implement this:<pre>df.set_index('Address', inplace=True)
df</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_06.jpg" alt="Figure 1.6: DataFrame with an indexed Address column" width="616" height="209"/></div><h6>Figure 1.6: DataFrame with an indexed Address column</h6><p>The <code>inplace</code> parameter in the <code>set_index()</code> function is by default set to <code>False</code>. If the value is changed to <code>True</code>, then whatever operation we perform the content of the DataFrame changes directly without the copy being created.</p></li>
				<li>In order to reset the index of the given object, we use the <code>reset_index()</code> function. Write the following code to implement this:<pre>df.reset_index(inplace=True)
df</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_07.jpg" alt="Figure 1.7: DataFrame with the index reset" width="617" height="209"/></div><h6>Figure 1.7: DataFrame with the index reset</h6><h4>Note</h4><p class="callout">The index is like a name given to a row and column. Rows and columns both have an index. You can index by row/column number or row/column name.</p></li>
				<li>We can retrieve the first four rows and the first three columns using a row number and column number. This can be done using the <code>iloc</code> indexer in pandas, which retrieves data using index positions. Add the following code to do this:<pre>df.iloc[0:4 , 0:3]</pre><div><img src="img/C13322_01_08.jpg" alt="Figure 1.8: Dataset of four rows and three columns" width="1392" height="461"/></div><h6>Figure 1.8: Dataset of four rows and three columns</h6></li>
				<li>To retrieve the data using labels, we use the <code>loc</code> indexer. Add the following code to retrieve the first five rows of the Income and Age columns:<pre>df.loc[0:4 , ["Avg. Area Income", "Avg. Area House Age"]]</pre><div><img src="img/C13322_01_09.jpg" alt="Figure 1.9: Dataset of five rows and two columns" width="1383" height="379"/></div><h6>Figure 1.9: Dataset of five rows and two columns</h6></li>
				<li>Now create a variable called <code>X</code> to store the independent features. In our dataset, we will consider all features except Price as independent variables, and we will use the <code>drop()</code> function to include them. Once this is done, we print out the top five instances of the <code>X</code> variable. Add the following code to do this: <pre>X = df.drop('Price', axis=1)
X.head()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_10.jpg" alt="Figure 1.10: Dataset showing the first five rows of the feature matrix" width="580" height="147"/></div><h6>Figure 1.10: Dataset showing the first five rows of the feature matrix</h6><h4>Note</h4><p class="callout">The default number of instances that will be taken for the head is five, so if you don't specify the number then it will by default output five observations. The axis parameter in the preceding screenshot denotes whether you want to drop the label from rows (axis = 0) or columns (axis = 1).</p></li>
				<li>Print the shape of your newly created feature matrix using the <code>X.shape</code> command. Add the following code to do this:<pre>X.shape</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_11.jpg" alt="" width="571" height="27"/></div><h6>Figure 1.11: Shape of the feature matrix</h6><p>In the preceding figure, the first value indicates the number of observations in the dataset (<strong class="bold">5000</strong>), and the second value represents the number of features (<strong class="bold">6</strong>).</p></li>
				<li>Similarly, we will create a variable called <code>y</code> that will store the target values. We will use indexing to grab the target column. Indexing allows you to access a section of a larger element. In this case, we want to grab the column named Price from the <code>df</code> DataFrame. Then, we want to print out the top 10 values of the variable. Add the following code to implement this:<pre>y = df['Price']
y.head(10)</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_12.jpg" alt="Figure 1.12: Dataset showing the first 10 rows of the target matrix" width="503" height="195"/></div><h6>Figure 1.12: Dataset showing the first 10 rows of the target matrix</h6></li>
				<li>Print the shape of your new variable using the <code>y.shape</code> command. The shape should be one-dimensional, with a length equal to the number of observations (<strong class="bold">5000</strong>) only. Add the following code to implement this:<pre>y.shape</pre><p>The preceding code generates the following output:</p></li>
			</ol>
			<div><div><img src="img/C13322_01_13.jpg" alt="Figure 1.13: Shape of the target matrix" width="592" height="28"/>
				</div>
			</div>
			<h6>Figure 1.13: Shape of the target matrix</h6>
			<p>You have successfully created the feature and target matrices of a dataset. You have completed the first step in the process of building a predictive model. This model will learn the patterns from the feature matrix (columns in <code>X</code>) and how they map to the values in the target vector (<code>y</code>). These patterns can then be used to predict house prices from new data based on the features of those new houses.</p>
			<p>In the next section, we will explore more steps involved in pre-processing.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor022"/>Data Cleaning </h2>
			<p>Data cleaning includes processes such as filling in missing values and handling inconsistencies. It detects corrupt data and replaces or modifies it. </p>
			<p><strong class="bold">Missing Values</strong></p>
			<p>The concept of missing values is important to understand if you want to master the skill of successful management and understanding of data. Let's take a look at the following figure:</p>
			<div><div><img src="img/C13322_01_14.jpg" alt="Figure 1.14: Bank customer credit data" width="841" height="331"/>
				</div>
			</div>
			<h6>Figure 1.14: Bank customer credit data</h6>
			<p>As you can see, the data belongs to a bank; each row is a separate customer and each column contains their details, such as age and credit amount. There are some cells that have either <strong class="bold">NA</strong> or are just empty. This is missing data. Each piece of information about the customer is crucial for the bank. If any of the information is missing, then it will be difficult for the bank to predict the risk of providing a loan to the customer. </p>
			<p><strong class="bold">Handling Missing Data</strong></p>
			<p>Intelligent handling of missing data will result in building a robust model capable of handling complex tasks. There are many ways to handle missing data. Let's now look at some of those ways.</p>
			<p><strong class="bold">Removing the Data</strong></p>
			<p>Checking missing values is the first and the most important step in data pre-processing. A model cannot accept data with missing values. This is a very simple and commonly used method to handle missing values: we delete a row if the missing value corresponds to the places in the row, or we delete a column if it has more than 70%-75% of missing data. Again, the threshold value is not fixed and depends on how much you wish to fix.</p>
			<p>The benefit of this approach is that it is quick and easy to do, and in many cases no data is better than bad data. The drawback is that you may end up losing important information, because you're deleting a whole feature based on a few missing values.</p>
			<h3 id="_idParaDest-22"><a id="_idTextAnchor023"/>Exercise 2: Removing Missing Data</h3>
			<p>In this exercise, we will be loading the <code>Banking_Marketing.csv</code> dataset into the pandas DataFrame and handling the missing data. This dataset is related to direct marketing campaigns of a Portuguese banking institution. The marketing campaigns involved phone calls to clients to try and get them to subscribe to a particular product. The dataset contains the details of each client contacted, and whether they subscribed to the product. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>Banking_Marketing.csv</code> dataset can be found at this location: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv</a>.</p>
			<ol>
				<li value="1">Open a Jupyter notebook. Insert a new cell and add the following code to import pandas and fetch the <code>Banking_Marketing.csv</code> dataset: <pre>import pandas as pd
dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv'
#reading the data into the dataframe into the object data
df = pd.read_csv(dataset, header=0)</pre></li>
				<li>Once you have fetched the dataset, print the datatype of each column. To do so, use the <code>dtypes</code> attribute from the pandas DataFrame:<pre>df.dtypes</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_15.jpg" alt="Figure 1.15: Data types of each feature" width="1500" height="946"/></div><h6>Figure 1.15: Data types of each feature</h6></li>
				<li>Now we need to find the missing values for each column. In order to do that, we use the <code>isna()</code> function provided by pandas:<pre>df.isna().sum()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_16.jpg" alt="Figure 1.16: Missing values of each column in the dataset" width="1514" height="930"/></div><h6>Figure 1.16: Missing values of each column in the dataset</h6><p>In the preceding figure, we can see that there is data missing from three columns, namely <code>age</code>, <code>contact</code>, and <code>duration</code>. There are two NAs in the <strong class="bold">age</strong> column, six NAs in <strong class="bold">contact</strong>, and seven NAs in <strong class="bold">duration</strong>.</p></li>
				<li>Once you have figured out all the missing details, we remove all the missing rows from the DataFrame. To do so, we use the <code>dropna()</code> function:<pre>#removing Null values
data = data.dropna()</pre></li>
				<li>To check whether the missing vales are still present, use the <code>isna()</code> function:<pre>df.isna().sum()</pre><p>The preceding code generates the following output:</p></li>
			</ol>
			<div><div><img src="img/C13322_01_17.jpg" alt="Figure 1.17: Each column of the dataset with zero missing values" width="1139" height="626"/>
				</div>
			</div>
			<h6>Figure 1.17: Each column of the dataset with zero missing values</h6>
			<p>You have successfully removed all missing data from the DataFrame. In the next section, we'll look at the second method of dealing with missing data, which uses imputation.</p>
			<p><strong class="bold">Mean/Median/Mode Imputation</strong></p>
			<p>In the case of numerical data, we can compute its mean or median and use the result to replace missing values. In the case of the categorical (non-numerical) data, we can compute its mode to replace the missing value. This is known as imputation.</p>
			<p>The benefit of using imputation, rather than just removing data, is that it prevents data loss. The drawback is that you don't know how accurate using the mean, median, or mode is going to be in a given situation.</p>
			<p>Let's look at an exercise in which we will use imputation method to solve missing data problems.</p>
			<h3 id="_idParaDest-23"><a id="_idTextAnchor024"/>Exercise 3: Imputing Missing Data</h3>
			<p>In this exercise, we will be loading the <code>Banking_Marketing.csv</code> dataset into the pandas DataFrame and handle the missing data. We'll make use of the imputation method. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>Banking_Marketing.csv</code> dataset can be found at this location: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv</a>.</p>
			<ol>
				<li value="1">Open a Jupyter notebook and add a new cell. Load the dataset into the pandas DataFrame. Add the following code to do this:<pre>import pandas as pd
dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv'
df = pd.read_csv(dataset, header=0)</pre></li>
				<li>Impute the numerical data of the <code>age</code> column with its mean. To do so, first find the mean of the <code>age</code> column using the <code>mean()</code> function of pandas, and then print it:<pre>mean_age = df.age.mean()
print(mean_age)</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_18.jpg" alt="Figure 1.18: Mean of the age column" width="635" height="30"/></div><h6>Figure 1.18: Mean of the age column</h6></li>
				<li>Once this is done, impute the missing data with its mean using the <code>fillna()</code> function. This can be done with the following code:<pre>df.age.fillna(mean_age, inplace=True)</pre></li>
				<li>Now we impute the numerical data of the duration column with its median. To do so, first find the median of the duration column using the <code>median()</code> function of the pandas. Add the following code to do so:<pre>median_duration = df.duration.median()
print(median_duration)</pre><div><img src="img/C13322_01_19.jpg" alt="Figure 1.19: Median of the duration" width="360" height="17"/></div><h6>Figure 1.19: Median of the duration</h6></li>
				<li>Impute the missing data of the duration with its median using the <code>fillna()</code> function. <pre>df. duration.fillna(median_duration,inplace=True)</pre></li>
				<li>Impute the categorical data of the contact column with its mode. To do so, first, find the mode of the contact column using the <code>mode()</code> function of pandas. Add the following code to do this:<pre>mode_contact = df.contact.mode()[0]
print(mode_contact)</pre><div><img src="img/C13322_01_20.jpg" alt="" width="402" height="19"/></div><h6>Figure 1.20: Mode of the contact</h6></li>
				<li>Impute the missing data of the contact column with its mode using the <code>fillna()</code> function. Add the following code to do this:<pre>df.contact.fillna(mode_contact,inplace=True)</pre><p>Unlike mean and median, there may be more than one mode in a column. So, we just take the first mode with index 0.</p></li>
			</ol>
			<p>You have successfully imputed the missing data in different ways and made the data complete and clean. </p>
			<p>Another part of data cleaning is dealing with outliers, which will be discussed in the next section.</p>
			<p><strong class="bold">Outliers</strong></p>
			<p>Outliers are values that are very large or very small with respect to the distribution of the other data. We can only find outliers in numerical data. Box plots are one good way to find the outliers in a dataset, as you can see in the following figure:</p>
			<div><div><img src="img/C13322_01.21.jpg" alt="Figure 1.21: Sample of outliers in a box plot" width="1733" height="888"/>
				</div>
			</div>
			<h6>Figure 1.21: Sample of outliers in a box plot</h6>
			<h4>Note</h4>
			<p class="callout">An outlier is not always bad data! With the help of business understanding and client interaction, you can discern whether to remove or retain the outlier.</p>
			<p>Let's learn how to find outliers using a simple example. Consider a sample dataset of temperatures from a place at different times:</p>
			<pre>71, 70, 90, 70, 70, 60, 70, 72, 72, 320, 71, 69</pre>
			<p>We can now do the following:</p>
			<ol>
				<li value="1">First, we'll sort the data:<pre>60,69, 70, 70, 70, 70, 71, 71, 72, 72, 90, 320</pre></li>
				<li>Next, we'll calculate the median (Q2). The median is the middle data after sorting.<p>Here, the middle terms are 70 and 71 after sorting the list.</p><p>The median is <em class="italics">(70 + 71) / 2 = 70.5</em></p></li>
				<li>Then we'll calculate the lower quartile (Q1). Q1 is the middle value (median) of the first half of the dataset. <p>First half of the data = <code>60, 69, 70, 70, 70, 70</code></p><p>Points 3 and 4 of the bottom 6 are both equal to 70.</p><p>The average is <em class="italics">(70 + 70) / 2 = 70</em> </p><p>Q1 = 70</p></li>
				<li>Then we calculate the upper quartile (Q3).<p>Q3 is the middle value (median) of the second half of the dataset. </p><p>Second half of the data = <code>71, 71, 72, 72, 90, 320</code></p><p>Points 3 and 4 of the upper 6 are 72 and 72.</p><p>The average is <em class="italics">(72 + 72) / 2 = 72</em> </p><p>Q3 = 72</p></li>
				<li>Then we find the interquartile range (IQR).<p>IQR = Q3 – Q1 = 72 – 70 </p><p>IQR = 2</p></li>
				<li>Next, we find the upper and lower fences.<p>Lower fence = Q1 – 1.5 (IQR) = 70 – 1.5(2) = 67</p><p>Upper fence = Q3 + 1.5 (IQR) = 71.5 + 1.5(2) = 74.5</p><p>Boundaries of our fences = 67 and 74.5</p></li>
			</ol>
			<p>Any data points lower than the lower fence and greater than the upper fence are outliers. Thus, the outliers from our example are 60, 90 and 320.</p>
			<h3 id="_idParaDest-24"><a id="_idTextAnchor025"/>Exercise 4: Finding and Removing Outliers in Data</h3>
			<p>In this exercise, we will be loading the <code>german_credit_data.csv</code> dataset into the pandas DataFrame and removing the outliers. The dataset contains 1,000 entries with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset, each entry represents a person who takes credit from a bank. Each person is classified as a good or bad credit risk according to the set of attributes. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The link to the <code>german_credit_data.csv</code> dataset can be found here:  <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/german_credit_data.csv</a>.</p>
			<ol>
				<li value="1">Open a Jupyter notebook and add a new cell. Write the following code to import the necessary libraries: pandas, NumPy, matplotlib, and seaborn. Fetch the dataset and load it into the pandas DataFrame. Add the following code to do this:<pre>import pandas as pd
import numpy as np
%matplotlib inline  
import seaborn as sbn
dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/german_credit_data.csv'
#reading the data into the dataframe into the object data
df = pd.read_csv(dataset, header=0)</pre><p>In the preceding code, <code>%matplotlib inline</code> is a magic function that is essential if we want the plot to be visible in the notebook.</p></li>
				<li>This dataset contains an <code>Age</code> column. Let's plot a boxplot of the <code>Age</code> column. To do so, use the <code>boxplot()</code> function from the seaborn library:<pre>sbn.boxplot(df['Age'])</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_22.jpg" alt="Figure 1.22: A box plot of the Age column" width="627" height="253"/></div><h6>Figure 1.22: A box plot of the Age column</h6><p>We can see that some data points are outliers in the boxplot.</p></li>
				<li>The boxplot uses the IQR method to display the data and the outliers (the shape of the data). But in order to print an outlier, we use a mathematical formula to retrieve it. Add the following code to find the outliers of the <code>Age</code> column using the IQR method:<pre>Q1 = df["Age"].quantile(0.25)
Q3 = df["Age"].quantile(0.75)
IQR = Q3 - Q1
print(IQR)
&gt;&gt;&gt; 15.0</pre><p>In the preceding code, Q1 is the first quartile and Q3 is the third quartile.</p></li>
				<li>Now we find the upper fence and lower fence by adding the following code, and print all the data above the upper fence and below the lower fence. Add the following code to do this: <pre>Lower_Fence = Q1 - (1.5 * IQR)
Upper_Fence = Q3 + (1.5 * IQR)
print(Lower_Fence)
print(Upper_Fence)
&gt;&gt;&gt; 4.5
&gt;&gt;&gt; 64.5</pre></li>
				<li>To print all the data above the upper fence and below the lower fence, add the following code:<pre>df[((df["Age"] &lt; Lower_Fence) |(df["Age"] &gt; Upper_Fence))]</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_23.jpg" alt="Figure 1.23: Outlier data based on the Age column" width="1398" height="433"/></div><h6>Figure 1.23: Outlier data based on the Age column</h6></li>
				<li>Filter out the outlier data and print only the potential data. To do so, just negate the preceding result using the <code>~</code> operator:<pre>df = df[~((df ["Age"] &lt; Lower_Fence) |(df["Age"] &gt; Upper_Fence))]
df</pre><p>The preceding code generates the following output:</p></li>
			</ol>
			<div><div><img src="img/C13322_01_24.jpg" alt="Figure 1.24: Potential data based on the Age column" width="826" height="316"/>
				</div>
			</div>
			<h6>Figure 1.24: Potential data based on the Age column</h6>
			<p>You have successfully found the outliers using the IQR. In the next section, we will explore another method of pre-processing called data integration.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor026"/>Data Integration</h2>
			<p>So far, we've made sure to remove the impurities in data and make it clean. Now, the next step is to combine data from different sources to get a unified structure with more meaningful and valuable information. This is mostly used if the data is segregated into different sources. To make it simple, let's assume we have data in CSV format in different places, all talking about the same scenario. Say we have some data about an employee in a database. We can't expect all the data about the employee to reside in the same table. It's possible that the employee's personal data will be located in one table, the employee's project history will be in a second table, the employee's time-in and time-out details will be in another table, and so on. So, if we want to do some analysis about the employee, we need to get all the employee data in one common place. This process of bringing data together in one place is called data integration. To do data integration, we can merge multiple pandas DataFrames using the <code>merge</code> function.</p>
			<p>Let's solve an exercise based on data integration to get a clear understanding of it.</p>
			<h3 id="_idParaDest-26"><a id="_idTextAnchor027"/>Exercise 5: Integrating Data</h3>
			<p>In this exercise, we'll merge the details of students from two datasets, namely <code>student.csv</code> and <code>marks.csv</code>. The <code>student</code> dataset contains columns such as <code>Age</code>, <code>Gender</code>, <code>Grade</code>, and <code>Employed</code>. The <code>marks.csv</code> dataset contains columns such as <code>Mark</code> and <code>City</code>. The <code>Student_id</code> column is common between the two datasets. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>student.csv</code> dataset can be found at this location: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/student.csv</a>.</p>
			<p class="callout">The <code>marks.csv</code> dataset can be found at this location: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/mark.csv</a>.</p>
			<ol>
				<li value="1">Open a Jupyter notebook and add a new cell. Write the following code to import pandas and load the <code>student.csv</code> and <code>marks.csv</code> datasets into the <code>df1</code> and <code>df2</code> pandas DataFrames:<pre>import pandas as pd
dataset1 = "https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/student.csv"
dataset2 = "https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/mark.csv"
df1 = pd.read_csv(dataset1, header = 0)
df2 = pd.read_csv(dataset2, header = 0)</pre></li>
				<li>To print the first five rows of the first DataFrame, add the following code:<pre>df1.head()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_25.jpg" alt="Figure 1.25: The first five rows of the first DataFrame" width="619" height="220"/></div><h6>Figure 1.25: The first five rows of the first DataFrame</h6></li>
				<li>To print the first five rows of the second DataFrame, add the following code:<pre>df2.head()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_26.jpg" alt="Figure 1.26: The first five rows of the second DataFrame" width="661" height="224"/></div><h6>Figure 1.26: The first five rows of the second DataFrame</h6></li>
				<li><code>Student_id</code> is common to both datasets. Perform data integration on both the DataFrames with respect to the <code>Student_id</code> column using the <code>pd.merge()</code> function, and then print the first 10 values of the new DataFrame:<pre>df = pd.merge(df1, df2, on = 'Student_id')
df.head(10)</pre></li>
			</ol>
			<div><div><img src="img/C13322_01_27.jpg" alt="Figure 1.27: First 10 rows of the merged DataFrame" width="685" height="421"/>
				</div>
			</div>
			<h6>Figure 1.27: First 10 rows of the merged DataFrame</h6>
			<p>Here, the data of the <code>df1</code> DataFrame is merged with the data of the <code>df2</code> DataFrame. The merged data is stored inside a new DataFrame called <code>df</code>.</p>
			<p>We have now learned how to perform data integration. In the next section, we'll explore another pre-processing task, data transformation.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor028"/>Data Transformation</h2>
			<p>Previously, we saw how we can combine data from different sources into a unified dataframe. Now, we have a lot of columns that have different types of data. Our goal is to transform the data into a machine-learning-digestible format. All machine learning algorithms are based on mathematics. So, we need to convert all the columns into numerical format. Before that, let's see all the different types of data we have.</p>
			<p>Taking a broader perspective, data is classified into numerical and categorical data:</p>
			<ul>
				<li><strong class="bold">Numerical</strong>: As the name suggests, this is numeric data that is quantifiable.</li>
				<li><strong class="bold">Categorical</strong>: The data is a string or non-numeric data that is qualitative in nature.</li>
			</ul>
			<p>Numerical data is further divided into the following:</p>
			<ul>
				<li><strong class="bold">Discrete</strong>: To explain in simple terms, any numerical data that is countable is called discrete, for example, the number of people in a family or the number of students in a class. Discrete data can only take certain values (such as 1, 2, 3, 4, etc).</li>
				<li><strong class="bold">Continuous</strong>: Any numerical data that is measurable is called continuous, for example, the height of a person or the time taken to reach a destination. Continuous data can take virtually any value (for example, 1.25, 3.8888, and 77.1276).</li>
			</ul>
			<p>Categorical data is further divided into the following:</p>
			<ul>
				<li><strong class="bold">Ordered</strong>: Any categorical data that has some order associated with it is called ordered categorical data, for example, movie ratings (excellent, good, bad, worst) and feedback (happy, not bad, bad). You can think of ordered data as being something you could mark on a scale.</li>
				<li><strong class="bold">Nominal</strong>: Any categorical data that has no order is called nominal categorical data. Examples include gender and country.</li>
			</ul>
			<p>From these different types of data, we will focus on categorical data. In the next section, we'll discuss how to handle categorical data.</p>
			<h3 id="_idParaDest-28"><a id="_idTextAnchor029"/>Handling Categorical Data</h3>
			<p>There are some algorithms that can work well with categorical data, such as decision trees. But most machine learning algorithms cannot operate directly with categorical data. These algorithms require the input and output both to be in numerical form. If the output to be predicted is categorical, then after prediction we convert them back to categorical data from numerical data. Let's discuss some key challenges that we face while dealing with categorical data:</p>
			<ul>
				<li><strong class="bold">High cardinality</strong>: Cardinality means uniqueness in data. The data column, in this case, will have a lot of different values. A good example is User ID – in a table of 500 different users, the User ID column would have 500 unique values.</li>
				<li><strong class="bold">Rare occurrences</strong>: These data columns might have variables that occur very rarely and therefore would not be significant enough to have an impact on the model.</li>
				<li><strong class="bold">Frequent occurrences</strong>: There might be a category in the data columns that occurs many times with very low variance, which would fail to make an impact on the model.</li>
				<li><strong class="bold">Won't fit</strong>: This categorical data, left unprocessed, won't fit our model.</li>
			</ul>
			<p><strong class="bold">Encoding</strong></p>
			<p>To address the problems associated with categorical data, we can use encoding. This is the process by which we convert a categorical variable into a numerical form. Here, we will look at three simple methods of encoding categorical data.</p>
			<p><strong class="bold">Replacing</strong></p>
			<p>This is a technique in which we replace the categorical data with a number. This is a simple replacement and does not involve much logical processing. Let's look at an exercise to get a better idea of this.</p>
			<h3 id="_idParaDest-29"><a id="_idTextAnchor030"/>Exercise 6: Simple Replacement of Categorical Data with a Number</h3>
			<p>In this exercise, we will use the <code>student</code> dataset that we saw earlier. We will load the data into a pandas dataframe and simply replace all the categorical data with numbers. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>student</code> dataset can be found at this location: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/student.csv</a>. </p>
			<ol>
				<li value="1">Open a Jupyter notebook and add a new cell. Write the following code to import pandas and then load the dataset into the pandas dataframe:<pre>import pandas as pd
import numpy as np
dataset = "https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/student.csv"
df = pd.read_csv(dataset, header = 0)</pre></li>
				<li>Find the categorical column and separate it out with a different dataframe. To do so, use the <code>select_dtypes()</code> function from pandas:<pre>df_categorical = df.select_dtypes(exclude=[np.number])
df_categorical</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_28.jpg" alt="Figure 1.28: Categorical columns of the dataframe" width="463" height="127"/></div><h6>Figure 1.28: Categorical columns of the dataframe</h6></li>
				<li>Find the distinct unique values in the <code>Grade</code> column. To do so, use the <code>unique()</code> function from pandas with the column name:<pre>df_categorical['Grade'].unique()</pre><p>The preceding code generates the following output: </p><div><img src="img/C13322_01_29.jpg" alt="Figure 1.29: Unique values in the Grade column" width="1565" height="74"/></div><h6>Figure 1.29: Unique values in the Grade column</h6></li>
				<li>Find the frequency distribution of each categorical column. To do so, use the <code>value_counts()</code> function on each column. This function returns the counts of unique values in an object:<pre>df_categorical.Grade.value_counts()</pre><p>The output of this step is as follows:</p><div><img src="img/C13322_01_30.jpg" alt="Figure 1.30: Total count of each unique value in the Grade column&#13;&#10;" width="1255" height="161"/></div><h6>Figure 1.30: Total count of each unique value in the Grade column</h6></li>
				<li>For the <code>Gender</code> column, write the following code:<pre>df_categorical.Gender.value_counts()</pre><p>The output of this code is as follows:</p><div><img src="img/C13322_01_31.jpg" alt="Figure 1.31: Total count of each unique value in the Gender column&#13;&#10;" width="1262" height="121"/></div><h6>Figure 1.31: Total count of each unique value in the Gender column</h6></li>
				<li>Similarly, for the <code>Employed</code> column, write the following code:<pre>df_categorical.Employed.value_counts()</pre><p>The output of this code is as follows:</p><div><img src="img/C13322_01_32.jpg" alt="Figure 1.32: Total count of each unique value in the Employed column&#13;&#10;" width="1168" height="112"/></div><h6>Figure 1.32: Total count of each unique value in the Employed column</h6></li>
				<li>Replace the entries in the <code>Grade</code> column. Replace <code>1st class</code> with <code>1</code>, <code>2nd class</code> with <code>2</code>, and <code>3rd class</code> with <code>3</code>. To do so, use the <code>replace()</code> function:<pre>df_categorical.Grade.replace({"1st Class":1, "2nd Class":2, "3rd Class":3}, inplace= True)</pre></li>
				<li>Replace the entries in the <code>Gender</code> column. Replace <code>Male</code> with <code>0</code> and <code>Female</code> with <code>1</code>. To do so, use the <code>replace()</code> function:<pre>df_categorical.Gender.replace({"Male":0,"Female":1}, inplace= True)</pre></li>
				<li>Replace the entries in the <code>Employed</code> column. Replace <code>no</code> with <code>0</code> and <code>yes</code> with <code>1</code>. To do so, use the <code>replace()</code> function:<pre>df_categorical.Employed.replace({"yes":1,"no":0}, inplace = True)</pre></li>
				<li>Once all the replacements for three columns are done, we need to print the dataframe. Add the following code:<pre>df_categorical.head()</pre></li>
			</ol>
			<div><div><img src="img/C13322_01_33.jpg" alt="Figure 1.33: Numerical data after replacement&#13;&#10;" width="539" height="152"/>
				</div>
			</div>
			<h6>Figure 1.33: Numerical data after replacement</h6>
			<p>You have successfully converted the categorical data to numerical data using a simple manual replacement method. We will now move on to look at another method of encoding categorical data.</p>
			<p><strong class="bold">Label Encoding</strong></p>
			<p>This is a technique in which we replace each value in a categorical column with numbers from 0 to N-1. For example, say we've got a list of employee names in a column. After performing label encoding, each employee name will be assigned a numeric label. But this might not be suitable for all cases because the model might consider numeric values to be weights assigned to the data. Label encoding is the best method to use for ordinal data. The scikit-learn library provides <code>LabelEncoder()</code>, which helps with label encoding. Let's look at an exercise in the next section.</p>
			<h3 id="_idParaDest-30"><a id="_idTextAnchor031"/>Exercise 7: Converting Categorical Data to Numerical Data Using Label Encoding</h3>
			<p>In this exercise, we will load the <code>Banking_Marketing.csv</code> dataset into a pandas dataframe and convert categorical data to numeric data using label encoding. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>Banking_Marketing.csv</code> dataset can be found here: <a href="">https://github.com/TrainingByPackt/Master-Data-Science-with-Python/blob/master/Chapter%201/Data/Banking_Marketing.csv</a>.</p>
			<ol>
				<li value="1">Open a Jupyter notebook and add a new cell. Write the code to import pandas and load the dataset into the pandas dataframe:<pre>import pandas as pd
import numpy as np
dataset = 'https://github.com/TrainingByPackt/Master-Data-Science-with-Python/blob/master/Chapter%201/Data/Banking_Marketing.csv'
df = pd.read_csv(dataset, header=0)</pre></li>
				<li>Before doing the encoding, remove all the missing data. To do so, use the <code>dropna()</code> function:<pre>df = df.dropna()</pre></li>
				<li>Select all the columns that are not numeric using the following code:<pre>data_column_category = df.select_dtypes(exclude=[np.number]).columns
data_column_category	</pre><p>To understand how the selection looks, refer to the following screenshot:</p><div><img src="img/C13322_01_34.jpg" alt="Figure 1.34: Non-numeric columns of the dataframe&#13;&#10;" width="1652" height="141"/></div><h6>Figure 1.34: Non-numeric columns of the dataframe</h6></li>
				<li>Print the first five rows of the new dataframe. Add the following code to do this:<pre>df[data_column_category].head()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_35.jpg" alt="Figure 1.35: Non-numeric values for the columns &#13;&#10;" width="744" height="180"/></div><h6>Figure 1.35: Non-numeric values for the columns </h6></li>
				<li>Iterate through this <code>category</code> column and convert it to numeric data using <code>LabelEncoder()</code>. To do so, import the <code>sklearn.preprocessing</code> package and use the <code>LabelEncoder()</code> class to transform the data:<pre>#import the LabelEncoder class
from sklearn.preprocessing import LabelEncoder
#Creating the object instance
label_encoder = LabelEncoder()
for i in data_column_category:
    df[i] = label_encoder.fit_transform(df[i])
print("Label Encoded Data: ")
df.head()</pre><p>The preceding code generates the following output:</p></li>
			</ol>
			<div><div><img src="img/C13322_01_36.jpg" alt="Figure 1.36: Values of non-numeric columns converted into numeric form&#13;&#10;" width="842" height="233"/>
				</div>
			</div>
			<h6>Figure 1.36: Values of non-numeric columns converted into numeric form</h6>
			<p>In the preceding screenshot, we can see that all the values have been converted from categorical to numerical. Here, the original values have been transformed and replaced with the newly encoded data.</p>
			<p>You have successfully converted categorical data to numerical data using the <code>LabelEncoder </code>method. In the next section, we'll explore another type of encoding: one-hot encoding.</p>
			<p><strong class="bold">One-Hot Encoding</strong></p>
			<p>In label encoding, categorical data is converted to numerical data, and the values are assigned labels (such as 1, 2, and 3). Predictive models that use this numerical data for analysis might sometimes mistake these labels for some kind of order (for example, a model might think that a label of 3 is "better" than a label of 1, which is incorrect). In order to avoid this confusion, we can use one-hot encoding. Here, the label-encoded data is further divided into n number of columns. Here, n denotes the total number of unique labels generated while performing label encoding. For example, say that three new labels are generated through label encoding. Then, while performing one-hot encoding, the columns will be divided into three parts. So, the value of n is 3. Let's look at an exercise to get further clarification.</p>
			<h3 id="_idParaDest-31"><a id="_idTextAnchor032"/>Exercise 8: Converting Categorical Data to Numerical Data Using One-Hot Encoding</h3>
			<p>In this exercise, we will load the <code>Banking_Marketing.csv</code> dataset into a pandas dataframe and convert the categorical data into numeric data using one-hot encoding. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>Banking_Marketing</code> dataset can be found here: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv</a>.</p>
			<ol>
				<li value="1">Open a Jupyter notebook and add a new cell. Write the code to import pandas and load the dataset into a pandas dataframe:<pre>import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
dataset = 'https://github.com/TrainingByPackt/Master-Data-Science-with-Python/blob/master/Chapter%201/Data/Banking_Marketing.csv'
#reading the data into the dataframe into the object data
df = pd.read_csv(dataset, header=0)</pre></li>
				<li>Before doing the encoding, remove all the missing data. To do so, use the <code>dropna()</code> function:<pre>df = df.dropna()</pre></li>
				<li>Select all the columns that are not numeric using the following code:<pre>data_column_category = df.select_dtypes(exclude=[np.number]).columns
data_column_category</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_37.jpg" alt="Figure 1.37: Non-numeric columns of the dataframe&#13;&#10;" width="1652" height="135"/></div><h6>Figure 1.37: Non-numeric columns of the dataframe</h6></li>
				<li>Print the first five rows of the new dataframe. Add the following code to do this:<pre>df[data_column_category].head()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_38.jpg" alt="Figure 1.38: Non-numeric values for the columns&#13;&#10;" width="744" height="180"/></div><h6>Figure 1.38: Non-numeric values for the columns</h6></li>
				<li>Iterate through these category columns and convert them to numeric data using <code>OneHotEncoder</code>. To do so, import the <code>sklearn.preprocessing</code> package and avail yourself of the <code>OneHotEncoder()</code> class do the transformation. Before performing one-hot encoding, we need to perform label encoding:<pre>#performing label encoding
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
for i in data_column_category:
    df[i] = label_encoder.fit_transform(df[i])
print("Label Encoded Data: ")
df.head()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_39.jpg" alt="Figure 1.39: Values of non-numeric columns converted into numeric data" width="842" height="233"/></div><h6>Figure 1.39: Values of non-numeric columns converted into numeric data</h6></li>
				<li>Once we have performed label encoding, we execute one-hot encoding. Add the following code to implement this:<pre>#Performing Onehot Encoding
onehot_encoder = OneHotEncoder(sparse=False)
onehot_encoded = onehot_encoder.fit_transform(df[data_column_category])</pre></li>
				<li>Now we create a new dataframe with the encoded data and print the first five rows. Add the following code to do this:<pre>#Creating a dataframe with encoded data with new column name
onehot_encoded_frame = pd.DataFrame(onehot_encoded, columns = onehot_encoder.get_feature_names(data_column_category))
onehot_encoded_frame.head()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_40.jpg" alt="Figure 1.40: Columns with one-hot encoded values&#13;&#10;" width="1800" height="326"/></div><h6>Figure 1.40: Columns with one-hot encoded values</h6></li>
				<li>Due to one-hot encoding, the number of columns in the new dataframe has increased. In order to view and print all the columns created, use the <code>columns</code> attribute:<pre>onehot_encoded_frame.columns</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_41.jpg" alt="Figure 1.41: List of new columns generated after one-hot encoding&#13;&#10;" width="1670" height="593"/></div><h6>Figure 1.41: List of new columns generated after one-hot encoding</h6></li>
				<li>For every level or category, a new column is created. In order to prefix the category name with the column name you can use this alternate way to create one-hot encoding. In order to prefix the category name with the column name, write the following code:<pre>df_onehot_getdummies = pd.get_dummies(df[data_column_category], prefix=data_column_category)
data_onehot_encoded_data = pd.concat([df_onehot_getdummies,df[data_column_number]],axis = 1)
data_onehot_encoded_data.columns</pre><p>The preceding code generates the following output:</p></li>
			</ol>
			<div><div><img src="img/C13322_01_42.jpg" alt="Figure 1.42: List of new columns containing the categories&#13;&#10;" width="600" height="314"/>
				</div>
			</div>
			<h6>Figure 1.42: List of new columns containing the categories</h6>
			<p>You have successfully converted categorical data to numerical data using the <code>OneHotEncoder</code> method.</p>
			<p>We will now move onto another data preprocessing step – how to deal with a range of magnitudes in your data.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor033"/>Data in Different Scales</h2>
			<p>In real life, values in a dataset might have a variety of different magnitudes, ranges, or scales. Algorithms that use distance as a parameter may not weigh all these in the same way. There are various data transformation techniques that are used to transform the features of our data so that they use the same scale, magnitude, or range. This ensures that each feature has an appropriate effect on a model's predictions.</p>
			<p>Some features in our data might have high-magnitude values (for example, annual salary), while others might have relatively low values (for example, the number of years worked at a company). Just because some data has smaller values does not mean it is less significant. So, to make sure our prediction does not vary because of different magnitudes of features in our data, we can perform feature scaling, standardization, or normalization (these are three similar ways of dealing with magnitude issues in data).</p>
			<h3 id="_idParaDest-33">Exercise 9: Implementing Scaling Using the<a id="_idTextAnchor034"/><a id="_idTextAnchor035"/> Standard Scaler Method</h3>
			<p>In this exercise, we will load the <code>Wholesale customer's data.csv</code> dataset into the pandas dataframe and perform scaling using the standard scaler method. This dataset refers to clients of a wholesale distributor. It includes the annual spending in monetary units on diverse product categories. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>Wholesale customer</code> dataset can be found here: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Wholesale%20customers%20data.csv</a>.	</p>
			<ol>
				<li value="1">Open a Jupyter notebook and add a new cell. Write the code to import pandas and load the dataset into the pandas dataframe:<pre>import pandas as pd
dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Wholesale%20customers%20data.csv'
df = pd.read_csv(dataset, header=0)</pre></li>
				<li>Check whether there is any missing data. If there is, drop the missing data:<pre>null_ = df.isna().any()
dtypes = df.dtypes
info = pd.concat([null_,dtypes],axis = 1,keys = ['Null','type'])
print(info)</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_43.jpg" alt="Figure 1.43: Different columns of the dataframe&#13;&#10;" width="1386" height="380"/></div><h6>Figure 1.43: Different columns of the dataframe</h6><p>As we can see, there are eight columns present in the dataframe, all of type <code>int64</code>. Since the null value is <code>False</code>, it means there are no null values present in any of the columns. Thus, there is no need to use the <code>dropna()</code> function.</p></li>
				<li>Now perform standard scaling and print the first five rows of the new dataset. To do so, use the <code>StandardScaler()</code> class from <code>sklearn.preprocessing</code> and implement the <code>fit_transorm()</code> method:<pre>from sklearn import preprocessing
std_scale = preprocessing.StandardScaler().fit_transform(df)
scaled_frame = pd.DataFrame(std_scale, columns=df.columns)
scaled_frame.head()</pre><p>The preceding code generates the following output:</p></li>
			</ol>
			<div><div><img src="img/C13322_01_44.jpg" alt="Figure 1.44: Data of the features scaled into a uniform unit&#13;&#10;" width="667" height="183"/>
				</div>
			</div>
			<h6>Figure 1.44: Data of the features scaled into a uniform unit</h6>
			<p>Using the <code>StandardScaler</code> method, we have scaled the data into a uniform unit over all the columns. As you can see in the preceding table, the values of all the features have been converted into a uniform range of the same scale. Because of this, it becomes easier for the model to make predictions. </p>
			<p>You have successfully scaled the data using the <code>StandardScaler</code> method. In the next section, we'll have a go at an exercise in which we'll implement scaling using the <code>MinMax</code> scaler method.</p>
			<h3 id="_idParaDest-34"><a id="_idTextAnchor036"/>Exercise 10: Implementing Scaling Using the MinMax Scaler Method</h3>
			<p>In this exercise, we will be loading the <code>Wholesale customers data.csv</code> dataset into a pandas dataframe and perform scaling using the <code>MinMax</code> scaler method. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>Whole customers data.csv</code> dataset can be found here: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Wholesale%20customers%20data.csv</a>.</p>
			<ol>
				<li value="1">Open a Jupyter notebook and add a new cell. Write the following code to import the pandas library and load the dataset into a pandas dataframe:<pre>import pandas as pd
dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Wholesale%20customers%20data.csv'
df = pd.read_csv(dataset, header=0)</pre></li>
				<li>Check whether there is any missing data. If there is, drop the missing data:<pre>null_ = df.isna().any()
dtypes = df.dtypes
info = pd.concat([null_,dtypes],axis = 1,keys = ['Null','type'])
print(info)</pre><p>The preceding code generates the following output:	</p><div><img src="img/C13322_01_45.jpg" alt="Figure 1.45: Different columns of the dataframe&#13;&#10;" width="1233" height="378"/></div><h6>Figure 1.45: Different columns of the dataframe</h6><p>As we can see, there are eight columns present in the dataframe, all of type <code>int64</code>. Since the null value is <code>False</code>, it means there are no null values present in any of the columns. Thus, there is no need to use the <code>dropna()</code> function.</p></li>
				<li>Perform <code>MinMax</code> scaling and print the initial five values of the new dataset. To do so, use the <code>MinMaxScaler()</code> class from <code>sklearn.preprocessing</code> and implement the <code>fit_transorm()</code> method. Add the following code to implement this:<pre>from sklearn import preprocessing
minmax_scale = preprocessing.MinMaxScaler().fit_transform(df)
scaled_frame = pd.DataFrame(minmax_scale,columns=df.columns)
scaled_frame.head()</pre><p>The preceding code generates the following output:</p></li>
			</ol>
			<h6> </h6>
			<div><div><img src="img/C13322_01_46.jpg" alt="Figure 1.46: Data of the features scaled into a uniform unit&#13;&#10;" width="627" height="179"/>
				</div>
			</div>
			<h6>Figure 1.46: Data of the features scaled into a uniform unit</h6>
			<p>Using the <code>MinMaxScaler</code> method, we have again scaled the data into a uniform unit over all the columns. As you can see in the preceding table, the values of all the features have been converted into a uniform range of the same scale. You have successfully scaled the data using the <code>MinMaxScaler</code> method. </p>
			<p>In the next section, we'll explore another pre-processing task: data discretization.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor037"/>Data Discretization</h2>
			<p>So far, we have done the categorical data treatment using encoding and numerical data treatment using scaling. </p>
			<p><strong class="keyword">Data discretization</strong> is the process of converting continuous data into discrete buckets by grouping it. Discretization is also known for easy maintainability of the data. Training a model with discrete data becomes faster and more effective than when attempting the same with continuous data. Although continuous-valued data contains more information, huge amounts of data can slow the model down. Here, discretization can help us strike a balance between both. Some famous methods of data discretization are <strong class="bold">binning</strong> and using a histogram. Although data discretization is useful, we need to effectively pick the range of each bucket, which is a challenge. </p>
			<p>The main challenge in discretization is to choose the number of intervals or bins and how to decide on their width.</p>
			<p>Here we make use of a function called <code>pandas.cut()</code>. This function is useful to achieve the bucketing and sorting of segmented data. </p>
			<h3 id="_idParaDest-36"><a id="_idTextAnchor038"/>Exercise 11: Discretization of Continuous Data </h3>
			<p>In this exercise, we will load the <code>Student_bucketing.csv</code> dataset and perform bucketing. The dataset consists of student details such as <code>Student_id</code>, <code>Age</code>, <code>Grade</code>, <code>Employed</code>, and <code>marks</code>. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>Student_bucketing.csv</code> dataset can be found here: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Student_bucketing.csv</a>.</p>
			<ol>
				<li value="1">Open a Jupyter notebook and add a new cell. Write the following code to import the required libraries and load the dataset into a pandas dataframe:<pre>import pandas as pd
dataset = "https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Student_bucketing.csv"
df = pd.read_csv(dataset, header = 0)</pre></li>
				<li>Once we load the dataframe, display the first five rows of the dataframe. Add the following code to do this:<pre>df.head()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_47.jpg" alt="Figure 1.47: First five rows of the dataframe&#13;&#10;" width="721" height="221"/></div><h6>Figure 1.47: First five rows of the dataframe</h6></li>
				<li>Perform bucketing using the <code>pd.cut()</code> function on the <code>marks</code> column and display the top 10 columns. The <code>cut()</code> function takes parameters such as <code>x</code>, <code>bins</code>, and <code>labels</code>. Here, we have used only three parameters. Add the following code to implement this: <pre>df['bucket']=pd.cut(df['marks'],5,labels=['Poor','Below_average','Average','Above_Average','Excellent'])
df.head(10)</pre><p>The preceding code generates the following output:</p></li>
			</ol>
			<div><div><img src="img/C13322_01_48.jpg" alt="Figure 1.48: Marks column with five discrete buckets&#13;&#10;" width="793" height="410"/>
				</div>
			</div>
			<h6>Figure 1.48: Marks column with five discrete buckets</h6>
			<p>In the preceding code, the first parameter represents an array. Here, we have selected the <code>marks</code> column as an array from the dataframe. <code>5</code> represents the number of bins to be used. As we have set bins to <code>5</code>, the labels need to be populated accordingly with five values: <code>Poor</code>, <code>Below_average</code>, <code>Average</code>, <code>Above_average</code>, and <code>Excellent</code>. In the preceding figure, we can see the whole of the continuous <strong class="bold">marks</strong> column is put into five discrete buckets. We have learned how to perform bucketing.</p>
			<p>We have now covered all the major tasks involved in pre-processing. In the next section, we'll look in detail at how to train and test your data. </p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor039"/>Train and Test Data</h2>
			<p>Once you've pre-processed your data into a format that's ready to be used by your model, you need to split up your data into train and test sets. This is because your machine learning algorithm will use the data in the training set to learn what it needs to know. It will then make a prediction about the data in the test set, using what it has learned. You can then compare this prediction against the actual target variables in the test set in order to see how accurate your model is. The exercise in the next section will give more clarity on this.</p>
			<p>We will do the train/test split in proportions. The larger portion of the data split will be the train set and the smaller portion will be the test set. This will help to ensure that you are using enough data to accurately train your model.</p>
			<p>In general, we carry out the train-test split with an 80:20 ratio, as per the Pareto principle. The Pareto principle states that "for many events, roughly 80% of the effects come from 20% of the causes." But if you have a large dataset, it really doesn't matter whether it's an 80:20 split or 90:10 or 60:40. (It can be better to use a smaller split set for the training set if our process is computationally intensive, but it might cause the problem of overfitting – this will be covered later in the book.)</p>
			<h3 id="_idParaDest-38"><a id="_idTextAnchor040"/>Exercise 12: Splitting Data into Train and Test Sets</h3>
			<p>In this exercise, we will load the <code>USA_Housing.csv</code> dataset (which you saw earlier) into a pandas dataframe and perform a train/test split. Follow these steps to complete this exercise:</p>
			<h4>Note</h4>
			<p class="callout">The <code>USA_Housing.csv</code> dataset is available here: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/USA_Housing.csv</a>.</p>
			<ol>
				<li value="1">Open a Jupyter notebook and add a new cell to import pandas and load the dataset into pandas:<pre>import pandas as pd
dataset = 'https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/USA_Housing.csv'
df = pd.read_csv(dataset, header=0)</pre></li>
				<li>Create a variable called <code>X</code> to store the independent features. Use the <code>drop()</code> function to include all the features, leaving out the dependent or the target variable, which in this case is named <code>Price</code>. Then, print out the top five instances of the variable. Add the following code to do this: <pre>X = df.drop('Price', axis=1)
X.head()</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_49.jpg" alt="Figure 1.49: Dataframe consisting of independent variables&#13;&#10;" width="576" height="146"/></div><h6>Figure 1.49: Dataframe consisting of independent variables</h6></li>
				<li>Print the shape of your new created feature matrix using the <code>X.shape</code> command: <pre>X.shape</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_50.jpg" alt="Figure 1.50: Shape of the X variable&#13;&#10;" width="486" height="23"/></div><h6>Figure 1.50: Shape of the X variable</h6><p>In the preceding figure, the first value indicates the number of observations in the dataset (<strong class="bold">5000</strong>), and the second value represents the number of features (<strong class="bold">6</strong>).</p></li>
				<li>Similarly, we will create a variable called <code>y</code> that will store the target values. We will use indexing to grab the target column. Indexing allows us to access a section of a larger element. In this case, we want to grab the column named Price from the <code>df</code> dataframe and print out the top 10 values. Add the following code to implement this:<pre>y = df['Price']
y.head(10)</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_51.jpg" alt="Figure 1.51: Top 10 values of the y variable&#13;&#10;" width="464" height="195"/></div><h6>   </h6><h6>Figure 1.51: Top 10 values of the y variable</h6></li>
				<li>Print the shape of your new variable using the <code>y.shape</code> command:<pre>y.shape</pre><p>The preceding code generates the following output:</p><div><img src="img/C13322_01_52.jpg" alt="Figure 1.52: Shape of the y variable&#13;&#10;" width="410" height="26"/></div><h6>Figure 1.52: Shape of the y variable</h6><p>The shape should be one-dimensional, with a length equal to the number of observations (<strong class="bold">5000</strong>).</p></li>
				<li>Make train/test sets with an 80:20 split. To do so, use the <code>train_test_split()</code> function from the <code>sklearn.model_selection</code> package. Add the following code to do this:<pre>from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)</pre><p>In the preceding code, <code>test_size</code> is a floating-point value that defines the size of the test data. If the value is 0.2, then it is an 80:20 split. <code>test_train_split</code> splits the arrays or matrices into train and test subsets in a random way. Each time we run the code without <code>random_state</code>, we will get a different result.</p></li>
				<li>Print the shape of <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code>. Add the following code to do this:<pre>print("X_train : ",X_train.shape)
print("X_test : ",X_test.shape)
print("y_train : ",y_train.shape)
print("y_test : ",y_test.shape)</pre><p>The preceding code generates the following output:</p></li>
			</ol>
			<div><div><img src="img/C13322_01_53.jpg" alt="Figure 1.53: Shape of train and test datasets&#13;&#10;" width="1204" height="174"/>
				</div>
			</div>
			<h6>Figure 1.53: Shape of train and test datasets</h6>
			<p>You have successfully split the data into train and test sets. </p>
			<p>In the next section, you will complete an activity wherein you'll perform pre-processing on a dataset.</p>
			<h3 id="_idParaDest-39"><a id="_idTextAnchor041"/>Activity 1: Pre-Processing Using the Bank Marketing Subscription Dataset</h3>
			<p>In this activity, we'll perform various pre-processing tasks on the <code>Bank Marketing Subscription</code> dataset. This dataset relates to the direct marketing campaigns of a Portuguese banking institution. Phone calls are made to market a new product, and the dataset records whether each customer subscribed to the product. </p>
			<p>Follow these steps to complete this activity:</p>
			<h4>Note</h4>
			<p class="callout">The <code>Bank Marketing Subscription</code> dataset is available here: <a href="">https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv</a>.</p>
			<ol>
				<li value="1">Load the dataset from the link given into a pandas dataframe. </li>
				<li>Explore the features of the data by finding the number of rows and columns, listing all the columns, finding the basic statistics of all columns (you can use the <code>describe().transpose()</code> function), and listing the basic information of the columns (you can use the <code>info()</code> function).</li>
				<li>Check whether there are any missing (or NULL) values, and if there are, find how many missing values there are in each column.</li>
				<li>Remove any missing values.</li>
				<li>Print the frequency distribution of the <code>education</code> column.</li>
				<li>The <code>education</code> column of the dataset has many categories. Reduce the categories for better modeling.</li>
				<li>Select and perform a suitable encoding method for the data.</li>
				<li>Split the data into train and test sets. The target data is in the <code>y</code> column and the independent data is in the remaining columns. Split the data with 80% for the train set and 20% for the test set.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 324.</p></li>
			</ol>
			<p>Now that we've covered the various data pre-processing steps, let's look at the different types of machine learning that are available to data scientists in some more detail.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor042"/>Supervised Learning</h2>
			<p>Supervised learning is a learning system that trains using labeled data (data in which the target variables are already known). The model learns how patterns in the feature matrix map to the target variables. When the trained machine is fed with a new dataset, it can use what it has learned to predict the target variables. This can also be called predictive modeling. </p>
			<p>Supervised learning is broadly split into two categories. These categories are as follows: </p>
			<p><strong class="keyword">Classification</strong> mainly deals with categorical target variables. A classification algorithm helps to predict which group or class a data point belongs to.</p>
			<p>When the prediction is between two classes, it is known as binary classification. An example is predicting whether or not a customer will buy a product (in this case, the classes are yes and no). </p>
			<p>If the prediction involves more than two target classes, it is known as multi-classification; for example, predicting all the items that a customer will buy. </p>
			<p><strong class="keyword">Regression</strong> deals with numerical target variables. A regression algorithm predicts the numerical value of the target variable based on the training dataset.</p>
			<p>Linear regression measures the link between one or more predictor variables and one outcome variable. For example, linear regression could help to enumerate the relative impacts of age, gender, and diet (the predictor variables) on height (the outcome variable).  </p>
			<p><strong class="keyword">Time series analysis</strong>, as the name suggests, deals with data that is distributed with respect to time, that is, data that is in a chronological order. Stock market prediction and customer churn prediction are two examples of time series data. Depending on the requirement or the necessities, time series analysis can be either a regression or classification task. </p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor043"/>Unsupervised Learning </h2>
			<p>Unlike supervised learning, the unsupervised learning process involves data that is neither classified nor labeled. The algorithm will perform analysis on the data without guidance. The job of the machine is to group unclustered information according to similarities in the data. The aim is for the model to spot patterns in the data in order to give some insight into what the data is telling us and to make predictions. </p>
			<p>An example is taking a whole load of unlabeled customer data and using it to find patterns to cluster customers into different groups. Different products could then be marketed to the different groups for maximum profitability.</p>
			<p>Unsupervised learning is broadly categorized into two types:</p>
			<ul>
				<li><strong class="bold">Clustering</strong>: A clustering procedure helps to discover the inherent patterns in the data. </li>
				<li><strong class="bold">Association</strong>: An association rule is a unique way to find patterns associated with a large amount of data, such as the supposition that when someone buys product 1, they also tend to buy product 2.</li>
			</ul>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor044"/>Reinforcement Learning</h2>
			<p>Reinforcement learning is a broad area in machine learning where the machine learns to perform the next step in an environment by looking at the results of actions already performed. Reinforcement learning does not have an answer, and the learning agent decides what should be done to perform the specified task. It learns from its prior knowledge. This kind of learning involves both a reward and a penalty. </p>
			<p>No matter the type of machine learning you're using, you'll want to be able to measure how effective your model is. You can do this using various performance metrics. You will see how these are used in later chapters in the book, but a brief overview of some of the most common ones is given here.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor045"/>Performance Metrics</h2>
			<p>There are different evaluation metrics in machine learning, and these depend on the type of data and the requirements. Some of the metrics are as follows:</p>
			<ul>
				<li>Confusion matrix</li>
				<li>Precision</li>
				<li>Recall</li>
				<li>Accuracy</li>
				<li>F1 score </li>
			</ul>
			<p class="Normal" lang="en-US" xml:lang="en-US"><strong class="bold">Confusion Matrix</strong></p>
			<p>A <strong class="keyword">confusion matrix</strong> is a table that is used to define the performance of the classification model on the test data for which the actual values are known. To understand this better, look at the following figure, showing predicted and actual values:</p>
			<div><div><img src="img/C13322_01_54.jpg" alt="" width="687" height="244"/>
				</div>
			</div>
			<h6>Figure 1.54: Predicted versus actual values</h6>
			<p>Let's examine the concept of a confusion matrix and its metrics, TP, TN, FP, and FN, in detail. Assume you are building a model that predicts pregnancy:</p>
			<ul>
				<li><code>True</code>. </li>
				<li><code>True</code>, which cannot happen. This is a type of error called a Type 1 error. </li>
				<li><code>False</code>, which is also an error. This is called a Type 2 error. </li>
				<li><code>False</code>; that is a <strong class="keyword">True Negative</strong>.</li>
			</ul>
			<p>The Type 1 error is a more dangerous error than the Type 2 error. Depending on the problem, we have to figure out whether we need to reduce Type 1 errors or Type 2 errors.</p>
			<p><strong class="bold">Precision</strong></p>
			<p>Precision is the ratio of TP outcomes to the total number of positive outcomes predicted by a model. The precision looks at how precise our model is as follows:</p>
			<div><div><img src="img/C13322_01_55.jpg" alt="" width="626" height="60"/>
				</div>
			</div>
			<h6>Figure 1.55: Precision equation</h6>
			<p><strong class="bold">Recall</strong></p>
			<p>Recall calculates what proportion of the TP outcomes our model has predicted:</p>
			<div><div><img src="img/C13322_01_56.jpg" alt="" width="690" height="55"/>
				</div>
			</div>
			<h6>Figure 1.56: Recall equation</h6>
			<p><strong class="bold">Accuracy</strong></p>
			<p>Accuracy calculates the ratio of the number of positive predictions made by a model out of the total number of predictions made:</p>
			<div><div><img src="img/C13322_01_57.jpg" alt="" width="819" height="52"/>
				</div>
			</div>
			<h6>Figure 1.57: Accuracy equation</h6>
			<p><strong class="bold">F1 score</strong></p>
			<p>F1 score is another accuracy measure, but one that allows us to seek a balance between precision and recall:</p>
			<div><div><img src="img/C13322_01_58.jpg" alt="" width="615" height="49"/>
				</div>
			</div>
			<h6>Figure 1.58: F1-score</h6>
			<p>When considering the performance of a model, we have to understand two other important concepts of prediction error: bias and variance. </p>
			<p><strong class="bold">What is bias?</strong></p>
			<p><strong class="keyword">Bias</strong> is how far a predicted value is from the actual value. High bias means the model is very simple and is not capable of capturing the data's complexity, causing what's called underfitting.</p>
			<p><strong class="bold">What is variance?</strong></p>
			<p><strong class="keyword">High variance</strong> is when the model performs too well on the trained dataset. This causes overfitting and makes the model too specific to the train data, meaning the model does not perform well on test data.</p>
			<div><div><img src="img/C13322_01_59.jpg" alt="" width="1184" height="296"/>
				</div>
			</div>
			<h6>Figure 1.59: High variance</h6>
			<p>Assume you are building a linear regression model to predict the market price of cars in a country. Let's say you have a large dataset about the cars and their prices, but there are still some more cars whose prices need to be predicted.</p>
			<p>When we train our model with the dataset, we want our model to just find that pattern within the dataset, nothing more, because if it goes beyond that, it will start to memorize the train set.</p>
			<p>We can improve our model by tuning its hyperparameters - there is more on this later in the book. We work towards minimizing the error and maximizing the accuracy by using another dataset, called the validation set. The first graph shows that the model has not learned enough to predict well in the test set. The third graph shows that the model has memorized the training dataset, which means the accuracy score will be 100, with 0 error. But if we predict on the test data, the middle model will outperform the third. </p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor046"/>Summary</h2>
			<p>In this chapter, we covered the basics of data science and explored the process of extracting underlying information from data using scientific methods, processes, and algorithms. We then moved on to data pre-processing, which includes data cleaning, data integration, data transformation, and data discretization.</p>
			<p>We saw how pre-processed data is split into train and test sets when building a model using a machine learning algorithm. We also covered supervised, unsupervised, and reinforcement learning algorithms. </p>
			<p>Lastly, we went over the different metrics, including confusion matrices, precision, recall, and accuracy.</p>
			<p>In the next chapter, we will cover data visualization.</p>
		</div>
	</div></body></html>