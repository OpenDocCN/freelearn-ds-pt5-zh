["```py\n//Create a Dataset from a simple collection \nscala> val ds1 = List.range(1,5).toDS() \nds1: org.apache.spark.sql.Dataset[Int] = [value: int] \n//Perform an action \nscala> ds1.collect() \nres3: Array[Int] = Array(1, 2, 3, 4) \n\n//Create from an RDD \nscala> val colors = List(\"red\",\"orange\",\"blue\",\"green\",\"yellow\") \nscala> val color_ds = sc.parallelize(colors).map(x => \n     (x,x.length)).toDS() \n//Add a case class \ncase class Color(var color: String, var len: Int) \nval color_ds = sc.parallelize(colors).map(x => \n     Color(x,x.length)).toDS() \n\n```", "```py\n//Examine the structure \nscala> color_ds.dtypes \nres26: Array[(String, String)] = Array((color,StringType), (len,IntegerType)) \nscala> color_ds.schema \nres25: org.apache.spark.sql.types.StructType = StructType(StructField(color,StringType,true), \nStructField(len,IntegerType,false)) \n//Examine the execution plan \nscala> color_ds.explain() \n== Physical Plan == \nScan ExistingRDD[color#57,len#58] \n\n```", "```py\n//Convert the dataset to a DataFrame \nscala> val color_df = color_ds.toDF() \ncolor_df: org.apache.spark.sql.DataFrame = [color: string, len: int] \n\nscala> color_df.show() \n+------+---+ \n| color|len| \n+------+---+ \n|   red|  3| \n|orange|  6| \n|  blue|  4| \n| green|  5| \n|yellow|  6| \n+------+---+ \n\n```", "```py\n//Construct a DataFrame first \nscala> val color_df = sc.parallelize(colors).map(x => \n           (x,x.length)).toDF(\"color\",\"len\") \ncolor_df: org.apache.spark.sql.DataFrame = [color: string, len: int] \n//Convert the DataFrame to a Dataset with a given structure \nscala> val ds_from_df = color_df.as[Color] \nds_from_df: org.apache.spark.sql.Dataset[Color] = [color: string, len: int] \n//Check the execution plan \nscala> ds_from_df.explain \n== Physical Plan == \nWholeStageCodegen \n:  +- Project [_1#102 AS color#105,_2#103 AS len#106] \n:     +- INPUT \n+- Scan ExistingRDD[_1#102,_2#103] \n\n```", "```py\n//Set filepath \nscala> val file_path = <Your path> \nfile_path: String = ./authors.json \n//Create case class to match schema \nscala> case class Auth(first_name: String, last_name: String,books: Array[String]) \ndefined class Auth \n\n//Create dataset from json using case class \n//Note that the json document should have one record per line \nscala> val auth = spark.read.json(file_path).as[Auth] \nauth: org.apache.spark.sql.Dataset[Auth] = [books: array<string>, firstName: string ... 1 more field] \n\n//Look at the data \nscala> auth.show() \n+--------------------+----------+---------+ \n|               books|first_name|last_name| \n+--------------------+----------+---------+ \n|                null|      Mark|    Twain| \n|                null|   Charles|  Dickens| \n|[Jude the Obscure...|    Thomas|    Hardy| \n+--------------------+----------+---------+ \n\n//Try explode to see array contents on separate lines \n\nscala> auth.select(explode($\"books\") as \"book\", \n            $\"first_name\",$\"last_name\").show(2,false) \n+------------------------+----------+---------+ \n|book                    |first_name|last_name| \n+------------------------+----------+---------+ \n|Jude the Obscure        |Thomas    |Hardy    | \n|The Return of the Native|Thomas    |Hardy    | \n+------------------------+----------+---------+ \n\n```", "```py\nscala> import org.apache.spark.sql.expressions.Window \nimport org.apache.spark.sql.expressions.Window \n//Create a DataFrame containing monthly sales data for two products \nscala> val monthlySales = spark.read.options(Map({\"header\"->\"true\"},{\"inferSchema\" -> \"true\"})). \n                            csv(\"<Your Path>/MonthlySales.csv\") \nmonthlySales: org.apache.spark.sql.DataFrame = [Product: string, Month: int ... 1 more field] \n\n//Prepare WindowSpec to create a 3 month sliding window for a product \n//Negative subscript denotes rows above current row \nscala> val w = Window.partitionBy(monthlySales(\"Product\")).orderBy(monthlySales(\"Month\")).rangeBetween(-2,0) \nw: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3cc2f15 \n\n//Define compute on the sliding window, a moving average in this case \nscala> val f = avg(monthlySales(\"Sales\")).over(w) \nf: org.apache.spark.sql.Column = avg(Sales) OVER (PARTITION BY Product ORDER BY Month ASC RANGE BETWEEN 2 PRECEDING AND CURRENT ROW) \n//Apply the sliding window and compute. Examine the results \nscala> monthlySales.select($\"Product\",$\"Sales\",$\"Month\", bround(f,2).alias(\"MovingAvg\")). \n                    orderBy($\"Product\",$\"Month\").show(6) \n+-------+-----+-----+---------+                                                  \n|Product|Sales|Month|MovingAvg| \n+-------+-----+-----+---------+ \n|     P1|   66|    1|     66.0| \n|     P1|   24|    2|     45.0| \n|     P1|   54|    3|     48.0| \n|     P1|    0|    4|     26.0| \n|     P1|   56|    5|    36.67| \n|     P1|   34|    6|     30.0| \n+-------+-----+-----+---------+ \n\n```", "```py\n    >>> from pyspark.sql import Window\n    >>> import pyspark.sql.functions as func\n    //Create a DataFrame containing monthly sales data for two products\n    >> file_path = <Your path>/MonthlySales.csv\"\n    >>> monthlySales = spark.read.csv(file_path,header=True, inferSchema=True)\n\n    //Prepare WindowSpec to create a 3 month sliding window for a product\n    //Negative subscript denotes rows above current row\n    >>> w = Window.partitionBy(monthlySales[\"Product\"]).orderBy(monthlySales[\"Month\"]).rangeBetween(-2,0)\n    >>> w\n    <pyspark.sql.window.WindowSpec object at 0x7fdc33774a50>\n    >>>\n    //Define compute on the sliding window, a moving average in this case\n    >>> f = func.avg(monthlySales[\"Sales\"]).over(w)\n    >>> f\n    Column<avg(Sales) OVER (PARTITION BY Product ORDER BY Month ASC RANGE BETWEEN 2 PRECEDING AND CURRENT ROW)>\n    >>>\n    //Apply the sliding window and compute. Examine the results\n    >>> monthlySales.select(monthlySales.Product,monthlySales.Sales,monthlySales.Month,\n                          func.bround(f,2).alias(\"MovingAvg\")).orderBy(\n                          monthlySales.Product,monthlySales.Month).show(6)\n    +-------+-----+-----+---------+                                                 \n    |Product|Sales|Month|MovingAvg|\n    +-------+-----+-----+---------+\n    |     P1|   66|    1|     66.0|\n    |     P1|   24|    2|     45.0|\n    |     P1|   54|    3|     48.0|\n    |     P1|    0|    4|     26.0|\n    |     P1|   56|    5|    36.67|\n    |     P1|   34|    6|     30.0|\n    +-------+-----+-----+---------+\n\n```", "```py\n//Run the following command from one terminal window \nsar -r 2 20 | nc -lk 9999 \n\n//In spark-shell window, do the following \n//Read stream \nscala> val myStream = spark.readStream.format(\"socket\"). \n                       option(\"host\",\"localhost\"). \n                       option(\"port\",9999).load() \nmyStream: org.apache.spark.sql.DataFrame = [value: string] \n\n//Filter out unwanted lines and then extract free memory part as a float \n//Drop missing values, if any \nscala> val myDF = myStream.filter($\"value\".contains(\"IST\")). \n               select(substring($\"value\",15,9).cast(\"float\").as(\"memFree\")). \n               na.drop().select($\"memFree\") \nmyDF: org.apache.spark.sql.DataFrame = [memFree: float] \n\n//Define an aggregate function \nscala> val avgMemFree = myDF.select(avg(\"memFree\")) \navgMemFree: org.apache.spark.sql.DataFrame = [avg(memFree): double] \n\n//Create StreamingQuery handle that writes on to the console \nscala> val query = avgMemFree.writeStream. \n          outputMode(\"complete\"). \n          format(\"console\"). \n          start() \nquery: org.apache.spark.sql.streaming.StreamingQuery = Streaming Query - query-0 [state = ACTIVE] \n\nBatch: 0 \n------------------------------------------- \n+-----------------+ \n|     avg(memFree)| \n+-----------------+ \n|4116531.380952381| \n+-----------------+ \n.... \n\n```", "```py\n    //Run the following command from one terminal window\n     sar -r 2 20 | nc -lk 9999\n\n    //In another window, open pyspark shell and do the following\n    >>> import pyspark.sql.functions as func\n    //Read stream\n    >>> myStream = spark.readStream.format(\"socket\"). \\\n                           option(\"host\",\"localhost\"). \\\n                           option(\"port\",9999).load()\n    myStream: org.apache.spark.sql.DataFrame = [value: string]\n\n    //Filter out unwanted lines and then extract free memory part as a float\n    //Drop missing values, if any\n    >>> myDF = myStream.filter(\"value rlike 'IST'\"). \\\n               select(func.substring(\"value\",15,9).cast(\"float\"). \\\n               alias(\"memFree\")).na.drop().select(\"memFree\")\n\n    //Define an aggregate function\n    >>> avgMemFree = myDF.select(func.avg(\"memFree\"))\n\n    //Create StreamingQuery handle that writes on to the console\n    >>> query = avgMemFree.writeStream. \\\n              outputMode(\"complete\"). \\\n              format(\"console\"). \\\n              start()\n    Batch: 0\n    -------------------------------------------\n    +------------+\n    |avg(memFree)|\n    +------------+\n    |   4042749.2|\n    +------------+\n    .....\n\n```"]