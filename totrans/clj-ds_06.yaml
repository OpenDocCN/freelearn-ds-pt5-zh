- en: Chapter 6. Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|   | *Things that have a common quality ever quickly seek their kind.* |  
    |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Marcus Aurelius* |'
  prefs: []
  type: TYPE_TB
- en: 'In previous chapters, we covered multiple learning algorithms: linear and logistic
    regression, C4.5, naive Bayes, and random forests. In each case we were required
    to train the algorithm by providing features and a desired output. In linear regression,
    for example, the desired output was the weight of an Olympic swimmer, whereas
    for the other algorithms we provided a class: whether the passenger survived or
    perished. These are examples of **supervised learning algorithms**: we tell our
    algorithm the desired output and it will attempt to learn a model that reproduces
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: There is another class of learning algorithm referred to as **unsupervised learning**.
    Unsupervised algorithms are able to operate on the data without a set of reference
    answers. We may not even know ourselves what structure lies within the data; the
    algorithm will attempt to determine the structure for itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering is an example of an unsupervised learning algorithm. The results
    of cluster analysis are groupings of input data that are more similar to each
    other in some way. The technique is general: any set entities that have a conceptual
    similarity or distance from each other can be clustered. For example, we could
    cluster groups of social media accounts by similarity in terms of shared followers,
    or we could cluster the results of market research by measuring the similarity
    of respondents'' answers to a questionnaire.'
  prefs: []
  type: TYPE_NORMAL
- en: One common application of clustering is to identify documents that share similar
    subject matter. This provides us with an ideal opportunity to talk about text
    processing, and this chapter will introduce a variety of techniques specific to
    dealing with text.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter makes use of the **Reuters-21578** dataset: a venerable collection
    of articles that were published on the Reuters newswire in 1987\. It is one of
    the most widely used for testing the categorization and classification of text.
    The copyright for the text of articles and annotations in the Reuters-21578 collection
    resides with Reuters Ltd. Reuters Ltd. and Carnegie Group, Inc. have agreed to
    allow the free distribution of this data for research purposes only.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can download the example code for this chapter from the Packt Publishing's
    website or from [https://github.com/clojuredatascience/ch6-clustering](https://github.com/clojuredatascience/ch6-clustering).
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, within the sample code is a script to download and unzip the files
    to the data directory. You can run it from within the project directory with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, at the time of writing, the Reuters dataset can be downloaded
    from [http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz).
    The rest of this chapter will assume that the files have been downloaded and installed
    to the project's data directory.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After you run the preceding script, the articles will be unzipped to the directory
    `data/reuters-sgml`. Each `.sgm` file in the extract contains around 1,000 short
    articles that have been wrapped in XML-style tags using Standard Generalized Markup
    Language (SGML). Rather than write our own parser for the format, we can make
    use of the one already written in the Lucene text indexer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here we're making use of Clojure's Java interop to simply call the extract method
    on Lucene's `ExtractReuters` class. Each article is extracted as its own text
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code can be run by executing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: on the command line within the project directory. The output will be a new directory,
    `data/reuters-text`, containing over 20,000 individual text files. Each file contains
    a single Reuters newswire article.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re short on disk space you can delete the `reuters-sgml` and `reuters21578.tar.gz`
    files now: the contents of the `reuters-text` directory are the only files we
    will be using in this chapter. Let''s look at a few now.'
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The year 1987 was the year of "Black Monday". On 19th October stock markets
    around the world crashed and the Dow Jones Industrial Average declined 508 points
    to 1738.74\. Articles such as the one contained in `reut2-020.sgm-962.txt` describe
    the event:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The structure of this article is representative of the majority of articles
    in the corpus. The first line is the timestamp indicating when the article was
    published, followed by a blank line. The article has a headline which is often—but
    not always—in upper case, and then another blank line. Finally comes the article
    body text. As is often the case when working with semi-structured text such as
    this, there are multiple spaces, odd characters, and abbreviations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other articles are simply headlines, for example in `reut2-020.sgm-761.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: These are the files on which we will be performing our cluster analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is the process of finding groups of objects that are similar to each
    other. The goal is that objects within a cluster should be more similar to each
    other than to objects in other clusters. Like classification, it is not a specific
    algorithm so much as a general class of algorithms that solve a general problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there are a variety of clustering algorithms, all rely to some extent
    on a distance measure. For an algorithm to determine whether two objects belong
    in the same or different clusters it must be able to determine a quantitative
    measure of the distance (or, if you prefer, the similarity) between them. This
    calls for a numeric measure of distance: the smaller the distance, the greater
    the similarity between two objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since clustering is a general technique that can be applied to diverse data
    types, there are a large number of possible distance measures. Nonetheless, most
    data can be represented by one of a handful of common abstractions: a set, a point
    in space, or a vector. For each of these there exists a commonly-used measure.'
  prefs: []
  type: TYPE_NORMAL
- en: Set-of-words and the Jaccard index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If your data can be represented as a set of things the Jaccard index, also
    known as the **Jaccard similarity**, can be used. It''s one of the simplest measures
    conceptually: it is the set intersection divided by the set union, or the number
    of shared elements in common out of the total unique elements in the sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Set-of-words and the Jaccard index](img/7180OS_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Many things can be represented as sets. Accounts on social networks can be represented
    as sets of friends or followers, and customers can be represented as sets of products
    purchased or viewed. For our text documents, a set representation could simply
    be the set of unique words used.
  prefs: []
  type: TYPE_NORMAL
- en: '![Set-of-words and the Jaccard index](img/7180OS_06_100.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Jaccard index is very simple to calculate in Clojure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It has the advantage that the sets don't have to be of the same cardinality
    for the distance measure to make sense. In the preceding diagram, **A** is "larger"
    than **B**, yet the intersection divided by the union is still a fair reflection
    of their similarity. To apply the Jaccard index to text documents, we need to
    translate them into sets of words. This is the process of **tokenization**.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing the Reuters files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenization is the name for the technique of taking a string of text and splitting
    it into smaller units for the purpose of analysis. A common approach is to split
    a text string into individual words. An obvious separator would be whitespace
    so that `"tokens like these"` become `["tokens" "like" "these"]`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is convenient and simple, but unfortunately, language is subtle and few
    simple rules can be applied universally. For example, our tokenizer treats apostrophes
    as whitespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Hyphens are treated as whitespace too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'and removing them rather changes the meaning of the sentence. However, not
    all hyphens should be preserved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The terms `"New"`, `"York"`, and `"based"` correctly represent the subject of
    the phrase, but it would be preferable to group `"New York"` into a single term,
    since it represents a specific name and really ought to be preserved intact. `York-based`,
    on the other hand, would be a meaningless token on its own.
  prefs: []
  type: TYPE_NORMAL
- en: In short, text is messy, and parsing meaning reliably from free text is an extremely
    rich and active area of research. In particular, for extracting names (e.g. `"New
    York"`) from text, we need to consider the context in which the terms are used.
    Techniques that label tokens within a sentence by their grammatical function are
    called **parts-of-speech taggers**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on advanced tokenization and parts-of-speech tagging, see
    the `clojure-opennlp` library at [https://github.com/dakrone/clojure-opennlp](https://github.com/dakrone/clojure-opennlp).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we have the luxury of a large quantity of documents and so we'll
    continue to use our simple tokenizer. We'll find that—in spite of its deficiencies—it
    will perform well enough to extract meaning from the documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write a function to return the tokens for a document from its file name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We're removing the timestamp from the top of the file and making the text lower-case
    before tokenizing. In the next section, we'll see how to measure the similarity
    between tokenized documents.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the Jaccard index to documents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having tokenized our input documents, we can simply pass the resulting sequence
    of tokens to our `jaccard-similarity` function defined previously. Let''s compare
    the similarity of a couple of documents from the Reuters corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The Jaccard index outputs a number between zero and one, so it has judged these
    documents to be 25 percent similar based on the words in their headlines. Notice
    how we''ve lost the order of the words in the headline. Without further tricks
    that we''ll come to shortly, the Jaccard index looks only at the items in common
    between two sets. Another aspect we''ve lost is the number of times a term occurs
    in the document. A document that repeats the same word many times may in some
    sense regard that word as more important. For example, `reut2-020.sgm-932.txt`
    has a headline like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: NYSE appears twice in the headline. We could infer that this headline is especially
    about the New York Stock Exchange, perhaps more so than a headline that mentioned
    NYSE only once.
  prefs: []
  type: TYPE_NORMAL
- en: The bag-of-words and Euclidean distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A possible improvement over the set-of-words approach is the **bag-of-words
    approach**. This preserves the word count of the terms within the document. The
    term count can be incorporated by distance measures for a potentially more accurate
    result.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common conceptions of distance is the Euclidean distance measure.
    In geometry, the Euclidean measure is how we calculate the distance between two
    points in space. In two dimensions, the Euclidean distance is given by the **Pythagoras
    formula**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The bag-of-words and Euclidean distance](img/7180OS_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This represents the difference between two points as the length of the straight-line
    distance between them.
  prefs: []
  type: TYPE_NORMAL
- en: '![The bag-of-words and Euclidean distance](img/7180OS_06_110.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be extended to three dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The bag-of-words and Euclidean distance](img/7180OS_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'and generalized to *n* dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The bag-of-words and Euclidean distance](img/7180OS_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *A*[i] and *B*[i] are the values of *A* or *B* at dimension *i*. The distance
    measure is thus the overall similarity between two documents, having taken into
    account how many times each word occurs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Since each word now represents a dimension in space, we need to make sure that
    when we calculate the Euclidean distance measure we are comparing the magnitude
    in the same dimension of each document. Otherwise, we may literally be comparing
    "apples" with "oranges".
  prefs: []
  type: TYPE_NORMAL
- en: Representing text as vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the Jaccard index, the Euclidean distance relies on a consistent ordering
    of words into dimensions. The word count, or term frequency, represents the position
    of that document in a large multi-dimensional space, and we need to ensure that
    when we compare values we do so in the correct dimension. Let's represent our
    documents as term **frequency vectors**.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine all the words that could appear in a document being given a unique number.
    For example, the word "apple" could be assigned the number 53, the word "orange"
    could be assigned the number 21,597\. If all numbers are unique, they could correspond
    to the index that a word appears in a term vector.
  prefs: []
  type: TYPE_NORMAL
- en: The dimension of these vectors can be very large. The maximum number of dimensions
    possible is the cardinality of the vector. The value of the element at the index
    corresponding to a word is usually the number of occurrences of the word in the
    document. This is known as the **term frequency** (**tf**), weighting.
  prefs: []
  type: TYPE_NORMAL
- en: In order to be able to compare text vectors it's important that the same word
    always appears at the same index in the vector. This means that we must use the
    same word/index mapping for each vector that we create. This word/index mapping
    is our dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dictionary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a valid dictionary, we need to make sure that the indexes for two
    words don''t clash. One way to do this is to have a monotonically increasing counter
    which is incremented for each word added to the dictionary. The count at the point
    the word is added becomes the index of the word. To both add a word to the dictionary
    and increment a counter in a thread-safe way, we can use an atom:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To perform an update to an atom, we have to execute our code in a `swap!` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding another word will cause the count to increase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'And adding the same word twice will have no effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Performing this update inside an atom ensures that each word gets its own index
    even when the dictionary is being simultaneously updated by multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Building a whole dictionary is as simple as reducing our `add-term-to-dict!`
    function over a supplied dictionary atom with a collection of terms.
  prefs: []
  type: TYPE_NORMAL
- en: Creating term frequency vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To calculate the Euclidean distance, let's first create a vector from our dictionary
    and document. This will allow us to easily compare the term frequencies between
    documents because they will occupy the same index of the vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `term-frequencies` function creates a map of term ID to frequency count
    for each term in the document. The `map->vector` function simply takes this map
    and associates the frequency count at the index of the vector given by the term
    ID. Since there may be many terms, and the vector may be very long, we're using
    Clojure's `transient!` and `persistent!` functions to temporarily create a mutable
    vector for efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s print the document, dictionary, and resulting vector for `reut2-020.sgm-742.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows (the formatting has been adjusted for legibility):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With 12 terms in the input, there are 12 terms in the dictionary and a vector
    of 12 elements returned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Like the Jaccard index, the Euclidean distance cannot decrease below zero. Unlike
    the Jaccard index, though, the value can grow indefinitely.
  prefs: []
  type: TYPE_NORMAL
- en: The vector space model and cosine distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The vector space model can be considered a generalization of the set-of-words
    and bag-of-words models. Like the bag-of-words model, the vector space model represents
    each document as a vector, each element of which represents a term. The value
    at each index is a measure of importance of the word, which may or may not be
    the term frequency.
  prefs: []
  type: TYPE_NORMAL
- en: If your data conceptually represents a vector (that is to say, a magnitude in
    a particular direction), then the cosine distance may be the most appropriate
    choice. The cosine distance measure determines the similarity of two elements
    as the cosine of the angle between their vector representations.
  prefs: []
  type: TYPE_NORMAL
- en: '![The vector space model and cosine distance](img/7180OS_06_120.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If both vectors point in the same direction, then the angle between them will
    be zero and the cosine of zero is one. The cosine similarity can be defined in
    the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The vector space model and cosine distance](img/7180OS_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a more complicated equation than the ones we've covered previously.
    It relies on calculating the dot product of the two vectors and the magnitude
    of each.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Examples of the cosine similarity are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The vector space model and cosine distance](img/7180OS_06_130.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The cosine similarity is often used as a similarity measure in high-dimensional
    spaces where each vector contains a lot of zeros because it can be very efficient
    to evaluate: only the non-zero dimensions need to be considered. Since most text
    documents use only a small fraction of all words (and therefore are zero for a
    large proportion of dimensions), the cosine measure is often used for clustering
    text.'
  prefs: []
  type: TYPE_NORMAL
- en: In the vector space model, we need a consistent strategy for measuring the importance
    of each term. In the set-of-words model, all terms are counted equally. This is
    equivalent to setting the value of the vector at that point to one. In the bag-of-words
    model, the term frequencies were counted. We'll continue to use the term frequency
    for now, but we'll see shortly how to use a more sophisticated measure of importance,
    called **term frequency-inverse document frequency** (**TF-IDF**).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The closer the cosine value is to `1`, the more similar the two entities are.
    To convert `cosine-similarity` to a distance measure, we can simply subtract the
    `cosine-similarity` from `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Although all the measures mentioned earlier produce different measures for the
    same input, they all satisfy the constraint that the distance between *A* and
    *B* should be the same as the difference between *B* and *A*. Often the same underlying
    data can be transformed to represent a set (Jaccard), a point in space (Euclidean),
    or a vector (Cosine). Sometimes the only way to know which is right is to try
    it and see how good the results are.
  prefs: []
  type: TYPE_NORMAL
- en: The number of unique words that appear in one document is typically small compared
    to the number of unique words that appear in any document in a collection being
    processed. As a result, these high-dimensional document vectors are quite sparse.
  prefs: []
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much of the similarity between the headlines has been generated by often-occurring
    words that don't add a great deal of meaning to the content. Examples are "a",
    "says", and "and". We should filter these out in order to avoid generating spurious
    similarities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following two idioms:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"Music is the food of love"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"War is the locomotive of history"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We could calculate the cosine similarity between them using the following Clojure
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The two documents are showing a similarity of `0.5` in spite of the fact that
    the only words they share in common are `is`, `the`, and `of`. Ideally we'll want
    to remove these.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let''s consider an alternative phrase:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"Music is the food of love"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"It''s lovely that you''re musical"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s compare their cosine similarity as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In spite of the fact that the two sentences refer to music and positive feelings,
    the two phrases have a cosine similarity of zero: there are no words in common
    between the two phrases. This makes sense but does not express the behavior we
    usually want, which is to capture the similarity between "concepts", rather than
    the precise words that were used.'
  prefs: []
  type: TYPE_NORMAL
- en: One way of tackling this problem is to **stem** words, which reduces them to
    their roots. Words which share a common meaning are more likely to stem to the
    same root. The Clojure library stemmers ([https://github.com/mattdw/stemmers](https://github.com/mattdw/stemmers))
    will do this for us, and fortunately they will also remove stop words too.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Much better. After stemming and stop word removal, the similarity between the
    phrases has dropped from 0.0 to 0.82\. This is a good outcome since, although
    the sentences used different words, the sentiments they expressed were related.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering with k-means and Incanter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, having tokenized, stemmed, and vectorized our input documents—and with
    a selection of distance measures to choose from—we're in a position to run clustering
    on our data. The first clustering algorithm we'll look at is called *k-means clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-means is an iterative algorithm that proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly pick *k* cluster centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each of the data points to the cluster with the closest centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust each cluster centroid to the mean of its assigned data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until convergence or the maximum number of iterations reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The process is visualized in the following diagram for *k=3* clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering with k-means and Incanter](img/7180OS_06_140.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding figure, we can see that the initial cluster centroids at iteration
    1 don't represent the structure of the data well. Although the points are clearly
    arranged in three groups, the initial centroids (represented by crosses) are all
    distributed around the top area of the graph. The points are colored according
    to their closest centroid. As the iterations proceed, we can see how the cluster
    centroids are moved closer to their "natural" positions in the center of each
    of the groups of points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we define the main *k*-means function, it''s useful to define a couple
    of utility functions first: a function to calculate the centroid for a cluster,
    and a function to group the data into their respective clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `centroid` function simply calculates the mean of each column of the input
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `clusters` function splits a larger matrix up into a sequence of smaller
    matrices based on the supplied cluster IDs. The cluster IDs are provided as a
    sequence of elements the same length as the clustered points, listing the cluster
    ID of the point at that index in the sequence. Items that share a common cluster
    ID will be grouped together. With these two functions in place, here''s the finished
    `k-means` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We start by picking `k` random cluster centroids by sampling the input data.
    Then, we use loop/recur to continuously update the cluster centroids until `previous-cluster-ids`
    are the same as `cluster-ids`. At this point, no documents have moved cluster,
    so the clustering has converged.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering the Reuters documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's use our `k-means` function to cluster the Reuters documents now. Let's
    go easy on our algorithm to start with, and pick a small sample of larger documents.
    Larger documents will make it more likely that the algorithm will be able to determine
    meaningful similarities between them. Let's set the minimum threshold at 500 characters.
    This means that at the very least our input documents will have a headline and
    a couple of sentences of body text to work with.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We're using the `fs` library ([https://github.com/Raynes/fs](https://github.com/Raynes/fs))
    to create a list of files to perform our clustering on by calling `fs/glob` with
    a pattern that matches all the text files. We remove those which are too short,
    tokenize the first 100, and add them to the dictionary. We create `tf` vectors
    for our inputs and then call `k-means` on them.
  prefs: []
  type: TYPE_NORMAL
- en: If you run the preceding example, you'll receive a list of clustered document
    vectors, which isn't very useful. Let's create a `summary` function that uses
    the dictionary to report the most common terms in each of the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '*k*-means is by its nature a stochastic algorithm, and is sensitive to the
    starting position for the centroids. I get the following output, but yours will
    almost certainly differ:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, we don't seem to be getting very good results. The first cluster
    contains two articles about rockets and space, and the third seems to consist
    of articles about Iran. The most popular word in most of the articles is "said".
  prefs: []
  type: TYPE_NORMAL
- en: Better clustering with TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Term** **Frequency-Inverse Document Frequency** (**TF-IDF**) is a general
    approach to weighting terms within a document vector so that terms that are popular
    across the whole dataset are not weighted as highly as terms that are less usual.
    This captures the intuitive conviction—and what we observed earlier—that words
    such as "said" are not a strong basis for building clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Zipf's law
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Zipf''s law states that the frequency of any word is inversely proportional
    to its rank in the frequency table. Thus, the most frequent word will occur approximately
    twice as often as the second most frequent word and three times as often as the
    next most frequent word, and so on. Let''s see if this applies across our Reuters
    corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Using the preceding code we can calculate the frequency graph of the top 25
    most popular terms in the first 1,000 Reuters documents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Zipf''s law](img/7180OS_06_150.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the first 1,000 documents, the most popular term appears almost 10,000 times.
    The 25^(th) most popular term appears around 1,000 times overall. In fact, the
    data is showing that words are appearing more commonly in the Reuters corpus than
    their placement in the frequency table would suggest. This is most likely due
    to the bulletin nature of the Reuters corpus, which tends to re-use the same short
    words repeatedly.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the TF-IDF weight
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Calculating TF-IDF only requires two modifications to the code we've created
    already. Firstly, we must keep track of how many documents a given term appears
    in. Secondly, we must weight the term appropriately when building the document
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: Since we've already created a dictionary of terms, we may as well store the
    document frequencies for each term there.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `build-df-dictionary` function earlier accepts a dictionary and a sequence
    of terms. We build the dictionary from the distinct terms and look up the `term-id`
    for each one. Finally, we iterate over the term IDs and increment the `:df` for
    each one.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a document has words *w*[1], …, *w*[n], then the inverse document frequency
    for word *w*[i] is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculating the TF-IDF weight](img/7180OS_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'That is, the reciprocal of the number of documents it appears in. If a word
    occurs commonly across a collection of documents, its *DF* value is large and
    its *IDF* value is small. With a large number of documents, it''s common to normalize
    the *IDF* value by multiplying it by a constant number, usually the document count
    *N*, so the *IDF* equation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculating the TF-IDF weight](img/7180OS_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The TF-IDF weight *W*[i] of word *w*[i] is given by the product of the term
    frequency and the inverse document frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculating the TF-IDF weight](img/7180OS_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the *IDF* value in the preceding equation is still not ideal since
    for large corpora the range of the *IDF* term is usually much greater than the
    *TF* and can overwhelm its effect. To reduce this problem, and balance the weight
    of the *TF* and the *IDF* terms, the usual practice is to use the logarithm of
    the *IDF* value instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculating the TF-IDF weight](img/7180OS_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the TF-IDF weight *w*[i] for a word *w*[i] becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculating the TF-IDF weight](img/7180OS_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a classic TF-IDF weighting: common words are given a small weight and
    terms that occur infrequently get a large weight. The important words for determining
    the topic of a document usually have a high *TF* and a moderately large *IDF*,
    so the product of the two becomes a large value, thereby giving more importance
    to these words in the resulting vector.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code calculates the TF-IDF from `term-frequencies` defined previously
    and `document-frequencies` extracted from our dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering with TF-IDF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the preceding adjustments in place, we''re in a position to calculate
    the TF-IDF vectors for the Reuters documents. The following example is a modification
    of `ex-6-12` using the new `tfidf-vector` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is very similar to the previous example, but we have substituted
    our new `build-df-dictionary` and `tfidf-vector` functions. If you run the example,
    you should see output that looks a little better than before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Although the top words may be hard to interpret because they have been stemmed,
    these represent the most unusually common words within each of the clusters. Notice
    that "said" is no longer the most highly rated word across all clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Better clustering with n-grams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It should be clear from looking at the earlier lists of words how much has been
    sacrificed by reducing our documents to unordered sequences of terms. Without
    the context of a sentence, it's very hard to get more than a vague sense of what
    each cluster might be about.
  prefs: []
  type: TYPE_NORMAL
- en: There is, however, nothing inherent in the vector space model that precludes
    maintaining the order of our input tokens. We can simply create a new term to
    represent a combination of words. The combined term, representing perhaps several
    input words in sequence, is called an ***n*-gram**.
  prefs: []
  type: TYPE_NORMAL
- en: An example of an *n*-gram might be "new york", or "stock market". In fact, because
    they contain two terms, these are called **bigrams**. *n*-grams can be of arbitrary
    length. The longer an *n*-gram, the more context it carries, but also the rarer
    it is.
  prefs: []
  type: TYPE_NORMAL
- en: '*n*-grams are closely related to the concept of **shingling**. When we shingle
    our *n*-grams, we''re creating overlapping sequences of terms. The term shingling
    comes from the way the terms overlap like roof shingles.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Already, using 2-grams would allow us (for example) to distinguish between
    the following uses of the word "coconut" in the dataset: "coconut oil", "coconut
    planters", "coconut plantations", "coconut farmers", "coconut association", "coconut
    authority", "coconut products", "coconut exports", "coconut industry", and the
    rather pleasing "coconut chief". Each of these pairs of words defines a different
    concept—sometimes subtly different—that we can capture and compare across documents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the best of both worlds with *n*-grams and shingling by combining
    the results of multiple lengths of *n*-gram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: While stemming and stop word removal had the effect of reducing the size of
    our dictionary, and using TF-IDF had the effect of improving the utility of the
    weight for each term in a document, producing *n*-grams has the effect of massively
    expanding the number of terms we need to accommodate.
  prefs: []
  type: TYPE_NORMAL
- en: This explosion of features is going to immediately overwhelm our implementation
    of *k*-means in Incanter. Fortunately, there's a machine learning library called
    **Mahout** that's specifically designed to run algorithms such as *k*-means on
    very large quantities of data.
  prefs: []
  type: TYPE_NORMAL
- en: Large-scale clustering with Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mahout ([http://mahout.apache.org/](http://mahout.apache.org/)) is a machine
    learning library intended for use in distributed computing environments. Version
    0.9 of the library targets Hadoop and is the version we'll be using here.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the time of writing, Mahout 0.10 has just been released and also targets
    Spark. Spark is an alternative distributed computing framework that we'll be introducing
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw in the previous chapter that one of Hadoop''s abstractions is the sequence
    file: a binary representation of Java keys and values. Many of Mahout''s algorithms
    expect to operate on sequence files, and we''ll need to create one as input to
    Mahout''s *k*-means algorithm. Mahout''s *k*-means algorithm also expects to receive
    its input as a vector, represented by one of Mahout''s vector types.'
  prefs: []
  type: TYPE_NORMAL
- en: Although Mahout contains classes and utility programs that will extract vectors
    from text, we'll use this as an opportunity to demonstrate how to use Parkour
    and Mahout together. Not only will we have finer-grained control over the vectors
    that are created, but it will allow us to demonstrate more of the capabilities
    of Parkour for specifying Hadoop jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Converting text documents to a sequence file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We won''t define a custom job to convert our text documents into a sequence
    file representation, though: Mahout already defines a useful `SequenceFilesFromDirectory`
    class to convert a directory of text files. We''ll use this to create a single
    file representing the entire contents of the `reuters-txt` directory.'
  prefs: []
  type: TYPE_NORMAL
- en: Though the sequence file may be physically stored in separate chunks (on HDFS,
    for example), it is logically one file, representing all the input documents as
    key/value pairs. The key is the name of the file, and the value is the file's
    text contents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Converting text documents to a sequence file](img/7180OS_06_160.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code will handle the conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`SequenceFilesFromDirectory` is a Mahout utility class, part of a suite of
    classes designed to be called on the command line.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since running the preceding example is a prerequisite for subsequent examples,
    it''s also available on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We're calling the `main` function directly, passing the arguments we would otherwise
    pass on the command line as a string array.
  prefs: []
  type: TYPE_NORMAL
- en: Using Parkour to create Mahout vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a sequence file representation of the Reuters corpus, we need
    to transform each document (now represented as a single key/value pair) into a
    vector. We saw how to do this earlier using a shared dictionary modeled as a Clojure
    atom. The atom ensures that each distinct term gets its own ID even in a multi-threaded
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using Parkour and Hadoop to generate our vectors, but this presents
    a challenge. How can we assign a unique ID to each word when the nature of MapReduce
    programming is that mappers operate in parallel and share no state? Hadoop doesn't
    provide the equivalent of a Clojure atom for sharing mutable state across nodes
    in a cluster, and in fact minimizing shared state is key to scaling distributed
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a shared set of unique IDs therefore presents an interesting challenge
    for our Parkour job: let''s see how we can produce unique IDs for our dictionary
    in a distributed way.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating distributed unique IDs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we look at Hadoop-specific solutions, though, it's worth noting that
    one easy way of creating a cluster-wide unique identifier is to create a universally
    unique identifier, or UUID.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a long string of bytes in the form: `3a65c7db-6f41-4087-a2ec-8fe763b5f185`
    that is virtually guaranteed not to clash with any other UUID generated anywhere
    else in the world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While this works for generating unique IDs, the number of possible IDs is astronomically
    large, and Mahout''s sparse vector representation needs to be initialized with
    the cardinality of the vector expressed as an integer. IDs generated with `uuid`
    are simply too big. Besides, it doesn''t help us coordinate the creation of IDs:
    every machine in the cluster will generate different UUIDs to represent the same
    terms.'
  prefs: []
  type: TYPE_NORMAL
- en: One way of getting around this is to use the term itself to generate a unique
    ID. If we used a consistent hashing function to create an integer from each input
    term, all machines in the cluster would generate the same ID. Since a good hashing
    function is likely to produce a unique output for unique input terms, this technique
    is likely to work well. There will be some hash collisions (where two words hash
    to the same ID) but this should be a small percentage of the overall.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The method of hashing the features themselves to create a unique ID is often
    referred to as the "hashing trick". Although it's commonly used for text vectorization,
    it can be applied to any problem that involves large numbers of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the challenge of producing distinct IDs that are unique across the
    whole cluster gives us the opportunity to talk about a useful feature of Hadoop
    that Parkour exposes: the distributed cache.'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed unique IDs with Hadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s consider what our Parkour mapper and reducer might look like if we were
    to calculate unique, cluster-wide IDs. The mapper is easy: we''ll want to calculate
    the document frequency for each term we encounter, so the following mapper simply
    returns a vector for each unique term: the first element of the vector (the key)
    is the term itself, and the second element (the value) is `1`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The reducer's job will be to take these key/value pairs of terms to document
    count and reduce them such that each unique term has a unique ID. A trivial way
    of doing this would be to ensure that there is only one reducer on the cluster.
    Since all the terms would all be passed to this single process, the reducer could
    simply keep an internal counter and assign each term an ID in a similar way to
    what we did with the Clojure atom earlier. This isn't taking advantage of Hadoop's
    distributed capabilities, though.
  prefs: []
  type: TYPE_NORMAL
- en: 'One feature of Parkour that we haven''t introduced yet is the runtime context
    that''s accessible from within every mapper and reducer. Parkour binds the `parkour.mapreduce/*context*`
    dynamic variable to the Hadoop task context of the task within which our mappers
    and reducers run. The task context contains, amongst other things, the following
    properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Property | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `mapred.job.id` | String | The job''s ID |'
  prefs: []
  type: TYPE_TB
- en: '| `mapred.task.id` | int | The task attempt ID |'
  prefs: []
  type: TYPE_TB
- en: '| `mapred.task.partition` | int | The ID of the task within the job |'
  prefs: []
  type: TYPE_TB
- en: The last of these, the `mapred.task.partition` property, is the number of the
    task assigned by Hadoop, guaranteed to be a monotonically increasing integer unique
    across the cluster. This number is our task's global offset. Within each task
    we can also keep a local offset and output both with each word processed. The
    two offsets together—global and local—provide a unique identifier for the term
    across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram visualizes the process for eight terms processed on three
    separate mappers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed unique IDs with Hadoop](img/7180OS_06_170.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each mapper is only aware of its own partition number and the term's local offset.
    However, these two numbers are all that's required to calculate a unique, global
    ID. The preceding **Calculate Offsets** box determines what the global offset
    should be for each task partition. Partition **1** has a global offset of **0**.
    Partition **2** has a global offset of **3**, because partition **1** processed
    **3** words. Partition **3** has an offset of **5**, because partitions **1**
    and **2** processed **5** words between them, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the preceding approach to work, we need to know three things: the global
    offset of the mapper, the local offset of the term, and the total number of terms
    processed by each mapper. These three numbers can be used to define a unique,
    cluster-wide ID for each term. The reducer that creates these three numbers is
    defined as follows. It introduces a couple of new concepts that we''ll discuss
    shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The first step the reducer performs is to fetch the `global-offset`, the task
    partition for this particular reducer. We're using `mapcat-state`, a function
    defined in the transduce library ([https://github.com/brandonbloom/transduce](https://github.com/brandonbloom/transduce))
    to build up a sequence of tuples in the format `[[:data ["apple" [1 4]] [:data
    ["orange" [1 5]] ...]` where the vector of numbers `[1 4]` represents the global
    and local offsets respectively. Finally, when we've reached the end of this reduce
    task, we append a tuple to the sequence in the format `[:counts [1 5]]`. This
    represents the final local count, `5`, for this particular reducer partition,
    `1`. Thus, a single reducer is calculating all three of the elements we require
    to calculate all the term IDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The keyword provided to `::mr/source-as` is not one we''ve encountered previously.
    In the previous chapter, we saw how the shaping options `:keyvals`, `:keys`, and
    `:vals` let Parkour know how we wanted our data provided, and the structure of
    the data we''d be providing in return. For reducers, Parkour describes a more
    comprehensive set of shaping functions that account for the fact that inputs may
    be grouped. The following diagram illustrates the available options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributed unique IDs with Hadoop](img/7180OS_06_175.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The option provided to `::mr/sink-as` is not one we''ve encountered before
    either. The `parkour.io.dux` namespace provides options for de-multiplexing outputs.
    In practice this means that, by sinking as `dux/named-keyvals`, a single reducer
    can write to several different outputs. In other words, we''ve introduced a fork
    into our data pipeline: some data is written to one branch, the rest to another.'
  prefs: []
  type: TYPE_NORMAL
- en: Having set a sink specification of `dux/named-keyvals`, the first element of
    our tuple will be interpreted as the destination to write to; the second element
    of our tuple will be treated as the key/value pair to be written. As a result,
    we can write out the `:data` (the local and global offset) to one destination
    and the `:counts` (number of terms processed by each mapper) to another.
  prefs: []
  type: TYPE_NORMAL
- en: The job that makes use of the mapper and reducer that we've defined is presented
    next. As with the Parkour job we specified in the previous chapter, we chain together
    an input, map, partition, reduce, and output step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two primary differences between the preceding code and the job specification
    we''ve seen previously. Firstly, our output specifies two named sinks: one for
    each of the outputs of our reducer. Secondly, we''re using the `parkour.io.avro`
    namespace as `mra` to specify a schema for our data with `(mra/dsink [:string
    long-pair])`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter we made use of Tesser''s `FressianWritable` to serialize
    arbitrary Clojure data structures to disk. This worked because the contents of
    the `FressianWritable` did not need to be interpreted by Hadoop: the value was
    completely opaque. With Parkour, we have the option to define custom key/value
    pair types. Since the key and value do need to be interpreted as separate entities
    by Hadoop (for the purpose of reading, partitioning, and writing sequence files),
    Parkour allows us to define a "tuple schema" using the `parkour.io.avro` namespace,
    which explicitly defines the type of the key and the value. `long-pair` is a custom
    schema used to store both the local and global offset in a single tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'And, since schemas are composable, we can refer to the `long-pair` schema when
    defining our output schema: `(mra/dsink [:string long-pair])`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parkour uses the library `Acbracad` to serialize Clojure data structures using
    Avro. For more information about serialization options consult the documentation
    for Abracad at [https://github.com/damballa/abracad](https://github.com/damballa/abracad).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at another feature of Hadoop that Parkour exposes which allows
    our term ID job to be more efficient than it would otherwise be: the distributed
    cache.'
  prefs: []
  type: TYPE_NORMAL
- en: Sharing data with the distributed cache
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in the previous section, if we know the local offset of each
    word for a particular mapper, and we know how many records each mapper processed
    overall, then we're in a position to calculate a unique, contiguous ID for each
    word.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram that showed the process a few pages ago contained two central boxes
    each labeled **Calculate Offsets** and a **Global ID**. Those boxes map directly
    to the functions that we present next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Once we've calculated the map of offsets to use for generating unique IDs, we'd
    really like them to be available to all of our map and reduce tasks as a shared
    resource. Having generated the offsets in a distributed fashion, we'd like to
    consume it in a distributed fashion too.
  prefs: []
  type: TYPE_NORMAL
- en: The distributed cache is Hadoop's way of allowing tasks to access common data.
    This is a much more efficient way of sharing small quantities of data (data that's
    small enough to reside in memory) than through potentially costly data joins.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sharing data with the distributed cache](img/7180OS_06_180.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Before reading from the distributed cache, we have to write something to it.
    This can be achieved with Parkour''s `parkour.io.dval` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we''re writing two sets of data to the distributed cache with the `dval/edn-dval`
    function. The first is the result of the `calculate-offsets` function just defined
    which is passed to the `word-id-m` mappers for their use. The second set of data
    written to the distributed cache is their output. We''ll see how this is generated
    in the `word-id-m` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The value returned by `dval/edn-dval` implements the `IDRef` interface. This
    means that we can use Clojure's `deref` function (or the deref macro character
    `@`) to retrieve the value that it wraps, just as we do with Clojure's atoms.
    Dereferencing the distributed value the first time causes the data to be downloaded
    from the distributed cache to a local mapper cache. Once the data is available
    locally, Parkour takes care of reconstructing the Clojure data structure (the
    map of offsets) that we wrote to it in EDN format.
  prefs: []
  type: TYPE_NORMAL
- en: Building Mahout vectors from input documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we took a detour to introduce several new Parkour
    and Hadoop concepts, but we're finally in a position to build text vectors for
    Mahout using unique IDs for every term. Some further code is omitted for brevity
    but the whole job is available to view in the `cljds.ch6.vectorizer` example code
    namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned previously, Mahout''s implementation of *k*-means expects us to
    provide a vector representation of our input using one of its vector classes.
    Since our dictionary is large, and most documents use few of these terms, we''ll
    be using a sparse vector representation. The following code makes use of a `dictionary`
    distributed value to create a `org.apache.mahout.math.RandomAccessSparseVector`
    for every input document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we make use of the `create-tfidf-vectors-m` function, which brings
    everything we''ve covered together into a single Hadoop job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This task handles the creation of the dictionary, writing the dictionary to
    the distributed cache, and then using the dictionary—with the mapper we just defined—to
    convert each input document to a Mahout vector. To ensure sequence file compatibility
    with Mahout, we set the key/value classes of our final output to be `Text` and
    `VectorWritable`, where the key is the original filename of the document and the
    value is the Mahout vector representation of the contents.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can call this job by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The job will write the dictionary out to the `dictionary-path` (we'll be needing
    it again), and the vectors out to the `vector-path`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since running the preceding example is a prerequisite for subsequent examples,
    it''s also available on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll discover how to use these vectors to actually perform clustering
    with Mahout.
  prefs: []
  type: TYPE_NORMAL
- en: Running k-means clustering with Mahout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a sequence file of vectors suitable for consumption by Mahout,
    it's time to actually run *k*-means clustering on the whole dataset. Unlike our
    local Incanter version, Mahout won't have any trouble dealing with the full Reuters
    corpus.
  prefs: []
  type: TYPE_NORMAL
- en: As with the `SequenceFilesFromDirectory` class, we've created a wrapper around
    another of Mahout's command-line programs, `KMeansDriver`. The Clojure variable
    names make it easier to see what each command-line argument is for.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We're providing the string `org.apache.mahout.common.distance.CosineDistanceMeasure`
    to indicate to the driver that we'd like to use Mahout's implementation of the
    cosine distance measure. Mahout also includes a `EuclideanDistanceMeasure` and
    a `TanimotoDistanceMeasure` (similar to the Jaccard distance, the complement of
    the Jaccard index, but one that will operate on vectors rather than sets). Several
    other distance measures are also defined; consult the Mahout documentation for
    all the available options.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the preceding `run-kmeans` function in place, we simply need to let Mahout
    know where to access our files. As in the previous chapter, we assume Hadoop is
    running in local mode and all file paths are relative to the project root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This example may run for a little while as Mahout iterates over our large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing k-means clustering results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once it's finished, we'll want to see a cluster summary for each cluster as
    we did with our Incanter implementation. Fortunately, Mahout defines a `ClusterDumper`
    class that will do exactly this for us. We need to provide the location of our
    clusters, of course, but we'll provide the location of our dictionary, too. Providing
    the dictionary means that the output can return the top terms for each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the code that will actually call the `run-cluster-dump` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: We're making use of the `me.raynes.fs` library once again to determine which
    directory the final clusters are contained by. Mahout will append `-final` to
    the directory containing the final clusters, but we don't know ahead of time which
    directory this will be. The `fs/glob` function will find a directory that matches
    the pattern `clusters-*-final`, and replace `*` with whichever iteration number
    the true directory name contains.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the clustered output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you open the file created by the previous example, `data/kmeans-clusterdump`,
    in any text editor, you''ll see output representing the top terms of the Mahout
    clusters. The file will be large, but an excerpt is provided next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line contains information about the cluster: the ID (in this case
    `VL-11417`) followed by curly braces containing the size of the cluster and the
    location of the cluster centroid. Since the text has been converted to weights
    and numeric IDs, the centroid is impossible to interpret on its own. The top terms
    beneath the centroid description hint at the contents of the cluster, though;
    they''re the terms around which the cluster has coalesced.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The two clusters earlier hint at two clear topics present in the data set, although
    your clusters may be different due to the stochastic nature of the *k*-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your initial centroids and how many iterations you let the algorithm
    run, you may see clusters that appear "better" or "worse" in some respect. This
    will be based on an instinctive response to how well the clustered terms go together.
    But often it's not clear simply by looking at the top terms how well clustering
    has performed. In any case, instinct is not a very reliable way of judging the
    quality of an unsupervised learning algorithm. What we'd ideally like is some
    quantitative measure for how well the clustering has performed.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster evaluation measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the bottom of the file we looked at in the previous section, you''ll see
    some statistics that suggest how well the data has been clustered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: These two numbers can be considered as the equivalent to the variance within
    and the variance between measures we have seen in [Chapter 2](ch02.xhtml "Chapter 2. Inference"),
    *Inference* and [Chapter 3](ch03.xhtml "Chapter 3. Correlation"), *Correlation*.
    Ideally, we are seeking a lower variance (or a higher density) within clusters
    compared to the density between clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Inter-cluster density
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inter-cluster density is the average distance between cluster centroids. Good
    clusters probably don't have centers that are too close to each other. If they
    did, it would indicate the clustering is creating groups with similar features,
    and perhaps drawing distinctions between cluster members that are hard to support.
  prefs: []
  type: TYPE_NORMAL
- en: '![Inter-cluster density](img/7180OS_06_190.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, ideally our clustering will produce clusters with a **large inter-cluster
    distance**.
  prefs: []
  type: TYPE_NORMAL
- en: Intra-cluster density
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By contrast, the intra-cluster density is a measure of how compact the clusters
    are. Ideally, clustering will identify groups of items that are similar to each
    other. Compact clusters indicate that all of the items within a cluster are strongly
    alike.
  prefs: []
  type: TYPE_NORMAL
- en: '![Intra-cluster density](img/7180OS_06_200.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The best clustering outcomes therefore produce compact, distinct clusters with
    a **high intra-cluster density** and a **low inter-cluster density**.
  prefs: []
  type: TYPE_NORMAL
- en: It is not always clear how many clusters are justified by the data, though.
    Consider the following that shows the same dataset grouped into varying numbers
    of clusters. It's hard to say with any degree of confidence what the ideal number
    of clusters is.
  prefs: []
  type: TYPE_NORMAL
- en: '![Intra-cluster density](img/7180OS_06_210.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although the preceding illustration is contrived, it illustrates a general issue
    with clustering data. There is often no one, clear "best" number of clusters.
    The most effective clustering will depend to a large degree on the ultimate use
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: We can however infer which might be better values of *k* by determining how
    the value of some quality score varies with the number of clusters. The quality
    score could be a statistic such as the inter- or intra-cluster density. As the
    number of clusters approaches its ideal, we would expect the value of this quality
    score to improve. Conversely, as the number of clusters diverges from its ideal
    we would expect the quality to decrease. To get a reasonable idea of how many
    clusters are justified in the dataset, therefore, we should run the algorithm
    many times for different values of *k*.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the root mean square error with Parkour
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most common measures of cluster quality is the **sum of squared
    errors** (**SSE**). For each point, the error is the measured distance to the
    nearest cluster centroid. The total clustering SSE is therefore the sum over all
    clusters for a clustered point to its corresponding centroid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Calculating the root mean square error with Parkour](img/7180OS_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *µ*[i] is the centroid of points in cluster *S*[i], *k* is the total number
    of clusters and *n* is the total number of points.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the *RMSE* in Clojure we therefore need to be able to relate each
    point in the cluster to its corresponding cluster centroid. Mahout saves cluster
    centroids and clustered points in two separate files, so in the next section we'll
    combine them.
  prefs: []
  type: TYPE_NORMAL
- en: Loading clustered points and centroids
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a parent directory (e.g. `data/reuters-kmeans/kmeans-10`), the following
    function will load points into vectors stored in a map indexed by cluster ID using
    Parkour's `seqf/dseq` function to load key/value pairs from a sequence file. In
    this case, the key is the cluster ID (as an integer) and the value is the TF-IDF
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding function is a map keyed by cluster ID whose values
    are sequences of clustered points. Likewise, the following function will convert
    each cluster into a map, keyed by cluster ID, whose values are maps containing
    the keys `:id` and `:centroid`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Having two maps keyed by cluster ID means that combining the clustered points
    and cluster centroids is a simple matter of calling `merge-with` on the maps supplying
    a custom merging function. In the following code, we merge the clustered points
    into the map containing the cluster `:id` and `:centroid`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The final output is a single map, keyed by cluster ID, with each value as a
    map of `:id`, `:centroid` and `:points`. We'll use this map in the next section
    to calculate the clustering RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the cluster RMSE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To calculate the RMSE, we need to be able to establish the distance between
    every point and its associated cluster centroid. Since we used Mahout's `CosineDistanceMeasure`
    to perform the initial clustering, we should use the cosine distance to evaluate
    the clustering as well. In fact, we can simply make use of Mahout's implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: If the RMSE is plotted against the number of clusters, you'll find that it declines
    as the number of clusters increases. A single cluster will have the highest RMSE
    error (the variance of the original dataset from the mean), whereas the lowest
    RMSE will be the degenerate case when every point is in its own cluster (an RMSE
    of zero). Clearly either of these extremes will provide a poor explanation for
    the structure of the data. However, the RMSE doesn't decline in a straight line.
    It declines sharply as the number of clusters is increased from 1, but will fall
    more slowly once the "natural" number of clusters has been exceeded.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, one way of judging the ideal number of clusters is to plot how the
    RMSE changes with respect to the number of clusters. This is called the **elbow
    method**.
  prefs: []
  type: TYPE_NORMAL
- en: Determining optimal k with the elbow method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to determine the value of *k* using the elbow method, we're going to
    have to re-run *k*-means a number of times. The following code accomplishes this
    for all *k* between `2` and `21`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'This will take a little while to run, so it might be time to go and make a
    hot drink: the `println` statement will log each clustering run to let you know
    how much progress has been made. On my laptop the whole process takes about 15
    minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it''s complete, you should be able to run the example to generate a scatter
    plot of the RMSE for each of the clustered values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'This should return a plot similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Determining optimal k with the elbow method](img/7180OS_06_220.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding scatter plot shows the RMSE plotted against the number of clusters.
    It should be clear how the rate of RMSE change slows as *k* exceeds around 13
    clusters and increasing the number of clusters further yields diminishing returns.
    Therefore, the preceding chart suggests for our Reuters data that around 13 clusters
    is a good choice.
  prefs: []
  type: TYPE_NORMAL
- en: The elbow method provides an intuitive means to determine the ideal number of
    clusters, but it is sometimes hard to apply in practice. This is because we must
    interpret the shape of the curve defined by the RMSE for each *k*. If *k* is small,
    or the RMSE contains a lot of noise, it may not be apparent where the elbow falls,
    or if there is an elbow at all.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since clustering is an unsupervised learning algorithm, we assume here that
    the internal structure of the clusters is the only means of validating the quality
    of clustering. If true cluster labels are known then it's possible to use external
    validation measures (such as entropy) of the kind that we encountered in [Chapter
    4](ch04.xhtml "Chapter 4. Classification"), *Classification* to validate the success
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Other clustering evaluation schemes aim to provide a clearer means of determining
    the precise number of clusters. The two we'll cover are the Dunn index and the
    Davies-Bouldin index. Both are internal evaluation schemes, meaning that they
    only look at the structure of the clustered data. Each aims to identify the clustering
    that has produced the most compact, well-separated clusters, in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: Determining optimal k with the Dunn index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Dunn index offers an alternative way to choose the optimal number of *k*.
    Rather than considering the average error remaining in the clustered data, the
    Dunn index instead considers the ratio of two "worst-case" situations: the minimum
    distance between two cluster centroids, divided by the maximum cluster diameter.
    A higher index therefore indicates better clustering since in general we would
    like large inter-cluster distances and small intra-cluster distances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For *k* clusters we can express the Dunn index in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Determining optimal k with the Dunn index](img/7180OS_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *δ(C[i],C[j])* distance between the two clusters *C*[i] and *C*[j] and
    ![Determining optimal k with the Dunn index](img/7180OS_06_13.jpg) represents
    the size (or scatter) of the largest cluster.
  prefs: []
  type: TYPE_NORMAL
- en: There are several possible ways to calculate the scatter of a cluster. We could
    take the distance between the furthest two points inside a cluster, or the mean
    of all the pairwise distances between data points inside the cluster, or the mean
    of each data point from the cluster centroid itself. In the following code, we
    calculate the size by taking the median distance from the cluster centroid.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code makes use of the `combinations` function from `clojure.math.combinatorics`
    ([https://github.com/clojure/math.combinatorics/](https://github.com/clojure/math.combinatorics/))
    to produce a lazy sequence of all pairwise combinations of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `dunn-index` function in the preceding code to generate a scatter
    plot for the clusters from *k=2* to *k=20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Determining optimal k with the Dunn index](img/7180OS_06_230.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A higher Dunn index indicates a better clustering. Thus, it appears that the
    best clustering is for *k=2*, followed by *k=6*, with *k=12* and *k=13* following
    closely behind. Let's try an alternative cluster evaluation scheme and compare
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: Determining optimal k with the Davies-Bouldin index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Davies-Bouldin index is an alternative evaluation scheme that measures
    the mean ratio of size and separation for all values in the cluster. For each
    cluster, an alternative cluster is found that maximizes the ratio of the sum of
    cluster sizes divided by the inter-cluster distance. The Davies-Bouldin index
    is defined as the mean of this value for all clusters in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Determining optimal k with the Davies-Bouldin index](img/7180OS_06_14.jpg)![Determining
    optimal k with the Davies-Bouldin index](img/7180OS_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where *δ(C[i],C[j])* is the distance between the two cluster centroids *C*[i]
    and *C*[j], and *S*[i] and *S*[j] are the scatter. We can calculate the Davies-Bouldin
    index using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now plot the Davies-Bouldin on a scatter plot for clusters *k=2* to
    *k=20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'This should generate the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Determining optimal k with the Davies-Bouldin index](img/7180OS_06_240.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Unlike the Dunn index, the Davies-Bouldin index is minimized for good clustering
    schemes since in general we seek out clusters that are compact in size and have
    high inter-cluster distances. The preceding chart suggests that *k=2* is the ideal
    cluster size followed by *k=13*.
  prefs: []
  type: TYPE_NORMAL
- en: The drawbacks of k-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*k*-means is one of the most popular clustering algorithms due to its relative
    ease of implementation and the fact that it can be made to scale well to very
    large datasets. In spite of its popularity, there are several drawbacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-means is stochastic, and does not guarantee to find the global optimum
    solution for clustering. In fact, the algorithm can be very sensitive to outliers
    and noisy data: the quality of the final clustering can be highly dependent on
    the position of the initial cluster centroids. In other words, *k*-means will
    regularly discover a local rather than global minimum.'
  prefs: []
  type: TYPE_NORMAL
- en: '![The drawbacks of k-means](img/7180OS_06_250.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram illustrates how *k*-means may converge to a local minimum
    based on poor initial cluster centroids. Non-optimal clustering may even occur
    if the initial cluster centroids are well-placed, since *k*-means prefers clusters
    with similar sizes and densities. Where clusters are not approximately equal in
    size and density, *k*-means may fail to converge to the most natural clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The drawbacks of k-means](img/7180OS_06_260.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Also, *k*-means strongly prefers clusters that are "globular" in shape. Clusters
    with more intricate shapes are not well-identified by the *k*-means algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we''ll see how a variety of dimensionality reduction techniques
    can help work around these problems. But before we get there, let''s develop an
    intuition for an alternative way of defining distance: as a measure of how far
    away from a "group" of things an element is.'
  prefs: []
  type: TYPE_NORMAL
- en: The Mahalanobis distance measure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We saw at the beginning of the chapter how some distance measures may be more
    appropriate than others, given your data, by showing how the Jaccard, Euclidean,
    and cosine measures relate to data representation. Another factor to consider
    when choosing a distance measure and clustering algorithm is the internal structure
    of your data. Consider the following scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mahalanobis distance measure](img/7180OS_06_270.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s "obvious" that the point indicated by the arrow is distinct from the
    other points. We can clearly see that it''s far from the distribution of the others
    and therefore represents an anomaly. Yet, if we calculate the Euclidean distance
    of all points from the mean (the "centroid"), the point will be lost amongst the
    others that are equivalently far, or even further, away:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mahalanobis distance measure](img/7180OS_06_280.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Mahalanobis distance takes into account the covariance among the variables
    in calculating distances. In two dimensions, we can imagine the Euclidean distance
    as a circle growing out from the centroid: all points at the edge of the circle
    are equidistant from the centroid. The Mahalanobis distance stretches and skews
    this circle to correct for the respective scales of the different variables, and
    to account for correlation amongst them. We can see the effect in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code uses the function provided by `incanter.stats` to plot the
    Mahalanobis distance between the same set of points. The result is shown on the
    following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Mahalanobis distance measure](img/7180OS_06_290.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This chart clearly identifies one point in particular as being much more distant
    than the other points. This matches our perception that this point in particular
    should be considered as being further away from the others.
  prefs: []
  type: TYPE_NORMAL
- en: The curse of dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is one fact that the Mahalanobis distance measure is unable to overcome,
    though, and this is known as the curse of dimensionality. As the number of dimensions
    in a dataset rises, every point tends to become equally far from every other point.
    We can demonstrate this quite simply with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code finds both the minimum and the maximum distance between
    any two pairs of points in a synthetic generated dataset of 100 points. As the
    number of dimensions approaches the number of elements in the set, we can see
    how the minimum and the maximum distance between each pair of elements approach
    one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The curse of dimensionality](img/7180OS_06_300.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The effect is striking: as the number of dimensions increases, the distance
    between the closest two points rises too. The distance between the furthest two
    points rises as well, but at a slower rate. Finally, with 100 dimensions and 100
    data points, every point appears to be equally far from every other.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is synthetic, randomly generated data. If we're attempting to
    cluster our data, we implicitly hope that it will have a discernible internal
    structure that we can tease out. Nonetheless, this structure will become more
    and more difficult to identify as the number of dimensions rises.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned about the process of clustering and covered the
    popular *k*-means clustering algorithm to cluster large numbers of text documents.
  prefs: []
  type: TYPE_NORMAL
- en: This provided an opportunity to cover the specific challenges presented by text
    processing where data is often messy, ambiguous, and high-dimensional. We saw
    how both stop words and stemming can help to reduce the number of dimensions and
    how TF-IDF can help identify the most important dimensions. We also saw how *n*-grams
    and shingling can help to tease out context for each word at the cost of a vast
    proliferation of terms.
  prefs: []
  type: TYPE_NORMAL
- en: We've explored Parkour in greater detail and seen how it can be used to write
    sophisticated, scalable, Hadoop jobs. In particular, we've seen how to make use
    of the distributed cache and custom tuple schemas to write Hadoop job process
    data represented as Clojure data structures. We used both of these to implement
    a method for generating unique, cluster-wide term IDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we witnessed the challenge presented by very high-dimensional spaces:
    the so-called "curse of dimensionality". In the next chapter, we''ll cover this
    topic in more detail and describe a variety of techniques to combat it. We''ll
    continue to explore the concepts of "similarity" and "difference" as we consider
    the problem of recommendation: how we can match users and items together.'
  prefs: []
  type: TYPE_NORMAL
