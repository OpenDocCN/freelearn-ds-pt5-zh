<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 7. Recommender Systems"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch07" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 7. Recommender Systems</h1></div></div></div><div class="calibre2"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote1" summary="Block quote"><tr class="calibre17"><td class="calibre18"> </td><td class="calibre18"><p class="calibre19"><span class="strong1"><em class="calibre13">"People who like this sort of thing will find this the sort of thing they like."</em></span></p></td><td class="calibre18"> </td></tr><tr class="calibre17"><td class="calibre18"> </td><td colspan="2" class="calibre20">--<span class="strong1"><span class="strong1"><em class="calibre13">attributed to Abraham Lincoln</em></span></span></td></tr></table></div><p class="calibre11">In the previous chapter, we performed clustering on text documents using the k-means algorithm. This required us to have a measure of similarity between the text documents to be clustered. In this chapter, we'll be investigating recommender systems and we'll use this notion of similarity to suggest items that we think users might like.</p><p class="calibre11">We also saw the challenge presented by high-dimensional data—the so-called <span class="strong1"><strong class="calibre12">curse of dimensionality</strong></span>. Although it's not a problem specific to recommender systems, this chapter will show a variety of techniques that tackle its effects. In particular, we'll look at the means of establishing the most important dimensions with principle component analysis and singular value decomposition, and probabilistic ways of compressing very high dimensional sets with Bloom filters and MinHash. In addition—because determining the similarity of items with each other involves making many pairwise comparisons—we'll learn how to efficiently precompute groups with the most probable similarity using locality-sensitive hashing.</p><p class="calibre11">Finally, we'll introduce Spark, a distributed computation framework, and an associated Clojure library called Sparkling. We'll show how Sparkling can be used with Spark's machine learning library MLlib to build a distributed recommender system.</p><p class="calibre11">But first, we'll begin this chapter with a discussion on the basic types of recommender systems and implement one of the simplest in Clojure. Then, we'll demonstrate how Mahout, introduced in the previous chapter, can be used to create a variety of different types of recommender.</p><div class="calibre2" title="Download the code and data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec115" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Download the code and data</h1></div></div></div><p class="calibre11">In this chapter, we'll<a id="id854" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> make use of data on film recommendations<a id="id855" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> from the website <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://movielens.org/">https://movielens.org/</a>. The site is run by<a id="id856" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> GroupLens, a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities.</p><p class="calibre11">Datasets have been made available in several different sizes at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://grouplens.org/datasets/movielens/">https://grouplens.org/datasets/movielens/</a>. In this chapter, we'll be making use of "MovieLens 100k"—a collection of 100,000 ratings from 1,000 users on 1,700 movies. As the data <a id="id857" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>was released in 1998, it's beginning to show its age, but it provides a modest dataset on which we can demonstrate the principles of recommender systems. This <a id="id858" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chapter will give you the tools you need to process the more recently released "MovieLens 20M" data: 20 million ratings by 138,000 users on 27,000 movies.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note58" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The code for this chapter is <a id="id859" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>available from the Packt Publishing's website or from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clojuredatascience/ch7-recommender-systems">https://github.com/clojuredatascience/ch7-recommender-systems</a>.</p></div></div><p class="calibre11">As usual, a shell script has been provided that will download and decompress the data to this chapter's <code class="literal">data</code> directory. You can run it from within the same code directory with:</p><div class="calibre2"><pre class="programlisting">
<span class="strong1"><strong class="calibre12">script/download-data.sh</strong></span>
</pre></div><p class="calibre11">Once you've run the script, or downloaded an unpacked data manually, you should see a variety of files beginning with the letter "u". The ratings data we'll be mostly using in this chapter is in the <code class="literal">ua.base</code> file. The <code class="literal">ua.base</code>, <code class="literal">ua.test</code>, <code class="literal">ub.base</code>, and <code class="literal">ub.test</code> files contain subsets of the data to perform cross-validation. We'll also be using the <code class="literal">u.item</code> file, which contains information on the movies themselves.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Inspect the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec116" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Inspect the data</h1></div></div></div><p class="calibre11">The ratings files<a id="id860" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> are tab-separated, containing the field's user ID, item ID, rating, and timestamp. The user ID links to a row in the <code class="literal">u.user</code> file, which provides basic demographic information such as age, sex, and occupation:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-1 []
  (-&gt;&gt; (io/resource "ua.base")
       (io/reader)
       (line-seq)
       (first)))

;; "1\t1\t5\t874965758"</pre></div><p class="calibre11">The string shows a single line from the file—a tab-separated line containing the user ID, item ID, rating (1-5), and timestamp showing when the rating was made. The rating is an integer from 1 to 5 and the timestamp is given as the number of seconds since January 1, 1970. The item ID links to a row in the <code class="literal">u.item</code> file.</p><p class="calibre11">We'll also want to<a id="id861" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> load the <code class="literal">u.item</code> file, so we can determine the names of the items being rated (and the items being predicted in return). The following example shows how data is stored in the <code class="literal">u.item</code> file:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-2 []
  (-&gt;&gt; (io/resource "u.item")
       (io/reader)
       (line-seq)
       (first)))

;; "1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0"</pre></div><p class="calibre11">The first two fields are the item ID and name, respectively. Subsequent fields, not used in this chapter, are the release date, the URL of the movie on IMDB, and a series of flags indicating the genre of the movie.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Parse the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec117" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Parse the data</h1></div></div></div><p class="calibre11">Since the data will all fit in<a id="id862" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the main memory for convenience, we'll define several functions that will load the ratings into Clojure data structures. The <code class="literal">line-&gt;rating</code> function takes a line, splits it into fields where a tab character is found, converts each field to a <code class="literal">long</code> datatype, then uses <code class="literal">zipmap</code> to convert the sequence into a map with the supplied keys:</p><div class="calibre2"><pre class="programlisting">(defn to-long [s]
  (Long/parseLong s))

(defn line-&gt;rating [line]
  (-&gt;&gt; (s/split line #"\t")
       (map to-long)
       (zipmap [:user :item :rating])))

(defn load-ratings [file]
  (with-open [rdr (io/reader (io/resource file))]
    (-&gt;&gt; (line-seq rdr)
         (map line-&gt;rating)
         (into []))))

(defn ex-7-3 []
  (-&gt;&gt; (load-ratings "ua.base")
       (first)))

;; {:rating 5, :item 1, :user 1}</pre></div><p class="calibre11">Let's write a<a id="id863" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> function to parse the <code class="literal">u.items</code> file as well, so that we know what the movie names are:</p><div class="calibre2"><pre class="programlisting">(defn line-&gt;item-tuple [line]
  (let [[id name] (s/split line #"\|")]
    (vector (to-long id) name)))

(defn load-items [path]
  (with-open [rdr (io/reader (io/resource path))]
    (-&gt;&gt; (line-seq rdr)
         (map line-&gt;item-tuple)
         (into {}))))</pre></div><p class="calibre11">The <code class="literal">load-items</code> function returns a map of an item ID to a movie name, so that we can look up the names of movies by their ID.</p><div class="calibre2"><pre class="programlisting">(defn ex-7-4 []
  (-&gt; (load-items "u.item")
      (get 1)))

;; "Toy Story (1995)"</pre></div><p class="calibre11">With these simple functions in place, it's time to learn about the different types of recommender systems.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Types of recommender systems"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec118" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Types of recommender systems</h1></div></div></div><p class="calibre11">There are typically two <a id="id864" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>approaches taken to the problem of recommendation. Both make use of the notion of similarity between things, as we encountered it in the previous chapter.</p><p class="calibre11">One approach is to start with an item we know the user likes and recommend the other items that have similar attributes. For example, if a user is interested in action adventure movies, we might present to them a list of all the action adventure movies that we can offer. Or, if we have more data available than simply the genre—perhaps a list of tags—then we could recommend movies <a id="id865" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that have the most tags in common. This approach is called <span class="strong1"><strong class="calibre12">content-based</strong></span> filtering, because we're using the attributes of the items themselves to generate recommendations for similar items.</p><p class="calibre11">Another approach to recommendation is to take as input some measure of the user's preferences. This may be in the form of numeric ratings for movies, or of movies bought or previously viewed. Once we have this data, we can identify the movies that other users with similar ratings (or purchase history, viewing habits, and so on) have a preference for that the user in question has not already stated a preference for. This approach takes into account the behavior of other users, so it's commonly called <span class="strong1"><strong class="calibre12">collaborative filtering</strong></span>. Collaborative filtering is a powerful means of recommendation, because it harnesses the so-called "wisdom of the crowd".</p><p class="calibre11">In this chapter, we'll primarily<a id="id866" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> study collaborative filtering approaches. However, by harnessing notions of similarity, we'll provide you with the concepts you'll need to implement content-based recommendation as well.</p><div class="calibre2" title="Collaborative filtering"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec147" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Collaborative filtering</h2></div></div></div><p class="calibre11">By taking account only <a id="id867" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the users' relationship to items, these techniques require no knowledge of the properties of the items themselves. This makes collaborative filtering a very general technique—the items in question can be anything that can be rated. We can picture collaborative filtering as the act of trying to fill a sparse matrix containing known ratings for users. We'd like to be able to replace the unknowns with predicted ratings and then recommend the predictions with the highest score.</p><div class="mediaobject"><img src="Images/7180OS_07_090.jpg" alt="Collaborative filtering" class="calibre279"/></div><p class="calibre11">Notice that each question mark sits at the intersection of a row and a column. The rows contain a particular user's preference for all the movies they've rated. The columns contain the ratings for a particular movie from all the users who have rated it. To substitute the question marks in this matrix using only the other numbers in this matrix is the core challenge of <a id="id868" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>collaborative filtering.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Item-based and user-based recommenders"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec119" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Item-based and user-based recommenders</h1></div></div></div><p class="calibre11">Within the field of <a id="id869" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>collaborative filtering, we can usefully make the distinction<a id="id870" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> between two types of filtering—item-based and user-based recommenders. With item-based recommenders, we take a set of items that a user has already rated highly and look for other items that are similar. The process is visualized in the next diagram:</p><div class="mediaobject"><img src="Images/7180OS_07_100.jpg" alt="Item-based and user-based recommenders" class="calibre280"/></div><p class="calibre11">A recommender might recommend item <span class="strong1"><strong class="calibre12">B</strong></span>, based on the information presented in the diagram, since it's similar to two items that are already highly rated.</p><p class="calibre11">We can contrast this approach to the process of a user-based recommendation shown in the following diagram. A user-based recommendation aims to identify users with similar tastes to the user in question to recommend items that they have rated highly, but which the user has not already rated.</p><div class="mediaobject"><img src="Images/7180OS_07_110.jpg" alt="Item-based and user-based recommenders" class="calibre281"/></div><p class="calibre11">The user-based<a id="id871" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> recommender is likely to recommend item <span class="strong1"><strong class="calibre12">B</strong></span>, because it has been rated highly by two other users with similar taste. We'll be implementing both kinds<a id="id872" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of recommenders in this chapter. Let's start with one of the simplest approaches—<span class="strong1"><strong class="calibre12">Slope One</strong></span><a id="id873" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> predictors for item-based recommendation.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Slope One recommenders"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec120" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Slope One recommenders</h1></div></div></div><p class="calibre11">Slope One recommenders <a id="id874" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>are a part of a family of algorithms introduced in a 2005 paper by Daniel Lemire and Anna Maclachlan. In this chapter, we'll introduce the weighted Slope One recommender.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note59" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">You can read the paper<a id="id875" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> introducing the Slope One recommender at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://lemire.me/fr/abstracts/SDM2005.html">http://lemire.me/fr/abstracts/SDM2005.html</a>.</p></div></div><p class="calibre11">To illustrate how weighted Slope One recommendation works, let's consider the simple example of four users, labeled <span class="strong1"><strong class="calibre12">W</strong></span>, <span class="strong1"><strong class="calibre12">X</strong></span>, <span class="strong1"><strong class="calibre12">Y</strong></span>, and <span class="strong1"><strong class="calibre12">Z</strong></span>, who have rated three movies—Amadeus, Braveheart, and<a id="id876" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> Casablanca. The ratings each user has provided are illustrated in the following diagram:</p><div class="mediaobject"><img src="Images/7180OS_07_112.jpg" alt="Slope One recommenders" class="calibre282"/></div><p class="calibre11">As with any recommendation problem, we're looking to replace the question marks with some estimate on how the user would rate the movie: the highest predicted ratings can be used to recommend new movies to users.</p><p class="calibre11">Weighted Slope One is an algorithm in two steps. Firstly, we must calculate the difference between the ratings for every pair of items. Secondly, we'll use this set of differences to make predictions.</p><div class="calibre2" title="Calculating the item differences"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec148" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating the item differences</h2></div></div></div><p class="calibre11">The first step<a id="id877" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of the Slope One algorithm is to calculate the average difference between each pair of items. The following equation may look intimidating but, in fact, it's simple:</p><div class="mediaobject"><img src="Images/7180OS_07_01.jpg" alt="Calculating the item differences" class="calibre283"/></div><p class="calibre11">The formula calculates <span class="inlinemediaobject"><img src="Images/7180OS_07_02.jpg" alt="Calculating the item differences" class="calibre284"/></span>, which is the average difference between the ratings for items <span class="strong1"><em class="calibre13">i</em></span> and <span class="strong1"><em class="calibre13">j</em></span>. It does so by summing over all the <span class="strong1"><em class="calibre13">u</em></span> taken from <span class="strong1"><em class="calibre13">S</em></span><sub class="calibre25">i</sub>,<sub class="calibre25">j</sub><span class="strong1"><em class="calibre13">(R)</em></span>, which is the set of all the users who have rated both the items. The quantity that is summed is <span class="strong1"><em class="calibre13">u</em></span><sub class="calibre25">i</sub><span class="strong1"><em class="calibre13"> - u</em></span><sub class="calibre25">j</sub>, the difference between the user's rating for items <span class="strong1"><em class="calibre13">i</em></span> and <span class="strong1"><em class="calibre13">j</em></span> divided by <span class="inlinemediaobject"><img src="Images/7180OS_07_05.jpg" alt="Calculating the item differences" class="calibre285"/></span>, the cardinality of set <span class="strong1"><em class="calibre13">S</em></span><sub class="calibre25">i</sub>,<sub class="calibre25">j</sub><span class="strong1"><em class="calibre13">(R)</em></span>, or the number of people who have rated both the items.</p><p class="calibre11">Let's make this more concrete by applying the algorithm to the ratings in the previous diagram. Let's calculate the difference between the ratings for "Amadeus" and "Braveheart".</p><p class="calibre11">There are two users <a id="id878" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>who have rated both the movies, so <span class="inlinemediaobject"><img src="Images/7180OS_07_05.jpg" alt="Calculating the item differences" class="calibre285"/></span> is two. For each of these users, we take the difference between their ratings for each of the two movies and add them together.</p><div class="mediaobject"><img src="Images/7180OS_07_06.jpg" alt="Calculating the item differences" class="calibre286"/></div><p class="calibre11">The result is <span class="strong1"><em class="calibre13">2</em></span>, meaning on average, users voted Amadeus two ratings higher than Braveheart. As you might expect, if we calculate the difference in the other direction, between Braveheart and Amadeus, we get <span class="strong1"><em class="calibre13">-2</em></span>:</p><div class="mediaobject"><img src="Images/7180OS_07_07.jpg" alt="Calculating the item differences" class="calibre287"/></div><p class="calibre11">We can think of the result as the average difference in the rating between the two movies, as judged by all the people who have rated both the movies. If we perform the calculation several more times, we could end up with the matrix in the following diagram, which shows the average pairwise difference in the rating for each of the three movies:</p><div class="mediaobject"><img src="Images/7180OS_07_114.jpg" alt="Calculating the item differences" class="calibre288"/></div><p class="calibre11">By definition, the values<a id="id879" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> on the main diagonal are zero. Rather than continuing our calculations manually, we can express the computation in the following Clojure code, which will build up a sequence of differences between the pairs of items that every user has rated:</p><div class="calibre2"><pre class="programlisting">(defn conj-item-difference [dict [i j]]
  (let [difference (-  (:rating j) (:rating i))]
    (update-in dict [(:item i) (:item j)] conj difference)))

(defn collect-item-differences [dict items]
  (reduce conj-item-difference dict
          (for [i items
                j items
                :when (not= i j)]
            [i j])))

(defn item-differences [user-ratings]
  (reduce collect-item-differences {} user-ratings))</pre></div><p class="calibre11">The following example loads the <code class="literal">ua.base</code> file into a sequence of ratings using the functions we defined at the beginning of the chapter. The <code class="literal">collect-item-differences</code> function takes each user's list of ratings and, for each pair of rated items, calculates the difference between the ratings. The <code class="literal">item-differences</code> function reduces over all the users to build up a sequence of pairwise differences between the items for all the users who have rated both the items:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-5 []
  (-&gt;&gt; (load-ratings "ua.base")
       (group-by :user)
       (vals)
       (item-differences)
       (first)))

;; [893 {558 (-2 4), 453 (-1), 637 (-1), 343 (-2 -2 3 2) ...]</pre></div><p class="calibre11">We're storing the lists in both directions as values contained within the nested maps, so we can retrieve the differences between any two items using <code class="literal">get-in</code>:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-6 []
  (let [diffs (-&gt;&gt; (load-ratings "ua.base")
                   (group-by :user)
                   (vals)
                   (item-differences))]
    (println "893:343" (get-in diffs [893 343]))
    (println "343:893" (get-in diffs [343 893]))))

;; 893:343 (-2 -2 3 2)
;; 343:893 (2 2 -3 -2)</pre></div><p class="calibre11">To use these differences<a id="id880" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> for prediction, we'll need to summarize them into a mean and keep track of the count of ratings on which the mean was based:</p><div class="calibre2"><pre class="programlisting">(defn summarize-item-differences [related-items]
  (let [f (fn [differences]
            {:mean  (s/mean differences)
             :count (count  differences)})]
    (map-vals f related-items)))

(defn slope-one-recommender [ratings]
  (-&gt;&gt; (item-differences ratings)
       (map-vals summarize-item-differences)))

(defn ex-7-7 []
  (let [recommender (-&gt;&gt; (load-ratings "ua.base")
                         (group-by :user)
                         (vals)
                         (slope-one-recommender))]
    (get-in recommender [893 343])))

;; {:mean 0.25, :count 4}</pre></div><p class="calibre11">One of the practical benefits of this method is that we have to perform the earlier step only once. From this point onwards, we can incorporate future user ratings by adjusting the mean difference and count only for the items that the user has already rated. For example, if a user has already rated 10 items, which have been incorporated into the earlier data structure, the eleventh<a id="id881" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> rating only requires that we recalculate the differences for the eleven items. It is not necessary to perform the computationally expensive differencing process from scratch to incorporate new information.</p></div><div class="calibre2" title="Making recommendations"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec149" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Making recommendations</h2></div></div></div><p class="calibre11">Now that we have calculated the average differences for each pair of items, we have all we need to recommend new items to users. To see how, let's return to one of our earlier examples.</p><p class="calibre11">User <span class="strong1"><strong class="calibre12">X</strong></span> has already <a id="id882" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>provided ratings for <span class="strong1"><strong class="calibre12">Amadeus</strong></span> and <span class="strong1"><strong class="calibre12">Braveheart</strong></span>. We'd like to infer how they would rate the movie <span class="strong1"><strong class="calibre12">Casablanca</strong></span> so that we can decide whether or not to recommend it to them.</p><div class="mediaobject"><img src="Images/7180OS_07_116.jpg" alt="Making recommendations" class="calibre289"/></div><p class="calibre11">In order to make predictions for a user, we need two things—the matrix of differences we calculated just now and the users' own previous ratings. Given these two things, we can calculate a predicted rating <span class="inlinemediaobject"><img src="Images/7180OS_07_08.jpg" alt="Making recommendations" class="calibre290"/></span> for item <span class="strong1"><em class="calibre13">j</em></span>, given user <span class="strong1"><em class="calibre13">u</em></span>, using the following formula:</p><div class="mediaobject"><img src="Images/7180OS_07_09.jpg" alt="Making recommendations" class="calibre291"/></div><div class="mediaobject"><img src="Images/7180OS_07_10.jpg" alt="Making recommendations" class="calibre292"/></div><p class="calibre11">As before, this equation looks more complicated than it is, so let's step through it, starting with the numerator.</p><p class="calibre11">The <span class="inlinemediaobject"><img src="Images/7180OS_07_11.jpg" alt="Making recommendations" class="calibre293"/></span> expression means that we're summing over all the <span class="strong1"><em class="calibre13">i</em></span> items that user <span class="strong1"><em class="calibre13">u</em></span> has rated (which clearly does not include <span class="strong1"><em class="calibre13">j</em></span>, the item for which we're trying to predict a rating). The sum we calculate is over the difference between the users' rating for <span class="strong1"><em class="calibre13">i</em></span> and <span class="strong1"><em class="calibre13">j</em></span>, plus u's rating for <span class="strong1"><em class="calibre13">i</em></span>. We multiply that quantity by <span class="strong1"><em class="calibre13">C</em></span><sub class="calibre25">j</sub>,<sub class="calibre25">i</sub>—the number of users that rated both.</p><p class="calibre11">The <span class="inlinemediaobject"><img src="Images/7180OS_07_13.jpg" alt="Making recommendations" class="calibre294"/></span> denominator<a id="id883" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is simply the sum of all the users who have rated <span class="strong1"><em class="calibre13">j</em></span> and any of the movies that user <span class="strong1"><em class="calibre13">u</em></span> has rated. It's a constant factor to adjust the size of the numerator downwards to ensure that the output can be interpreted as a rating.</p><p class="calibre11">Let's illustrate the previous formula by calculating the predicted rating of user X for "Casablanca" using the table of differences and the ratings provided earlier:</p><div class="mediaobject"><img src="Images/7180OS_07_14.jpg" alt="Making recommendations" class="calibre295"/></div><div class="mediaobject"><img src="Images/7180OS_07_15.jpg" alt="Making recommendations" class="calibre296"/></div><p class="calibre11">So, given the previous ratings, we would predict that user X would rate Casablanca <span class="strong1"><strong class="calibre12">3.375</strong></span>. By performing the same process for all the items also rated by the people who rated any of the other items rated by user X, we can arrive at a set of recommendations for user X.</p><p class="calibre11">The Clojure code calculates the weighted rating for all such candidates:</p><div class="calibre2"><pre class="programlisting">(defn candidates [recommender {:keys [rating item]}]
  (-&gt;&gt; (get recommender item)
       (map (fn [[id {:keys [mean count]}]]
              {:item id
               :rating (+ rating mean)
               :count count}))))

(defn weighted-rating [[id candidates]]
  (let [ratings-count (reduce + (map :count candidates))
        sum-rating (map #(* (:rating %) (:count %)) candidates)
        weighted-rating (/ (reduce + sum-rating) ratings-count)]
    {:item id
     :rating weighted-rating
     :count  ratings-count}))</pre></div><p class="calibre11">Next, we calculate a weighted rating, which is the weighted average rating for each candidate. The weighted <a id="id884" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>average ensures that the differences generated by large numbers of users count for more than those generated by only a small number of users:</p><div class="calibre2"><pre class="programlisting">(defn slope-one-recommend [recommender rated top-n]
  (let [already-rated  (set (map :item rated))
        already-rated? (fn [{:keys [id]}]
                         (contains? already-rated id))
        recommendations (-&gt;&gt; (mapcat #(candidates recommender %)
                                     rated)
                             (group-by :item)
                             (map weighted-rating)
                             (remove already-rated?)
                             (sort-by :rating &gt;))]
    (take top-n recommendations)))</pre></div><p class="calibre11">Finally, we remove from the candidate pool any items we have already rated and order the remainder by rating descending: we can take just the highest rated results and present these as our top recommendations. The following example calculates the top ratings for user ID 1:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-8 []
  (let [user-ratings (-&gt;&gt; (load-ratings "ua.base")
                          (group-by :user)
                          (vals))
        user-1       (first user-ratings)
        recommender  (-&gt;&gt; (rest user-ratings)
                          (slope-one-recommender))
        items     (load-items "u.item")
        item-name (fn [item]
                    (get items (:item item)))]
    (-&gt;&gt; (slope-one-recommend recommender user-1 10)
         (map item-name))))</pre></div><p class="calibre11">The earlier example will take a while to build the Slope One recommender and output the differences. It will take a couple of minutes, but when it's finished, you should see something like the following:</p><div class="calibre2"><pre class="programlisting">;; ("Someone Else's America (1995)" "Aiqing wansui (1994)"
;;  "Great Day in Harlem, A (1994)" "Pather Panchali (1955)"
;;  "Boys, Les (1997)" "Saint of Fort Washington, The (1993)"
;;  "Marlene Dietrich: Shadow and Light (1996) " "Anna (1996)"
;;  "Star Kid (1997)" "Santa with Muscles (1996)")</pre></div><p class="calibre11">Try running <code class="literal">slope-one-recommender</code> in the REPL and predicting recommendations for multiple users. You'll<a id="id885" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> find that once the differences have been built, making recommendations is very fast.</p></div><div class="calibre2" title="Practical considerations for user and item recommenders"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec150" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Practical considerations for user and item recommenders</h2></div></div></div><p class="calibre11">As we've seen in the previous section, compiling pairwise differences for all items is a time-consuming job. One of the advantages of item-based recommenders is that pairwise differences between items are likely to remain relatively stable over time. The differences matrix need only be<a id="id886" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> calculated periodically. As we've seen, it's possible to incrementally update very easily too; for a user who has already rated 10 items, if they rate an additional item, we only need to adjust the difference for the 11 items they have now rated. We don't need to calculate the differences from <a id="id887" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>scratch whenever we want to update the matrix.</p><p class="calibre11">The runtime of item-based recommenders scales with the number of items they store though. In situations where the number of users is small compared to the number of items, it may be more efficient to implement a user-based recommender. For example content aggregation sites, where items could outnumber users by orders of magnitude, are good candidates for user-based recommendation.</p><p class="calibre11">The <code class="literal">Mahout</code> library, which we encountered in the previous chapter, contains the tools to create a variety of recommenders, including user-based recommenders. Let's look at these next.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Building a user-based recommender with Mahout"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec121" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Building a user-based recommender with Mahout</h1></div></div></div><p class="calibre11">The Mahout library<a id="id888" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> comes with a lot <a id="id889" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of built-in classes, which are designed to work together to assist in building custom recommendation engines. Mahout's functionality to construct recommenders is in the <code class="literal">org.apache.mahout.cf.taste</code> namespace.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note60" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Mahout's recommendation engine capabilities come from the Taste open source project with which it merged in 2008.</p></div></div><p class="calibre11">In the previous chapter, we discovered how to make use of Mahout to cluster with Clojure's Java interop capabilities. In this chapter, we'll make use of Mahout's recommenders with <code class="literal">GenericUserBasedRecommender</code> available in the <code class="literal">org.apache.mahout.cf.taste.impl.recommender</code> package.</p><p class="calibre11">As with many user-based<a id="id890" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> recommenders, we also need to define a similarity metric to quantify how alike two users are. We'll also define<a id="id891" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> a user neighborhood as each user's set of 10 most similar users.</p><p class="calibre11">First, we must load the data. Mahout includes a utility class, <code class="literal">FileDataModel</code>, to load the MovieLens data in the <code class="literal">org.apache.mahout.cf.taste.impl.model.file</code> package, which we use next:</p><div class="calibre2"><pre class="programlisting"> (defn load-model [path]
  (-&gt; (io/resource path)
      (io/file)
      (FileDataModel.)))</pre></div><p class="calibre11">Having loaded the data, we can produce recommendations with the following code:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-9 []
  (let [model        (load-model "ua.base")
        similarity   (EuclideanDistanceSimilarity. model)
        neighborhood (NearestNUserNeighborhood. 10 similarity
                                                model)
        recommender  (GenericUserBasedRecommender. model
                                                   neighborhood
                                                   similarity)
        items     (load-items "u.item")
        item-name (fn [id] (get items id))]
    (-&gt;&gt; (.recommend recommender 1 5)
         (map #(item-name (.getItemID %))))))

;; ("Big Lebowski, The (1998)" "Peacemaker, The (1997)"
;;  "Rainmaker, The (1997)" "Game, The (1997)"
;;  "Cool Hand Luke (1967)")</pre></div><p class="calibre11">The distance metric that we used in the previous example was the Euclidean distance. This places each user in a high-dimensional space defined by the ratings for the movies they have rated.</p><div class="mediaobject"><img src="Images/7180OS_07_120.jpg" alt="Building a user-based recommender with Mahout" class="calibre256"/></div><p class="calibre11">The earlier <a id="id892" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chart places three users <span class="strong1"><strong class="calibre12">X</strong></span>, <span class="strong1"><strong class="calibre12">Y</strong></span>, and <span class="strong1"><strong class="calibre12">Z</strong></span> on a two-dimensional chart according to their ratings for movies <span class="strong1"><strong class="calibre12">A</strong></span> and <span class="strong1"><strong class="calibre12">B</strong></span>. We can see that <a id="id893" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>users <span class="strong1"><strong class="calibre12">Y</strong></span> and <span class="strong1"><strong class="calibre12">Z</strong></span> are more similar to each other, based on these two movies, than they are to user <span class="strong1"><strong class="calibre12">X</strong></span>.</p><p class="calibre11">If we were trying to produce recommendations for user <span class="strong1"><strong class="calibre12">Y</strong></span>, we might reason that other items rated highly by user <span class="strong1"><strong class="calibre12">X</strong></span> would be good candidates.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="k-nearest neighbors"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec122" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>k-nearest neighbors</h1></div></div></div><p class="calibre11">Our Mahout user-based recommender is<a id="id894" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> making recommendations by looking at the neighborhood of the most similar users. This is commonly called <span class="strong1"><strong class="calibre12"><span class="strong1"><em class="calibre13">k</em></span>-nearest neighbors</strong></span> or <span class="strong1"><strong class="calibre12"><span class="strong1"><em class="calibre13">k</em></span>-NN</strong></span>.</p><p class="calibre11">It might appear that a user neighborhood is a lot like the <span class="strong1"><em class="calibre13">k</em></span>-means clusters we encountered in the previous chapter, but this is not quite the case. This is because each user sits at the center of their own neighborhood. With clustering, we aim to establish a smaller number of groupings, but with <span class="strong1"><em class="calibre13">k</em></span>-NN, there are as many neighborhoods as there are users; each user is their own neighborhood centroid.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note61" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Mahout also defines <code class="literal">ThresholdUserNeighbourhood</code> that we could use to construct a neighborhood containing only the users that fall within a certain similarity from each other.</p></div></div><p class="calibre11">The <span class="strong1"><em class="calibre13">k</em></span>-NN algorithm<a id="id895" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> means that we only generate recommendations based on the taste of the <span class="strong1"><em class="calibre13">k</em></span> most similar users. This makes intuitive sense; the users with taste most similar to your own are most likely to offer meaningful recommendations.</p><p class="calibre11">Two questions naturally arise—what's the best neighborhood size? Which similarity measure should we use? To answer these questions, we can turn to Mahout's recommender evaluation capabilities and see how our recommender behaves against our data for a variety of different configurations.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Recommender evaluation with Mahout"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec123" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Recommender evaluation with Mahout</h1></div></div></div><p class="calibre11">Mahout provides a set of classes to help<a id="id896" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> with the task of evaluating our recommender. Like the cross-validation we performed with the <code class="literal">clj-ml</code> library in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Classification">Chapter 4</a>, <span class="strong1"><em class="calibre13">Classification</em></span>, Mahout's evaluation proceeds by splitting the our ratings into two sets: a test set and a training set.</p><p class="calibre11">By training our<a id="id897" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> recommender on the training set and then<a id="id898" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> evaluating its performance on the test set, we can gain an understanding of how well, or poorly, our algorithm is performing against real data. To handle the task of training a model on the training data provided by Mahout's evaluator, we must supply an object conforming to the <code class="literal">RecommenderBuilder</code> interface. The interface defines just one method: <code class="literal">buildRecommender</code>. We can create an anonymous <code class="literal">RecommenderBuilder</code> type using reify:</p><div class="calibre2"><pre class="programlisting">(defn recommender-builder [sim n]
  (reify RecommenderBuilder
    (buildRecommender [this model]
      (let [nhood (NearestNUserNeighborhood. n sim model)]
        (GenericUserBasedRecommender. model nhood sim)))))</pre></div><p class="calibre11">Mahout provides a variety of evaluators in the <code class="literal">org.apache.mahout.cf.taste.impl.eval</code> namespace. In the following code, we construct a root-mean-square error evaluator using the <code class="literal">RMSRecommenderEvaluator</code> class by passing in a recommender builder and the data model that we've loaded:</p><div class="calibre2"><pre class="programlisting">(defn evaluate-rmse [builder model]
  (-&gt; (RMSRecommenderEvaluator.)
      (.evaluate builder nil model 0.7 1.0)))</pre></div><p class="calibre11">The <code class="literal">nil</code> value we pass to evaluate in the preceding code indicates that we aren't supplying a custom model builder, which means the <code class="literal">evaluate</code> function will use the default model builder based on the model we supply. The numbers <code class="literal">0.7</code> and <code class="literal">1.0</code> are the proportion of data<a id="id899" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> used for training, and the proportion of the test data to evaluate on. In the earlier code, we're using 70 percent of the data for training and evaluate the model <a id="id900" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>on 100 percent of what's left. The <span class="strong1"><strong class="calibre12">root mean square error</strong></span> (<span class="strong1"><strong class="calibre12">RMSE</strong></span>) evaluator will calculate the square root of the mean squared error between the<a id="id901" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> predicted rating and the actual rating for each of the test data.</p><p class="calibre11">We can use both of the<a id="id902" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> previous functions to evaluate the performance of the user-based recommender using a Euclidean distance and a neighborhood of 10 like this:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-10 []
  (let [model   (load-model "ua.base")
        builder (recommender-builder 10
                 (EuclideanDistanceSimilarity. model))]
    (evaluate-rmse builder model)))

;; 0.352</pre></div><p class="calibre11">Your result may differ of course, since the evaluation is performed on random subsets of the data.</p><p class="calibre11">We defined the Euclidean distance <span class="strong1"><em class="calibre13">d</em></span> in the previous chapter to be a positive value where zero represents perfect similarity. This could be converted into a similarity measure <span class="strong1"><em class="calibre13">s</em></span> in the following way:</p><div class="mediaobject"><img src="Images/7180OS_07_16.jpg" alt="Recommender evaluation with Mahout" class="calibre297"/></div><p class="calibre11">Unfortunately, the previous measure would bias against users with more rated items in common, since each dimension would provide an opportunity to be further apart. To correct this, Mahout computes the Euclidean similarity as:</p><div class="mediaobject"><img src="Images/7180OS_07_17.jpg" alt="Recommender evaluation with Mahout" class="calibre298"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">n</em></span> is the number of <a id="id903" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>dimensions. As this formula might result in a similarity which exceeds 1, Mahout clips similarities at 1.</p><div class="calibre2" title="Evaluating distance measures"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec151" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Evaluating distance measures</h2></div></div></div><p class="calibre11">We encountered a<a id="id904" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> variety of other distance and similarity measures in the previous chapter; in particular, we made use of the Jaccard, Euclidean, and cosine distances. Mahout includes implementations of these as similarity measures in the <code class="literal">org.apache.mahout.cf.taste.impl.similarity</code> package as <code class="literal">TanimotoCoefficientSimilarity</code>, <code class="literal">EuclideanDistanceSimilarity</code>, and <code class="literal">UncenteredCosineSimilarity</code> respectively.</p><p class="calibre11">We've just evaluated<a id="id905" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the performance of the Euclidean similarity on our ratings data, so let's see how well the others perform. While we're at it, let's try two other similarity measures that Mahout makes available—<code class="literal">PearsonCorrelationSimilarity</code> and <code class="literal">SpearmanCorrelationSimilarity</code>.</p><div class="calibre2" title="The Pearson correlation similarity"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch07lvl3sec19" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The Pearson correlation similarity</h3></div></div></div><p class="calibre11">The Pearson correlation<a id="id906" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> similarity is a similarity measure based on the correlation between users' tastes. The following diagram shows the ratings of two users for three movies <span class="strong1"><strong class="calibre12">A</strong></span>, <span class="strong1"><strong class="calibre12">B</strong></span>, and <span class="strong1"><strong class="calibre12">C</strong></span>.</p><div class="mediaobject"><img src="Images/7180OS_07_140.jpg" alt="The Pearson correlation similarity" class="calibre256"/></div><p class="calibre11">One of the potential drawbacks of the Euclidean distance is that it fails to account for the cases where one user agrees with another precisely in their relative ratings for movies, but tends to be<a id="id907" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> more generous with their rating. Consider the two users in the earlier example. There is perfect correlation between their ratings for movies <span class="strong1"><strong class="calibre12">A</strong></span>, <span class="strong1"><strong class="calibre12">B</strong></span>, and <span class="strong1"><strong class="calibre12">C</strong></span>, but user <span class="strong1"><strong class="calibre12">Y</strong></span> rates the movies more highly than user <span class="strong1"><strong class="calibre12">X</strong></span>. The Euclidean distance between these two users could be calculated with the following formula:</p><div class="mediaobject"><img src="Images/7180OS_07_18.jpg" alt="The Pearson correlation similarity" class="calibre299"/></div><p class="calibre11">Yet, in a sense, they are in complete agreement. Back in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch03.xhtml" title="Chapter 3. Correlation">Chapter 3</a>, <span class="strong1"><em class="calibre13">Correlation</em></span> we calculated the Pearson correlation between two series as:</p><div class="mediaobject"><img src="Images/7180OS_07_19.jpg" alt="The Pearson correlation similarity" class="calibre86"/></div><p class="calibre11">Here, <span class="inlinemediaobject"><img src="Images/7180OS_07_20.jpg" alt="The Pearson correlation similarity" class="calibre300"/></span> and <span class="inlinemediaobject"><img src="Images/7180OS_07_21.jpg" alt="The Pearson correlation similarity" class="calibre301"/></span>. The example given earlier yields a Pearson correlation of 1.</p><p class="calibre11">Let's try making <a id="id908" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>predictions with the Pearson correlation similarity. Mahout implements the Pearson correlation with the <code class="literal">PearsonCorrelationSimilarity</code> class:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-11 []
  (let [model   (load-model "ua.base")
        builder (recommender-builder
                 10 (PearsonCorrelationSimilarity. model))]
    (evaluate-rmse builder model)))

;; 0.796</pre></div><p class="calibre11">In fact, the RMSE has increased for the movies data using the Pearson correlation.</p><p class="calibre11">The Pearson correlation similarity is mathematically equivalent to the cosine similarity for data which have been centered (data for which the mean is zero). In the example of our two users <span class="strong1"><em class="calibre13">X</em></span> and <span class="strong1"><em class="calibre13">Y</em></span> illustrated earlier, the means are not identical, so the cosine similarity measure would give a different result to the Pearson correlation similarity. Mahout implements the cosine similarity as <code class="literal">UncenteredCosineSimilarity</code>.</p><p class="calibre11">Although the Pearson method makes intuitive sense, it has some drawbacks in the context of recommendation engines. It doesn't take into account the number of rated items that two users have in common. If they only share one item, then no similarity can be computed. Also, if one user always gives items the same rating, then no correlation can be computed between the user and any other user, even another user who does the same. Perhaps there's simply not enough variety of ratings in the data for the Pearson correlation similarity to work well.</p></div><div class="calibre2" title="Spearman's rank similarity"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch07lvl3sec20" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Spearman's rank similarity</h3></div></div></div><p class="calibre11">Another way in<a id="id909" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> which users may be similar is that the rankings are not particularly closely correlated, but the ordering of the ranks are preserved between users. Consider the following diagram showing the ratings of two users for five different movies:</p><div class="mediaobject"><img src="Images/7180OS_07_160.jpg" alt="Spearman's rank similarity" class="calibre256"/></div><p class="calibre11">We can see that the linear correlation between users' ratings is not perfect, since their ratings aren't plotted on a straight line. This would result in a moderate Pearson correlation<a id="id910" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> similarity and an even lower cosine similarity. Yet, the ordering between their preferences is identical. If we were to compare a ranked list of users' preferences, they would be exactly the same.</p><p class="calibre11">The Spearman's rank correlation coefficient uses this measure to calculate the difference between users. It is defined as the Pearson correlation coefficient between the ranked items:</p><div class="mediaobject"><img src="Images/7180OS_07_22.jpg" alt="Spearman's rank similarity" class="calibre57"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">n</em></span> is the number of ratings and <span class="inlinemediaobject"><img src="Images/7180OS_07_23.jpg" alt="Spearman's rank similarity" class="calibre302"/></span> is the difference between the ranks for item <span class="strong1"><em class="calibre13">i</em></span>. Mahout implements the Spearman's rank correlation similarity with the <code class="literal">SpearmanCorrelationSimilarity</code> class which we use in the next code. The algorithm has much more work to do, so we evaluate on a much smaller subset, just 10 percent of the test data:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-12 []
  (let [model   (load-model "ua.base")
        builder (recommender-builder
                 10 (SpearmanCorrelationSimilarity. model))]
    (-&gt; (RMSRecommenderEvaluator.)
        (.evaluate builder nil model 0.9 0.1))))

;; 0.907</pre></div><p class="calibre11">The RMSE <a id="id911" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>evaluation score is even higher than it is for the Pearson correlation similarity. It appears that the best similarity measure so far for the MovieLens data is the Euclidean similarity.</p></div></div><div class="calibre2" title="Determining optimum neighborhood size"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec152" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Determining optimum neighborhood size</h2></div></div></div><p class="calibre11">One aspect we <a id="id912" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>haven't altered in the earlier comparisons is the size of the user neighborhood on which the recommendations are based. Let's see how the RMSE is affected by the neighborhood size:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-13 []
  (let [model (load-model "ua.base")
        sim   (EuclideanDistanceSimilarity. model)
        ns    (range 1 10)
        stats (for [n ns]
                (let [builder (recommender-builder n sim)]
                  (do (println n)
                      (evaluate-rmse builder model))))]
    (-&gt; (c/scatter-plot ns stats
                        :x-label "Neighborhood size"
                        :y-label "RMSE")
        (i/view))))</pre></div><p class="calibre11">The previous code creates a scatterplot of the RMSE for the Euclidean similarity as the neighborhood increases from 1 to 10.</p><div class="mediaobject"><img src="Images/7180OS_07_165.jpg" alt="Determining optimum neighborhood size" class="calibre45"/></div><p class="calibre11">Perhaps surprisingly, as<a id="id913" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the size of the neighborhood grows, the RMSE of the predicted rating rises. The most accurate predicted ratings are based on a neighborhood of just two people. But, perhaps this should not surprise us: for the Euclidean similarity, the most similar other users are defined as being the users who most closely agree with a user's ratings. The larger the neighborhood, the more diverse a range of ratings we'll observe for the same item.</p><p class="calibre11">The earlier RMSE ranges between <span class="strong1"><strong class="calibre12">0.25</strong></span> and <span class="strong1"><strong class="calibre12">0.38</strong></span>. On this basis alone, it's hard to know if the recommender is performing well or not. Does getting the rating wrong by <span class="strong1"><strong class="calibre12">0.38</strong></span> matter much in practice? For example, if we always guess a rating that's exactly <span class="strong1"><strong class="calibre12">0.38</strong></span> too high (or too low), we'll be making recommendations of a relative value that precisely agrees with the users' own. Fortunately, Mahout supplies an alternative evaluator that returns a variety of statistics from the<a id="id914" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> field of information retrieval. We'll look at these next.</p></div><div class="calibre2" title="Information retrieval statistics"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec153" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Information retrieval statistics</h2></div></div></div><p class="calibre11">One way for us to get a<a id="id915" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> better handle on how to improve our recommendations is to use an evaluator that provides more detail on how well the<a id="id916" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> evaluator is performing in a number of different aspects. The <code class="literal">GenericRecommenderIRStatsEvaluator</code> function includes several information retrieval statistics that provide this detail.</p><p class="calibre11">In many cases, it's not necessary to guess the exact rating that a user would have assigned a movie; presenting an ordered list from best to worst is enough. In fact, even the exact order may not be particularly important either.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note62" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Information retrieval systems are those which return results in response to user queries. Recommender systems can be considered a subset of information retrieval systems where the query is the set of prior ratings associated with the user.</p></div></div><p class="calibre11">The<a id="id917" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> <span class="strong1"><strong class="calibre12">Information Retrieval statistics</strong></span> (<span class="strong1"><strong class="calibre12">IR stats</strong></span>) evaluator treats recommendation evaluation a bit like search engine evaluation. A search engine should strive to return as many of the results that the user is looking for without also returning a lot of unwanted information. These proportions are quantified by the statistics precision and recall.</p><div class="calibre2" title="Precision"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch07lvl3sec21" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Precision</h3></div></div></div><p class="calibre11">The precision of an<a id="id918" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> information<a id="id919" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> retrieval system is the percentage of items it returns that are relevant. If the correct recommendations<a id="id920" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> are the <span class="strong1"><strong class="calibre12">true positives</strong></span> and the incorrect<a id="id921" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> recommendations are the <span class="strong1"><strong class="calibre12">false positives</strong></span>, then the precision can be measured as the total number of <span class="strong1"><strong class="calibre12">true positives</strong></span> returned:</p><div class="mediaobject"><img src="Images/7180OS_07_24.jpg" alt="Precision" class="calibre303"/></div><p class="calibre11">Since we return a defined number of recommendations, for example, the top 10, we would talk about the precision at 10. For example, if the model returns 10 recommendations, eight of which were a part of the users' true top 10, the model's precision is 80 <a id="id922" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>percent at 10.</p></div><div class="calibre2" title="Recall"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch07lvl3sec22" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Recall</h3></div></div></div><p class="calibre11">Recall complements<a id="id923" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> precision and the two measures are often quoted together. Recall measures the fraction of relevant recommendations that are returned:</p><div class="mediaobject"><img src="Images/7180OS_07_25.jpg" alt="Recall" class="calibre304"/></div><p class="calibre11">We could think of this as being the proportion of possible good recommendations the recommender actually made. For example, if the system only recommended five of the user's top 10 movies, then we could say the recall was 50 percent at 10.</p></div></div><div class="calibre2" title="Mahout's information retrieval evaluator"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec154" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Mahout's information retrieval evaluator</h2></div></div></div><p class="calibre11">The statistics of information retrieval can reframe<a id="id924" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the recommendation problem as a search problem on a user-by-user basis. Rather than divide the data into test and training sets randomly, <code class="literal">GenericRecommenderIRStatsEvaluator</code> evaluates the performance of the recommender for each user. It <a id="id925" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>does this by removing some quantity of the users' top-rated items (say, the top five). The evaluator will then see how many of the users' true top-five rated items were actually recommended by the system.</p><p class="calibre11">We implement this as follows:</p><div class="calibre2"><pre class="programlisting">(defn evaluate-ir [builder model]
  (-&gt; (GenericRecommenderIRStatsEvaluator.)
      (.evaluate builder nil model nil 5
         GenericRecommenderIRStatsEvaluator/CHOOSE_THRESHOLD
         1.0)
      (bean)))

(defn ex-7-14 []
  (let [model   (load-model "ua.base")
        builder (recommender-builder
                 10 (EuclideanDistanceSimilarity. model))]
    (evaluate-ir builder model)))</pre></div><p class="calibre11">The "at" value in the preceding code is <code class="literal">5</code>, which we pass immediately before the <code class="literal">GenericRecommenderIRStatsEvaluator/CHOOSE_THRESHOLD</code> that causes Mahout to compute a <a id="id926" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sensible relevance threshold. The previous code returns the following output:</p><div class="calibre2"><pre class="programlisting">;; {:recall 0.002538071065989847, :reach 1.0,
;;  :precision 0.002538071065989847,
;;  :normalizedDiscountedCumulativeGain 0.0019637198336778725,
;;  :fallOut 0.0011874376015289575,
;;  :f1Measure 0.002538071065989847,
;;  :class org.apache.mahout.cf.taste.impl.eval.IRStatisticsImpl}</pre></div><p class="calibre11">The evaluator returns an instance of <code class="literal">org.apache.mahout.cf.taste.eval.IRStatistics</code>, which we can convert into a map with Clojure's <code class="literal">bean</code> function. The map contains all the information retrieval statistics calculated by the evaluator. Their meaning is explained in the next section.</p><div class="calibre2" title="F-measure and the harmonic mean"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch07lvl3sec23" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>F-measure and the harmonic mean</h3></div></div></div><p class="calibre11">Also called<a id="id927" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the <span class="strong1"><strong class="calibre12">F1 measure</strong></span> <a id="id928" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>or the<a id="id929" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> <span class="strong1"><strong class="calibre12">balanced F-score</strong></span>, the F-measure<a id="id930" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is the weighted harmonic mean of precision and recall:</p><div class="mediaobject"><img src="Images/7180OS_07_26.jpg" alt="F-measure and the harmonic mean" class="calibre305"/></div><p class="calibre11">The harmonic mean is related to the more common arithmetic mean and, in fact, is one of the three Pythagorean means. It's defined as the reciprocal of the arithmetic mean of the reciprocals and it's particularly useful in situations involving rates and ratios.</p><p class="calibre11">For example, consider a vehicle traveling a distance <span class="strong1"><em class="calibre13">d</em></span> at a certain speed <span class="strong1"><em class="calibre13">x</em></span>, then travelling distance <span class="strong1"><em class="calibre13">d</em></span> again at speed <span class="strong1"><em class="calibre13">y</em></span>. Speed is measured as a ratio of distance traveled over time taken and therefore the average speed is the harmonic mean of <span class="strong1"><em class="calibre13">x</em></span> and <span class="strong1"><em class="calibre13">y</em></span>. If <span class="strong1"><em class="calibre13">x</em></span> is 60 mph and <span class="strong1"><em class="calibre13">y</em></span> is 40 mph, then the average speed is 48 mph, which we can calculate like this:</p><div class="mediaobject"><img src="Images/7180OS_07_27.jpg" alt="F-measure and the harmonic mean" class="calibre306"/></div><p class="calibre11">Note that this is lower than the arithmetic mean, which would be 50 mph. If instead <span class="strong1"><em class="calibre13">d</em></span> represented a certain amount of time rather than distance, so the vehicle traveled for a certain amount of time at speed <span class="strong1"><em class="calibre13">x</em></span> and then the same amount of time at speed <span class="strong1"><em class="calibre13">y</em></span>, then its average speed would be the arithmetic mean of <span class="strong1"><em class="calibre13">x</em></span> and <span class="strong1"><em class="calibre13">y</em></span>, or 50 mph.</p><p class="calibre11">The F-Measure<a id="id931" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> can be generalized to the <span class="strong1"><em class="calibre13">F</em></span><sub class="calibre25">β</sub>-measure that allows the weight associated with either precision or recall to be<a id="id932" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> adjusted independently:</p><div class="mediaobject"><img src="Images/7180OS_07_28.jpg" alt="F-measure and the harmonic mean" class="calibre307"/></div><p class="calibre11">Common measures are <span class="strong1"><em class="calibre13">F</em></span><sub class="calibre25">2</sub>, which weights recall twice as much as precision, and <span class="strong1"><em class="calibre13">F</em></span><sub class="calibre25">0.5</sub>, which weights precision twice as much as recall.</p></div><div class="calibre2" title="Fall-out"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch07lvl3sec24" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Fall-out</h3></div></div></div><p class="calibre11">Also called the <span class="strong1"><strong class="calibre12">false positive</strong></span> rate, the proportion of nonrelevant recommendations that are retrieved out<a id="id933" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of all the nonrelevant recommendations:</p><div class="mediaobject"><img src="Images/7180OS_07_29.jpg" alt="Fall-out" class="calibre308"/></div><p class="calibre11">Unlike the<a id="id934" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> other IR statistics we've seen so far, the lower the fall-out, the better our recommender is doing.</p></div><div class="calibre2" title="Normalized discounted cumulative gain"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch07lvl3sec25" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Normalized discounted cumulative gain</h3></div></div></div><p class="calibre11">The <span class="strong1"><strong class="calibre12">Discounted</strong></span><a id="id935" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12"> Cumulative Gain</strong></span> (<span class="strong1"><strong class="calibre12">DCG</strong></span>) is a measure of the performance of a recommendation system<a id="id936" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> based on the graded relevance of the recommended entities. It varies <a id="id937" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>between zero and one, with one representing perfect ranking.</p><p class="calibre11">The premise of discounted cumulative gain is that highly relevant results appearing lower in a search result list should be penalized as a function of both their relevance and how far down<a id="id938" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the result list they appear. It can be calculated with the following formula:</p><div class="mediaobject"><img src="Images/7180OS_07_30.jpg" alt="Normalized discounted cumulative gain" class="calibre309"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">rel</em></span><sub class="calibre25">i</sub> is the relevance of the result at position <span class="strong1"><em class="calibre13">i</em></span> and <span class="strong1"><em class="calibre13">p</em></span> is the position in the rank. The version presented earlier is a popular formulation that places strong emphasis on retrieving relevant results.</p><p class="calibre11">Since the search result lists vary in length depending on the query, we can't consistently compare results using the DCG alone. Instead, we can sort the result by their relevance and calculate the DCG <a id="id939" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>again. Since this will give the best possible cumulative discounted gain for the results (as we sorted them in the order of relevance), the result is called the <a id="id940" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">Ideal Discounted Cumulative Gain</strong></span> (<span class="strong1"><strong class="calibre12">IDCG</strong></span>).</p><p class="calibre11">Taking the ratio of the DCG and the IDCG gives the normalized discounted cumulative gain:</p><div class="mediaobject"><img src="Images/7180OS_07_31.jpg" alt="Normalized discounted cumulative gain" class="calibre310"/></div><p class="calibre11">In a perfect ranking algorithm, the <span class="strong1"><em class="calibre13">DCG</em></span> will equal the <span class="strong1"><em class="calibre13">IDCG</em></span> resulting in an <span class="strong1"><em class="calibre13">nDCG</em></span> of 1.0. Since the <span class="strong1"><em class="calibre13">nDCG</em></span> provides a result in the range of zero to one, it provides a means to compare the relative performance of different query engines, where each returns different numbers of results.</p></div><div class="calibre2" title="Plotting the information retrieval results"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch07lvl3sec26" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Plotting the information retrieval results</h3></div></div></div><p class="calibre11">We can plot the results <a id="id941" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the information retrieval evaluation with the following code:</p><div class="calibre2"><pre class="programlisting">(defn plot-ir [xs stats]
  (-&gt; (c/xy-plot xs (map :recall stats)
                 :x-label "Neighbourhood Size"
                 :y-label "IR Statistic"
                 :series-label "Recall"
                 :legend true)
      (c/add-lines xs (map :precision stats)
                   :series-label "Precision")
      (c/add-lines xs
                   (map :normalizedDiscountedCumulativeGain stats)
                   :series-label "NDCG")
      (i/view)))

(defn ex-7-15 []
  (let [model   (load-model "ua.base")
        sim     (EuclideanDistanceSimilarity. model)
        xs      (range 1 10)
        stats   (for [n xs]
                  (let [builder (recommender-builder n sim)]
                    (do (println n)
                        (evaluate-ir builder model))))]
    (plot-ir xs stats)))</pre></div><p class="calibre11">This generates<a id="id942" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the following chart:</p><div class="mediaobject"><img src="Images/7180OS_07_168.jpg" alt="Plotting the information retrieval results" class="calibre45"/></div><p class="calibre11">In the previous chart, we can see that the highest precision corresponds to a neighborhood size of two; consulting <a id="id943" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the most similar user generates the fewest false positives. You may have noticed, though that the values reported for precision and recall are quite low. As the neighborhood grows larger, the recommender will have more candidate recommendations to make. Remember, however, that the information retrieval statistics are calculated at 5, meaning that only the top five recommendations will be counted.</p><p class="calibre11">There's a subtle problem concerning these measures in the context of recommenders—the precision is based entirely on <span class="strong1"><em class="calibre13">how well we can predict the other items the user has rated</em></span>. The recommender will be penalized for making recommendations for rare items that the user has not rated, even if they are brilliant recommendations for items the user would love.</p></div></div><div class="calibre2" title="Recommendation with Boolean preferences"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec155" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Recommendation with Boolean preferences</h2></div></div></div><p class="calibre11">There's been <a id="id944" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>an assumption throughout this chapter that the rating a user gives to an item is an important fact. The distance measures we've been looking at so far attempt in different ways to predict the numeric value of a user's future rating.</p><p class="calibre11">An alternative distance measure takes the view that the rating a user assigns to an item is much less important than the fact that they rated it at all. In other words, all ratings, even poor ones, could be treated the same. Consider that, for every movie a user rates poorly, there are many more that the user will not even bother to watch—let alone rate. There are many other situations where Boolean preferences are the primary basis on which a recommendation is made; user's likes or favorites on social media, for example.</p><p class="calibre11">To use a Boolean similarity measure, we first have to convert our model into a Boolean preferences model, which we can do with the following code:</p><div class="calibre2"><pre class="programlisting">(defn to-boolean-preferences [model]
  (-&gt; (GenericBooleanPrefDataModel/toDataMap model)
      (GenericBooleanPrefDataModel.)))

(defn boolean-recommender-builder [sim n]
  (reify RecommenderBuilder
    (buildRecommender [this model]
      (let [nhood (NearestNUserNeighborhood. n sim model)]
        (GenericBooleanPrefUserBasedRecommender.
         model nhood sim)))))</pre></div><p class="calibre11">Treating a user's ratings as Boolean values can reduce the user's list of movie ratings to a set representation and, as we saw in the previous chapter, the Jaccard index can be used to determine set similarity. Mahout implements a similarity measure that's closely related to the Jaccard index called the<a id="id945" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> <span class="strong1"><strong class="calibre12">Tanimoto coefficient</strong></span>.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note63" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The Tanimoto coefficient applies to vectors where each index represents a feature that can be zero or one, whereas the Jaccard index applies to sets which may contain, or not contain, an element. Which measure to use depends only on your data representation—the two measures are equivalent.</p></div></div><p class="calibre11">Let's plot the IR statistics for several different neighborhood sizes using Mahout's IR statistics evaluator:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-16 []
  (let [model   (to-boolean-preferences (load-model "ua.base"))
        sim     (TanimotoCoefficientSimilarity. model)
        xs      (range 1 10)
        stats   (for [n xs]
                  (let [builder
                        (boolean-recommender-builder n sim)]
                    (do (println n)
                        (evaluate-ir builder model))))]
    (plot-ir xs stats)))</pre></div><p class="calibre11">The previous<a id="id946" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_07_169.jpg" alt="Recommendation with Boolean preferences" class="calibre45"/></div><p class="calibre11">For a Boolean recommender, a larger neighborhood improves the precision score. This is an intriguing result, given what we observed for the Euclidean similarity. Bear in mind though that with Boolean preferences, there is no notion of relative item preference, they are either rated or not rated. The most similar users, and therefore the group forming a neighborhood, will be the ones who have simply rated the same items. The larger this group is, the more chance we will have of predicting the items a user rated.</p><p class="calibre11">Also, because<a id="id947" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> there's no relative score for Boolean preferences, the normalized discounted cumulative gain is missing from the earlier chart. The lack of order might make Boolean preferences seem less desirable than the other data, but they can be very useful, as we'll see next.</p><div class="calibre2" title="Implicit versus explicit feedback"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch07lvl3sec27" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Implicit versus explicit feedback</h3></div></div></div><p class="calibre11">In fact, rather than<a id="id948" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> trying to elicit explicit ratings from users on what they like and dislike, a common technique is to simply observe user activity. For example, on an E-commerce site, the set of items viewed could provide an indicator of the sort of products a user is interested in. In the same way, the list of pages a user browses on a website is a strong indicator of the sort of content they're interested in reading.</p><p class="calibre11">Using implicit sources such as clicks and page views can vastly increase the amount of information on which to base predictions. It also avoids the so-called "cold start" problem, where a user must provide explicit ratings before you can offer any recommendations at all; the user will begin generating data as soon as they arrive on your site.</p><p class="calibre11">In these cases, each page view could be treated as an element in a large set of pages representing the users' preferences, and a Boolean similarity measure could be used to recommend related content. For a popular site, such sets will clearly grow very large very quickly. Unfortunately, Mahout 0.9's recommendation engines are designed to run on a single server in memory. So, they impose a limit on the quantity of data we can process.</p><p class="calibre11">Before we look at an alternative recommender that's designed to run on a cluster of machines and scale with the volume of data you have, let's take a detour to look at the ways of performing dimensionality reduction. We'll begin with the ways of probabilistically reducing the size of very large sets.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Probabilistic methods for large sets"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec124" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Probabilistic methods for large sets</h1></div></div></div><p class="calibre11">Large sets appear in<a id="id949" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> many contexts in data science. We're likely to encounter them while dealing with users' implicit feedback as previously mentioned, but the approaches described next can be applied to any data that can be represented as a set.</p><div class="calibre2" title="Testing set membership with Bloom filters"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec156" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Testing set membership with Bloom filters</h2></div></div></div><p class="calibre11">Bloom filters are data structures that provide a means to compress the size of a set while preserving our ability<a id="id950" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to tell whether a given item is a member of the set or not. The price of this compression is some uncertainty. A Bloom filter <a id="id951" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>tells us when an item may be in a set, although it will tell us for certain if it isn't. In situations where disk space saving is worth the small sacrifice in certainty, they are a very popular choice for set compression.</p><p class="calibre11">The base data structure of a Bloom filter is a bit vector—a sequence of cells that may contain 1 or 0 (or true or false). The<a id="id952" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> level of compression (and the corresponding increase in uncertainty) is configurable with two <a id="id953" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>parameters—<span class="strong1"><strong class="calibre12">k hash functions</strong></span> and <span class="strong1"><strong class="calibre12">m bits</strong></span>.</p><div class="mediaobject"><img src="Images/7180OS_07_190.jpg" alt="Testing set membership with Bloom filters" class="calibre269"/></div><p class="calibre11">The previous diagram illustrates the process of taking an input item (the top square) and hashing it multiple times. Each hash function outputs an integer, which is used as an index into the bit vector. The elements matching the hash indices are set to 1. The following illustration shows a different element being hashed into a different bit vector, generating a different set of indices that will be assigned the value 1:</p><div class="mediaobject"><img src="Images/7180OS_07_200.jpg" alt="Testing set membership with Bloom filters" class="calibre288"/></div><p class="calibre11">We can implement <a id="id954" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Bloom filters using the following<a id="id955" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> Clojure. We're using Google's implementation of MurmurHash with different seeds to provide <span class="strong1"><em class="calibre13">k</em></span> different hash functions:</p><div class="calibre2"><pre class="programlisting">(defn hash-function [m seed]
  (fn [x]
    (-&gt; (Hashing/murmur3_32 seed)
        (.hashUnencodedChars x)
        (.asInt)
        (mod m))))

(defn hash-functions [m k]
  (map (partial hash-function m) (range k)))

(defn indices-fn [m k]
  (let [f (apply juxt (hash-functions m k))]
    (fn [x]
      (f x))))

(defn bloom-filter [m k]
  {:filter     (vec (repeat m false))
   :indices-fn (indices-fn m k)})</pre></div><p class="calibre11">The earlier code defines a Bloom filter as a map containing a <code class="literal">:filter</code> (the bit vector) and an <code class="literal">:indices</code> function. The <code class="literal">indices</code> function<a id="id956" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> handles the task of applying the <span class="strong1"><em class="calibre13">k</em></span> hash functions to generate <span class="strong1"><em class="calibre13">k</em></span> indices. We're representing the 0s as <code class="literal">false</code> and the 1s as <code class="literal">true</code>, but the effect is the <a id="id957" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>same. We use the code to create a Bloom filter of length <code class="literal">8</code> with <code class="literal">5</code> hash functions in the following example:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-17 []
  (bloom-filter 8 5))

;; {:filter [false false false false false false false false],
;;  :indices-fn #&lt;Bloom_filter$indices_fn$fn__43538 
;;  cljds.ch7.Bloom_filter$indices_fn$fn__43538@3da200c&gt;}</pre></div><p class="calibre11">The response is a<a id="id958" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> map of two keys—the filter itself (a vector of Boolean values, all false), and an indices function, which has been generated from five hash functions. We can bring the earlier code together with a simple <code class="literal">Bloom-assoc</code> function:</p><div class="calibre2"><pre class="programlisting">(defn set-bit [seq index]
  (assoc seq index true))

(defn set-bits [seq indices]
  (reduce set-bit seq indices))

(defn bloom-assoc [{:keys [indices-fn] :as bloom} element]
  (update-in bloom [:filter] set-bits (indices-fn element)))</pre></div><p class="calibre11">Given a Bloom filter, we simply call the <code class="literal">indices-fn</code> function to get the indices we need to set in the Bloom filter:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-18 []
  (-&gt; (bloom-filter 8 5)
      (bloom-assoc "Indiana Jones")
      (:filter)))

;; [true true false true false false false true]</pre></div><p class="calibre11">To determine whether the Bloom filter contains an item, we simply need to query whether all of the indices that should be true are actually true. If they are, we reason that the item has been added to the filter:</p><div class="calibre2"><pre class="programlisting">(defn bloom-contains? [{:keys [filter indices-fn]} element]
  (-&gt;&gt; (indices-fn element)
       (map filter)
       (every? true?)))

(defn ex-7-19 []
  (-&gt; (bloom-filter 8 5)
      (bloom-assoc "Indiana Jones")
      (bloom-contains? "Indiana Jones")))

;; true</pre></div><p class="calibre11">We add <code class="literal">"Indiana Jones"</code> to the Bloom filter and find that it contains <code class="literal">"Indiana Jones"</code>. Let's instead search for another of Harrison Ford's movies <code class="literal">"The Fugitive"</code>:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-20 []
  (-&gt; (bloom-filter 8 5)
      (bloom-assoc "Indiana Jones")
      (bloom-contains? "The Fugitive")))

;; false</pre></div><p class="calibre11">So far, so good. But we have<a id="id959" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> traded some<a id="id960" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> accuracy for this huge compression. Let's search for a movie that shouldn't be in the Bloom filter. Perhaps, the 1996 movie <code class="literal">Bogus</code>:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-21 []
  (-&gt; (bloom-filter 8 5)
      (bloom-assoc "Indiana Jones")
      (bloom-contains? "Bogus (1996)")))

;; true</pre></div><p class="calibre11">This is not what we want. The filter claims to contain <code class="literal">"Bogus (1996)"</code>, even though we haven't associated it into the filter yet. This is the tradeoff that Bloom filters make; although a filter will never claim that an item hasn't been added when it has, it may incorrectly claim that an item has been added when it hasn't.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note64" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">In the information retrieval terminology we encountered earlier in the chapter, Bloom filters have 100 percent recall, but their precision is less than 100 percent. How much less is configurable through the values we choose for <span class="strong1"><em class="calibre13">m</em></span> and <span class="strong1"><em class="calibre13">k</em></span>.</p></div></div><p class="calibre11">In all, there are 56 movie titles out of the 1,682 titles in the MovieLens dataset that the Bloom filter incorrectly reports on after adding "Indiana Jones"—a 3.3 percent false positive rate. Given that we are only using five hash functions and an eight element filter, you may have expected it to be much higher. Of course, our Bloom filter only contains one element and, as we add more, the probability of obtaining a collision will rise sharply. In fact, the probability of a false positive is approximately:</p><div class="mediaobject"><img src="Images/7180OS_07_32.jpg" alt="Testing set membership with Bloom filters" class="calibre311"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">k</em></span> and <span class="strong1"><em class="calibre13">m</em></span> are the number of hash functions and the length of the filter as it was before, and <span class="strong1"><em class="calibre13">n</em></span> is the <a id="id961" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of items added to the set. For our earlier singular Bloom, this gives:</p><div class="mediaobject"><img src="Images/7180OS_07_33.jpg" alt="Testing set membership with Bloom filters" class="calibre312"/></div><p class="calibre11">So, in fact, the theoretical false positive rate is even lower than what we've observed.</p><p class="calibre11">Bloom filters are a very general algorithm, and are very useful when we want to test set membership and don't have the resources to store all the items in the set explicitly. The fact that the precision<a id="id962" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is configurable through the choice of values for <span class="strong1"><em class="calibre13">m</em></span> and <span class="strong1"><em class="calibre13">k</em></span> means that it's possible to select the false positive rate you're willing to tolerate. As a result, they're used in a large variety of data-intensive systems.</p><p class="calibre11">A drawback of Bloom filters is that it's impossible to retrieve the values you've added to the filter; although we can use the filter to test for set membership, we aren't able to say what that set contains without exhaustive checks. For recommendation systems (and indeed for others too, such as clustering), we're primarily interested in the similarity between two sets rather than their precise contents. But here, the Bloom lets us down; we can't reliably use the compressed filter as a measure of the similarity between two sets of items.</p><p class="calibre11">Next, we'll introduce an algorithm that will preserve set similarity as measured by the Jaccard similarity. It does so while also preserving the configurable compression provided by the Bloom filter.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Jaccard similarity for large sets with MinHash"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec125" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Jaccard similarity for large sets with MinHash</h1></div></div></div><p class="calibre11">The Bloom filter is a probabilistic data structure to determine whether an item is a member of a set. While <a id="id963" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>comparing user or item <a id="id964" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>similarities, what we are usually interested in is the intersection between sets, as opposed to their precise contents. MinHash is a technique that enables a large set to be compressed in such a way that we can still perform the Jaccard similarity on the compressed representations.</p><p class="calibre11">Let's see how it works with a reference to two of the most prolific raters in the MovieLens dataset. Users 405 and 655 have rated 727 and 675 movies respectively. In the following code, we<a id="id965" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> extract their ratings and convert them into sets before passing to Incanter's <code class="literal">jaccard-index</code> function. Recall that this returns<a id="id966" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the ratio of movies they've both rated out of all the movies they've rated:</p><div class="calibre2"><pre class="programlisting">(defn rated-items [user-ratings id]
  (-&gt;&gt; (get user-ratings id)
       (map :item)))

(defn ex-7-22 []
  (let [ratings      (load-ratings "ua.base")
        user-ratings (group-by :user ratings)
        user-a       (rated-items user-ratings 405)
        user-b       (rated-items user-ratings 655)]
    (println "User 405:" (count user-a))
    (println "User 655:" (count user-b))
    (s/jaccard-index (set user-a) (set user-b))))

;; User 405: 727
;; User 655: 675
;; 158/543</pre></div><p class="calibre11">There is an approximate similarity of 29 percent between the two large sets of ratings. Let's see how we can reduce the size of these sets while also preserving the similarity between them using MinHash.</p><p class="calibre11">The MinHash algorithm shares much in common with the Bloom filter. Our first task is to pick <span class="strong1"><em class="calibre13">k</em></span> hash functions. Rather than hashing the set representation itself, these <span class="strong1"><em class="calibre13">k</em></span> hash functions are used to hash each element within the set. For each of the <span class="strong1"><em class="calibre13">k</em></span> hash functions, the MinHash algorithm stores the minimum value generated by any of the set elements. The output therefore, is a set of <span class="strong1"><em class="calibre13">k</em></span> numbers; each equals the minimum hash value for that hash function. The output is referred to as the MinHash signature.</p><p class="calibre11">The following diagram illustrates the process for two sets, each containing three elements, being converted into MinHash signatures with a <span class="strong1"><em class="calibre13">k</em></span> of 2:</p><div class="mediaobject"><img src="Images/7180OS_07_210.jpg" alt="Jaccard similarity for large sets with MinHash" class="calibre261"/></div><p class="calibre11">The input sets share two elements out of a total of four unique elements, which equates to Jaccard index of 0.5. The<a id="id967" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> MinHash signatures for the two sets are <code class="literal">#{3, 0}</code> and <code class="literal">#{3, 55}</code> respectively, which equates to a Jaccard Index of 0.33. Thus, MinHash has reduced the size of our input sets (by just one, in this case), while <a id="id968" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>conserving the approximate similarity between them.</p><p class="calibre11">As with the Bloom filter, an appropriate choice of <span class="strong1"><em class="calibre13">k</em></span> allows you to specify the loss of precision that it is acceptable to tolerate. We can implement the MinHash algorithm using the following Clojure code:</p><div class="calibre2"><pre class="programlisting">(defn hash-function [seed]
  (let [f (Hashing/murmur3_32 seed)]
    (fn [x]
      (-&gt; (.hashUnencodedChars f (str x))
          (.asInt)))))

(defn hash-functions [k]
  (map hash-function (range k)))

(defn pairwise-min [a b]
  (map min a b))

(defn minhasher [k]
  (let [f (apply juxt (hash-functions k))]
    (fn [coll]
      (-&gt;&gt; (map f coll)
           (reduce pairwise-min)))))</pre></div><p class="calibre11">In the following code, we define a <code class="literal">minhasher</code> function with a <span class="strong1"><em class="calibre13">k</em></span> of 10 and use it to perform a set test using the Jaccard<a id="id969" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> index on the compressed ratings for users 405 and 655:</p><div class="calibre2"><pre class="programlisting"> (defn ex-7-23 []
  (let [ratings      (load-ratings "ua.base")
        user-ratings (group-by :user ratings)
        minhash (minhasher 10)
        user-a  (minhash (rated-items user-ratings 405))
        user-b  (minhash (rated-items user-ratings 655))]
    (println "User 405:" user-a)
    (println "User 655:" user-b)
    (s/jaccard-index (set user-a) (set user-b))))

;; User 405: #{-2147145175 -2141119028 -2143110220 -2143703868 –
;; 2144897714 -2145866799 -2139426844 -2140441272 -2146421577 –
;; 2146662900}
;; User 655: #{-2144975311 -2140926583 -2141119028 -2141275395 –
;; 2145738774 -2143703868 -2147345319 -2147134300 -2146421577 –
;; 2146662900}
;; 1/4</pre></div><p class="calibre11">The Jaccard index based<a id="id970" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> on our MinHash signatures is remarkably close to that on the original sets—25 percent compared to 29 percent—despite the fact that we compressed the sets down to only 10 elements each.</p><p class="calibre11">The benefit of much smaller sets is twofold: clearly storage space is much reduced, but so is the computational complexity required to check the similarity between the two sets as well. It's much less work to check the similarity of the sets that contain only 10 elements than the sets that contain many hundreds. MinHash is, therefore, not just a space-saving algorithm, but also a time-saving algorithm in cases where we need to make a large number of set similarity tests; cases that occur in recommender systems, for example.</p><p class="calibre11">If we're trying to establish a user neighborhood for the purposes of recommending items, we'll still need to perform a large number of set tests in order to determine which the most similar users are. In fact, for a large number of users, it may be prohibitively time-consuming to check<a id="id971" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> every other user exhaustively, even after we've calculated MinHash signatures. The final probabilistic technique will look<a id="id972" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> at addressing this specific problem: how to reduce the number of candidates that have to be compared while looking for similar items.</p><div class="calibre2" title="Reducing pair comparisons with locality-sensitive hashing"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec157" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Reducing pair comparisons with locality-sensitive hashing</h2></div></div></div><p class="calibre11">In the previous <a id="id973" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chapter, we computed the similarity matrix for a large number of documents. With the 20,000 documents in the Reuters corpus, this was already a time-consuming process. As the size of the dataset doubles, the length of time required to check every pair of items is multiplied by four. It can, therefore, become prohibitively time-consuming to perform this sort of analysis at scale.</p><p class="calibre11">For example, suppose we had a million documents and that we computed MinHash signatures of length 250 for each of them. This means we use 1,000 bytes to store each document. As all the signatures can be stored in a Gigabyte, they can all be stored in the main system memory for speed. However, there are <span class="inlinemediaobject"><img src="Images/7180OS_07_34.jpg" alt="Reducing pair comparisons with locality-sensitive hashing" class="calibre313"/></span> pairs of documents, or 499,999, 500,000 pairwise combinations to be checked. Even if it takes only a microsecond to compare two signatures, it will still take almost 6 days to compute all the similarities overall.</p><p class="calibre11">
<span class="strong1"><strong class="calibre12">Locality-sensitive hashing</strong></span> (<span class="strong1"><strong class="calibre12">LSH</strong></span>), addresses this problem by significantly reducing the number of pairwise<a id="id974" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> comparisons that have to be made. It does this by bucketing sets that are likely to have a minimum threshold of similarity together; only the sets that are bucketed together need to be checked for similarity.</p><div class="calibre2" title="Bucketing signatures"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch07lvl3sec28" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Bucketing signatures</h3></div></div></div><p class="calibre11">We consider any pair <a id="id975" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of items that hash to the same bucket a candidate pair and check only the candidate pairs for similarity. The aim is that only similar items should become candidate pairs. Dissimilar pairs that happen to hash to the same bucket will be false positives and we seek to minimize these. Similar pairs that hash to different buckets are false negatives and we likewise seek to minimize these too.</p><p class="calibre11">If we have computed MinHash signatures for the items, an effective way to bucket them would be to divide the signature matrix into <span class="strong1"><em class="calibre13">b</em></span> bands consisting of <span class="strong1"><em class="calibre13">r</em></span> elements each. This is illustrated in the following diagram:</p><div class="mediaobject"><img src="Images/7180OS_07_220.jpg" alt="Bucketing signatures" class="calibre314"/></div><p class="calibre11">Having already <a id="id976" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>written the code to produce the MinHash signatures in the previous section, performing LSH in Clojure is simply a matter of partitioning the signature into a certain number of bands, each of length <span class="strong1"><em class="calibre13">r</em></span>. Each band is hashed (for simplicity, we're using the same hashing function for each band) to a particular bucket:</p><div class="calibre2"><pre class="programlisting">(def lsh-hasher (hash-function 0))

(defn locality-sensitive-hash [r]
  {:r r :bands {}})

(defn buckets-for [r signature]
  (-&gt;&gt; (partition-all r signature)
       (map lsh-hasher)
       (map-indexed vector)))

(defn lsh-assoc [{:keys [r] :as lsh} {:keys [id signature]}]
  (let [f (fn [lsh [band bucket]]
            (update-in lsh [:bands band bucket] conj id))]
    (-&gt;&gt; (buckets-for r signature)
         (reduce f lsh))))</pre></div><p class="calibre11">The earlier <a id="id977" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>example defines a locality-sensitive hash simply as a map containing empty bands and some value, <span class="strong1"><em class="calibre13">r</em></span>. When we come to associate an item into the LSH with <code class="literal">lsh-assoc</code>, we split the signature into bands based on the value of <span class="strong1"><em class="calibre13">r</em></span> and determine the bucket for each band. The item's ID gets added to each of these buckets. Buckets are grouped by the band ID so that items which share a bucket in different bands are not bucketed together:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-24 []
  (let [ratings (load-ratings "ua.base")
        user-ratings (group-by :user ratings)
        minhash (minhasher 27)
        user-a  (minhash (rated-items user-ratings 13))
        lsh     (locality-sensitive-hash 3)]
    (lsh-assoc lsh {:id 13 :signature user-a})))

;; {:r 3, :bands {8 {220825369 (13)}, 7 {-2054093854 (13)},
;; 6 {1177598806 (13)}, 5 {-1809511158 (13)}, 4 {-143738650 (13)},
;; 3 {-704443054 (13)}, 2 {-1217282814 (13)},
;; 1 {-100016681 (13)}, 0 {1353249231 (13)}}}</pre></div><p class="calibre11">The preceding example shows the result of performing LSH on the signature of user 13 with <span class="strong1"><em class="calibre13">k=27</em></span> and <span class="strong1"><em class="calibre13">r=3</em></span>. The buckets for 9 bands are returned. Next, we add further items to the locality-sensitive hash:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-25 []
  (let [ratings (load-ratings "ua.base")
        user-ratings (group-by :user ratings)
        minhash (minhasher 27)
        user-a  (minhash (rated-items user-ratings 13))
        user-b  (minhash (rated-items user-ratings 655))]
    (-&gt; (locality-sensitive-hash 3)
        (lsh-assoc {:id 13  :signature user-a})
        (lsh-assoc {:id 655 :signature user-b}))))

;; {:r 3, :bands {8 {220825369 (655 13)}, 7 {1126350710 (655),
;; -2054093854 (13)}, 6 {872296818 (655), 1177598806 (13)},
;; 5 {-1272446116 (655), -1809511158 (13)}, 4 {-154360221 (655),
;; -143738650 (13)}, 3 {123070264 (655), -704443054 (13)},
;; 2 {-1911274538 (655), -1217282814 (13)}, 1 {-115792260 (655),
;; -100016681 (13)}, 0 {-780811496 (655), 1353249231 (13)}}}</pre></div><p class="calibre11">In the previous example, we can see that both the user IDs <code class="literal">655</code> and <code class="literal">13</code> are placed in the same bucket for band <code class="literal">8</code>, although they're in different buckets for all the other bands.</p><p class="calibre11">The probability that the signatures agree for one particular band is <span class="strong1"><em class="calibre13">s</em></span><sup class="calibre42">r</sup>, where <span class="strong1"><em class="calibre13">s</em></span> is the true similarity of the sets and <span class="strong1"><em class="calibre13">r</em></span> is the length of each band. It follows that the probability that the signatures do not <a id="id978" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>agree in at least one particular band is <span class="inlinemediaobject"><img src="Images/7180OS_07_35.jpg" alt="Bucketing signatures" class="calibre315"/></span> and so, the probability that signatures don't agree across all bands is <span class="inlinemediaobject"><img src="Images/7180OS_07_36.jpg" alt="Bucketing signatures" class="calibre316"/></span>. Therefore, we can say the probability that two items become a candidates pair is <span class="inlinemediaobject"><img src="Images/7180OS_07_37.jpg" alt="Bucketing signatures" class="calibre317"/></span>.</p><p class="calibre11">Regardless of the specific values of <span class="strong1"><em class="calibre13">b</em></span> and <span class="strong1"><em class="calibre13">r</em></span>, this equation describes an S-curve. The threshold (the value of the similarity at which the probability of becoming a candidate is 0.5) is a function of <span class="strong1"><em class="calibre13">b</em></span> and <span class="strong1"><em class="calibre13">r</em></span>. Around the threshold, the S-curve rises steeply. Thus, pairs with similarity above the threshold are very likely to become candidates, while those below are correspondingly unlikely to become candidates.</p><div class="mediaobject"><img src="Images/7180OS_07_230.jpg" alt="Bucketing signatures" class="calibre318"/></div><p class="calibre11">To search for candidate pairs, we now only need to perform the same process on a target signature and see which other items hash to the same buckets in the same bands:</p><div class="calibre2"><pre class="programlisting">(defn lsh-candidates [{:keys [bands r]} signature]
  (-&gt;&gt; (buckets-for r signature)
       (mapcat (fn [[band bucket]]
                 (get-in bands [band bucket])))
       (distinct)))</pre></div><p class="calibre11">The preceding code returns the<a id="id979" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> distinct list of items that share at least one bucket in at least one band with the target signature:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-26 []
  (let [ratings (load-ratings "ua.base")
        user-ratings (group-by :user ratings)
        minhash   (minhasher 27)
        user-b    (minhash (rated-items user-ratings 655))
        user-c    (minhash (rated-items user-ratings 405))
        user-a    (minhash (rated-items user-ratings 13))]
    (-&gt; (locality-sensitive-hash 3)
        (lsh-assoc {:id 655 :signature user-b})
        (lsh-assoc {:id 405 :signature user-c})
        (lsh-candidates user-a))))

;; (655)</pre></div><p class="calibre11">In the previous example, we associate the signature for users <code class="literal">655</code> and <code class="literal">405</code> into the locality-sensitive hash. We then ask for the candidates for user ID <code class="literal">13</code>. The result is a sequence containing the single ID <code class="literal">655</code>. Thus, <code class="literal">655</code> and <code class="literal">13</code> are candidate pairs and should be checked for similarity. User <code class="literal">405</code> has been judged by the algorithm as not being sufficiently similar, and we therefore will not check them for similarity.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title8"><a id="note65" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">For more information<a id="id980" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> on locality-sensitive hashing, MinHash, and other useful algorithms to deal with huge volumes of data, refer to the excellent <span class="strong1"><em class="calibre13">Mining of Massive Datasets</em></span> online book for free at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.mmds.org/">http://www.mmds.org/</a>.</p></div></div><p class="calibre11">Locality-sensitive hashing is a way of significantly reducing the space of pairwise comparisons that we need to make while comparing sets for similarity. Thus, with appropriate values set for <span class="strong1"><em class="calibre13">b</em></span> and <span class="strong1"><em class="calibre13">r</em></span>, locality-sensitive hashing allows us to precompute the user neighborhood. The task of finding similar users, given a target user, is as simple as finding the other users who share the same bucket across any of the bands; a task whose time complexity is related to the number of bands rather than the number of users.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Dimensionality reduction"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec126" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Dimensionality reduction</h1></div></div></div><p class="calibre11">What algorithms <a id="id981" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>such as MinHash and LSH aim to do is reduce the quantity of data that must be stored without compromising on the essence of the original. They're a form of compression and they define helpful representations that preserve our ability to do useful work. In particular, MinHash and LSH are designed to work with data that can be represented as a set.</p><p class="calibre11">In fact, there is a whole class of dimensionality-reducing algorithms that will work with data that is not so easily represented as a set. We saw, in the previous chapter with k-means clustering, how certain data could be most usefully represented as a weighted vector. Common approaches to reduce the dimensions of data represented as vectors are principle component analysis and singular-value decomposition. To demonstrate these, we'll return to Incanter and make use of one of its included datasets: the Iris dataset:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-27 []
  (i/view (d/get-dataset :iris)))</pre></div><p class="calibre11">The previous code should return the following table:</p><div class="mediaobject"><img src="Images/7180OS_07_240.jpg" alt="Dimensionality reduction" class="calibre319"/></div><p class="calibre11">The first four columns of the Iris dataset contain measurements of the sepal length, sepal width, petal length, and petal width of Iris plants. The dataset is ordered by the species of plants. Rows 0 to 49 represent Iris setosa, rows 50 to 99 represent Iris virsicolor, and rows above 100 contain Iris virginica. The exact species aren't important; we'll only be interested in the differences between them.</p><div class="calibre2" title="Plotting the Iris dataset"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec158" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Plotting the Iris dataset</h2></div></div></div><p class="calibre11">Let's visualize<a id="id982" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> some of the attributes of the Iris dataset on a scatter plot. We'll make use of the following helper function to plot each of the species as a separate color:</p><div class="calibre2"><pre class="programlisting">(defn plot-iris-columns [a b]
  (let [data (-&gt;&gt; (d/get-dataset :iris)
                  (i/$ [a b])
                  (i/to-matrix))]
    (-&gt; (c/scatter-plot (i/$ (range 50) 0 data)
                        (i/$ (range 50) 1 data)
                        :x-label (name a)
                        :y-label (name b))
        (c/add-points (i/$ (range 50 100) 0 data)
                      (i/$ (range 50 100) 1 data))
        (c/add-points (i/$ [:not (range 100)] 0 data)
                      (i/$ [:not (range 100)] 1 data))
        (i/view))))</pre></div><p class="calibre11">Having defined this function, let's see how the sepal widths and lengths compare for each of the three species:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-28 []
  (plot-iris-columns :Sepal.Width
                     :Sepal.Length))</pre></div><p class="calibre11">The previous example should generate the following chart:</p><div class="mediaobject"><img src="Images/7180OS_07_250.jpg" alt="Plotting the Iris dataset" class="calibre45"/></div><p class="calibre11">We can see how one<a id="id983" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of the species is quite different from the other two while comparing these two attributes, but two of the species are barely distinguishable: the widths and heights for several of the points are evenly overlaid.</p><p class="calibre11">Let's instead plot the petal width and height to see how these compare:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-29 []
  (plot-iris-columns :Petal.Width
                     :Petal.Length))</pre></div><p class="calibre11">This should generate the following chart:</p><div class="mediaobject"><img src="Images/7180OS_07_255.jpg" alt="Plotting the Iris dataset" class="calibre45"/></div><p class="calibre11">This does a much better <a id="id984" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>job of distinguishing between the different species. This is partly because the variance of the petal width and length is greater—the length, for example, stretches a full 6 units on the <span class="strong1"><em class="calibre13">y</em></span> axis. A useful side effect of this greater spread is that it allows us to draw a much clearer distinction between the species of Iris.</p></div><div class="calibre2" title="Principle component analysis"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec159" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Principle component analysis</h2></div></div></div><p class="calibre11">In principle<a id="id985" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> component analysis, often abbreviated to PCA, we're looking to find a rotation of data that maximizes the variance. In the previous scatter plot, we identified a way of looking at the data that provided a high degree of variance on the <span class="strong1"><em class="calibre13">y</em></span> axis, but the variance of the <span class="strong1"><em class="calibre13">x</em></span> axis was not as great.</p><p class="calibre11">We have four dimensions available in the Iris dataset, each representing the value of the length and width of a petal or a sepal. Principle component analysis allows us to determine whether there is a another basis, which is some linear combination of all the available dimensions, that best re-expresses our data to maximize the variance.</p><p class="calibre11">We can apply <a id="id986" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>principle component analysis with the Incanter.stats' <code class="literal">principle-components</code> function. In the following code, we pass it a matrix of data and plot the first two returned rotations:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-30 []
  (let [data (-&gt;&gt; (d/get-dataset :iris)
                  (i/$ (range 4))
                  (i/to-matrix))
        components (s/principal-components data)
        pc1 (i/$ 0 (:rotation components))
        pc2 (i/$ 1 (:rotation components))
        xs (i/mmult data pc1)
        ys (i/mmult data pc2)]
    (-&gt; (c/scatter-plot (i/$ (range 50) 0 xs)
                        (i/$ (range 50) 0 ys)
                        :x-label "Principle Component 1"
                        :y-label "Principle Component 2")
        (c/add-points (i/$ (range 50 100) 0 xs)
                      (i/$ (range 50 100) 0 ys))
        (c/add-points (i/$ [:not (range 100)] 0 xs)
                      (i/$ [:not (range 100)] 0 ys))
        (i/view))))</pre></div><p class="calibre11">The preceding example produces the following chart:</p><div class="mediaobject"><img src="Images/7180OS_07_280.jpg" alt="Principle component analysis" class="calibre45"/></div><p class="calibre11">Notice how the axes can no longer be identified as being sepals or petals—the components have<a id="id987" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> been derived as a linear combination of the values across all the dimensions and define a new basis to view the data that maximizes the variance within each component. In fact, the <code class="literal">principle-component</code> function returns <code class="literal">:std-dev</code> along with <code class="literal">:rotation</code> for each dimension.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note66" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">For interactive examples demonstrating principle component analysis, see <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://setosa.io/ev/principal-component-analysis/">http://setosa.io/ev/principal-component-analysis/</a>.</p></div></div><p class="calibre11">As a result of taking the principle components of the data, the variance across the <span class="strong1"><em class="calibre13">x</em></span> and the <span class="strong1"><em class="calibre13">y</em></span> axis is greater than even the previous scatter plot showing petal width and length. The points corresponding to the different species of iris are therefore spread out as wide as they can be, so the relative difference of the species is clearly observable.</p></div><div class="calibre2" title="Singular value decomposition"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec160" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Singular value decomposition</h2></div></div></div><p class="calibre11">A technique that's <a id="id988" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>closely related to PCA is <span class="strong1"><strong class="calibre12">Singular Value Decomposition</strong></span> (<span class="strong1"><strong class="calibre12">SVD</strong></span>). SVD is, in fact, a more general <a id="id989" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>technique than PCA which also seeks to change the basis of a matrix.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note67" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">An excellent mathematical description of PCA and its relationship to SVD is available at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://arxiv.org/pdf/1404.1100.pdf">http://arxiv.org/pdf/1404.1100.pdf</a>.</p></div></div><p class="calibre11">As its name implies, SVD decomposes a matrix into three related matrices, commonly referred to as the <span class="strong1"><em class="calibre13">U</em></span>, <span class="strong1"><em class="calibre13">Σ</em></span> (or <span class="strong1"><em class="calibre13">S</em></span>), and <span class="strong1"><em class="calibre13">V</em></span> matrices, such that:</p><div class="mediaobject"><img src="Images/7180OS_07_38.jpg" alt="Singular value decomposition" class="calibre320"/></div><p class="calibre11">If <span class="strong1"><em class="calibre13">X</em></span> is an m x n matrix, <span class="strong1"><em class="calibre13">U</em></span> is an m x m matrix, <span class="strong1"><em class="calibre13">Σ</em></span> is an m x n matrix, and <span class="strong1"><em class="calibre13">V</em></span> is an n x n matrix. <span class="strong1"><em class="calibre13">Σ</em></span> is, in fact, a diagonal matrix, meaning that all the cells with the exception of those on the main diagonal (top left to bottom right) are zero. Although clearly, it need not be square. The columns of the matrices returned by SVD are ordered by their singular value with the most important dimensions coming first. SVD thus allows us to represent the matrix <span class="strong1"><em class="calibre13">X</em></span> more approximately by discarding the least important dimensions.</p><p class="calibre11">For example, the decomposition of our 150 x 4 Iris matrix will result in a <span class="strong1"><em class="calibre13">U</em></span> of 150 x 150, <span class="strong1"><em class="calibre13">Σ</em></span> of 150 x 4 and <span class="strong1"><em class="calibre13">V</em></span> of 4 x 4. Multiplying these matrices together will yield our original Iris matrix.</p><p class="calibre11">However, we could choose instead to take only the top two singular values and adjust our matrices such that <span class="strong1"><em class="calibre13">U</em></span> is 150 x 2, <span class="strong1"><em class="calibre13">Σ</em></span> is 2 x 2, and <span class="strong1"><em class="calibre13">V</em></span> is 2 x 4. Let's construct a function that takes a matrix and projects it into a specified number of dimensions by taking this number of columns from each of the <span class="strong1"><em class="calibre13">U</em></span>, <span class="strong1"><em class="calibre13">Σ</em></span>, and <span class="strong1"><em class="calibre13">V</em></span> matrices:</p><div class="calibre2"><pre class="programlisting">(defn project-into [matrix d]
  (let [svd (i/decomp-svd matrix)]
    {:U (i/$ (range d) (:U svd))
     :S (i/diag (take d (:S svd)))
     :V (i/trans
         (i/$ (range d) (:V svd)))}))</pre></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">d</em></span> is the number of dimensions that we want to retain. Let's demonstrate this with a simple example by taking a multivariate normal distribution generated by Incanter using <code class="literal">s/sample-mvn</code> and reducing it to just one dimension:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-31 []
  (let [matrix (s/sample-mvn 100
                             :sigma (i/matrix [[1 0.8]
                                               [0.8 1]]))]
    (println "Original" matrix)
    (project-into matrix 1)))

;; Original  A 100x2 matrix
;; :U  A 100x1 matrix
;; :S  A 1x1 matrix
;; :V  A 1x2 matrix</pre></div><p class="calibre11">The output of the previous example contains the most important aspects of the data reduced to just one<a id="id990" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> dimension. To recreate <a id="id991" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>an approximation of the original dataset in two dimensions, we can simply multiply the three matrices together. In the following code, we project the one-dimensional approximation of the distribution back into two dimensions:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-32 []
  (let [matrix (s/sample-mvn 100
                             :sigma (i/matrix [[1 0.8]
                                               [0.8 1]]))
        svd (project-into matrix 1)
        projection (i/mmult (:U svd)
                            (:S svd)
                            (:V svd))]
    (-&gt; (c/scatter-plot (i/$ 0 matrix) (i/$ 1 matrix)
                        :x-label "x"
                        :y-label "y"
                        :series-label "Original"
                        :legend true)
        (c/add-points (i/$ 0 projection) (i/$ 1 projection)
                      :series-label "Projection")
        (i/view))))</pre></div><p class="calibre11">This produces the following chart:</p><div class="mediaobject"><img src="Images/7180OS_07_290.jpg" alt="Singular value decomposition" class="calibre45"/></div><p class="calibre11">Notice how SVD has preserved the primary feature of the multivariate distribution, the strong diagonal, but has<a id="id992" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> collapsed the variance of the off-diagonal points. In this way, SVD preserves the most important<a id="id993" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> structure in the data while discarding less important information. Hopefully, the earlier example makes it even clearer than the PCA example that the preserved features need not be explicit in the original data. In the example, the strong diagonal is a <span class="strong1"><em class="calibre13">latent</em></span> feature of the data.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note68" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Latent features are those which are not directly observable, but which can be inferred from other features. Sometimes, latent features refer to aspects that could be measured directly, such as the correlation in the previous example or—in the context of recommendation—they can be considered to represent underlying preferences or attitudes.</p></div></div><p class="calibre11">Having observed the<a id="id994" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> principle of SVD at <a id="id995" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>work on the earlier synthetic data, let's see how it performs on the Iris dataset:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-33 []
  (let [svd (-&gt;&gt; (d/get-dataset :iris)
                 (i/$ (range 4))
                 (i/to-matrix)
                 (i/decomp-svd))
        dims 2
        u (i/$     (range dims) (:U svd))
        s (i/diag  (take dims   (:S svd)))
        v (i/trans (i/$ (range dims) (:V svd)))
        projection (i/mmult u s v)]
    (-&gt; (c/scatter-plot (i/$ (range 50) 0 projection)
                        (i/$ (range 50) 1 projection)
                        :x-label "Dimension 1"
                        :y-label "Dimension 2")
        (c/add-points (i/$ (range 50 100) 0 projection)
                      (i/$ (range 50 100) 1 projection))
        (c/add-points (i/$ [:not (range 100)] 0 projection)
                      (i/$ [:not (range 100)] 1 projection))
        (i/view))))</pre></div><p class="calibre11">This code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_07_300.jpg" alt="Singular value decomposition" class="calibre45"/></div><p class="calibre11">After comparing the<a id="id996" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> Iris charts for PCA and SVD, it should be clear that the two approaches are closely related. This scatter plot<a id="id997" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> looks a lot like an inverted version of the PCA plot that we saw previously.</p><p class="calibre11">Let's return to the problem of movie recommendation now, and see how dimensionality reduction could assist. In the next section, we'll make use of the Apache Spark distributed computing framework and an associated machine learning library, MLlib, to perform movie recommendations on dimensionally-reduced data.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Large-scale machine learning with Apache Spark and MLlib"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec127" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Large-scale machine learning with Apache Spark and MLlib</h1></div></div></div><p class="calibre11">The<a id="id998" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> Spark project (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://spark.apache.org/">https://spark.apache.org/</a>) is a cluster computing framework that emphasizes low-latency job execution. It's a relatively recent project, growing <a id="id999" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>out of UC Berkley's AMP<a id="id1000" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> Lab in 2009.</p><p class="calibre11">Although Spark is<a id="id1001" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> able to coexist with Hadoop (by connecting to the files stored on <span class="strong1"><strong class="calibre12">Hadoop Distributed File System</strong></span> (<span class="strong1"><strong class="calibre12">HDFS</strong></span>), for example), it targets much faster job execution times by<a id="id1002" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> keeping much of the computation in memory. In contrast with Hadoop's two-stage MapReduce paradigm, which stores files on the disk in between each iteration, Spark's in-memory<a id="id1003" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> model can perform tens or hundreds of times faster for some applications, particularly those performing multiple iterations over the data.</p><p class="calibre11">In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch05.xhtml" title="Chapter 5. Big Data">Chapter 5</a>, <span class="strong1"><em class="calibre13">Big Data</em></span>, we discovered the value of iterative algorithms to the implementation of optimization techniques on large quantities of data. This makes Spark an excellent choice for large-scale machine learning. In fact, the <a id="id1004" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>MLlib library (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://spark.apache.org/mllib/">https://spark.apache.org/mllib/</a>) is built on top of Spark and implements a variety of machine learning algorithms out of the box.</p><p class="calibre11">We won't provide an in-depth account of Spark here, but will explain just enough on the key concepts required to run a Spark job<a id="id1005" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> using the Clojure library, Sparkling (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/gorillalabs/sparkling">https://github.com/gorillalabs/sparkling</a>). Sparkling wraps much of Spark's functionality behind a friendly Clojure interface. In particular, the use of the thread-last macro <code class="literal">-&gt;&gt;</code> to chain Spark operations together can make Spark jobs written in Sparkling appear a lot like the code we would write to process data using Clojure's own sequence abstractions.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note69" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><a id="id1006" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><p class="calibre22">Be sure also to check out Flambo, which makes use of the thread-first macro to chain tasks: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/yieldbot/flambo">https://github.com/yieldbot/flambo</a>.</p></div></div><p class="calibre11">We're going to be producing recommendations based on the MovieLens ratings, so the first step will be to load this data with Sparkling.</p><div class="calibre2" title="Loading data with Sparkling"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec161" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Loading data with Sparkling</h2></div></div></div><p class="calibre11">Spark can load data<a id="id1007" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> from any storage source supported by <a id="id1008" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Hadoop, including the local file system and HDFS, as well as other data sources such as Cassandra, HBase, and Amazon S3. Let's start with the basics by writing a job to simply count the number of ratings.</p><p class="calibre11">The MovieLens ratings are stored as a text file, which can be loaded in Sparkling using the <code class="literal">text-file</code> function in the <code class="literal">sparkling.core</code> namespace (referred to as <code class="literal">spark</code> in the code). To tell Spark<a id="id1009" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> where the file is located, we pass a URI that can point to a remote source such as <code class="literal">hdfs://..., s3n://....</code> Since we're running Spark in local mode, it could simply be a local file path. Once we have the text file, we'll <a id="id1010" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>call <code class="literal">spark/count</code> to get the number of lines:</p><div class="calibre2"><pre class="programlisting">(defn count-ratings [sc]
  (-&gt; (spark/text-file sc "data/ml-100k/ua.base")
      (spark/count)))

(defn ex-7-34 []
  (spark/with-context sc (-&gt; (conf/spark-conf)
                             (conf/master "local")
                             (conf/app-name "ch7"))
    (count-ratings sc)))

;; 90570</pre></div><p class="calibre11">If you run the previous example, you may see many logging statements from Spark printed to the console. One of the final lines will be the count that has been calculated.</p><p class="calibre11">Notice that we have to pass a Spark context as the first argument to the <code class="literal">text-file</code> function. The Spark context tells Spark how to access your cluster. The most basic configuration specifies the location of the Spark master and the application name Spark should use for this job. For running locally, the Spark master is <code class="literal">"local"</code>, which is useful for REPL-based interactive development.</p></div><div class="calibre2" title="Mapping data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec162" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Mapping data</h2></div></div></div><p class="calibre11">Sparkling provides <a id="id1011" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>analogues to many of the Clojure core <a id="id1012" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sequence functions you would expect such as map, reduce, and filter. At the beginning of this chapter, we stored our ratings as a map with the <code class="literal">:item</code>, <code class="literal">:user</code>, and <code class="literal">:rating</code> keys. While we could parse our data into a map again, let's parse each rating into a <code class="literal">Rating</code> object instead. This will allow us to more easily interact with MLlib later in the chapter.</p><p class="calibre11">The <code class="literal">Rating</code> class is defined in the <code class="literal">org.apache.spark.mllib.recommendation</code> package. The constructor takes three numeric arguments: representations of the user, the item, and the user's rating for the item. As well as creating a <code class="literal">Rating</code> object, we're also calculating the time <a id="id1013" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>modulo <code class="literal">10</code>, returning a number between 0 and 9 and creating <code class="literal">tuple</code> of both values:</p><div class="calibre2"><pre class="programlisting">(defn parse-rating [line]
  (let [[user item rating time] (-&gt;&gt; (str/split line #"\t")
                                     (map parse-long))]
    (spark/tuple (mod time 10)
                 (Rating. user item rating))))

(defn parse-ratings [sc]
  (-&gt;&gt; (spark/text-file sc "data/ml-100k/ua.base")
       (spark/map-to-pair parse-rating)))

(defn ex-7-35 []
  (spark/with-context sc (-&gt; (conf/spark-conf)
                             (conf/master "local")
                             (conf/app-name "ch7"))
    (-&gt;&gt; (parse-ratings sc)
         (spark/collect)
         (first))))

;; #sparkling/tuple [8 #&lt;Rating Rating(1,1,5.0)&gt;]</pre></div><p class="calibre11">The returned value<a id="id1014" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> is a tuple with an integer key (defined as the time modulo <code class="literal">10</code>) and a rating as the value. Having a key which partitions the data into ten groups will be useful when we come to split the data into test and training sets.</p></div><div class="calibre2" title="Distributed datasets and tuples"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec163" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Distributed datasets and tuples</h2></div></div></div><p class="calibre11">Tuples are used <a id="id1015" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>extensively by Spark to represent pairs of keys and<a id="id1016" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> values. In the preceding example the key was an integer, but this is not a requirement—keys and values can be any type serializable by Spark.</p><p class="calibre11">Datasets in Spark are represented <a id="id1017" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>as <span class="strong1"><strong class="calibre12">Resilient Distributed Datasets</strong></span> (<span class="strong1"><strong class="calibre12">RDDs</strong></span>). In fact, RDDs are the core abstraction that Spark provides—a fault-tolerant collection of records partitioned across all the nodes in your cluster that can be operated in parallel. There are two fundamental types of RDDs: those that represent sequences of arbitrary objects (such as the kind returned by <code class="literal">text-file</code>—a sequence of lines), and those which represent sequences of key/value pairs.</p><p class="calibre11">We can convert between plain RDDs and pair RDDs simply, and this is accomplished in the previous example with the <code class="literal">map-to-pair</code> function. The tuple returned by our <code class="literal">parse-rating</code> function <a id="id1018" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>specifies the key and the value that should be<a id="id1019" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> used for each pair in the sequence. As with Hadoop, there's no requirement that the key be unique within the dataset. In fact, as we'll see, keys are often a useful means of grouping similar records together.</p></div><div class="calibre2" title="Filtering data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec164" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Filtering data</h2></div></div></div><p class="calibre11">Let's now filter our <a id="id1020" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data based on the value of the key and create a subset of the overall data that we can use for training. Like the core Clojure function of the same name, Sparkling provides a <code class="literal">filter</code> function that will keep only those rows for which a predicate returns logical true.</p><p class="calibre11">Given our pair RDD of ratings, we can filter only those ratings that have a key value less than 8. Since the keys roughly and uniformly distributed integers 0-9, this will retain approximately 80 percent of the dataset:</p><div class="calibre2"><pre class="programlisting">(defn training-ratings [ratings]
  (-&gt;&gt; ratings
       (spark/filter (fn [tuple]
                       (&lt; (s-de/key tuple) 8)))
       (spark/values)))</pre></div><p class="calibre11">Our ratings are stored in a pair RDD, so the result of filter is also a pair RDD. We're calling <code class="literal">values</code> on the result so that we're left with a plain RDD containing only the <code class="literal">Rating</code> objects. This will be the RDD that we pass to our machine learning algorithm. We perform exactly the same process, but for the keys greater than or equal to 8, to obtain the test data we'll be using.</p></div><div class="calibre2" title="Persistence and caching"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec165" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Persistence and caching</h2></div></div></div><p class="calibre11">Spark's actions <a id="id1021" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>are lazy and won't be calculated until they're needed. Similarly, once data has been calculated, it won't be explicitly cached. Sometimes, we'd like to keep data around though. In particular, if we're running an iterative algorithm, we<a id="id1022" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> don't want the dataset to be recalculated from source each time we perform an iteration. In cases where the results of a transformed dataset should be saved for subsequent use within a job, Spark provides the ability to persist RDDs. Like the RDDs themselves, the persistence is fault-tolerant, meaning that if any partition is lost, it will be recomputed using the transformations that originally created it.</p><p class="calibre11">We can persist an RDD using the <code class="literal">spark/persist</code> function, which expects us to pass the RDD and also<a id="id1023" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> configure the storage level most appropriate<a id="id1024" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> for our application. In most cases, this will be in-memory storage. But in cases where recomputing the data would be computationally expensive, we can spill to disk or even replicate the cache across disks for fast fault recovery. In-memory is most common, so Sparkling provides the <code class="literal">spark/cache</code> function shorthand that will set this storage level on an RDD:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-36 []
  (spark/with-context sc (-&gt; (conf/spark-conf)
                             (conf/master "local")
                             (conf/app-name "ch7"))
    (let [ratings (spark/cache (parse-ratings sc))
          train (training-ratings ratings)
          test  (test-ratings ratings)]
      (println "Training:" (spark/count train))
      (println "Test:"     (spark/count test)))))

;; Training: 72806
;; Test: 8778</pre></div><p class="calibre11">In the preceding example, we cache the result of the call to <code class="literal">parse-ratings</code>. This means that the loading and parsing of ratings is performed a single time, and the training and test ratings functions both use the cached data to filter and perform their counts. The call to <code class="literal">cache</code> optimizes the performance of jobs and allows spark to avoid recalculating data more than necessary.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Machine learning on Spark with MLlib"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec128" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Machine learning on Spark with MLlib</h1></div></div></div><p class="calibre11">We've covered<a id="id1025" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> enough of the basics of Spark now to use our RDDs for machine learning. While Spark handles the infrastructure, the actual work of performing machine learning is handled by an apache Spark subproject called MLlib.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note70" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">An overview of<a id="id1026" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> all the capabilities of the MLlib library are at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://spark.apache.org/docs/latest/mllib-guide.html">https://spark.apache.org/docs/latest/mllib-guide.html</a>.</p></div></div><p class="calibre11">MLlib provides a wealth of machine learning algorithms for use on Spark, including those for regression, classification, and clustering covered elsewhere in this book. In this chapter, we'll be using the algorithm MLlib provides for performing collaborative filtering: alternating least squares.</p><div class="calibre2" title="Movie recommendations with alternating least squares"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec166" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Movie recommendations with alternating least squares</h2></div></div></div><p class="calibre11">In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch05.xhtml" title="Chapter 5. Big Data">Chapter 5</a>, <span class="strong1"><em class="calibre13">Big Data</em></span>, we<a id="id1027" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> discovered how to use gradient descent to identify the parameters that minimize a cost function for a large quantity of data. In this chapter, we've seen how SVD can be used to calculate latent factors <a id="id1028" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>within a matrix of data through decomposition.</p><p class="calibre11">The <span class="strong1"><strong class="calibre12">alternating least squares</strong></span> (<span class="strong1"><strong class="calibre12">ALS</strong></span>) algorithm can be thought of as a combination of both of these approaches. It is an iterative algorithm that uses least-squares estimates to decompose the user-movies matrix of rankings into two matrices of latent factors: the user factors and the movie factors.</p><div class="mediaobject"><img src="Images/7180OS_07_310.jpg" alt="Movie recommendations with alternating least squares" class="calibre321"/></div><p class="calibre11">Alternating least squares is therefore based on the assumption that the users' ratings are based on some latent property of the movie that can't be measured directly, but can be inferred from the ratings matrix. The earlier diagram shows how the sparse matrix of user-movie ratings can be decomposed into two matrices containing the user factors and the movie factors. The diagram associates just three factors with each user and movie, but let's make it even more simplistic by just using two factors.</p><p class="calibre11">We could hypothesize that all the movies exist in a two-dimensional space identified by their level of action, romance, and how realistic (or not) they may be. We visualize such a space as follows:</p><div class="mediaobject"><img src="Images/7180OS_07_320.jpg" alt="Movie recommendations with alternating least squares" class="calibre232"/></div><p class="calibre11">We could likewise<a id="id1029" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> imagine all the users represented<a id="id1030" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> in an equivalent two-dimensional space, where their tastes were simply expressed as their relative preference for <span class="strong1"><strong class="calibre12">Romance</strong></span>/<span class="strong1"><strong class="calibre12">Action</strong></span> and <span class="strong1"><strong class="calibre12">Realist</strong></span>/<span class="strong1"><strong class="calibre12">Escapist</strong></span>.</p><p class="calibre11">Once we've reduced all the movies and users to their factor representation, the problem of prediction is reduced to a simple matrix multiplication—our predicted rating for a user, given a movie, is simply the product of their factors. The challenge for ALS then is to calculate the two factor matrices.</p></div><div class="calibre2" title="ALS with Spark and MLlib"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec167" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>ALS with Spark and MLlib</h2></div></div></div><p class="calibre11">At the time<a id="id1031" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of writing, no <a id="id1032" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Clojure wrapper exists for the MLlib library, so we'll be using Clojure's interop capabilities to access<a id="id1033" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> it directly. MLlib's implementation of <a id="id1034" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>alternating least squares is provided by the ALS class in the <code class="literal">org.apache.spark.mllib.recommendation</code> package. Training ALS is almost as simple as calling the <code class="literal">train</code> static method on the class with our RDD and provided arguments:</p><div class="calibre2"><pre class="programlisting">(defn alternating-least-squares [data {:keys [rank num-iter
                                              lambda]}]
  (ALS/train (to-mllib-rdd data) rank num-iter lambda 10))</pre></div><p class="calibre11">The slight complexity is<a id="id1035" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> that the RDD of training data returned by our preceding Sparkling job is expressed as a <code class="literal">JavaRDD</code> type. MLlib, since it has no Java API, expects to receive standard Spark <code class="literal">RDD</code> types. Converting between the two is a straightforward enough process, albeit somewhat tedious. The following functions convert back and forth between RDD types; into <code class="literal">RDDs</code> ready for consumption by MLlib and then back into <code class="literal">JavaRDDs</code> for use in Sparkling:</p><div class="calibre2"><pre class="programlisting">(defn to-mlib-rdd [rdd]
  (.rdd rdd))

(defn from-mlib-rdd [rdd]
  (JavaRDD/fromRDD rdd scala/OBJECT-CLASS-TAG))</pre></div><p class="calibre11">The second argument<a id="id1036" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> in <code class="literal">from-mllib-rdd</code> is a value defined in the <code class="literal">sparkling.scalaInterop</code> namespace. This is required to interact with the JVM bytecode generated by Scala's function definition.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note71" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">For more on Clojure/Scala interop consult the excellent from the <code class="literal">scala</code> library by Tobias Kortkamp at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://t6.github.io/from-scala/">http://t6.github.io/from-scala/</a>.</p></div></div><p class="calibre11">With the previous boilerplate out of the way, we can finally perform ALS on the training ratings. We do this in the following example:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-37 []
  (spark/with-context sc (-&gt; (conf/spark-conf)
                             (conf/master "local")
                             (conf/app-name "ch7"))
    (-&gt; (parse-ratings sc)
        (training-ratings)
        (alternating-least-squares {:rank 10
                                    :num-iter 10
                                    :lambda 1.0}))))</pre></div><p class="calibre11">The function takes several arguments—<code class="literal">rank</code>, <code class="literal">num-iter</code>, and <code class="literal">lambda</code>, and it returns a MLlib <code class="literal">MatrixFactorisationModel</code> function. The rank is the number of features to use for the factor matrices.</p></div><div class="calibre2" title="Making predictions with ALS"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec168" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Making predictions with ALS</h2></div></div></div><p class="calibre11">Once we've <a id="id1037" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculated <code class="literal">MatrixFactorisationModel</code>, we can use it to make predictions with the <code class="literal">recommendProducts</code> method. This expects to receive the ID of the user to recommend to and the number of recommendations to return:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-38 []
  (spark/with-context sc (-&gt; (conf/spark-conf)
                             (conf/master "local")
                             (conf/app-name "ch7"))
    (let [options {:rank 10
                   :num-iter 10
                   :lambda 1.0}
          model (-&gt; (parse-ratings sc)
                    (training-ratings )
                    (alternating-least-squares options))]
      (into [] (.recommendProducts model 1 3)))))

;; [#&lt;Rating Rating(1,1463,3.869355232995907)&gt;
;; #&lt;Rating Rating(1,1536,3.7939806028920993)&gt;
;; #&lt;Rating Rating(1,1500,3.7130689437266646)&gt;]</pre></div><p class="calibre11">You can see that the output of the model, like the input, are the <code class="literal">Rating</code> objects. They contain the user ID, the item ID, and a predicted rating calculated as the product of the factor matrices. Let's make <a id="id1038" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>use of the function that we defined at the beginning of the chapter to give these ratings names:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-39 []
  (spark/with-context sc (-&gt; (conf/spark-conf)
                             (conf/master "local")
                             (conf/app-name "ch7"))
    (let [items   (load-items "u.item")
          id-&gt;name (fn [id] (get items id))
          options {:rank 10
                   :num-iter 10
                   :lambda 1.0}
          model (-&gt; (parse-ratings sc)
                    (training-ratings )
                    (alternating-least-squares options))]
      (-&gt;&gt; (.recommendProducts model 1 3)
           (map (comp id-&gt;name #(.product %)))))))

;; ("Boys, Les (1997)" "Aiqing wansui (1994)"
;; "Santa with Muscles (1996)")</pre></div><p class="calibre11">It's not particularly clear that these are good recommendations though. For this, we'll need to evaluate the performance of our ALS model.</p></div><div class="calibre2" title="Evaluating ALS"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec169" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Evaluating ALS</h2></div></div></div><p class="calibre11">Unlike Mahout, Spark <a id="id1039" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>doesn't include a built-in evaluator for the model, so we're going to have to write our own. One of the simplest evaluators, and one we've used<a id="id1040" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> already in this chapter, is the root mean square error (RMSE) evaluator.</p><p class="calibre11">The first step for our evaluation is to use the model to predict ratings for all of our training set. Spark's implementation of ALS includes a predict function that we can use, which will accept an RDD containing all of the user IDs and item IDs to return predictions for:</p><div class="calibre2"><pre class="programlisting">(defn user-product [rating]
  (spark/tuple (.user rating)
               (.product rating)))

(defn user-product-rating [rating]
  (spark/tuple (user-product rating)
               (.rating rating))) 

(defn predict [model data]
  (-&gt;&gt; (spark/map-to-pair user-product data)
       (to-mlib-rdd data)
       (.predict model)
       (from-mlib-rdd)
       (spark/map-to-pair user-product-rating)))</pre></div><p class="calibre11">The <code class="literal">.recommendProducts</code> method we called previously uses the model to return product recommendations for a specific user. By contrast, the <code class="literal">.predict</code> method will predict the rating for many users and items at once.</p><div class="mediaobject"><img src="Images/7180OS_07_330.jpg" alt="Evaluating ALS" class="calibre322"/></div><p class="calibre11">The result of our<a id="id1041" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> call to the <code class="literal">.predict</code> function is a pair RDD, where the key is<a id="id1042" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> itself a tuple of user and product. The value of the pair RDD is the predicted rating.</p></div><div class="calibre2" title="Calculating the sum of squared errors"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch07lvl2sec170" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Calculating the sum of squared errors</h2></div></div></div><p class="calibre11">To calculate the<a id="id1043" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> difference between the predicted rating and the actual rating given to the product by the user, we'll need to join the <code class="literal">predictions</code> and the <code class="literal">actuals</code> together based on a matching user/product tuple. As the keys will be the same in both the <code class="literal">predictions</code> and <code class="literal">actuals</code> RDDs, we can simply pass them both to Sparkling's <code class="literal">join</code> function:</p><div class="calibre2"><pre class="programlisting">(defn squared-error [y-hat y]
  (Math/pow (- y-hat y) 2))

(defn sum-squared-errors [predictions actuals]
  (-&gt;&gt; (spark/join predictions actuals)
       (spark/values)
       (spark/map (s-de/val-val-fn squared-error))
       (spark/reduce +)))</pre></div><p class="calibre11">We can visualize the whole <code class="literal">sum-squared-errors </code>function as the following flow, comparing the predicted and actual ratings:</p><div class="mediaobject"><img src="Images/7180OS_07_340.jpg" alt="Calculating the sum of squared errors" class="calibre323"/></div><p class="calibre11">Once <a id="id1044" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we've calculated the <code class="literal">sum-squared-errors</code>, calculating the root mean square is simply a matter of dividing it by the count and taking the square root:</p><div class="calibre2"><pre class="programlisting">(defn rmse [model data]
  (let [predictions  (spark/cache (predict model data))
        actuals (-&gt;&gt; (spark/map-to-pair user-product-rating
                                        data)
                     (spark/cache))]
    (-&gt; (sum-squared-errors predictions actuals)
        (/ (spark/count data))
        (Math/sqrt))))</pre></div><p class="calibre11">The <code class="literal">rmse</code> function will take a model and some data and calculate RMSE of the prediction against the actual rating. Earlier in the chapter, we plotted the different values of RMSE as the size of the neighborhood changed with a user-based recommender. Let's employ the same technique now, but alter the rank of the factor matrix:</p><div class="calibre2"><pre class="programlisting">(defn ex-7-40 []
  (spark/with-context sc (-&gt; (conf/spark-conf)
                             (conf/master "local")
                             (conf/app-name "ch7"))
    (let [options {:num-iter 10 :lambda 0.1}
          training (-&gt; (parse-ratings sc)
                       (training-ratings)
                       (spark/cache))
          ranks    (range 2 50 2)
          errors   (for [rank ranks]
                     (doto (-&gt; (als training
                                    (assoc options :rank rank))
                               (rmse training))
                       (println "RMSE for rank" rank)))]
      (-&gt; (c/scatter-plot ranks errors
                          :x-label "Rank"
                          :y-label "RMSE")
          (i/view)))))</pre></div><p class="calibre11">The earlier code generates the following plot:</p><div class="mediaobject"><img src="Images/7180OS_07_350.jpg" alt="Calculating the sum of squared errors" class="calibre45"/></div><p class="calibre11">Observe how, as we increase the rank of the factor matrix, the ratings returned by our model become closer and closer to the ratings that the model was trained on. As the dimensions of the <a id="id1045" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>factor matrix grow, it can capture more of the variation in individual users' ratings.</p><p class="calibre11">What we'd really like to do though is to see how well the recommender performs against the test set—the data it hasn't already seen. The final example in this chapter, <code class="literal">ex-7-41</code>, runs the preceding analysis again, but tests the RMSE of the model against the test set rather than the training set. The example generates the following plot:</p><div class="mediaobject"><img src="Images/7180OS_07_355.jpg" alt="Calculating the sum of squared errors" class="calibre45"/></div><p class="calibre11">As we would hope, the<a id="id1046" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> RMSE of the predictions fall as the rank of the factor matrix is increased. A larger factor matrix is able to capture more of the latent features that lie within the ratings, and more accurately predict the rating a user will give a movie.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch07lvl1sec129" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">We've covered a lot of ground in this chapter. Although the subject was principally recommender systems, we've also discussed dimensionality reduction and introduced the Spark distributed computation framework as well.</p><p class="calibre11">We started by discussing the difference between content- and collaborative filtering-based approaches to the problem of recommendation. Within the context of collaborative filtering, we discussed item-item recommenders and built a Slope One recommender. We also discussed user-user recommenders and used Mahout's implementations of a variety of similarity measures and evaluators to implement and test several user-based recommenders too. The challenge of evaluation provided an opportunity to introduce the statistics of information retrieval.</p><p class="calibre11">We spent a lot of time in this chapter covering several different types of dimensionality reduction. For example, we learned about the probabilistic methods offered by Bloom filters and MinHash, and the analytic methods offered by principle component analysis and singular value decomposition. While not specific to recommender systems, we saw how such techniques could be used to help implement more efficient similarity comparisons.</p><p class="calibre11">Finally, we introduced the distributed computation framework Spark and learned how the alternating least squares algorithm uses dimensionality reduction to discover latent factors in a matrix of ratings. We implemented ALS and a RMSE evaluator using Spark, MLlib, and the Clojure library Sparkling.</p><p class="calibre11">Many of the techniques we learned this chapter are very general, and the next chapter will be no different. We'll continue to explore the Spark and Sparkling libraries as we learn about network analysis: the study of connections and relationships.</p></div></div>



  </body></html>