- en: Chapter 1. Getting Ready to Use R and Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first chapter has been bundled with several topics on R and Hadoop basics
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: R Installation, features, and data modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop installation, features, and components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preface, we introduced you to R and Hadoop. This chapter will focus on
    getting you up and running with these two technologies. Until now, R has been
    used mainly for statistical analysis, but due to the increasing number of functions
    and packages, it has become popular in several fields, such as machine learning,
    visualization, and data operations. R will not load all data (Big Data) into machine
    memory. So, Hadoop can be chosen to load the data as Big Data. Not all algorithms
    work across Hadoop, and the algorithms are, in general, not R algorithms. Despite
    this, analytics with R have several issues related to large data. In order to
    analyze the dataset, R loads it into the memory, and if the dataset is large,
    it will fail with exceptions such as "cannot allocate vector of size x". Hence,
    in order to process large datasets, the processing power of R can be vastly magnified
    by combining it with the power of a Hadoop cluster. Hadoop is very a popular framework
    that provides such parallel processing capabilities. So, we can use R algorithms
    or analysis processing over Hadoop clusters to get the work done.
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting Ready to Use R and Hadoop](img/3282OS_01_00.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we think about a combined RHadoop system, R will take care of data analysis
    operations with the preliminary functions, such as data loading, exploration,
    analysis, and visualization, and Hadoop will take care of parallel data storage
    as well as computation power against distributed data.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to the advent of affordable Big Data technologies, analysis used to be
    run on limited datasets on a single machine. Advanced machine learning algorithms
    are very effective when applied to large datasets, and this is possible only with
    large clusters where data can be stored and processed with distributed data storage
    systems. In the next section, we will see how R and Hadoop can be installed on
    different operating systems and the possible ways to link R and Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: Installing R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the appropriate version by visiting the official R website.
  prefs: []
  type: TYPE_NORMAL
- en: Here are the steps provided for three different operating systems. We have considered
    Windows, Linux, and Mac OS for R installation. Download the latest version of
    R as it will have all the latest patches and resolutions to the past bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Windows, follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to [www.r-project.org](http://www.r-project.org).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **CRAN** section, select **CRAN mirror**, and select your Windows
    OS (stick to Linux; Hadoop is almost always used in a Linux environment).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the latest R version from the mirror.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the downloaded `.exe` to install R.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For Linux-Ubuntu, follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to [www.r-project.org](http://www.r-project.org).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **CRAN** section, select **CRAN mirror**, and select your OS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `/etc/apt/sources.list` file, add the CRAN `<mirror>` entry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download and update the package lists from the repositories using the `sudo
    apt-get update` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install R system using the `sudo apt-get install r-base` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For Linux-RHEL/CentOS, follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to [www.r-project.org](http://www.r-project.org).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **CRAN**, select **CRAN mirror**, and select Red Hat OS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the `R-*core-*.rpm` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the `.rpm` package using the `rpm -ivh R-*core-*.rpm` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install R system using `sudo yum install R`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For Mac, follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to [www.r-project.org](http://www.r-project.org).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **CRAN**, select **CRAN mirror**, and select your OS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the following files: `pkg`, `gfortran-*.dmg`, and `tcltk-*.dmg`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the `R-*.pkg` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, install the `gfortran-*.dmg` and `tcltk-*.dmg` files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After installing the base R package, it is advisable to install RStudio, which
    is a powerful and intuitive **Integrated Development Environment** (**IDE**) for
    R.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can use R distribution of Revolution Analytics as a Modern Data analytics
    tool for statistical computing and predictive analytics, which is available in
    free as well as premium versions. Hadoop integration is also available to perform
    Big Data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Installing RStudio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install RStudio, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to [http://www.rstudio.com/ide/download/desktop](http://www.rstudio.com/ide/download/desktop.).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the latest version of RStudio for your operating system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the installer file and install RStudio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The RStudio organization and user community has developed a lot of R packages
    for graphics and visualization, such as `ggplot2`, `plyr`, `Shiny`, `Rpubs`, and
    `devtools`.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the features of R language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are over 3,000 R packages and the list is growing day by day. It would
    be beyond the scope of any book to even attempt to explain all these packages.
    This book focuses only on the key features of R and the most frequently used and
    popular packages.
  prefs: []
  type: TYPE_NORMAL
- en: Using R packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: R packages are self-contained units of R functionality that can be invoked as
    functions. A good analogy would be a `.jar` file in Java. There is a vast library
    of R packages available for a very wide range of operations ranging from statistical
    operations and machine learning to rich graphic visualization and plotting. Every
    package will consist of one or more R functions. An R package is a re-usable entity
    that can be shared and used by others. R users can install the package that contains
    the functionality they are looking for and start calling the functions in the
    package. A comprehensive list of these packages can be found at [http://cran.r-project.org/](http://cran.r-project.org/)
    called **Comprehensive R Archive Network** (**CRAN**).
  prefs: []
  type: TYPE_NORMAL
- en: Performing data operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'R enables a wide range of operations. Statistical operations, such as mean,
    min, max, probability, distribution, and regression. Machine learning operations,
    such as linear regression, logistic regression, classification, and clustering.
    Universal data processing operations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data cleaning**: This option is to clean massive datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data exploration**: This option is to explore all the possible values of
    datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data analysis**: This option is to perform analytics on data with descriptive
    and predictive analytics data visualization, that is, visualization of analysis
    output programming'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To build an effective analytics application, sometimes we need to use the online
    **Application Programming Interface** (**API**) to dig up the data, analyze it
    with expedient services, and visualize it by third-party services. Also, to automate
    the data analysis process, programming will be the most useful feature to deal
    with.
  prefs: []
  type: TYPE_NORMAL
- en: R has its own programming language to operate data. Also, the available package
    can help to integrate R with other programming features. R supports object-oriented
    programming concepts. It is also capable of integrating with other programming
    languages, such as Java, PHP, C, and C++. There are several packages that will
    act as middle-layer programming features to aid in data analytics, which are similar
    to `sqldf`, `httr`, `RMongo`, `RgoogleMaps`, `RGoogleAnalytics`, and `google-prediction-api-r-client`.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing community support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the number of R users are escalating, the groups related to R are also increasing.
    So, R learners or developers can easily connect and get their uncertainty solved
    with the help of several R groups or communities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are many popular sources that can be found useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '**R mailing list**: This is an official R group created by R project owners.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**R blogs**: R has countless bloggers who are writing on several R applications.
    One of the most popular blog websites is [http://www.r-bloggers.com/](http://www.r-bloggers.com/)
    where all the bloggers contribute their blogs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stack overflow**: This is a great technical knowledge sharing platform where
    the programmers can post their technical queries and enthusiast programmers suggest
    a solution. For more information, visit [http://stats.stackexchange.com/](http://stats.stackexchange.com/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Groups**: There are many other groups existing on LinkedIn and Meetup where
    professionals across the world meet to discuss their problems and innovative ideas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Books**: There are also lot of books about R. Some of the popular books are
    *R in Action*, by *Rob Kabacoff*, *Manning Publications*, *R in a Nutshell*, by
    *Joseph Adler*, *O''Reilly Media*, *R and Data Mining*, by *Yanchang Zhao*, *Academic
    Press*, and *R Graphs Cookbook*, by *Hrishi Mittal*, *Packt Publishing*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing data modeling in R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data modeling is a machine learning technique to identify the hidden pattern
    from the historical dataset, and this pattern will help in future value prediction
    over the same data. This techniques highly focus on past user actions and learns
    their taste. Most of these data modeling techniques have been adopted by many
    popular organizations to understand the behavior of their customers based on their
    past transactions. These techniques will analyze data and predict for the customers
    what they are looking for. Amazon, Google, Facebook, eBay, LinkedIn, Twitter,
    and many other organizations are using data mining for changing the definition
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common data mining techniques are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression**: In statistics, regression is a classic technique to identify
    the scalar relationship between two or more variables by fitting the state line
    on the variable values. That relationship will help to predict the variable value
    for future events. For example, any variable y can be modeled as linear function
    of another variable x with the formula *y = mx+c*. Here, x is the predictor variable,
    y is the response variable, m is slope of the line, and c is the intercept. Sales
    forecasting of products or services and predicting the price of stocks can be
    achieved through this regression. R provides this regression feature via the `lm`
    method, which is by default present in R.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: This is a machine-learning technique used for labeling
    the set of observations provided for training examples. With this, we can classify
    the observations into one or more labels. The likelihood of sales, online fraud
    detection, and cancer classification (for medical science) are common applications
    of classification problems. Google Mail uses this technique to classify e-mails
    as spam or not. Classification features can be served by `glm`, `glmnet`, `ksvm`,
    `svm`, and `randomForest` in R.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: This technique is all about organizing similar items into groups
    from the given collection of items. User segmentation and image compression are
    the most common applications of clustering. Market segmentation, social network
    analysis, organizing the computer clustering, and astronomical data analysis are
    applications of clustering. Google News uses these techniques to group similar
    news items into the same category. Clustering can be achieved through the `knn`,
    `kmeans`, `dist`, `pvclust`, and `Mclust` methods in R.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation**: The recommendation algorithms are used in recommender systems
    where these systems are the most immediately recognizable machine learning techniques
    in use today. Web content recommendations may include similar websites, blogs,
    videos, or related content. Also, recommendation of online items can be helpful
    for cross-selling and up-selling. We have all seen online shopping portals that
    attempt to recommend books, mobiles, or any items that can be sold on the Web
    based on the user''s past behavior. Amazon is a well-known e-commerce portal that
    generates 29 percent of sales through recommendation systems. Recommender systems
    can be implemented via `Recommender()`with the `recommenderlab` package in R.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we presume that you are aware of R, what it is, how to install it, what
    it's key features are, and why you may want to use it. Now we need to know the
    limitations of R (this is a better introduction to Hadoop). Before processing
    the data; R needs to load the data into **random access memory** (**RAM**). So,
    the data needs to be smaller than the available machine memory. For data that
    is larger than the machine memory, we consider it as Big Data (only in our case
    as there are many other definitions of Big Data).
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this Big Data issue, we need to scale the hardware configuration; however,
    this is a temporary solution. To get this solved, we need to get a Hadoop cluster
    that is able to store it and perform parallel computation across a large computer
    cluster. Hadoop is the most popular solution. Hadoop is an open source Java framework,
    which is the top level project handled by the Apache software foundation. Hadoop
    is inspired by the Google filesystem and MapReduce, mainly designed for operating
    on Big Data by distributed processing.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop mainly supports Linux operating systems. To run this on Windows, we need
    to use VMware to host Ubuntu within the Windows OS. There are many ways to use
    and install Hadoop, but here we will consider the way that supports R best. Before
    we combine R and Hadoop, let us understand what Hadoop is.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning contains all the data modeling techniques that can be explored
    with the web link [http://en.wikipedia.org/wiki/Machine_learning](http://en.wikipedia.org/wiki/Machine_learning).
  prefs: []
  type: TYPE_NORMAL
- en: The structure blog on Hadoop installation by Michael Noll can be found at [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding different Hadoop modes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hadoop is used with three different modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The standalone mode**: In this mode, you do not need to start any Hadoop
    daemons. Instead, just call `~/Hadoop-directory/bin/hadoop` that will execute
    a Hadoop operation as a single Java process. This is recommended for testing purposes.
    This is the default mode and you don''t need to configure anything else. All daemons,
    such as NameNode, DataNode, JobTracker, and TaskTracker run in a single Java process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The pseudo mode**: In this mode, you configure Hadoop for all the nodes.
    A separate **Java Virtual Machine** (**JVM**) is spawned for each of the Hadoop
    components or daemons like mini cluster on a single host.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The full distributed mode**: In this mode, Hadoop is distributed across multiple
    machines. Dedicated hosts are configured for Hadoop components. Therefore, separate
    JVM processes are present for all daemons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Hadoop installation steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hadoop can be installed in several ways; we will consider the way that is better
    to integrate with R. We will choose Ubuntu OS as it is easy to install and access
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Hadoop on Linux, Ubuntu flavor (single and multinode cluster).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Cloudera Hadoop on Ubuntu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Hadoop on Linux, Ubuntu flavor (single node cluster)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To install Hadoop over Ubuntu OS with the pseudo mode, we need to meet the
    following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: Sun Java 6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dedicated Hadoop system user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring SSH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disabling IPv6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The provided Hadoop installation will be supported with Hadoop MRv1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the given steps to install Hadoop:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the latest Hadoop sources from the Apache software foundation. Here
    we have considered Apache Hadoop 1.0.3, whereas the latest version is 1.1.x.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the `$JAVA_HOME` and `$HADOOP_HOME` variables to the`.bashrc` file of Hadoop
    system user and the updated `.bashrc` file looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Update the Hadoop configuration files with the `conf/*-site.xml` format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, the three files will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`conf/core-site.xml`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`conf/mapred-site.xml`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`conf/hdfs-site.xml`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After completing the editing of these configuration files, we need to set up
    the distributed filesystem across the Hadoop clusters or node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Format **Hadoop Distributed File System** (**HDFS**) via NameNode by using
    the following command line:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start your single node cluster by using the following command line:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Hadoop on Linux, Ubuntu flavor (multinode cluster)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned how to install Hadoop on a single node cluster. Now we will see how
    to install Hadoop on a multinode cluster (the full distributed mode).
  prefs: []
  type: TYPE_NORMAL
- en: For this, we need several nodes configured with a single node Hadoop cluster.
    To install Hadoop on multinodes, we need to have that machine configured with
    a single node Hadoop cluster as described in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: 'After getting the single node Hadoop cluster installed, we need to perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the networking phase, we are going to use two nodes for setting up a full
    distributed Hadoop mode. To communicate with each other, the nodes need to be
    in the same network in terms of software and hardware configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Among these two, one of the nodes will be considered as master and the other
    will be considered as slave. So, for performing Hadoop operations, master needs
    to be connected to slave. We will enter `192.168.0.1` in the master machine and
    `192.168.0.2` in the slave machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the `/etc/hosts` directory in both the nodes. It will look as `192.168.0.1
    master` and `192.168.0.2 slave`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: You can perform the **Secure Shell** (**SSH**) setup similar to what we did
    for a single node cluster setup. For more details, visit [http://www.michael-noll.com](http://www.michael-noll.com).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Updating `conf/*-site.xml`: We must change all these configuration files in
    all of the nodes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`conf/core-site.xml` and `conf/mapred-site.xml`: In the single node setup,
    we have updated these files. So, now we need to just replace `localhost` by `master`
    in the value tag.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conf/hdfs-site.xml`: In the single node setup, we have set the value of `dfs.replication`
    as `1`. Now we need to update this as `2`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the formatting HDFS phase, before we start the multinode cluster, we need
    to format HDFS with the following command (from the master node):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we have completed all the steps to install the multinode Hadoop cluster.
    To start the Hadoop clusters, we need to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start HDFS daemons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start MapReduce daemons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, we can start all the daemons with a single command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To stop all these daemons, fire:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These installation steps are reproduced after being inspired by the blogs ([http://www.michael-noll.com](http://www.michael-noll.com))
    of Michael Noll, who is a researcher and Software Engineer based in Switzerland,
    Europe. He works as a Technical lead for a large scale computing infrastructure
    on the Apache Hadoop stack at VeriSign.
  prefs: []
  type: TYPE_NORMAL
- en: Now the Hadoop cluster has been set up on your machines. For the installation
    of the same Hadoop cluster on single node or multinode with extended Hadoop components,
    try the Cloudera tool.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Cloudera Hadoop on Ubuntu
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Cloudera Hadoop** (**CDH**) is Cloudera''s open source distribution that
    targets enterprise class deployments of Hadoop technology. Cloudera is also a
    sponsor of the Apache software foundation. CDH is available in two versions: CDH3
    and CDH4\. To install one of these, you must have Ubuntu with either 10.04 LTS
    or 12.04 LTS (also, you can try CentOS, Debian, and Red Hat systems). Cloudera
    manager will make this installation easier for you if you are installing a Hadoop
    on cluster of computers, which provides GUI-based Hadoop and its component installation
    over a whole cluster. This tool is very much recommended for large clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to meet the following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring SSH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OS with the following criteria:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ubuntu 10.04 LTS or 12.04 LTS with 64 bit
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Red Hat Enterprise Linux 5 or 6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CentOS 5 or 6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Oracle Enterprise Linux 5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SUSE Linux Enterprise server 11 (SP1 or lasso)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Debian 6.0
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The installation steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download and run the Cloudera manager installer: To initialize the Cloudera
    manager installation process, we need to first download the `cloudera-manager-installer.bin`
    file from the download section of the Cloudera website. After that, store it at
    the cluster so that all the nodes can access this. Allow ownership for execution
    permission of `cloudera-manager-installer.bin` to the user. Run the following
    command to start execution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Read the Cloudera manager **Readme** and then click on **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start the Cloudera manager admin console: The Cloudera manager admin console
    allows you to use Cloudera manager to install, manage, and monitor Hadoop on your
    cluster. After accepting the license from the Cloudera service provider, you need
    to traverse to your local web browser by entering `http://localhost:7180` in your
    address bar. You can also use any of the following browsers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Firefox 11 or higher
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Chrome
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Internet Explorer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Safari
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Log in to the Cloudera manager console with the default credentials using `admin`
    for both the username and password. Later on you can change it as per your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the Cloudera manager for automated CDH3 installation and configuration
    via browser: This step will install most of the required Cloudera Hadoop packages
    from Cloudera to your machines. The steps are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install and validate your Cloudera manager license key file if you have chosen
    a full version of software.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the hostname or IP address range for your CDH cluster installation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect to each host with SSH.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the **Java Development Kit** (**JDK**) (if not already installed), the
    Cloudera manager agent, and CDH3 or CDH4 on each cluster host.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure Hadoop on each node and start the Hadoop services.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After running the wizard and using the Cloudera manager, you should change
    the default administrator password as soon as possible. To change the administrator
    password, follow these steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the icon with the gear sign to display the administration page.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the **Password** tab.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a new password twice and then click on **Update**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test the Cloudera Hadoop installation: You can check the Cloudera manager installation
    on your cluster by logging into the Cloudera manager admin console and by clicking
    on the **Services** tab. You should see something like the following screenshot:![Installing
    Cloudera Hadoop on Ubuntu](img/3282OS_01_01.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cloudera manager admin console
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also click on each service to see more detailed information. For example,
    if you click on the **hdfs1** link, you might see something like the following
    screenshot:![Installing Cloudera Hadoop on Ubuntu](img/3282OS_01_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cloudera manger admin console—HDFS service
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: To avoid these installation steps, use preconfigured Hadoop instances with Amazon
    Elastic MapReduce and MapReduce.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you want to use Hadoop on Windows, try the HDP tool by Hortonworks. This
    is 100 percent open source, enterprise grade distribution of Hadoop. You can download
    the HDP tool at [http://hortonworks.com/download/](http://hortonworks.com/download/).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Understanding Hadoop features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hadoop is specially designed for two core concepts: HDFS and MapReduce. Both
    are related to distributed computation. MapReduce is believed as the heart of
    Hadoop that performs parallel processing over distributed data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see more details on Hadoop''s features:'
  prefs: []
  type: TYPE_NORMAL
- en: HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MapReduce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding HDFS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HDFS is Hadoop's own rack-aware filesystem, which is a UNIX-based data storage
    layer of Hadoop. HDFS is derived from concepts of Google filesystem. An important
    characteristic of Hadoop is the partitioning of data and computation across many
    (thousands of) hosts, and the execution of application computations in parallel,
    close to their data. On HDFS, data files are replicated as sequences of blocks
    in the cluster. A Hadoop cluster scales computation capacity, storage capacity,
    and I/O bandwidth by simply adding commodity servers. HDFS can be accessed from
    applications in many different ways. Natively, HDFS provides a Java API for applications
    to use.
  prefs: []
  type: TYPE_NORMAL
- en: The Hadoop clusters at Yahoo! span 40,000 servers and store 40 petabytes of
    application data, with the largest Hadoop cluster being 4,000 servers. Also, one
    hundred other organizations worldwide are known to use Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the characteristics of HDFS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us now look at the characteristics of HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: Fault tolerant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs with commodity hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Able to handle large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master slave paradigm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write once file access only
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding MapReduce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MapReduce is a programming model for processing large datasets distributed on
    a large cluster. MapReduce is the heart of Hadoop. Its programming paradigm allows
    performing massive data processing across thousands of servers configured with
    Hadoop clusters. This is derived from Google MapReduce.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hadoop MapReduce is a software framework for writing applications easily, which
    process large amounts of data (multiterabyte datasets) in parallel on large clusters
    (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.
    This MapReduce paradigm is divided into two phases, Map and Reduce that mainly
    deal with key and value pairs of data. The Map and Reduce task run sequentially
    in a cluster; the output of the Map phase becomes the input for the Reduce phase.
    These phases are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Map phase**: Once divided, datasets are assigned to the task tracker to perform
    the Map phase. The data functional operation will be performed over the data,
    emitting the mapped key and value pairs as the output of the Map phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce phase**: The master node then collects the answers to all the subproblems
    and combines them in some way to form the output; the answer to the problem it
    was originally trying to solve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The five common steps of parallel computing are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Preparing the `Map()` input: This will take the input data row wise and emit
    key value pairs per rows, or we can explicitly change as per the requirement.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Map input: list (k1, v1)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the user-provided `Map()` code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Map output: list (k2, v2)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Shuffle the Map output to the Reduce processors. Also, shuffle the similar keys
    (grouping them) and input them to the same reducer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the user-provided `Reduce()` code: This phase will run the custom reducer
    code designed by developer to run on shuffled data and emit key and value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reduce input: (k2, list(v2))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reduce output: (k3, v3)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Produce the final output: Finally, the master node collects all reducer output
    and combines and writes them in a text file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The reference links to review on Google filesystem can be found at [http://research.google.com/archive/gfs.html](http://research.google.com/archive/gfs.html)
    and Google MapReduce can be found at [http://research.google.com/archive/mapreduce.html](http://research.google.com/archive/mapreduce.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Learning the HDFS and MapReduce architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since HDFS and MapReduce are considered to be the two main features of the Hadoop
    framework, we will focus on them. So, let's first start with HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the HDFS architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HDFS can be presented as the master/slave architecture. HDFS master is named
    as NameNode whereas slave as DataNode. NameNode is a sever that manages the filesystem
    namespace and adjusts the access (open, close, rename, and more) to files by the
    client. It divides the input data into blocks and announces which data block will
    be store in which DataNode. DataNode is a slave machine that stores the replicas
    of the partitioned dataset and serves the data as the request comes. It also performs
    block creation and deletion.
  prefs: []
  type: TYPE_NORMAL
- en: The internal mechanism of HDFS divides the file into one or more blocks; these
    blocks are stored in a set of data nodes. Under normal circumstances of the replication
    factor three, the HDFS strategy is to place the first copy on the local node,
    second copy on the local rack with a different node, and a third copy into different
    racks with different nodes. As HDFS is designed to support large files, the HDFS
    block size is defined as 64 MB. If required, this can be increased.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding HDFS components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'HDFS is managed with the master-slave architecture included with the following
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NameNode**: This is the master of the HDFS system. It maintains the directories,
    files, and manages the blocks that are present on the DataNodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataNode**: These are slaves that are deployed on each machine and provide
    actual storage. They are responsible for serving read-and-write data requests
    for the clients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secondary NameNode**: This is responsible for performing periodic checkpoints.
    So, if the NameNode fails at any time, it can be replaced with a snapshot image
    stored by the secondary NameNode checkpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the MapReduce architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MapReduce is also implemented over master-slave architectures. Classic MapReduce
    contains job submission, job initialization, task assignment, task execution,
    progress and status update, and job completion-related activities, which are mainly
    managed by the JobTracker node and executed by TaskTracker. Client application
    submits a job to the JobTracker. Then input is divided across the cluster. The
    JobTracker then calculates the number of map and reducer to be processed. It commands
    the TaskTracker to start executing the job. Now, the TaskTracker copies the resources
    to a local machine and launches JVM to map and reduce program over the data. Along
    with this, the TaskTracker periodically sends update to the JobTracker, which
    can be considered as the heartbeat that helps to update JobID, job status, and
    usage of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MapReduce components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MapReduce is managed with master-slave architecture included with the following
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**JobTracker**: This is the master node of the MapReduce system, which manages
    the jobs and resources in the cluster (TaskTrackers). The JobTracker tries to
    schedule each map as close to the actual data being processed on the TaskTracker,
    which is running on the same DataNode as the underlying block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TaskTracker**: These are the slaves that are deployed on each machine. They
    are responsible for running the map and reducing tasks as instructed by the JobTracker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the HDFS and MapReduce architecture by plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this plot, both HDFS and MapReduce master and slave components have been
    included, where NameNode and DataNode are from HDFS and JobTracker and TaskTracker
    are from the MapReduce paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both paradigms consisting of master and slave candidates have their own specific
    responsibility to handle MapReduce and HDFS operations. In the next plot, there
    is a plot with two sections: the preceding one is a MapReduce layer and the following
    one is an HDFS layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the HDFS and MapReduce architecture by plot](img/3282OS_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The HDFS and MapReduce architecture
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop is a top-level Apache project and is a very complicated Java framework.
    To avoid technical complications, the Hadoop community has developed a number
    of Java frameworks that has added an extra value to Hadoop features. They are
    considered as Hadoop subprojects. Here, we are departing to discuss several Hadoop
    components that can be considered as an abstraction of HDFS or MapReduce.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Hadoop subprojects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mahout** is a popular data mining library. It takes the most popular data
    mining scalable machine learning algorithms for performing clustering, classification,
    regression, and statistical modeling to prepare intelligent applications. Also,
    it is a scalable machine-learning library.'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mahout is distributed under a commercially friendly Apache software license.
    The goal of Apache Mahout is to build a vibrant, responsive, and diverse community
    to facilitate discussions not only on the project itself but also on potential
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some companies that are using Mahout:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon**: This a shopping portal for providing personalization recommendation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AOL**: This is a shopping portal for shopping recommendations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drupal**: This is a PHP content management system using Mahout for providing
    open source content-based recommendation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**iOffer**: This is a shopping portal, which uses Mahout''s Frequent Pattern
    Set Mining and collaborative filtering to recommend items to users'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LucidWorks Big Data**: This is a popular analytics firm, which uses Mahout
    for clustering, duplicate document detection, phase extraction, and classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Radoop**: This provides a drag-and-drop interface for Big Data analytics,
    including Mahout clustering and classification algorithms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Twitter**: This is a social networking site, which uses Mahout''s **Latent
    Dirichlet Allocation** (**LDA**) implementation for user interest modeling and
    maintains a fork of Mahout on GitHub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Yahoo**!: This is the world''s most popular web service provider, which uses
    Mahout''s Frequent Pattern Set Mining for Yahoo! Mail'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: The reference links on the Hadoop ecosystem can be found at [http://www.revelytix.com/?q=content/hadoop-ecosystem](http://www.revelytix.com/?q=content/hadoop-ecosystem).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Apache HBase** is a distributed Big Data store for Hadoop. This allows random,
    real-time read/write access to Big Data. This is designed as a column-oriented
    data storage model innovated after inspired by Google BigTable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the companies using HBase:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Yahoo!**: This is the world''s popular web service provider for near duplicate
    document detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Twitter**: This is a social networking site for version control storage and
    retrieval'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mahalo**: This is a knowledge sharing service for similar content recommendation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NING**: This is a social network service provider for real-time analytics
    and reporting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**StumbleUpon**: This is a universal personalized recommender system, real-time
    data storage, and data analytics platform'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Veoh**: This is an online multimedia content sharing platform for user profiling
    system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: For Google Big Data, distributed storage system for structured data, refer the
    link [http://research.google.com/archive/bigtable.html](http://research.google.com/archive/bigtable.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Hive** is a Hadoop-based data warehousing like framework developed by Facebook.
    It allows users to fire queries in SQL-like languages, such as HiveQL, which are
    highly abstracted to Hadoop MapReduce. This allows SQL programmers with no MapReduce
    experience to use the warehouse and makes it easier to integrate with business
    intelligence and visualization tools for real-time query processing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pig** is a Hadoop-based open source platform for analyzing the large scale
    datasets via its own SQL-like language: Pig Latin. This provides a simple operation
    and programming interface for massive, complex data-parallelization computation.
    This is also easier to develop; it''s more optimized and extensible. Apache Pig
    has been developed by Yahoo!. Currently, Yahoo! and Twitter are the primary Pig
    users.'
  prefs: []
  type: TYPE_NORMAL
- en: For developers, the direct use of Java APIs can be tedious or error-prone, but
    also limits the Java programmer's use of Hadoop programming's flexibility. So,
    Hadoop provides two solutions that enable making Hadoop programming for dataset
    management and dataset analysis with MapReduce easier—these are Pig and Hive,
    which are always confusing.
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Sqoop** provides Hadoop data processing platform and relational databases,
    data warehouse, and other non-relational databases quickly transferring large
    amounts of data in a new way. Apache Sqoop is a mutual data tool for importing
    data from the relational databases to Hadoop HDFS and exporting data from HDFS
    to relational databases.'
  prefs: []
  type: TYPE_NORMAL
- en: It works together with most modern relational databases, such as MySQL, PostgreSQL,
    Oracle, Microsoft SQL Server, and IBM DB2, and enterprise data warehouse. Sqoop
    extension API provides a way to create new connectors for the database system.
    Also, the Sqoop source comes up with some popular database connectors. To perform
    this operation, Sqoop first transforms the data into Hadoop MapReduce with some
    logic of database schema creation and transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Zookeeper** is also a Hadoop subproject used for managing Hadoop,
    Hive, Pig, HBase, Solr, and other projects. Zookeeper is an open source distributed
    applications coordination service, which is designed with Fast Paxos algorithm-based
    synchronization and configuration and naming services such as maintenance of distributed
    applications. In programming, Zookeeper design is a very simple data model style,
    much like the system directory tree structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zookeeper is divided into two parts: the server and client. For a cluster of
    Zookeeper servers, only one acts as a leader, which accepts and coordinates all
    rights. The rest of the servers are read-only copies of the master. If the leader
    server goes down, any other server can start serving all requests. Zookeeper clients
    are connected to a server on the Zookeeper service. The client sends a request,
    receives a response, accesses the observer events, and sends a heartbeat via a
    TCP connection with the server.'
  prefs: []
  type: TYPE_NORMAL
- en: For a high-performance coordination service for distributed applications, Zookeeper
    is a centralized service for maintaining configuration information, naming, and
    providing distributed synchronization and group services. All these kinds of services
    are used in some form or another by distributed applications. Each time they are
    implemented, there is a lot of work that goes into fixing the bugs and race conditions
    that are inevitable. These services lead to management complexity when the applications
    are deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Solr is an open source enterprise search platform from the Apache license
    project. Apache Solr is highly scalable, supporting distributed search and index
    replication engine. This allows building web application with powerful text search,
    faceted search, real-time indexing, dynamic clustering, database integration,
    and rich document handling.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Solr is written in Java, which runs as a standalone server to serve the
    search results via REST-like HTTP/XML and JSON APIs. So, this Solr server can
    be easily integrated with an application, which is written in other programming
    languages. Due to all these features, this search server is used by Netflix, AOL,
    CNET, and Zappos.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ambari** is very specific to Hortonworks. Apache Ambari is a web-based tool
    that supports Apache Hadoop cluster supply, management, and monitoring. Ambari
    handles most of the Hadoop components, including HDFS, MapReduce, Hive, Pig, HBase,
    Zookeeper, Sqoop, and HCatlog as centralized management.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Ambari is able to install security based on the Kerberos authentication
    protocol over the Hadoop cluster. Also, it provides role-based user authentication,
    authorization, and auditing functions for users to manage integrated LDAP and
    Active Directory.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what is R, Hadoop, and their features, and how to
    install them before going on to analyzing the datasets with R and Hadoop. In the
    next chapter, we are going to learn what MapReduce is and how to develop MapReduce
    programs with Apache Hadoop.
  prefs: []
  type: TYPE_NORMAL
