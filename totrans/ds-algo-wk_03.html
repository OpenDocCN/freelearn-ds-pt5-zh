<html><head></head><body><div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Decision Trees</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">A decision tree is the arrangement of data in a tree structure where, at each node, data is separated into different branches according to the value of the attribute at the node.</p>
<p class="mce-root">To construct a decision tree, we will use a standard ID3 learning algorithm that chooses an attribute that classifies data samples in the best possible way to maximize the information gain—a measure based on information entropy.</p>
<p class="mce-root">In this chapter, we will cover the following topics:</p>
<ul class="calibre14">
<li class="calibre15">What a decision tree is and how to represent data in a decision tree through the swim preference example</li>
<li class="calibre15">The concepts of information entropy and information gain, theoretically in the first instance, before applying the swim preference example in practical terms in the <em class="calibre5">Information theory</em> section</li>
<li class="calibre15">How to use a ID3 algorithm to construct a decision tree from the training data, and its implementation in Python</li>
<li class="calibre15">How to classify new data items using the constructed decision tree through the swim preference example</li>
<li class="calibre15">How to carry out an alternative analysis of the chess playing problem in <a href="4f54d05d-6791-47c5-a260-c2fd563a55cf.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 2</a>, Naive Bayes, using decision trees and how the results of the two algorithms may differ</li>
<li class="calibre15">You will verify your understanding in the <em class="calibre5">Problems</em> section and look at when to use decision trees as a method of analysis</li>
<li class="calibre15">How to deal with data inconsistencies during decision tree construction with the <em class="calibre5">Going shopping</em> example</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Swim preference – representing data using a decision tree</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">We may have certain preferences that determine whether or not we would swim. These can be recorded in a table, as follows:</p>
<table class="msotablegrid" border="1">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Swimming suit</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Water temperature</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Swim preference</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Small</p>
</td>
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Small</p>
</td>
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Good</p>
</td>
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre28">
<td class="calibre25">
<p class="calibre26">Good</p>
</td>
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">The data in this table can alternatively be presented in the following decision tree:</p>
<p class="cdpaligncenter"><img class="alignnone15" src="Images/b974fd83-2b36-4328-8c77-02a7deccd42d.png" width="479" height="243"/></p>
<div class="cdpaligncenter2">Figure 3.1: Decision tree for the Swim preference example</div>
<p class="mce-root">At the root node, we ask the question—do you have a swimming suit? The response to the question separates the available data into three groups, each with two rows. If the attribute is <kbd class="calibre17">swimming suit = none</kbd>, then two rows have the swim preference<span class="calibre9"> </span>attribute as <kbd class="calibre17">no</kbd>. Therefore, there is no need to ask a question about the temperature of the water, as all the samples with the <kbd class="calibre17">swimming suit = none</kbd> attribute would be classified as <kbd class="calibre17">no</kbd>. This is also true for the <kbd class="calibre17">swimming suit = small</kbd> attribute. In the case of <kbd class="calibre17">swimming suit = good</kbd>, the remaining two rows can be divided into two classes – <kbd class="calibre17">no</kbd> and <kbd class="calibre17">yes</kbd>.</p>
<p class="mce-root">Without further information, we would not be able to classify each row correctly. Fortunately, there is one more question that can be asked about each row that classifies it correctly. For the row with the <kbd class="calibre17">water=cold</kbd> attribute, the swimming preference is <kbd class="calibre17">no</kbd>. For the row with the <kbd class="calibre17">water=warm</kbd> attribute, the swimming preference is <kbd class="calibre17">yes</kbd>.</p>
<p class="mce-root">To summarize, starting with the root node, we ask a question at every node and, based on the answer, we move down the tree until we reach a leaf node where we find the class of the data item corresponding to those answers.</p>
<p class="mce-root">This is how we can use a ready-made decision tree to classify samples of data. But it is also important to know how to construct a decision tree from data.</p>
<p class="mce-root">Which attribute has a question at which node? How does this reflect on the construction of a decision tree? If we change the order of the attributes, can the resulting decision tree classify better than another tree?</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Information theory</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Information theory studies the quantification of information, its storage, and communication. We introduce concepts of information entropy and information gain, which are used to construct a decision tree using the ID3 algorithm.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Information entropy</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">The information entropy of any given piece data is a measure of the smallest amount of information necessary to represent a data item from that data. The units of information entropy are familiar - bits, bytes, kilobytes, and so on. The lower the information entropy, the more regular the data is, and the more patterns occur in the data, thus, the smaller the quantity of information required to represent it. That is how compression tools on computers can take large text files and compress them to a much smaller size, as words and word expressions keep reoccurring, forming a pattern.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Coin flipping</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Imagine we flip an unbiased coin. We would like to know whether the result is heads or tails. How much information do we need to represent the result? Both words, head, and tail, consist of four characters, and if we represent one character with one byte (8 bits), as is standard in the ASCII table, then we would need four bytes, or 32 bits, to represent the result.</p>
<p class="mce-root">But information entropy is the smallest amount of data necessary to represent the result. We know that there are only two possible results—heads or tails. If we agree to represent head with 0 and tail with 1, then 1 bit would be sufficient to communicate the result efficiently. Here, the data is the space of the possibilities of the result of the coin throw. It is the set <kbd class="calibre17">{head,tail}</kbd> that can be represented as a set <kbd class="calibre17">{0,1}</kbd>. The actual result is a data item from this set. It turns out that the entropy of the set is 1. This is owing to the probability of head and tail both being 50%.</p>
<p class="mce-root">Now imagine that the coin is biased and throws heads 25% of the time and tails 75% of the time. What would the entropy of the probability space <kbd class="calibre17">{0,1}</kbd> be this time? We could certainly represent the result with one bit of information. But can we do better? One bit is, of course, indivisible, but maybe we could generalize the concept of information to indiscrete amounts.</p>
<p class="mce-root">In the previous example, we knew nothing about the previous result of the coin flip unless we looked at the coin. But in the example with the biased coin, we know that the result is more likely to be tails. If we recorded <em class="calibre18">n</em> results of coin flips in a file representing heads with 0 and tails with 1, then about 75 percent of the bits there would have the value 1, and 25 percent of them would have the value 0. The size of such a file would be <em class="calibre18">n</em> bits. But since it is more regular (a pattern of 1s prevails in it), a good compression tool should be able to compress it to less than <em class="calibre18">n</em> bits.</p>
<p class="mce-root">To learn the theory behind to compression and how much information is necessary to represent a data item, let's take a look at a precise definition of information entropy.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Definition of information entropy</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Suppose that we are given a probability space, <em class="calibre18">S</em>, with the elements <em class="calibre18">1, 2, ..., n</em>. The probability that an element, <em class="calibre18">i</em>, will be chosen from the probability space is <em class="calibre18">p<sub class="calibre29">i</sub></em>. The information entropy of the probability space is then defined as follows:</p>
<p class="mce-root"><img class="fm-editor-equation95" src="Images/7e5fdefb-a6bf-44f2-8797-bb12161e973b.png" width="3350" height="220"/></p>
<p class="mce-root">In the preceding formula, <em class="calibre18">log<sub class="calibre29">2</sub></em> is a binary logarithm.</p>
<p class="mce-root">Hence, the information entropy of the probability space of unbiased coin throws is as follows:</p>
<p class="cdpalignleft1"><img class="fm-editor-equation96" src="Images/25c77162-1f29-42fb-9d8a-3aacab40d80e.png" width="4280" height="220"/></p>
<p class="mce-root">When the coin is biased, with a 25% chance of heads and a 75% chance of tails, then the information entropy of such a space is as follows:</p>
<p class="cdpalignleft1"><img class="fm-editor-equation97" src="Images/cd1627da-fe47-48b2-8b52-dde0f8070f10.png" width="4780" height="220"/></p>
<p class="mce-root">This is less than 1. Thus, for example, if we had a large file with about 25% of 0 bits and 75% of 1 bits, a good compression tool should be able to compress it down to about 81.12% of its size.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Information gain</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Information gain is the amount of information entropy gained as a result of a certain procedure. For example, if we would like to know the results of three fair coins, then the information entropy is 3. But if we could look at the third coin, then the information entropy of the result for the remaining two coins would be 2. Thus, by looking at the third coin, we gained one bit of information, so the information gain is 1.</p>
<p class="mce-root">We may also gain the information entropy by dividing the whole set, <em class="calibre18">S</em>, into sets, grouping them by a similar pattern. If we group elements by their value of an attribute, <em class="calibre18">A</em>, then we define the information gain as follows:</p>
<p class="calibre47"><img src="Images/f5d65d98-1431-4e05-9704-64d62f7dc1a5.png" width="1614" height="239" class="calibre48"/></p>
<p class="mce-root">Here, <em class="calibre18">S<sub class="calibre29">v</sub></em>, is a set with the elements of <em class="calibre18">S</em> that have the value, <em class="calibre18">v</em>, for the attribute, <em class="calibre18">A</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Swim preference – information gain calculation</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Let's calculate the information gain for the six rows in the swim preference example by taking a <kbd class="calibre17">swimming suit</kbd> as an attribute. Because we are interested in whether a given row of data is classified as <kbd class="calibre17">no</kbd> or <kbd class="calibre17">yes</kbd> in response to the question as to whether you should go for a swim, we will use the swim preference to calculate the entropy and information gain. We partition the set <em class="calibre18">S</em> using the <kbd class="calibre17">swimming suit</kbd> <span class="calibre9">attribute</span>:</p>
<p class="mce-root"><em class="calibre18">S<sub class="calibre29">none</sub>={(none,cold,no),(none,warm,no)}</em></p>
<p class="mce-root"><em class="calibre18">S<sub class="calibre29">small</sub>={(small,cold,no),(small,warm,no)}</em></p>
<p class="mce-root"><em class="calibre18">S<sub class="calibre29">good</sub>={(good,cold,no),(good,warm,yes)}</em></p>
<p class="mce-root">The information entropy of <em class="calibre18">S</em> is:</p>
<p class="mce-root"><em class="calibre18">E(S)=<img class="fm-editor-equation98" src="Images/36bd8ff7-5288-4b68-aa7e-e2ae36444bce.png" width="4470" height="220"/></em></p>
<p class="mce-root">The information entropy of the partitions is:</p>
<p class="mce-root"><em class="calibre18">E(S<sub class="calibre29">none</sub>)=-(2/2)*log<sub class="calibre29">2</sub>(2/2)=-log<sub class="calibre29">2</sub>(1)=0,</em> since all instances have the class <kbd class="calibre17">no</kbd>.</p>
<p class="mce-root"><em class="calibre18">E(S<sub class="calibre29">small</sub>)=0</em> for a similar reason.</p>
<p class="mce-root"><em class="calibre18">E(S<sub class="calibre29">good</sub>)=<img class="fm-editor-equation99" src="Images/c6aad04d-c94c-4ac3-8bcb-009ec62772bc.png" width="3400" height="220"/></em></p>
<p class="mce-root">Therefore, the information gain is:</p>
<p class="mce-root"><em class="calibre18">IG(S,swimming suit)=E(S)-[(2/6)*E(S<sub class="calibre29">none</sub>)+(2/6)*E(S<sub class="calibre29">small</sub>)+(2/6)*E(S<sub class="calibre29">good</sub>)]</em></p>
<p class="mce-root"><em class="calibre18">=0.65002242164-(1/3)=0.3166890883</em></p>
<p class="mce-root">If we chose the <kbd class="calibre17">water temperature</kbd> <span class="calibre9">attribute </span>to partition the set, <em class="calibre18">S</em>, what would be the information gain, <em class="calibre18">IG(S,water temperature)</em>? The water temperature partitions the set, <em class="calibre18">S</em>, into the following sets:</p>
<p class="mce-root"><em class="calibre18">S<sub class="calibre29">cold</sub>={(none,cold,no),(small,cold,no),(good,cold,no)}</em></p>
<p class="mce-root"><em class="calibre18">S<sub class="calibre29">warm</sub>={(none,warm,no),(small,warm,no),(good,warm,yes)}</em></p>
<p class="mce-root">Their entropies are as follows:</p>
<p class="mce-root"><em class="calibre18">E(S<sub class="calibre29">cold</sub>)=0,</em> since all instances are classified as no.</p>
<p class="mce-root"><em class="calibre18">E(S<sub class="calibre29">warm</sub>)=-(2/3)*log<sub class="calibre29">2</sub>(2/3)-(1/3)*log<sub class="calibre29">2</sub>(1/3)~0.91829583405</em></p>
<p class="mce-root">Therefore, the information gain from partitioning the set, <em class="calibre18">S</em>, using the <em class="calibre18">water temperature</em> <span class="calibre9">attribute </span>is as follows:</p>
<p class="mce-root"><em class="calibre18">IG(S,water temperature)=E(S)-[(1/2)*E(S<sub class="calibre29">cold</sub>)+(1/2)*E(S<sub class="calibre29">warm</sub>)]</em></p>
<p class="mce-root"><em class="calibre18">= 0.65002242164-0.5*0.91829583405=0.19087450461</em></p>
<p class="mce-root">This is less than <em class="calibre18">IG(S,swimming suit)</em>. Therefore, we can gain more information about the set, <em class="calibre18">S</em>, (the classification of its instances) by partitioning it as per the <kbd class="calibre17">swimming suit</kbd> <span class="calibre9">attribute </span>instead of the <kbd class="calibre17">water temperature</kbd> <span class="calibre9">attribute</span>. This finding will be the basis of the ID3 algorithm when constructing a decision tree in the next section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">ID3 algorithm – decision tree construction</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">The ID3 algorithm constructs a decision tree from data based on the information gain. In the beginning, we start with the set, <em class="calibre18">S</em>. The data items in the set, <em class="calibre18">S</em>, have various properties, according to which we can partition the set, <em class="calibre18">S</em>. If an attribute, <em class="calibre18">A</em>, has the values <em class="calibre18">{v<sub class="calibre29">1</sub>, ..., v<sub class="calibre29">n</sub>}</em>, then we partition the set, <em class="calibre18">S</em>, into the sets <em class="calibre18">S<sub class="calibre29">1</sub></em>, ..., <em class="calibre18">S<sub class="calibre29">n</sub></em>, where the set, S<sub class="calibre29">i</sub>, is a subset of the set, <em class="calibre18">S</em>, where the elements have the value, <em class="calibre18">v<sub class="calibre29">i</sub></em>, for the attribute, <em class="calibre18">A</em>.</p>
<p class="mce-root">If each element in the set, <em class="calibre18">S</em>, has the attributes <em class="calibre18">A<sub class="calibre29">1</sub>, ..., A<sub class="calibre29">m</sub></em>, then we can partition the set, <em class="calibre18">S</em>, according to any of the possible attributes. The ID3 algorithm partitions the set, <em class="calibre18">S</em>, according to the attribute that yields the highest information gain. Now suppose that it has the attribute, <em class="calibre18">A<sub class="calibre29">1</sub></em>. Then, for the set, <em class="calibre18">S</em>, we have the partitions <em class="calibre18">S<sub class="calibre29">1</sub>, ..., S<sub class="calibre29">n</sub></em>, where <em class="calibre18">A<sub class="calibre29">1</sub></em> has the possible values <em class="calibre18">{v<sub class="calibre29">1</sub>,..., v<sub class="calibre29">n</sub>}</em>.</p>
<p class="mce-root">Since we have not constructed a tree yet, we first place a root node. For every partition of <em class="calibre18">S</em>, we place a new branch from the root. Every branch represents one value of the selected attributes. A branch has data samples with the same value for that attribute. For every new branch, we can define a new node that will have data samples from its ancestor branch.</p>
<p class="mce-root">Once we have defined a new node, we choose another of the remaining attributes with the highest information gain for the data at that node to partition the data further at that node, and then define new branches and nodes. This process can be repeated until we run out of all the attributes for the nodes, or even earlier, until all the data at the node has the same class as our interest. In the case of the swim preference example, there are only two possible classes for the swimming preference—class <kbd class="calibre17">no</kbd> and class <kbd class="calibre17">yes</kbd>. The last node is called a <strong class="calibre8">leaf node</strong>, and decides the class of a data item from the data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Swim preference – decision tree construction by the ID3 algorithm</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Here, we describe, step by step, how an ID3 algorithm would construct a decision tree from the given data samples in the swim preference example. The initial set consists of six data samples:</p>
<pre class="calibre22">S={(none,cold,no),(small,cold,no),(good,cold,no),(none,warm,no),(small,warm,no),(good,warm,yes)}</pre>
<p class="mce-root">In the previous sections, we calculated the information gains for both, and the only non- classifying attributes, <kbd class="calibre17">swimming suit</kbd>, and <kbd class="calibre17">water temperature</kbd>, as follows:</p>
<pre class="calibre22">IG(S,swimming suit)=0.3166890883<br class="calibre2"/>IG(S,water temperature)=0.19087450461</pre>
<p class="mce-root">Hence, we would choose the <kbd class="calibre17">swimming suit</kbd> attribute as it has a higher information gain. There is no tree drawn yet, so we start from the root node. As the <kbd class="calibre17">swimming suit</kbd><span class="calibre9"> attribute</span> has three possible values – <kbd class="calibre17">{none, small, good}</kbd>, we draw three possible branches out of it for each. Each branch will have one partition from the partitioned set <em class="calibre18">S: S<sub class="calibre29">none</sub></em>, <em class="calibre18">S<sub class="calibre29">small</sub></em>, and <em class="calibre18">S<sub class="calibre29">good</sub></em>. We add nodes to the ends of the branches. <em class="calibre18">S</em><sub class="calibre29"><em class="calibre18">none</em></sub> data samples have the same swimming preference class = <kbd class="calibre17">no</kbd>, so we do not need to branch that node with a further attribute and partition the set. Thus, the node with the data, <em class="calibre18">S<sub class="calibre29">none</sub></em>, is already a leaf node. The same is true for the node with the data, <em class="calibre18">S<sub class="calibre29">small</sub></em>.</p>
<p class="mce-root">But the node with the data, <em class="calibre18">S<sub class="calibre29">good</sub></em>, has two possible classes for swimming preference. Therefore, we will branch the node further. There is only one non-classifying attribute left—<kbd class="calibre17">water temperature</kbd>. So there is no need to calculate the information gain for that attribute with the data, <em class="calibre18">S<sub class="calibre29">good</sub></em>. From the node, <em class="calibre18">S<sub class="calibre29">good</sub></em>, we will have two branches, each with a partition from the set, <em class="calibre18">S<sub class="calibre29">good</sub></em>. One branch will have the set of the data sample, <em class="calibre18">S<sub class="calibre29">good,</sub> <sub class="calibre29">cold</sub>={(good,cold,no)};</em> the other branch will have the partition, <em class="calibre18">S<sub class="calibre29">good,</sub> <sub class="calibre29">warm</sub>={(good,warm,yes)}</em>. Each of these two branches will end with a node. Each node will be a leaf node, because each node has data samples of the same value for the classifying swimming preference <span class="calibre9">attribute</span>.</p>
<p class="mce-root">The resulting decision tree has four leaf nodes and is the tree in <em class="calibre18">Figure 3.1 – Decision tree for the swim preference example</em>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Implementation</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">We implement an ID3 algorithm that constructs a decision tree for the data given in a CSV file. All sources are in the chapter directory. The most important parts of the source code are given here:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/3/construct_decision_tree.py</strong><br class="calibre2"/># Constructs a decision tree from data specified in a CSV file.
# Format of a CSV file:
# Each data item is written on one line, with its variables separated
# by a comma. The last variable is used as a decision variable to
# branch a node and construct the decision tree.

import math
# anytree module is used to visualize the decision tree constructed by<br class="calibre2"/># this ID3 algorithm.
from anytree import Node, RenderTree
import sys
sys.path.append('../common')
import common
import decision_tree

<strong class="calibre3"># Program start
</strong>csv_file_name = sys.argv[1]
verbose = int(sys.argv[2])  # verbosity level, 0 - only decision tree
<br class="calibre2"/># Define the enquired column to be the last one.<br class="calibre2"/># I.e. a column defining the decision variable.
(heading, complete_data, incomplete_data,
 enquired_column) = common.csv_file_to_ordered_data(csv_file_name)

tree = decision_tree.constuct_decision_tree(
    verbose, heading, complete_data, enquired_column)
decision_tree.display_tree(tree)</pre>
<pre class="calibre22"><strong class="calibre3"># source_code/common/decision_tree.py</strong><br class="calibre2"/># ***Decision Tree library ***
# Used to construct a decision tree and a random forest.
import math
import random
import common
from anytree import Node, RenderTree
from common import printfv

# Node for the construction of a decision tree.
<strong class="calibre3">class TreeNode:
</strong>
    def __init__(self, var=None, val=None):
        self.children = []
        self.var = var
        self.val = val

    def add_child(self, child):
        self.children.append(child)

    def get_children(self):
        return self.children

    def get_var(self):
        return self.var

    def get_val(self):
        return self.val

    def is_root(self):
        return self.var is None and self.val is None

    def is_leaf(self):
        return len(self.children) == 0

    def name(self):
        if self.is_root():
            return "[root]"
        return "[" + self.var + "=" + self.val + "]"

# Constructs a decision tree where heading is the heading of the table
# with the data, i.e. the names of the attributes.
# complete_data are data samples with a known value for every attribute.
# enquired_column is the index of the column (starting from zero) which
# holds the classifying attribute.
<strong class="calibre3">def construct_decision_tree(verbose, heading, complete_data, enquired_column):</strong>
    return construct_general_tree(verbose, heading, complete_data,
                                  enquired_column, len(heading))

# m is the number of the classifying variables that should be at most
# considered at each node. <em class="calibre5"><span class="underline">m needed only for a random forest.</span></em>
<strong class="calibre3">def construct_general_tree(verbose, heading, complete_data,
                           enquired_column, m):</strong>
    available_columns = []
    for col in range(0, len(heading)):
        if col != enquired_column:
            available_columns.append(col)
    tree = TreeNode()
    printfv(2, verbose, "We start the construction with the root node" +
                        " to create the first node of the tree.\n")
    add_children_to_node(verbose, tree, heading, complete_data,
                         available_columns, enquired_column, m)
    return tree

# Splits the data samples into the groups with each having a different
# value for the attribute at the column col.
<strong class="calibre3">def split_data_by_col(data, col):
</strong>    data_groups = {}
    for data_item in data:
        if data_groups.get(data_item[col]) is None:
            data_groups[data_item[col]] = []
        data_groups[data_item[col]].append(data_item)
    return data_groups

# Adds a leaf node to node.
<strong class="calibre3">def add_leaf(verbose, node, heading, complete_data, enquired_column):
</strong>    leaf_node = TreeNode(heading[enquired_column],
                         complete_data[0][enquired_column])
    printfv(2, verbose,
            "We add the leaf node " + leaf_node.name() + ".\n")
    node.add_child(leaf_node)

# Adds all the descendants to the node.
<strong class="calibre3">def add_children_to_node(verbose, node, heading, complete_data,
                         available_columns, enquired_column, m):</strong>
    if len(available_columns) == 0:
        printfv(2, verbose, "We do not have any available variables " +
                "on which we could split the node further, therefore " +
                "we add a leaf node to the current branch of the tree. ")
        add_leaf(verbose, node, heading, complete_data, enquired_column)
        return -1

    printfv(2, verbose, "We would like to add children to the node " +
            node.name() + ".\n")

    selected_col = select_col(
        verbose, heading, complete_data, available_columns,
        enquired_column, m)
    for i in range(0, len(available_columns)):
        if available_columns[i] == selected_col:
            available_columns.pop(i)
            break

    data_groups = split_data_by_col(complete_data, selected_col)
    if (len(data_groups.items()) == 1):
        printfv(2, verbose, "For the chosen variable " +
                heading[selected_col] +
                " all the remaining features have the same value " +
                complete_data[0][selected_col] + ". " +
                "Thus we close the branch with a leaf node. ")
        add_leaf(verbose, node, heading, complete_data, enquired_column)
        return -1

    if verbose &gt;= 2:
        printfv(2, verbose, "Using the variable " +
                heading[selected_col] +
                " we partition the data in the current node, where" +
                " each partition of the data will be for one of the " +
                "new branches from the current node " + node.name() +
                ". " + "We have the following partitions:\n")
        for child_group, child_data in data_groups.items():
            printfv(2, verbose, "Partition for " +
                    str(heading[selected_col]) + "=" +
                    str(child_data[0][selected_col]) + ": " +
                    str(child_data) + "\n")
        printfv(
            2, verbose, "Now, given the partitions, let us form the " +
                        "branches and the child nodes.\n")
    for child_group, child_data in data_groups.items():
        child = TreeNode(heading[selected_col], child_group)
        printfv(2, verbose, "\nWe add a child node " + child.name() +
                " to the node " + node.name() + ". " +
                "This branch classifies %d feature(s): " +
                str(child_data) + "\n", len(child_data))
        add_children_to_node(verbose, child, heading, child_data, list(
            available_columns), enquired_column, m)
        node.add_child(child)
    printfv(2, verbose,
            "\nNow, we have added all the children nodes for the " +
            "node " + node.name() + ".\n")

# Selects an available column/attribute with the highest<br class="calibre2"/># information gain.
<strong class="calibre3">def select_col(verbose, heading, complete_data, available_columns,
               enquired_column, m):</strong>
    # Consider only a subset of the available columns of size m.
    printfv(2, verbose,
            "The available variables that we have still left are " +
            str(numbers_to_strings(available_columns, heading)) + ". ")
    if len(available_columns) &lt; m:
        printfv(
            2, verbose, "As there are fewer of them than the " +
                        "parameter m=%d, we consider all of them. ", m)
        sample_columns = available_columns
    else:
        sample_columns = random.sample(available_columns, m)
        printfv(2, verbose,
                "We choose a subset of them of size m to be " +
                str(numbers_to_strings(available_columns, heading)) +
                ".")

    selected_col = -1
    selected_col_information_gain = -1
    for col in sample_columns:
        current_information_gain = col_information_gain(
            complete_data, col, enquired_column)
        # print len(complete_data),col,current_information_gain
        if current_information_gain &gt; selected_col_information_gain:
            selected_col = col
            selected_col_information_gain = current_information_gain
    printfv(2, verbose,
            "Out of these variables, the variable with " +
            "the highest information gain is the variable " +
            heading[selected_col] +
            ". Thus we will branch the node further on this " +
            "variable. " +
            "We also remove this variable from the list of the " +
            "available variables for the children of the current node. ")
    return selected_col

# Calculates the information gain when partitioning complete_data
# according to the attribute at the column col and classifying by the
# attribute at enquired_column.
<strong class="calibre3">def col_information_gain(complete_data, col, enquired_column):
</strong>    data_groups = split_data_by_col(complete_data, col)
    information_gain = entropy(complete_data, enquired_column)
    for _, data_group in data_groups.items():
        information_gain -= (float(len(data_group)) / len(complete_data)
                             ) * entropy(data_group, enquired_column)
    return information_gain

# Calculates the entropy of the data classified by the attribute
# at the enquired_column.
<strong class="calibre3">def entropy(data, enquired_column):
</strong>    value_counts = {}
    for data_item in data:
        if value_counts.get(data_item[enquired_column]) is None:
            value_counts[data_item[enquired_column]] = 0
        value_counts[data_item[enquired_column]] += 1
    entropy = 0
    for _, count in value_counts.items():
        probability = float(count) / len(data)
        entropy -= probability * math.log(probability, 2)
    return entropy</pre>
<p class="mce-root"><strong class="calibre8">Program input</strong>:</p>
<p class="mce-root">We input the data from the swim preference example into the program to construct a decision tree:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/3/swim.csv</strong><br class="calibre2"/>swimming_suit,water_temperature,swim<br class="calibre2"/>None,Cold,No<br class="calibre2"/>None,Warm,No<br class="calibre2"/>Small,Cold,No<br class="calibre2"/>Small,Warm,No<br class="calibre2"/>Good,Cold,No<br class="calibre2"/>Good,Warm,Yes </pre>
<p class="mce-root"><strong class="calibre8">Program output</strong>:</p>
<p class="mce-root">We construct a decision tree from the <kbd class="calibre17">swim.csv</kbd> data file, with the verbosity set to <kbd class="calibre17">0</kbd>. The reader is encouraged to set the verbosity to <kbd class="calibre17">2</kbd> to see a detailed explanation of how exactly the decision tree is constructed:</p>
<pre class="calibre22"><strong class="calibre3">$ python construct_decision_tree.py swim.csv 0<br class="calibre2"/></strong>Root<br class="calibre2"/>├── [swimming_suit=Small]<br class="calibre2"/>│ ├── [water_temperature=Cold]<br class="calibre2"/>│ │ └── [swim=No]<br class="calibre2"/>│ └── [water_temperature=Warm]<br class="calibre2"/>│   └── [swim=No]<br class="calibre2"/>├── [swimming_suit=None]<br class="calibre2"/>│ ├── [water_temperature=Cold]<br class="calibre2"/>│ │ └── [swim=No]<br class="calibre2"/>│ └── [water_temperature=Warm]<br class="calibre2"/>│   └── [swim=No]<br class="calibre2"/>└── [swimming_suit=Good]<br class="calibre2"/>    ├── [water_temperature=Cold]<br class="calibre2"/>    │   └── [swim=No]<br class="calibre2"/>    └── [water_temperature=Warm]<br class="calibre2"/>        └── [swim=Yes]</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Classifying with a decision tree</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Once we have constructed a decision tree from the data with the attributes <em class="calibre18">A<sub class="calibre29">1</sub>, ..., A<sub class="calibre29">m</sub></em> and the classes <em class="calibre18">{c<sub class="calibre29">1</sub>, ..., c<sub class="calibre29">k</sub>}</em>, we can use this decision tree to classify a new data item with the attributes <em class="calibre18">A<sub class="calibre29">1</sub>, ..., A<sub class="calibre29">m</sub></em> into one of the classes <em class="calibre18">{c<sub class="calibre29">1</sub>, ..., c<sub class="calibre29">k</sub>}</em>.</p>
<p class="mce-root"/>
<p class="mce-root">Given a new data item that we would like to classify, we can think of each node, including the root, as a question for the data sample: <em class="calibre18">What value does that data sample have for the selected attribute, A<sub class="calibre29">i</sub>?</em> Then, based on the answer, we select a branch of the decision tree and move on to the next node. Then, another question is answered about the data sample, and another, until the data sample reaches the leaf node. A leaf node has one of the classes <em class="calibre18">{c<sub class="calibre29">1</sub>, ..., c<sub class="calibre29">k</sub>}</em> associated with it; for example, <em class="calibre18">c<sub class="calibre29">i</sub></em>. Then, the decision tree algorithm would classify the data sample into the class, <em class="calibre18">c<sub class="calibre29">i</sub></em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Classifying a data sample with the swimming preference decision tree</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Let's construct a decision tree for the swimming preference example using the ID3 algorithm. Now that we have constructed the decision tree, we would like to use it to classify a data sample, <em class="calibre18">(good,cold,?)</em> into one of the two classes in the set <em class="calibre18">{no,yes}</em>.</p>
<p class="mce-root">Start with a data sample at the root of the tree. The first attribute that branches from the root is <kbd class="calibre17">swimming suit</kbd>, so we ask for the value of the <kbd class="calibre17">swimming suit</kbd> attribute of the sample <em class="calibre18">(good, cold,?)</em>. We learn that the value of the attribute is <kbd class="calibre17">swimming suit=good</kbd>; therefore, move down the rightmost branch with that value for its data samples. We arrive at the node with the <kbd class="calibre17">water temperature</kbd> attribute and ask the question: <em class="calibre18">What is the value of the water temperature <span class="calibre9">attribute </span>for the data sample (good, cold,?)?</em> We learn that for that data sample, we have <kbd class="calibre17">water temperature=cold</kbd>; therefore, we move down the left-hand branch into the leaf node. This leaf is associated with the <kbd class="calibre17">swimming preference=no</kbd> class. Therefore, the decision tree would classify the data sample <em class="calibre18">(good, cold,?)</em> to be in that swimming preference class; in other words, to complete it (by replacing the question mark with the exact data) to the data sample <em class="calibre18">(</em><em class="calibre18">good, cold, no)</em>.</p>
<p class="mce-root">Therefore, the decision tree says that if you have a good swimming suit, but the water temperature is cold, then you would still not want to swim based on the data collected in the table.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Playing chess – analysis with a decision tree</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Let's take an example from <span class="calibre9"><a href="" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 2</a>, <em class="calibre18">Naive Bayes</em></span>, again:</p>
<table class="msotablegrid" border="1">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Temperature</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Wind</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Sunshine</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Play</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Sunny</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Sunny</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Sunny</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Sunny</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre33">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Sunny</p>
</td>
<td class="calibre25">
<p class="calibre26">?</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p class="mce-root">We would like to find out whether our friend would like to play chess with us in the park. But this time, we would like to use decision trees to find the answer.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">We have the initial set, <em class="calibre18">S</em>, of the data samples, as follows:</p>
<pre class="calibre22">S={(Cold,Strong,Cloudy,No),(Warm,Strong,Cloudy,No),(Warm,None,Sunny,Yes), (Hot,None,Sunny,No),(Hot,Breeze,Cloudy,Yes),(Warm,Breeze,Sunny,Yes),(Cold,Breeze,Cloudy,No),(Cold,None,Sunny,Yes),(Hot,Strong,Cloudy,Yes),(Warm,None,Cloudy,Yes)}</pre>
<p class="mce-root">First, we determine the information gain for each of the three non-classifying attributes: <kbd class="calibre17">temperature</kbd>, <kbd class="calibre17">wind</kbd>, and <kbd class="calibre17">sunshine</kbd>. The possible values for <kbd class="calibre17">temperature</kbd> are <kbd class="calibre17">Cold</kbd>, <kbd class="calibre17">Warm</kbd>, and <kbd class="calibre17">Hot</kbd>. Therefore, we will partition the set, <em class="calibre18">S</em>, into three sets:</p>
<pre class="calibre22">S<sub class="calibre29">cold</sub>={(Cold,Strong,Cloudy,No),(Cold,Breeze,Cloudy,No),(Cold,None,Sunny,Yes)}<br class="calibre2"/>S<sub class="calibre29">warm</sub>={(Warm,Strong,Cloudy,No),(Warm,None,Sunny,Yes),(Warm,Breeze,Sunny,Yes),(Warm,None,Cloudy,Yes)}<br class="calibre2"/>S<sub class="calibre29">hot</sub>={(Hot,None,Sunny,No),(Hot,Breeze,Cloudy,Yes),(Hot,Strong,Cloudy,Yes)}</pre>
<p class="mce-root">We calculate the information entropies for the sets first:</p>
<p class="mce-root"><img class="fm-editor-equation100" src="Images/b6d23d69-5c62-455a-808f-83f5f3a7ba2b.png" width="5500" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation101" src="Images/9d5fdd96-20e6-42be-a66f-168df9b6fba5.png" width="5370" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation102" src="Images/73ac61fc-b892-4050-9542-6cae5c95cf0d.png" width="5480" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation103" src="Images/55f6d338-236f-4860-bcfc-a6afa7c374e3.png" width="5330" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation104" src="Images/04be8669-816b-4abf-b8a9-84f08fa5d441.png" width="8950" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation105" src="Images/1bc93be0-e80e-4478-a7eb-7770febac121.png" width="7730" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation106" src="Images/5cc2ba99-5f78-406c-9384-04a1f5897775.png" width="1360" height="160"/></p>
<p class="mce-root">The possible values for the <kbd class="calibre17">wind</kbd> attribute are <kbd class="calibre17">None</kbd>, <kbd class="calibre17">Breeze</kbd>, and <kbd class="calibre17">Strong</kbd>. Thus, we will split the set, <em class="calibre18">S</em>, into the three partitions:</p>
<p class="mce-root"><img class="fm-editor-equation107" src="Images/913e09cf-1f4a-4838-a44d-7e386857de33.png" width="9600" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation108" src="Images/21c1f8f6-ef95-446f-8f5b-0ef3181db4af.png" width="7640" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation109" src="Images/050cedb3-7fc0-4d70-aced-ccd53cc97b6a.png" width="7610" height="220"/></p>
<p class="mce-root">The information entropies of the sets are as follows:</p>
<p class="mce-root"><img class="fm-editor-equation110" src="Images/29b98f94-6d23-4d88-be49-85cb9528fa23.png" width="2120" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation111" src="Images/25ec3d85-19a3-4732-9cbe-b70e8bc0361b.png" width="2200" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation112" src="Images/03d4f9db-4723-4d5f-b1c6-603be3bcef37.png" width="2210" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation113" src="Images/66ec446d-ffd5-4028-b7ed-485fabe7551b.png" width="8860" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation105" src="Images/7f61d2d0-fb0c-4ac0-a6bf-c9dd108c8bf0.png" width="7730" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation106" src="Images/c30c697d-c7f2-4338-86c5-74c992d534b1.png" width="1360" height="160"/></p>
<p class="mce-root">Finally, the third attribute, <kbd class="calibre17">Sunshine</kbd>, has two possible values, <kbd class="calibre17">Cloudy</kbd> and <kbd class="calibre17">Sunny</kbd>. Hence, it splits the set, <em class="calibre18">S</em>, into two sets:</p>
<p class="mce-root"><img class="fm-editor-equation114" src="Images/db8b17d8-8329-4731-9ca0-520393b753d8.png" width="7480" height="750"/></p>
<p class="mce-root"><img class="fm-editor-equation115" src="Images/9f9c929a-dd35-49a6-8e98-3646356ffa2b.png" width="9650" height="220"/></p>
<p class="mce-root">The entropies of the sets are as follows:</p>
<p class="mce-root"><img class="fm-editor-equation116" src="Images/74517c8e-9378-421b-841a-ba76bc3fbd31.png" width="1270" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation117" src="Images/0013e3ef-8416-470d-a331-41774c28103f.png" width="2200" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation118" src="Images/b022dcbf-097b-4106-b005-385de8861495.png" width="7120" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation119" src="Images/b5b3caaa-c2e7-4042-849b-82707ebfb928.png" width="5980" height="220"/></p>
<p class="mce-root"><em class="calibre18">IG(S,wind)</em> and <em class="calibre18">IG(S,temperature)</em> are greater than <em class="calibre18">IG(S,sunshine)</em>. Both of them are equal; therefore, we can choose any of the attributes to form the three branches; for example, the first one, <kbd class="calibre17">Temperatur</kbd><kbd class="calibre17">e</kbd>. In that case, each of the three branches would have the data samples <em class="calibre18">S<sub class="calibre29">cold</sub></em>, <em class="calibre18">S<sub class="calibre29">warm</sub></em>, and <em class="calibre18">S<sub class="calibre29">hot</sub></em>. At those branches, we could apply the algorithm further to form the rest of the decision tree. Instead, we will use the program to complete it:</p>
<p class="mce-root"><strong class="calibre8">Input</strong>:</p>
<pre class="calibre22"><strong class="calibre3">source_code/3/chess.csv</strong><br class="calibre2"/>Temperature,Wind,Sunshine,Play<br class="calibre2"/>Cold,Strong,Cloudy,No<br class="calibre2"/>Warm,Strong,Cloudy,No<br class="calibre2"/>Warm,None,Sunny,Yes<br class="calibre2"/>Hot,None,Sunny,No<br class="calibre2"/>Hot,Breeze,Cloudy,Yes<br class="calibre2"/>Warm,Breeze,Sunny,Yes<br class="calibre2"/>Cold,Breeze,Cloudy,No<br class="calibre2"/>Cold,None,Sunny,Yes<br class="calibre2"/>Hot,Strong,Cloudy,Yes<br class="calibre2"/>Warm,None,Cloudy,Yes</pre>
<p class="mce-root"><strong class="calibre8">Output</strong>:</p>
<pre class="calibre22"><strong class="calibre3">$ python construct_decision_tree.py chess.csv 0</strong><br class="calibre2"/>Root<br class="calibre2"/>├── [Temperature=Cold]<br class="calibre2"/>│ ├── [Wind=Breeze]<br class="calibre2"/>│ │ └── [Play=No]<br class="calibre2"/>│ ├── [Wind=Strong]<br class="calibre2"/>│ │ └── [Play=No]<br class="calibre2"/>│ └── [Wind=None]<br class="calibre2"/>│   └── [Play=Yes]<br class="calibre2"/>├── [Temperature=Warm]<br class="calibre2"/>│ ├── [Wind=Breeze]<br class="calibre2"/>│ │ └── [Play=Yes]<br class="calibre2"/>│ ├── [Wind=None]<br class="calibre2"/>│ │ ├── [Sunshine=Sunny]<br class="calibre2"/>│ │ │ └── [Play=Yes]<br class="calibre2"/>│ │ └── [Sunshine=Cloudy]<br class="calibre2"/>│ │   └── [Play=Yes]<br class="calibre2"/>│ └── [Wind=Strong]<br class="calibre2"/>│   └── [Play=No]<br class="calibre2"/>└── [Temperature=Hot]<br class="calibre2"/>    ├── [Wind=Strong]<br class="calibre2"/>    │ └── [Play=Yes]<br class="calibre2"/>    ├── [Wind=None]<br class="calibre2"/>    │ └── [Play=No]<br class="calibre2"/>    └── [Wind=Breeze]<br class="calibre2"/>        └── [Play=Yes]</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Classification</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Now that we have constructed the decision tree, we would like to use it to classify a data sample <em class="calibre18">(warm,strong,sunny,?)</em> into one of the two classes in the set <em class="calibre18">{no,yes}</em>.</p>
<p class="mce-root">We start at the root. What value does the <kbd class="calibre17">temperature</kbd> attribute have in that instance? <kbd class="calibre17">Warm</kbd>, so we go to the middle branch. What value does the <kbd class="calibre17">wind</kbd> <span class="calibre9">attribute </span>have in that instance? <kbd class="calibre17">Strong</kbd>, so the instance would fall into the class <kbd class="calibre17">No</kbd> since we have already arrived at the leaf node.</p>
<p class="mce-root">So, our friend would not want to play chess with us in the park, according to the decision tree classification algorithm. Please note that the Naive Bayes algorithm stated otherwise. An understanding of the problem is required to choose the best possible method. At other times, a method with greater accuracy is one that takes into consideration the results of several algorithms or several classifiers, as in the case of the random forest algorithm in <a href="0a6e5d42-ab32-49c4-934f-7f1954eb1a25.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 4</a>, <em class="calibre18">Random Forests</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Going shopping – dealing with data inconsistencies</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">We have the following data about the shopping preferences of our friend, Jane:</p>
<table class="msotablegrid" border="1">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Temperature</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Rain</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Shopping</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre33">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">?</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">We would like to find out, using a decision tree, whether Jane would go shopping if the outside temperature was cold with no rain.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">Here, we should be careful, as there are instances of the data that have the same values for the same attributes, but different classes; that is, <kbd class="calibre17">(cold,none,yes)</kbd> and <kbd class="calibre17">(cold,none,no)</kbd>. The program we made would form the following decision tree:</p>
<pre class="calibre22">    Root
    ├── [Temperature=Cold]
    │    ├──[Rain=None]
    │    │    └──[Shopping=Yes]
    │    └──[Rain=Strong]
    │         └──[Shopping=Yes]
    └── [Temperature=Warm]
         ├──[Rain=None]
         │    └──[Shopping=No]
         └── [Rain=Strong]
              └── [Shopping=No]
  </pre>
<p class="mce-root">But at the leaf node <kbd class="calibre17">[Rain=None]</kbd> with the parent <kbd class="calibre17">[Temperature=Cold]</kbd>, there are two data samples with both classes, <kbd class="calibre17">no</kbd> and <kbd class="calibre17">yes</kbd>. We cannot, therefore, classify an instance <kbd class="calibre17">(cold,none,?)</kbd> accurately. For the decision tree algorithm to work better, we would have to provide a class at the leaf node with the greatest weight—that is, the majority class. An even better approach would be to collect values for more attributes for the data samples so that we can make a decision more accurately.</p>
<p class="mce-root">Therefore, given the available data, we are uncertain whether Jane would go shopping or not.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root">In this chapter, we looked at how a decision tree ID3 algorithm first constructs a decision tree from the input data and then classifies a new data instance using the constructed tree. The decision tree was constructed by selecting the attribute for branching with the highest information gain. We studied how information gain measures the amount of information that can be learned in terms of the gain in information entropy.</p>
<p class="mce-root">We also learned that the decision tree algorithm can achieve a different result from other algorithms, such as Naive Bayes.</p>
<p class="mce-root">In the next chapter, we will learn how to combine various algorithms or classifiers into a decision forest (called<span class="calibre9"> </span><strong class="calibre8">random forest</strong>) in order to achieve a more accurate result.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Problems</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root"><strong class="calibre8">Problem 1</strong>: What is the information entropy of the following multisets?<br class="calibre10"/>
a) {1,2}, b) {1,2,3}, c) {1,2,3,4}, d) {1,1,2,2}, e) {1,1,2,3}</p>
<p class="mce-root"><strong class="calibre8">Problem 2</strong>: What is the information entropy of the probability space induced by the biased coin that shows head with a probability of 10%, and tail with a probability of 90%?</p>
<p class="mce-root"><strong class="calibre8">Problem 3</strong><span class="calibre9">: Let's tak</span><span class="calibre9">e another example of playing chess from</span> <a href="4f54d05d-6791-47c5-a260-c2fd563a55cf.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13"><span class="calibre4">Chapter 2</span></a><span class="calibre9">,</span> <em class="calibre18">Naive Bayes</em><span class="calibre9">:</span></p>
<p class="mce-root">a) What is the information gain for each of the non-classifying attributes in the table?</p>
<p class="mce-root">b) What is the decision tree constructed from the given table?</p>
<p class="mce-root">c) How would you classify a data sample <kbd class="calibre17">(Warm,Strong,Spring,?)</kbd> according to the constructed decision tree?</p>
<table class="msotablegrid" border="1">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Temperature</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Wind</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Season</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Play</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Winter</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Autumn</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Summer</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Spring</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Autumn</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Spring</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Winter</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Spring</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Summer</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Autumn</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre33">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Spring</p>
</td>
<td class="calibre25">
<p class="calibre26">?</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root"><strong class="calibre8">Problem 4</strong>: <strong class="calibre8">Mary and temperature preferences</strong>: Let's take the example from <a href="e0824a1e-65dc-4fee-a0e5-56170fdb36b9.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13"><span class="calibre4">Chapter 1</span></a>, <em class="calibre18">Classification Using K Nearest Neighbors</em>, regarding Mary's temperature preferences:</p>
<table class="msotablegrid" border="1">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Temperature in °C</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Wind speed in kmph</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Mary's perception</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">10</p>
</td>
<td class="calibre25">
<p class="calibre26">0</p>
</td>
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">25</p>
</td>
<td class="calibre25">
<p class="calibre26">0</p>
</td>
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">15</p>
</td>
<td class="calibre25">
<p class="calibre26">5</p>
</td>
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">20</p>
</td>
<td class="calibre25">
<p class="calibre26">3</p>
</td>
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">18</p>
</td>
<td class="calibre25">
<p class="calibre26">7</p>
</td>
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">20</p>
</td>
<td class="calibre25">
<p class="calibre26">10</p>
</td>
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">22</p>
</td>
<td class="calibre25">
<p class="calibre26">5</p>
</td>
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
</tr>
<tr class="calibre28">
<td class="calibre25">
<p class="calibre26">24</p>
</td>
<td class="calibre25">
<p class="calibre26">6</p>
</td>
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">We would like to use decision trees to decide whether our friend, Mary, would feel warm or cold in a room with a temperature of 16°C <span class="calibre9">and a wind speed of</span> 3 km/h.</p>
<p class="mce-root">Can you please explain how a decision tree algorithm could be used here and how beneficial it would be to use it for this example?</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content" class="calibre1"><section class="calibre2">

                            <header class="calibre2">
                    <h1 class="header-title">Analysis</h1>
                </header>
            
            <article class="calibre2">
                
<p class="mce-root"><strong class="calibre8">Problem 1</strong>: Here are <span class="calibre9">entropies</span> of the multisets:</p>
<p class="mce-root">a) <img class="fm-editor-equation120" src="Images/7f1d570b-442a-4461-bcf2-a3969e95bac8.png" width="4200" height="220"/></p>
<p class="mce-root">b) <img class="fm-editor-equation121" src="Images/cdb58fb4-3f36-4ea9-84ae-99a1d32285d3.png" width="6650" height="220"/></p>
<p class="mce-root">c) <img class="fm-editor-equation109" src="Images/7912ec03-379f-4fc1-9d4d-d9e98db19757.png" width="7690" height="220"/></p>
<p class="mce-root">d) <img class="fm-editor-equation122" src="Images/20279a9c-d349-4c0b-bf66-d277eb2ff08c.png" width="4550" height="220"/></p>
<p class="mce-root">e) <img class="fm-editor-equation123" src="Images/4b287548-1003-4188-aced-deabb5645e5b.png" width="6260" height="220"/></p>
<p class="mce-root"/>
<p class="calibre35">Note here that the information entropy of the multisets that have more than two classes is greater than 1, so we need more than one bit of information to represent the result. But is this true for every multiset that has more than two classes of elements?</p>
<p class="mce-root"><strong class="calibre8">Problem 2</strong>: <em class="calibre18"><img class="fm-editor-equation124" src="Images/bcd665ff-2e2f-444c-9b5f-bc9e17a3eae3.png" width="6880" height="220"/></em></p>
<p class="mce-root"><strong class="calibre8">Problem 3</strong>: a) The information gains for the three attributes are as follows:</p>
<p class="mce-root"><img class="fm-editor-equation125" src="Images/7a94df6b-1916-46b4-a830-dc6ce3356092.png" width="3210" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation126" src="Images/43db9692-58e3-423a-9431-040c14eee8ac.png" width="2600" height="220"/></p>
<p class="mce-root"><img class="fm-editor-equation127" src="Images/a1267b6c-2d2e-450c-a578-38aec73de774.png" width="2660" height="220"/></p>
<p class="cdpalignleft1">b) Therefore, we would choose the <kbd class="calibre17">season</kbd> <span class="calibre9">attribute </span>to branch from the root node as it has the highest information gain. Alternatively, we can put all the input data into the program to construct a decision tree, as follows:</p>
<pre class="calibre49">    Root
    ├── [Season=Autumn]
    │    ├──[Wind=Breeze]
    │    │    └──[Play=Yes]
    │    ├──[Wind=Strong]
    │    │    └──[Play=No]
    │    └──[Wind=None]
    │         └──[Play=Yes]
    ├── [Season=Summer]
    │    ├──[Temperature=Hot]
    │    │    └──[Play=Yes]
    │    └──[Temperature=Warm]
    │         └──[Play=Yes]
    ├── [Season=Winter]
    │    └──[Play=No]
    └── [Season=Spring]
         ├── [Temperature=Hot]
         │    └──[Play=No]
         ├── [Temperature=Warm]
         │    └──[Play=Yes]
         └── [Temperature=Cold]
              └── [Play=Yes]
  </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">c) According to the decision tree <span class="calibre9">constructed</span>, we would classify the data sample <kbd class="calibre17">(warm,strong,spring,?)</kbd> to the class <kbd class="calibre17">Play=Yes</kbd> by going to the bottom-most branch from the root node and then arriving at the leaf node by taking the middle branch.</p>
<p class="mce-root"><strong class="calibre8">Problem 4</strong>: Here, the decision tree algorithm may not perform that well without any processing of the data. If we considered every class of a temperature, then 25°C would still not occur in the decision tree as it is not in the input data, so we would not be able to classify how Mary would feel at 16°C and a wind speed of 3 km/h.</p>
<p class="mce-root">We could alternatively divide the temperature and wind speed into intervals in order to reduce the classes so that the resulting decision tree could classify the input instance. But it is this division, the intervals into which 25°C and 3 km/h should be classified, that is the fundamental part of the analysis procedure for this type of problem. Thus, decision trees without any serious modification could not analyze the problem well.</p>


            </article>

            
        </section>
    </div>



  </body></html>