- en: Chapter 2. The Spark Programming Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large-scale data processing using thousands of nodes with built-in fault tolerance
    has become widespread due to the availability of open source frameworks, with
    Hadoop being a popular choice. These frameworks are quite successful in executing
    specific tasks such as **Extract, Transform, and Load** (**ETL**) and storage
    applications that deal with web-scale data. However, developers were left with
    a myriad of tools to work with, along with the well-established Hadoop ecosystem.
    There was a need for a single, general-purpose development platform that caters
    to batch, streaming, interactive, and iterative requirements. This was the motivation
    behind Spark.
  prefs: []
  type: TYPE_NORMAL
- en: The previous chapter outlined the big data analytics challenges and how Spark
    addressed most of them at a very high level. In this chapter, we will examine
    the design goals and choices involved in the making of Spark to get a clearer
    understanding of its suitability as a data science platform for big data. We will
    also cover the core abstraction **Resilient Distributed Dataset** (**RDD**) in
    depth with examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a prerequisite for this chapter, a basic understanding of Python or Scala
    along with elementary understanding of Spark is needed. The topics covered in
    this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The programming paradigm - language support and design benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported programming languages
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right language
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spark engine - Spark core components and their implications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driver program
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark shell
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkContext
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker nodes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Executors
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared variables
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Flow of execution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The RDD API - understanding the RDD fundamentals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RDD basics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistence
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: RDD operations - let's get your hands dirty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with the shell
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating RDDs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformations on normal RDDs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformations on pair RDDs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The programming paradigm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For Spark to address the big data challenges and serve as a platform for data
    science and other scalable applications, it was built with well-thought-out design
    considerations and language support.
  prefs: []
  type: TYPE_NORMAL
- en: There are Spark APIs designed for varieties of application developers to create
    Spark-based applications using standard API interfaces. Spark provides APIs for
    Scala, Java, R and Python programming languages, as explained in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Supported programming languages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With built-in support for so many languages, Spark can be used interactively
    through a shell, which is otherwise known as **Read-Evaluate-Print-Loop** (**REPL**),
    in a way that will feel familiar to developers of any language. The developers
    can use the language of their choice, leverage existing libraries, and seamlessly
    interact with Spark and its ecosystem. Let us see the ones supported on Spark
    and how they fit into the Spark ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Scala
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark itself is written in Scala, a **Java Virtual Machine** (**JVM**) based
    functional programming language. The Scala compiler generates byte code that executes
    on the JVM. So, it can seamlessly integrate with any other JVM-based systems such
    as HDFS, Cassandra, HBase, and so on. Scala was the language of choice because
    of its concise programming interface, an interactive shell, and its ability to
    capture functions and efficiently ship them across the nodes in a cluster. Scala
    is an extensible (scalable, hence the name), statically typed, efficient multi-paradigm
    language that supports functional and object-oriented language features.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the full-blown applications, Scala also supports shell (Spark shell)
    for interactive data analysis on Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Java
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since Spark is JVM based, it naturally supports Java. This helps existing Java
    developers to develop data science applications along with other scalable applications.
    Almost all the built-in library functions are accessible from Java. Coding in
    Java for data science assignments is comparatively difficult in Spark, but someone
    very hands-on with Java might find it easy.
  prefs: []
  type: TYPE_NORMAL
- en: This Java API only lacks a shell-based interface for interactive data analysis
    on Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python is supported on Spark through PySpark, which is built on top of Spark's
    Java API (using Py4J). From now on, we will be using the term **PySpark** to refer
    to the Python environment on Spark. Python was already very popular amongst developers
    for data wrangling, data munging, and other data science related tasks. Support
    for Python on Spark became even more popular as Spark could address the scalable
    computation challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Through Python's interactive shell on Spark (PySpark), interactive data analysis
    at scale is possible.
  prefs: []
  type: TYPE_NORMAL
- en: R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: R is supported on Spark through SparkR, an R package through which Spark's scalability
    is accessible through R. SparkR empowered R to address its limitation of single-threaded
    runtime, because of which computation was limited only to a single node.
  prefs: []
  type: TYPE_NORMAL
- en: Since R was originally designed only for statistical analysis and machine learning,
    it was already enriched with most of the packages. Data scientists can now work
    on huge data at scale with a minimal learning curve. R is still a default choice
    for many data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right language
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apart from the developer''s language preference, at times there are other constraints
    that may draw attention. The following aspects could supplement your development
    experience while choosing one language over the other:'
  prefs: []
  type: TYPE_NORMAL
- en: An interactive shell comes in handy when developing complex logic. All languages
    supported by Spark except Java have an interactive shell.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R is the lingua franca of data scientists. It is definitely more suitable for
    pure data analytics because of its richer set of libraries. R support was added
    in Spark 1.4.0 so that Spark reaches out to data scientists working on R.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java has a broader base of developers. Java 8 has included lambda expressions
    and hence the functional programming aspect. Nevertheless, Java tends to be verbose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python is gradually gaining more popularity in the data science space. The availability
    of Pandas and other data processing libraries, and its simple and expressive nature,
    make Python a strong candidate. Python gives more flexibility than R in scenarios
    such as data aggregation from different sources, data cleaning, natural language
    processing, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala is perhaps the best choice for real-time analytics because this is the
    closest to Spark. The initial learning curve for developers coming from other
    languages should not be a deterrent for serious production systems. The latest
    inclusions to Spark are usually first available in Scala. Its static typing and
    sophisticated type inference improve efficiency as well as compile-time checks.
    Scala can draw from Java's libraries as Scala's own library base is still at an
    early stage, but catching up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spark engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To program with Spark, a basic understanding of Spark components is needed.
    In this section, some of the important Spark components along with their execution
    mechanism will be explained so that developers and data scientists can write programs
    and build applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before getting into the details, we suggest you take a look at the following
    diagram so that the descriptions of the Spark gears are more comprehensible as
    you read further:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Spark engine](img/image_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Driver program
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spark shell is an example of a driver program. A driver program is a process
    that executes in the JVM and runs the user's *main* function on it. It has a SparkContext
    object which is a connection to the underlying cluster manager. A Spark application
    is initiated when the driver starts and it completes when the driver stops. The
    driver, through an instance of SparkContext, coordinates all processes within
    a Spark application.
  prefs: []
  type: TYPE_NORMAL
- en: Primarily, an RDD lineage **Directed Acyclic Graph** (**DAG**) is built on the
    driver side with data sources (which may be RDDs) and transformations. This DAG
    is submitted to the DAG scheduler when an *action* method is encountered. The
    DAG scheduler then splits the DAG into logical units of work (for example, map
    or reduce) called stages. Each stage, in turn, is a set of tasks, and each task
    is assigned to an executor (worker) by the task scheduler. Jobs may be executed
    in FIFO order or round robin, depending on the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inside a single Spark application, multiple parallel jobs can run simultaneously
    if they were submitted from separate threads.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark shell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spark shell is none other than the interface provided by Scala and Python.
    It looks very similar to any other interactive shell. It has a SparkContext object
    (created by default for you) that lets you leverage the distributed cluster. An
    interactive shell is quite useful for exploratory or ad hoc analysis. You can
    develop your complex scripts step by step through the shell without going through
    the compile-build-execute cycle.
  prefs: []
  type: TYPE_NORMAL
- en: SparkContext
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SparkContext is the entry point to the Spark core engine. This object is required
    to create and manipulate RDDs and create shared variables on a cluster. The SparkContext
    object connects to a cluster manager, which is responsible for resource allocation.
    Spark comes with its own standalone cluster manager. Since the cluster manager
    is a pluggable component in Spark, it can be managed through external cluster
    managers such as Apache Mesos or YARN.
  prefs: []
  type: TYPE_NORMAL
- en: When you start a Spark shell, a SparkContext object is created by default for
    you. You can also create it by passing a SparkConf object that is used to set
    various Spark configuration parameters as key value pairs. Please note that there
    can be only one SparkContext object in a JVM.
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Worker nodes are the nodes that run the application code in a cluster, obeying
    the driver program. The real work is actually executed by the worker nodes. Each
    machine in the cluster may have one or more worker instances (default one). A
    worker node executes one or more executors that belong to one or more Spark applications.
    It consists of a *block manager* component, which is responsible for managing
    data blocks. The blocks can be cached RDD data, intermediate shuffled data, or
    broadcast data. When the available RAM is not sufficient, it automatically moves
    some data blocks to disk. Data replication across nodes is another responsibility
    of block manager.
  prefs: []
  type: TYPE_NORMAL
- en: Executors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each application has a set of executor processes. Executors reside on worker
    nodes and communicate directly with the driver once the connection is made by
    the cluster manager. All executors are managed by SparkContext. An executor is
    a single JVM instance that serves a single Spark application. An executor is responsible
    for managing computation through tasks, storage, and caching on each worker node.
    It can run multiple tasks concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Shared variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normally, the code is shipped to partitions along with separate copies of variables.
    These variables cannot be used to propagate results (for example, intermediate
    work counts) back to the driver program. Shared variables are used for this purpose.
    There are two kinds of shared variables, **broadcast variables** and **accumulators**.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcast variables enable the programmers to retain a read-only copy cached
    on each node rather than shipping a copy of it with tasks. If large, read-only
    data is used in multiple operations, it can be designated as broadcast variables
    and shipped only once to all worker nodes. The data broadcast in this way is cached
    in serialized form and is deserialized before running each task. Subsequent operations
    can access these variables along with the local variables moved along with the
    code. Creating broadcast variables is not necessary in all cases, except the ones
    where tasks across multiple stages need the same read-only copy of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Accumulators are variables that are always incremented, such as counters or
    cumulative sums. Spark natively supports accumulators of numeric types, but allows
    programmers to add support for new types. Please note that the worker nodes cannot
    read the value of accumulators; they can only modify their values.
  prefs: []
  type: TYPE_NORMAL
- en: Flow of execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Spark application consists of a set of processes with one *driver* program
    and multiple *worker* (*executor*) programs. The driver program contains the application's
    *main* function and a SparkContext object, which represents a connection to the
    Spark cluster. Coordination between driver and the other processes happens through
    the SparkContext object.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical Spark client program performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: When a program is run on a Spark shell, it is called the driver program with
    the user's `main` method in it. It gets executed in the JVM of the system where
    you are running the driver program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first step is to create a SparkContext object with the required configuration
    parameters. When you run the PySpark or Spark shell, it is instantiated by default,
    but for other applications, you have to create it explicitly. SparkContext is
    actually the gateway to Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to define one or more RDDs, either by loading a file or programmatically
    by passing an array of items, referred to parallel collection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then more RDDs can be defined by a sequence of transformations, which are tracked
    and managed by a **lineage graph**. These RDD transformations may be viewed as
    piped UNIX commands where the output of one command becomes the input to the next
    command and so on. Each resulting RDD of a *transformation* step has a pointer
    to its parent RDD and also has a function for calculating its data. The RDD is
    acted on only after encountering an *action* statement. So, the *transformations*
    are lazy operations used to define new RDDs and *actions* launch a computation
    to return a value to the program or write data to external storage. We will discuss
    this aspect a little more in the following sections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this stage, Spark creates an execution graph where nodes represent the RDDs
    and edges represent the transformation steps. Spark breaks the job into multiple
    tasks to run on separate machines. This is how Spark sends the **compute** to
    the data across the nodes in a cluster, rather than getting all the data together
    and computing it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The RDD API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RDD is a read-only, partitioned, fault-tolerant collection of records. From
    a design perspective, there was a need for a single data structure abstraction
    that hides the complexity of dealing with a wide variety of data sources, be it
    HDFS, filesystems, RDBMS, NOSQL data structures, or any other data source. The
    user should be able to define the RDD from any of these sources. The goal was
    to support a wide array of operations and let users compose them in any order.
  prefs: []
  type: TYPE_NORMAL
- en: RDD basics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each dataset is represented as an object in Spark's programming interface called
    RDD. Spark provides two ways for creating RDDs. One way is to parallelize an existing
    collection. The other way is to reference a dataset in an external storage system
    such as a filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: An RDD is composed of one or more data sources, maybe after performing a series
    of transformations including several operators. Every RDD or RDD partition knows
    how to recreate itself in case of failure. It has the log of transformations,
    or a *lineage* that is required to recreate itself from stable storage or another
    RDD. Thus, any program using Spark can be assured of built-in fault tolerance,
    regardless of the underlying data source and the type of RDD.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two kinds of methods available on RDDs: transformations, and actions.
    Transformations are the methods that are used to create RDDs. Actions are the
    methods that utilize RDDs. RDDs are usually partitioned. Users may choose to persist
    RDDs that may be reused in their programs.'
  prefs: []
  type: TYPE_NORMAL
- en: RDDs are immutable (read-only) data structures, so any transformation results
    in the creation of a new RDD. The transformations are applied lazily, only when
    any action is applied on them, and not when an RDD is defined. An RDD is recomputed
    every time it is used in an action unless the user explicitly persists the RDD
    in memory. Saving in memory saves a lot of time. If the memory is not sufficient
    to accommodate the RDD fully, the remaining portion of that RDD will be stored
    (spilled) on the hard disk automatically. One advantage of lazy transformations
    is that it is possible to optimize the transformation steps. For example, if the
    action is to return the first line, Spark computes only a single partition and
    skips the rest.
  prefs: []
  type: TYPE_NORMAL
- en: An RDD may be viewed as a set of partitions (splits) with a list of dependencies
    on parent RDDs and a function to compute a partition given its parents. Sometimes,
    each partition of a parent RDD is used by a single child RDD. This is called *narrow
    dependency*. Narrow dependency is desirable because when a parent RDD partition
    is lost, only a single child partition needs to be recomputed. On the other hand,
    computing a single child RDD partition that involves operations such as *group-by-keys*
    depends on several parent RDD partitions. Data from each parent RDD partition
    in turn is required in creating data in several child RDD partitions. Such a dependency
    is called *wide dependency*. In the case of narrow dependency, it is possible
    to keep both parent and child RDD partitions on a single node (co-partition).
    But this is not possible in the case of wide dependency because parent data is
    scattered across several partitions. In such cases, data should be *shuffled*
    across partitions. Data shuffling is a resource-intensive operation that should
    be avoided to the extent possible. Another issue with wide dependency is that
    all child RDD partitions need to be recomputed even when a single parent RDD partition
    is lost.
  prefs: []
  type: TYPE_NORMAL
- en: Persistence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RDDs are computed on the fly every time they are acted upon through an action
    method. The developer has the ability to override this default behavior and instruct
    to *persist* or *cache* a dataset in memory across partitions. If this dataset
    is required to participate in several actions, then persisting saves a significant
    amount of time, CPU cycles, disk I/O, and network bandwidth. The fault-tolerance
    mechanism is applicable to the cached partitions too. When any partition is lost
    due to node failure, it is recomputed using a lineage graph. If the available
    memory is insufficient, Spark gracefully spills the persisted partitions on to
    the disk. The developer may remove unwanted RDDs using *unpersist*. Nevertheless,
    Spark automatically monitors the cache and removes old partitions using **Least
    Recently Used** (**LRU**) algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Cache()` is the same as `persist()` or `persist (MEMORY_ONLY)`. While the
    `persist()` method can have many other arguments for different levels of persistence,
    such as only memory, memory and disk, only disk, and so on, the `cache()` method
    is designed only for persistence in the memory.'
  prefs: []
  type: TYPE_NORMAL
- en: RDD operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark programming usually starts by choosing a suitable interface that you are
    comfortable with. If you intend to do interactive data analysis, then a shell
    prompt would be the obvious choice. However, choosing a Python shell (PySpark)
    or Scala shell (Spark-Shell) depends on your proficiency with these languages
    to some extent. If you are building a full-blown scalable application then proficiency
    matters a great deal, so you should develop the application in your language of
    choice between Scala, Java, and Python, and submit it to Spark. We will discuss
    this aspect in more detail later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will use both a Python shell (PySpark) and a Scala shell
    (Spark-Shell) to create an RDD. Both of these shells have a predefined, interpreter-aware
    SparkContext that is assigned to a variable `sc`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us get started with some simple code examples. Note that the code assumes
    the current working directory is Spark''s home directory. The following code snippet
    initiates the Spark interactive shell, reads a file from the local filesystem,
    and prints the first line from that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In both the preceding examples, the first line has invoked the interactive shell.
    The SparkContext variable `sc` is already defined as expected. We have created
    an RDD by the name `fileRDD` that points to a file `RELEASE`. This statement is
    just a transformation and will not be executed until an action is encountered.
    You can try giving a nonexistent filename but you will not get any error until
    you execute the next statement, which happens to be an *action* statement.
  prefs: []
  type: TYPE_NORMAL
- en: We have completed the whole cycle of initiating a Spark application (shell),
    creating an RDD, and consuming it. Since RDDs are recomputed every time an action
    is executed, `fileRDD` is not persisted in the memory or hard disk. This allows
    Spark to optimize the sequence of steps and execute intelligently. In fact, in
    the previous example, the optimizer would have just read one partition of the
    input file because `first()` does not require a complete file scan.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that there are two ways to create an RDD: one way is to create a pointer
    to a data source and the other is to parallelize an existing collection. The previous
    examples covered one way, by loading a file from a storage system. We will now
    see the second way, which is parallelizing an existing collection. RDD creation
    by passing in-memory collections is simple but may not work very well for large
    collections, because the input collection should fit completely in the driver
    node''s memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example creates an RDD by passing a Python/Scala list with the
    `parallelize` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A lambda function is an unnamed function, typically used as function arguments
    to other functions. A Python lambda function can be a single expression only.
    If your logic requires multiple steps, create a separate function and use it in
    the lambda expression.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw in the previous example, we were able to pass a Scala/Python collection
    to create an RDD and we also had the liberty to specify the number of partitions
    to cut those collections into. Spark runs one task for each partition of the cluster,
    so it has to be carefully decided to optimize the computation effort. Though Spark
    sets the number of partitions automatically based on the cluster, we have the
    liberty to set it manually by passing it as a second argument to the `parallelize`
    function (for example, `sc.parallelize(data, 3)`). The following is a diagrammatic
    representation of an RDD which is created with a dataset with, say, 14 records
    (or tuples) and is partitioned into 3, distributed across 3 nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating RDDs](img/1-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Writing a Spark program usually consists of transformations and actions. Transformations
    are lazy operations that define how to build an RDD. Most of the transformations
    accept a single function argument. All these methods convert one data source to
    another. Every time you perform a transformation on any RDD, a new RDD will be
    generated, even if it is a small change as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating RDDs](img/image_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is because the RDDs are immutable (read-only) abstractions by design. The
    resulting output from an action can either be written back to the storage system
    or it can be returned to the driver program for local computation if needed to
    produce the final output.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen some simple transformations that define RDDs and some actions
    to process them and generate some output. Let us go on a quick tour of some handy
    transformations and actions followed by transformations on pair RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations on normal RDDs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spark API includes a rich set of transformation operators, and developers
    can compose them in arbitrary ways. Try out the following examples on the interactive
    shell to gain a better understanding of these operations.
  prefs: []
  type: TYPE_NORMAL
- en: The filter operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `filter` operation returns an RDD with only those elements that satisfy
    a `filter` condition, similar to the `WHERE` condition in SQL.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The distinct operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The distinct (`[numTasks]`) operation returns an RDD with a new dataset after
    eliminating duplicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The intersection operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The intersection operation takes another dataset as input. It returns a dataset
    that contains common elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The union operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The union operation takes another dataset as input. It returns a dataset that
    contains elements of itself and the input dataset supplied to it. If there are
    common values in both sets, then they will appear as duplicate values in the resulting
    set after union:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The map operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The map operation returns a distributed dataset formed by executing an input
    function on each of the elements in the input dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The flatMap operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The flatMap operation is similar to the `map` operation. While `map` returns
    one element per input element, `flatMap` returns a list of zero or more elements
    for each input element:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The keys operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The keys operation returns an RDD with the key of each tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The cartesian operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `cartesian` operation takes another dataset as argument and returns the
    Cartesian product of both datasets. This can be an expensive operation, returning
    a dataset of size `m` x `n` where `m` and `n` are the sizes of input datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Transformations on pair RDDs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some Spark operations are available only on RDDs of key value pairs. Note that
    most of these operations, except counting operations, usually involve shuffling,
    because the data related to a key may not always reside on a single partition.
  prefs: []
  type: TYPE_NORMAL
- en: The groupByKey operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to the SQL `groupBy` operation, this groups input data based on the
    key and you can use `aggregateKey` or `reduceByKey` to perform aggregate operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The join operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The join operation takes another dataset as input. Both datasets should be
    of the key value pairs type. The resulting dataset is yet another key value dataset
    having keys and values from both datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The reduceByKey operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The reduceByKey operation merges the values for each key using an associative
    reduce function. This will also perform the merging locally on each mapper before
    sending results to a reducer and producing hash-partitioned output:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The aggregate operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The aggregrate operation returns an RDD with the keys of each tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that in the preceding aggregate examples, the resultant strings (for example,
    `abcd`, `xxabxcd`, `53`, `01`) you get need not match the output shown here exactly.
    It depends on the order in which the individual tasks return their output.
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once an RDD has been created, the various transformations get executed only
    when an *action* is performed on it. The result of an action can either be data
    written back to the storage system or returned to the driver program that initiated
    this for further computation locally to produce the final result.
  prefs: []
  type: TYPE_NORMAL
- en: We have already covered some of the action functions in the previous examples
    of transformations. The following are a few more, but there are a lot more that
    you have to explore.
  prefs: []
  type: TYPE_NORMAL
- en: The collect() function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `collect()` function returns all the results of an RDD operation as an array
    to the driver program. This is usually useful for operations that produce sufficiently
    small datasets. Ideally, the result should easily fit in the memory of the system
    that's hosting the driver program.
  prefs: []
  type: TYPE_NORMAL
- en: The count() function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This returns the number of elements in a dataset or the resulting output of
    an RDD operation.
  prefs: []
  type: TYPE_NORMAL
- en: The take(n) function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `take(n)` function returns the first (`n`) elements of a dataset or the
    resulting output of an RDD operation.
  prefs: []
  type: TYPE_NORMAL
- en: The first() function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `first()` function returns the first element of the dataset or the resulting
    output of an RDD operation. It works similarly to the `take(1)` function.
  prefs: []
  type: TYPE_NORMAL
- en: The takeSample() function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `takeSample(withReplacement, num, [seed])` function returns an array with
    a random sample of elements from a dataset. It has three arguments as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`withReplacement`/`withoutReplacement`: This indicates sampling with or without
    replacement (while taking multiple samples, it indicates whether to replace the
    old sample back to the set and then take a fresh sample or sample without replacing).
    For `withReplacement`, argument should be `True` and `False` otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num`: This indicates the number of elements in the sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Seed`: This is a random number generator seed (optional).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The countByKey() function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `countByKey()` function is available only on RDDs of type key value. It
    returns a table of (`K`, `Int`) pairs with the count of each key.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some example code snippets on Python and Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we touched upon the supported programming languages, their
    advantages and when to choose one language over the other. We discussed the design
    of the Spark engine along with its core components and their execution mechanism.
    We saw how Spark sends the data to be computed across many cluster nodes. We then
    discussed some RDD concepts. We learnt how to create RDDs and perform transformations
    and actions on them through both Scala and Python. We also discussed some advanced
    operations on RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about DataFrames in detail and how they justify
    their suitability for all sorts of data science requirements.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scala language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.scala-lang.org](http://www.scala-lang.org)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Spark architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://lintool.github.io/SparkTutorial/slides/day1_context.pdf](http://lintool.github.io/SparkTutorial/slides/day1_context.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Spark programming guide is the primary resource for concepts; refer to
    the language-specific API documents for a complete list of operations available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory
    Cluster Computing by Matei Zaharia and others is the original source for RDD basics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf](https://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf](http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark Summit, the official event series of Apache Spark, has a wealth of the
    latest information. Check out past events'' presentations and videos:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark-summit.org/2016/](https://spark-summit.org/2016/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
