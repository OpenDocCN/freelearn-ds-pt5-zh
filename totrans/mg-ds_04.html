<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding AI</h1>
                </header>
            
            <article>
                
<p>You now have a good understanding of what data science can do and how we can check whether it works. We have covered the main domains of data science, including machine learning and deep learning, but still, the inner workings of the algorithms are difficult to discern through the fog. In this chapter, we will look at algorithms. You will get an intuitive understanding of how the learning process is defined using mathematics and statistics. Deep neural networks won't be so mystical anymore, and common machine learning jargon will not scare you but provide understanding and ideas to complete the ever-growing list of potential projects.</p>
<p>You are not the only one who will benefit from reading this chapter. Your new knowledge will streamline communication with colleagues, making meetings short and purposeful and teamwork more efficient. We will start at the heart of every machine learning problem: defining the learning process. To do this, we will start with the two subjects that lie at the root of data science: mathematics and statistics.</p>
<p>In this chapter, we will be covering the following topics:</p>
<ul>
<li>Understanding mathematical optimization</li>
<li>Thinking with statistics</li>
<li>How do machines learn?</li>
<li>Exploring machine learning</li>
<li>Exploring deep learning</li>
</ul>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding mathematical optimization</h1>
                </header>
            
            <article>
                
<p>First, we will explore the concept of mathematical optimization. Optimization is the central component of machine learning problem. It turns out that the learning process is nothing more than a mere mathematical optimization problem. The trick is to define it properly. To come up with a good definition, we first need to understand how mathematical optimization works and which problems it can solve.</p>
<p class="mce-root"/>
<p>If you work in the business sector, I bet that you hear the word optimization several times a day. To optimize something means to make it more efficient, cut costs, increase revenues, and minimize risks. Optimization involves taking a number of actions, measuring results, and deciding whether you have ended up in a better place.</p>
<p>For example, to optimize your daily route to work, you can minimize the total time you spend driving from home to the office. Let's suppose that in your case the only thing that matters is time. Thus, optimization means minimization of the time. You may try different options such as using another road or going by public transport instead of driving your car. To choose the best, you will evaluate all routes using the same quantity, that is, the total time to make it from home to the office.</p>
<p>To get a better idea of defining optimization problems, let's consider another example. Our friend Jonathan was tired of his day-to-day job at a bank, so he has started a rabbit farm. It turns out that rabbits breed fast. To start, he bought four rabbits, and after a while he had 16. A month later, there were 256 of them. All of those new rabbits causing additional expense. Jonathan's rabbit sale rates fell lower than the rate at which the rabbits bred. Jonathan's smart farmer friend Aron was impressed with his rabbit production rates, so he proposed to buy all excess rabbits for a discounted price. Now, Jonathan needs to find out how many rabbits to sell to Aron so that he can keep within the following boundaries:</p>
<ul>
<li>He won't get in a situation where he can't sell a rabbit to someone who desperately wants one. The rabbit breeding rate should not fall below the rabbit selling forecasts.</li>
<li>His total expenses for rabbit care stay within the budget.</li>
</ul>
<p>As you can see, we have defined another optimization problem, and the rabbit farm started to remind the bank job that Jonathan left before. This optimization task is quite different though, it looks harder. In the first problem, we tried to minimize commuting time. In this problem, we need to seek the minimum amount of rabbits to sell so it does not violate other conditions. We call problems like these <strong>constrained optimization</strong>. Additional constraints allow us to model more realistic scenarios in complex environments. To name a few, constrained optimization can solve planning, budgeting, and routing problems. In the end, Jonathan was disappointed with his rabbit farm and sold it to Aron. He then continued his path of finding a perfect occupation that wouldn't end up similar to his banking job.</p>
<p>There is one place where profits and losses cease making you mad; that is the mathematics department at a technical university. To get a position there, they ask you to pass an exam. The first task is to find a minimum of a function, <img class="fm-editor-equation" src="Images/bddf2c16-f3e3-4ee5-a413-152095024615.png" style="width:3.83em;height:1.17em;" width="720" height="220"/>.</p>
<p>The following is the plot of this function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-377 image-border" src="Images/911bbd48-87ee-4d8d-b9b9-d7fb2b90ba24.png" style="width:39.58em;height:24.67em;" width="754" height="470"/></p>
<p>While examining the plot, you notice that this function takes values no less than 0, so the answer is obviously 0. The next question looks like the previous one but with a twist—<em>Find a minimum of the function</em> <img class="fm-editor-equation" src="Images/d94e8a03-ff14-400a-8131-1bd7086194e4.png" style="width:6.17em;height:1.33em;" width="1010" height="220"/>, <em>where <img class="fm-editor-equation" src="Images/1def254f-e730-4c92-aaf4-e592208ef0ec.png" style="width:0.67em;height:0.75em;" width="100" height="110"/> is an arbitrary number</em>. To solve it, you draw a bunch of plots and find out that the minimum value is always <em>a</em>.</p>
<p>The last question takes it to the extreme. It says that you won't be given a formula for <img class="fm-editor-equation" src="Images/65cb02ad-8d7e-4ab8-ae3d-3a0fb57d2388.png" style="width:1.58em;height:1.00em;" width="350" height="220"/>, but you can go to your teacher and ask for values of <img class="fm-editor-equation" src="Images/93434734-953b-4790-86f1-da915f2d31de.png" style="width:0.58em;height:1.17em;" width="100" height="200"/> for some <img class="fm-editor-equation" src="Images/d9035d65-10fa-4f72-ace4-b66d9f566858.png" style="width:0.75em;height:0.75em;" width="110" height="110"/> as many times as you want. It is impossible to draw a plot. In other plots, the minimum was always the lowest point. How can we find that point without looking at the plot? To tackle this problem, we will first imagine that we have a plot of this function.</p>
<p>First, we will draw a line between two arbitrary points of the function, as seen in the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-459 image-border" src="Images/ffd06268-af3c-47f9-aca4-fa7b27087a62.png" style="width:35.25em;height:22.92em;" width="728" height="473"/></p>
<p>We will call the distance between those points <img class="fm-editor-equation" src="Images/15e681ce-db67-4f73-8d4b-505deea6e04e.png" style="width:0.67em;height:0.92em;" width="80" height="110"/>. If we make <img class="fm-editor-equation" src="Images/6a7e6175-b05f-4c10-9d92-f13938a7ba93.png" style="width:0.67em;height:0.92em;" width="80" height="110"/> smaller and smaller, two points will be so close that they will visually converge to a single point:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-460 image-border" src="Images/d40fa9e2-4e3f-49a0-a572-563935a309c9.png" style="width:35.75em;height:23.67em;" width="723" height="478"/></p>
<p class="mce-root"/>
<p><span>The line in the preceding plot is called a tangent</span>. It has a very convenient property, and the slope of this line can help us find the minimum or maximum of a function. If the line is flat, then we have found either the minimum or maximum of a function. If all nearby points are higher, then it should be a maximum. If all nearby points are lower, then this is the minimum.</p>
<p>The following plot shows a function (drawn in blue) and its maximum, along with the tangent (drawn in orange):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-379 image-border" src="Images/8205186f-79a2-4ac6-8a63-695d7ce6c7ae.png" style="width:35.75em;height:23.50em;" width="728" height="478"/></p>
<p>Drawing a bunch of lines and points becomes mundane after a while. Thankfully, there is a simple way to compute a slope of this line between <img class="fm-editor-equation" src="Images/b41a95fb-d14c-4d8a-bdbc-4740c20405e3.png" style="width:1.83em;height:1.17em;" width="350" height="220"/> and <img class="fm-editor-equation" src="Images/da500071-60c5-4e9f-a9f1-bcbff0c8c5c0.png" style="width:3.67em;height:1.25em;" width="590" height="200"/>. If you recall the Pythagorean theorem, you will quickly find an answer: <sub><img class="fm-editor-equation" src="Images/429f9755-80d6-431a-b87d-9a7467ec2461.png" style="width:6.08em;height:2.00em;" width="1300" height="430"/></sub>. We can easily find the slope using this formula.</p>
<p>Congratulations, we have just invented our first mathematical optimization algorithm, a gradient descent. As always, the name is scary, but the intuition is simple. To have a good understanding of function optimization, imagine that you are standing on a large hill. You need to descend from it with your eyes closed. You will probably test the area around you by moving your feet around. When you feel a descending direction, you will take a step there and repeat. In mathematical terminology, the hill would be a function, <img class="fm-editor-equation" src="Images/05f59972-aef1-4530-89de-6725bf754977.png" style="width:2.00em;height:1.25em;" width="320" height="200"/>. Each time you evaluate a slope, you calculate the gradient of the function, <img class="fm-editor-equation" src="Images/e17eff6c-1b96-4465-8ce1-da2bc478e56a.png" style="width:2.92em;height:1.25em;" width="510" height="220"/>. You can follow this gradient to find the minimum or maximum of a function. That's why it is called gradient descent.</p>
<p><span>You can solve the final task by using gradient descent. You can choose a starting point, </span><sub><img class="fm-editor-equation" src="Images/c10f822a-4c2a-4f53-be28-925894d94bf8.png" style="width:1.17em;height:0.83em;" width="190" height="140"/></sub><span>, ask for the value of </span><sub><img class="fm-editor-equation" src="Images/20764d2d-3746-443c-a6f9-55212c8b176d.png" style="width:2.33em;height:1.17em;" width="440" height="220"/></sub><span>, and calculate the slope using the small number, </span><img class="fm-editor-equation" src="Images/212858db-b721-4543-bc1c-b54c9fb0ec9f.png" style="width:0.67em;height:0.92em;" width="80" height="110"/><span>. By looking at the slope, you can decide whether your next pick, </span><sub><img class="fm-editor-equation" src="Images/77706a78-632d-40df-87dc-d753b3fa43b4.png" style="width:1.08em;height:0.83em;" width="190" height="140"/></sub>,<span> should be greater or less than </span><sub><img class="fm-editor-equation" src="Images/c8400b52-92cc-463f-94a7-17ff7f8bf387.png" style="width:1.25em;height:0.92em;" width="190" height="140"/></sub><span>. When the slope becomes zero, you can test whether your current value of </span><sub><img class="fm-editor-equation" src="Images/51db79c8-9fd4-4570-97ca-bcf1fa28f884.png" style="width:1.92em;height:1.17em;" width="350" height="220"/></sub><span> is the minimum or maximum by looking at several nearby values. If every value is less than <sub><img class="fm-editor-equation" src="Images/afff9b66-3b6c-46b2-abb6-4fb90cc6c18e.png" style="width:1.83em;height:1.17em;" width="350" height="220"/></sub>, then <sub><img class="fm-editor-equation" src="Images/8e4a7548-7761-46f4-9756-099ce8f2a4d7.png" style="width:0.83em;height:0.83em;" width="110" height="110"/></sub> is the maximum. Otherwise, it is a minimum.</span></p>
<p>As always, there is a caveat. Let's examine this function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-380 image-border" src="Images/cf871086-e906-4a3e-9e7e-59c38b9ca861.png" style="width:32.92em;height:22.00em;" width="705" height="470"/></p>
<p>If we start gradient descent at point <strong>A</strong>, we will end up with a real minimum. But if we start at point <strong>B</strong>, we will be stuck at the local minimum. When you use gradient descent, you can never actually know whether you are in a local or global minimum. One way to check is to repeat the descent from various points that are far away from each other. The other way to avoid local minima is to increase the step size, <img class="fm-editor-equation" src="Images/04a9240d-d9ff-430a-818d-c2b77eb5ee44.png" style="width:0.67em;height:0.92em;" width="80" height="110"/>. But be careful; if <img class="fm-editor-equation" src="Images/60e0e937-c4e9-4c72-b04b-5af0ef22adf6.png" style="width:0.67em;height:0.92em;" width="80" height="110"/> is too large, you will just jump over the minima again and again, never reaching your true goal, the global minimum.</p>
<p>Like in machine learning, there are many mathematical optimization algorithms with different trade-offs. Gradient descent is one of the simplest and easiest to get started with. Despite being simple, gradient descent is commonly used to train machine learning models.</p>
<p class="mce-root">Let's review a few key points before moving on:</p>
<ul>
<li>Mathematical optimization is the central component of machine learning.</li>
<li>There are two kinds of optimization problems: constrained and unconstrained.</li>
<li>Gradient descent is a simple and widely applied optimization algorithm. To understand the intuition behind gradient descent, recall the hill descent analogy.</li>
</ul>
<p>Now you have a good grip on the main principles of mathematical optimization in your tool belt, we can research the field of statistics—the grandfather of machine learning.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Thinking with statistics</h1>
                </header>
            
            <article>
                
<p>Statistics deal with all things about data, namely, collection, analysis, interpretation, inference, and presentation. It is a vast field, incorporating many methods for analyzing data. Covering it all is out of the scope of this book, but we will look into one concept that lies at the heart of machine learning, that is, <strong>maximum likelihood estimation</strong> (<strong>MLE</strong>). As always, do not fear the terminology, as the underlying concepts are simple and intuitive. To understand MLE, we will need to dive into probability theory, the cornerstone of statistics.</p>
<p>To start, let's look at why we need probabilities when we already are equipped with such great mathematical tooling. We use calculus to work with functions on an infinitesimal scale and to measure how they change. We developed algebra to solve equations, and we have dozens of other areas of mathematics that help us to tackle almost any kind of hard problem we can think of. We even came up with category theory that provides a universal language for all mathematics that almost no one can understand (Haskell programmers included).</p>
<p>The difficult part is that we all live in a chaotic universe where things can't be measured exactly. When we study real-world processes we want to learn about many random events that distort our experiments. Uncertainty is everywhere, and we must tame and use it for our needs. That is when probability theory and statistics come into play. Probabilities allow us to quantify and measure uncertain events so we can make sound decisions. Daniel Kahneman showed in his widely known book <em>Thinking, Fast and Slow</em>, that our intuition is notoriously bad in solving statistical problems. Probabilistic thinking helps us to avoid biases and act rationally.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Frequentist probabilities</h1>
                </header>
            
            <article>
                
<p>Imagine that a stranger suggested you play a game: he gives you a coin. You toss it. If it comes up heads, you get $100. If it is tails, you lose $75. Before playing a game, you will surely want to check whether or not it is fair. If the coin is biased toward tails, you can lose money pretty quickly. How can we approach this? Let's conduct an experiment, wherein we will record 1 if heads come up and 0 if we see tails. The fun part is that we will need to make 1,000 tosses to be sure that our calculations are right. Imagine we got the following results: 600 heads (1s) and 400 tails (0s). If we then count how frequent heads or tails came up in the past, we will get 60% and 40%, respectively. We can interpret those frequencies as probabilities of a coin coming up heads or tails. We call this a frequentist view on the probabilities. It turns out that our coin is actually biased toward heads. The expected value of this game can be calculated by multiplying probabilities with their values and summing everything up (the value in the following formula is negative because $40 is a potential loss, not gain):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/c9ef427c-e4dd-4b65-ac08-e900e07fa5b0.png" style="width:13.17em;height:1.08em;" width="2060" height="170"/></p>
<p>The more you play, the more you get. Even after having several consecutive unlucky throws in a row, you can be sure that the returns will average out soon. Thus, a frequentist probability measures a proportion of some event to all other possible events.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Conditional probabilities</h1>
                </header>
            
            <article>
                
<p>It is handy to know the probability of an event given some other event has occurred. We write the conditional probability of an event <img class="fm-editor-equation" src="Images/9f4edf0c-02f4-4f5d-b327-e42b6dcc3a16.png" style="width:0.83em;height:1.00em;" width="130" height="150"/> given event <img class="fm-editor-equation" src="Images/0550de15-ba34-43f1-89cb-7092ac03c448.png" style="width:0.83em;height:1.00em;" width="130" height="150"/> as <img class="fm-editor-equation" src="Images/a2f7111c-68ab-4eff-80fb-67c0fb20a134.png" style="width:2.83em;height:1.00em;" width="560" height="200"/>. Take rain, for example:</p>
<ul>
<li>What is the probability of rain given we hear thunder?</li>
<li>What is the probability of rain given it is sunny?</li>
</ul>
<p>In the following diagram, you can see the probabilities of different events occurring together:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-381 image-border" src="Images/481bc35d-6f6e-46b7-897d-ad0651b4d5d5.png" style="width:22.75em;height:22.58em;" width="749" height="745"/></p>
<p>From this Euler diagram, we can see that <img class="fm-editor-equation" src="Images/28e92085-79d8-4c5a-872b-cc0c512fb639.png" style="width:9.50em;height:1.17em;" width="1630" height="200"/>, meaning that there is always rain when we hear thunder (yes, it is not exactly true, but we'll take this as true for the sake of simplicity).</p>
<p>What about <img class="fm-editor-equation" src="Images/43ee27c6-2ae9-48ce-a87b-33bfbef3869d.png" style="width:6.33em;height:1.08em;" width="1170" height="200"/>? Visually, this probability is small, but how can we formulate this mathematically to do the exact calculations? Conditional probability is defined as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a4c54506-0805-410b-aad6-05bd8cf1501f.png" style="width:16.33em;height:2.67em;" width="2640" height="430"/></p>
<p>In words, we divide the joint probability of both <em>Rain</em> and <em>Sunny</em> by the probability of sunny weather.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dependent and independent events</h1>
                </header>
            
            <article>
                
<p>We call a pair of events independent, if the probability of one event does not influence the other. For example, take the probability of rolling a die and getting a 2 twice in a row. Those events are independent. We can state this as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4da71ad8-0de3-4f03-84b2-5f4d26d8111e.png" style="width:21.08em;height:1.42em;" width="3280" height="220"/></p>
<p class="mce-root"/>
<p>But why does this formula work? First, let's rename events for the first and second tosses as A and B to remove notational clutter and then rewrite the probability of a roll explicitly as a joint probability of both rolls we have seen so far:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/50559135-6ee9-4325-b8dc-956b27439e7b.png" style="width:9.25em;height:1.17em;" width="1740" height="220"/></p>
<p>And now multiply and divide <img class="fm-editor-equation" src="Images/44c56de8-4513-4459-a57c-53b0054b58a1.png" style="width:2.08em;height:1.08em;" width="420" height="220"/> by <img class="fm-editor-equation" src="Images/1d94f705-b256-4be6-97c3-906f2cd98549.png" style="width:2.08em;height:1.08em;" width="430" height="220"/> (nothing changes, it can be canceled out), and recall the definition of conditional probability:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b54ec1b2-12c0-4a51-8eb7-4e828a1b8480.png" style="width:18.92em;height:2.67em;" width="3410" height="480"/></p>
<p>If we read the previous expression from right to left, we find that <img class="fm-editor-equation" src="Images/f9030a42-ae3c-4599-8d4b-b2fcf30b9b2c.png" style="width:6.33em;height:1.08em;" width="1290" height="220"/>. Basically, this means that A is independent of B! The same argument goes for <img class="fm-editor-equation" src="Images/0213653a-f971-423a-9b01-2ec9716cb496.png" style="width:2.08em;height:1.08em;" width="430" height="220"/>.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Bayesian view on probability</h1>
                </header>
            
            <article>
                
<p>Before this point, we always measured probabilities as frequencies. The frequentist approach is not the only way to define probabilities. While frequentists think about probabilities as proportions, the Bayesian approach takes prior information into account. Bayes' theory is centered around a simple theorem that allows us to compute conditional probabilities based on prior knowledge:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/044af8c0-e62f-4e60-9fa5-d9d23981ca19.png" style="width:20.75em;height:2.67em;" width="3730" height="480"/></p>
<p>In this example, the prior value is <img class="fm-editor-equation" src="Images/77bcd102-059d-4827-8e48-ebb7ebe62e33.png" style="width:6.33em;height:1.08em;" width="1290" height="220"/>. If we do not know the real prior value, we can substitute an estimate that is based on our experience to make an approximate calculation. This is the beauty of Bayes' theorem. You can calculate complex conditional probabilities with simple components.</p>
<p>Bayes' theorem has immense value and a vast area of application. The Bayesian theory even has its own branch of statistics and inference methods. Many people think that the Bayesian view is a lot closer to how we humans understand uncertainties, in particular, how prior experience affects decisions we make.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Distributions</h1>
                </header>
            
            <article>
                
<p>Probabilities work with sets of outcomes or events. Many problems we describe with probabilities share common properties. In the following plot, you can see the bell curve:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-382 image-border" src="Images/e75f1b9f-bef0-4888-96d2-2268f20267be.png" style="width:37.50em;height:25.00em;" width="705" height="470"/></p>
<p>The bell curve, or Gaussian distribution, is centered around the most probable set of outcomes, and the tails on both ends represent the least likely outcomes. Because of its mathematical properties, the bell curve appears everywhere in our world. Measure the height of lots of random people, and you will see a bell curve; look at the height of all grass blades in your lawn, and you will see it again. Calculate the probability of people in your city having a certain income, and here it is again.</p>
<p>The Gaussian distribution is one of the most common distributions, but there are many more. A probability distribution is a mathematical law that tells us the probabilities of different possible outcomes of events formulated as a mathematical function.</p>
<p>When we measured relative frequencies of a coin-toss event, we calculated the so-called empirical probability distribution. Coin tosses also can be formulated as a Bernoulli distribution. And if we wanted to calculate the probability of heads after <em>n</em> trials, we may use a binomial distribution.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>It is convenient to introduce a concept analogous to a variable that may be used in probabilistic environments—a random variable. Random variables are the basic building blocks of statistics. Each random variable has a distribution assigned to it. Random variables are written in uppercase by convention, and we use the ~ symbol to specify a distribution assigned to a variable:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/32946440-4404-446d-bd28-815b1df0db25.png" style="width:8.42em;height:1.17em;" width="1580" height="220"/></p>
<p>This means that the random variable <em>X</em> is distributed according to a Bernoulli law with the probability of success (heads) equal to 0.6.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Calculating statistics from data samples</h1>
                </header>
            
            <article>
                
<p>Suppose you are doing research on human height and are eager to publish a mind-blowing scientific paper. To complete your research, you need to measure the average person's height in your area. You can do this in two ways:</p>
<ul>
<li>Collect the heights of every person in your city and calculate average</li>
<li>Apply statistics</li>
</ul>
<p>Statistics allows us to reason about different properties of the population without collecting a full dataset for each person in the population. The process of selecting a random subset of data from the true population is called sampling. A statistic is any function that is used to summarize the data using values from the sample. The ubiquitous statistic that is used by almost everyone on a daily basis is the sample mean or arithmetic average:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6cfed050-984c-4092-b7ca-c64ae9932998.png" style="width:6.17em;height:3.25em;" width="1030" height="540"/></p>
<p>We have collected a random sample of 16 people to calculate an average height. In the following table, we can see the heights over the course of four days:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Day</strong></p>
</td>
<td>
<p><strong>Heights</strong></p>
</td>
<td>
<p><strong>Average</strong></p>
</td>
</tr>
<tr>
<td>
<p>Monday</p>
</td>
<td>
<p>162 cm, 155 cm, 160 cm, 171 cm</p>
</td>
<td>
<p>162.00 cm</p>
</td>
</tr>
<tr>
<td>
<p>Tuesday</p>
</td>
<td>
<p>180 cm, 200 cm, 210 cm, 179 cm</p>
</td>
<td>
<p>192.25 cm</p>
</td>
</tr>
<tr>
<td>
<p>Wednesday</p>
</td>
<td>
<p>160 cm, 170 cm, 158 cm, 176 cm</p>
</td>
<td>
<p>166.00 cm</p>
</td>
</tr>
<tr>
<td>
<p>Thursday</p>
</td>
<td>
<p>178 cm, 169 cm, 157 cm, 165 cm</p>
</td>
<td>
<p>167.25 cm</p>
</td>
</tr>
<tr>
<td>
<p>Total</p>
</td>
<td/>
<td>
<p>171.88 cm</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>We collected a sample of four heights for each day, a total of 16 heights. Your statistician friend Fred told you on Friday that he had already collected a sample of 2,000 people and the average height in the area was about 170 cm.</p>
<p>To investigate, we can look at how your sample average changed with each new data point:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-383 image-border" src="Images/00996332-e5a5-406b-96ab-6d4f476833d9.png" style="width:36.33em;height:24.50em;" width="781" height="527"/></p>
<p>Notice, that on day 2, the average value was unexpectedly high. It may just have happened that we had stumbled upon four tall people. The random fluctuations in the data are called variance.</p>
<p>We can measure sample variance using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/634c6613-06d9-452d-959d-2541f3dc820c.png" style="width:12.25em;height:3.42em;" width="1950" height="540"/></p>
<p>Sample variance summarizes our data, so we can consider it as another statistic. The larger the variance is, the more sample size you need to collect before calculating the accurate average value, which will be close to the real one. This phenomenon has a name—the law of large numbers. The more measurements you make, the better your estimate will be.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Statistical modeling</h1>
                </header>
            
            <article>
                
<p>Statistics is more than simply calculating summary numbers. One of the most interesting aspects of statistics is modeling. Statistical modeling studies mathematical models that make a set of statistical assumptions about data. To be more clear, let's return to our weather example. We have collected a dataset with random variables that describe the current weather:</p>
<ul>
<li>Average speed of the wind</li>
<li>Air humidity</li>
<li>Air temperature</li>
<li>Total number of birds seen in the sky in a local area</li>
<li>Statistician's mood</li>
</ul>
<p>Using this data, we want to infer which variables are related to rain. To do this, we will build a statistical model. Besides the previous data, we have recorded a binary rain variable that takes the value 1 if it rained, and 0 otherwise.</p>
<p>Now, we pose a set of assumptions in relation to the data:</p>
<ul>
<li>Rain probability has a Bernoulli distribution.</li>
<li>Rain probability depends on data we have collected. In other words, there is a relationship between the data and rain probability.</li>
</ul>
<p>You may find it strange thinking about rain in terms of probability. What does it mean to say that last Wednesday, the probability of <span>rain </span>was 45%? Last Wednesday is a past date, so we can examine the data and check whether there was rain. The trick is to understand that in our dataset, there are many days similar to Wednesday. Let's suppose that we have collected the following values:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Day of week</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Speed of wind</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Humidity</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Temperature</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Outcome</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Monday</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5 m/s</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>50%</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>30 C</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>no rain</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Tuesday</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>10 m/s</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>80%</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>25 C</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>rain</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Wednesday</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>5 m/s</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>52%</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>28 C</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>rain</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Thursday</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>3 m/s</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>30%</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>23 C</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>no rain</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Friday</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>8 m/s</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>35%</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>27 C</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>no rain</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>In this example, Monday and Wednesday are very similar, but their rain outcomes are different. In a sufficiently large dataset, we could find two rows that match exactly but have different outcomes. Why is this happening? First, our dataset does not include all the possible variables that can describe rain. It is impossible to collect such a dataset, so we make an assumption that our data is related to rain, but does not describe it fully. Measurement errors, the randomness of events, and incomplete data make rain probabilistic. You may wonder if rain is probabilistic in nature? Or is every period of rain predetermined? To check whether rain events are deterministic, we must collect a daily snapshot of the complete state of the universe, which is impossible. Statistics and probability theory help us to understand our world even if we have imperfect information. For example, imagine that we have 10 days similar to last Wednesday in our dataset. By similar, I mean that all variables we have collected differ only by a small amount. Out of those 10 days, 8 were rainy and 2 were sunny. We may say that on a day typical to last Wednesday there is an 80% probability of rain. That is the most accurate answer we can give using this data.</p>
<p>Having assumptions about data in place, we can proceed to modeling. We can make another assumption that there exists some mathematical model <strong>M</strong>, that uses data to estimate rain probability. That is, model <strong>M</strong> uses data <strong>d</strong> to learn the relationship between the data and rain probability. The model will infer this relationship by assigning rain probabilities that are closest to real outcomes in our dataset.</p>
<p>The main goal of model <strong>M</strong> is not to make accurate predictions, but to find and explain relationships. This is where we can draw a line between statistics and machine learning. Machine learning seeks to find accurate predictive models, while statistics uses models to find explanations and interpretations. Goals differ, but the underlying concepts that allow models to learn from data are the same. Now, we can finally uncover how this model <strong>M</strong> can learn from data. We will disentangle the magic, leaving a straightforward understanding of the mathematics behind machine learning.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How do machines learn?</h1>
                </header>
            
            <article>
                
<p>How do algorithms learn? How can we define learning? As humans, we learn a lot throughout our lives. It is a natural task for us. In the first few years of our lives, we learn how to control our body, walk, speak, and recognize different objects. We constantly get new experiences, and these experiences change the way we think, behave, and act. Can a piece of computer code learn like we do? To approach machine learning, we first need to come up with a way to transmit experience directly to the algorithm.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In practical cases, we are interested in teaching algorithms to perform all kinds of specific tasks faster, better, and more reliably that we can do ourselves. For now, we will focus on prediction and recognition tasks. Thus, we want to build algorithms that are able to recognize patterns and predict future outcomes. The following table shows some examples of recognition and prediction tasks:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p>Recognition tasks</p>
</td>
<td>
<p>Is this a high-paying customer?</p>
<p>How much does this house cost in the current market?</p>
<p>What are those objects in an image?</p>
</td>
</tr>
<tr>
<td>
<p>Prediction tasks</p>
</td>
<td>
<p>Is this customer likely to return his debt in the next 6 months?</p>
<p>How much will we sell in the next quarter?</p>
<p>How risky is this investment?</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>The first idea may be to approach learning as humans do, and provide explanations and examples through speech, images, and sets of examples. Unfortunately, while learning in this way, we perform<span class="Apple-converted-space"> </span>many complex cognitive tasks, such as listening, writing, and speaking. A computer algorithm by itself cannot collect new experiences the way we do. What if, instead, we take a simplified model of our world in the form of digital data? For example, predicting customer churn for Acme Co could be done only using data about customer purchases and product ratings. The more complete and full the dataset is, the more accurate the model of customer churn is likely to be.</p>
<p>Let's look at another example. We will build a machine learning project cost estimator. This model will use the attributes of a project to calculate the cost estimate. Suppose that we have collected the following data attributes for each project in our company:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Attribute name</strong></p>
</td>
<td>
<p><strong>Attribute type</strong></p>
</td>
<td>
<p><strong>Attribute description</strong></p>
</td>
<td>
<p><strong>Possible values</strong></p>
</td>
</tr>
<tr>
<td>
<p>Number of attributes</p>
</td>
<td>
<p>Integer</p>
</td>
<td>
<p>Number of data attributes in the project dataset</p>
</td>
<td>
<p>0 to ∞</p>
</td>
</tr>
<tr>
<td>
<p>Number of data scientists</p>
</td>
<td>
<p>Integer</p>
</td>
<td>
<p>Number of data scientists requested by the customer for project implementation</p>
</td>
<td>
<p>0 to ∞</p>
</td>
</tr>
<tr>
<td>
<p>Integration</p>
</td>
<td>
<p>Integer</p>
</td>
<td>
<p>Integration with customer's software systems requested by the customer</p>
</td>
<td>
<p>0 for no integration in project scope</p>
<p>1 for integration in project scope</p>
</td>
</tr>
<tr>
<td>
<p>Is a large company</p>
</td>
<td>
<p>Integer</p>
</td>
<td>
<p>Indicates if the customer has a large number of employees</p>
</td>
<td>
<p>0 = customer's company employee number greater than 100</p>
<p>1 = customer's company employee number less or equal to 100</p>
</td>
</tr>
<tr>
<td>
<p>Total project cost</p>
</td>
<td>
<p>Integer</p>
</td>
<td>
<p>Total cost in USD</p>
</td>
<td>
<p>0 to ∞</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>The example dataset containing these attributes is provided in the following table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Number of attributes</strong></p>
</td>
<td>
<p><strong>Number of data scientists</strong></p>
</td>
<td>
<p><strong>Integration</strong></p>
</td>
<td>
<p><strong>Is a large company</strong></p>
</td>
<td>
<p><strong>Total project cost</strong></p>
</td>
</tr>
<tr>
<td>
<p>10</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>135,000</p>
</td>
</tr>
<tr>
<td>
<p>20</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>140,000</p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>173,200</p>
</td>
</tr>
<tr>
<td>
<p>100</p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>300,000</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>The simplest model we can imagine is a so-called linear model. It sums data attributes multiplied by variable coefficients to calculate the project cost estimate:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4e5bca27-1c7f-4532-ac51-06629d71190b.png" style="width:45.42em;height:4.25em;" width="7480" height="700"/></p>
<p>In this simplified scenario, we do not know the real values of cost variables. However, we can use statistical methods and estimate them from data. Let's start with a random set of parameters:</p>
<ul>
<li>Base cost = 50,000</li>
<li>Cost per data attribute = 115</li>
<li>Cost per data scientist = 40,000</li>
<li>Integration cost = 50,000</li>
<li>Customer relation complexity cost = 5,000</li>
</ul>
<p>If we use the parameters for every project we have in our dataset, we will get the following results:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><em><strong>Total project 1 cost = 50,000 + 115 x 10 + 40,000 x 1 + 50,000 x 1 + 50,000 x 0 = 141,150</strong></em></p>
<p class="CDPAlignCenter CDPAlign"><em><strong><span>Total project 2 cost = 50,000 + 115 x 20 + 40,000 x 1 + 50,000 x 0 + 50,000 x 1 = 142,300</span></strong></em></p>
<p class="CDPAlignCenter CDPAlign"><em><strong><span>Total project 3 cost = 50,000 + 115 x 5 + 40,000 x 2 + 50,000 x 1 + 50,000 x 0 = 180,575</span></strong></em></p>
<p class="CDPAlignCenter CDPAlign"><em><strong><span>Total project 4 cost = 50,000 + 115 x 100 + 40,000 x 3 + 50,000 x 1 + 50,000 x 1 = 281,500</span></strong></em></p>
<p><span>You have probably noticed that the values differ from the real project costs in our dataset. This means that if we use this model on any real project our estimates will be erroneous. We can measure this error in multiple ways, but let's stick with one of the most popular choices:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/8a0ef489-b139-450a-b4af-da5d63b5b94c.png" style="width:26.58em;height:2.25em;" width="4490" height="380"/></p>
<div class="packt_infobox"><span>There are many ways to quantify prediction errors. All of them introduce different trade-offs and limitations. Error measure selection is one of the most important technical aspects of building a machine learning model. </span></div>
<p>For the overall error, let's take an arithmetic average of individual errors of all projects. The number that we calculated is called the <strong>root mean squared error</strong> (<strong>RMSE</strong>).</p>
<div class="packt_infobox"><span>The exact mathematical form of this measure is not a consequence. RMSE has straightforward logic behind it. While we can derive the RMSE formula using several technical constraints posed on a linear model, mathematical proofs are out of the scope of this book.</span></div>
<p>It turns out that we can use optimization algorithms to tweak our cost parameters so that RMSE will be minimized. In other words, we can find the best fit for cost parameters that minimize the error for all rows in the dataset. We call this procedure MLE.</p>
<div class="packt_infobox"><span>MLE gives a way to estimate the parameters of a statistical model given data. It seeks to maximize the probability of parameters given data. It may sound difficult, but the concept becomes very intuitive if we rephrase the definition as a question: what parameters should we set so the results we get will be closest to the data? MLE helps us to find the answer to this question.</span></div>
<p>Let's focus on another example to get a more general approach. Imagine that we have started a coffee subscription service. A customer chooses her favorite flavors of coffee in our mobile app and fills in an address and payment information. After that, our courier delivers a hot cup of coffee every morning. There is a feedback system built in the app. We promote seasonal offerings and discounts to clients via push notifications. There was a big growth in subscribers last year: almost 2,000 people are already using the service and 100 more are subscribing each month. However, our customer churn percentage is growing disturbingly fast. Marketing offers do not seem to make a big difference. To solve this problem, we have decided to build a machine learning model that predicts customer churn in advance. Knowing that a customer will churn, we can tailor an individual offer that will turn them into active user again.</p>
<p>This time, we will be more rigorous and abstract in our definitions. We will define a model <strong>M</strong> that takes customer data <strong>X</strong> and historical churn outcomes <strong>Y</strong>.<span class="Apple-converted-space"> </span>We will call <strong>Y</strong> the target variable.</p>
<p>The following table describes the attributes of our dataset:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Attribute name</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Attribute type</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p class="CDPAlignCenter CDPAlign"><strong>Attribute description</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Possible values</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Months subscribed</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Integer</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>The number of months a user has been subscribed to our service</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0 to ∞</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Special offerings activated</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Integer</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>The number of special offers the user activated last month</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0 to ∞</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Number of cups on weekdays</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Float</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>The average number of cups the user orders on weekdays, last month</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1.0 to 5.0</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Number of cups on the weekend</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Float</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>The average number of cups the user orders on weekends, last month</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>1.0 to 2.0</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>This kind of table is called a data dictionary. We can use it to understand the data coming in and out of the model, without looking into the code or databases. Every data science project must have<span class="Apple-converted-space"> </span>an up-to-date data dictionary. More complete examples of data dictionaries will be shown later in this book.</p>
<p>Our target variable, <strong>Y</strong>, can be described in the following way:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Target variable name</strong></p>
</td>
<td>
<p><strong>Target variable type</strong></p>
</td>
<td>
<p><strong>Target variable description</strong></p>
</td>
<td>
<p><strong>Target variable possible values</strong></p>
</td>
</tr>
<tr>
<td>
<p>Churn</p>
</td>
<td>
<p>Integer</p>
</td>
<td>
<p>Indicates whether the user stopped using our service last month</p>
</td>
<td>
<p>0 or 1</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>Given the customer description, <img class="fm-editor-equation" src="Images/47db3c92-704a-4938-a88d-aededf2bbb93.png" style="width:0.75em;height:0.75em;"/>, the model outputs churn probability, <img class="fm-editor-equation" src="Images/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png" style="width:0.58em;height:1.08em;"/>. A hat over <img class="fm-editor-equation" src="Images/e52dbafb-bbd9-49d6-b4b2-c433dcdc1282.png" style="width:0.58em;height:1.00em;" width="90" height="160"/> means that <img class="fm-editor-equation" src="Images/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png" style="width:0.58em;height:1.08em;"/> is not a real churn probability, but only an estimate that can contain errors. This value will not be strictly zero or one. Instead, the model will output a probability between 0% and 100%. For example, for some customer <img class="fm-editor-equation" src="Images/47db3c92-704a-4938-a88d-aededf2bbb93.png" style="width:0.75em;height:0.75em;" width="110" height="110"/>, we got a <img class="fm-editor-equation" src="Images/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png" style="width:0.58em;height:1.08em;" width="100" height="190"/> of 76%. We may interpret this value like this: based on historical data, the expectancy of this customer to churn is 76%. Or, out of 100 customers like customer <img class="fm-editor-equation" src="Images/30287e69-02b1-4fbb-9a6d-527793dd6d66.png" style="width:0.75em;height:0.75em;" width="110" height="110"/>, 76 will churn.</p>
<p>A machine learning model must have some variable parameters that can change to better match churn outcomes. Now that we have used formulas, we can't go on without introducing at least one Greek letter. All the parameters of our model will be represented by <img class="fm-editor-equation" src="Images/3457eb94-12b2-4edd-ab4e-622dec683370.png" style="width:0.58em;height:1.08em;" width="90" height="170"/>.</p>
<p>Now, we have everything in place:</p>
<ul>
<li>Historical customer data <img class="fm-editor-equation" src="Images/e5c22cd2-7bee-4f0f-9ab5-4a14b5f72c4a.png" style="width:0.92em;height:0.92em;" width="160" height="160"/> and churn outcomes <img class="fm-editor-equation" src="Images/b9742446-b92d-4048-988c-bd56185c6a35.png" style="width:0.83em;height:0.92em;" width="140" height="150"/>, which we will call the training dataset <img class="fm-editor-equation" src="Images/03798ff0-0bc9-462d-acdc-7444803b369b.png" style="width:2.58em;height:1.00em;" width="470" height="180"/></li>
<li>Machine learning algorithm <img class="fm-editor-equation" src="Images/5b08d2dd-01fb-4d28-90dd-af668b42920d.png" style="width:1.25em;height:1.00em;" width="200" height="160"/> that accepts customer description <img class="fm-editor-equation" src="Images/747c1c74-c079-4d1c-b27c-f584b284b30c.png" style="width:0.83em;height:0.83em;" width="110" height="110"/> and outputs churn probability <img class="fm-editor-equation" src="Images/80513bf6-2166-4fb5-b16d-ba7c898db5cb.png" style="width:0.67em;height:1.25em;" width="100" height="190"/></li>
<li>Model parameters <img class="fm-editor-equation" src="Images/667c9f2f-7ffa-4be7-9f9e-9b705773a02e.png" style="width:0.50em;height:0.92em;" width="90" height="170"/> that can be tuned using MLE</li>
</ul>
<p>We will estimate the parameters <img class="fm-editor-equation" src="Images/25e1af35-cbdd-413b-9e89-de80eadc420a.png" style="width:0.67em;height:1.25em;" width="110" height="210"/> for our model <img class="fm-editor-equation" src="Images/0c1721ef-3faa-45cd-98a4-77a06950ed10.png" style="width:1.00em;height:0.83em;" width="200" height="160"/> using MLE on the training dataset <img class="fm-editor-equation" src="Images/8d37f0bc-1c35-40e5-9b5c-e2533fdc2bcd.png" style="width:3.08em;height:1.17em;" width="470" height="180"/>. I have placed a hat on top of <img class="fm-editor-equation" src="Images/2ceb9466-e565-4ecc-8ac6-eeef6dc17921.png" style="width:0.58em;height:1.08em;"/> to indicate that in theory there may be the best parameter set <img class="fm-editor-equation" src="Images/2ceb9466-e565-4ecc-8ac6-eeef6dc17921.png" style="width:0.58em;height:1.08em;" width="90" height="170"/>, but in reality we have limited data. Thus, the best parameter set we can get is only an estimate of true that may contain errors.</p>
<p>Now, we can finally use our model to make predictions about customer churn<br/>
probability <img class="fm-editor-equation" src="Images/b8b4a596-2252-4c58-9284-940b53d5864b.png" style="width:0.75em;height:1.42em;" width="100" height="190"/>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/af320444-5ea3-465c-b4ac-ac4cefe58fe4.png" style="width:6.58em;height:1.50em;" width="970" height="220"/></p>
<div class="packt_infobox"><span>The exact interpretation of probability depends heavily on the model <strong>M</strong> we used to estimate this probability. Some models can be used to give probabilistic interpretations, while others do not have such qualities.</span></div>
<p class="mce-root"/>
<p>Note that we had not explicitly defined the kind of machine learning model <strong>M</strong> we use. We have defined an abstract framework for learning from data that does not depend on the specific data or concrete algorithms it uses. This is the beauty of mathematics that opens up limitless practical applications. With this abstract framework, we can come up with many models <strong>M</strong> that have different trade-offs and capabilities. This is how machines learn.</p>
<div class="packt_tip">
<p><strong>How to choose a model</strong></p>
There are many different types of machine learning models and many estimation methods. Linear regression and MLE are among the simplest examples that show the underlying principles that lie beneath many machine learning models. A theorem called the <strong>no free lunch theorem</strong> says that there is no model that will give you the best results for each task for every dataset. Our machine learning framework is abstract, but this does not mean it can yield a perfect algorithm. Some models are best for one task, but are terrible for another. One model may classify images better than humans do, but it will fail at credit scoring. The process of choosing the best model for a given task requires deep knowledge of several disciplines, such as machine learning, statistics, and software engineering. It depends on many factors, such as statistical data properties, the type of task we are trying to solve, business constraints, and risks. That is why only a professional data scientist can handle the selection and training machine of learning models. This process has many intricacies and explaining all of them is beyond the scope of this book. An interested reader may refer to the book list at the end of this book. There you can find free books that explain the technical side of machine learning in great depth.</div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exploring machine learning</h1>
                </header>
            
            <article>
                
<p>Now that you understand the general flow of thought on how to define learning processes using mathematics and statistics, we can explore the inner workings of machine learning. Machine learning studies algorithms and statistical models that are able to learn and perform specific tasks without explicit instruction. As every software development manager should have some expertise in computer programming, the data science project manager should understand machine learning. Grasping the underlying concepts between any machine learning algorithm will allow you to understand better the limitations and requirements for your project. It will ease communication and improve understanding between you and the data scientists on your team. Knowledge of basic machine learning terminology will make you speak in the language of data science.</p>
<p class="mce-root"/>
<p>We will now dive into the main intuitions behind popular machine learning algorithms, leaving out technical details for the sake of seeing the wood for the trees.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Defining goals of machine learning</h1>
                </header>
            
            <article>
                
<p>When we speak about machine learning we speak about accurate predictions and recognition.<span class="Apple-converted-space"> </span>Statisticians often use simple but interpretable models with a rigorous mathematical base to explain data and prove points. Machine learning specialists build more complex models that are harder to interpret and often work like black boxes. Thus, many machine learning algorithms are more suited to prediction quality than model interpretability. Trends change slowly, and while more researchers look into topics of model interpretation and prediction explanation, the prime goal of machine learning continues to be the creation of faster and more accurate models.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using a life cycle to build machine learning models</h1>
                </header>
            
            <article>
                
<p>When creating machine learning models, we typically follow a fixed set of stages:</p>
<ul>
<li><strong>Exploratory data analysts</strong>:<strong> </strong>In this stage, a data scientist uses a set of statistical and visualization techniques to have a better understanding of data.</li>
<li><strong>Data preparation</strong>: In this part, a data scientist transforms data into a format suitable for applying a machine learning algorithm.</li>
<li><strong>Data preprocessing</strong>: Here, we clean prepared data and transform it so that a machine learning algorithm can properly use every part of the data.</li>
<li><strong>Modeling</strong>: In this part, a data scientist trains machine learning models.</li>
<li><strong>Testing</strong>: In this stage, we evaluate the model using a set of metrics that measure its performance.</li>
</ul>
<p>This process repeats many times before we achieve sufficiently good results. You can apply the life cycle to train many kinds of machine learning models, which we will explore next.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear models</h1>
                </header>
            
            <article>
                
<p>The most basic type of machine learning models is a linear model. We have already seen a detailed example of a linear model in the previous section. Predictions of a linear model can be interpreted by looking at coefficients of a model. The greater the coefficient, the more its contribution to the final prediction. While simple, those models are often not the most accurate. Linear models are fast and computationally efficient, which makes them valuable in settings with lots of data and limited computational resources.</p>
<p>Linear models are fast, efficient, simple, and interpretable. They can solve both classification and regression problems.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classification and regression trees</h1>
                </header>
            
            <article>
                
<p><strong>Classification and regression tree</strong> (<strong>CART</strong>) take a very intuitive approach for making predictions. CART build a decision tree based on the training data. If we use CART for a credit default risk task, we may see a model like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-386 image-border" src="Images/5bd3570c-9d49-4528-8331-e3d564709153.png" style="width:207.33em;height:106.00em;" width="2488" height="1272"/></p>
<p class="CDPAlignLeft CDPAlign">To make a prediction, an algorithm starts from the top of the tree and makes consecutive choices based on values in the data. For binary classification, at the bottom of the tree you will get a proportion of positive cases for similar customers.</p>
<p class="CDPAlignLeft CDPAlign">While simple, CART models suffer from two disadvantages:</p>
<ul>
<li>Low prediction accuracy.</li>
<li>There are many possible trees for a single dataset. One tree may have significantly better prediction accuracy than the other.</li>
</ul>
<p class="mce-root"/>
<p>But how does CART choose columns and values for splits? We will explore the general logic of CART on binary classification:</p>
<ol>
<li>At first, it takes a single column and divides the data into two parts for each value of this column.</li>
<li>Then it computes a proportion of positive cases for each split.</li>
<li><em>Step 1</em> and <em>step 2</em> are repeated for each column in the data.</li>
<li>We rank each split by how well it divides the data. If the split divides the dataset perfectly, then positive cases would be for all values, lower than a threshold and negative cases would be on the other side. To illustrate, if <strong>Age &gt; 25</strong> is a perfect split, then all customers younger than 25 will have credit defaults and all customers older than 25 will have a perfect credit history.</li>
<li>According to <em>step 4</em>, the best split is chosen for the current level of the tree. The dataset is divided into two parts according to the split value.</li>
<li><em>Steps 1</em> to <em>5</em> are repeated for each new dataset part.</li>
<li>The procedure continues before the algorithm meets a stopping criterion. For example, we can stop building a tree by looking at the depth of the decision tree or the minimum number of data points available for the next split.</li>
</ol>
<p>We can also apply CART to regression problems, although the algorithm would be slightly more complicated. CART is simple and interpretable, but it produces very weak models that are rarely applied in the practice. However, the properties of the algorithm and implementation tricks allow us to use their weakest point as their main strength. We will learn how to exploit these properties in the following section.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Ensemble models</h1>
                </header>
            
            <article>
                
<p>Suppose that you own a retail store franchise. Business is growing and you are ready to build a new store. The question is, where should you build it? Selection of a building location is extremely important as it is permanent and it defines the local customer base that will go into your shop.</p>
<p>You have several options to make this decision:</p>
<ol>
<li>Decide yourself.</li>
<li>Ask for the advice of the most competent employee.</li>
<li>Ask the opinion of many slightly less experienced employees.</li>
</ol>
<p><em>Options 1</em> and <em>2</em> encompass one and two persons that make a decision. <em>Option 3</em> encompasses opinions of several of experts. Statistically, <em>option 3</em> is likely to yield a better decision. Even world-class experts can make a mistake. Several professionals, sharing information between each other, are much more likely to succeed. That is the reason why living in big communities and working in large organizations leads to great results.</p>
<p>In machine learning, this principle works too. Many models can contribute to making a single decision in an ensemble. Model ensembles tend to be more accurate than single models, including the most advanced ones. Be wary, though; you need to build many models to create an ensemble. A large number of models increase computational resource requirements quite rapidly, making a tradeoff between prediction accuracy and speed.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tree-based ensembles</h1>
                </header>
            
            <article>
                
<p>A particularly useful model for ensembles is a decision tree. There is an entire class of machine learning models devoted to different ways of creating tree ensembles. This type of model is the most frequent winner of Kaggle competitions on structured data, so it is important to understand how it works.</p>
<p>Trees are good candidates to build ensembles because they have high variance. Because of the randomness in a tree-building algorithm, every decision tree differs from the previous one, even if the dataset does not change. Each time we build a decision tree, we may come up with something that's different from before. Thus, each tree will make different errors. Recall the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-387 image-border" src="Images/d148d81a-041f-4c10-ac00-aee5b322721b.png" style="width:22.75em;height:22.75em;" width="682" height="682"/></p>
<p class="mce-root"/>
<p>It turns out that decision trees have extremely low bias and high variance. Imagine that many different trees make hundreds of predictions for each individual, creating an ensemble. What would happen if we average all predictions? <span>We will be a lot closer to the real answer. When used in an ensemble, decision trees can handle complex datasets with high prediction accuracy.</span></p>
<p><span>In the following diagram, you can see how multiple trees can create an ensemble:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-388 image-border" src="Images/9686fcd0-4f5a-4686-89e7-53a94528923b.png" style="width:61.83em;height:42.25em;" width="2358" height="1612"/></p>
<p>If you are working with structured data, be sure to try decision tree ensembles before jumping into other areas of machine learning, including deep learning. Nine times out of ten, the results will satisfy both you and your customer. Media often overlooks the value of this algorithm. Praise for ensemble models is rare to find, yet it is arguably the most commonly used algorithm family for solving practical applied machine learning problems. Be sure to give tree ensembles a chance.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Clustering models</h1>
                </header>
            
            <article>
                
<p>Another useful application of machine learning is clustering. In contrast to other machine learning problems we have studied in this section, clustering is an unsupervised learning problem. This means that clustering algorithms can work with unlabeled data. To illustrate, let's look at a task that's central to marketing departments—customer segmentation. Coming up wit<span>h marketing offers for each individual client may be impossible. For example, if you own a large retail store network, you want to apply different discounts at stores depending on customer interests to boost sales. To do this, marketing departments create customer segments and tailor marketing companies to each specific segment.</span></p>
<p><span>In the following diagram, you can see six customers assigned to two different segments:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-389 image-border" src="Images/455c6dc0-8c82-42f8-b509-634317dc759e.png" style="width:31.17em;height:15.17em;" width="1104" height="536"/></p>
<p>We may automate customer segmentation by taking all purchase histories for all customers and applying a clustering algorithm to group similar customers together. The algorithm will assign each customer to a single segment, allowing you to further analyze those segments. While exploring data inside each segment, you may find interesting patterns that will give insights for new marketing offers targeted at this specific segment of customers.</p>
<p>We can apply clustering algorithms to data in an ad hoc manner because they don't require prior labeling. However, the situation can get complicated, as many of the algorithms suffer from the curse of dimensionality and can't work with many columns in the data.</p>
<p>The most popular clustering algorithm is K-means. In its simplest form, the algorithm has only one parameter: the number of clusters to find in the data. K-means approaches clustering from a geometrical standpoint. Imagine each data row as a point in space. For us, this idea is easy for datasets with two or three points, but it works well beyond three dimensions. Having laid out our dataset in a geometric space, we can now see that some points will be closer to each other. K-means finds center points around which other points cluster.</p>
<p class="mce-root"/>
<p>It does this iteratively, as follows:</p>
<ol>
<li>It takes the current cluster centers (for the first iteration, it takes random points).</li>
<li>It goes through all data rows and assigns them to the closest cluster's center point.</li>
<li>It updates cluster centers by averaging the locations of all points from <em>step 2</em>.</li>
</ol>
<p>The algorithm is explained in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-390 image-border" src="Images/47842a77-e1b1-48d7-a5e7-af9423a38a22.png" style="width:225.17em;height:68.25em;" width="2702" height="819"/></p>
<p class="mce-root"/>
<p>At this point, we conclude the introduction to machine learning. While there are many more machine learning algorithms to study, describing them is beyond the scope of this book. I am sure you will find that knowledge about regression, decision trees, ensemble models, and clustering covers a surprisingly large portion of practical applications and will serve you well. Now we are ready to move on to deep learning.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exploring deep learning</h1>
                </header>
            
            <article>
                
<p>Deep neural networks that classify images and play Go better than we do create an impression of extremely complex models whose internals are inspired by our own brain's structure. In fact, the central ideas behind neural networks are easy to grasp. While first neural networks were indeed inspired by the physical structure of our brain, the analogy no longer holds and the relation to physical processes inside the human brain is mostly historical.</p>
<p>To demystify neural networks, we will start with the basic building blocks: artificial neurons. An artificial neuron is nothing more than two mathematical functions. The first takes a bunch of numbers as input and combines them by using its internal state—weights. The second, an activation function, takes the output of the first and applies special transformations. The activation function tells us how active this neuron is to a particular input combination. In the following diagram, you can see how an artificial neuron converts the input to output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-391 image-border" src="Images/9c3f5af2-af84-4bb6-9cc6-68693be66f4a.png" style="width:40.25em;height:13.08em;" width="2302" height="749"/></p>
<p>In the following diagram, we can see the plot of the most popular activation function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-392 image-border" src="Images/5077ba7a-3bcb-48b0-9387-9512feaad043.png" style="width:32.75em;height:21.50em;" width="760" height="500"/></p>
<p class="mce-root"/>
<p>If the output is less than 0, the function will output 0. If it is greater, it will echo its input. Simple, isn't it? Try to come up with the name for this function. I know, naming is hard. Names should be simple, while conveying deep insights about the core concept of the thing you are naming. Of course, mathematicians knew this and, as we have witnessed many times before, came up with a perfect and crystal-clear name—<strong>rectified linear unit</strong> (<strong>ReLU</strong>). An interesting fact is that ReLU does not conform to basic requirements for an activation function, but still gives better results than other alternatives. Other activation functions may be better in specific situations, but none of them beat ReLU in being a sensible default.</p>
<p>Another important activation function you need to know about is sigmoid. You can see it in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-393 image-border" src="Images/262339b8-8361-415e-9196-6a9a8132fd08.png" style="width:34.50em;height:23.08em;" width="747" height="500"/></p>
<p>Before ReLU came to the throne, sigmoid was a popular choice as an activation function. While its value as an activation function has faded, the sigmoid is still important for another reason. It often comes up in binary classification problems. If you look at the plot closely, you will find it can map any number to a range between 0 and 1. This property makes sigmoid useful when modeling binary classification problems.</p>
<div class="packt_infobox">Note that we use sigmoid in binary classification problems not because it conveniently maps any number to something between 0 and 1. The reason behind this useful property is that sigmoid, also called the logistic function, is tightly related to the Bernoulli probability distribution. This distribution describes events that can happen with probability <em><strong>p</strong></em> between 0 and 1. For example, a Bernoulli distribution can describe a coin toss with <em><strong>p = 0.5</strong></em> or 50%. As you can see, any binary classification problem can be naturally described by the Bernoulli distribution. To see how, look at the following questions: <em>What is the probability of a client clicking on an ad?</em> <em>What is the probability of a client stating a default while being in debt?</em> We can model these cases as Bernoulli distributions.</div>
<p>Now, we know the main components of an artificial neuron: weights and activation function. To make a neuron work, we need to take its input and combine it with neuron weights. To do this, we can recall linear regression. Linear regression models combine data attributes by multiplying each attribute to a weight and then summing them up. Then, apply an activation function, and you will get an artificial neuron. If our data row had two columns named <em>a</em> and <em>b</em>, the neuron would have two weights, <img class="fm-editor-equation" src="Images/99978480-9cf3-4eea-aaaf-8ea1d21db412.png" style="width:1.33em;height:0.92em;" width="220" height="150"/> and <img class="fm-editor-equation" src="Images/9fa78070-be91-46b4-8591-9efde6594188.png" style="width:1.33em;height:0.92em;" width="220" height="150"/>. A formula for a neuron with ReLU activation is shown as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/126417e1-835a-47ee-9965-2ca8eae45ded.png" style="width:20.25em;height:1.33em;" width="3340" height="220"/></p>
<p>Note that <img class="fm-editor-equation" src="Images/ccfa7b45-6039-45f6-a898-d338ee0a57aa.png" style="width:1.33em;height:0.92em;" width="220" height="150"/> is a special weight called a bias that is not tied to any input.</p>
<p>So, an artificial neuron is just a bunch of multiplications and additions:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-394 image-border" src="Images/e7aa98f4-39cf-4175-8f6a-edebbe2efc87.png" style="width:26.25em;height:13.83em;" width="1040" height="549"/></p>
<p>Or, to give you a more concrete example of an actual calculation, take a look at the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-395 image-border" src="Images/4b19e3f6-cd57-4c7f-93e9-0ea1b7d8ec5c.png" style="width:19.67em;height:11.50em;" width="935" height="549"/></p>
<div class="packt_infobox">The operation of combining numbers by multiplying each term by a constant and adding the results is omnipresent in machine learning and statistics. It is called a linear combination of two vectors. You can think of a vector as a fixed set of numbers. In our example, the first vector would be a data row and the second vector would contain the weights for each data attribute.</div>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building neural networks</h1>
                </header>
            
            <article>
                
<p>We are ready to build our first neural network. Let's start with an example: our company is struggling with customer retention. We know a lot about our customers, and we can create an offer that would make them want to stay. The problem is, we cannot identify which customers will churn. So, our boss, Jane, asks us to build a churn prediction model. This model will take customer data and predict the probability of churning in the next month. With this probability estimate, Jane can decide if she needs to create a personalized marketing offer for this client.</p>
<p>We have decided to use a neural network to solve this churn prediction problem. Our network will comprise multiple layers of neurons. <span>Neurons in each layer will be connected to neurons in the next layer</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-396 image-border" src="Images/f6042a5e-ca74-4211-8ab6-71fc811bb9e3.png" style="width:34.92em;height:9.67em;" width="1409" height="392"/></p>
<p class="mce-root"/>
<p>That is a lot of arrows, isn't it? A connection between two neurons means that a neuron will pass its output to the next neuron. If a neuron receives multiple inputs, they are all summed up. This type of network is called a <strong>fully connected neural network</strong> (<strong>FCNN</strong>). </p>
<p>We see how we can make predictions using neural networks, but how can we learn what predictions to make? If you look closer, a neural network is nothing more than a large function with lots of weights. The model prediction is determined by using weights and information incoming through the neuron inputs. Thus, to have an accurate neural network, you must set the right weights. We already know that we can use mathematical optimization and statistics to minimize prediction error by changing the parameters of a function. A neural network is nothing more than a large and complex mathematical function with variable weights. Therefore, we can use MLE and gradient descent to do the optimization. I will give the formal names of each stage in bold, followed by intuitive explanations of each stage:</p>
<ol>
<li><strong>Network initialization</strong>: At first, we can initialize our weights with random values.</li>
<li><strong>Forward pass</strong>: We can take an example from our training dataset and make a prediction using our current set of weights.</li>
<li><strong>Loss function calculation</strong>: We measure the difference between our prediction and ground truth. We want to make this difference as closely to 0 as possible. That is, we want to minimize the loss function.</li>
<li><strong>Backward pass</strong>: We can use an optimization algorithm to adjust the weights so that the prediction will be more accurate. A special algorithm called backpropagation can calculate updates to each layer of neurons, going from the last layer to the first.</li>
<li><em>Steps 1</em> to <em>4</em> are repeated until the desired accuracy level is achieved, or until the network stops learning:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="Images/831809c3-879a-425f-817f-e9794f8d8d9a.png" width="1698" height="520"/></p>
<p>Backpropagation is the most widely used learning algorithm for training neural networks. It takes a prediction error and calculates how much we should change each weight in the network to make predictions closer to the ground truth. The name backpropagation comes from the specific way of how the algorithm updates the weights: it starts from the last layer, propagating changes to every neuron until it reaches the network input. When inputs go through the network to calculate an output prediction, we call it a forward pass. When we change the weights by propagating the error, we call it a backward pass.</p>
<p>Nowadays, there are many different types of building blocks that you can use to compose a neural network. Some specific neuron types work better with image data, while others can utilize the sequential nature of text. Many specialized layers were invented to improve the speed of training and fight overfitting. A specific composition of layers in a neural network devoted to solving a specific task is called a neural network architecture. All neural network architectures, no matter how complex and deep, still conform to basic laws of backpropagation. Next, we will explore the domain-specific applications of deep learning.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to computer vision</h1>
                </header>
            
            <article>
                
<p>First, we will look at computer vision. Let's start with an example. Our client, Joe, likes animals. He is the happy owner of six cats and three dogs. Being a happy owner, he also likes to take pictures of his pets. Large photo archives have accumulated on his computer over the years. Joe has decided that he needs to bring order into his dreaded photo folder, containing 50,000 pet photos. To help Joe, we have decided to create a neural network that takes an image and decides whether a cat or dog is present on the photo. The following diagram shows how a neural network classifier works with a cat photo:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-398 image-border" src="Images/9b1832d8-40c9-4ec3-9fc2-139563cc4f82.png" style="width:47.00em;height:13.50em;" width="2742" height="787"/></p>
<p>At first, we transform an image into three tables of numbers, one each for the red, green and blue channel of every pixel. If we try to use a plain FCNN as before, we will see unimpressive results. Deep neural networks shine at computer vision tasks because of a specific neuron type called a convolutional filter or a convolution. <strong>Convolutional neural networks</strong> (<strong>CNNs</strong>) were invented by a French machine learning researcher Yann LeCun. In CNNs, a single neuron can look at a small patch of an image, say, 16x16 pixels, instead of taking the entire set of pixels as an input. This neuron can go through each 16x16 region of an image, detecting some feature of an image it had learned through backpropagation. Then, this neuron can pass information to further layers. In the following illustration, you can see a single convolutional neuron going through small image patches and trying to detect a fur-like pattern:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-399 image-border" src="Images/f2936bd0-cc7e-4adb-b9ed-c5b1c69afbd5.png" style="width:43.58em;height:35.67em;" width="712" height="583"/></p>
<p>The remarkable achievement of CNNs is that a single neuron can reuse a small number of weights and still cover the entire image. This feature makes CNNs a lot faster and lighter than regular neural networks. The idea found implementation only in the 2010s, when a CNN surpassed all other computer vision methods in an ImageNet competition, where an algorithm had to learn to classify photos between 21,000 possible categories. The development of CNNs took so long because we lacked the computational capabilities to train deep neural networks with a large number of parameters on big datasets. To achieve good accuracy, CNNs require significant amounts of data. For example, the ImageNet competition includes 1,200,000 training images.</p>
<p>At first layers, CNNs tend to detect simple patterns, such as edges and contours in an image. As the layer depth progresses, convolutional filters become more complex, detecting features such as eyes, noses, and so on.</p>
<p>In the following visualizations, you can see an example visualization of convolutional filters at different layers of the neural network:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-400 image-border" src="Images/2a479028-5b00-4e7d-a173-4cc0daff5cdf.png" style="width:32.50em;height:32.25em;" width="887" height="881"/></p>
<p class="mce-root"/>
<p>Many neurons learn to recognize simple patterns that are useful for any computer vision task. This observation leads us to a very important idea: a neural network that was trained to perform one task well can be retrained to perform another task. Moreover, you will need much less training data for the second task, as the network has already learned many useful features from the previous training dataset. In particular, if you want to train a CNN classifier for two classes from scratch, you will need to label tens of thousands of images to reach a good performance level. However, if you use a network that was pretrained on ImageNet, you will probably get good results with only 1,000 to 2,000 images. This approach is called transfer learning. Transfer learning is not limited to computer vision tasks. In recent years, researchers made significant progress in using it for other domains: natural language processing, reinforcement learning, and sound processing.</p>
<p>Now that you have an understanding of how deep CNNs work, we will proceed to the language domain, where deep learning has changed everything.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to natural language processing</h1>
                </header>
            
            <article>
                
<p>Before the deep learning revolution, <strong>natural language processing</strong> (<strong>NLP</strong>) systems were almost fully rule based. Linguists created intricate parsing rules and tried to define our language's grammar to automate tasks such as part of speech tagging or named entity recognition. Human-level translation between different languages and free-form question answering were in the domain of science fiction. NLP systems were hard to maintain and took a long time to develop.</p>
<p>As with computer vision, deep learning took the NLP world by storm. Deep-learning-based NLP algorithms successfully perform near-human-level translation between different languages, can measure the emotional sentiment of a text, can learn to retrieve information from a text, and can generate answers on free-form questions. Another great benefit of deep learning is a unified approach. A single part-of-speech tagging model architecture will work for French, English, Russian, German, and other languages. You will need training data for all those languages, but the underlying model will be the same. With deep learning, we need not try to hardcode the rules of our ambiguous language. While many tasks, such as long-form writing and human-level dialogue, are yet unconquerable for deep learning, NLP algorithms are a great help in business and daily life.</p>
<p>For NLP deep learning, everything began with an idea: the meaning of a word is defined by its neighbors. That is, to learn a language and the meaning of words, all you need is to understand the context for each word in the text. This idea may seem to be too simple to be true. To check its validity, we can create a neural network that will predict a word by receiving surrounding words as an input. To create a training dataset, we may use any text in any language.</p>
<p class="mce-root"/>
<p>If we take a context window of two words, then we can generate the following training samples for this sentence:</p>
<p style="padding-left: 120px" class="CDPAlignLeft CDPAlign">If, we, take, a <span>→</span> will</p>
<p style="padding-left: 120px" class="CDPAlignLeft CDPAlign">We, can, following, training <span>→</span> generate</p>
<p style="padding-left: 120px" class="CDPAlignLeft CDPAlign">Following, training, for, this → samples</p>
<p style="padding-left: 120px" class="CDPAlignLeft CDPAlign">And so on…</p>
<p>Next, we need to come up with a way to convert all words to numbers, because neural networks only understand numbers. One approach would be to take all unique words in a text and assign them to a number:</p>
<p style="padding-left: 120px">Following → 0</p>
<p style="padding-left: 120px">Training <span>→</span> 1</p>
<p style="padding-left: 120px">Samples <span>→</span> 2</p>
<p style="padding-left: 120px">For <span>→</span> 3</p>
<p style="padding-left: 120px">This <span>→</span> 4</p>
<p style="padding-left: 120px">…</p>
<p>Then, we represent each word by a set of weights inside a neural network. In particular, we start with two random numbers between 0 and 1 for each word.</p>
<p>We place all the numbers into a table as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Word identifier</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Word vector</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.63, 0.26</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.52, 0.51</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>2</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.72, 0.16</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.28, 0.93</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.27, 0.71</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>…</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>…</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>N</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.37, 0.34</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now we have a way to convert every word in the text into a pair of numbers. We will take all numbers that we have generated as weights for our neural network. It will get four words as input, convert them into eight numbers, and use them to predict the identifier for the word in the middle.</p>
<p>For example, for the training sample <strong>Following</strong>, <strong>Training</strong>, <strong>For</strong>, <strong>This</strong> → <strong>Samples</strong>:</p>
<p style="padding-left: 120px">Input:</p>
<p style="padding-left: 150px">Following <span>→</span> 0 <span>→</span> 0.63, 0.26</p>
<p style="padding-left: 150px">Training <span>→</span> 1 <span>→</span> 0.52, 0.51</p>
<p style="padding-left: 150px">For <span>→</span> 3 <span>→</span> 0.28, 0.93</p>
<p style="padding-left: 150px">This <span>→</span> 4 <span>→</span> 0.27, 0.71</p>
<p style="padding-left: 120px">Output:</p>
<p style="padding-left: 150px">2 <span>→</span> Samples</p>
<p>We call each pair of numbers associated with a word, a word vector. Our neural network will output a vector of probabilities from zero to one. The length of this vector will match the total number of unique words in our dataset. Then, the number with the largest probability will represent the word that is the most likely completion of our input according to the model.</p>
<p>In this setup, we can apply the backpropagation algorithm to adjust word vectors until the model matches right words to their contexts. In our example, you can imagine each word lives on a coordinate grid. The elements of the word vector could represent <em>X</em> and <em>Y</em> coordinates. If you think about words in this geometric fashion, you may conclude that you can add or subtract word vectors to get another word vector. In the real world, such vectors contain not two but 100 to 300 elements, but the intuition remains the same. After many training iterations, you will see remarkable results.</p>
<p>Try to calculate the following using word vectors:</p>
<p class="CDPAlignCenter CDPAlign">King - Man + Woman = ?</p>
<p>You will get a vector for the word Queen. By learning to put words into their surrounding contexts, the model learns how different words relate to each other.</p>
<p>The model we have built is called Word2Vec. We can train Word2Vec models in two ways:</p>
<ul>
<li>Predict a word by using its surrounding context. This setup is called a <strong>continuous bag of words</strong> (<strong>CBOW</strong>).</li>
<li>Predict the surrounding context by word. This setup is called <strong>Skipgram</strong>.</li>
</ul>
<p>The two approaches do not differ in anything, except model input and output specifications.</p>
<p>Word vectors are also referred to as word embeddings. Embeddings contain much more information about words than simple numeric identifiers, and NLP models can use them to achieve better accuracy. For example, you can train a sentiments classification model by following these steps:</p>
<ol>
<li>Create a training dataset that contains user reviews and their sentiment, labeled 0 as negative and 1 as positive.</li>
<li>Embed the user reviews into sets of word vectors.</li>
<li>Train deep learning classifier using this dataset.</li>
</ol>
<div class="packt_infobox">Current state-of-the-art models rarely use word embeddings created by training a separate model. Newer architectures allow learning task-specific word embeddings on the fly, without the need to use Word2Vec. Nonetheless, we have covered word embeddings in this chapter as they give us an idea of how computers can understand the meaning of a text. While modern models are more complex and robust, this idea remains intact.<br/>
<br/>
The concept of embeddings originated in NLP, but now it has found applications in recommended systems, face recognition, classification problems with lots of categorical data, and many other domains.</div>
<p>To train a classifier that uses word embeddings, you can use a CNN. In a CNN, each neuron progressively scans the input text in windows of words. Convolutional neurons learn weights that combine word vectors of nearby words into more compact representations that are used by the output layer to estimate sentence sentiment.</p>
<p>You can see how a single convolutional neuron works on a single sentence in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-401 image-border" src="Images/646d01d6-47ca-4bfb-90a4-4fe58f5377c2.png" style="width:40.92em;height:10.92em;" width="668" height="178"/></p>
<p>CNNs process text in a fixed window, which is an oversimplification. In reality, a word at the beginning of the sentence can affect its ending, and vice versa. Another architecture called <strong>recurrent neural networks</strong> (<strong>RNNs</strong>) can process sequences of any length, passing information from start to end. This is possible because all recurrent neurons are connected to themselves:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-402 image-border" src="Images/22883415-c01e-4a97-a129-683dfaea54c0.png" style="width:17.75em;height:8.17em;" width="295" height="135"/></p>
<p>Self-connection allows a neuron to cycle through its input, pulling its internal state through each iteration:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="Images/4d0fd715-8483-4a27-bf57-9285aa83c472.png" style="width:32.75em;height:22.58em;" width="523" height="361"/></p>
<p class="mce-root"/>
<p>The preceding screenshot depicts a single recurrent neuron as it unfolds. With each new word, a recurrent neuron changes its previous state. When the final word is processed, it returns its internal state as the output. This is the most basic recurrent architecture. Neural networks that are used in practice have a more complex internal structure, but the idea of recurrent connections holds. When speaking about recurrent networks, you will probably hear about <strong>long short-term memory networks</strong> (<strong>LSTMs</strong>). While they differ in the details, the flow of thought is the same for both RNNs and LSTMs.</p>


            </article>

            
        </section>
    </div>
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we uncovered the inner workings of machine learning and deep learning. We learned the main concepts of mathematical optimization and statistics. We connected them to machine learning and, finally, learned how machines learn and how we can use optimization algorithms to define learning. Lastly, we covered popular machine learning and deep learning algorithms, including linear regression, tree ensembles, CNNs, word embeddings, and recurrent neural networks. This chapter concludes our introduction to data science.</p>
<p>In the next chapter, we will learn how to build and sustain a data science team capable of delivering complex cross-functional projects.</p>


            </article>

            
        </section>
    </div></body></html>