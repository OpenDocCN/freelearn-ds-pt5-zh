<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Stack Overflow Project</h1></div></div></div><p>This is the first of two full, chapter-length projects where we will put everything we have learned about data cleaning into practice. We can think of each project as a dinner party where we show off our best skills from our data science kitchen. To host a successful dinner party, we should of course have our menu and guest list planned in advance. However, the mark of a true expert is how we react when things do not go exactly according to plan. We have all had that moment when we forget to buy an important ingredient, despite our carefully prepared recipes and shopping lists. Will we be able to adjust our plan to meet the new challenges we meet along the way?</p><p>In this chapter, we will tackle some data cleaning using the publicly-released Stack Overflow <a id="id558" class="indexterm"/>database dump. Stack Overflow is part of the Stack Exchange family of question-and-answer websites. On these sites, writing good questions and answers can earn a user points and badges that accumulate over time. To practice our data cleaning skills, we will use the same six-step method we introduced back in <a class="link" title="Chapter 1. Why Do You Need Clean Data?" href="part0014.xhtml#aid-DB7S1">Chapter 1</a>, <em>Why Do You Need Clean Data?</em>.</p><div><ul class="itemizedlist"><li class="listitem">Decide what kind of problem we are trying to solve — why are we looking at this data?</li><li class="listitem">Collect and store our data, which consists of downloading and extracting the data dump provided by Stack Overflow, creating a MySQL database to hold the data, and writing scripts to import the data into the MySQL database. Because the Stack Overflow dataset is so massive, we will also create some smaller test tables, filled with randomly selected rows.</li><li class="listitem">Perform some trial cleaning tasks on the test tables before attempting to clean the entire dataset.</li><li class="listitem">Analyze the data. Do we need to perform any calculations? Should we write some aggregate functions to count or sum the data? Do we need to transform the data in some way?</li><li class="listitem">Provide visualizations of the data, if possible.</li><li class="listitem">Resolve the problem we set out to investigate. Did our process work? Were we successful?</li></ul></div><p>That is a lot of work, but the more we prepare in advance and the earlier we start, the more likely we will be able to call our data science dinner party a success.</p><div><div><div><div><h1 class="title"><a id="ch09lvl1sec53"/>Step one – posing a question about Stack Overflow</h1></div></div></div><p>To <a id="id559" class="indexterm"/>start our project, we need to pose a reasonably interesting question that requires some simple data analysis to answer. Where should we begin? First, let's review what we know about Stack Overflow. We know that it is a question-and-answer website for programmers, and we can assume that programmers probably use a lot of source code, error logs, and configuration files in the questions they are asking and answering. Furthermore, we know that sometimes posting these kinds of long text<a id="id560" class="indexterm"/> dumps on a web-based platform like Stack Overflow can be awkward because of line lengths, formatting, and other readability issues.</p><p>Seeing so many questions and answers with frustratingly large amounts of text made me wonder whether programmers on Stack Overflow ever link to their code or log files through an <a id="id561" class="indexterm"/>external <strong>paste site</strong> such as Pastebin or JSFiddle, for example <a class="ulink" href="http://www.Pastebin.com">http://www.Pastebin.com</a> is a website where you can paste in large amounts of text, such as source code or a log file, and the site gives you back a short URL that you can share with others. Most paste sites also allow for source code syntax highlighting, which Stack Overflow does not do by default.</p><p>Using paste sites is very common on IRC and e-mail, but what about on Stack Overflow? On one hand, just like in IRC or e-mail, providing a link could make a question or an answer shorter, and therefore the rest of the question is much easier to read. But on the other hand, depending on the paste site being used, the URL is not guaranteed to be functional forever. This<a id="id562" class="indexterm"/> means that a question or an answer could lose its value over time due to <strong>link rot</strong>.</p><p>Tools like JSFiddle complicate this issue in one additional way. On an <strong>interactive paste</strong> site like <a id="id563" class="indexterm"/>JSFiddle, not only can you paste in your source code and get a URL for it, but you can also allow others to edit and run the code right in the browser. This could be extremely helpful in a question-and-answer scenario on Stack Overflow, especially in a browser-based language like JavaScript. However, the issue of link rot still exists. Additionally, JSFiddle is a little trickier to use for beginners than a simple code dump site like Pastebin.</p><div><img src="img/image00300.jpeg" alt="Step one – posing a question about Stack Overflow"/><div><p>JSFiddle has four windows, one each for HTML, CSS, JavaScript, and the result.</p></div></div><p style="clear:both; height: 1em;"> </p><div><h3 class="title"><a id="note20"/>Note</h3><p>In the <a id="id564" class="indexterm"/>community discussion area for Stack Overflow, there has been quite a bit of debate about whether paste sites should be used at all, and what the policy should be for questions or answers that include paste site links and no actual code. In general, even though people tend to agree that a paste site can be useful, they also recognize that it is important to protect the longevity and utility of Stack Overflow itself. The community has decided that posting questions or answers with only links and no code should be avoided. A good place to start if you want to recap that discussion is this link: <a class="ulink" href="http://meta.stackexchange.com/questions/149890/">http://meta.stackexchange.com/questions/149890/</a>.</p></div><p>For our purposes here, we do not need to choose sides in this debate. Instead, we can just ask some simple data-driven questions like:</p><div><ol class="orderedlist arabic"><li class="listitem">How frequently do people use tools like Pastebin and JSFiddle (and other similar paste sites) on Stack Overflow?</li><li class="listitem">Do they use paste sites more in questions or in answers?</li><li class="listitem">Do posts that reference a paste site URL tend to also include source code; if so, how much?</li></ol><div></div><p>We can use these questions for our motivation as we collect, store, and clean our Stack Overflow data. Even if it turns out that some of these questions are too hard or impossible to answer, remembering what our overall purpose is will help direct the type of cleaning we need to do. Keeping our questions in the forefront of our minds will stop us from getting too far off track or performing tasks that will turn out to be pointless or a waste of time.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec54"/>Step two – collecting and storing the Stack Overflow data</h1></div></div></div><p>At <a id="id565" class="indexterm"/>the time of writing, Stack Exchange provides <a id="id566" class="indexterm"/>the data for their entire family of websites—including Stack Overflow—as XML files free for anyone to download. In this section, we will download the Stack Overflow files, and import the data into a database on our MySQL server. Finally, we will create a few smaller versions of these tables for testing purposes.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec90"/>Downloading the Stack Overflow data dump</h2></div></div></div><p>All the<a id="id567" class="indexterm"/> data available from Stack Exchange can be downloaded at the Internet Archive. The September 2014 dump is the latest one<a id="id568" class="indexterm"/> available at the time of writing. Each Stack Exchange site has one or more files for it, each of which is linked to this details page: <a class="ulink" href="https://archive.org/details/stackexchange">https://archive.org/details/stackexchange</a>.</p><p>We are only interested in the eight Stack Overflow files that appear alphabetically as shown in the following list:</p><div><img src="img/image00301.jpeg" alt="Downloading the Stack Overflow data dump"/><div><p>Archive.org listing showing the eight Stack Overflow files we are interested in.</p></div></div><p style="clear:both; height: 1em;"> </p><p>For each file in the list, right-click the link and direct your browser to save the file to disk.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec91"/>Unarchiving the files</h2></div></div></div><p>Notice<a id="id569" class="indexterm"/> that each file has a <code class="literal">.7z</code> extension. This is a compressed archive format. It can be uncompressed and unarchived using the matching 7-Zip software, or another compatible software package. 7-Zip was not one of the more common file archivers that we discussed in <a class="link" title="Chapter 2. Fundamentals – Formats, Types, and Encodings" href="part0020.xhtml#aid-J2B82">Chapter 2</a>, <em>Fundamentals – Formats, Types, and Encodings</em> and you may not already have a compatible unarchiver installed on <a id="id570" class="indexterm"/>your computer, so we can consider this our first small wrinkle that we need to work around. Try double-clicking on the file to open it, but if you have no installed software associated with the <code class="literal">.7z</code> extension, you will need to install an appropriate 7-Zip unarchiver.</p><div><ul class="itemizedlist"><li class="listitem">For Windows, you<a id="id571" class="indexterm"/> can download the 7-Zip software from their website: <a class="ulink" href="http://www.7-zip.org">http://www.7-zip.org</a></li><li class="listitem">For Mac<a id="id572" class="indexterm"/> OS X, you can download and install The Unarchiver, a no-cost utility available at <a class="ulink" href="http://unarchiver.c3.cx">http://unarchiver.c3.cx</a></li></ul></div><p>Once you have the software installed, unpack each file in turn. The uncompressed files are quite large, so make sure you have disk space that is large enough to hold them.</p><div><h3 class="title"><a id="tip26"/>Tip</h3><p>On my system right now, comparing the compressed to uncompressed file sizes shows that the uncompressed versions are about ten times larger than the compressed versions. These files also take several minutes each to unarchive, depending on the specifications of the system you are working on, so set aside time for this step.</p></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec92"/>Creating MySQL tables and loading data</h2></div></div></div><p>We<a id="id573" class="indexterm"/> now have eight <code class="literal">.xml</code> files, each of <a id="id574" class="indexterm"/>which will map to one table in the database we are about to build. To create the database and tables, we could either point and click our way through it using phpMyAdmin or some other graphical tool, or we can run the following simple SQL written by Georgios Gousios and available at <a class="ulink" href="https://gist.github.com/gousiosg/7600626">https://gist.github.com/gousiosg/7600626</a>. This code includes <code class="literal">CREATE</code> and <code class="literal">LOAD INFILE</code> statements for the first six tables, but since this script was written, the database dump has had two additional tables added to it.</p><p>To build the new table structure, we can run the <code class="literal">head</code> command in our Terminal window or shell in order to inspect the first few lines of this file. From Terminal, run it on the smallest of the XML files, <code class="literal">PostLinks.xml</code>, as follows:</p><div><pre class="programlisting"><strong>head PostLinks.xml</strong>
</pre></div><p>The first four lines from the results are shown here:</p><div><pre class="programlisting"><strong>&lt;?xml version="1.0" encoding="utf-8"?&gt;</strong>
<strong>&lt;postlinks&gt;</strong>
<strong>  &lt;row Id="19" CreationDate="2010-04-26T02:59:48.130" PostId="109" RelatedPostId="32412" LinkTypeId="1" /&gt;</strong>
<strong>  &lt;row Id="37" CreationDate="2010-04-26T02:59:48.600" PostId="1970" RelatedPostId="617600" LinkTypeId="1" /&gt;</strong>
</pre></div><p>Each row in our new database table should correspond to one of the XML <code class="literal">&lt;row&gt;</code> lines, and each attribute shown in the row line represents one column in the database table. We can perform<a id="id575" class="indexterm"/> the same <code class="literal">head</code> command<a id="id576" class="indexterm"/> on the <code class="literal">Tags.xml</code> file to see what its columns should be. The following SQL code will handle the <code class="literal">CREATE</code> statements and the <code class="literal">LOAD</code> statements for the two additional tables:</p><div><pre class="programlisting">CREATE TABLE post_links (
  Id INT NOT NULL PRIMARY KEY,
  CreationDate DATETIME DEFAULT NULL,
  PostId INT NOT NULL,
  RelatedPostId INT NOT NULL,
  LinkTypeId INT DEFAULT NULL
);

CREATE TABLE tags (
  Id INT NOT NULL PRIMARY KEY,
  TagName VARCHAR(50) DEFAULT NULL,
  Count INT DEFAULT NULL,
  ExcerptPostId INT DEFAULT NULL,
  WikiPostId INT DEFAULT NULL
);

LOAD XML LOCAL INFILE 'PostLinks.xml'
INTO TABLE post_links
ROWS IDENTIFIED BY '&lt;row&gt;';

LOAD XML LOCAL INFILE 'Tags.xml'
INTO TABLE tags
ROWS IDENTIFIED BY '&lt;row&gt;';</pre></div><div><h3 class="title"><a id="note21"/>Note</h3><p>Note that the <code class="literal">LOAD XML</code> syntax is slightly changed, so that we can keep our files locally. If your <code class="literal">.xml</code> files are on your local machine rather than on the database server itself, simply add the word <code class="literal">LOCAL</code> to the <code class="literal">LOAD XML</code> statements, as shown in the preceding code, and you can reference the full path to your file.</p><p>More<a id="id577" class="indexterm"/> information about the MySQL <code class="literal">LOAD XML</code> syntax is described in the MySQL documentation here: <a class="ulink" href="http://dev.mysql.com/doc/refman/5.5/en/load-xml.html">http://dev.mysql.com/doc/refman/5.5/en/load-xml.html</a>.</p></div><p>At this point, we have a fully functional MySQL database comprised of eight tables, each of which is filled with data. However, these tables are very large. There are over 190 million rows in only eight tables. One thing we will notice as we start to clean the data and prepare it for analysis, is that if we make a mistake on a very large table like <code class="literal">posts</code>, <code class="literal">comments</code>, <code class="literal">votes</code>, or <code class="literal">post_history</code>, rebuilding the table will take a long time. In the next step, we learn how to create test tables, so that we contain the damage if one of our programs <a id="id578" class="indexterm"/>or<a id="id579" class="indexterm"/> queries goes awry.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec93"/>Building test tables</h2></div></div></div><p>In this <a id="id580" class="indexterm"/>section, we will build eight smaller versions of our original tables, each randomly populated with data from the original tables.</p><p>Our first step is to re-run the <code class="literal">CREATE</code> statements, but this time prefix each table name with <code class="literal">test_</code>, as shown with one table here:</p><div><pre class="programlisting">DROP TABLE IF EXISTS test_post_links;
CREATE TABLE test_post_links (
  Id INT NOT NULL PRIMARY KEY,
  CreationDate INT,
  PostId INT,
  RelatedPostId INT,
  LinkTypeId INT
);</pre></div><p>Other than the addition of <code class="literal">test_</code> to the front of the table name, these eight test tables will be identical to the others we made earlier.</p><p>Next, we need to populate our new test tables with data. We could simply select the first 1,000 rows from each table and load those into our test tables. However, the downside of doing that is that the rows are in order based on when they were inserted into the Stack Overflow database, so we will not have a good sample of rows from different dates and time in our subset if we just ask for the first 1,000 rows. We would like the rows we select to be of a fairly random distribution. How can we select a set of rows randomly? We have not had to tackle this question before in this book, so here is another case where we have to be ready to try new things in order to have our data science dinner party go off without a hitch.</p><p>There are a few possibilities for selecting random rows, some of which are more efficient than others. Efficiency will be important to us in this project, since the tables we are working with are quite large. One thing that makes our random row selection a little trickier than expected, is that while our tables do have a numeric primary key as the <code class="literal">Id</code> column, these <code class="literal">Id</code> numbers are not sequential. There are many holes, for example, in the <code class="literal">post_links</code> table, the first few values in the <code class="literal">Id</code> column are 19, 37, 42, and 48.</p><p>Holes in the data are problematic because a simple randomizer operates like this:</p><div><ol class="orderedlist arabic"><li class="listitem">Construct a PHP script that asks for the lowest and highest <code class="literal">Id</code> values in the table, like this:<div><pre class="programlisting">SELECT min(Id) FROM post_links;
SELECT max(Id) FROM post_links;</pre></div></li><li class="listitem">Then, still in the script, generate some random number between the <code class="literal">min</code> and the <code class="literal">max</code> value, and request the row with that random value:<div><pre class="programlisting">SELECT * FROM post_links WHERE Id = [random value];</pre></div></li><li class="listitem">Repeat<a id="id581" class="indexterm"/> step 2 for as many rows as you need.</li></ol><div></div><p>Unfortunately, doing this in the Stack Overflow database tables, for example, on our <code class="literal">post_links</code> table, will result in many failed queries, since our data has so many holes in the <code class="literal">Id</code> column, for example, what if step 2 in the preceding example generated the number 38? There is no <code class="literal">Id</code> of 38 in our <code class="literal">post_links</code> table. This means we will need to detect this error and try again with a new random value.</p><div><h3 class="title"><a id="note22"/>Note</h3><p>At this point, someone who knows a little SQL — but not a lot — will usually suggest that we just ask MySQL to <code class="literal">ORDER BY rand()</code> on the column with the <code class="literal">Id</code> in it, and then perform a <code class="literal">LIMIT</code> command to skim off the number of records we want. The problem with this idea is that even if the column we are ordering is an indexed column, <code class="literal">ORDER BY rand()</code> has to read every row in order to assign a new random number to it. So, on a very large table, like the tables we have in the Stack Overflow database, this does not scale at all. We will be waiting way too long for an <code class="literal">ORDER BY rand()</code> query to finish. <code class="literal">ORDER BY rand()</code> is a tolerable solution for small tables, but not for the sizes we are working with here.</p></div><p>The following PHP script shows how our final random row selection process will work to build eight test tables, each with exactly 1,000 rows. Each table will be populated by row values that are selected as randomly as possible with as little effort as possible, and without us over-engineering this simple problem:</p><div><pre class="programlisting">&lt;?php //randomizer.php
// how many rows should be in each of the test tables?
$table_target_size = 1000;

// connect to db, set up query, run the query
$dbc = mysqli_connect('localhost','username','password','stackoverflow')
       or die('Error connecting to database!' . mysqli_error());
$dbc-&gt;set_charset("utf8");

$tables = array("badges",
    "comments",
    "posts",
    "post_history",
    "post_links",
    "tags",
    "users",
    "votes");

foreach ($tables as $table)
{
  echo "\n=== Now working on $table ===\n";
    $select_table_info = "SELECT count(Id) as c, min(Id) as mn, max(Id) as mx FROM $table";
    $table_info = mysqli_query($dbc, $select_table_info);
    $table_stuff = mysqli_fetch_object($table_info);
    $table_count = $table_stuff-&gt;c;
    $table_min = $table_stuff-&gt;mn;
    $table_max = $table_stuff-&gt;mx;

    // set up loop to grab a random row and insert into new table
    $i=0;
    while($i &lt; $table_target_size)
    {
        $r = rand($table_min, $table_max);
        echo "\nIteration $i: $r";
        $insert_rowx = "INSERT IGNORE INTO test_$table (SELECT * FROM $table WHERE Id = $r)";
        $current_row = mysqli_query($dbc, $insert_rowx);

        $select_current_count = "SELECT count(*) as rc FROM test_$table";
        $current_count= mysqli_query($dbc, $select_current_count);
        $row_count = mysqli_fetch_object($current_count)-&gt;rc;
        $i = $row_count;
    }
}
?&gt;</pre></div><p>After <a id="id582" class="indexterm"/>running that code, we can see we have a nice selection of eight test tables to work with if we need them. Testing with these smaller tables ensures that our cleaning exercises will go more smoothly and mistakes can be contained. If we find that we need more rows in our random tables, we can simply raise the <code class="literal">$table_target_size</code> command and run this again.</p><p>Building test tables is a great habit to get into, once you know how easy it is to create them in a simple and useful way.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec55"/>Step three – cleaning the data</h1></div></div></div><p>Remembering<a id="id583" class="indexterm"/> that our goal is to begin analyzing how frequently certain URLs are referenced in questions, answers, and comments, it makes sense to begin in the text of the Stack Overflow <code class="literal">posts</code> and <code class="literal">comments</code> tables. However, since those tables are so large, we will use the <code class="literal">test_posts</code> and <code class="literal">test_comments</code> tables that we just created instead. Then, once we are confident that the queries work perfectly, we can re-run them on the larger tables.</p><p>This cleaning task is very similar to the way we stored the URLs extracted from tweets in <a class="link" title="Chapter 7. RDBMS Cleaning Techniques" href="part0045.xhtml#aid-1AT9A1">Chapter 7</a>, <em>RDBMS Cleaning Techniques</em>. However, this project has its own set of specific rules:</p><div><ul class="itemizedlist"><li class="listitem">Since posts and comments are different entities to begin with, we should make separate tables for the URLs that come from posts (including questions and answers) and the URLs that come from comments.</li><li class="listitem">Each question, answer, or comment can have multiple URLs inside it. We should store all of the URLs, and we should also track the unique identifier for which post or comment that URL came from.</li><li class="listitem">Each question and answer can also have formatted source code in it. The <code class="literal">&lt;code&gt;</code> tag is used to delimit source code in the Stack Overflow posts. Separating code from posts will help us answer questions about the co-existence of paste site URLs and source code. How much code will typically accompany such a link, if any?<div><h3 class="title"><a id="note23"/>Note</h3><p>Technically, posts <em>can</em> be created without the <code class="literal">&lt;code&gt;</code> tags, but usually someone will quickly edit the errant post to include these useful tags, and will get the Stack Overflow points for doing so. For brevity's sake, in this project, we will assume that code is included in the <code class="literal">&lt;code&gt;</code> tags.</p></div></li><li class="listitem">According to<a id="id584" class="indexterm"/> the Stack Overflow database dump documentation (available at <a class="ulink" href="http://meta.stackexchange.com/questions/2677/">http://meta.stackexchange.com/questions/2677/</a>), there are actually eight types of posts, of which questions and answers are just two types. So, we will need to limit our queries to posts that have <code class="literal">postTypeId=1</code> for questions and <code class="literal">postTypeId=2</code> for answers.</li><li class="listitem">To ensure that we are only extracting URLs from comments made to questions or answers, and not other types of posts, we will need to do a join back to the posts table and limit our results to <code class="literal">postTypeId=1</code> or <code class="literal">postTypeId=2</code>.</li></ul></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec94"/>Creating the new tables</h2></div></div></div><p>The SQL query to <a id="id585" class="indexterm"/>create the database tables we need to store these URLs is as follows:</p><div><pre class="programlisting">CREATE TABLE clean_comments_urls (
  id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
  commentId INT NOT NULL,
  url VARCHAR(255) NOT NULL
) ENGINE=MyISAM  DEFAULT CHARSET=utf8;

CREATE TABLE IF NOT EXISTS clean_posts_urls (
  id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
  postId INT NOT NULL,
  url VARCHAR(255) NOT NULL
) ENGINE=MyISAM  DEFAULT CHARSET=utf8;</pre></div><p>We also need to create a table that can hold the code we stripped out of the posts:</p><div><pre class="programlisting">CREATE TABLE clean_posts_code (
  id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
  postId INT NOT NULL,
  code TEXT NOT NULL
) ENGINE=MyISAM DEFAULT CHARSET=utf8;</pre></div><p>At this point, we have three new tables that will store our cleaned URLs and cleaned source code. In the next section, we will extract URLs and code and fill up these new tables.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec95"/>Extracting URLs and populating the new tables</h2></div></div></div><p>We <a id="id586" class="indexterm"/>can <a id="id587" class="indexterm"/>modify the scripts we wrote earlier in <a class="link" title="Chapter 7. RDBMS Cleaning Techniques" href="part0045.xhtml#aid-1AT9A1">Chapter 7</a>, <em>RDBMS Cleaning Techniques</em>, to extract URLs in this new Stack Overflow environment as follows:</p><div><pre class="programlisting">&lt;?php // urlExtractor.php
// connect to db
$dbc = mysqli_connect('localhost', 'username', 'password', 'stackoverflow')
    or die('Error connecting to database!' . mysqli_error());
$dbc-&gt;set_charset("utf8");

// pull out the text for posts with
// postTypeId=1 (questions)
// or postTypeId=2 (answers)
$post_query = "SELECT Id, Body
    FROM test_posts
    WHERE postTypeId=1 OR postTypeId=2";

$comment_query = "SELECT tc.Id, tc.Text
    FROM test_comments tc
    INNER JOIN posts p ON tc.postId = p.Id
    WHERE p.postTypeId=1 OR p.postTypeId=2";

$post_result = mysqli_query($dbc, $post_query);
// die if the query failed
if (!$post_result)
    die ("post SELECT failed! [$post_query]" .  mysqli_error());

// pull out the URLS, if any
$urls = array();
$pattern  = '#\b(([\w]+://?|www[.])[^\s()&lt;&gt;]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#';

while($row = mysqli_fetch_array($post_result))
{
    echo "\nworking on post: " . $row["id"];
    if (preg_match_all(
        $pattern,
        $row["Body"],
        $urls
    ))
    {
        foreach ($urls[0] as $url)
        {
          $url = mysqli_escape_string($dbc, $url);
            echo "\n----url: ".$url;
            $post_insert = "INSERT INTO clean_posts_urls (id, postid, url)
                VALUES (NULL," . $row["Id"] . ",'$url')";
            echo "\n$post_insert";
            $post_insert_result = mysqli_query($dbc, $post_insert);
        }
    }
}

$comment_result = mysqli_query($dbc, $comment_query);
// die if the query failed
if (!$comment_result)
    die ("comment SELECT failed! [$comment_query]" .  mysqli_error());

while($row = mysqli_fetch_array($comment_result))
{
    echo "\nworking on comment: " . $row["id"];
    if (preg_match_all(
        $pattern,
        $row["Text"],
        $urls
    ))
    {
        foreach ($urls[0] as $url)
        {
            echo "\n----url: ".$url;
            $comment_insert = "INSERT INTO clean_comments_urls (id, commentid, url)
                VALUES (NULL," . $row["Id"] . ",'$url')";
            echo "\n$comment_insert";
            $comment_insert_result = mysqli_query($dbc, $comment_insert);
        }
    }
}
?&gt;</pre></div><p>We <a id="id588" class="indexterm"/>now <a id="id589" class="indexterm"/>have fully populated <code class="literal">clean_post_urls</code> and <code class="literal">clean_comment_urls</code> tables. For my randomly filled test tables, running this script only yields around 100 comment URLs and 700 post URLs. Still, that is enough to test out our ideas before running them on the full dataset.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec96"/>Extracting code and populating new tables</h2></div></div></div><p>To extract<a id="id590" class="indexterm"/> the text embedded in <code class="literal">&lt;code&gt;</code> tags and populate <a id="id591" class="indexterm"/>our new <code class="literal">clean_posts_code</code> table, we can run the following script. It is similar to the URL extractor, except that it does not need to search comments, because those do not have code delimited with a <code class="literal">&lt;code&gt;</code> tag.</p><p>In my version of the randomly selected test table, the initial <code class="literal">SELECT</code> yields about 800 rows out of 1,000 total rows in the <code class="literal">test_post</code> table. However, each post can have multiple code snippets in it, so the final table ends up having over 2,000 rows in it. The following PHP code extracts the text embedded in the <code class="literal">&lt;code&gt;</code> tag:</p><div><pre class="programlisting">&lt;?php // codeExtractor.php
// connect to db
$dbc = mysqli_connect('localhost', 'username, 'password', 'stackoverflow')
    or die('Error connecting to database!' . mysqli_error());
$dbc-&gt;set_charset("utf8");

// pull out the text for posts with
// postTypeId=1 (questions)
// or postTypeId=2 (answers)
$code_query = "SELECT Id, Body
    FROM test_posts
    WHERE postTypeId=1 OR postTypeId=2
    AND Body LIKE '%&lt;code&gt;%'";

$code_result = mysqli_query($dbc, $code_query);
// die if the query failed
if (!$code_result)
    die ("SELECT failed! [$code_query]" .  mysqli_error());

// pull out the code snippets from each post
$codesnippets = array();
$pattern  = '/&lt;code&gt;(.*?)&lt;\/code&gt;/';

while($row = mysqli_fetch_array($code_result))
{
    echo "\nworking on post: " . $row["Id"];
    if (preg_match_all(
        $pattern,
        $row["Body"],
        $codesnippets
    ))
    {
      $i=0;
        foreach ($codesnippets[0] as $code)
        {
          $code = mysqli_escape_string($dbc, $code);
            $code_insert = "INSERT INTO clean_posts_code (id, postid, code)
                VALUES (NULL," . $row["Id"] . ",'$code')";
            $code_insert_result = mysqli_query($dbc, $code_insert);
            if (!$code_insert_result)
                die ("INSERT failed! [$code_insert]" .  mysqli_error());
            $i++;
        }
        if($i&gt;0)
        {
          echo "\n   Found $i snippets";
        }
    }
}
?&gt;</pre></div><p>We <a id="id592" class="indexterm"/>now have a list of all the code that has been printed in <a id="id593" class="indexterm"/>each post, and we have stored that in the <code class="literal">clean_post_code</code> table.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec56"/>Step four – analyzing the data</h1></div></div></div><p>In this<a id="id594" class="indexterm"/> section, we write some code to answer our three questions from the beginning of this chapter. We were interested in finding:</p><div><ul class="itemizedlist"><li class="listitem">The counts of different paste sites mentioned by URLs in posts and comments</li><li class="listitem">The counts of paste site URLs in questions compared to answers</li><li class="listitem">Statistics about <code class="literal">&lt;code&gt;</code> prevalence in posts with a paste site URL</li></ul></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec97"/>Which paste sites are most popular?</h2></div></div></div><p>To answer<a id="id595" class="indexterm"/> this first question, we will generate a JSON representation of the paste site URLs and counts using the <code class="literal">clean_posts_urls</code> and <code class="literal">clean_comments_urls</code> tables. This simple analysis will help us find out which pastebin websites are popular in this Stack Overflow data dump. The following PHP queries the database for the paste sites we have prelisted in the <code class="literal">$pastebins</code> array and then performs a count of the matching URLs from the posts and comments. It uses the test tables, so the numbers are much smaller than they would be if we used the real tables:</p><div><pre class="programlisting">&lt;?php // q1.php
// connect to db
$dbc = mysqli_connect('localhost', 'username', 'password', 'stackoverflow')
    or die('Error connecting to database!' . mysqli_error());
$dbc-&gt;set_charset("utf8");

// these are the web urls we want to look for and count
$pastebins = array("pastebin",
    "jsfiddle",
    "gists",
    "jsbin",
    "dpaste",
    "pastie");
$pastebin_counts = array();

foreach ($pastebins as $pastebin)
{
    $url_query = "SELECT count(id) AS cp,
          (SELECT count(id)
          FROM clean_comments_urls
          WHERE url LIKE '%$pastebin%') AS cc
        FROM clean_posts_urls
        WHERE url LIKE '%$pastebin%'";
    $query = mysqli_query($dbc, $url_query);
    if (!$query)
        die ("SELECT failed! [$url_query]" .  mysqli_error());
    $result = mysqli_fetch_object($query);
    $countp = $result-&gt;cp;
    $countc = $result-&gt;cc;
    $sum = $countp + $countc;

    array_push($pastebin_counts, array('bin' =&gt; $pastebin,
                                        'count' =&gt; $sum));
}
// sort the final list before json encoding it
// put them in order by count, high to low
foreach ($pastebin_counts as $key =&gt; $row)
{
    $first[$key]  = $row['bin'];
    $second[$key] = $row['count'];
}

array_multisort($second, SORT_DESC, $pastebin_counts);
echo json_encode($pastebin_counts);
?&gt;</pre></div><p>We can<a id="id596" class="indexterm"/> view the JSON output from this script when run against the test tables by looking at the output of the script. My random rows produced the following counts:</p><div><pre class="programlisting">[{"bin":"jsfiddle","count":44},{"bin":"jsbin","count":4},{"bin":"pastebin","count":3},{"bin":"dpaste","count":0},{"bin":"gists","count":0},{"bin":"pastie","count":0}]</pre></div><div><h3 class="title"><a id="note24"/>Note</h3><p>Remember that your values may be different since you have a different set of randomly selected URLs.</p></div><p>When we move to the <em>Step 5 – visualizing the data</em> section of this chapter, we will use this JSON code to build a bar graph. But first, let's answer the other two questions we posed earlier.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec98"/>Which paste sites are popular in questions and which are popular in answers?</h2></div></div></div><p>Our <a id="id597" class="indexterm"/>second question was whether the pastebin<a id="id598" class="indexterm"/> URLs are more prevalent in question posts or answer posts. To start to develop this answer, we will run a series of SQL queries. The first query simply asks how many posts are in the <code class="literal">clean_posts_urls</code> table of each type, both questions and answers:</p><div><pre class="programlisting">SELECT tp.postTypeId, COUNT(cpu.id)
FROM test_posts tp
INNER JOIN clean_posts_urls cpu ON tp.Id = cpu.postid
GROUP BY 1;</pre></div><p>The results show that in my randomly selected test set, I have 237 questions and 440 answers:</p><div><img src="img/image00302.jpeg" alt="Which paste sites are popular in questions and which are popular in answers?"/><div><p>phpMyAdmin shows the count of question URLs and answer URLs.</p></div></div><p style="clear:both; height: 1em;"> </p><p>Now, we would want to know the answer to this question: of those 677 URLs, divided by questions and answers, how many reference specifically one of the six pastebin URLs? We can run the following SQL code to find out:</p><div><pre class="programlisting">SELECT  tp.postTypeId, count(cpu.id)
FROM test_posts tp
INNER JOIN clean_posts_urls cpu ON tp.Id = cpu.postId
WHERE cpu.url LIKE '%jsfiddle%'
OR cpu.url LIKE '%jsbin%'
OR cpu.url LIKE '%pastebin%'
OR cpu.url LIKE '%dpaste%'
OR cpu.url LIKE '%gist%'
OR cpu.url LIKE '%pastie%'
GROUP BY 1;</pre></div><p>The results look like the following table. A total of 18 questions reference one of the paste sites, whereas 24 answers reference one of the paste sites.</p><div><img src="img/image00303.jpeg" alt="Which paste sites are popular in questions and which are popular in answers?"/><div><p>phpMyAdmin shows the count of question and answer URLs referencing a pastebin.</p></div></div><p style="clear:both; height: 1em;"> </p><p>One thing to keep in mind about these queries is that they count individual URLs. So, if a particular<a id="id599" class="indexterm"/> <code class="literal">postId</code> referenced five URLs, those <a id="id600" class="indexterm"/>get counted five times. If I am interested in how many posts used a paste site URL once or more, I need to modify the first line of both queries as follows. This query counts distinct postings in the URLs table:</p><div><pre class="programlisting">SELECT tp.postTypeId, COUNT(DISTINCT cpu.postId)
FROM test_posts tp
INNER JOIN clean_posts_urls cpu ON tp.Id = cpu.postId
GROUP BY 1;</pre></div><p>The following screenshot shows how many questions and answers include a URL:</p><div><img src="img/image00304.jpeg" alt="Which paste sites are popular in questions and which are popular in answers?"/><div><p>phpMyAdmin shows how many questions and answers include any URL.</p></div></div><p style="clear:both; height: 1em;"> </p><p>This query counts the particular posts in the URLs table that mention a paste site:</p><div><pre class="programlisting">SELECT  tp.postTypeId, count(DISTINCT cpu.postId)
FROM test_posts tp
INNER JOIN clean_posts_urls cpu ON tp.Id = cpu.postId
WHERE cpu.url LIKE '%jsfiddle%'
OR cpu.url LIKE '%jsbin%'
OR cpu.url LIKE '%pastebin%'
OR cpu.url LIKE '%dpaste%'
OR cpu.url LIKE '%gist%'
OR cpu.url LIKE '%pastie%'
GROUP BY 1;</pre></div><p>The results for this paste site query are as follows, and, as expected, the numbers are smaller. In our test set, <strong>11</strong> questions used at least one pastebin URL, and so did <strong>16</strong> answers. Combined, 37 posts reference a pastebin URL at least once.</p><div><img src="img/image00305.jpeg" alt="Which paste sites are popular in questions and which are popular in answers?"/><div><p>PhpMyAdmin shows how many questions and answers include any paste site URL.</p></div></div><p style="clear:both; height: 1em;"> </p><p>Even<a id="id601" class="indexterm"/> though these results seem to show<a id="id602" class="indexterm"/> that people reference paste site URLs more in answers than questions, we need to compare them in terms of the overall number of questions and answers. We should report our result values as a percentage of the total for that post type, question or answer. Taking the totals into account, we can now say something like this: "Considering only the questions and answers that used any kind of URL at least once, 11 questions out of 81 used at least one paste site URL (13.6 percent), and 16 answers out of 222 used at least one paste site URL (7.2 percent)." With that in mind, it appears that the questions actually outstripped answers in referencing a paste site, almost two to one.</p><p>At this point in any data analysis project, you must have a flood of questions, like:</p><div><ul class="itemizedlist"><li class="listitem">How has the usage of paste site URLs in questions and answers changed over time?</li><li class="listitem">How do questions with paste site URLs fare on voting and favorites?</li><li class="listitem">What are the characteristics of users who post questions with paste site URLs?</li></ul></div><p>But since this is a book about data cleaning, and since we still have not even visualized this data, I will restrain myself and not answer these for the time being. We still have one of our original three questions to answer, and then we will move on to visualizing some of our results.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec99"/>Do posts contain both URLs to paste sites and source code?</h2></div></div></div><p>Answering our third question requires us to compare the amount of code in the Stack Overflow<a id="id603" class="indexterm"/> questions to the amount in the Stack Overflow answers, paying particular attention to the posts that include some sort of source code, delimited by the <code class="literal">&lt;code&gt;</code> tags. In the <em>Step three – cleaning the data</em> section, we extracted all code from the posts in our test tables, and created a new table to hold these code snippets. Now, a simple query to figure out how many code-containing posts there are is as follows:</p><div><pre class="programlisting">SELECT count(DISTINCT postid)
FROM clean_posts_code;</pre></div><p>In my sample set, this yields 664 code-containing posts, out of the total 1,000 test posts. Another way to put this is: 664 out of 1,000 posts contain at least one &lt;code&gt; tag.</p><p>To figure out how many of these code-containing posts also contained any URL, we can run the following SQL query:</p><div><pre class="programlisting">SELECT count(DISTINCT cpc.postid)
FROM clean_posts_code cpc
INNER JOIN clean_posts_urls cpu
ON cpu.postId = cpc.postId;</pre></div><p>My sample set yields 175 rows for this. We can interpret that by saying that 17.5 percent of the original test set of 1,000 posts contains code and a URL.</p><p>Now, to figure out how many of the code-containing posts also contained a paste site URL, we will narrow down the SQL query even further:</p><div><pre class="programlisting">SELECT count(DISTINCT cpc.postid)
FROM clean_posts_code cpc
INNER JOIN clean_posts_urls cpu
ON cpu.postId = cpc.postId
WHERE cpu.url LIKE '%jsfiddle%'
OR cpu.url LIKE '%jsbin%'
OR cpu.url LIKE '%pastebin%'
OR cpu.url LIKE '%dpaste%'
OR cpu.url LIKE '%gist%'
OR cpu.url LIKE '%pastie%';</pre></div><p>From these results, we can see that only a tiny set of 25 posts contained both source code and a paste site URL. From the second question, we know that 37 distinct posts (both questions and answers) used some sort of paste site URL at least once. So, 25 out of 37 is about 68 percent. It will be interesting to run these queries on the larger dataset to see how those values come out.</p><p>In the meantime, we will carry out some simple visualizations of at least one of our questions so that we can close the loop on one complete round of the data science six-step <a id="id604" class="indexterm"/>process.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec57"/>Step five – visualizing the data</h1></div></div></div><p>The <a id="id605" class="indexterm"/>visualization step is sort of like the dessert course in our dinner party. Everyone loves a rich graphic and they look so nice. However, since our focus in this book is on cleaning rather than analysis and visualization, our graphics here will be very simple. In the code that follows, we will use the JavaScript D3 visualization libraries to display the results of the first question graphically. This visualization will be much simpler than the D3 visualization we did in <a class="link" title="Chapter 4. Speaking the Lingua Franca – Data Conversions" href="part0028.xhtml#aid-QMFO2">Chapter 4</a>, <em>Speaking the Lingua Franca – Data Conversions</em>. In that chapter, you will recall that we built a fairly complicated network diagram, but here, a simple bar graph will suffice since all we have to display is just a few labels and counts.</p><p>The HTML and JavaScript/D3 code is as follows. This code extends the <em>Let's Build a Bar Graph</em> tutorial by<a id="id606" class="indexterm"/> Mike Bostock, available at <a class="ulink" href="http://bl.ocks.org/mbostock/3885304">http://bl.ocks.org/mbostock/3885304</a>. One of the ways that I extended this code was to make it read the JSON file we generated earlier in our <code class="literal">q1.php</code> script. Our JSON file printed really nicely, and was already sorted high to low, so building a little bar graph from that will be easy:</p><div><pre class="programlisting">&lt;!DOCTYPE html&gt;
&lt;meta charset="utf-8"&gt;
&lt;!--
this code is modeled on mbostock's
"Let's Make a Bar Chart" D3 tutorial
available at http://bl.ocks.org/mbostock/3885304
My modifications:
* formatting for space
* colors
* y axis labels
* changed variable names to match our data
* loads data via JSON rather than .tsv file
--&gt;

&lt;style&gt;
.bar {fill: lightgrey;}
.bar:hover {fill: lightblue;}
.axis {font: 10px sans-serif;}
.axis path, .axis line {
  fill: none;
  stroke: #000;
  shape-rendering: crispEdges;
}
.x.axis path {display: none;}
&lt;/style&gt;
&lt;body&gt;
&lt;script src="img/d3.min.js"&gt;&lt;/script&gt;
&lt;script&gt;

var margin = {top: 20, right: 20, bottom: 30, left: 40},
    width = 960 - margin.left - margin.right,
    height = 500 - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);

var xAxis = d3.svg.axis()
    .scale(x)
    .orient("bottom");

var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left");

var svg = d3.select("body").append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
    .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

d3.json("bincounter.php", function(error, json)
{
    data = json;
    draw(data);
});

function draw(data)
{
  x.domain(data.map(function(d) { return d.bin; }));
  y.domain([0, d3.max(data, function(d) { return d.count; })]);

  svg.append("g")
      .attr("class", "x axis")
      .attr("transform", "translate(0," + height + ")")
      .call(xAxis);

  svg.append("g")
      .attr("class", "y axis")
      .call(yAxis)
    .append("text")
      .attr("transform", "rotate(-90)")
      .attr("y", 6)
      .attr("dy", ".71em")
      .style("text-anchor", "end")
      .text("Frequency");

  svg.selectAll(".bar")
      .data(data)
    .enter().append("rect")
      .attr("class", "bar")
      .attr("x", function(d) { return x(d.bin) ; })
      .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.count); })
      .attr("height", function(d) { return height - y(d.count); });
}

&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</pre></div><p>We <a id="id607" class="indexterm"/>can save this as <code class="literal">q1chart.html</code>, and view it in the browser. The code calls our <code class="literal">q1.php</code> script, which generates the JSON file that D3 then uses to build this chart, the left-hand-side of which is shown here:</p><div><img src="img/image00306.jpeg" alt="Step five – visualizing the data"/><div><p>D3 visualization of the JSON produced from a count of three URLs.</p></div></div><p style="clear:both; height: 1em;"> </p><p>The <a id="id608" class="indexterm"/>bar graph shows that the URLs pointing to JSFiddle seem to be the most common, at least in my version of the randomly selected test dataset. We knew that just from looking at the JSON output from <code class="literal">q1.php</code>, but it is nice to see it graphically as well. In the next section, we will summarize the results and our procedure, and talk about where to go next with this project.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec58"/>Step six – problem resolution</h1></div></div></div><p>From the queries<a id="id609" class="indexterm"/> and visualizations we developed in the <em>Step four – analyzing the data</em> and <em>Step five – visualizing the data</em> sections, we can now attempt to answer each of the  three questions that prompted this project in the first place.</p><p>With our first question, we wanted to find counts of the different paste sites mentioned by URL in posts and comments. The <code class="literal">q1.php</code> script and bar graph we made to visualize the data show that, at least in the test data, JSFiddle was the most commonly referenced of the six paste site URLs we looked at.</p><p>The second question was about whether paste site URLs were more prevalent in questions or answers. Our queries show that paste site URLs were about twice as likely to occur in questions as opposed to answers, but the numbers for both were very small, at least in our test set.</p><p>For the third question, we wanted to look for whether people were actually heeding the advice of Stack Overflow and posting code in addition to a paste site URL. In our test set, the queries show that 25 postings (out of 37) include both a paste site URL and the recommended <a id="id610" class="indexterm"/>accompanying source code. This is a rate of about 68 percent compliance.</p><p>There are many additional questions we could ask and answer at this point, and many exciting ways we could extend this simple study into something that could be even more interesting. For now though, we will focus on the storage and cleaning procedures needed to extend this project to use the full dataset.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec59"/>Moving from test tables to full tables</h1></div></div></div><p>At the<a id="id611" class="indexterm"/> beginning of this project, we made a set of test tables <a id="id612" class="indexterm"/>so that we could develop our project in a stress-free environment using tables with only 1,000 rows each. Using small tables with manageable numbers of rows is important in cases where we are not sure that our queries will work as we want them to, or where we want to experiment with tricky joins, subqueries, weird regular expressions, and so on. At this point, though, if we feel good about the queries and scripts we have written, it is time to rewrite our procedures to use the full-size tables.</p><p>Here are the steps we will follow to move the project over to full-size tables:</p><div><ol class="orderedlist arabic"><li class="listitem"><code class="literal">DROP</code> the test tables:<div><pre class="programlisting">DROP TABLE IF EXISTS test_badges;
DROP TABLE IF EXISTS test_comments;
DROP TABLE IF EXISTS test_posts;
DROP TABLE IF EXISTS test_post_history;
DROP TABLE IF EXISTS test_post_links;
DROP TABLE IF EXISTS test_tags;
DROP TABLE IF EXISTS test_users;
DROP TABLE IF EXISTS test_votes;</pre></div></li><li class="listitem">Empty the <code class="literal">cleaned_posts_code</code>, <code class="literal">cleaned_posts_urls</code>, and <code class="literal">cleaned_comments_urls</code> tables as follows:<div><pre class="programlisting">TRUNCATE TABLE cleaned_posts_code;
TRUNCATE TABLE cleaned_posts_urls;
TRUNCATE TABLE cleaned_comments_urls;</pre></div></li><li class="listitem">Edit the <code class="literal">urlExtractor.php</code> and <code class="literal">codeExtractor.php</code> scripts to <code class="literal">SELECT</code> from the <code class="literal">posts</code> table rather than the <code class="literal">test_posts</code> table. These queries can be edited as follows:<div><pre class="programlisting">SELECT Id, Body FROM posts</pre></div></li><li class="listitem">Re-run the <code class="literal">urlExtractor.php</code> and <code class="literal">codeExtractor.php</code> scripts so that they will repopulate the clean code and URL tables we emptied (truncated) earlier.</li></ol><div></div><p>At this <a id="id613" class="indexterm"/>point, we have the cleaned code and URL tables<a id="id614" class="indexterm"/> ready to be analyzed and visualized. Take your time when performing these steps, and know that many of these queries and scripts will likely take a long time to finish. The posts table is quite large and many of the queries we are writing are selected against a text column using wildcards.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec60"/>Summary</h1></div></div></div><p>In this project, we posed a few questions about the prevalence of URLs on Stack Overflow, specifically those related to paste sites like <a class="ulink" href="http://www.Pastebin.com">http://www.Pastebin.com</a> and <a class="ulink" href="http://www.JSFiddle.net">http://www.JSFiddle.net</a>. To get started answering these questions, we downloaded data from the Stack Overflow postings (and other Stack Overflow data as well) from the Stack Exchange public file release. We built a MySQL database and eight tables to hold this data. We then created smaller 1,000-row versions of each of those tables for testing purposes, populated with a randomly selected sample of the data. From these test tables, we extracted the URLs mentioned in each question, answer, and comment, and saved them to a new clean table. We also extracted the source code found in the questions and answers, and saved those snippets to a new table as well. Finally, we were able to build some simple queries and visualizations to help us answer the questions we posed at the beginning.</p><p>Despite its modest results, from a data cleaning perspective, our dinner party was a success. We were able to make a coherent plan, and take methodical steps to put the plan into action and alter it when needed. We are now ready for our final project, and a completely different dinner party menu.</p><p>In the next chapter, we will collect and clean our own version of a famous Twitter dataset.</p></div></body></html>