<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Getting Ready to Use R and Hadoop</h1></div></div></div><p>The first chapter has been bundled with several topics on R and Hadoop basics as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">R Installation, features, and data modeling</li><li class="listitem" style="list-style-type: disc">Hadoop installation, features, and components</li></ul></div><p>In the preface, we introduced you to R and Hadoop. This chapter will focus on getting you up and<a id="id0" class="indexterm"/> running with these two technologies. Until now, R has been used mainly for <a id="id1" class="indexterm"/>statistical analysis, but due to the increasing number of functions and packages, it has become popular in several fields, such as machine learning, visualization, and data operations. R will not load all data (Big Data) into machine memory. So, Hadoop can be chosen to load the data as Big Data. Not all algorithms work across Hadoop, and the algorithms are, in general, not R algorithms. Despite this, analytics with R have several issues related to large data. In order to analyze the dataset, R loads it into the memory, and if the dataset is large, it will fail with exceptions such as "cannot allocate vector of size x". Hence, in order to process large datasets, the processing power of R can be vastly magnified by combining it with the power of a Hadoop cluster. Hadoop is very a popular framework that provides such parallel processing capabilities. So, we can use R algorithms or analysis processing over Hadoop clusters to get the work done.</p><div><img src="img/3282OS_01_00.jpg" alt="Getting Ready to Use R and Hadoop"/></div><p>If we think about a combined RHadoop system, R will take care of data analysis operations with the preliminary functions, such as data loading, exploration, analysis, and visualization, and Hadoop will take care of parallel data storage as well as computation power against distributed data.</p><p>Prior to the advent of affordable Big Data technologies, analysis used to be run on limited datasets on a single machine. Advanced machine learning algorithms are very effective when applied to large datasets, and this is possible only with large clusters where data can be stored and processed with distributed data storage systems. In the next section, we will see how R and Hadoop can be installed on different operating systems and the possible ways to link R and Hadoop.</p><div><div><div><div><h1 class="title"><a id="ch01lvl1sec15"/>Installing R</h1></div></div></div><p>You can download the appropriate version by visiting the official R website.</p><p>Here are the <a id="id2" class="indexterm"/>steps provided for three different operating systems. We have considered <a id="id3" class="indexterm"/>Windows, Linux, and Mac OS for R installation. Download the latest version of R as it will have all the latest patches and resolutions to the past bugs.</p><p>For Windows, follow the given steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Navigate to <a class="ulink" href="http://www.r-project.org">www.r-project.org</a>.</li><li class="listitem">Click on the <strong>CRAN</strong> section, select <strong>CRAN mirror</strong>, and select your Windows OS (stick to Linux; Hadoop is almost always used in a Linux environment).</li><li class="listitem">Download the latest R version from the mirror.</li><li class="listitem">Execute the downloaded <code class="literal">.exe</code> to install R.</li></ol></div><p>For Linux-Ubuntu, follow the given steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Navigate to <a class="ulink" href="http://www.r-project.org">www.r-project.org</a>.</li><li class="listitem">Click on the <strong>CRAN</strong> section, select <strong>CRAN mirror</strong>, and select your OS.</li><li class="listitem">In the <code class="literal">/etc/apt/sources.list</code> file, add the CRAN <code class="literal">&lt;mirror&gt;</code> entry.</li><li class="listitem">Download and update the package lists from the repositories using the <code class="literal">sudo apt-get update</code> command.</li><li class="listitem">Install R system using the <code class="literal">sudo apt-get install r-base</code> command.</li></ol></div><p>For Linux-RHEL/CentOS, follow the given steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Navigate to <a class="ulink" href="http://www.r-project.org">www.r-project.org</a>.</li><li class="listitem">Click on <strong>CRAN</strong>, select <strong>CRAN mirror</strong>, and select Red Hat OS.</li><li class="listitem">Download the <code class="literal">R-*core-*.rpm</code> file.</li><li class="listitem">Install the <code class="literal">.rpm</code> package using the <code class="literal">rpm -ivh R-*core-*.rpm</code> command.</li><li class="listitem">Install R system using <code class="literal">sudo yum install R</code>.</li></ol></div><p>For Mac, follow the given steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Navigate to <a class="ulink" href="http://www.r-project.org">www.r-project.org</a>.</li><li class="listitem">Click on <strong>CRAN</strong>, select <strong>CRAN mirror</strong>, and select your OS.</li><li class="listitem">Download the following files: <code class="literal">pkg</code>, <code class="literal">gfortran-*.dmg</code>, and <code class="literal">tcltk-*.dmg</code>.</li><li class="listitem">Install the <code class="literal">R-*.pkg</code> file.</li><li class="listitem">Then, install the <code class="literal">gfortran-*.dmg</code> and <code class="literal">tcltk-*.dmg</code> files.</li></ol></div><p>After installing <a id="id4" class="indexterm"/>the <a id="id5" class="indexterm"/>base R package, it is advisable to <a id="id6" class="indexterm"/>install RStudio, which is a powerful and intuitive <strong>Integrated Development Environment</strong> (<strong>IDE</strong>) for R.</p><div><div><h3 class="title"><a id="tip02"/>Tip</h3><p>We can use R distribution of Revolution Analytics as a Modern Data analytics tool for statistical computing and predictive analytics, which is available in free as well as premium versions. Hadoop integration is also available to perform Big Data analytics.</p></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec16"/>Installing RStudio</h1></div></div></div><p>To install <a id="id7" class="indexterm"/>RStudio, perform the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Navigate to <a class="ulink" href="http://www.rstudio.com/ide/download/desktop.">http://www.rstudio.com/ide/download/desktop</a>.</li><li class="listitem">Download the latest version of RStudio for your operating system.</li><li class="listitem">Execute the installer file and install RStudio.</li></ol></div><p>The RStudio<a id="id8" class="indexterm"/> organization and user community has developed a lot of R packages for graphics and visualization, such as <code class="literal">ggplot2</code>, <code class="literal">plyr</code>, <code class="literal">Shiny</code>, <code class="literal">Rpubs</code>, and <code class="literal">devtools</code>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec17"/>Understanding the features of R language</h1></div></div></div><p>There are over 3,000 R packages and the list is growing day by day. It would be beyond the scope of <a id="id9" class="indexterm"/>any book to even attempt to explain all these packages. This book focuses <a id="id10" class="indexterm"/>only on the key features of R and the most frequently used and popular packages.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec07"/>Using R packages</h2></div></div></div><p>R packages are <a id="id11" class="indexterm"/>self-contained units of R functionality that can be invoked as <a id="id12" class="indexterm"/>functions. A good analogy would be a <code class="literal">.jar</code> file in Java. There is a vast library of R packages available for a very wide range of operations ranging from statistical operations and machine learning to rich graphic visualization and plotting. Every package will consist of one or more R functions. An R package is a re-usable entity that can be shared and used by others. R users can install the package that contains the <a id="id13" class="indexterm"/>functionality they are looking for and start calling the functions in the package. A comprehensive <a id="id14" class="indexterm"/>list of these packages can be found at <a class="ulink" href="http://cran.r-project.org/">http://cran.r-project.org/</a> called <strong>Comprehensive R Archive Network</strong> (<strong>CRAN</strong>).</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec08"/>Performing data operations</h2></div></div></div><p>R enables a <a id="id15" class="indexterm"/>wide range of operations. Statistical operations, such as<a id="id16" class="indexterm"/> mean, min, max, probability, distribution, and regression. Machine learning operations, such as linear regression, logistic regression, classification, and clustering. Universal data processing operations are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Data cleaning</strong>: This<a id="id17" class="indexterm"/> option is to clean<a id="id18" class="indexterm"/> massive datasets</li><li class="listitem" style="list-style-type: disc"><strong>Data exploration</strong>: This <a id="id19" class="indexterm"/>option is to <a id="id20" class="indexterm"/>explore all the possible values of datasets</li><li class="listitem" style="list-style-type: disc"><strong>Data analysis</strong>: This<a id="id21" class="indexterm"/> option is to perform <a id="id22" class="indexterm"/>analytics on data with descriptive and predictive analytics data visualization, that is, visualization of analysis output programming</li></ul></div><p>To <a id="id23" class="indexterm"/>build an effective analytics application, sometimes we need to use the online <strong>Application Programming Interface</strong> (<strong>API</strong>) to dig up the data, analyze it with expedient services, and visualize it by third-party services. Also, to automate the data analysis process, programming will be the most useful feature to deal with.</p><p>R has its own programming language to operate data. Also, the available package can help to integrate R<a id="id24" class="indexterm"/> with other programming features. R supports <a id="id25" class="indexterm"/>object-oriented programming concepts. It is also capable of integrating with other programming languages, such as Java, PHP, C, and C++. There are several packages that will act as middle-layer programming features to aid in data analytics, which are similar to <code class="literal">sqldf</code>, <code class="literal">httr</code>, <code class="literal">RMongo</code>, <code class="literal">RgoogleMaps</code>, <code class="literal">RGoogleAnalytics</code>, and <code class="literal">google-prediction-api-r-client</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec09"/>Increasing community support</h2></div></div></div><p>As the number of R users are<a id="id26" class="indexterm"/> escalating, the groups related to R are also <a id="id27" class="indexterm"/>increasing. So, R learners or developers can easily connect and get their uncertainty solved with the help of several R groups or communities.</p><p>The following are many popular sources that can be found useful:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>R mailing list</strong>: This <a id="id28" class="indexterm"/>is an official R group created by R project owners.</li><li class="listitem" style="list-style-type: disc"><strong>R blogs</strong>: R<a id="id29" class="indexterm"/> has countless bloggers who are writing on several R applications. One of the most popular blog websites is <a class="ulink" href="http://www.r-bloggers.com/">http://www.r-bloggers.com/</a> where <a id="id30" class="indexterm"/>all the bloggers contribute their blogs.</li><li class="listitem" style="list-style-type: disc"><strong>Stack overflow</strong>: This<a id="id31" class="indexterm"/> is a great technical knowledge sharing platform where the programmers can post their technical queries and enthusiast programmers suggest a solution. For more information, visit <a class="ulink" href="http://stats.stackexchange.com/">http://stats.stackexchange.com/</a>.</li><li class="listitem" style="list-style-type: disc"><strong>Groups</strong>: There are <a id="id32" class="indexterm"/>many other groups existing on LinkedIn and Meetup where professionals across the world meet to discuss their problems and innovative ideas.</li><li class="listitem" style="list-style-type: disc"><strong>Books</strong>: There are also lot of <a id="id33" class="indexterm"/>books about R. Some of the popular books are <em>R in Action</em>, by <em>Rob Kabacoff</em>, <em>Manning Publications</em>, <em>R in a Nutshell</em>, by <em>Joseph Adler</em>, <em>O'Reilly Media</em>, <em>R and Data Mining</em>, by <em>Yanchang Zhao</em>, <em>Academic Press</em>, and <em>R Graphs Cookbook</em>, by <em>Hrishi Mittal</em>, <em>Packt Publishing</em>.</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Performing data modeling in R</h2></div></div></div><p>Data modeling is a machine learning technique to identify the hidden pattern from the historical <a id="id34" class="indexterm"/>dataset, and this pattern will help in future value prediction <a id="id35" class="indexterm"/>over the same data. This techniques highly focus on past user actions and learns their taste. Most of these data modeling techniques have been adopted by many popular organizations to understand the behavior of their customers based on their past transactions. These techniques will analyze data and predict for the customers what they are looking for. Amazon, Google, Facebook, eBay, LinkedIn, Twitter, and many other organizations are using data mining for changing the definition applications.</p><p>The most common<a id="id36" class="indexterm"/> data mining techniques are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Regression</strong>: In statistics, regression is a classic technique to identify the scalar <a id="id37" class="indexterm"/>relationship between two or more variables by fitting the state line on the variable values. That relationship will help to predict the variable value for future events. For example, any variable y can be modeled as linear function of another variable x with the formula <em>y = mx+c</em>. Here, x is the predictor variable, y is the response variable, m is slope of the line, and c is the intercept. Sales forecasting of products or services and predicting the price of stocks can be achieved through this regression. R provides this regression feature via the <code class="literal">lm</code> method, which is by default present in R.</li><li class="listitem" style="list-style-type: disc"><strong>Classification</strong>: This is a machine-learning technique used for labeling the set of observations <a id="id38" class="indexterm"/>provided for training<a id="id39" class="indexterm"/> examples. With this, we can classify the observations into one or more labels. The likelihood of sales, online fraud detection, and cancer classification (for medical science) are common applications of classification problems. Google Mail uses this technique to classify e-mails as spam or not. Classification features can be served by <code class="literal">glm</code>, <code class="literal">glmnet</code>, <code class="literal">ksvm</code>, <code class="literal">svm</code>, and <code class="literal">randomForest</code> in R.</li><li class="listitem" style="list-style-type: disc"><strong>Clustering</strong>: This technique is all about organizing similar items into groups from the given <a id="id40" class="indexterm"/>collection of items. User <a id="id41" class="indexterm"/>segmentation and image compression are the most common applications of clustering. Market segmentation, social network analysis, organizing the computer clustering, and astronomical data analysis are applications of clustering. Google News uses these techniques to group similar news items into the same category. Clustering can <a id="id42" class="indexterm"/>be achieved through the <code class="literal">knn</code>, <code class="literal">kmeans</code>, <code class="literal">dist</code>, <code class="literal">pvclust</code>, and <code class="literal">Mclust</code> methods in R.</li><li class="listitem" style="list-style-type: disc"><strong>Recommendation</strong>: The recommendation algorithms are used in recommender<a id="id43" class="indexterm"/> systems where<a id="id44" class="indexterm"/> these systems are the most immediately recognizable machine learning techniques in use today. Web content recommendations may include similar websites, blogs, videos, or related content. Also, recommendation of online items can be helpful for cross-selling and up-selling. We have all seen online shopping portals that attempt to recommend books, mobiles, or any<a id="id45" class="indexterm"/> items that can be sold on the Web based on the user's past behavior. Amazon is a well-known e-commerce portal that generates 29 percent of sales through recommendation systems. Recommender<a id="id46" class="indexterm"/> systems can<a id="id47" class="indexterm"/> be implemented via <code class="literal">Recommender()</code>with the <code class="literal">recommenderlab</code> package in R.</li></ul></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec18"/>Installing Hadoop</h1></div></div></div><p>Now, we presume that you are aware of R, what it is, how to install it, what it's key features are, and why you may want to use it. Now we need to know the limitations of R (this is a better introduction to Hadoop). Before processing the data; R needs to load the data into <strong>random access memory</strong> (<strong>RAM</strong>). So, the data needs to be smaller than the available <a id="id48" class="indexterm"/>machine memory. For data that is larger <a id="id49" class="indexterm"/>than the machine memory, we consider it as Big Data (only in our case as there are many other definitions of Big Data).</p><p>To avoid this<a id="id50" class="indexterm"/> Big Data issue, we need to scale the hardware configuration; however, this is a temporary solution. To get this solved, we need to get a Hadoop cluster that is able to store it and perform parallel computation across a large computer cluster. Hadoop is the most popular solution. Hadoop is an open source Java framework, which is the top level project handled by the Apache software foundation. Hadoop is inspired by the Google filesystem and MapReduce, mainly designed for operating on Big Data by distributed processing.</p><p>Hadoop mainly supports Linux operating systems. To run this on Windows, we need to use VMware to host Ubuntu within the Windows OS. There are many ways to use and install Hadoop, but here we will consider the way that supports R best. Before we combine R and Hadoop, let us understand what Hadoop is.</p><div><div><h3 class="title"><a id="tip03"/>Tip</h3><p>Machine <a id="id51" class="indexterm"/>learning contains all the data modeling techniques<a id="id52" class="indexterm"/> that <a id="id53" class="indexterm"/>can be explored with the web link <a class="ulink" href="http://en.wikipedia.org/wiki/Machine_learning">http://en.wikipedia.org/wiki/Machine_learning</a>.</p><p>The structure blog on Hadoop installation by Michael Noll can be found at <a class="ulink" href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</a>.</p></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec11"/>Understanding different Hadoop modes</h2></div></div></div><p>Hadoop<a id="id54" class="indexterm"/> is <a id="id55" class="indexterm"/>used with three different modes:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>The standalone mode</strong>: In <a id="id56" class="indexterm"/>this mode, you <a id="id57" class="indexterm"/>do not need to start any Hadoop daemons. Instead, just call <code class="literal">~/Hadoop-directory/bin/hadoop</code> that will execute a Hadoop operation as a single Java process. This is recommended for testing purposes. This is the default mode and you don't need to configure anything else. All daemons, such as NameNode, DataNode, JobTracker, and TaskTracker run in a single Java process.</li><li class="listitem" style="list-style-type: disc"><strong>The pseudo mode</strong>: In <a id="id58" class="indexterm"/>this mode, you configure<a id="id59" class="indexterm"/> Hadoop for all the nodes. A separate <strong>Java Virtual Machine</strong> (<strong>JVM</strong>)<a id="id60" class="indexterm"/> is spawned for each of the Hadoop components or daemons like mini cluster on a single host.</li><li class="listitem" style="list-style-type: disc"><strong>The full distributed mode</strong>: In this mode, Hadoop is distributed across multiple<a id="id61" class="indexterm"/> machines. Dedicated hosts are<a id="id62" class="indexterm"/> configured for Hadoop components. Therefore, separate JVM processes are present for all daemons.</li></ul></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec12"/>Understanding Hadoop installation steps</h2></div></div></div><p>Hadoop can be <a id="id63" class="indexterm"/>installed in several ways; we will consider the way that is<a id="id64" class="indexterm"/> better to integrate with R. We will choose Ubuntu OS as it is easy to install and access it.</p><div><ol class="orderedlist arabic"><li class="listitem">Installing Hadoop on Linux, Ubuntu flavor (single and multinode cluster).</li><li class="listitem">Installing Cloudera Hadoop on Ubuntu.</li></ol></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec02"/>Installing Hadoop on Linux, Ubuntu flavor (single node cluster)</h3></div></div></div><p>To install <a id="id65" class="indexterm"/>Hadoop over Ubuntu OS <a id="id66" class="indexterm"/>with the pseudo mode, we need to meet the following <a id="id67" class="indexterm"/>prerequisites:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Sun Java 6</li><li class="listitem" style="list-style-type: disc">Dedicated Hadoop system user</li><li class="listitem" style="list-style-type: disc">Configuring SSH</li><li class="listitem" style="list-style-type: disc">Disabling IPv6</li></ul></div><div><div><h3 class="title"><a id="tip04"/>Tip</h3><p>The provided <a id="id68" class="indexterm"/>Hadoop installation will be supported with Hadoop MRv1.</p></div></div><p>Follow the given steps to install Hadoop:</p><div><ol class="orderedlist arabic"><li class="listitem">Download the latest Hadoop sources from the Apache software foundation. Here we have considered <a id="id69" class="indexterm"/>Apache Hadoop 1.0.3, whereas the latest version is 1.1.x.<div><pre class="programlisting">
<strong>// Locate to Hadoop installation directory</strong>
<strong>$ cd /usr/local</strong>

<strong>// Extract the tar file of Hadoop distribution</strong>
<strong>$ sudo tar xzf hadoop-1.0.3.tar.gz</strong>

<strong>// To move Hadoop resources to hadoop folder </strong>
<strong>$ sudo mv hadoop-1.0.3 hadoop</strong>

<strong>// Make user-hduser from group-hadoop as owner of hadoop directory</strong>
<strong>$ sudo chown -R hduser:hadoop hadoop</strong>
</pre></div></li><li class="listitem">Add the <code class="literal">$JAVA_HOME</code> and <code class="literal">$HADOOP_HOME</code> variables to the<code class="literal">.bashrc</code> file of Hadoop system user and the updated <code class="literal">.bashrc</code> file looks as follows:<div><pre class="programlisting">
<strong>// Setting the environment variables for running Java and Hadoop commands</strong>
<strong>export HADOOP_HOME=/usr/local/hadoop</strong>
<strong>export JAVA_HOME=/usr/lib/jvm/java-6-sun</strong>

<strong>// alias for Hadoop commands</strong>
<strong>unalias fs &amp;&gt; /dev/null</strong>
<strong>alias fs="hadoop fs"</strong>
<strong>unalias hls &amp;&gt; /dev/null</strong>
<strong>aliashls="fs -ls"</strong>

<strong>// Defining the function for compressing the MapReduce job output by lzop command</strong>
<strong>lzohead () {</strong>
<strong>hadoopfs -cat $1 | lzop -dc | head -1000 | less</strong>
<strong>}</strong>

<strong>// Adding Hadoop_HoME variable to PATH </strong>
<strong>export PATH=$PATH:$HADOOP_HOME/bin</strong>
</pre></div></li><li class="listitem">Update<a id="id70" class="indexterm"/> the Hadoop<a id="id71" class="indexterm"/> configuration files with the <code class="literal">conf/*-site.xml</code> format.</li></ol></div><p>Finally, the three files will look as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">conf/core-site.xml</code>:<div><pre class="programlisting">&lt;property&gt;
&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
&lt;value&gt;/app/hadoop/tmp&lt;/value&gt;
&lt;description&gt;A base for other temporary directories.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;fs.default.name&lt;/name&gt;
&lt;value&gt;hdfs://localhost:54310&lt;/value&gt;
&lt;description&gt;The name of the default filesystem. A URI whose
scheme and authority determine the FileSystem implementation. The
uri's scheme determines the config property (fs.SCHEME.impl) naming
theFileSystem implementation class. The uri's authority is used to
determine the host, port, etc. for a filesystem.&lt;/description&gt;
&lt;/property&gt;</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">conf/mapred-site.xml</code>:<div><pre class="programlisting">&lt;property&gt;
&lt;name&gt;mapred.job.tracker&lt;/name&gt;
&lt;value&gt;localhost:54311&lt;/value&gt;
&lt;description&gt;The host and port that the MapReduce job tracker runs
at. If "local", then jobs are run in-process as a single map
and reduce task.
&lt;/description&gt;
&lt;/property&gt;</pre></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">conf/hdfs-site.xml</code>:<div><pre class="programlisting">&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;1&lt;/value&gt;
&lt;description&gt;Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
&lt;/description&gt;</pre></div></li></ul></div><p>After <a id="id72" class="indexterm"/>completing the editing of<a id="id73" class="indexterm"/> these configuration files, we need to set up the distributed filesystem across the Hadoop clusters or node.</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Format <a id="id74" class="indexterm"/><strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>) via NameNode by using the following command line:<div><pre class="programlisting">
<strong>hduser@ubuntu:~$ /usr/local/hadoop/bin/hadoopnamenode -format</strong>
</pre></div></li><li class="listitem" style="list-style-type: disc">Start your single node cluster by using the following command line:<div><pre class="programlisting">
<strong>hduser@ubuntu:~$ /usr/local/hadoop/bin/start-all.sh</strong>
</pre></div></li></ul></div><div><div><h3 class="title"><a id="tip05"/>Tip</h3><p><strong>Downloading the example code</strong></p><p>You<a id="id75" class="indexterm"/> can download<a id="id76" class="indexterm"/> the example code files for all Packt books you have purchased from your account at <a class="ulink" href="http://www.packtpub.com">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div></div></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec03"/>Installing Hadoop on Linux, Ubuntu flavor (multinode cluster)</h3></div></div></div><p>We learned how to install Hadoop on a single node cluster. Now we will see how to install Hadoop <a id="id77" class="indexterm"/>on a multinode cluster (the full distributed mode).</p><p>For this, we<a id="id78" class="indexterm"/> need several nodes configured with a single node Hadoop cluster. To install Hadoop on multinodes, we need to have that machine configured with a single node Hadoop cluster as described in the last section.</p><p>After getting the single node Hadoop cluster installed, we need to perform the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">In the networking phase, we are going to use two nodes for setting up a full distributed Hadoop mode. To communicate with each other, the nodes need to be in the same network in terms of software and hardware configuration.</li><li class="listitem">Among these two, one of the nodes will be considered as master and the other will be considered as slave. So, for performing Hadoop operations, master needs to be connected to slave. We will enter <code class="literal">192.168.0.1</code> in the master machine and <code class="literal">192.168.0.2</code> in the slave machine.</li><li class="listitem">Update the <code class="literal">/etc/hosts</code> directory in both the nodes. It will look as <code class="literal">192.168.0.1 master</code> and <code class="literal">192.168.0.2 slave</code>.<div><div><h3 class="title"><a id="tip06"/>Tip</h3><p>You can perform the <a id="id79" class="indexterm"/>
<strong>Secure Shell</strong> (<strong>SSH</strong>) setup similar to what we did for a single node cluster setup. For more details, visit <a class="ulink" href="http://www.michael-noll.com">http://www.michael-noll.com</a>.</p></div></div></li><li class="listitem">Updating <code class="literal">conf/*-site.xml</code>: We must change all these configuration files in all of the nodes.<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">conf/core-site.xml</code> and <code class="literal">conf/mapred-site.xml</code>: In the single node setup, we have updated these files. So, now we need to just replace <code class="literal">localhost</code> by <code class="literal">master</code> in the value tag.</li><li class="listitem" style="list-style-type: disc"><code class="literal">conf/hdfs-site.xml</code>: In the single node setup, we have set the value of <code class="literal">dfs.replication</code> as <code class="literal">1</code>. Now we need to update this as <code class="literal">2</code>.</li></ul></div></li><li class="listitem"> In the<a id="id80" class="indexterm"/> formatting HDFS phase, before we start the multinode cluster, we need to format HDFS with the<a id="id81" class="indexterm"/> following command (from the master node):<div><pre class="programlisting">
<strong>bin/hadoop namenode -format</strong>
</pre></div></li></ol></div><p>Now, we have completed all the steps to install the multinode Hadoop cluster. To start the Hadoop clusters, we need to follow these steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Start HDFS daemons:<div><pre class="programlisting">
<strong>hduser@master:/usr/local/hadoop$ bin/start-dfs.sh</strong>
</pre></div></li><li class="listitem">Start MapReduce daemons:<div><pre class="programlisting">
<strong>hduser@master:/usr/local/hadoop$ bin/start-mapred.sh</strong>
</pre></div></li><li class="listitem">Alternatively, we can start all the daemons with a single command:<div><pre class="programlisting">
<strong>hduser@master:/usr/local/hadoop$ bin/start-all.sh</strong>
</pre></div></li><li class="listitem">To stop all these daemons, fire:<div><pre class="programlisting">
<strong>hduser@master:/usr/local/hadoop$ bin/stop-all.sh</strong>
</pre></div></li></ol></div><p>These installation steps are reproduced after being inspired by the blogs (<a class="ulink" href="http://www.michael-noll.com">http://www.michael-noll.com</a>) of Michael Noll, who is a researcher and Software Engineer based in Switzerland, Europe. He works as a Technical lead for a large scale computing infrastructure on the Apache Hadoop stack at VeriSign.</p><p>Now the  Hadoop cluster has been set up on your machines. For the installation of the same Hadoop cluster on single node or multinode with extended Hadoop components, try the Cloudera tool.</p></div><div><div><div><div><h3 class="title"><a id="ch01lvl3sec04"/>Installing Cloudera Hadoop on Ubuntu</h3></div></div></div><p><strong>Cloudera Hadoop</strong> (<strong>CDH</strong>) is Cloudera's open source <a id="id82" class="indexterm"/>distribution that targets enterprise class deployments of Hadoop<a id="id83" class="indexterm"/> technology. Cloudera is also a sponsor of the Apache <a id="id84" class="indexterm"/>software foundation. CDH is available in two versions: CDH3 and CDH4. To install one of these, you must have Ubuntu with either 10.04 LTS or 12.04 LTS (also, you can try CentOS, Debian, and Red Hat systems). Cloudera manager will make this installation easier for you if you are installing a Hadoop on cluster of<a id="id85" class="indexterm"/> computers, which provides GUI-based Hadoop and its <a id="id86" class="indexterm"/>component installation over a whole cluster. This tool is very much recommended for large clusters.</p><p>We need to<a id="id87" class="indexterm"/> meet the following prerequisites:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Configuring SSH</li><li class="listitem" style="list-style-type: disc">OS with the following criteria:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ubuntu 10.04 LTS or 12.04 LTS with 64 bit</li><li class="listitem" style="list-style-type: disc">Red Hat Enterprise Linux 5 or 6</li><li class="listitem" style="list-style-type: disc">CentOS 5 or 6</li><li class="listitem" style="list-style-type: disc">Oracle Enterprise Linux 5</li><li class="listitem" style="list-style-type: disc">SUSE Linux Enterprise server 11 (SP1 or lasso)</li><li class="listitem" style="list-style-type: disc">Debian 6.0</li></ul></div></li></ul></div><p>The installation steps are as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Download and run the Cloudera manager installer: To initialize the Cloudera manager installation process, we need to first download the <code class="literal">cloudera-manager-installer.bin</code> file from the download section of the Cloudera website. After that, store it at the cluster so that all the nodes can access this. Allow ownership for execution permission of <code class="literal">cloudera-manager-installer.bin</code> to the user. Run the following command to start execution.<div><pre class="programlisting">
<strong>$ sudo ./cloudera-manager-installer.bin</strong>
</pre></div></li><li class="listitem">Read the Cloudera manager <strong>Readme</strong> and then click on <strong>Next</strong>.</li><li class="listitem">Start the Cloudera manager admin console: The Cloudera manager admin console allows you to use Cloudera manager to install, manage, and monitor Hadoop on your cluster. After accepting the license from the Cloudera service provider, you need to traverse to your local web browser by entering <code class="literal">http://localhost:7180</code> in your address bar. You can also use any of the following browsers:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Firefox 11 or higher</li><li class="listitem" style="list-style-type: disc">Google Chrome</li><li class="listitem" style="list-style-type: disc">Internet Explorer</li><li class="listitem" style="list-style-type: disc">Safari</li></ul></div></li><li class="listitem">Log in to the Cloudera manager console with the default credentials using <code class="literal">admin</code> for both the username and password. Later on you can change it as per your choice.</li><li class="listitem">Use the <a id="id88" class="indexterm"/>Cloudera manager for automated CDH3 installation and configuration via browser: This step will install most of the<a id="id89" class="indexterm"/> required Cloudera Hadoop packages from Cloudera to your machines. The steps are as follows:<div><ol class="orderedlist arabic"><li class="listitem">Install and validate your Cloudera manager license key file if you have chosen a full version of software.</li><li class="listitem">Specify the hostname or IP address range for your CDH cluster installation.</li><li class="listitem">Connect to each host with SSH.</li><li class="listitem">Install the <a id="id90" class="indexterm"/><strong>Java Development Kit</strong> (<strong>JDK</strong>) (if not already installed), the Cloudera manager agent, and CDH3 or CDH4 on each cluster host.</li><li class="listitem">Configure Hadoop on each node and start the Hadoop services.</li></ol></div></li><li class="listitem">After running the wizard and using the Cloudera manager, you should change the default administrator password as soon as possible. To change the administrator password, follow these steps:<div><ol class="orderedlist arabic"><li class="listitem">Click on the icon with the gear sign to display the administration page.</li><li class="listitem">Open the <strong>Password</strong> tab.</li><li class="listitem">Enter a new password twice and then click on <strong>Update</strong>.</li></ol></div></li><li class="listitem">Test the Cloudera Hadoop installation: You can check the Cloudera manager installation on your cluster by logging into the Cloudera manager admin console and by clicking on the <strong>Services</strong> tab. You should see something like the following screenshot:<div><img src="img/3282OS_01_01.jpg" alt="Installing Cloudera Hadoop on Ubuntu"/><div><p>Cloudera manager admin console</p></div></div></li><li class="listitem">You can also click on each service to see more detailed information. For example, if <a id="id91" class="indexterm"/>you click on the <strong>hdfs1</strong> link, you might <a id="id92" class="indexterm"/>see something like the following screenshot:<div><img src="img/3282OS_01_02.jpg" alt="Installing Cloudera Hadoop on Ubuntu"/><div><p>Cloudera manger admin console—HDFS service</p></div></div><div><div><h3 class="title"><a id="tip07"/>Tip</h3><p>To avoid these installation steps, use preconfigured Hadoop instances with Amazon Elastic MapReduce and MapReduce.</p><p>If you <a id="id93" class="indexterm"/>want to use Hadoop on Windows, try the HDP <a id="id94" class="indexterm"/>tool by Hortonworks. This is 100 percent open source, enterprise grade distribution of Hadoop. You can download the HDP tool at <a class="ulink" href="http://hortonworks.com/download/">http://hortonworks.com/download/</a>.</p></div></div></li></ol></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec19"/>Understanding Hadoop features</h1></div></div></div><p>Hadoop is<a id="id95" class="indexterm"/> specially designed for two core concepts: HDFS and MapReduce. Both are related to distributed computation. MapReduce is believed as the heart of Hadoop that performs parallel processing over distributed data.</p><p>Let us see more details on Hadoop's features:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">HDFS</li><li class="listitem" style="list-style-type: disc">MapReduce</li></ul></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec13"/>Understanding HDFS</h2></div></div></div><p>HDFS is <a id="id96" class="indexterm"/>Hadoop's own rack-aware filesystem, which is a UNIX-based<a id="id97" class="indexterm"/> data storage layer of Hadoop. HDFS is derived from concepts of Google filesystem. An important characteristic of Hadoop is the partitioning of data and computation across many (thousands of) hosts, and the execution of application computations in parallel, close to their data. On HDFS, data files are replicated as sequences of blocks in the cluster. A Hadoop cluster scales computation capacity, storage capacity, and I/O bandwidth by simply adding commodity servers. HDFS can be accessed from applications in many different ways. Natively, HDFS provides a Java API for applications to use.</p><p>The Hadoop clusters at Yahoo! span 40,000 servers and store 40 petabytes of application data, with the largest Hadoop cluster being 4,000 servers. Also, one hundred other organizations worldwide are known to use Hadoop.</p><div><div><div><div><h3 class="title"><a id="ch01lvl3sec05"/>Understanding the characteristics of HDFS</h3></div></div></div><p>Let us now <a id="id98" class="indexterm"/>look at the characteristics of HDFS:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Fault tolerant</li><li class="listitem" style="list-style-type: disc">Runs with commodity hardware</li><li class="listitem" style="list-style-type: disc">Able to handle large datasets</li><li class="listitem" style="list-style-type: disc">Master<a id="id99" class="indexterm"/> slave paradigm</li><li class="listitem" style="list-style-type: disc">Write once file access only</li></ul></div></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec14"/>Understanding MapReduce</h2></div></div></div><p>MapReduce is a<a id="id100" class="indexterm"/> programming model for processing large<a id="id101" class="indexterm"/> datasets distributed on a large cluster. MapReduce is the heart of Hadoop. Its programming paradigm allows performing massive data processing across thousands of servers configured with Hadoop clusters. This is derived from Google MapReduce.</p><p>Hadoop MapReduce is a software framework for writing applications easily, which process large amounts of data (multiterabyte datasets) in parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner. This MapReduce paradigm is divided into two phases, Map and Reduce that mainly deal with key and value pairs of data. The Map and Reduce task run sequentially in a cluster; the output of the Map phase becomes the input for the Reduce phase. These phases are explained as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Map phase</strong>: Once<a id="id102" class="indexterm"/> divided, datasets are assigned to the <a id="id103" class="indexterm"/>task tracker to perform the Map phase. The data functional operation will be performed over the data, emitting the mapped key and value pairs as the output of the Map phase.</li><li class="listitem" style="list-style-type: disc"><strong>Reduce phase</strong>: The <a id="id104" class="indexterm"/>master node then collects the<a id="id105" class="indexterm"/> answers to all the subproblems and combines them in some way to form the output; the answer to the problem it was originally trying to solve.</li></ul></div><p>The five common <a id="id106" class="indexterm"/>steps of parallel computing are as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Preparing the <code class="literal">Map()</code> input: This will take the input data row wise and emit key value pairs per rows, or we can explicitly change as per the requirement.<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Map input: list (k1, v1)</li></ul></div></li><li class="listitem">Run the user-provided <code class="literal">Map()</code> code<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Map output: list (k2, v2)</li></ul></div></li><li class="listitem">Shuffle the Map output to the Reduce processors. Also, shuffle the similar keys (grouping them) and input them to the same reducer.</li><li class="listitem">Run the<a id="id107" class="indexterm"/> user-provided <code class="literal">Reduce()</code> code: This phase will run the custom reducer code designed by developer to run on shuffled data and emit key and value.<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reduce input: (k2, list(v2))</li><li class="listitem" style="list-style-type: disc">Reduce output: (k3, v3)</li></ul></div></li><li class="listitem">Produce the final output: Finally, the master node collects all reducer output and combines and writes them in a text file.<div><div><h3 class="title"><a id="tip08"/>Tip</h3><p>The <a id="id108" class="indexterm"/>reference links to review on Google filesystem can be found at <a class="ulink" href="http://research.google.com/archive/gfs.html">http://research.google.com/archive/gfs.html</a> and Google MapReduce<a id="id109" class="indexterm"/> can be found at <a class="ulink" href="http://research.google.com/archive/mapreduce.html">http://research.google.com/archive/mapreduce.html</a>.</p></div></div></li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec20"/>Learning the HDFS and MapReduce architecture</h1></div></div></div><p>Since HDFS and MapReduce are considered to be the two main features of the Hadoop framework, we will focus on them. So, let's first start with HDFS.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec15"/>Understanding the HDFS architecture</h2></div></div></div><p>HDFS can be presented as the master/slave architecture. HDFS master is named as NameNode <a id="id110" class="indexterm"/>whereas slave as DataNode. NameNode is a sever that<a id="id111" class="indexterm"/> manages the filesystem namespace and adjusts the access (open, close, rename, and more) to files by the client. It divides the input data into blocks and announces which data block will be store in which DataNode. DataNode is a slave machine that stores the replicas of the partitioned dataset and serves the data as the request comes. It also performs block creation and deletion.</p><p>The internal mechanism of HDFS divides the file into one or more blocks; these blocks are stored in a set of data nodes. Under normal circumstances of the replication factor three, the HDFS strategy is to place the first copy on the local node, second copy on the local rack with a different node, and a third copy into different racks with different nodes. As HDFS is designed to support large files, the HDFS block size is defined as 64 MB. If required, this can be increased.</p><div><div><div><div><h3 class="title"><a id="ch01lvl3sec06"/>Understanding HDFS components</h3></div></div></div><p>HDFS is <a id="id112" class="indexterm"/>managed with the master-slave architecture included with the following components:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>NameNode</strong>: This is <a id="id113" class="indexterm"/>the master of the HDFS system. It maintains the directories, files, and manages the blocks that are <a id="id114" class="indexterm"/>present on the DataNodes.</li><li class="listitem" style="list-style-type: disc"><strong>DataNode</strong>: These <a id="id115" class="indexterm"/>are slaves that are deployed <a id="id116" class="indexterm"/>on each machine and provide actual storage. They are responsible for serving read-and-write data requests for the clients.</li><li class="listitem" style="list-style-type: disc"><strong>Secondary NameNode</strong>: This is responsible for performing periodic checkpoints. So, if the NameNode fails at any time, it can be replaced with a snapshot<a id="id117" class="indexterm"/> image stored by the <a id="id118" class="indexterm"/>secondary NameNode checkpoints.</li></ul></div></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec16"/>Understanding the MapReduce architecture</h2></div></div></div><p>MapReduce is <a id="id119" class="indexterm"/>also implemented over master-slave architectures. Classic MapReduce contains job submission, job initialization, task assignment, task <a id="id120" class="indexterm"/>execution, progress and status update, and job completion-related activities, which are mainly managed by the JobTracker node and executed by TaskTracker. Client application submits a job to the JobTracker. Then input is divided across the cluster. The JobTracker then calculates the number of map and reducer to be processed. It commands the TaskTracker to start executing the job. Now, the TaskTracker copies the resources to a local machine and launches JVM to map and reduce program over the data. Along with this, the TaskTracker periodically sends update to the JobTracker, which can be considered as the heartbeat that helps to update JobID, job status, and usage of resources.</p><div><div><div><div><h3 class="title"><a id="ch01lvl3sec07"/>Understanding MapReduce components</h3></div></div></div><p>MapReduce is<a id="id121" class="indexterm"/> managed with master-slave architecture included with the following components:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>JobTracker</strong>: This<a id="id122" class="indexterm"/> is the master node <a id="id123" class="indexterm"/>of the MapReduce system, which manages the jobs and resources in the cluster (TaskTrackers). The JobTracker tries to schedule each map as close to the actual data being processed on the TaskTracker, which is running on the same DataNode as the underlying block.</li><li class="listitem" style="list-style-type: disc"><strong>TaskTracker</strong>: These<a id="id124" class="indexterm"/> are the slaves<a id="id125" class="indexterm"/> that are deployed on each machine. They are responsible<a id="id126" class="indexterm"/> for running the map and reducing tasks as instructed by the JobTracker.</li></ul></div></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec17"/>Understanding the HDFS and MapReduce architecture by plot</h2></div></div></div><p>In this plot, both <a id="id127" class="indexterm"/>HDFS and MapReduce <a id="id128" class="indexterm"/>master and slave components have been included, where NameNode and DataNode are from HDFS and JobTracker and TaskTracker are from the MapReduce paradigm.</p><p>Both paradigms consisting of master and slave candidates have their own specific responsibility to handle MapReduce and HDFS operations. In the next plot, there is a plot with two sections: the preceding one is a MapReduce layer and the following one is an HDFS layer.</p><div><img src="img/3282OS_01_03.jpg" alt="Understanding the HDFS and MapReduce architecture by plot"/><div><p>The HDFS and MapReduce architecture</p></div></div><p>Hadoop is a top-level Apache project and is a very complicated Java framework. To avoid technical <a id="id129" class="indexterm"/>complications, the Hadoop<a id="id130" class="indexterm"/> community has developed a number of Java frameworks that has added an extra value to Hadoop features. They are considered as Hadoop subprojects. Here, we are departing to discuss several Hadoop components that can be considered as an abstraction of HDFS or MapReduce.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec21"/>Understanding Hadoop subprojects</h1></div></div></div><p><strong>Mahout</strong> is a<a id="id131" class="indexterm"/> popular data mining library. It takes the most popular data mining scalable <a id="id132" class="indexterm"/>machine learning algorithms for performing clustering, classification, regression, and statistical modeling to prepare intelligent applications. Also, it is a scalable machine-learning library.</p><p>Apache Mahout is distributed under a commercially friendly Apache software license. The goal of Apache Mahout is to build a vibrant, responsive, and diverse community to facilitate discussions not only on the project itself but also on potential use cases.</p><p>The following are some companies that are using Mahout:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Amazon</strong>: This a shopping portal for providing personalization recommendation</li><li class="listitem" style="list-style-type: disc"><strong>AOL</strong>: This is a shopping portal for shopping recommendations</li><li class="listitem" style="list-style-type: disc"><strong>Drupal</strong>: This is a PHP content management system using Mahout for providing open source content-based recommendation</li><li class="listitem" style="list-style-type: disc"><strong>iOffer</strong>: This is a shopping portal, which uses Mahout's Frequent Pattern Set Mining and collaborative filtering to recommend items to users</li><li class="listitem" style="list-style-type: disc"><strong>LucidWorks Big Data</strong>: This is a popular analytics firm, which uses Mahout for clustering, duplicate document detection, phase extraction, and classification</li><li class="listitem" style="list-style-type: disc"><strong>Radoop</strong>: This provides a drag-and-drop interface for Big Data analytics, including Mahout clustering and classification algorithms</li><li class="listitem" style="list-style-type: disc"><strong>Twitter</strong>: This is a social networking site, which uses Mahout's <strong>Latent Dirichlet Allocation</strong> (<strong>LDA</strong>) implementation for user interest modeling and maintains a fork of Mahout on GitHub.</li><li class="listitem" style="list-style-type: disc"><strong>Yahoo</strong>!: This is the world's most popular web service provider, which uses Mahout's Frequent Pattern Set Mining for Yahoo! Mail<div><div><h3 class="title"><a id="tip09"/>Tip</h3><p>The reference links on the Hadoop ecosystem can be found at <a class="ulink" href="http://www.revelytix.com/?q=content/hadoop-ecosystem">http://www.revelytix.com/?q=content/hadoop-ecosystem</a>.</p></div></div></li></ul></div><p><strong>Apache HBase</strong><a id="id133" class="indexterm"/> is a distributed Big Data store for Hadoop. This allows random, real-time read/write access to Big Data. This is designed as a column-oriented data storage model innovated after inspired by Google BigTable.</p><p>The following are<a id="id134" class="indexterm"/> the companies using HBase:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Yahoo!</strong>: This is the world's popular web service provider for near duplicate document detection</li><li class="listitem" style="list-style-type: disc"><strong>Twitter</strong>: This is a social networking site for version control storage and retrieval</li><li class="listitem" style="list-style-type: disc"><strong>Mahalo</strong>: This is a knowledge sharing service for similar content recommendation</li><li class="listitem" style="list-style-type: disc"><strong>NING</strong>: This is a social network service provider for real-time analytics and reporting</li><li class="listitem" style="list-style-type: disc"><strong>StumbleUpon</strong>: This is a universal personalized recommender system, real-time data storage, and data analytics platform</li><li class="listitem" style="list-style-type: disc"><strong>Veoh</strong>: This is an online multimedia content sharing platform for user profiling system<div><div><h3 class="title"><a id="tip10"/>Tip</h3><p>For Google Big Data, distributed storage system for structured data, refer the link <a class="ulink" href="http://research.google.com/archive/bigtable.html">http://research.google.com/archive/bigtable.html</a>.</p></div></div></li></ul></div><p><strong>Hive</strong><a id="id135" class="indexterm"/> is a Hadoop-based data warehousing like framework developed by Facebook. It allows users to fire queries in SQL-like languages, such as HiveQL, which are highly abstracted to Hadoop MapReduce. This allows SQL programmers with no MapReduce experience to use the warehouse and makes it easier to integrate with business intelligence and visualization tools for real-time query processing.</p><p><strong>Pig</strong><a id="id136" class="indexterm"/> is a Hadoop-based open source platform for analyzing the large scale datasets via its own SQL-like language: Pig Latin. This provides a simple operation and programming interface for massive, complex data-parallelization computation. This is also easier to develop; it's more optimized and extensible. Apache Pig has been developed by Yahoo!. Currently, Yahoo! and Twitter are the primary Pig users.</p><p>For developers, the direct use of Java APIs can be tedious or error-prone, but also limits the Java programmer's use of Hadoop programming's flexibility. So, Hadoop provides two solutions that enable making Hadoop programming for dataset management and dataset analysis with MapReduce easier—these are Pig and Hive, which are always confusing.</p><p><strong>Apache Sqoop</strong><a id="id137" class="indexterm"/> provides Hadoop data processing platform and relational databases, data warehouse, and other non-relational databases quickly transferring<a id="id138" class="indexterm"/> large amounts of data in a new way. Apache Sqoop is a mutual data tool for importing data from the relational databases to Hadoop HDFS and exporting data from HDFS to relational databases.</p><p>It works together with most modern relational databases, such as MySQL, PostgreSQL, Oracle, Microsoft SQL Server, and IBM DB2, and enterprise data warehouse. Sqoop extension API provides a way to create new connectors for the database system. Also, the Sqoop source comes up with some popular database connectors. To perform this operation, Sqoop first transforms the data into Hadoop MapReduce with some logic of database schema creation and transformation.</p><p><strong>Apache Zookeeper</strong><a id="id139" class="indexterm"/> is also a Hadoop subproject used for managing Hadoop, Hive, Pig, HBase, Solr, and other projects. Zookeeper is an open source distributed applications coordination service, which is designed with Fast Paxos algorithm-based synchronization and configuration and naming services such as maintenance of distributed applications. In programming, Zookeeper design is a very simple data model style, much like the system directory tree structure.</p><p>Zookeeper is divided into two parts: the server and client. For a cluster of Zookeeper servers, only one acts as a leader, which accepts and coordinates all rights. The rest of the servers are read-only copies of the master. If the leader server goes down, any other server can start serving all requests. Zookeeper clients are connected to a server on the Zookeeper service. The client sends a request, receives a response, accesses the observer events, and sends a heartbeat via a TCP connection with the server.</p><p>For a high-performance coordination service for distributed applications, Zookeeper is a centralized service for maintaining configuration information, naming, and providing distributed synchronization and group services. All these kinds of services are used in some form or another by distributed applications. Each time they are implemented, there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. These services lead to management complexity when the applications are deployed.</p><p>Apache Solr<a id="id140" class="indexterm"/> is an open source enterprise search platform from the Apache license project. Apache Solr is highly scalable, supporting distributed search and index replication engine. This allows building web application with powerful text search, faceted search, real-time indexing, dynamic clustering, database integration, and rich document handling.</p><p>Apache Solr is written in Java, which runs as a standalone server to serve the search results via REST-like HTTP/XML and JSON APIs. So, this Solr server can be easily integrated with an application, which is written in other programming languages. Due to all these features, this search server is used by Netflix, AOL, CNET, and Zappos.</p><p><strong>Ambari</strong><a id="id141" class="indexterm"/> is very specific to Hortonworks. Apache Ambari is a web-based tool that <a id="id142" class="indexterm"/>supports Apache Hadoop cluster supply, management, and monitoring. Ambari handles most of the Hadoop components, including HDFS, MapReduce, Hive, Pig, HBase, Zookeeper, Sqoop, and HCatlog as centralized management.</p><p>In addition, Ambari is<a id="id143" class="indexterm"/> able to install security based on the Kerberos authentication<a id="id144" class="indexterm"/> protocol over the Hadoop cluster. Also, it provides role-based user authentication, authorization, and auditing functions for users to manage integrated LDAP and Active Directory.</p></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec22"/>Summary</h1></div></div></div><p>In this chapter, we learned what is R, Hadoop, and their features, and how to install them before going on to analyzing the datasets with R and Hadoop. In the next chapter, we are going to learn what MapReduce is and how to develop MapReduce programs with Apache Hadoop.</p></div></body></html>