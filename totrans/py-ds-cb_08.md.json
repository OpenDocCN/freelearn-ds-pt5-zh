["```py\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.cross_validation import train_test_split\n\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n    print no_features,redundant_features,informative_features,repeated_features\n    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\n            n_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n    return x,y\n```", "```py\ndef build_single_model(x,y):\n    model = KNeighborsClassifier()\n    model.fit(x,y)\n    return model\n\ndef build_bagging_model(x,y):\n\tbagging = BaggingClassifier(KNeighborsClassifier(),n_estimators=100,random_state=9 \\\n             ,max_samples=1.0,max_features=0.7,bootstrap=True,bootstrap_features=True)\n\tbagging.fit(x,y)\n\treturn bagging\n\ndef view_model(model):\n    print \"\\n Sampled attributes in top 10 estimators\\n\"\n    for i,feature_set in  enumerate(model.estimators_features_[0:10]):\n        print \"estimator %d\"%(i+1),feature_set\n```", "```py\nif __name__ == \"__main__\":\n    x,y = get_data()    \n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    # Build a single model    \n    model = build_single_model(x_train,y_train)\n    predicted_y = model.predict(x_train)\n    print \"\\n Single Model Accuracy on training data\\n\"\n    print classification_report(y_train,predicted_y)\n    # Build a bag of models\n    bagging = build_bagging_model(x_train,y_train)\n    predicted_y = bagging.predict(x_train)\n    print \"\\n Bagging Model Accuracy on training data\\n\"\n    print classification_report(y_train,predicted_y)\n\tview_model(bagging)\n\n    # Look at the dev set\n    predicted_y = model.predict(x_dev)\n    print \"\\n Single Model Accuracy on Dev data\\n\"\n    print classification_report(y_dev,predicted_y)\n\n    print \"\\n Bagging Model Accuracy on Dev data\\n\"\n    predicted_y = bagging.predict(x_dev)\n    print classification_report(y_dev,predicted_y)\n```", "```py\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n x,y =make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\nn_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n```", "```py\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n```", "```py\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n```", "```py\nmodel = build_single_model(x_train,y_train)\n```", "```py\ndef build_single_model(x,y):\n    model = KNeighborsClassifier()\n    model.fit(x,y)\n    return model\n```", "```py\n    predicted_y = model.predict(x_train)\n    print \"\\n Single Model Accuracy on training data\\n\"\n    print classification_report(y_train,predicted_y)\n```", "```py\n    bagging = build_bagging_model(x_train,y_train)\n```", "```py\ndef build_bagging_model(x,y):\nbagging =             BaggingClassifier(KNeighborsClassifier(),n_estimators=100,random_state=9 \\\n           ,max_samples=1.0,max_features=0.7,bootstrap=True,bootstrap_features=True)\nbagging.fit(x,y)\nreturn bagging\n```", "```py\n    bagging = build_bagging_model(x_train,y_train)\n    predicted_y = bagging.predict(x_train)\n    print \"\\n Bagging Model Accuracy on training data\\n\"\n    print classification_report(y_train,predicted_y)\n```", "```py\n    view_model(bagging)\n```", "```py\ndef view_model(model):\n    print \"\\n Sampled attributes in top 10 estimators\\n\"\n    for i,feature_set in  enumerate(model.estimators_features_[0:10]):\n        print \"estimator %d\"%(i+1),feature_set\n```", "```py\n    # Look at the dev set\n    predicted_y = model.predict(x_dev)\n    print \"\\n Single Model Accuracy on Dev data\\n\"\n    print classification_report(y_dev,predicted_y)\n\n    print \"\\n Bagging Model Accuracy on Dev data\\n\"\n    predicted_y = bagging.predict(x_dev)\n    print classification_report(y_dev,predicted_y)\n```", "```py\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import classification_report,zero_one_loss\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n    print no_features,redundant_features,informative_features,repeated_features\n    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\n            n_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n    return x,y\n\ndef build_single_model(x,y):\n    model = DecisionTreeClassifier()\n    model.fit(x,y)\n    return model\n\ndef build_boosting_model(x,y,no_estimators=20):\n    boosting = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,min_samples_leaf=1),random_state=9 \\\n    ,n_estimators=no_estimators,algorithm=\"SAMME\")\n    boosting.fit(x,y)\n    return boosting\n\ndef view_model(model):\n    print \"\\n Estimator Weights and Error\\n\"\n    for i,weight in  enumerate(model.estimator_weights_):\n        print \"estimator %d weight = %0.4f error = %0.4f\"%(i+1,weight,model.estimator_errors_[i])\n\n    plt.figure(1)\n    plt.title(\"Model weight vs error\")\n    plt.xlabel(\"Weight\")\n    plt.ylabel(\"Error\")\n    plt.plot(model.estimator_weights_,model.estimator_errors_)\n\ndef number_estimators_vs_err_rate(x,y,x_dev,y_dev):\n    no_estimators = range(20,120,10)\n    misclassy_rate = []\n    misclassy_rate_dev = []\n\n    for no_estimator in no_estimators:\n        boosting = build_boosting_model(x,y,no_estimators=no_estimator)\n        predicted_y = boosting.predict(x)\n        predicted_y_dev = boosting.predict(x_dev)        \n        misclassy_rate.append(zero_one_loss(y,predicted_y))\n        misclassy_rate_dev.append(zero_one_loss(y_dev,predicted_y_dev))\n\n    plt.figure(2)\n    plt.title(\"No estimators vs Mis-classification rate\")\n    plt.xlabel(\"No of estimators\")\n    plt.ylabel(\"Mis-classification rate\")\n    plt.plot(no_estimators,misclassy_rate,label='Train')\n    plt.plot(no_estimators,misclassy_rate_dev,label='Dev')\n\n    plt.show() \n\nif __name__ == \"__main__\":\n    x,y = get_data()    \n    plot_data(x,y)\n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    # Build a single model    \n    model = build_single_model(x_train,y_train)\n    predicted_y = model.predict(x_train)\n    print \"\\n Single Model Accuracy on training data\\n\"\n    print classification_report(y_train,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_train,predicted_y)*100),\"%\"\n\n    # Build a bag of models\n    boosting = build_boosting_model(x_train,y_train, no_estimators=85)\n    predicted_y = boosting.predict(x_train)\n    print \"\\n Boosting Model Accuracy on training data\\n\"\n    print classification_report(y_train,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_train,predicted_y)*100),\"%\"\n\n    view_model(boosting)\n\n    # Look at the dev set\n    predicted_y = model.predict(x_dev)\n    print \"\\n Single Model Accuracy on Dev data\\n\"\n    print classification_report(y_dev,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_dev,predicted_y)*100),\"%\"\n\n    print \"\\n Boosting Model Accuracy on Dev data\\n\"\n    predicted_y = boosting.predict(x_dev)\n    print classification_report(y_dev,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_dev,predicted_y)*100),\"%\"\n\n    number_estimators_vs_err_rate(x_train,y_train,x_dev,y_dev)\n```", "```py\ndef build_single_model(x,y):\n    model = DecisionTreeClassifier()\n    model.fit(x,y)\n    return model\n\ndef build_boosting_model(x,y,no_estimators=20):\n    boosting = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,min_samples_leaf=1),random_state=9 \\\n    ,n_estimators=no_estimators,algorithm=\"SAMME\")\n    boosting.fit(x,y)\n    return boosting\n\ndef view_model(model):\n    print \"\\n Estimator Weights and Error\\n\"\n    for i,weight in  enumerate(model.estimator_weights_):\n        print \"estimator %d weight = %0.4f error = %0.4f\"%(i+1,weight,model.estimator_errors_[i])\n\n    plt.figure(1)\n    plt.title(\"Model weight vs error\")\n    plt.xlabel(\"Weight\")\n    plt.ylabel(\"Error\")\n    plt.plot(model.estimator_weights_,model.estimator_errors_)\n```", "```py\ndef number_estimators_vs_err_rate(x,y,x_dev,y_dev):\n    no_estimators = range(20,120,10)\n    misclassy_rate = []\n    misclassy_rate_dev = []\n\n    for no_estimator in no_estimators:\n        boosting = build_boosting_model(x,y,no_estimators=no_estimator)\n        predicted_y = boosting.predict(x)\n        predicted_y_dev = boosting.predict(x_dev)        \n        misclassy_rate.append(zero_one_loss(y,predicted_y))\n        misclassy_rate_dev.append(zero_one_loss(y_dev,predicted_y_dev))\n\n    plt.figure(2)\n    plt.title(\"No estimators vs Mis-classification rate\")\n    plt.xlabel(\"No of estimators\")\n    plt.ylabel(\"Mis-classification rate\")\n    plt.plot(no_estimators,misclassy_rate,label='Train')\n    plt.plot(no_estimators,misclassy_rate_dev,label='Dev')\n\n    plt.show()\n```", "```py\nif __name__ == \"__main__\":\n    x,y = get_data()    \n    plot_data(x,y)\n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    # Build a single model    \n    model = build_single_model(x_train,y_train)\n    predicted_y = model.predict(x_train)\n    print \"\\n Single Model Accuracy on training data\\n\"\n    print classification_report(y_train,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_train,predicted_y)*100),\"%\"\n\n    # Build a bag of models\n    boosting = build_boosting_model(x_train,y_train, no_estimators=85)\n    predicted_y = boosting.predict(x_train)\n    print \"\\n Boosting Model Accuracy on training data\\n\"\n    print classification_report(y_train,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_train,predicted_y)*100),\"%\"\n\n    view_model(boosting)\n\n    # Look at the dev set\n    predicted_y = model.predict(x_dev)\n    print \"\\n Single Model Accuracy on Dev data\\n\"\n    print classification_report(y_dev,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_dev,predicted_y)*100),\"%\"\n\n    print \"\\n Boosting Model Accuracy on Dev data\\n\"\n    predicted_y = boosting.predict(x_dev)\n    print classification_report(y_dev,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_dev,predicted_y)*100),\"%\"\n\n    number_estimators_vs_err_rate(x_train,y_train,x_dev,y_dev)\n```", "```py\n    no_features = 30\n    redundant_features = int(0.1*no_features)\n    informative_features = int(0.6*no_features)\n    repeated_features = int(0.1*no_features)\n x,y =make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\\\nn_informative = informative_features, n_redundant = redundant_features \\\n            ,n_repeated = repeated_features,random_state=7)\n```", "```py\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n```", "```py\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n```", "```py\n    # Build a single model    \n    model = build_single_model(x_train,y_train)\n```", "```py\ndef build_single_model(x,y):\n    model = DecisionTreeClassifier()\n    model.fit(x,y)\n    return model\n```", "```py\n    predicted_y = model.predict(x_train)\n    print \"\\n Single Model Accuracy on training data\\n\"\n    print classification_report(y_train,predicted_y)\n    print \"Fraction of misclassfication =     \n           %0.2f\"%(zero_one_loss(y_train,predicted_y)*100),\"%\"\n```", "```py\n    # Build a bag of models\n    boosting = build_boosting_model(x_train,y_train, no_estimators=85)\n```", "```py\n    boosting = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,min_samples_leaf=1),random_state=9 \\\n    ,n_estimators=no_estimators,algorithm=\"SAMME\")\n    boosting.fit(x,y)\n```", "```py\n    predicted_y = boosting.predict(x_train)\n    print \"\\n Boosting Model Accuracy on training data\\n\"\n    print classification_report(y_train,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_train,predicted_y)*100),\"%\"\n```", "```py\n    view_model(boosting)\n```", "```py\n    print \"\\n Estimator Weights and Error\\n\"\n    for i,weight in  enumerate(model.estimator_weights_):\n        print \"estimator %d weight = %0.4f error = %0.4f\"%(i+1,weight,model.estimator_errors_[i])\n```", "```py\n    plt.figure(1)\n    plt.title(\"Model weight vs error\")\n    plt.xlabel(\"Weight\")\n    plt.ylabel(\"Error\")\n    plt.plot(model.estimator_weights_,model.estimator_errors_)\n```", "```py\n    # Look at the dev set\n    predicted_y = model.predict(x_dev)\n    print \"\\n Single Model Accuracy on Dev data\\n\"\n    print classification_report(y_dev,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_dev,predicted_y)*100),\"%\"\n\n    print \"\\n Boosting Model Accuracy on Dev data\\n\"\n    predicted_y = boosting.predict(x_dev)\n    print classification_report(y_dev,predicted_y)\n    print \"Fraction of misclassfication = %0.2f\"%(zero_one_loss(y_dev,predicted_y)*100),\"%\"\n```", "```py\n    number_estimators_vs_err_rate(x_train,y_train,x_dev,y_dev)\n```", "```py\ndef number_estimators_vs_err_rate(x,y,x_dev,y_dev):\n    no_estimators = range(20,120,10)\n    misclassy_rate = []\n    misclassy_rate_dev = []\n\n    for no_estimator in no_estimators:\n        boosting = build_boosting_model(x,y,no_estimators=no_estimator)\n        predicted_y = boosting.predict(x)\n        predicted_y_dev = boosting.predict(x_dev)        \n        misclassy_rate.append(zero_one_loss(y,predicted_y))\n        misclassy_rate_dev.append(zero_one_loss(y_dev,predicted_y_dev))\n\n    plt.figure(2)\n    plt.title(\"No estimators vs Mis-classification rate\")\n    plt.xlabel(\"No of estimators\")\n    plt.ylabel(\"Mis-classification rate\")\n    plt.plot(no_estimators,misclassy_rate,label='Train')\n    plt.plot(no_estimators,misclassy_rate_dev,label='Dev')\n\n    plt.show()\n```", "```py\n# Load libraries\nfrom sklearn.datasets import load_boston\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef get_data():\n    \"\"\"\n    Return boston dataset\n    as x - predictor and\n    y - response variable\n    \"\"\"\n    data = load_boston()\n    x    = data['data']\n    y    = data['target']\n    return x,y    \n\ndef build_model(x,y,n_estimators=500):\n    \"\"\"\n    Build a Gradient Boost regression model\n    \"\"\"\n    model = GradientBoostingRegressor(n_estimators=n_estimators,verbose=10,\\\n            subsample=0.7, learning_rate= 0.15,max_depth=3,random_state=77)\n    model.fit(x,y)\n    return model    \n\ndef view_model(model):\n    \"\"\"\n    \"\"\"\n    print \"\\n Training scores\"\n    print \"======================\\n\"\n    for i,score in enumerate(model.train_score_):\n        print \"\\tEstimator %d score %0.3f\"%(i+1,score)\n\n    plt.cla()\n    plt.figure(1)\n    plt.plot(range(1,model.estimators_.shape[0]+1),model.train_score_)\n    plt.xlabel(\"Model Sequence\")\n    plt.ylabel(\"Model Score\")\n    plt.show()\n\n    print \"\\n Feature Importance\"\n    print \"======================\\n\"\n    for i,score in enumerate(model.feature_importances_):\n        print \"\\tFeature %d Importance %0.3f\"%(i+1,score)\n\ndef model_worth(true_y,predicted_y):\n    \"\"\"\n    Evaluate the model\n    \"\"\"\n    print \"\\tMean squared error = %0.2f\"%(mean_squared_error(true_y,predicted_y))\n\nif __name__ == \"__main__\":\n\n    x,y = get_data()\n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    #Prepare some polynomial features\n    poly_features = PolynomialFeatures(2,interaction_only=True)\n    poly_features.fit(x_train)\n    x_train_poly = poly_features.transform(x_train)\n    x_dev_poly   = poly_features.transform(x_dev)\n\n    # Build model with polynomial features\n    model_poly = build_model(x_train_poly,y_train)\n    predicted_y = model_poly.predict(x_train_poly)\n    print \"\\n Model Performance in Training set (Polynomial features)\\n\"\n    model_worth(y_train,predicted_y)  \n\n    # View model details\n    view_model(model_poly)\n\n    # Apply the model on dev set\n    predicted_y = model_poly.predict(x_dev_poly)\n    print \"\\n Model Performance in Dev set  (Polynomial features)\\n\"\n    model_worth(y_dev,predicted_y)  \n\n    # Apply the model on Test set\n    x_test_poly = poly_features.transform(x_test)\n    predicted_y = model_poly.predict(x_test_poly)\n\n    print \"\\n Model Performance in Test set  (Polynomial features)\\n\"\n    model_worth(y_test,predicted_y)  \n```", "```py\ndef build_model(x,y,n_estimators=500):\n    \"\"\"\n    Build a Gradient Boost regression model\n    \"\"\"\n    model = GradientBoostingRegressor(n_estimators=n_estimators,verbose=10,\\\n            subsample=0.7, learning_rate= 0.15,max_depth=3,random_state=77)\n    model.fit(x,y)\n    return model    \n\ndef view_model(model):\n    \"\"\"\n    \"\"\"\n    print \"\\n Training scores\"\n    print \"======================\\n\"\n    for i,score in enumerate(model.train_score_):\n        print \"\\tEstimator %d score %0.3f\"%(i+1,score)\n\n    plt.cla()\n    plt.figure(1)\n    plt.plot(range(1,model.estimators_.shape[0]+1),model.train_score_)\n    plt.xlabel(\"Model Sequence\")\n    plt.ylabel(\"Model Score\")\n    plt.show()\n\n    print \"\\n Feature Importance\"\n    print \"======================\\n\"\n    for i,score in enumerate(model.feature_importances_):\n        print \"\\tFeature %d Importance %0.3f\"%(i+1,score)\n\ndef model_worth(true_y,predicted_y):\n    \"\"\"\n    Evaluate the model\n    \"\"\"\n    print \"\\tMean squared error = %0.2f\"%(mean_squared_error(true_y,predicted_y))\n```", "```py\nif __name__ == \"__main__\":\n\n    x,y = get_data()\n\n    # Divide the data into Train, dev and test    \n    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)\n    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n\n    #Prepare some polynomial features\n    poly_features = PolynomialFeatures(2,interaction_only=True)\n    poly_features.fit(x_train)\n    x_train_poly = poly_features.transform(x_train)\n    x_dev_poly   = poly_features.transform(x_dev)\n\n    # Build model with polynomial features\n    model_poly = build_model(x_train_poly,y_train)\n    predicted_y = model_poly.predict(x_train_poly)\n    print \"\\n Model Performance in Training set (Polynomial features)\\n\"\n    model_worth(y_train,predicted_y)  \n\n    # View model details\n    view_model(model_poly)\n\n    # Apply the model on dev set\n    predicted_y = model_poly.predict(x_dev_poly)\n    print \"\\n Model Performance in Dev set  (Polynomial features)\\n\"\n    model_worth(y_dev,predicted_y)  \n\n    # Apply the model on Test set\n    x_test_poly = poly_features.transform(x_test)\n    predicted_y = model_poly.predict(x_test_poly)\n\n    print \"\\n Model Performance in Test set  (Polynomial features)\\n\"\n    model_worth(y_test,predicted_y)  \n```", "```py\ndef get_data():\n    \"\"\"\n    Return boston dataset\n    as x - predictor and\n    y - response variable\n    \"\"\"\n    data = load_boston()\n    x    = data['data']\n    y    = data['target']\n    return x,y    \n```", "```py\nx_train,x_test_all,y_train,y_test_all = \ntrain_test_split(x,y,test_size = 0.3,random_state=9)\n```", "```py\nx_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)\n```", "```py\npoly_features = PolynomialFeatures(interaction_only=True)\npoly_features.fit(x_train)\n```", "```py\nx_train_poly = poly_features.transform(x_train)\nx_dev_poly = poly_features.transform(x_dev)\nx_test_poly = poly_features.transform(x_test)\n```", "```py\n    # Build model with polynomial features\n    model_poly = build_model(x_train_poly,y_train)\n```", "```py\n    model = GradientBoostingRegressor(n_estimators=n_estimators,verbose=10,\\\n            subsample=0.7, learning_rate= 0.15,max_depth=3,random_state=77)\n```", "```py\n    predicted_y = model_poly.predict(x_train_poly)\n    print \"\\n Model Performance in Training set (Polynomial features)\\n\"\n    model_worth(y_train,predicted_y)  \n```", "```py\nprint \"\\n Training scores\"\nprint \"======================\\n\"\nfor i,score in enumerate(model.train_score_):\nprint \"\\tEstimator %d score %0.3f\"%(i+1,score)\n```", "```py\nplt.cla()\nplt.figure(1)\nplt.plot(range(1,model.estimators_.shape[0]+1),model.train_score_)\nplt.xlabel(\"Model Sequence\")\nplt.ylabel(\"Model Score\")\nplt.show()\n```", "```py\n    print \"\\n Feature Importance\"\n    print \"======================\\n\"\n    for i,score in enumerate(model.feature_importances_):\n        print \"\\tFeature %d Importance %0.3f\"%(i+1,score)\n```", "```py\n    # Apply the model on dev set\n    predicted_y = model_poly.predict(x_dev_poly)\n    print \"\\n Model Performance in Dev set  (Polynomial features)\\n\"\n    model_worth(y_dev,predicted_y)  \n```"]