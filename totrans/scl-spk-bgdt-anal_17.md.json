["```py\nShuffle memory= Heap Size * spark.shuffle.safetyFraction * spark.shuffle.memoryFraction\n\n```", "```py\nUnroll memory = spark.storage.unrollFraction * spark.storage.memoryFraction * spark.storage.safetyFraction\n\n```", "```py\nspark://<hostname>:7077\n\n```", "```py\nmesos://<hostname>:5050\n\n```", "```py\nmesos://zk://<hostname>:2181\n\n```", "```py\n$ spark-submit [options] <app-jar | python-file> [app arguments]\n\n```", "```py\n$ SPARK_HOME/bin/spark-submit \n --class com.chapter15.Clustering.KMeansDemo \n --master local[8] \n KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar \n Saratoga_NY_Homes.txt\n\n```", "```py\n$ SPARK_HOME/sbin/start-master.sh\n\n```", "```py\nStarting org.apache.spark.deploy.master.Master, logging to <SPARK_HOME>/logs/spark-asif-org.apache.spark.deploy.master.Master-1-ubuntu.out\n\n```", "```py\nSPARK_MASTER_WEBUI_PORT=8080\n\n```", "```py\n$ sudo chmod +x SPARK_HOME/sbin/start-master.sh.\n\n```", "```py\n$ SPARK_HOME/sbin/start-slave.sh <master-spark-URL>\n\n```", "```py\nStarting org.apache.spark.deploy.worker.Worker, logging to <SPARK_HOME>//logs/spark-asif-org.apache.spark.deploy.worker.Worker-1-ubuntu.out \n\n```", "```py\n$ SPARK_HOME/bin/spark-submit  \n--class \"com.chapter15.Clustering.KMeansDemo\"  \n--master spark://ubuntu:7077   \nKMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar  \nSaratoga_NY_Homes.txt\n\n```", "```py\n$  cd /home\n$  wget http://mirrors.ibiblio.org/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\n\n```", "```py\n$  mkdir –p /opt/yarn\n$  cd /opt/yarn\n$  tar xvzf /root/hadoop-2.7.3.tar.gz\n\n```", "```py\n$  groupadd hadoop\n$  useradd -g hadoop yarn\n$  useradd -g hadoop hdfs\n$  useradd -g hadoop mapred\n\n```", "```py\n$  mkdir -p /var/data/hadoop/hdfs/nn\n$  mkdir -p /var/data/hadoop/hdfs/snn\n$  mkdir -p /var/data/hadoop/hdfs/dn\n$  chown hdfs:hadoop /var/data/hadoop/hdfs –R\n$  mkdir -p /var/log/hadoop/yarn\n$  chown yarn:hadoop /var/log/hadoop/yarn -R\n\n```", "```py\n$  cd /opt/yarn/hadoop-2.7.3\n$  mkdir logs\n$  chmod g+w logs\n$  chown yarn:hadoop . -R\n\n```", "```py\n<configuration>\n       <property>\n               <name>fs.default.name</name>\n               <value>hdfs://localhost:9000</value>\n       </property>\n       <property>\n               <name>hadoop.http.staticuser.user</name>\n               <value>hdfs</value>\n       </property>\n</configuration>\n\n```", "```py\n<configuration>\n <property>\n   <name>dfs.replication</name>\n   <value>1</value>\n </property>\n <property>\n   <name>dfs.namenode.name.dir</name>\n   <value>file:/var/data/hadoop/hdfs/nn</value>\n </property>\n <property>\n   <name>fs.checkpoint.dir</name>\n   <value>file:/var/data/hadoop/hdfs/snn</value>\n </property>\n <property>\n   <name>fs.checkpoint.edits.dir</name>\n   <value>file:/var/data/hadoop/hdfs/snn</value>\n </property>\n <property>\n   <name>dfs.datanode.data.dir</name>\n   <value>file:/var/data/hadoop/hdfs/dn</value>\n </property>\n</configuration>\n\n```", "```py\n$  cp mapred-site.xml.template mapred-site.xml\n\n```", "```py\n<configuration>\n<property>\n   <name>mapreduce.framework.name</name>\n   <value>yarn</value>\n </property>\n</configuration>\n\n```", "```py\n<configuration>\n<property>\n   <name>yarn.nodemanager.aux-services</name>\n   <value>mapreduce_shuffle</value>\n </property>\n <property>\n   <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n   <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n </property>\n</configuration>\n\n```", "```py\nHADOOP_HEAPSIZE=\"500\"\nHADOOP_NAMENODE_INIT_HEAPSIZE=\"500\"\n\n```", "```py\nHADOOP_JOB_HISTORYSERVER_HEAPSIZE=250\n\n```", "```py\nJAVA_HEAP_MAX=-Xmx500m\nYARN_HEAPSIZE=500\n\n```", "```py\n$  su - hdfs\n$ cd /opt/yarn/hadoop-2.7.3/bin\n$ ./hdfs namenode -format\n\n```", "```py\nINFO common.Storage: Storage directory /var/data/hadoop/hdfs/nn has been successfully formatted\n\n```", "```py\n$ cd ../sbin\n$ ./hadoop-daemon.sh start namenode\n\n```", "```py\nstarting namenode, logging to /opt/yarn/hadoop-2.7.3/logs/hadoop-hdfs-namenode-limulus.out\n\n```", "```py\n$ ./hadoop-daemon.sh start secondarynamenode\n\n```", "```py\nStarting secondarynamenode, logging to /opt/yarn/hadoop-2.7.3/logs/hadoop-hdfs-secondarynamenode-limulus.out\n\n```", "```py\n$ ./hadoop-daemon.sh start datanode\n\n```", "```py\nstarting datanode, logging to /opt/yarn/hadoop-2.7.3/logs/hadoop-hdfs-datanode-limulus.out\n\n```", "```py\n$ jps\n\n```", "```py\n35180 SecondaryNameNode\n45915 NameNode\n656335 Jps\n75814 DataNode\n\n```", "```py\n$  su - yarn\n$ cd /opt/yarn/hadoop-2.7.3/sbin\n$ ./yarn-daemon.sh start resourcemanager\n\n```", "```py\nstarting resourcemanager, logging to /opt/yarn/hadoop-2.7.3/logs/yarn-yarn-resourcemanager-limulus.out\n\n```", "```py\n$ ./yarn-daemon.sh start nodemanager\n\n```", "```py\nstarting nodemanager, logging to /opt/yarn/hadoop-2.7.3/logs/yarn-yarn-nodemanager-limulus.out\n\n```", "```py\n$ ./yarn-daemon.sh stop nodemanager\n$ ./yarn-daemon.sh stop resourcemanager\n\n```", "```py\n$ SPARK_HOME/bin/spark-submit --classpath.to.your.Class --master yarn --deploy-mode cluster [options] <app jar> [app options]\n\n```", "```py\n$ SPARK_HOME/bin/spark-submit  \n    --class \"com.chapter15.Clustering.KMeansDemo\"  \n    --master yarn  \n    --deploy-mode cluster  \n    --driver-memory 16g  \n    --executor-memory 4g  \n    --executor-cores 4  \n    --queue the_queue  \n    KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar  \n    Saratoga_NY_Homes.txt\n\n```", "```py\n$ SPARK_HOME/bin/spark-shell --master yarn --deploy-mode client\n\n```", "```py\n$ SPARK_HOME/bin/spark-submit   \n    --class \"com.chapter13.Clustering.KMeansDemo\"  \n    --master yarn  \n    --deploy-mode cluster  \n    --driver-memory 16g  \n    --executor-memory 4g  \n    --executor-cores 4  \n    --queue the_queue  \n    --conf spark.dynamicAllocation.enabled=true  \n    --conf spark.shuffle.service.enabled=true  \n    --conf spark.dynamicAllocation.minExecutors=1  \n    --conf spark.dynamicAllocation.maxExecutors=4  \n    --conf spark.dynamicAllocation.initialExecutors=4  \n    --conf spark.executor.instances=4  \n    KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar  \n    Saratoga_NY_Homes.txt\n\n```", "```py\n$ export MESOS_NATIVE_JAVA_LIBRARY=<path to libmesos.so>\n\n```", "```py\n$ export SPARK_EXECUTOR_URI=<URL of spark-2.1.0.tar.gz uploaded above>\n\n```", "```py\nval conf = new SparkConf()              \n                   .setMaster(\"mesos://HOST:5050\")  \n                   .setAppName(\"My app\")             \n                  .set(\"spark.executor.uri\", \"<path to spark-2.1.0.tar.gz uploaded above>\")\nval sc = new SparkContext(conf)\n\n```", "```py\n$ SPARK_HOME/bin/spark-shell --master mesos://host:5050\n\n```", "```py\n$ SPARK_HOME /bin/spark-class org.apache.spark.deploy.mesos.MesosClusterDispatcher\n\n```", "```py\n$ SPARK_HOME/bin/spark-submit   \n--class com.chapter13.Clustering.KMeansDemo   \n--master mesos://207.184.161.138:7077    \n--deploy-mode cluster   \n--supervise   \n--executor-memory 20G   \n--total-executor-cores 100   \nKMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar   \nSaratoga_NY_Homes.txt\n\n```", "```py\n$ SPARK_HOME/bin/spark-submit --class my.main.Class    \n     --master yarn    \n     --deploy-mode cluster    \n     --jars my-other-jar.jar, my-other-other-jar.jar    \n     my-main-jar.jar    \n     app_arg1 app_arg2\n\n```", "```py\n$ sudo chmod 400 /usr/local/key/aws_key_pair.pem\n\n```", "```py\n$ echo \"export AWS_ACCESS_KEY_ID=<access_key_id>\" >> ~/.bashrc \n$ echo \" export AWS_SECRET_ACCESS_KEY=<secret_access_key_id>\" >> ~/.bashrc \n$ source ~/.bashrc\n\n```", "```py\n$ SPARK_HOME/spark-ec2 \n--key-pair=<name_of_the_key_pair> \n--identity-file=<path_of_the key_pair>  \n--instance-type=<AWS_instance_type > \n--region=<region> zone=<zone> \n--slaves=<number_of_slaves> \n--hadoop-major-version=<Hadoop_version> \n--spark-version=<spark_version> \n--instance-profile-name=<profile_name>\nlaunch <cluster-name>\n\n```", "```py\n$ SPARK_HOME/spark-ec2 \n --key-pair=aws_key_pair \n --identity-file=/usr/local/aws_key_pair.pem \n --instance-type=m3.2xlarge \n--region=eu-west-1 --zone=eu-west-1a --slaves=2 \n--hadoop-major-version=yarn \n--spark-version=2.1.0 \n--instance-profile-name=rezacsedu_aws\nlaunch ec2-spark-cluster-1\n\n```", "```py\n$ SPARK_HOME/spark-ec2 \n--key-pair=<name_of_the_key_pair> \n--identity-file=<path_of_the _key_pair> \n--region=<region> \n--zone=<zone>\nlogin <cluster-name> \n\n```", "```py\n$ SPARK_HOME/spark-ec2 \n--key-pair=my-key-pair \n--identity-file=/usr/local/key/aws-key-pair.pem \n--region=eu-west-1 \n--zone=eu-west-1\nlogin ec2-spark-cluster-1\n\n```", "```py\n$ scp -i /usr/local/key/aws-key-pair.pem /usr/local/code/KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar ec2-user@ec2-52-18-252-59.eu-west-1.compute.amazonaws.com:/home/ec2-user/\n\n```", "```py\n$ scp -i /usr/local/key/aws-key-pair.pem /usr/local/data/Saratoga_NY_Homes.txt ec2-user@ec2-52-18-252-59.eu-west-1.compute.amazonaws.com:/home/ec2-user/\n\n```", "```py\n$SPARK_HOME/bin/spark-submit \n --class com.chapter13.Clustering.KMeansDemo \n--master spark://ec2-52-48-119-121.eu-west-1.compute.amazonaws.com:7077 \nfile:///home/ec2-user/KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar \nfile:///home/ec2-user/Saratoga_NY_Homes.txt\n\n```", "```py\n$SPARK_HOME/bin/spark-submit \n --class com.chapter13.Clustering.KMeansDemo \n--master spark://ec2-52-48-119-121.eu-west-1.compute.amazonaws.com:7077 \nhdfs://localhost:9000/KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar \nhdfs://localhost:9000//Saratoga_NY_Homes.txt\n\n```", "```py\n$ SPARK_HOME/ec2/spark-ec2 --region=<ec2-region> stop <cluster-name>\n\n```", "```py\n$ SPARK_HOME/ec2/spark-ec2 --region=eu-west-1 stop ec2-spark-cluster-1\n\n```", "```py\n$ SPARK_HOME/ec2/spark-ec2 -i <key-file> --region=<ec2-region> start <cluster-name>\n\n```", "```py\n$ SPARK_HOME/ec2/spark-ec2 --identity-file=/usr/local/key/-key-pair.pem --region=eu-west-1 start ec2-spark-cluster-1\n\n```", "```py\n$ SPARK_HOME/ec2/spark-ec2 destroy <cluster-name>\n\n```", "```py\n$ SPARK_HOME /spark-ec2 --region=eu-west-1 destroy ec2-spark-cluster-1\n\n```"]