- en: Drivers behind Marketing Engagement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you run marketing campaigns, one of the important measures that you will
    want to look at and analyze is customer engagement with your marketing efforts.
    For example, in email marketing, customer engagement can be measured by how many
    of your marketing emails were opened or ignored by your customers. Customer engagement
    can also be measured by the amount of website visits from individual customers.
    Successful marketing campaigns will draw a lot of engagement from your customers,
    while ineffective marketing campaigns will not only drive a lower amount of engagement
    from your customers, but will also negatively impact your business. Customers
    might mark emails from your business as spam or unsubscribe from your mailing
    list.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand what affects customer engagement, in this chapter, we
    will discuss how we can use explanatory analysis (more specifically, regression
    analysis). We will briefly cover the definition of explanatory analysis, what
    regression analysis is, and how to use a logistic regression model for explanatory
    analysis. Then, we will cover how to build and interpret regression analysis results
    in Python, using the `statsmodels` package. For R programmers, we will discuss
    how we can build and interpret regression analysis results with `glm`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using regression analysis for explanatory analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression analysis with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression analysis with R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using regression analysis for explanatory analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml), *Key Performance
    Indicators and Visualizations*, we discussed what **descriptive analysis** is
    and how it is used to better understand a dataset. We experimented using various
    visualization techniques and building different types of plots in Python and R.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to expand our knowledge and start to discuss why,
    when, and how to use **explanatory analysis** for marketing.
  prefs: []
  type: TYPE_NORMAL
- en: Explanatory analysis and regression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we briefly discussed in [Chapter 1](c169428b-e0db-4624-896c-24316e9b29cc.xhtml),
    *Data Science and Marketing*, the purpose of explanatory analysis is to answer
    why we are using the data, whereas the purpose of descriptive analysis is to answer
    what we are using the data for, and how we are using it. When you run different
    marketing campaigns, often times, you will notice that some marketing campaigns
    perform much better than others; you might wonder why it is that some of your
    marketing campaigns work so well, while others do not. For example, you might
    want to understand what types and groups of customers typically open your marketing
    emails more often than others. As another example, you might want to analyze what
    attributes of the customer base are highly correlated with higher conversion rates
    and item purchases.
  prefs: []
  type: TYPE_NORMAL
- en: 'With explanatory analysis, you can analyze and understand the key factors that
    are highly and significantly correlated with the outcomes that you want. **Regression
    analysis** and regression models are frequently used to model the relationships
    between the attributes and the outcomes. Simply put, regression analysis estimates
    the values of output variables by finding a function of the attributes or features
    that best approximates the output values. One of the frequently used forms of
    regression analysis is **linear regression**. As the name suggests, in linear
    regression, we try to estimate the output variables via linear combinations of
    the features. If we use *Y* for the output variable and *X[i]* for each of the
    features, where *i* is the *i*th feature, then the linear regression formula will
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4141c7bf-bbc5-4f2d-9b36-2c3877c5a77c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the preceding formula, the output variable *Y* is expressed
    as a linear combination of the features, *X[i]*. The purpose of the linear regression
    models is to find the intercept, *a*, and the coefficients, *b[i]*, that best
    estimate the output variable, using the given features. A fitted linear regression
    line will look something like the following (image from [https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2](https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d60fa219-a286-4b2c-a92e-209e6f4d3109.png)'
  prefs: []
  type: TYPE_IMG
- en: The blue dots in this diagram are the data points, and the red line is the fitted,
    or trained, linear regression line. As you can see in the graph, linear regression
    tries to estimate the target variable through a linear combination of the features.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how we can use regression analysis, and, more
    specifically, **logistic regression** models, to understand what drives higher
    customer engagement.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Logistic regression** is a type of regression analysis that is used when
    the output variable is binary (one for a positive outcome versus zero for a negative
    outcome). Like any other linear regression models, logistic regression models
    estimate the output from linear combinations of the feature variables. The only
    difference is what the model estimates. Unlike other linear regression models,
    logistic regression models estimate the log odds of an event, or, in other words,
    the log ratios between the probabilities of positive and negative events. The
    equation looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1946fce3-7d03-485d-a545-bd55a8d47e10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The ratio on the left is the odds of success, which represents the ratio between
    the probability of success and the probability of failure. The curve of the log
    odds, also called the **logit curve**, looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef7f8d76-83b1-4317-8df0-623cc1512349.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The logistic regression model output is simply the inverse of logit, which
    ranges from zero to one. In this chapter, we are going to use regression analysis
    to understand what drives customer engagement, and the output variable will be
    whether a customer responded to marketing calls. Hence, logistic regression fits
    perfectly in this case, as the output is a binary variable that can take two values:
    responded versus did not respond. In the following sections, we will discuss how
    we can use and build logistic regression models in Python and R, and then we will
    cover how we can interpret regression analysis results in order to understand
    what attributes of customers are highly correlated with higher marketing engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how to use the `statsmodels` package in Python
    to conduct regression analysis. For those readers that would like to use R instead
    of Python, for this exercise, you can skip to the next section. We will start
    this section by looking at the data more closely, using the `pandas` and `matplotlib`
    packages, and then we will discuss how to build regression models and interpret
    the results by using the `statsmodels` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, we will be using one of the publicly available datasets
    from IBM Watson, which can be found at [https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/](https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/).
    You can follow the link and download the data file in a CSV format. In order to
    load this data into your Jupyter Notebook, you can run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Similar to what we did in [Chapter 2](1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml),
    *Key Performance Indicators and Visualizations*, we are importing the `matplotlib`
    and `pandas` packages first; using the `read_csv` function in `pandas`, we can
    read the data into a `pandas` DataFrame. We will use `matplotlib` later, for data
    analysis and visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loaded DataFrame, `df`, looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/333d7a69-95a2-4044-bcc1-5b6e92b72fc6.png)'
  prefs: []
  type: TYPE_IMG
- en: As we discussed in [Chapter 2](1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml), *Key
    Performance Indicators and Visualizations*, a DataFrame `shape` attribute tells
    us the number of rows and columns in the DataFrame, and the `head` function will
    display the first five records of the dataset. Once you have successfully read
    the data into a `pandas` DataFrame, your data should look like it does in the
    screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive into regression analysis, we will first take a more detailed
    look at the data, in order to have a better understanding of what data points
    we have and what patterns we can see in the data. If you look at the data, you
    will notice a column named `Response`. It contains information on whether a customer
    responded to marketing calls. We will use this field as a measure of customer
    engagement. For future computations, it will be better to encode this field with
    numerical values. Let''s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in this code, using the `apply` function of a `pandas` DataFrame,
    we are encoding those who did not respond to marketing calls (`No`) with a value
    of `0` and those who did respond (`Yes`) with a value of `1`. We are creating
    a new field named `Engaged` with these encoded values.
  prefs: []
  type: TYPE_NORMAL
- en: Engagement rate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing that we are going to look at is the aggregate engagement rate.
    This engagement rate is simply the percentage of customers that responded to the
    marketing calls. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from this code, we are grouping by the newly created field,
    `Engaged`, using the `groupby` function of a `pandas` DataFrame. Then, we are
    counting the number of records (or customers) in each `Engaged` group with the `count`
    function. By dividing by the total number of customers in the DataFrame and multiplying
    by `100.0`, we get the engagement rate. The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97193318-df6f-461f-9404-86cb8ed4b87b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To make this easier to read, we can transpose the DataFrame, meaning that we
    can flip the rows and columns in the DataFrame. You can transpose a `pandas` DataFrame
    by using the `T` attribute of a DataFrame. It looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04dd8f15-66c7-4bdd-a2c3-1e04839e1447.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, about 14% of the customers have responded to marketing calls,
    and the remaining 86% of the customers have not responded.
  prefs: []
  type: TYPE_NORMAL
- en: Sales channels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s see whether we can find any noticeable patterns in the sales channel
    and engagement. We are going to analyze how the engaged and non-engaged customers
    are distributed among different sales channels. Let''s first look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code snippet, we are using the `pivot_table` function
    in the `pandas` library to group by the `Sales Channel` and `Response` variables.
    Once you run this code, `engagement_by_sales_channel_df` will have the following
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0cd77263-2dc4-4680-b552-5a1d92ed0bcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you will have noticed in the previous section, there are significantly more
    customers that are not engaged with the marketing efforts, so it is quite difficult
    to look at the differences in the sales channel distributions between the engaged
    and non-engaged customers from raw numbers. To make the differences more visually
    identifiable, we can build pie charts using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you run this code, you will see the following pie charts, which show the
    distributions of engaged and non-engaged customers across different sales channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e5402ab-645b-41ea-92ef-4e00ba53ac02.png)'
  prefs: []
  type: TYPE_IMG
- en: Compared to the previous table that shows raw counts of engaged and non-engaged
    customers in each sales channel, these pie charts help us to visually spot the
    differences in the distributions more easily. As you can see from these charts,
    more than half of the engaged customers were from agents, whereas non-engaged
    customers are more evenly distributed across all four different channels. As you
    can see from these charts, analyzing and visualizing data can help us to notice
    interesting patterns in the data, which will further help when we run regression
    analysis in the later parts of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Total claim amounts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last thing that we are going to look at before we dive into the regression
    analysis are the differences in the distributions of `Total Claim Amount` between
    the engaged and non-engaged groups. We are going to visualize this by using box
    plots. Let''s first look at how we can build box plots in Python, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code, it is quite straightforward to build box plots
    from a `pandas` DataFrame. You can simply call the `boxplot` function. Box plots
    are a great way to visualize the distributions of continuous variables. They show
    the min, max, first quartile, median, and third quartile, all in one view. The
    following box plots show the distributions of the `Total Claim Amount` between
    the engaged and non-engaged groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c38b00b9-e2ca-4edd-a34a-cf67ba11ea74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The central rectangle spans from the first quartile to the third quartile,
    and the green line shows the median. The lower and upper ends show the minimum
    and maximum of the distribution, respectively. One thing to note from the previous
    code is the `showfliers=False` argument. Let''s see what happens when we set that
    argument to `True`, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this code and the `showfliers=True` flag, the resulting box plots now
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ff9f5b8-a67b-4e31-b2df-6f1c2661ac25.png)'
  prefs: []
  type: TYPE_IMG
- en: As you notice in these box plots, they plot many dots above the upper boundary
    lines, which suggested maximum values in the previous box plots. The dots above
    the upper boundary line show the suspected outliers that are decided based on
    the **Interquartile range** (**IQR**). The IQR is simply the range between the
    first and third quartiles, and the points that fall `1.5*IQR` above the third
    quartile or `1.5*IQR` below the first quartile are suspected outliers and are
    shown with the dots.
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have analyzed the types of fields that we have in the data and how
    the patterns differ between the engaged group and the non-engaged group. Now,
    we are going to discuss how to conduct and interpret regression analysis in Python
    by using the `statsmodels` package. We will first build a logistic regression
    model with continuous variables, and you'll learn how to interpret the results.
    Then, we are going to discuss different ways to handle categorical variables when
    fitting regression models, and what impact those categorical variables have on
    the fitted logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In linear regression, including logistic regression, it is straightforward
    to fit a regression model when the feature variables are continuous, as it just
    needs to find a linear combination of feature variables with numerical values
    for estimating the output variables. In order to fit a regression model with continuous
    variables, let''s first take a look at how to get the data types of the columns
    in a `pandas` DataFrame. Take a look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13cfa30a-b8b2-45db-8b1b-cc3b0b115cd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from this Jupyter Notebook screenshot, the `dtype` attribute
    of a `pandas` `Series` object tells you what type of data it contains. As you
    can see from this snapshot, the `Income` variable has integers and the `Customer
    Lifetime Value` feature has floating point numbers. In order to take a quick look
    at the distributions of variables with numerical values, you can also do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58348fa5-b7e6-49c3-b9de-5175614c4b24.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in this Jupyter Notebook snapshot, the `describe` function of
    a `pandas` DataFrame shows the distributions of all of the columns with numerical
    values. For example, you can see that there are a total of `9134` records in the `Customer
    Lifetime Value` column, with a mean of `8004.94` and ranges from `1898.01` to
    `83325.38`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to store this list of the names of continuous variables in a separate
    variable, named `continuous_vars`. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know which columns are continuous variables, let''s start to fit
    a logistic regression model. In order to do that, we need to first import the `statsmodels`
    package, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `statsmodels` package imported, the code to initiate a logistic regression
    model is quite simple, and looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from this code, we are using the `Logit` function within the
    `statsmodels` package. We are supplying the `Engaged` column as the output variable,
    which the model will learn to estimate, and the `continuous_vars` that contain
    all of the continuous variables as the input variables. Once a logistic regression
    object is created with the output and input variables defined, we can train or
    fit this model by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code, we are using the `fit` function of the logistic
    regression object, `logit`, to train a logistic regression model. Once this code
    is run, the trained model, `logit_fit`, will have learned the optimal solution
    that best estimates the output variable, `Engaged`, by using the input variables.
    In order to get a detailed description of the trained model, you can use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, the `summary` function will display the following output
    in the Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39146c4c-eacf-42ab-80f4-4801a8e3b76b.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's take a closer look at this model output. `coef` represents the coefficients
    for each of the input variables, and `z` represents the *z*-score, which is the
    number of standard deviations from the mean. The `P>|z|` column represents the
    *p*-value, which means how likely it is to observe the relationship between the
    feature and the output variable by chance. So, the lower the value of `P>|z|`
    is, the more likely it is that the relationship between the given feature and
    the output variable is strong and is not by chance. Typically, `0.05` is a good
    cut-off point for the *p*-value, and any value less than `0.05` signifies a strong
    relationship between the given feature and the output variable.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this model output, we can see that `Income`, `Monthly Premium Auto`,
    `Months Since Last Claim`, `Months Since Policy Inception`, and `Number of Policies` variables
    have significant relationships with the output variable, `Engaged`. For example, `Number
    of Policies` variable is significant and is negatively correlated with `Engaged`.
    This suggests that the more policies that the customers have, the less likely
    they are to respond to marketing calls. As another example, the `Months Since
    Last Claim` variable is significant and is negatively correlated with the output
    variable, `Engaged`. This means that the longer it has been since the last claim,
    the less likely that the customer is going to respond to marketing calls.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from these examples, you can interpret the regression analysis
    results quite easily by looking at the *p*-values and coefficients of the features
    from the model output. This is a good way to understand which attributes of customers
    are significantly and highly correlated with your outcomes of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you saw in the case of continuous variables in the previous section, it
    is quite straightforward to understand the relationships between the input and
    output variables from the coefficients and *p*-values. However, it becomes not
    so straightforward when we introduce **categorical variables**. Categorical variables
    often do not have any natural order, or they are encoded with non-numerical values,
    but in linear regression, we need the input variables to have numerical values
    that signify the order or magnitudes of the variables. For example, we cannot
    easily encode the `State` variable in our dataset with certain orderings or values.
    That is why we need to handle categorical variables differently from continuous
    variables when conducting regression analysis. In Python, there are multiple ways
    to handle categorical variables when using the `pandas` package. Let''s first
    look at factorizing categorical variables, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pandas` function, `factorize`, encodes categorical variables with numerical
    values by enumerating through the values. Let''s take a look at the following
    output first:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bd1e57a-4ea2-4d5d-b7a6-96c59a34773e.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from this output, the values of this `Gender` variable are encoded
    with zeros and ones, where `0` symbolizes female (`F`) and `1` symbolizes male
    (`M`). This is a quick way to encode categorical variables with numerical values.
    However, this function does not work when we want to embed natural orderings into
    the encoded values. For example, the `Education` variable in our dataset has five
    different categories: `High School or Below`, `Bachelor`, `College`, `Master`,
    and `Doctor`. We might want to embed the orderings when encoding different categories
    within this `Education` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows another way to encode categorical variables with orderings
    when using `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code, we are using the `pd.Categorical` function to
    encode the values of `df[''Education'']`. We can define the orderings that we
    want with the argument, `categories`. In our example, we are giving values of
    `0`, `1`, `2`, `3`, and `4` for the `High School or Below`, `Bachelor`, `College`, `Master`,
    and `Doctor`, categories respectively. The output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now add these encoded variables to the pandas DataFrame, `df`, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With these encodings for the two categorical variables, `Gender` and `Education`,
    we can now fit a logistic regression model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to how we fit a logistic regression model with continuous variables
    previously, we can fit a logistic regression model with the encoded categorical
    variables, `GenderFactorized` and `EducationFactorized`, by using the `Logit`
    function in the `statsmodels` package. Using the `summary` function of the fitted
    logistic regression model object, we will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ebbe76b-7881-455c-923a-8f3a1025b757.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in this output and by looking at the *p*-values in the `P>|z|`
    column, both the `GenderFactorized` and `EducationFactorized` variables seem to
    have significant relationships with the output variable `Engaged`. If we look
    at the coefficients of these two variables, we can see that both are negatively
    correlated with the output. This suggests that male customers, encoded with `1`
    in the `GenderFactorized` variable, are less likely to be engaged with marketing
    calls, as compared to female customers, encoded with `0` in the `GenderFactorized` variable.
    Similarly, the higher the customers' education levels are, the less likely that
    they will be engaged with marketing calls.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed two ways of handling categorical variables in `pandas`, using
    the `factorize` and `Categorical` functions. With these techniques, we can understand
    how different categories of categorical variables are correlated with the output
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Combining continuous and categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last Python exercise that we are going to do in this chapter involves combining
    continuous and categorical variables for our regression analysis. We can fit a
    logistic regression model by using both categorical and continuous variables,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The only difference from the previous codes is the features that we selected
    to fit a logistic regression model. As you can see in this code, we are now fitting
    a logistic regression model with the continuous variables, as well as the two
    encoded categorical variables, `GenderFactorized` and `EducationFactorized`, that
    we created in the previous section. The results look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d55b21f-2e47-4da4-a572-f517ad414c2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's take a closer look at this output. The `Income`, `Monthly Premium Auto`,
    `Months Since Last Claim,` `Months Since Policy Inception`, `Number of Open Complaints`,
    `Number of Policies`, and `GenderFactorized` variable are significant at a `0.05` significance
    level, and all of them have negative relationships with the output variable, `Engaged`.
    Hence, the higher the income is, the less likely that the customer will be engaged
    with marketing calls. Similarly, the more policies that the customer has, the
    less likely that he or she will be engaged with marketing calls.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, male customers are less likely to engage with marketing calls than female
    customers, which we can see from looking at the coefficient of `GenderFactorized`. From
    looking at this regression analysis output, we can easily see the relationships
    between the input and output variables, and we can understand which attributes
    of customers are positively or negatively related to customer engagement with
    marketing calls.
  prefs: []
  type: TYPE_NORMAL
- en: The full code for the Python exercise in this chapter can be found at [https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.3/python/RegressionAnalysis.ipynb](https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.3/python/RegressionAnalysis.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you are going to learn how to use the `glm` function in R to
    conduct regression analysis. For those readers that would like to use Python instead
    of R for this exercise, the step-by-step instructions for Python are in the previous
    section. We will start this section by analyzing the data more closely, using
    the `dplyr` package, and then we will discuss how to build regression models and
    interpret the results using the `glm` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, we will be using one of the publicly available datasets
    from IBM Watson, which can be found at [https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/](https://www.ibm.com/communities/analytics/watson-analytics-blog/marketing-customer-value-analysis/).
    You can follow this link and download the data file in a CSV format. In order
    to load this data into your RStudio, you can run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Similar to what we did in [Chapter 2](1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml), *Key
    Performance Indicators and Visualizations*, we will first import the `dplyr` and `ggplot2` packages
    for data analysis and plotting in the following sections. Using the `read.csv` function
    in R, we can read the data into a DataFrame. Since this CSV file contains the
    header in the first row and the fields are separated by commas, we are using the `header=TRUE`
    and `sep=","` flags for the correct parsing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows how the raw data looks in the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfe00dbe-78f4-45a4-a4aa-2376f7431410.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have loaded the data into a DataFrame, let's start to look at and
    analyze the data more closely, so that we can better understand the structure
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive into regression analysis, let''s first take a more detailed
    look at the data, in order to have a better understanding of what data points
    we have and what patterns we can see in the data. If you look at the data, you
    will notice a column named `Response`. It contains information on whether a customer
    responded to their marketing calls. We will use this field as a measure of customer
    engagement. For future computations, it will be better to encode this field with
    numerical values. Let''s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in this code, using the `as.integer` function, we are encoding
    those who did not respond to marketing calls (`No`) with a value of `0` and those
    who did respond (`Yes`) with a value of `1`. Because `as.integer function` encodes
    values to `1` and `2` by default, we are subtracting the values by `1` to encode
    the response values with zeros and ones. Then, we are creating a new field named `Engaged` with
    these encoded values.
  prefs: []
  type: TYPE_NORMAL
- en: Engagement rate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing that we are going to look at is the aggregate engagement rate.
    This engagement rate is simply the percentage of customers who responded to the
    marketing calls. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code, we are grouping by the newly created field, `Engaged`,
    using the `group_by` function. Then, we are counting the number of records or
    customers in each `Engaged` group with the `n()` function. By dividing by the
    total number of customers in the DataFrame, `df`, and multiplying by `100.0`,
    we get the engagement rate. The results look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46c83142-7a95-4635-abf9-0bf6c55b1192.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To make it easier to read, we can transpose the DataFrame, meaning that we
    can flip the rows and columns in the DataFrame. You can transpose a DataFrame by
    using the `t` function in R. The code looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The transposed DataFrame appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61534cae-683e-4984-ae49-6a1f375555b3.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, it is easier to see the total number and percentage of engaged
    and non-engaged customers by transposing the DataFrame. From this data, we can
    see that about 14% of the customers have responded to marketing calls, and the
    remaining 86% of the customers have not responded.
  prefs: []
  type: TYPE_NORMAL
- en: Sales channels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s see if we can find any noticeable patterns in sales channels and
    engagement. We are going to analyze how the engaged and non-engaged customers
    are distributed among different sales channels. Let''s first look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code snippet, we are using the `group_by` function in
    R to group by the `Sales Channel` and `Engaged` variables. Then, using the `n()`
    function, we will count the number of customers in each group. Once you have run
    this code, the `salesChannel` DataFrame will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34a46f66-6a8f-4c2d-aa4c-b8ee50112ab4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you will have noticed from the previous section, there are significantly
    more customers that are not engaged with the marketing efforts, so it is quite
    difficult to compare and see the differences in the sales channel distributions
    between the engaged and non-engaged customers with the raw numbers. To make it
    easier to differentiate visually, we can build pie charts using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to what we did in [Chapter 2](1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml), *Key
    Performance Indicators and Visualizations*, we are using `ggplot` to build a chart
    in R. If you remember that chapter, we can build pie charts by using `geom_bar`
    with `coord_polar("y")`. By using `face_wrap(~Engaged)`, we can split the pie
    charts in two: one for non-engaged customers and another for engaged customers.
    Once you have run this code, you will see the following pie charts, which show
    the distributions of engaged and non-engaged customers across different sales
    channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c162b53-d864-4865-93eb-13c38d1fc0a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Compared to the previous data table that shows raw counts of engaged and non-engaged
    customers in each sales channel, these pie charts can help us to visually see
    the differences in the distributions more easily. As you can see from these charts,
    more than half of the engaged customers were from agents, whereas non-engaged
    customers are more evenly distributed across all four different channels. As you
    can see from these charts, analyzing and visualizing data can help us to notice
    interesting patterns in the data, which will further help us when we run regression
    analysis in the later parts of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Total claim amounts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last thing that we are going to look at before we dive into the regression
    analysis is are the differences in the distributions of `Total Claim Amount` between
    the engaged and non-engaged groups. We are going to visualize this by using box
    plots. Let''s first look at how we can build box plots in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code, it is quite straightforward to build box plots
    in R. You can simply call the `ggplot` function with `geom_boxplot`. A box plot
    is a great way to visualize the distributions of continuous variables. It shows
    the min, max, first quartile, median, and third quartile, all in one view. The
    following box plot shows the distributions of `Total Claim Amount` between the
    engaged and non-engaged groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25b89a8f-f883-400c-812b-6866d25c84b7.png)'
  prefs: []
  type: TYPE_IMG
- en: The central rectangle spans from the first quartile to the third quartile, and
    the line within the rectangle shows the median. The lower and upper ends of the
    lines from the rectangle show the minimum and maximum of the distribution, respectively.
    Another thing that you will notice from these box plots are the dots above the
    upper end of the line.
  prefs: []
  type: TYPE_NORMAL
- en: The dots beyond the end of the upper line show the suspected outliers, which
    are decided based on the IQR. The IQR is simply the range between the first and
    third quartiles, which is the same as the height of the rectangle in the box plot
    that spans from the first quartile to the third quartile. The data points that
    fall `1.5*IQR` above the third quartile or `1.5*IQR` below the first quartile
    are suspected outliers, and are shown with the dots.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your analysis goals, you might not care about (or you might not
    want to show) the outliers in box plots. Let''s take a look at the following code
    to see how we can remove those outliers from the box plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As you will notice in this code snippet, the only difference between this code
    and the previous one is `outlier.shape=NA` in the `geom_boxplot` function. Let''s
    take a look at how the box plots look now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/427c69d6-87c2-4137-bd7d-f9d4d565c7c9.png)'
  prefs: []
  type: TYPE_IMG
- en: In these plots, we can no longer see the dots beyond the end of the upper line.
    Depending on what you would like to show and analyze, having outliers in box plots
    may or may not help.
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have analyzed the types of fields that we have in the data and how
    the patterns differ between the engaged group and the non-engaged group. Now,
    we are going to discuss how to conduct and interpret regression analysis in R,
    using the `glm` function. We will first build a logistic regression model with
    continuous variables, and you will learn how to interpret the results. Then, we
    are going to discuss how to handle categorical variables when fitting regression
    models in R, and what impact those categorical variables have on the fitted logistic
    regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In linear regression, including logistic regression, it is straightforward
    to fit a regression model when the feature variables are continuous, as it just
    needs to find a linear combination of feature variables with numerical values
    for estimating the output variable. In order to fit a regression model with continuous
    variables, let''s first take a look at how to get the data types of the columns
    in an `R` DataFrame. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `sapply` function in `R`, we can apply the `class` function across
    the columns in a DataFrame, and the `class` function tells us the types of data
    in each column. The results of this code are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99e6ea9c-822d-4e6b-8904-853a3b426c2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Shown in the preceding screenshot, we can easily see which columns have numerical
    values and which do not. For example, the type of the `State` column is `"factor"`,
    which means that the variable is a categorical variable. On the other hand, the
    type of the `Customer.Lifetime.Value` column is `"numeric"`, and this means that
    this variable is a continuous variable with numeric values. Aside from this, we
    can also use an `R` function, `summary`, to get the summary statistics for each
    column of a DataFrame, so that we can see not only the types of each column, but
    can also take a look at a summary of what the distributions for each column look
    like. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you will get output that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fc841d4-179a-4089-b299-b0934b7fd763.png)'
  prefs: []
  type: TYPE_IMG
- en: In this output, we can easily see a snapshot of the distributions of each column
    in an `R` DataFrame. For example, for the `State` variable, we can easily see
    that there are `1703` records or customers from `Arizona` and `3150` customers
    from `California`. On the other hand, we can easily see that the minimum value
    for the`Customer.Lifetime.Value` variable is `1898`, whereas, the mean is `8005`
    and the maximum value is `83325`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this information from the previous code, we can easily select only the
    columns with numerical values, by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code snippet, we are using the `select_if` function,
    and the arguments for this function are the DataFrame, `df`, and a conditional
    statement, `is.numeric`, to define the type of column that we want to sub-select
    from the DataFrame. Using this function, only the numerical columns in the DataFrame,
    `df`, are selected and stored as a separate variable, named `continuousDF`. With
    the `colnames` function, we can see what columns are in the newly created DataFrame,
    `continuousDF`. You should see an output that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa4c358f-ccd2-4da8-9e75-a743a319ce02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are now ready to fit a logistic regression model with continuous variables.
    Let''s take a look at the following code first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In R, you can fit regression models by using the `glm` function, which stands
    for **generalized linear models**. The R function `glm` can be used for various
    linear models. By default, the value of the family argument is `gaussian`, which
    tells the algorithm to fit a simple linear regression model. On the other hand,
    like in our case, if you use `binomial` for `family`, then it is going to fit
    a logistic regression model. For more detailed descriptions of the different values
    that you can use for the `family` argument, you can refer to [https://stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The other two arguments that we passed on to the `glm` function are `formula`
    and `data`. The first argument, `formula`, is where you define how you want the
    model to be fit. The variable on the left side of `~` is the output variable,
    and the one on the right side of `~` is the input variable. In our case, we are
    telling the model to learn how to estimate the output variable, `Engaged`, by
    using all of the other variables as the input variables. If you want to use only
    a subset of the variables as the input variables, then you can use something like
    the following for the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In this formula, we are telling the model to learn how to estimate the output
    variable, `Engaged`, by only using `Income` and `Customer.Lifetime.Value` as the
    features. Lastly, the second argument in our `glm` function, `data`, defines which
    data to use to train a regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a trained logistic regression model, let''s take a look at
    the following code, which shows how we can get the detailed regression analysis
    results from this model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `summary` function in R provides a detailed description of the regression
    analysis results, which look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c37619a-21ef-4ffa-b36e-9fc327da669a.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's take a more detailed look at this output. The `Estimate` column in the `Coefficients`
    section gives us the computed value for each of the feature coefficients. For
    example, the coefficient for the `Income` variable is `0.000002042`, and the coefficient
    for `Number.of.Policies` is `-0.02443`. We can also see that the estimated `Intercept`
    value is `-1.787`. The column `z value` gives us the *z*-score, which is the number
    of standard deviations from the mean of the population, and the column `Pr(>|z|)` is
    the *p*-value, which means how likely it is to observe the relationship between
    the feature and the output variable by chance. So, the lower the value of `Pr(>|z|)` is,
    the more likely it is that the relationship between the given feature and the
    output variable is strong and is not by chance. Typically, `0.05` is a good cut-off
    point for the *p*-value, and any value less than `0.05` signifies a strong relationship
    between the given feature and the output variable.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the `Signif. codes` section under the `Coefficients` section
    in the output, the `***` symbol, next to the *p*-value in the `Coefficients` section,
    indicates the strongest relationship with the *p*-value at `0`; `**` means that
    the *p*-value is less than 0.001; `*` means that the *p*-value is less than `0.05`,
    and so forth. If you look at the regression analysis output again, only three
    variables, `Income`, `Number.of.Policies`, and `Total.Claim.Amount`, have significant
    relationships with the output variable, `Engaged`, at a `0.1` significance level.
    Also, we can see that `Income` and `Total.Claim.Amount` are positively correlated
    with `Engaged`, meaning that the higher the income is or the higher the total
    claim amount is, the more likely that a customer will be engaged with marketing
    calls. On the other hand, the variable `Number.of.Policies` is negatively correlated
    with `Engaged`, which suggests that the higher the number of policies that a customer
    has, the less likely that the given customer will be engaged with marketing calls.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in these examples, you can interpret the regression analysis
    results quite easily, by looking at the *p*-values and coefficients of the features
    from the model output. This is a good way to understand which attributes of customers
    are significantly and highly correlated with your outcomes of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you saw in the case with continuous variables in the previous section, it
    is quite straightforward to understand the relationships between the input and
    output variables from the coefficients and *p*-values. However, it becomes not
    so straightforward when we introduce categorical variables. Categorical variables
    often do not have any natural order but, in linear regression, we need the `input`
    variables to have numerical values that signify the orderings or magnitudes of
    the variables. For example, we cannot easily encode the `State` variable in our
    dataset with certain orders or values. That is why we need to handle categorical
    variables differently from continuous variables when conducting regression analysis.
    In R, the `factor` function helps you to handle these categorical variables easily
    when running regression analysis. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code, we are fitting a logistic regression model with
    `Engaged` as the output variable and the factorized `Education` as the input variable.
    Before we dive deeper into what this means, let''s first look at the following
    regression analysis results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31ab77a5-aeb4-4ed5-8288-144e0ea82443.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in this output, the `factor` function created four additional
    variables: `factor(Education)College`, `factor(Education)Doctor`, `factor(Education)High
    School or Below`, and `factor(Education)Master`. These variables are encoded with
    `0` if the given customer does not belong to the given category, or `1` if the
    given customer belongs to the given category. This way, we can understand the
    positive or negative relationship between each of the `Education` category and
    the `output` variable, `Engaged`. For example, the factor variable, `factor(Education)Doctor`,
    has a positive coefficient, which suggests that if a customer has a doctoral degree,
    then it is more likely that the given customer will be engaged with marketing
    calls.'
  prefs: []
  type: TYPE_NORMAL
- en: If you look closely, you will notice that this output does not have a separate
    factor variable for the `Bachelor` category in the `Education` variable. This
    is because `(Intercept)` contains the information for the `Bachelor` category.
    If a customer has a bachelor's degree, then all of the other factor variables
    would have been encoded with `0`s. Hence, all of the coefficient values are cancelled
    out, and only the `(Intercept)` value stays. Since the estimated `(Intercept)`
    value is negative, if a customer has a `Bachelor` degree, then it is less likely
    that the given customer will be engaged with marketing calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in this code, we are now fitting a regression model with the `Education`
    and `Gender` variables, and the output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86d48265-182c-4c31-b7e1-755ea30c0488.png)'
  prefs: []
  type: TYPE_IMG
- en: If you look closely at this output, you can only see one additional factor variable,
    `factor(Gender)M`, for male customers, where the data clearly has female customers.
    This is because the `Bachelor` category of the `Education` variable and the `F`
    (female) category of the `Gender` variable are lumped together as `(Intercept)`
    of this regression model. Thus, the base case, wherein the values of all of the
    factor variables are `0`, is for `female` customers with a `Bachelor` degree.
  prefs: []
  type: TYPE_NORMAL
- en: For male customers with a `Bachelor` degree, the factor variable `factor(Gender)M` will
    now have a value of `1`, and hence, the estimated value for the output variable,
    `Engaged`, will be the value of `(Intercept)` plus the coefficient value of `factor(Gender)M`.
  prefs: []
  type: TYPE_NORMAL
- en: As we have discussed so far, we can handle categorical variables by using the
    `factor` function in R. It is essentially the same as creating one separate input
    variable per category for each of the categorical variables. Using this technique,
    we can understand how different categories of categorical variables are correlated
    with the output variable.
  prefs: []
  type: TYPE_NORMAL
- en: Combining continuous and categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last exercise that we are going to do in this chapter involves combining
    continuous and categorical variables for our regression analysis. Let''s first
    factorize the two categorical variables, `Gender` and `Education`, that we discussed
    in the previous section, and store them in a DataFrame by using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The DataFrame, `continuousDF`, now contains the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51c0514f-23f0-474c-ae49-53136c894319.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we are going to fit a logistic regression model with both the categorical
    and continuous variables, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get an output that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3a76724-02ca-4dbb-b3db-9cbb77ee9554.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's take a closer look at this output. The `Total.Claim.Amount` variables and
    `EducationDoctor` variables are significant at a `0.05` significance level, and
    both of them have positive relationships with the output variable, `Engaged`.
    Hence, the higher the total claim amount is, the more likely that the customer
    is going to engage with the marketing calls. Also, customers with doctoral degrees
    are more likely to engage with marketing calls than those with other educational
    backgrounds. At a `0.1` significance level, we can see that `Income`, `Number.of.Policies`,
    and `EducationMaster` now have significant relationships with the output variable,
    `Engaged`. From looking at this regression analysis output, we can easily see
    the relationships between the input and output variables, and we can understand
    which attributes of customers are positively or negatively related to customer
    engagement with marketing calls.
  prefs: []
  type: TYPE_NORMAL
- en: The full code for the R exercise can be found in the repository at [https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.3/R/RegressionAnalysis.R](https://github.com/yoonhwang/hands-on-data-science-for-marketing/blob/master/ch.3/R/RegressionAnalysis.R).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to use explanatory analysis to draw insight
    on customer behavior. We discussed how regression analysis can be used to dive
    deeper into understanding customer behavior. More specifically, you learned how
    to use logistic regression to understand what attributes of customers drive higher
    engagement rates. In Python and R exercises, we employed the descriptive analysis
    that we covered in [Chapter 2](1fb7a852-d8fa-43f6-9807-6e3292dfa280.xhtml), *Key
    Performance Indicators and Visualizations*, as well as regression analysis for
    explanatory analysis. We started the exercises by analyzing the data in order
    to better understand and identify noticeable patterns in the data. While analyzing
    the data, you learned one additional way to visualize the data, through box plots,
    using the `matplotlib` and `pandas` packages in Python and the `ggplot2` library
    in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'While fitting regression models, we discussed the two different types of variables:
    continuous and categorical. You learned about the challenges in handling categorical
    variables when fitting logistic regression models, and how to handle such variables.
    For Python, we covered two ways of handling categorical variables: the `factorize`
    and `Categorical` functions from the `pandas` package. For R, we discussed how
    we can use the `factor` function to handle categorical variables when fitting
    a logistic regression model. With the regression analysis results, we showed how
    you can interpret the results and relationships between the input and output variables
    by looking at the coefficients and *p*-values. By looking at the regression analysis
    output, we can understand what attributes of customers show significant relationships
    with customer marketing engagement.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to expand your knowledge of explanatory analysis.
    We will analyze what drives conversions after customer engagements. You will also
    learn about another machine learning algorithm, decision trees, and how to use
    them for explanatory analysis.
  prefs: []
  type: TYPE_NORMAL
