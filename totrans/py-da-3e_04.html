<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Statistics
                </header>
            
            <article>
                
<p><strong>Explo</strong><strong>ratory data analysis</strong> (<strong>EDA</strong>) is the first step toward data analysis and building a machine learning model. <span>Statistics provide fundamental knowledge and a set of tools for exploratory or descriptive data analysis. </span>This chapter is designed to make you data-ready. For any kind of data professional role, you need to understand real-world data that is generally noisy, has missing values, and is collected from various sources.</p>
<p>Before performing any kind of preprocessing and analysis, you need to get familiar with the data present, and statistics is the only tool that will help you here. This makes statistics a primary and very necessary skill for data professionals, helping them gain initial insights and an understanding of the data. For example, the arithmetic mean of the monthly working hours of an employee can help us to understand the load of an employee in an organization. Similarly, the standard deviation of monthly working hours can help us to infer the range of working hours. Correlation between two variables such as blood pressure and age of patients can help us understand the relationship between blood pressure and age. Sampling methods can be useful in any kind of primary data collection. We can also perform parametric and non-parametric hypothesis tests to infer facts about the population.  </p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Understanding attributes and their types</li>
<li>Measuring central tendency</li>
<li>Measuring dispersion</li>
<li>Skewness and kurtosis</li>
<li>Understanding relationships using covariance and correlation coefficients</li>
<li>Central limit theorem</li>
<li>Collecting samples </li>
<li>Performing parametric tests</li>
<li>Performing non-parametric tests</li>
</ul>
<h1 id="uuid-98b211d8-31dd-4e80-ac04-16bded0e7f53">Technical requirements</h1>
<p>For this chapter, the following technical information is available:</p>
<ul>
<li class="mce-root">You can find the code and the dataset at the following GitHub link: <a href="https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter03">https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter03</a><span>.</span></li>
<li class="mce-root">All the code blocks are available in <kbd>ch3.ipynb</kbd>.</li>
<li class="mce-root"><span>In this chapter, we will use the NumPy, Pandas, and SciPy Python libraries.</span></li>
</ul>
<h1 id="uuid-1ab6ca6e-7bd5-493c-8307-0f543aa8bfe2" class="">Understanding attributes and their types</h1>
<p>Data is the collection of raw facts and statistics such as numbers, words, and observations. An attribute is a column or data field or series that represents the characteristics of an object and is also known as a variable, a feature, or a dimension. Statisticians use the term <em>variable</em>, while machine learning engineers prefer the term <em>feature</em>. The term <em>dimension</em> is used in data warehousing, while database professionals use the term <em>attribute</em>.</p>
<h2 id="uuid-7351a5a1-10b7-4a34-98e7-24124c134121">Types of attributes</h2>
<p>The data type of attributes is more crucial for data analysis because certain situations require certain data types. The data type of attributes helps analysts select the correct method for data analysis and visualization plots. The following list shows the various attributes:</p>
<ol>
<li><strong>Nominal attributes:</strong> Nominal refers to names or labels of categorized variables. The value of a nominal attribute can be the symbol or name of items. The values are categorical, qualitative, and unordered in nature such as product name, brand name, zip code, state, gender, and marital status. Finding the mean and median values of qualitative and categorical values will not make any sense but data analysts can calculate the mode, which is the most commonly occurring value. </li>
<li><strong>Ordinal attributes:</strong> <span>Ordinal refers to names or labels with a meaningful order or ranking, but the magnitude of values is not known. These types of attributes measure subjective qualities alone. That is why they are used in surveys for customer satisfaction ratings, product ratings, and movie rating reviews. Customer satisfaction ratings appear in the following order</span>:  
<ul>
<li>1: Very dissatisfied</li>
<li>2: Somewhat dissatisfied</li>
<li>3: Neutral</li>
<li>4: Satisfied</li>
<li>5: Very satisfied</li>
</ul>
</li>
</ol>
<p style="padding-left: 60px">Another example could be the size of a drink: small, medium, or large. Ordinal attributes can only be measured via the mode and the median. The mean cannot be calculated for ordinal attributes because of their qualitative nature. Ordinal attributes can also be recreated by discretization of a quantitative variable by dividing their values in a range of finite numbers.</p>
<ol start="3">
<li><strong>Numeric attributes:</strong> A numeric attribute is quantitatively presented as integer or real values. Numeric attributes can be of two types: interval-scaled or ratio-scaled.</li>
</ol>
<p style="padding-left: 60px">Interval-scaled attributes are measured on an ordered scale of equal-sized units. The meaningful difference between the two values of an interval-scaled attribute can be calculated, and this allows a comparison between the two values—for example, the birth year, and the temperature in °C. The main problem with interval-scaled attribute values is that they don't have a "true zero"—for example, if the temperature in <span>°C</span> is <span><span>0 </span></span>then it doesn't mean that temperature doesn't exist. Interval-scaled data can add and subtract but can't multiply and divide because of no true zero. We can also calculate the mean value of an interval-scaled attribute, in addition to the median and mode.</p>
<p style="padding-left: 60px">Ratio-scaled attributes are measured on an ordered scale of equal-sized units, similar to an interval scale with an inherent zero point. Examples of ratio-scaled attributes are height, weight, latitude, longitude, years of experience, and the number of words in a document.<span> </span>We can perform multiplication and division, and calculate the difference between ratio-scaled values. We can also compute central tendency measures such as mean, median, and mode. The Celsius and Fahrenheit temperature scales are measured on an interval scale, while the Kelvin temperature scale is measured on a ratio scale because it has a true zero point. </p>
<h2 id="uuid-8289835d-4797-4020-b113-a53db6b12a56">Discrete and continuous attributes</h2>
<p><span>There are various ways to classify attributes. In the previous sub-section, we have seen nominal, ordinal, and numeric attributes. In this sub-section, we will see another type of attribute classification. Here, we will talk about discrete or continuous attributes. A discrete variable accepts only a countable finite number, such as how many students are present in a class, how many cars are sold, and how many books are published. It can be obtained by counting numbers. A continuous variable accepts an infinite number of possible values, such as the weight and height of students. It can be obtained by measuring.</span></p>
<p class="mce-root">A discrete variable accepts integral values, while a continuous variable accepts real values. In other words, we can say a discrete variable accepts values whose fraction doesn't make sense, whereas a continuous variable accepts values whose fraction makes sense<span>. A</span> discrete attribute uses a limited number of values, while a continuous attribute uses an unlimited number of values.</p>
<p>After understanding the attributes and their types, it's time to focus on basic statistical descriptions such as central tendency measures. </p>
<h1 id="uuid-f0ca152f-a30a-4af5-a7e4-2c9a240fde64">Measuring central tendency</h1>
<p>Central tendency is the trend of values clustered around the averages such as the mean, mode, and median values of data. The main objective of central tendency is to compute the center-leading value of observations. Central tendency determines the descriptive summary and provides quantitative information about a group of observations. It has the capability to represent a whole set of observations. Let's see each type of central tendency measure in detail in the coming sections.</p>
<h2 id="uuid-79336041-bcad-4ed7-8c8e-82942bb80fdb">Mean</h2>
<p>The mean value is the arithmetic mean or average, which is computed by the sum of observations divided by the number of observations. It is sensitive to outliers and noise, with the result that whenever uncommon or unusual values are added to a group, its mean gets deviated from the typical central value. Assume x<sub>1</sub>, x<sub><sup>2</sup></sub>, . . . , x <sub>N</sub> is <em>N</em> observations. The formula for the mean of these values is shown here:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/c2e54702-8e82-4603-aa97-cfae46ae7ac1.png" style="width:10.33em;height:4.08em;"/></div>
<p><span>Let's compute the mean value of the communication</span><span> skill score </span><span>column using the <kbd>pandas</kbd> library, as follows: </span></p>
<pre># Import pandas library<br/>import pandas as pd<br/><br/># Create dataframe<br/>sample_data = {'name': ['John', 'Alia', 'Ananya', 'Steve', 'Ben'], <br/>               'gender': ['M', 'F', 'F', 'M', 'M'], <br/>               'communication_skill_score': [40, 45, 23, 39, 39],<br/>               'quantitative_skill_score': [38, 41, 42, 48, 32]}<br/><br/>data = pd.DataFrame(sample_data, columns = ['name', 'gender', 'communcation_skill_score', 'quantitative_skill_score'])<br/><br/># find mean of communication_skill_score column <br/>data['communcation_skill_score'].mean(axis=0)<br/><br/><strong>Output:</strong><br/>37.2</pre>
<p>In the preceding code block, we have created one DataFrame named <kbd>data</kbd> that has four columns (<kbd>name</kbd>, <kbd>gender</kbd>, <kbd>communication_skill_score</kbd>, and <kbd>quantitative_skill_score</kbd>) and computed the mean using the <kbd>mean(axis=0)</kbd> function. Here, <kbd>axis=0</kbd> represents the mean along the rows.</p>
<h2 id="uuid-dbc1a3eb-93c7-43d2-9d72-4a143891500c">Mode</h2>
<p>The mode is the highest-occurring item in a group of observations. The mode value occurs frequently in data and is mostly used for categorical values. If all the values in a group are unique or non-repeated, then there is no mode. It is also possible that more than one value has the same occurrence frequency. In such cases, there can be multiple modes. </p>
<p>Let's compute the mode value of the communication skill score column using the <kbd>pandas</kbd> library, as follows: </p>
<pre># find mode of communication_skill_score column<br/>data['communcation_skill_score'].mode()<br/><br/><strong>Output:</strong><br/>39</pre>
<p><span>In the preceding code block, we have computed the mode of the communication skill score column using the <kbd>mode()</kbd> function. Let's compute another central tendency measure: the median. </span></p>
<h2 id="uuid-766f1f74-ced1-4f7f-86a5-545537919b0a">Median</h2>
<p>The median is the midpoint or middle value in a group of observations. It is also called the 50<sup>th</sup> percentile. The median is less affected by outliers and noise than the mean, and that is why it is considered a more suitable statistic measure for reporting. It is much near to a typical central value. <span>Let's compute the median value of the communication skill score column using the <kbd>pandas</kbd> library, as follows: </span></p>
<pre># find median of communication_skill_score column<br/>data['communcation_skill_score'].median()<br/><br/><strong>Output:</strong><br/>39.0</pre>
<p><span>In the preceding code block, we have computed the median of the communication skill score column using the <kbd>median()</kbd> function. Let's understand dispersion measures and compute them in the next section.</span></p>
<h1 id="uuid-501b9358-73cc-45f3-9f94-53dc0ae64962">Measuring dispersion</h1>
<p>As we have seen, central tendency presents the middle value of a group of observations but does not provide the overall picture of an observation. Dispersion metrics measure the deviation in observations. The most popular dispersion metrics are range, <strong>interquartile range</strong> (<strong>IQR</strong>), variance, and standard deviation. These dispersion metrics value the variability in observations or the spread of observations. Let's see each dispersion measure in detail, as follows:</p>
<ul>
<li><strong>Range:</strong> The range is the difference between the maximum and minimum value of an observation. It is easy to compute and easy to understand. Its unit is the same as the unit of observations. Let's compute the range of <span>communication skill scores, as follows:</span></li>
</ul>
<pre style="padding-left: 60px">column_range=data['communcation_skill_score'].max()-data['communcation_skill_score'].min()<br/>print(column_range)<br/><br/><strong>Output:</strong><br/>22</pre>
<p style="padding-left: 60px">In the preceding code block, we have computed the range of communication skill scores by finding the difference between the maximum and minimum scores. The maximum and minimum scores were computed using the <kbd>max()</kbd> and <kbd>min()</kbd> functions. </p>
<ul>
<li><strong>IQR:</strong> IQR is the difference between the third and first quartiles. It is easy to compute and easy to understand. Its unit is the same as the unit of observations. It measures the middle 50% in the observation. It represents the range where most of the observation lies. IQR is also known as midspread or middle 50%, or H-spread. <span>Let's compute the IQR of </span><span>communication skill scores, as follows:</span></li>
</ul>
<pre style="padding-left: 60px"># First Quartile<br/>q1 = data['communcation_skill_score'].quantile(.25)<br/><br/># Third Quartile<br/>q3 = data['communcation_skill_score'].quantile(.75)<br/><br/># Inter Quartile Ratio<br/>iqr=q3-q1<br/>print(iqr)<br/><br/><strong>Output:</strong><br/>1.0</pre>
<p style="padding-left: 60px">In the preceding code block, we have computed the IQR of communication skill scores by finding the difference between the first and third quartile of scores. Both the <span>first and third quartile</span> scores were computed using the <kbd>quantile(.25)</kbd> and <kbd>quantile(.75)</kbd> functions.</p>
<ul>
<li><strong>Variance:</strong> The variance measures the deviation from the mean. It is the average value of the squared difference between observed values and the mean. The main problem with the variance is its unit of measurement because of squaring the difference between observations and mean. Let's assume x<sub>1</sub><span> , x</span><sub><sup>2</sup></sub><span>, . . . , x </span><sub>N</sub><span> are <em>N</em> observations. </span><span>The formula for the variance of these values will then be the following:</span></li>
</ul>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/0744c65a-7790-418b-9d99-55061aac3071.png" style="width:19.00em;height:4.83em;"/></div>
<p style="padding-left: 60px">Let's compute the variance of communication skill scores, as follows:</p>
<pre style="padding-left: 60px"># Variance of communication_skill_score<br/>data['communcation_skill_score'].var()<br/><br/><strong>Output:</strong><br/>69.2</pre>
<p style="padding-left: 60px">In the preceding code block, we have computed the variance of communication skill scores using the <kbd>var()</kbd> function.</p>
<ul>
<li><strong>Standard deviation:</strong> This is the square root of the variance. Its unit is the same as for the original observations. This makes it easier for an analyst to evaluate the exact deviation from the mean. The lower value of standard deviation represents the lesser distance of observations from the mean; this means observations are less widely spread. The higher value of standard deviation represents a large distance of observations from the mean—that is, observations are widely spread. Standard deviation is mathematically represented by the Greek letter<span> sigma (Σ)</span>. <span>Assume x</span><sub>1</sub><span>, x</span><sub><sup>2</sup></sub><span>, . . . , x </span><sub>N</sub><span> are <em>N</em> observations. </span><span>The formula for the standard deviation of these values is the following:</span></li>
</ul>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/80967c25-ffab-4a46-8fd3-487420d8c829.png" style="width:23.17em;height:4.42em;"/></div>
<p style="padding-left: 60px"><span>Let's compute the standard deviation of communication skill scores, as follows:</span></p>
<pre style="padding-left: 60px"># Standard deviation of communication_skill_score<br/>data['communcation_skill_score'].std()<br/><br/><strong>Output:</strong><br/>8.318653737234168</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have computed the standard deviation </span><span>of communication skill scores using the <kbd>std()</kbd> function.</span></p>
<p style="padding-left: 60px">We can also try to describe the function to get all the summary statistics in a single command. The <kbd>describe()</kbd> function returns the count, mean, standard deviation, first quartile, median, third quartile, and minimum and maximum values for each numeric column in the DataFrame, and is illustrated in the following code block:</p>
<pre style="padding-left: 60px"># Describe dataframe<br/>data.describe()<br/><br/><strong>Output:</strong><br/>      communcation_skill_score quantitative_skill_score<br/>count         5.000000         5.000000<br/>mean         37.200000         40.200000<br/>std           8.318654         5.848077<br/>min          23.000000         32.000000<br/>25%          39.000000         38.000000<br/>50%          39.000000         41.000000<br/>75%          40.000000         42.000000<br/>max          45.000000         48.000000<br/><br/></pre>
<p>In the preceding code block, we have generated a descriptive statistics summary of data using the <kbd>describe()</kbd> method.</p>
<h1 id="uuid-1e18e55a-be59-4d59-ad28-245540ef77c6">Skewness and kurtosis</h1>
<p>Skewness measures the symmetry of a distribution. It shows how much the distribution deviates from a normal distribution. Its values can be zero, positive, and negative. A zero value represents a perfectly normal shape of a distribution. Positive skewness is shown by the tails pointing toward the right—that is, outliers are skewed to the right and data stacked up on the left. Negative skewness is shown by the tails pointing toward the left—that is, outliers are skewed to the left and data stacked up on the right. Positive skewness occurs when the mean is greater than the median and the mode. Negative skewness occurs when the mean is less than the median and mode. Let's compute skewness in the following code block:</p>
<pre># skewness of communication_skill_score column<br/>data['communcation_skill_score'].skew()<br/><br/><strong>Output:</strong><br/>-1.704679180800373<br/><br/></pre>
<p><span>In the preceding code block, we have computed the skewness of the communication skill score column using the <kbd>skew()</kbd> method.</span></p>
<p>Kurtosis measures the tailedness (thickness of tail) compared to a normal distribution. High kurtosis is heavy-tailed, which means more outliers are present in the observations, and low values of kurtosis are light-tailed, which means fewer outliers are present in the observations. There are three types of kurtosis shapes: mesokurtic, p<span>latykurtic, and leptokurtic</span>. Let's define them one by one, as follows:</p>
<ul>
<li>A normal distribution having zero kurtosis is known as a mesokurtic distribution.</li>
<li>A platykurtic distribution has a negative kurtosis value and is thin-tailed compared to a normal distribution.</li>
<li>A leptokurtic distribution has a kurtosis value greater than 3 and is fat-tailed compared to a normal distribution. </li>
</ul>
<p>Let's see the type of kurtosis shapes in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cd7c3330-09d7-44a6-8517-5efa18a505da.png" style=""/></div>
<p>A histogram is an effective medium to present skewness and kurtosis. Let's compute the kurtosis of the communication skill score column, as follows:</p>
<pre># kurtosis of communication_skill_score column<br/>data['communcation_skill_score'].kurtosis()<br/><br/><strong>Output:</strong><br/>3.6010641852384015</pre>
<p><span>In the preceding code block, we have computed the kurtosis of the communication skill score column using the <kbd>kurtosis()</kbd> method.</span></p>
<h1 id="uuid-55df4e8d-0606-4773-966b-4fd23789eaf2">Understanding relationships using covariance and correlation coefficients</h1>
<p>Measuring the relationship between variables will be helpful for data analysts to understand the dynamics between variables—for example, an HR manager needs to understand the strength of the relationship between employee performance score and satisfaction score. Statistics offers two measures of covariance and correlation to understand the relationship between variables. Covariance measures the relationship between a pair of variables. It shows the degree of change in the variables—that is, how the change in one variable affects the other variable. Its value ranges from -infinity to + infinity. The problem with covariance is that it does not provide effective conclusions because it is not normalized. Let's find the relationship between the communication and quantitative skill score using covariance, as follows: </p>
<pre># Covariance between columns of dataframe<br/>data.cov()</pre>
<p>This results in the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7b8b6b19-eae9-4d77-aea7-df83e333cb34.png"/></div>
<p>In the preceding code block, covariance is computed using the <kbd>cov()</kbd> method. Here, the output of this method is the covariance matrix.  </p>
<h2 id="uuid-304e80e3-8fc7-4bc3-822e-10dccd7a7080">Pearson's correlation coefficient</h2>
<p>Correlation shows how variables are correlated with each other. Correlation offers a better understanding than covariance and is a normalized version of covariance. Correlation ranges from -1 to 1. A negative value represents the increase in one variable, causing a decrease in other variables or variables to move in the same direction. A positive value represents the increase in one variable, causing an increase in another variable, or a decrease in one variable causes decreases in another variable. A zero value means that there is no relationship between the variable or that variables are independent of each other. Have a look at the following code snippet:</p>
<pre># Correlation between columns of dataframe<br/>data.corr(method ='pearson')</pre>
<p>This results in the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/910acc6b-7f60-4164-8dd9-41a562e70855.png"/></div>
<p><span><span>The <kbd>'method'</kbd></span></span> parameter can take one of the following three parameters:</p>
<ul>
<li><kbd>pearson</kbd>: Standard correlation coefficient</li>
<li><kbd>kendall</kbd>: Kendall's tau correlation coefficient</li>
<li><kbd>spearman</kbd>: Spearman's rank correlation coefficient</li>
</ul>
<h2 id="uuid-c4bed68b-380f-4429-a332-4154947251ca">Spearman's rank correlation coefficient</h2>
<p>Spearman's rank correlation coefficient is Pearson's correlation coefficient on the ranks of the observations. It is a non-parametric measure for rank correlation. It assesses the strength of the association between two ranked variables. Ranked variables are ordinal numbers, arranged in order. First, we rank the observations and then compute the correlation of ranks. It can apply to both continuous and discrete ordinal variables. When the distribution of data is skewed or an outlier is affected, then Spearman's rank correlation is used instead of Pearson's correlation because it doesn't have any assumptions for data distribution.</p>
<h2 id="uuid-467f337b-ed42-47db-b8a5-5c6665db90f5">Kendall's rank correlation coefficient</h2>
<p>Kendall's rank correlation coefficient or Kendall's tau coefficient is a non-parametric statistic used to measure<span> the </span>association between two ordinal variables. It is a type of rank correlation. It measures the similarity or dissimilarity between two variables. If both the variables are binary, then Pearson's = Spearman's = Kendall's tau. </p>
<p><span>Till now, we have seen descriptive statistics topics such as central measures, dispersion measures, distribution measures, and variable relationship measures. It's time to jump to the inferential statistics topics such as the central limit theorem, sampling techniques, and parametric and non-parametric tests.</span></p>
<h1 id="uuid-db2fedd3-2557-4d8e-9b12-d0ce4ae3c5c0">Central limit theorem</h1>
<p>Data analysis methods involve hypothesis testing and deciding confidence intervals. All statistical tests assume that the population is normally distributed. The central limit theorem is the core of hypothesis testing. According to this theorem, the sampling distribution approaches a normal distribution with an increase in the sample size. Also, the mean of the sample gets closer to the population means and the standard deviation of the sample gets reduced. This theorem is essential for working with inferential statistics,  helping data analysts figure out how samples can be useful in getting insights about the population.</p>
<p>Does it provide answers to questions such as what size of <span>sample should be taken or which sample size is an accurate representation of the population? You can understand this with the help of the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/df7154da-c951-47c8-8478-7dfb8bafe6ce.png"/></div>
<p class="mce-root"/>
<p>In the preceding diagram, you can see four histograms for different-different sample sizes 50, 100, 200, and 500. If you observe here, as the sample size increases, the histogram approaches a normal curve. Let's learn sampling techniques in the next section.</p>
<h1 id="uuid-70ff7578-b827-4be0-8d74-e655e43bd5ef">Collecting samples</h1>
<p>A sample is a small set of the population used for data analysis purposes. Sampling is a method or process of collecting sample data from various sources. It is the most crucial part of data collection. The success of an experiment depends upon how well the data is collected. If anything goes wrong with sampling, it will hugely affect the final interpretations. Also, it is impossible to collect data for the whole population. Sampling helps researchers to infer the population from the sample and reduces the survey cost and workload to collect and manage data. There are lots of sampling techniques available, for various purposes. These techniques can be categorized into two categories: probability sampling and non-probability sampling, described in more detail here:</p>
<ul>
<li><strong>Probability sampling:</strong> With this technique, there is a random selection of every respondent of the population, with an equal chance of the selected sample. Such types of sampling techniques are more time-consuming and expensive, and include the following:
<ul>
<li><strong>Simple random sampling:</strong> With this technique, each respondent is selected by chance, meaning that each respondent has an equal chance of being selected. It is a simple and straightforward method—for example, 20 products being randomly selected from 500 products for quality testing.</li>
<li><strong>Stratified sampling:</strong> With this technique, the whole population is divided into small groups known as strata that are based on some similarity criteria. These strata can be of unequal size. This technique improves accuracy by reducing selection bias.</li>
<li><strong>Systematic sampling:</strong> With this technique, respondents are selected at regular intervals. In other words, we can say respondents are selected in systematic order from the target population, such as every <em>n</em>th respondent from the population.</li>
<li><strong>Cluster sampling:</strong> With this sampling technique, the entire population is divided into clusters or sections. Clusters are formed based on gender, location, occupation, and so on. These entire clusters are used for sampling rather than the individual respondent.</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Non-probability sampling:</strong> This sampling non-randomly selects every respondent of the population, with an unequal chance of the selected sample. Its outcome might be biased. Such types of sampling techniques are cheaper and more convenient, and include the following:
<ul>
<li><strong>Convenience sampling:</strong> This is the easiest technique for data collection. It selects respondents based on their availability and willingness to participate. Statisticians prefer this technique for the initial survey due to cost and fast collection of data, but the results are more prone to bias.</li>
<li><strong>Purposive sampling:</strong> This is also known as judgmental sampling because it depends upon the statistician's judgment. Statisticians decide at runtime who will participate in the survey based on certain predefined characteristics. News reporters use this technique to select people whose opinions they wish to obtain.</li>
<li><strong>Quota sampling:</strong> This technique predefines the properties of strata and proportions for the sample. Sample respondents are selected until a definitive proportion is met. It differs from stratified sampling in terms of selection strategy; it selects items in strata using random sampling.</li>
<li><strong>Snowball sampling:</strong> This technique is used in a situation where finding respondents in a population is rare and difficult to trace, in areas such as illegal immigration or HIV. Statisticians contact volunteers to reach out to the victims. It is also known as referral sampling because the initial person taking part in the survey refers to another person who fits the sample description.</li>
</ul>
</li>
</ul>
<p>In this section, we have seen sampling methods and their types: p<span>robability sampling and non-probability sampling. </span>Now, it's time to jump to hypothesis testing techniques. In upcoming sections, we will focus on parametric and non-parametric hypothesis testing.   </p>
<h1 id="uuid-d86d7b07-38c8-4cab-b36a-c82ad3c85e5e">Performing parametric tests</h1>
<p>The hypothesis is the main core topic of inferential statistics. In this section, we will focus on parametric tests. The basic assumption of a parametric test is the underlying statistical distribution. Most elementary statistical methods are parametric in nature. Parametric tests are used for quantitative and continuous data. Parameters are numeric quantities that represent the whole population. Parametric tests are more powerful and reliable than non-parametric tests. The hypothesis is developed on the parameters of the population distribution. Here are some examples of parametric tests:</p>
<ul>
<li>A t-test is a kind of parametric test that is used for checking if there is a significant difference between the means of the two groups concerned. It is the most commonly used inferential statistic that follows the normal distribution. A t-test has two types: a one-sample t-test and a two-sample t-test. A one-sample t-test is used for checking if there is a significant difference between a sample and hypothesized population means. Let's take 10 students and check whether their average weight is 68 kg or not by using a t-test, as follows:</li>
</ul>
<pre style="padding-left: 60px">import numpy as np<br/><br/>from scipy.stats import ttest_1samp<br/><br/># Create data<br/>data=np.array([63, 75, 84, 58, 52, 96, 63, 55, 76, 83])<br/><br/># Find mean<br/>mean_value = np.mean(data)<br/><br/>print("Mean:",mean_value)<br/><br/><strong>Output:</strong><br/><br/>Mean: 70.5</pre>
<p style="padding-left: 60px">In the preceding code block, we have created an array of <span>10 students' weight and computed its arithmetic mean using <kbd>numpy.mean()</kbd>.</span></p>
<p style="padding-left: 60px"><span>Let's perform a one-sample t-test, as follows:</span></p>
<pre style="padding-left: 60px"># Perform one-sample t-test<br/>t_test_value, p_value = ttest_1samp(data, 68)<br/><br/>print("P Value:",p_value)<br/><br/>print("t-test Value:",t_test_value)<br/><br/># 0.05 or 5% is significance level or alpha.<br/>if p_value &lt; 0.05: <br/><br/>    print("Hypothesis Rejected")<br/><br/>else:<br/>   <br/>    print("Hypothesis Accepted")<br/><br/><strong>Output:<br/></strong><br/>P Value: 0.5986851106160134<br/>t-test Value: 0.5454725779039431<br/>Hypothesis Accepted</pre>
<p style="padding-left: 60px">In the preceding code block, we have tested the null hypothesis (<span>average weight of 10 students is </span><span>68 kg) by using <kbd>ttest_1samp()</kbd>. The output results have shown that the null hypothesis is accepted with a 95% confidence interval, which means that the average weight of 10 students is 68 kg.</span></p>
<ul>
<li>A two-sample t-test is used for comparing the significant difference between two independent groups. This test is also known as an independent samples t-test. Let's compare the average weight of two independent student groups, as follows: </li>
</ul>
<p style="padding-left: 90px"><strong>Null Hypothesis<span> </span>H<sub>0</sub>:</strong> Sample means are equal—<span>μ </span><sub>1</sub><span> = μ </span><sub>2  </sub></p>
<p style="padding-left: 90px"><strong>Alternative Hypothesis<span> </span>H<sub>a</sub>:</strong> <span>Sample means are not equal—μ <sub>1</sub> &gt; μ <sub>2 </sub>or μ <sub>2</sub> &gt; μ <sub>1</sub></span></p>
<p>                 Have a look at the following code block:</p>
<pre style="padding-left: 60px">from scipy.stats import ttest_ind<br/><br/># Create numpy arrays<br/>data1=np.array([63, 75, 84, 58, 52, 96, 63, 55, 76, 83])<br/><br/>data2=np.array([53, 43, 31, 113, 33, 57, 27, 23, 24, 43])</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have created two arrays of </span><span>10 students' weights.</span></p>
<p style="padding-left: 60px"><span>Let's perform a two-sample t-test, as follows:</span></p>
<pre style="padding-left: 60px"># Compare samples<br/><br/>stat, p = ttest_ind(data1, data2)<br/><br/>print("p-values:",p)<br/><br/>print("t-test:",stat)<br/><br/># 0.05 or 5% is significance level or alpha.<br/><br/>if p &lt; 0.05: <br/><br/>    print("Hypothesis Rejected")<br/><br/>else:<br/><br/>    print("Hypothesis Accepted") <br/><br/><strong>Output:</strong><br/>p-values: 0.015170931362451255<br/>t-test: 2.6835879913819185<br/>Hypothesis Rejected</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have tested the hypothesis </span><span>average weight of two groups using the <kbd>ttest_ind()</kbd> method, </span><span>and results show that the null hypothesis is rejected with a 95% confidence interval, which means that the sample means are different.</span></p>
<ul>
<li>A paired sample t-test is a dependent sample t-test, which is used to decide whether the mean difference between two observations of the same group is zero—for example, to compare the difference in blood pressure level for a group of patients before and after some drug treatment. This is equivalent to a one-sample t-test and is also known as a dependent sample t-test. Let's perform a paired t-test to assess the impact of weight loss treatment. We have collected the weight of patients before and after treatment. This can be represented using the following hypothesis:</li>
</ul>
<p style="padding-left: 90px"><strong><em>Null Hypothesis H<sub>0</sub></em>:</strong> <span>M</span><span>ean difference between the two dependent samples is 0.</span></p>
<p style="padding-left: 90px"><strong><em>Alternative Hypothesis H<sub>a</sub></em>:</strong> <span>M</span><span>ean difference between the two dependent samples is not 0.</span></p>
<p>                Have a look at the following code block:</p>
<pre style="padding-left: 60px"># paired test<br/>from scipy.stats import ttest_rel<br/><br/># Weights before treatment<br/>data1=np.array([63, 75, 84, 58, 52, 96, 63, 65, 76, 83])<br/><br/># Weights after treatment<br/>data2=np.array([53, 43, 67, 59, 48, 57, 65, 58, 64, 72])</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have created two arrays of </span><span>10 patients' weights before and after treatment. Let's perform a paired sample t-test, as follows:  </span></p>
<pre style="padding-left: 60px"># Compare weights<br/><br/>stat, p = ttest_rel(data1, data2)<br/><br/>print("p-values:",p)<br/><br/>print("t-test:",stat)<br/><br/># 0.05 or 5% is the significance level or alpha.<br/><br/>if p &lt; 0.05: <br/>    <br/>    print("Hypothesis Rejected")<br/><br/>else:<br/><br/>    print("Hypothesis Accepted")<br/><br/><strong>Output:</strong><br/>p-values: 0.013685575312467715<br/>t-test: 3.0548295044306903<br/>Hypothesis Rejected</pre>
<p style="padding-left: 60px">In the preceding code block<span>, we have tested the hypothesis of the </span><span>average weight of two groups before and after treatment using the <kbd>ttest_rel()</kbd> method. R</span><span>esults show that the null hypothesis is rejected with a 95% confidence interval, which means that weight loss treatment has a significant impact on the patient's weight. </span></p>
<ul>
<li>ANOVA: A t-test only deals with two groups, but sometimes we have more than two groups or multiple groups at the same time to compare. <strong>ANOVA </strong>(<strong>ANalysis Of</strong> <strong>VAriance</strong>) is a statistical inference test used for comparing multiple groups. It analyzes the variance between and within multiple groups and tests several null hypotheses at the same time. It usually compares more than two sets of data and checks statistical significance. We can use ANOVA in three ways: one-way ANOVA, two-way ANOVA, and N-way multivariate ANOVA.</li>
<li>With the one-way ANOVA method, we compare multiple groups based on only one independent variable—for example, an IT company wants to compare multiple employee groups' or teams' productivity based on performance score. In our example, we are comparing the performance of employees in an IT company based in three locations: Mumbai, Chicago, and London. Here, we will perform a one-way ANOVA test and check for a significant difference in performance. Let's define the null and alternative hypotheses, as follows:</li>
</ul>
<p style="padding-left: 90px"><strong><em>Null Hypothesis H<sub>0</sub></em>:</strong> <span>There is no</span> <span>difference between the mean performance score of multiple locations. </span></p>
<p style="padding-left: 90px"><strong><em>Alternative Hypothesis H<sub>a</sub></em>:</strong><span> </span><span>There is a </span><span>difference between the mean performance score of multiple locations</span><span>. </span></p>
<p>                  Have a look at the following code block:</p>
<pre style="padding-left: 60px">from scipy.stats import f_oneway<br/><br/># Performance scores of Mumbai location<br/>mumbai=[0.14730927, 0.59168541, 0.85677052, 0.27315387, 0.78591207,0.52426114, 0.05007655, 0.64405363, 0.9825853 , 0.62667439]<br/><br/># Performance scores of Chicago location<br/>chicago=[0.99140754, 0.76960782, 0.51370154, 0.85041028, 0.19485391,0.25269917, 0.19925735, 0.80048387, 0.98381235, 0.5864963 ]<br/><br/># Performance scores of London location<br/>london=[0.40382226, 0.51613408, 0.39374473, 0.0689976 , 0.28035865,0.56326686, 0.66735357, 0.06786065, 0.21013306, 0.86503358]</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have created three lists of employee performance scores for three locations: Mumbai, Chicago, and London.</span><span> </span></p>
<p style="padding-left: 60px"><span>L</span><span>et's perform a one-way ANOVA test, as follows:</span></p>
<pre style="padding-left: 60px"># Compare results using Oneway ANOVA<br/>stat, p = f_oneway(mumbai, chicago, london)<br/><br/>print("p-values:", p)<br/><br/>print("ANOVA:", stat)<br/><br/>if p &lt; 0.05: <br/>    <br/>    print("Hypothesis Rejected")<br/><br/>else:<br/><br/>    print("Hypothesis Accepted")<br/><br/><strong>Output:</strong><br/>p-values: 0.27667556390705783<br/>ANOVA: 1.3480446381965452<br/>Hypothesis Accepted</pre>
<p style="padding-left: 60px"><span>In the preceding code block</span><span>, we have tested the hypothesis that there is no difference between the mean performance score of various locations using the <kbd>f_oneway()</kbd> method.</span> <span>The preceding results</span><span> show that the null hypothesis is accepted with a 95% confidence interval, which means that there is no significant difference between the mean performance score of all the locations.</span></p>
<ul>
<li>With the two-way ANOVA method, we compare multiple groups based on two independent variables—for example, if an IT company wants to compare multiple employee groups' or teams' productivity based on working hours and project complexity.</li>
<li>In N-way ANOVA, we compare multiple groups based on <em>N</em> independent variables—for example, if an IT company wants to compare multiple employee groups' or teams' productivity based on working hours, project complexity, employee training, and other employee perks and facilities.</li>
</ul>
<p class="CDPAlignLeft CDPAlign">In this section, we have explored parametric tests such as the <span>t-test and ANOVA tests </span>in detail. Let's jump to the non-parametric hypothesis test.</p>
<h1 id="uuid-3c7d680b-4cd9-4426-9a61-d2098465f85e" class="CDPAlignLeft CDPAlign">Performing non-parametric tests </h1>
<p class="CDPAlignLeft CDPAlign">A non-parametric test doesn't rely on any statistical distribution; that is why it is known as a "distribution-free" hypothesis test. Non-parametric tests don't have parameters of the population. Such types of tests are used for order and rank of observations and require special ranking and counting methods. Here are some examples of non-parametric tests:</p>
<ul>
<li>A <strong>Chi-Square test</strong> is determined by a significant difference or relationship between two categorical variables from a single population. In general, this test assesses whether distributions of categorical variables differ from each other. It is also known as a Chi-Square goodness of fit test or a Chi-Square test for independence. A small value of the Chi-Square statistic means observed data fit with expected data, and a larger value of the Chi-Square statistic means observed data doesn't fit with expected data. For example, the impact of gender on voting preference <span>or the impact of company size on health insurance coverage </span>can be assessed by a Chi-Square test:</li>
</ul>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/104b9b36-4efc-4d4f-9c58-1c99f988bb85.png" style="width:11.42em;height:4.50em;"/></div>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">Here, <em>O</em> is the observed value, <em>E</em> is the expected value, and "<em>i</em>" is the "i<sup>th</sup>" position in the contingency table.</p>
<p style="padding-left: 60px">Let's understand the Chi-Square test using an example. Suppose we have done a survey in a company of 200 employees and asked about their highest qualification such as High School, Higher Secondary, Graduate, Post-Graduate, and compare it with performance levels such as Average and Outstanding. Here is the hypothesis and contingency criteria: </p>
<p style="padding-left: 90px"><strong><em>Null Hypothesis H<sub>0</sub></em>:</strong><span> The two categorical variables are independent—that is, employee performance is independent of the highest qualification level.</span></p>
<p style="padding-left: 90px"><strong><em>Alternative Hypothesis H<sub>a</sub></em>:</strong> <span>The two categorical variables are not independent—that is, employee performance is not independent of the highest qualification level.</span></p>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">The contingency table can be represented as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 133px" class="CDPAlignCenter CDPAlign"/>
<td style="width: 107px" class="CDPAlignCenter CDPAlign"><span>High School </span></td>
<td style="width: 153px" class="CDPAlignCenter CDPAlign"><span>Higher Secondary</span></td>
<td style="width: 96px" class="CDPAlignCenter CDPAlign"><span>Graduate</span></td>
<td style="width: 118px" class="CDPAlignCenter CDPAlign"><span>Post-Graduate</span></td>
</tr>
<tr>
<td style="width: 133px" class="CDPAlignCenter CDPAlign">Average</td>
<td style="width: 107px" class="CDPAlignCenter CDPAlign">20</td>
<td style="width: 153px" class="CDPAlignCenter CDPAlign">16</td>
<td style="width: 96px" class="CDPAlignCenter CDPAlign">13</td>
<td style="width: 118px" class="CDPAlignCenter CDPAlign">7</td>
</tr>
<tr>
<td style="width: 133px" class="CDPAlignCenter CDPAlign">Outstanding</td>
<td style="width: 107px" class="CDPAlignCenter CDPAlign">31</td>
<td style="width: 153px" class="CDPAlignCenter CDPAlign">40</td>
<td style="width: 96px" class="CDPAlignCenter CDPAlign">50</td>
<td style="width: 118px" class="CDPAlignCenter CDPAlign">13</td>
</tr>
</tbody>
</table>
<p> </p>
<p style="padding-left: 60px" class="mce-root"><span>Let's perform a Chi-Square test and check for a significant difference in the association between variables, as follows:</span></p>
<pre style="padding-left: 60px">from scipy.stats import chi2_contingency<br/><br/># Average performing employees<br/>average=[20, 16, 13, 7]<br/><br/># Outstanding performing employees<br/>outstanding=[31, 40, 60, 13]<br/><br/># contingency table<br/>contingency_table= [average, outstanding]</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have created two lists of average and outstanding performing employees and created a contingency table.</span><span> </span></p>
<p style="padding-left: 60px"><span>Let's perform a Chi-Square test, as follows:</span></p>
<pre style="padding-left: 60px"># Apply Test<br/>stat, p, dof, expected = chi2_contingency(contingency_table)<br/><br/>print("p-values:",p)<br/><br/>if p &lt; 0.05: <br/>    <br/>    print("Hypothesis Rejected")<br/><br/>else:<br/><br/>    print("Hypothesis Accepted")<br/><br/><strong>Output:</strong><br/>p-values: 0.059155602774381234<br/>Hypothesis Accepted</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have tested the hypothesis that employee performance is independent of the highest qualification level. </span><span>The preceding results</span><span> show that the null hypothesis is accepted with a 95% confidence interval, which means that employee performance is independent of the highest qualification level.</span></p>
<ul>
<li>The <strong>Mann-Whitney U test</strong> is the non-parametric counterpart of the t-test for two samples. It doesn't assume that the difference between the samples is normally distributed. The Mann-Whitney U test is used when the observation is ordinal and assumptions of the t-test were not met—for example, comparing two groups of movie test preferences from their given movie ratings. Let's compare two groups of movie ratings using the following criteria:</li>
</ul>
<p style="padding-left: 90px"><strong><em>Null Hypothesis H<sub>0</sub></em>:</strong><span> </span>There is no<span> </span><span>difference between the two sample distributions. </span></p>
<p style="padding-left: 90px"><strong><em>Alternative Hypothesis H<sub>a</sub></em>:</strong> <span>There is a </span><span>difference between the two sample distributions.</span></p>
<p>                Have a look at the following code block:</p>
<pre style="padding-left: 60px">from scipy.stats import mannwhitneyu<br/><br/># Sample1<br/><br/>data1=[7,8,4,9,8]<br/><br/># Sample2<br/><br/>data2=[3,4,2,1,1]</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have created two lists of data.</span><span> </span></p>
<p style="padding-left: 60px"><span>Let's perform a Mann-Whitney U test, as follows:</span></p>
<pre style="padding-left: 60px"># Apply Test<br/><br/>stat, p = mannwhitneyu(data1, data2)<br/><br/>print("p-values:",p)<br/><br/># 0.01 or 1% is significance level or alpha.<br/><br/>if p &lt; 0.01: <br/><br/>    print("Hypothesis Rejected")<br/><br/>else:<br/>    print("Hypothesis Accepted")<br/><br/><strong>Output:</strong><br/>p-values: 0.007666581056801412<br/>Hypothesis Rejected</pre>
<p style="padding-left: 60px">In the preceding code block, we have tested the hypothesis that there is no <span>difference between the distribution of two movie rating groups using the <kbd>mannwhitneyu()</kbd> method. </span><span>The results</span><span> show that the null hypothesis is rejected with a 99% confidence interval, which means that there is a significant difference between the two movie rating groups. </span></p>
<ul>
<li>The <strong>Wilcoxon signed-rank test</strong> compares two paired samples. It is a non-parametric counterpart version of the paired t-test. It tests the null hypothesis as to whether the two paired samples belong to the same distribution or not—for example, to compare the difference between two treatment observations for multiple groups. Let's <span>compare the difference between two treatment observations using the following criteria:</span></li>
</ul>
<p style="padding-left: 90px"><strong><em>Null Hypothesis H<sub>0</sub></em>:</strong><span> </span>There is no<span> </span><span>difference between the dependent sample distributions. </span></p>
<p style="padding-left: 90px"><strong><em>Alternative Hypothesis H<sub>a</sub></em>:</strong><span> </span><span>There is a </span><span>difference between</span> <span>the dependent sample distributions. </span></p>
<p>                  Have a look at the following code block:</p>
<pre style="padding-left: 60px">from scipy.stats import wilcoxon<br/><br/># Sample-1<br/>data1 = [1, 3, 5, 7, 9]<br/><br/># Sample-2 after treatement <br/>data2 = [2, 4, 6, 8, 10]</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have created two lists of data.</span><span> </span></p>
<p style="padding-left: 60px"><span>Let's perform a Wilcoxon signed-rank test, as follows:</span></p>
<pre style="padding-left: 60px"># Apply <br/>stat, p = wilcoxon(data1, data2)<br/><br/>print("p-values:",p)<br/><br/># 0.01 or 1% is significance level or alpha.<br/><br/>if p &lt; 0.01: <br/><br/>    print("Hypothesis Rejected")<br/><br/>else:<br/>    print("Hypothesis Accepted")<br/><br/><strong>Output:</strong><br/>p-values: 0.025347318677468252<br/>Hypothesis Accepted</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have tested the hypothesis that there is no </span><span>difference between the distribution of groups before and after treatment using the <kbd>wilcoxon()</kbd> method.</span> <span>The preceding results</span><span> show that the null hypothesis is accepted with a 99% confidence interval, which means that there is no significant difference between the groups before and after treatment. </span></p>
<ul>
<li>The <strong>Kruskal-Wallis</strong> test is the non-parametric version of one-way ANOVA, to assess whether samples belong to the same distribution or not. It compares two or more independent samples. It extends the limit of the Mann-Whitney U test, which compares only two groups. Let's compare three sample groups using the following code:</li>
</ul>
<pre style="padding-left: 60px">from scipy.stats import kruskal<br/><br/># Data sample-1<br/>x = [38, 18, 39, 83, 15, 38, 63, 1, 34, 50]<br/><br/># Data sample-2<br/>y = [78, 32, 58, 59, 74, 77, 29, 77, 54, 59]<br/><br/># Data sample-3<br/>z = [117, 92, 42, 79, 58, 117, 46, 114, 86, 26]</pre>
<p><span>In the preceding code block, we have created three lists of data.</span><span> Let's perform a Kruskal-Wallis test, as follows:</span></p>
<pre style="padding-left: 60px"># Apply kruskal-wallis test<br/>stat, p = kruskal(x,y,z)<br/><br/>print("p-values:",p)<br/><br/># 0.01 or 1% is significance level or alpha.<br/><br/>if p &lt; 0.01: <br/><br/>    print("Hypothesis Rejected")<br/><br/>else:<br/>    print("Hypothesis Accepted")<br/><br/><strong>Output:</strong><br/>p-values: 0.01997922369138151<br/>Hypothesis Accepted</pre>
<p style="padding-left: 60px"><span>In the preceding code block, we have tested the hypothesis that</span> <span>there is no difference between the three sample groups using the <kbd>kruskal()</kbd> method. </span><span>The preceding results</span><span> show that the null hypothesis is accepted with a 99% confidence interval, which means that there is no difference between the three sample groups. </span>Let's compare both parametric and non-parametric tests, as follows: </p>
<table style="border-collapse: collapse;width: 100%" class="table" border="1">
<tbody>
<tr>
<td style="width: 252px" class="CDPAlignCenter CDPAlign">
<p><strong>Features</strong></p>
</td>
<td style="width: 209px" class="CDPAlignCenter CDPAlign">
<p><strong>Parametric Tests</strong></p>
</td>
<td style="width: 266px" class="CDPAlignCenter CDPAlign">
<p><strong>Non-Parametric Tests</strong></p>
</td>
</tr>
<tr>
<td style="width: 252px" class="CDPAlignCenter CDPAlign">
<p><strong>Test Statistic</strong></p>
</td>
<td style="width: 209px" class="CDPAlignCenter CDPAlign">
<p>Distribution</p>
</td>
<td style="width: 266px" class="CDPAlignCenter CDPAlign">
<p>Arbitrary or "Distribution-Free"</p>
</td>
</tr>
<tr>
<td style="width: 252px" class="CDPAlignCenter CDPAlign">
<p><strong>Attribute Type</strong></p>
</td>
<td style="width: 209px" class="CDPAlignCenter CDPAlign">
<p>Numeric</p>
</td>
<td style="width: 266px" class="CDPAlignCenter CDPAlign">
<p>Nominal and Ordinal</p>
</td>
</tr>
<tr>
<td style="width: 252px" class="CDPAlignCenter CDPAlign">
<p><strong>Central Tendency Measures</strong></p>
</td>
<td style="width: 209px" class="CDPAlignCenter CDPAlign">
<p>Mean</p>
</td>
<td style="width: 266px" class="CDPAlignCenter CDPAlign">
<p>Median</p>
</td>
</tr>
<tr>
<td style="width: 252px" class="CDPAlignCenter CDPAlign">
<p><strong>Correlation Tests</strong></p>
</td>
<td style="width: 209px" class="CDPAlignCenter CDPAlign">
<p>Pearson's Correlation</p>
</td>
<td style="width: 266px" class="CDPAlignCenter CDPAlign">
<p>Spearman's Correlation</p>
</td>
</tr>
<tr>
<td style="width: 252px" class="CDPAlignCenter CDPAlign">
<p><strong>Information about Population</strong></p>
</td>
<td style="width: 209px" class="CDPAlignCenter CDPAlign">
<p>Complete Information</p>
</td>
<td style="width: 266px" class="CDPAlignCenter CDPAlign">
<p>No Information</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In the preceding table, you have seen examples of parametric and non-parametric tests based on various features such as test statistic, attribute type, central tendency measures, correlation tests, and population information. Finally, you made it to the end. In this chapter, we have explored the fundamentals of descriptive as well as inferential statistics with Python.</p>
<h1 id="uuid-1f6eaa23-e20e-4d44-a45b-9b1c482fbbed" class="CDPAlignLeft CDPAlign">Summary</h1>
<p class="CDPAlignLeft CDPAlign">The core fundamentals of statistics will provide the foundation for data analysis, facilitating how data is described and understood. In this chapter, you have learned the basics of statistics such as attributes and their different types such as nominal, ordinal, and numeric. You have also learned about mean, median, and mode for measuring central tendency. Range, IQR, variance, and standard deviation measures are used to estimate variability in the data; skewness and kurtosis are used for understanding data distribution; covariance and correlation are used to understand the relationship between variables. You have also seen inferential statistics topics such as the central limit theorem, collecting samples, and parametric and non-parametric tests. You have also performed hands-on coding on statistics concepts using the <kbd>pandas</kbd> and <kbd>scipy.stats</kbd> libraries.</p>
<p class="CDPAlignLeft CDPAlign">The next chapter, <a href="0f9d094c-3d35-438c-bff9-368f2d6a9dd6.xhtml">Chapter 4</a>, <em>Linear Algebra</em>, will help us to learn how to solve the linear system of equations, find Eigenvalues and Eigenvectors, and learn about binomial and normal distribution, normality tests, and masked arrays using the Python packages NumPy and SciPy.</p>


            </article>

            
        </section>
    </body></html>