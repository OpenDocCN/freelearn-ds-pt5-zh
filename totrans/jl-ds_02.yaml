- en: Chapter 2. Data Munging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is said that around 50% of the data scientist's time goes into transforming
    raw data into a usable format. Raw data can be in any format or size. It can be
    structured like RDBMS, semi-structured like CSV, or unstructured like regular
    text files. These contain some valuable information. And to extract that information,
    it has to be converted into a data structure or a usable format from which an
    algorithm can find valuable insights. Therefore, usable format refers to the data
    in a model that can be consumed in the data science process. This usable format
    differs from use case to use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will guide you through data munging, or the process of preparing
    the data. It covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is data munging?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrames.jl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploading data from a file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the required data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joins and indexing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split-Apply-Combine strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshaping the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formula (ModelFrame and ModelMatrix)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PooledDataArray
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web scraping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is data munging?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Munging comes from the term "munge," which was coined by some students of Massachusetts
    Institute of Technology, USA. It is considered one of the most essential parts
    of the data science process; it involves collecting, aggregating, cleaning, and
    organizing the data to be consumed by the algorithms designed to make discoveries
    or to create models. This involves numerous steps, including extracting data from
    the data source and then parsing or transforming the data into a predefined data
    structure. Data munging is also referred to as data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: The data munging process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So what's the data munging process? As mentioned, data can be in any format
    and the data science process may require data from multiple sources. This data
    aggregation phase includes scraping it from websites, downloading thousands of
    `.txt` or `.log` files, or gathering the data from RDBMS or NoSQL data stores.
  prefs: []
  type: TYPE_NORMAL
- en: It is very rare to find data in a format that can be used directly by the data
    science process. The data received is generally in a format unsuitable for modeling
    and analysis. Generally, algorithms require data to be stored in a tabular format
    or in matrices. This phase of converting the gathered raw data into the required
    format can get very complex and time consuming. But this phase creates the foundation
    of the sophisticated data analysis that can now be done.
  prefs: []
  type: TYPE_NORMAL
- en: It is good to define the structure of the data that you will be feeding the
    algorithms in advance. This data structure is defined according to the nature
    of the problem. The algorithms that you have designed or will be designing should
    not just be able to accept this format of data, but they should also be able to
    easily identify the patterns, find the outliers, make discoveries, or meet whatever
    the desired outcomes are.
  prefs: []
  type: TYPE_NORMAL
- en: After defining how the data will be structured, you define the process to achieve
    that. This is like a pipeline that will accept some forms of data and will give
    out meaningful data in a predefined format. This phase consists of various steps.
    These steps include converting data from one form to another, which may or may
    not require string operations or regular expressions, and finding the missing
    values and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, data science problems revolve around two kinds of data. These two
    kinds of data will be either categorical or numerical. Categorical data comes
    with labels. These labels are formed by some group of values. For example, we
    can treat weather with categorical features. Weather can be sunny, cloudy, rainy,
    foggy, or snowy. These labels are formed when the underlying values are associated
    with one of the groups of the data (which comes under a label). These labels have
    some unique characteristics and we may not be able to apply arithmetic operations
    on them.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical data is much more common, for example, temperature. Temperature will
    be in floating-point numbers and we can certainly apply mathematical operations
    on it. Every value is comparable with other values in the dataset, so we can say
    that they have a direct relation with each other.
  prefs: []
  type: TYPE_NORMAL
- en: What is a DataFrame?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A DataFrame is a data structure that has labeled columns, which individually
    may have different data types. Like a SQL table or a spreadsheet, it has two dimensions.
    It can also be thought of as a list of dictionaries, but fundamentally, it is
    different.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames are the recommended data structure for statistical analysis. Julia
    provides a package called `DataFrames.jl` , which have all necessary functions
    to work with DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Julia''s package, DataFrames, provides three data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NA`: A missing value in Julia is represented by a specific data type, `NA.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataArray`: The array type defined in the standard Julia library, though it
    has many features, doesn''t provide any specific functionalities for data analysis.
    DataArray provided in `DataFrames.jl` provides such features (for example, if
    we required to store in an array some missing values).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrame`: DataFrame is 2-D data structure, like spreadsheets. It is much
    like R or pandas''s DataFrames, and provides many functionalities to represent
    and analyze data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NA data type and its importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the real world, we come across data with missing values. It is very common
    but it's not provided in Julia by default. This functionality is added using the
    `DataFrames.jl` package. The DataFrames package brings with it DataArray packages,
    which provide NA data type. Multiple dispatch is one of the most powerful features
    of Julia and NA is one such example. Julia has NA type, which provides the singleton
    object NA that we are using to represent missing values.
  prefs: []
  type: TYPE_NORMAL
- en: Why is the NA data type needed?
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose, for example, we have a dataset having floating-point numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will create a six-element `Array{Float64,1}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, suppose this dataset has a missing value at position [1]. That means instead
    of 1.1, there is no value. This cannot be represented by the array type in Julia.
    When we try to assign an NA value, we get this error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, right now we cannot add `NA` values to the array that we have created.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to load the data into an array that does have `NA` values, we use `DataArray.`
    This enables us to have NA values in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will create a six-element `DataArrays.DataArray{Float64,1}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, when we try to have an `NA` value, it gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, by using DataArrays, we can handle missing data. One more feature
    provided is that NA doesn't always affect functions applied on the particular
    dataset. So, the method that doesn't involve an NA value or is not affected by
    it can be applied on the dataset. If it does involve the NA value, then it will
    give NA as the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we are applying the mean function and `true || x`.
    The mean function doesn''t work as it involves an NA value, but `true || x` works
    as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: DataArray – a series-like data structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we discussed how DataArrays are used to store datasets
    containing missing (NA) values, as Julia's standard Array type cannot do so.
  prefs: []
  type: TYPE_NORMAL
- en: There are other features similar to Julia's Array type. Type aliases of Vector
    (one-dimensional Array type) and Matrix (two-dimensional Array type) are DataVector
    and DataMatrix provided by DataArray.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a 1-D DataArray is similar to creating an Array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have NA values, unlike in Arrays. Similarly, we can create a 2-D DataArray,
    which will be a DataMatrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous example, to the calculate mean, we used slicing. This is not
    a convenient method to remove or not to consider the NA values while applying
    a function. A much better way is to use `dropna`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: DataFrames – tabular data structures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Arguably, this is the most important and commonly used data type in statistical
    computing, whether it is in R (data.frame) or Python (Pandas). This is due to
    the fact that all the real-world data is mostly in tabular or spreadsheet-like
    format. This cannot be represented by a simple DataArray:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![DataFrames – tabular data structures](img/B05321_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This dataset, for example, can''t be represented using DataArray. The given
    dataset has the following features because it cannot be represented by DataArray:'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset has different types of data in different columns. These different
    data types in different columns cannot be represented using a matrix. Matrix can
    only contain values of one type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a tabular data structure and records have relations with other records
    in the same row of different columns. Therefore, it is a must that all the columns
    are of the same length. Vectors cannot be used because same-length columns cannot
    be enforced using them. Therefore, a column in DataFrame is represented by DataArray.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding example, we can see that the columns are labeled. This labeling
    helps us to easily become familiar with the data and access it without the need
    to remember its exact positions. So, the columns are accessible using numerical
    indices and also by their label.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, due to these reasons, the DataFrames package is used. So, DataFrames
    are used to represent tabular data having DataArrays as columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the given example, we constructed a DataFrame by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Using the keyword arguments, column names can be defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take another example by constructing a new DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To find out the size of the DataFrame created, we use the size function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, `10` refers to the number of rows and `2` refers to the number of columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'To view the first few lines of the dataset, we use `head()`, and for the last
    few lines, we use the `tail()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![DataFrames – tabular data structures](img/B05321_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we have given names to the columns of the DataFrame, these can be accessed
    using these names.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This simplifies access to the columns as we can give meaningful names to real-world
    datasets that have numerous columns without the need to remember their numeric
    indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'If needed, we can also rename using these columns by using the rename function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If there is a need to rename multiple columns, then it is done by using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: But right now, we are sticking to old column names for ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Julia also provides a function called `describe()`, which summarizes the entire
    dataset. For a dataset with many columns, it can turn out to be very useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Installation and using DataFrames.jl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Installation is quite straightforward as it is a registered Julia package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This adds all the required packages to the current namespace. To use the `DataFrames`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also good to have classical datasets that are common for learning purposes.
    These datasets can be found in the `RDatasets` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The list of the R packages available can be found using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you can see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It contains datasets available to R. To use this `dataset`, simply use the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here, dataset is the function that takes two arguments.
  prefs: []
  type: TYPE_NORMAL
- en: The first argument is the name of the package and the second is the name of
    the dataset that we want to load.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we loaded the famous iris dataset into the memory.
    You can see that the `dataset()` function has returned a DataFrame. The dataset
    contains five columns: `SepalLength`, `SepalWidth`, `PetalLength`, `PetalWidth`,
    and `Species`. It is quite easy to understand the data. A large number of samples
    have been taken for every species, and the length and width of sepal and petal
    have been measured, which can be used later to distinguish between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Installation and using DataFrames.jl](img/B05321_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Actual data science problems generally do not deal with the artificial randomly
    generated data or data read through the command line. But they work on data that
    is loaded from files or any other external source. These files can have data in
    any format and we may have to process it before loading it to the dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: Julia provides a `readtable()` function that can be used to read a tabular file
    in a dataframe. Generally, we come across datasets in comma-separated or tab-separated
    formats (CSV or TSV). The `readtable()` works perfectly with them.
  prefs: []
  type: TYPE_NORMAL
- en: We can give the location of the file as UTF8String and the separator type to
    the readtable() function as arguments. The default separator type is comma (',')
    for CSV, tab ('\t') for TSV, and whitespace (' ') for WSV.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we load the sample iris dataset into a dataframe using
    the `readtable()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Although the iris dataset is available in the RDatasets package, we will download
    the CSV to work with the external datasets. The iris CSV can be downloaded from
    [https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/data/iris.csv](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/data/iris.csv).
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember to put the downloaded CSV into the current working directory (from
    where the REPL was started—generally it is the `~/home/<username>` directory):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: It is the same dataset that we used in the previous example, but now we are
    loading the data from a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `readtable()` is used in a similar way for other text-based datasets such
    as TSV, WSV, or TXT. Suppose the same iris dataset is in TSV, WSV, or TXT format.
    It will be used in a similar way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And for example, if we have a dataset without a header and separated by `;`,
    we would use `readtable()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `readtable()` exploits Julia''s functionality of multiple dispatch and
    has been implemented with different method behaviors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We can see that there are three methods for the `readtable()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'These methods implement some of the advanced options to ease the loading and
    to support various kinds of data formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '`header::Bool`: In the iris example we used, we had headers such as Sepal Length,
    Sepal Width, and so on, which makes it easier to describe the data. But headers
    are not always available in the dataset. The default value of `header` is `true`;
    therefore, whenever headers are not available, we pass the argument as false.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`separator::Char`: Data in a file must have been organized in the file in a
    way to form a tabular structure. This is generally by using `,`, `\t`, `;`, or
    combinations of these sometimes. The `readtable()` guesses the separator type
    by the extension of the file, but it is a good practice to provide it manually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nastrings::Vector{ASCIIString}`: Suppose there are missing values or some
    other values and we want NA to replace them. This is done using nastrings. By
    default, it takes empty records and replaces them with NA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truestrings::Vector{ASCIIString}`: This transforms the strings to Boolean,
    true. It is used when we want a set of strings to be treated as true in the dataset.
    By default, `True`, `true`, `T`, and `t` are transformed if no argument is given.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`falsestrings::Vector{ASCIIString}`: This works just like truestrings but transforms
    the strings to Boolean, false. By default, `False`, `false`, `F`, and `f` are
    transformed if no argument is given.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nrows::Int`: If we want only a specific number of rows to be read by `readtable()`,
    we use nrows as the argument. By default, it is `-1`, which means that `readtable()`
    will read the whole file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names::Vector{Symbol}`: If we want some specific names for our columns, different
    from what is mentioned in the header, then we use names. Here, we pass a vector
    having the names of the columns that we want to use. By default, it is `[]`, which
    means the names in the headers should be used if they are there; otherwise, the
    numeric indices must be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eltypes::Vector{DataType}`: We can specify the column types by passing a vector,
    by using eltypes. It is an empty vector (`[]`) by default if nothing is passed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`allowcomments::Bool`: In the dataset, we may have records having comments
    with them. These comments can be ignored. By default, it is `false`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commentmark::Char`: If we are using allowcomments, we will also have to mention
    the character (symbol) where the comment starts. By default, it is `#`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ignorepadding::Bool`: Our dataset might not be as perfect as we want. The
    records may contain whitespace characters on either side. This can be ignored
    using ignorepadding. By default, it is true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipstart::Int`: Our dataset can have some rows describing the data with the
    header that we might not want, or we just want to skip the first few rows. This
    is done by skipstart, by specifying the number of rows to skip. By default, it
    is 0 and will read the entire file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skiprows::Vector{Int}`: If want to skip some specific rows in the data then
    skiprows is used. We only need to specify the indices of the rows in a vector
    that we want to skip. By default, it is `[]` and will read the entire file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skipblanks::Bool`: As mentioned earlier, our dataset may not be perfect. There
    can be some blank lines if we have scraped the data from the Web or extracted
    the data from other sources. We can skip these blank lines by using skipblanks.
    By default it is true, but we can choose otherwise if we do not want it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoding::Symbol`: We can specify the encoding of the file if it is other
    than UTF8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing the data to a file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We may also want to output our results or transform a dataset and store it in
    a file. In Julia we do this by using the `writetable()` function. It is very similar
    to the `readtable()` function that we discussed in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we want to write the `df_iris_sample` dataframe into a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This is the way of writing to a file with the default set of arguments. One
    visible difference is that we are passing the dataframe that we want to write
    with the name of the file that we want to write to.
  prefs: []
  type: TYPE_NORMAL
- en: '`writetable()` also accepts various arguments such as `readtable()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We could have also written the previous statement like this with the separator
    defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, we can have a header and quote marks in the arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Working with DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will follow or inherit some of the traditional strategies to manipulate the
    data. We will go through these strategies and methods in this section and discuss
    how and why they are important to data science.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DataFrames joins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While working with multiple datasets, we often need to merge the datasets in
    a particular fashion to make the analysis easier or to use it with a particular
    function.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the *Road Safety Data* published by the Department for Transport,
    UK, and it is open under the OGL-Open Government Licence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The datasets can be found here: [https://data.gov.uk/dataset/road-accidents-safety-data](https://data.gov.uk/dataset/road-accidents-safety-data).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Road Safety: Accidents 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Road Safety: Vehicles 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`DfTRoadSafety_Accidents_2015` contains columns such as `Accident_Index`, `Location_Easting_OSGR`,
    `Location_Northing_OSGR`, `Longitude`, `Latitude`, `Police_Force`, `Accident_Severity`,
    `Number_of_Vehicles`, `Number_of_Casualties`, `Date`, `Day_of_Week`, `Time`, and
    so on. `DfTRoadSafety_Vehicles_2015` contains columns such as `Accident_Index`,
    `Vehicle_Reference`, `Vehicle_Type`, `Towing_and_Articulation`, `Vehicle_Manoeuvre`,
    `Vehicle_Location-Restricted_Lane`, `Junction_Location`, `Skidding_and_Overturning`,
    `Hit_Object_in_Carriageway`, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that `Accident_Index` is a common field and is unique. It is used
    as the index in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will be making the DataFrames package available and then we will load
    the data. We load the data into two different dataframes using the readtable function
    that we discussed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![Understanding DataFrames joins](img/image_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first dataset is loaded into the DataFrame and we try getting information
    about the dataset using `head`. It gives a few starting columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are more interested in knowing the names of the columns, we can use the
    `names` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we will be loading the second dataset in a dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The second dataset is loaded into the memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later we will delve deeper, but for now let''s do a full join between the two
    datasets. A join between these two datasets will tell us which accident involved
    which vehicles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![Understanding DataFrames joins](img/image_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the full join has worked. Now we have the data, which can tell
    us the time of the accident, the location of the vehicle, and many more details.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit is that the join is really easy to do and is really quick, even
    over large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have read about other joins available in relation databases. Julia''s DataFrames
    package provides these joins too:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inner join**: The output, which is the DataFrame, contains only those rows
    that have keys in both the dataframes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Left join**: The output DataFrame has the rows for keys that are present
    in the first (left) DataFrame, irrespective of them being present in the second
    (right) DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Right join**: The output DataFrame has the rows for keys that are present
    in the second (right) DataFrame, irrespective of them being present in the first
    (left) DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outer join**: The output DataFrame has the rows for the keys that are present
    in the first or second DataFrame, which we are joining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi join**: The output DataFrame has only the rows from the first (left)
    DataFrame for the keys that are present in both the first (left) and second (right)
    DataFrames. The output contains only rows from the first DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anti join**: The output DataFrame has the rows for keys that are present
    in the first (left) DataFrame but rows for the same keys are not present in the
    second (right) DataFrame. The output contains only rows from the first DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross join**: The output DataFrame has the rows that are the Cartesian product
    of the rows from the first DataFrame (left) and the second DataFrame (right).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cross join doesn''t involve a key; therefore it is used like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Here we have used the `kind` argument to pass the type of join that we want.
    Other joins are also done using this argument.
  prefs: []
  type: TYPE_NORMAL
- en: The kind of join that we want to use is done using the `kind` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this using a simpler dataset. We will create a dataframe
    and will apply different joins on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'For left join, we can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This created two dataframes having 10 rows. The first dataframe, df1, has three
    columns: `ID`, `City`, and `RandomValue1`. The second dataframe has df2 with three
    columns: `ID`, `City`, and `RandomValue2`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying full join, we can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We have used two columns to apply the join.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will generate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding DataFrames joins](img/image_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Other joins can also be applied using the `kind` argument. Let's go through
    our old dataset of accidents and vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: 'The different joins using `kind` are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The Split-Apply-Combine strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A paper was published by Hadley Wickham (Wickham, Hadley. "The split-apply-combine
    strategy for data analysis." *Journal of Statistical Software* 40.1 (2011): 1-29),
    defining the Split-Apply-Combine strategy for data analysis. In this paper, he
    explained why it is good to break up a big problem into manageable pieces, independently
    operate on each piece, obtain the necessary results, and then put all the pieces
    back together.'
  prefs: []
  type: TYPE_NORMAL
- en: This is needed when a dataset contains a large number of columns and for some
    operations all the columns are not necessary. It is better to split the dataset
    and then apply the necessary functions; and we can always put the dataset back
    together.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done using the by function by takes three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame (this is the dataframe that we would be splitting)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The column name (or numerical index) on which the DataFrame would be split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that can be applied on every subset of the DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s try to apply by to our same dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Split-Apply-Combine strategy](img/B05321_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `aggregate()` function provides an alternative to apply the Split-Apply-Combine
    strategy. The `aggregate()` function uses the same three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame (this is the DataFrame that we would be splitting)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The column name (or numerical index) on which the DataFrame would be split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that can be applied on the every subset of the DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function provided in the third argument is applied to every column, which
    wasn't used in splitting up the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The use case may require data to be in a different shape than we currently have.
    To facilitate this, Julia provides reshaping of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the same dataset that we were using, but before that let''s check
    the size of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We can see that there are greater than 100,000 rows. Although we can work on
    this data, for simplicity of understanding, let's take a smaller dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets provided in RDataset are always good to start with. We will use the
    tried and tested iris dataset for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will import `RDatasets` and `DataFrames` (if we have started a new terminal
    session):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will load the iris dataset into a `DataFrame`. We can see that the
    dataset has 150 rows and 5 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reshaping the data](img/B05321_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we use the `stack()` function to reshape the dataset. Let's use it without
    any arguments except the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stack works by creating a dataframe for categorical variables with all of the
    information one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reshaping the data](img/B05321_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that our dataset has been stacked. Here we have stacked all the
    columns. We can also provide specific columns to stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The second argument depicts the columns that we want to stack. We can see in
    the result that column 1 to 4 have been stacked, which means we have reshaped
    the dataset into a new dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![Reshaping the data](img/image_02_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that there is a new column `:id`. That's the identifier of the stacked
    dataframe. Its value is repeated the number of times the rows are repeated.
  prefs: []
  type: TYPE_NORMAL
- en: As all the columns are included in the resultant DataFrame, there is repetition
    for some columns. These columns are actually the identifiers for this DataFrame
    and are denoted by the column (`id`). Other than the identifiers column (`:id`),
    there are two more columns, `:variable` and `:values`. These are the columns that
    actually contain the stacked values.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reshaping the data](img/image_02_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can also provide a third argument (optional). This is the column whose values
    are repeated. Using this, we can specify which column to include and which not
    to include.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reshaping the data](img/B05321_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `melt()` function is similar to the stack function but has some special
    features. Here we need to specify the identifier columns and the rest are stacked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![Reshaping the data](img/B05321_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The remaining columns are stacked with the assumption that they contain measured
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Opposite to stack and melt is unstack, which is used to convert from a long
    format to wide format. We need to specify the identifier columns and variable/value
    columns to the unstack function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![Reshaping the data](img/B05321_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`:id` (identifier) in the arguments of the unstack can be skipped if the remaining
    columns are unique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`meltdf` and `stackdf` are two additional functions that work like melt and
    stack but also provide a view into the original wide DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![Reshaping the data](img/B05321_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This seems exactly similar to the stack function, but we can see the difference
    by looking at their storage representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To look at the storage representation, dump is used. Let''s apply it to the
    stack function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reshaping the data](img/B05321_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that `:variable` is of type `Array(Symbol,(600,))`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:value` is of type `DataArrays.DataArray{Float64,1}(600)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifier (`:Species`) is of type `DataArrays.PooledDataArray{ASCIIString,UInt8,1}(600)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will look at the storage representation of `stackdf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reshaping the data](img/B05321_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: '`:variable` is of type `DataFrames.RepeatedVector{Symbol}`. Variable is repeated
    n times, where n refers to the number of rows in the original `AbstractDataFrame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`:value` is of type `DataFrames.StackedVector`. This facilitates the view of
    the columns stacked together as in the original DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Identifier (`:Species`) is of type `Species: DataFrames.RepeatedVector{ASCIIString}`.
    The original column is repeated n times where n is the number of the columns stacked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these AbstractVectors, we are now able to create views, thus saving memory
    by using this implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping functions don't provide the capabilities to perform aggregation. So
    to perform aggregation, a combination of the Split-Apply-Combine strategy with
    reshaping is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `iris_stack`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![Reshaping the data](img/B05321_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we created a new column having the mean values of the columns according
    to the species. We can now unstack this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Reshaping the data](img/B05321_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Sorting a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sorting is one of the most used techniques in data analysis. Sorting is facilitated
    in Julia by calling the `sort` or `sort!` function.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the `sort` and `sort!` is that `sort!` works in-place,
    which sorts the actual array rather than creating a copy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `sort!` function on the iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sorting a dataset](img/B05321_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the columns are not sorted according to `[:SepalLength, :SepalWidth,
    :PetalLength, :PetalWidth]`. But these are actually sorted according to the :Species
    column.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sorting function takes some arguments and provides a few features. For
    example, to sort in reverse, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To sort some specific columns, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We can also use the by function with `sort!` to apply another function on the
    DataFrame or the single column.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sorting a dataset](img/B05321_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`order` is used to specify ordering a specific column amongst a set of columns.'
  prefs: []
  type: TYPE_NORMAL
- en: Formula - a special data type for mathematical expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data science involves various statistical formulas to get insights from data.
    The creation and application of these formulas is one of the core processes of
    data science. It maps input variables with some function and mathematical expression
    to an output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Julia facilitates this by providing a formula type in the `DataFrame` package,
    which is used with the symbol `~`. `~` is a binary operator. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: For statistical modeling, it is recommended to use ModelMatrix, which constructs
    a Matrix{Float64}, making it more suited to fit in a statistical model. Formula
    can also be used to transform to a ModelFrame object from a DataFrame, which is
    a wrapper over it, to meet the needs of statistical modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a dataframe with random values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Formula - a special data type for mathematical expressions](img/B05321_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Use formula to transform it into a `ModelFrame` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Formula - a special data type for mathematical expressions](img/B05321_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Creating a `ModelMatrix` from a `ModelFrame` is quite easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Formula - a special data type for mathematical expressions](img/B05321_02_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There is an extra column containing only `value = 1.0`. It is used in a regression
    model to fit an intercept term.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To analyze huge datasets efficiently, PooledDataArray is used. DataArray uses
    an encoding that represents a full string for every entry of a vector. This is
    not very efficient, especially for large datasets and memory-intensive algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our use case more often deals with factors involving a small number of levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling data](img/B05321_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`PooledDataArray` uses indices in a small pool of levels instead of strings
    to represent data efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling data](img/B05321_02_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`PooledDataArray` also provides us with the functionality to find out the levels
    of the factor using the levels function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling data](img/B05321_02_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`PooledDataArray` even provides a compact function to efficiently use memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it provides a pool function for converting a single column when factors
    are encoded not in `PooledDataArray` columns but in DataArray or DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![Pooling data](img/B05321_02_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`PooledDataArray` facilitates the analysis of categorical data, as columns
    in ModelMatrix are treated as 0-1 indicator columns. Each of the levels of PooledDataArray
    is associated with one column.'
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Real-world use cases also include scraping data from the Web for analysis. Let's
    build a small web scraper to fetch Reddit posts.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we will need the JSON and Requests packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Here, we defined a URL from where we will be scraping the data. We are scraping
    from Julia's section on Reddit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we are getting the content from the defined URL using the get function
    from the Requests package. We can see that we''ve got response 200 OK with the
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We then parse the JSON data received using the JSON parser provided by the JSON
    package of Julia. We can now start reading the record.
  prefs: []
  type: TYPE_NORMAL
- en: '![Web scraping](img/B05321_02_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can store the data received in an Array or DataFrame (depending on the use
    case and ease of use). Here, we are using an Array to store the parsed data. We
    can check the data stored in an Array.
  prefs: []
  type: TYPE_NORMAL
- en: '![Web scraping](img/B05321_02_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Suppose we only need to see the title of these posts and know what we have scraped;
    we just need to know in which column they are.
  prefs: []
  type: TYPE_NORMAL
- en: '![Web scraping](img/B05321_02_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can now see the title of the Reddit posts. But what if we had too many columns
    or we had some missing values? DataFrames would definitely be a better option.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned what data munging is and why it is necessary for
    data science. Julia provides functionalities to facilitate data munging with the
    DataFrames.jl package, with features such as these:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NA`: A missing value in Julia is represented by a specific data type, NA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataArray`: DataArray provided in the `DataFrames.jl` provides features such
    as allowing us to store some missing values in an array.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrame`: DataFrame is 2-D data structure like spreadsheets. It is very
    similar to R or pandas''s dataframes, and provides many functionalities to represent
    and analyze data. DataFrames has many features well suited for data analysis and
    statistical modeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dataset can have different types of data in different columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Records have a relation with other records in the same row of different columns
    of the same length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columns can be labeled. Labeling helps us to easily become familiar with the
    data and access it without the need to remember their numerical indices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learned about importing data from a file using the `readtable()` function
    and exporting data to a file. The `readtable()` function provides flexibility
    when using many arguments.
  prefs: []
  type: TYPE_NORMAL
- en: We also explored joining of datasets, such as RDBMS tables. Julia provides various
    joins that we can exploit according to our use case.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the Split-Apply-Combine Strategy, one of the most widely used techniques
    deployed by data scientists, and why it is needed. We went through reshaping or
    pivoting data using stack and melt (stackdf, meltdf) functions and explored the
    various possibilities involved. We were also introduced to `PooledDataArray` and
    learned why it is required for efficient memory management.
  prefs: []
  type: TYPE_NORMAL
- en: We were introduced to web scraping, which is sometimes a must for a data scientist
    to gather data. We also used the Requests package to fetch an HTTP response.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[http://julia.readthedocs.org/en/latest/manual/](http://julia.readthedocs.org/en/latest/manual/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://dataframesjl.readthedocs.io/en/latest/](http://dataframesjl.readthedocs.io/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://data.gov.uk/dataset/road-accidents-safety-data](https://data.gov.uk/dataset/road-accidents-safety-data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wickham, Hadley. "The split-apply-combine strategy for data analysis." *Journal
    of Statistical Software* 40.1 (2011): 1-29'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
